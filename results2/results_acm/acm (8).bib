@inproceedings{10.1145/3382025.3414956,
author = {Grüner, Sten and Burger, Andreas and Kantonen, Tuomas and Rückert, Julius},
title = {Incremental migration to software product line engineering},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414956},
doi = {10.1145/3382025.3414956},
abstract = {Current market developments require organizations to produce high-quality tailored software variants while simultaneously coping with increased software complexity. Software Product Line Engineering (SPLE) is a well-known approach to manage this variability and complexity, however no step-wise migration process is available allowing a co-existence of SPLE along with established development processes. In this paper, we introduce an incremental SPLE migration strategy and process starting from using the feature model as synchronized product and variant documentation. They can be applied as a first step of SPLE migration along with the continuous software development cycle. We performed initial steps of the process on industrial low voltage drive embedded firmware spanning around few millions lines of code using a commercial SPLE tool and validated short-term benefits by means of stakeholder feedback.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {5},
numpages = {11},
keywords = {incremental migration, migration process, non-invasive migration, software product line engineering},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382026.3425771,
author = {Morais Ferreira, David and Becker, Martin and Tenev, Vasil L.},
title = {Experience Report on Variability Improvement in a Product Line Engineering Unaware Company},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425771},
doi = {10.1145/3382026.3425771},
abstract = {Product Line Engineering (PLE) enables strategic reuse within an organisation, thus reducing development costs, decreasing the time to market, and increasing product quality. As a core activity in PLE, variability management supports modelling of commonality and variability throughout the engineering life cycle. Given the increased complexity of modern software-intensive systems, variability management is becoming increasingly important. Transitioning to PLE approaches is a challenging task, as potential benefits must be carefully weighed against costs introduced by PLE approaches. This paper presents a collaborative approach for reverse-engineering variability and configuration knowledge with minimal domain expert involvement and provides insights into the experience we gained from our industrial collaboration.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {21–28},
numpages = {8},
keywords = {Experience report, industrial collaboration, product line engineering, reverse engineering, variability improvement},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461002.3473951,
author = {Morais Ferreira, David and Tenev, Vasil L. and Becker, Martin},
title = {Product-line analysis cookbook: a classification system for complex analysis toolchains},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473951},
doi = {10.1145/3461002.3473951},
abstract = {Adopting Product Line Engineering (PLE) approaches in the context of software-intensive systems reduces overall development and maintenance costs, reduces time to market and leads to an overall improvement in product quality. The Software and System Product Line (SPL) community has provided a large number of different analysis approaches and tools, which were developed in different contexts, answer different questions, and can contribute to the fulfillment of different analysis goals. Typically, these analysis tools are initially developed as part of a research study, where they serve a specific purpose, e. g. for investigating the use of a new technology, or to demonstrate the transfer of methods from other fields. Generally, such purpose is aligned with a specific, but not explicitly stated, high-level goal. The pursuit of these goals requires holistic approaches, i. e. integrated toolchains and classification of analyses, which are documented as a centralized collection of wisdom. Therefore, we propose a classification system which describes existing analyses and reveals possible combinations, i. e. integrated toolchains, and provide first examples. This method supports the search for toolchains which address complex industrial needs. With the support of the SPL community, we hope to collaboratively document existing analyses and corresponding goals on an open platform.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {99–104},
numpages = {6},
keywords = {holistic toolchain, product line engineering, product-line aware analyses, reverse engineering},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {deep neural networks, machine learning, software product lines, transfer learning, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233045,
author = {Becker, Martin and Zhang, Bo},
title = {How do our neighbours do product line engineering? a comparison of hardware and software product line engineering approaches from an industrial perspective},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233045},
doi = {10.1145/3233027.3233045},
abstract = {Product line engineering (PLE) approaches have been followed in industry for hardware and software solutions for more than three decades now. However, the different engineering disciplines (e.g. mechanics, electrics, software) have developed and evolved their approaches within their own realms, which is fine as long as there is no need for integrated approaches. Driven by the increasing complexity of systems, there is a rising need for interdisciplinary systems engineering these days. Companies engineering cyber-physical systems and their components have to integrate product line engineering approaches across the involved engineering disciplines to enable a global optimization of portfolio, solution structures, and assets along their lifecycle. From a bird's-eye view, there is noticeable commonality but also variety in the approaches followed for PLE in the different engineering disciplines, which renders the integration of approaches a non-trivial endeavour. In order to foster the development of integrated PLE approaches, this paper explores, maps, and compares PLE approaches in the field of hardware and software engineering. Furthermore, the paper identifies integration opportunities and challenges. As the paper targets industrial practitioners, it mainly provides references to respective industrial events and material and does not fully cover related work in the respective research communities.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {190–195},
numpages = {6},
keywords = {SPLC, academia, industry, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.infsof.2020.106389,
author = {Chacón-Luna, Ana Eva and Gutiérrez, Antonio Manuel and Galindo, José A. and Benavides, David},
title = {Empirical software product line engineering: A systematic literature review},
year = {2020},
issue_date = {Dec 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {128},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106389},
doi = {10.1016/j.infsof.2020.106389},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {22},
keywords = {Software product lines, Empirical strategies, Case study, Experiment, Systematic literature review}
}

@inproceedings{10.1145/3382026.3425774,
author = {Rincón, Luisa and Mazo, Raúl and Salinesi, Camille},
title = {A multi-company empirical evaluation of a framework that evaluates the convenience of adopting product line engineering},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425774},
doi = {10.1145/3382026.3425774},
abstract = {Companies considering adopting a product line engineering approach should ideally analyze the pros and cons to determine the sound reasons for this decision. In order to support this analysis, in previous work we proposed the APPLIES evaluation framework. This framework provides information to evaluate the convenience of adopting a product line engineering approach.This paper presents an empirical evaluation of APPLIES. This experience includes 18 potential practitioners that used the framework to evaluate the convenience of adopting product line engineering in 19 different companies. The collected evidence was used to evaluate the perceived usefulness, intention to use and ease of use of APPLIES. The results presented increase confidence that APPLIESis a useful tool, but also identify some possibilities for improvement. In addition, four categories for classifying potential adopters of product line engineering emerged during the analysis of the results: unprepared adopter, potential adopter, ready adopter, and unmotivated adopter. These categories could be useful to classify companies that are considering adopting product line engineering.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {13–20},
numpages = {8},
keywords = {Empirical evaluation, ease of use, intention to use, perceived usefulness, product line engineering adoption},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382025.3414953,
author = {Abbas, Muhammad and Jongeling, Robbert and Lindskog, Claes and Enoiu, Eduard Paul and Saadatmand, Mehrdad and Sundmark, Daniel},
title = {Product line adoption in industry: an experience report from the railway domain},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414953},
doi = {10.1145/3382025.3414953},
abstract = {The software system controlling a train is typically deployed on various hardware architectures and must process various signals across those deployments. The increase of such customization scenarios and the needed adherence of the software to various safety standards in different application domains has led to the adoption of product line engineering within the railway domain. This paper explores the current state-of-practice of software product line development within a team developing industrial embedded software for a train propulsion control system. Evidence is collected using a focus group session with several engineers and through inspection of archival data. We report several benefits and challenges experienced during product line adoption and deployment. Furthermore, we identify and discuss improvement opportunities, focusing mainly on product line evolution and test automation.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {3},
numpages = {11},
keywords = {challenges and opportunities, overloaded assets, software product-line engineering},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2934466.2934491,
author = {Fogdal, Thomas and Scherrebeck, Helene and Kuusela, Juha and Becker, Martin and Zhang, Bo},
title = {Ten years of product line engineering at Danfoss: lessons learned and way ahead},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934491},
doi = {10.1145/2934466.2934491},
abstract = {Software and systems product line engineering (PLE) has been an established approach for reducing time to market as well as cost and increasing quality in a set of related products for two decades now. Although there is a huge body of knowledge on PLE, adopting a concrete PLE approach is still not a trivial endeavor for interested companies. With the increasing importance of development speed, the advent of agile engineering approaches, and decreasing management interest in improvements that require large organizational transformations and only show benefits after several years, companies are facing challenges in successfully adopting this approach. They often hesitate as there is no clear adoption path, nor any certainty, that the intended improvement steps will also provide added value in the short- and mid-term perspective. In consequence, a considerable amount of PLE potential still remains unexploited.To help such companies with the adoption of PLE, the goal of this paper is to provide inspiration and evidence that PLE is a sound approach and its successful introduction is possible even in settings that differ substantially from those of pioneer product lines.To this end, this paper presents the following main contributions with the PLE adoption case at Danfoss Drives: an overview of the key change drivers and the motivation for adopting a PLE approach, a discussion of incremental PLE introduction in an agile engineering context, a presentation of the current PLE setting with a focus on key concepts, and finally a presentation of motivators and directions for future improvements.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {252–261},
numpages = {10},
keywords = {industrial experiences, product line adoption, product line evaluation},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3106195.3106223,
author = {Iglesias, Aitziber and Lu, Hong and Arellano, Cristóbal and Yue, Tao and Ali, Shaukat and Sagardui, Goiuria},
title = {Product Line Engineering of Monitoring Functionality in Industrial Cyber-Physical Systems: A Domain Analysis},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106223},
doi = {10.1145/3106195.3106223},
abstract = {In recent years, manufacturing technology is evolving and progressively becoming more dynamic and complex. This means that manufacturing technology (e.g., based on Industry 4.0) should be able to control the production process at runtime by monitoring physical elements and adapting itself. Such functionality is aimed at increasing production effectiveness and reducing the production cost. We argue that monitoring process can be viewed as a software product line having commonalities and variability. To support our argument, we analyzed and conducted domain analysis of two monitoring systems of Industrial Cyber-Physical Systems (ICPSs) from two industrial domains including automated warehouses and press machines. Based on the domain analysis, we present a common solution for monitoring including a software product line. With such product line, a user can configure, monitor, and visualize data of an ICPS at runtime. However, such solution could not handle the dynamic functionality related to monitoring of ICPS. Thus, we propose the use of dynamic product line and present a set of research questions that must be addressed for such solution.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {195–204},
numpages = {10},
keywords = {Cyber Physical System, Dynamic Software Product Line, Industrial domains, Software Product Line},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2791060.2791067,
author = {Yue, Tao and Ali, Shaukat and Selic, Bran},
title = {Cyber-physical system product line engineering: comprehensive domain analysis and experience report},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791067},
doi = {10.1145/2791060.2791067},
abstract = {Cyber-Physical Systems (CPSs) are the future generation of highly connected embedded systems having applications in diverse domains including Oil and Gas. Employing Product Line Engineering (PLE) is believed to bring potential benefits with respect to reduced cost, higher productivity, higher quality, and faster time-to-market. However, relatively few industrial field studies are reported regarding the application of PLE to develop large-scale systems, and more specifically CPSs. In this paper, we report about our experiences and insights gained from investigating the application of model-based PLE at a large international organization developing subsea production systems (typical CPSs) to manage the exploitation of oil and gas production fields. We report in this paper 1) how two systematic domain analyses (on requirements engineering and product configuration/derivation) were conducted to elicit CPS PLE requirements and challenges, 2) key results of the domain analysis (commonly observed in other domains), and 3) our initial experience of developing and applying two Model Based System Engineering (MBSE) PLE solution to address some of the requirements and challenges elicited during the domain analyses.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {338–347},
numpages = {10},
keywords = {cyber physical system (CPS), domain analysis, model based system engineering, product line engineering (PLE), requirements engineering},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3461001.3473060,
author = {Schäfer, Andreas and Becker, Martin and Andres, Markus and Kistenfeger, Tim and Rohlf, Florian},
title = {Variability realization in model-based system engineering using software product line techniques: an industrial perspective},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473060},
doi = {10.1145/3461001.3473060},
abstract = {Efficiently handling system variants is rising of importance in industry and challenges the application of model-based systems engineering.This paper reveals the increasing industrial demand of guidance and decision support on how to handle variants and variability within SysML and UML models. While a substantial amount of variability realization approaches has already been published on source code level, there is little guidance for practitioners on system model level. Hence, there is major uncertainty in dealing with system changes or concurrent system modeling of related system. Due to a poor modularization and variability realization these model variants are ending up in interwoven and complex system models.In this paper, we aim to raise awareness of the need for appropriate guidance and decision support, identify important contextual factors of MBSE that influence variability realization, and derive well known variability mechanisms used in software coding for their applicability in system modeling.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {25–34},
numpages = {10},
keywords = {SysML, UML, decision support, model-based systems engineering, system and software product line engineering, variability mechanism, variability realization, variant management},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1007/978-3-030-64148-1_6,
author = {Hayashi, Kengo and Aoyama, Mikio},
title = {A Portfolio-Driven Development Model and Its Management Method of Agile Product Line Engineering Applied to Automotive Software Development},
year = {2020},
isbn = {978-3-030-64147-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64148-1_6},
doi = {10.1007/978-3-030-64148-1_6},
abstract = {In recent automotive systems development, realizing both variability and agility is the key competitiveness to meet the diverse requirements in global markets and rapidly increasing intelligent functions. This article proposes a portfolio-driven development method and its management method of APLE (Agile Product Line Engineering). The proposed method is intended to manage agile evolution of multiple product lines while increasing variability of products. To establish a portfolio management of development resources, it is necessary for an organization to manage multiple product lines on APLE in an entire development. We propose a portfolio-driven development method of three layers on APLE and its management method based on a concept of portfolio management life cycle. We applied the proposed management model and method to the multiple product lines of automotive software systems, and demonstrated an improvement of manageability with better predictability of both productivity and development size. This article contributes to provide an entire development management method for APLE, and its practical experience in the automotive multiple product lines.},
booktitle = {Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings},
pages = {88–105},
numpages = {18},
keywords = {Software product line, Agile software development, Agile product line engineering, Portfolio management, Automotive software development},
location = {Turin, Italy}
}

@inproceedings{10.1145/2648511.2648545,
author = {Nasr, Sana Ben and Sannier, Nicolas and Acher, Mathieu and Baudry, Benoit},
title = {Moving toward product line engineering in a nuclear industry consortium},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648545},
doi = {10.1145/2648511.2648545},
abstract = {Nuclear power plants are some of the most sophisticated and complex energy systems ever designed. These systems perform safety critical functions and must conform to national safety institutions and international regulations. In many cases, regulatory documents provide very high level and ambiguous requirements that leave a large margin for interpretation. As the French nuclear industry is now seeking to spread its activities outside France, it is but necessary to master the ins and the outs of the variability between countries safety culture and regulations. This sets both an industrial and a scientific challenge to introduce and propose a product line engineering approach to an unaware industry whose safety culture is made of interpretations, specificities, and exceptions.This paper presents our current work within the French R&amp;D project CONNEXION, while introducing variability modeling to the French nuclear industry. In particular, we discuss the background, the quest for the best variability paradigm, the practical modeling of requirements variability as well as the mapping between variable requirements and variable architecture elements.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {294–303},
numpages = {10},
keywords = {product line engineering, regulations, requirements variability modeling, variability mining},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Human-computer interaction, Machine Learning, Product Line Architecture},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/2593882.2593888,
author = {Metzger, Andreas and Pohl, Klaus},
title = {Software product line engineering and variability management: achievements and challenges},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593888},
doi = {10.1145/2593882.2593888},
abstract = {Software product line engineering has proven to empower organizations to develop a diversity of similar software-intensive systems (applications) at lower cost, in shorter time, and with higher quality when compared with the development of single systems. Over the last decade the software product line engineering research community has grown significantly. It has produced impressive research results both in terms of quality as well as quantity. We identified over 600 relevant research and experience papers published within the last seven years in established conferences and journals. We briefly summarize the major research achievements of these past seven years. We structure this research summary along a standardized software product line framework. Further, we outline current and future research challenges anticipated from major trends in software engineering and technology.},
booktitle = {Future of Software Engineering Proceedings},
pages = {70–84},
numpages = {15},
keywords = {Software product lines, design, quality assurance, requirements engineering, variability management, variability modeling},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@article{10.1016/j.infsof.2018.01.016,
author = {Soares, Larissa Rocha and Schobbens, Pierre-Yves and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Feature interaction in software product line engineering: A systematic mapping study},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.01.016},
doi = {10.1016/j.infsof.2018.01.016},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {44–58},
numpages = {15},
keywords = {Feature interaction, Software product lines, Systematic mapping}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Michał and Bąk, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2362536.2362571,
author = {Flores, Rick and Krueger, Charles and Clements, Paul},
title = {Mega-scale product line engineering at General Motors},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362571},
doi = {10.1145/2362536.2362571},
abstract = {General Motors faces probably the most complex Systems and Software Product Line Engineering (PLE) challenges ever, in terms of product complexity, richness of variation, size of organization, and an unforgiving requirement to support over a dozen simultaneous development streams all geared towards each new model year. To meet this challenge, GM turned to an advanced set of explicitly defined product line engineering solutions, which have been referred to as Second Generation PLE (2GPLE). This includes reliance on features as the lingua franca to express product differences in all phases of the lifecycle, deeply nested hierarchical product lines, industrial strength automation to provide modeling consistency throughout, and more. This paper explains how 2GPLE is being applied at General Motors, and the technical and organizational lessons learned so far.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {259–268},
numpages = {10},
keywords = {bill-of-features, feature modeling, feature profiles, hierarchical product lines, product baselines, product configurator, product line engineering, product portfolio, software product lines, variation points},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {Software product lines, product-line analysis}
}

@inproceedings{10.1145/3336294.3336316,
author = {Tolvanen, Juha-Pekka and Kelly, Steven},
title = {How Domain-Specific Modeling Languages Address Variability in Product Line Development: Investigation of 23 Cases},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336316},
doi = {10.1145/3336294.3336316},
abstract = {Domain-Specific Modeling raises the level of abstraction beyond programming by specifying the solution directly with domain concepts. Within product lines domain-specific approaches are applied to specify variability and then generate final products together with commonality. Such automated product derivation is possible because both the modeling language and generator are made for a particular product line --- often inside a single company. In this paper we examine which kinds of reuse and product line approaches are applied in industry with domain-specific modeling. Our work is based on empirical analysis of 23 cases and the languages and models created there. The analysis reveals a wide variety and some commonalities in the size of languages and in the ways they apply reuse and product line approaches.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {155–163},
numpages = {9},
keywords = {code generation, domain-specific language, domain-specific modeling, product derivation, product line variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.5555/2814058.2814112,
author = {Lobato, Luanna Lopes and Bittar, Thiago Jabur},
title = {A Risk Management Approach for Software Product Line Engineering},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {TSoftware Product Line (SPL) Engineering is a software development paradigm that fosters systematic reuse. It is focused on improving software practices, leading companies to experience benefits, such as reduced time-to-market and effort, and higher quality for the products delivered to customers. However, establishing a SPL is neither a simple nor a cheap task, and may affect several aspects of a software company. Besides, it involves a range of risks that may hinder project success. These have to be managed accordingly, so as to minimize the likelihood of project failure. Despite the importance of Risk Management (RM) for SPL Engineering, little has been published in terms of suitable and structured practices to cope with that. This present paper reports an approach for RM in SPL Engineering, named RiPLERM (Rise Product Line Engineering and Risk Management). The approach presents activities to structure RM in SPL projects, The design of the RiPLE-RM approach elaborated on results from empirical investigations, and was proposed to facilitate the management and provide significant insights that can be used to avoid and solve risks.},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
pages = {331–338},
numpages = {8},
keywords = {Project management, Risk Management, Software Process, Software Product Line Engineering},
location = {Goiania, Goias, Brazil},
series = {SBSI '15}
}

@inproceedings{10.1145/2811681.2811703,
author = {Tan, Lei and Lin, Yuqing},
title = {An Aspect-Oriented Feature Modelling Framework for Software Product Line Engineering},
year = {2015},
isbn = {9781450337960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811681.2811703},
doi = {10.1145/2811681.2811703},
abstract = {Software Product Line Engineering (SPLE) is a software development paradigm that focusing on systematic software assets reuse. SPLE treats software products in the same application domains as a product family and developing various of assets could be reused in the product family. Feature modelling is a critical activity of SPLE, which developing the requirement model for product families and providing guidance for individual product implementation. In this paper, we discuss several drawbacks of current feature modelling and propose a solution which adopting aspect-oriented development ideas and approaches. The proposed framework is intended to better manage complex feature relationships, and enhance quality-aware feature modelling. We include a case study of a real-life experience to demonstrate the proposed approach.},
booktitle = {Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference},
pages = {111–115},
numpages = {5},
keywords = {aspectoriented, feature modelling, software product line engineering},
location = {Adelaide, SA, Australia},
series = {ASWEC ' 15 Vol. II}
}

@inproceedings{10.1145/3233027.3233038,
author = {Martinez, Jabier and Tërnava, Xhevahire and Ziadi, Tewfik},
title = {Software product line extraction from variability-rich systems: the robocode case study},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233038},
doi = {10.1145/3233027.3233038},
abstract = {The engineering of a Software Product Line (SPL), either by creating it from scratch or through the re-engineering of existing variants, it uses to be a project that spans several years with a high investment. It is often hard to analyse and quantify this investment, especially in the context of extractive SPL adoption when the related software variants are independently created by different developers following different system architectures and implementation conventions. This paper reports an experience on the creation of an SPL by re-engineering system variants implemented around an educational game called Robocode. The objective of this game is to program a bot (a battle tank) that battles against the bots of other developers. The world-wide Robocode community creates and maintains a large base of knowledge and implementations that are mainly organized in terms of features, although not presented as an SPL. Therefore, a group of master students analysed this variability-rich domain and extracted a Robocode SPL. We present the results of such extraction augmented with an analysis and a quantification regarding the spent time and effort. We believe that the results and the a-posteriori analysis can provide insights on global challenges on SPL adoption. We also provide all the elements to SPL educators to reproduce the teaching activity, and we make available this SPL to be used for any research purpose.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {132–142},
numpages = {11},
keywords = {education, extractive software product line adoption, reverse-engineering, robocode, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233048,
author = {Hayashi, Kengo and Aoyama, Mikio},
title = {A multiple product line development method based on variability structure analysis},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233048},
doi = {10.1145/3233027.3233048},
abstract = {This article proposes a multiple product line development method based on variability structure analysis. In product line development, the problem area is divided into the domain engineering and application engineering for delivering diverse products. Now, the development of automotive software requires to meet both agility and extreme diversity, which is a big challenge. We developed a structural analysis method of variability for multiple product lines using an extended model of OVM (Orthogonal Variability Model). Together with the variability analysis method, we propose an agile application development method to refine development items according to variability dependency based on the analysis, and develop them incrementally. We applied the proposed method to the development of the multiple product lines of automotive software systems, and demonstrated to reduce the volatility of the test efforts and usage of the test environment, and higher velocity and better manageability of the value stream.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {160–169},
numpages = {10},
keywords = {agile development, automotive software, multiple product lines, software product line, variability analysis},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233032,
author = {Kröher, Christian and Gerling, Lea and Schmid, Klaus},
title = {Identifying the intensity of variability changes in software product line evolution},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233032},
doi = {10.1145/3233027.3233032},
abstract = {The evolution of a Software Product Line (SPL) typically affects a variety of artifact types. The intensity (the frequency and the amount) in which developers change variability information in these different types of artifacts is currently unknown. In this paper, we present a fine-grained approach for the variability-centric extraction and analysis of changes to code, build, and variability model artifacts introduced by commits. This approach complements existing work that is typically based on a feature-perspective and, thus, abstracts from this level of detail. Further, it provides a detailed understanding of the intensity of changes affecting variability information in these types of artifacts. We apply our approach to the Linux kernel revealing that changes to variability information occur infrequently and only affect small parts of the analyzed artifacts. Further, we outline how these results may improve certain analysis and verification tasks during SPL evolution.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {54–64},
numpages = {11},
keywords = {evolution analysis, intensity, software product line evolution, variability changes},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Clarisó, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {NLTK, feature model extraction, natural language processing, requirements engineering, software product line},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.scico.2012.06.006,
author = {Ubayashi, Naoyasu and Nakajima, Shin and Hirayama, Masayuki},
title = {Context-dependent product line engineering with lightweight formal approaches},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.06.006},
doi = {10.1016/j.scico.2012.06.006},
abstract = {This paper proposes a new style of product line engineering methods. It focuses on constructing embedded systems that take into account the contexts such as the external physical environments. In current product line development projects, Feature Analysis is mainly conducted from the viewpoint of system configurations: how hardware and software components are configured to constitute a system. In most cases, contexts are not considered explicitly. As a result, unexpected and unfavorable behavior might emerge in a system if a developer does not recognize any possible conflicting combinations between the system and contexts. To deal with this problem, this paper provides the notion of a context-dependent product line, which is composed of the system and context lines. The former is obtained by analyzing a family of systems. The latter is obtained by analyzing features of contexts associated to the systems. The system and context lines contain reusable core assets. The configuration of selected system components and contexts can be formally checked at the specification level. In this paper, we show a development process that includes the creation of both product line assets as well as context assets.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {2331–2346},
numpages = {16},
keywords = {Context analysis, Formal methods, Product line engineering}
}

@article{10.1007/s11219-020-09522-1,
author = {Bhushan, Megha and Negi, Arun and Samant, Piyush and Goel, Shivani and Kumar, Ajay},
title = {A classification and systematic review of product line feature model defects},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09522-1},
doi = {10.1007/s11219-020-09522-1},
abstract = {Product line (PL)-based development is a thriving research area to develop software-intensive systems. Feature models (FMs) facilitate derivation of valid products from a PL by managing commonalities and variabilities among software products. However, the researchers in academia as well as in the industries experience difficulties in quality assessment of FMs. The increasing complexity and size of FMs may lead to defects, which outweigh the benefits of PL. This paper provides a systematic literature review and key research issues related to the FM defects in PL. We derive a typology of FM defects according to their level of importance. The information on defects’ identification and explanations are provided with formalization. Further, corrective explanations are presented which incorporates various techniques used to fix defects with their implementation. This information would help software engineering community by enabling developers or modelers to find the types of defects and their causes and to choose an appropriate technique to fix defects in order to produce defect-free products from FMs, thereby enhancing the overall quality of PL-based development.},
journal = {Software Quality Journal},
month = dec,
pages = {1507–1550},
numpages = {44},
keywords = {Feature model, Software product line, Defect, Product line model, Quality}
}

@inproceedings{10.1007/978-3-030-29983-5_9,
author = {Wägemann, Tobias and Tavakoli Kolagari, Ramin and Schmid, Klaus},
title = {ADOOPLA - Combining Product-Line- and Product-Level Criteria in Multi-objective Optimization of Product Line Architectures},
year = {2019},
isbn = {978-3-030-29982-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29983-5_9},
doi = {10.1007/978-3-030-29983-5_9},
abstract = {Product lines of software-intensive systems have a great diversity of features and products, which leads to vast design spaces that are difficult to explore. In addition, finding optimal product line system architectures usually requires a consideration of several quality trade-offs at once, involving both product-level as well as product-line-wide criteria. This challenge cannot be solved manually for all but the smallest problems, and can therefore benefit from automated support. In this paper we propose ADOOPLA, a tool-supported approach for the optimization of product line system architectures. In contrast to existing approaches where product-level approaches only support product-level criteria and product-line oriented approaches only support product-line-wide criteria, our approach integrates criteria from both levels in the optimization of product line architectures. Further, the approach can handle multiple objectives at once, supporting the architect in exploring the multi-dimensional Pareto-front of a given problem. We describe the theoretical principles of the ADOOPLA approach and demonstrate its application to a simplified case study from the automotive domain.},
booktitle = {Software Architecture: 13th European Conference, ECSA 2019, Paris, France, September 9–13, 2019, Proceedings},
pages = {126–142},
numpages = {17},
keywords = {Product line architectures, Design space exploration, Architecture optimization, Multiobjective, Variability modeling, Automotive},
location = {Paris, France}
}

@inproceedings{10.1145/3483899.3483909,
author = {Furtado, Viviane and OliveiraJr, Edson and Kalinowski, Marcos},
title = {Guidelines for Promoting Software Product Line Experiments},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483909},
doi = {10.1145/3483899.3483909},
abstract = {The importance of experimentation for Software Engineering research has been notably established in the last years. The software engineering community has discussed how to proper report and evaluate experiments using different approaches, such as quality criteria, scales, and checklists. Nevertheless, there are no guidelines to support researchers and practitioners active in straightforward software engineering research areas, as in Software Product Lines (SPL), at conducting experiments. We hypothesize that experimentation guidelines may aid such a specific area by providing advice and actual excerpts reflecting good practices of SPL experimentation, thus experimentally evolving this area. Therefore, the goal of this paper is to provide guidelines for properly reporting and promoting SPL experiments. We defined such guidelines based on well-known software engineering experiment reports, quality evaluation checklists, and data extracted from 211 SPL experiments identified in a systematic mapping study. We evaluated the guidelines with a qualitative study with SPL and experimentation experts applying open and axial coding procedures. The evaluation enabled us to improve the guidelines. The resulting guidelines contain specific advice to researchers active in SPL and provide examples taken from published SPL experiments. The experts’ positive points indicate that the proposed guidelines can aid SPL researchers and practitioners. Sharing the resulting guidelines could support conducting SPL experiments and allow further area evolution based on prospective experiment replications and reproductions from well-designed and reported experiments.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {31–40},
numpages = {10},
keywords = {Experiment Reporting and Sharing, Guidelines, Qualitative Study, SPL Experiments},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1007/978-3-030-61362-4_5,
author = {Damiani, Ferruccio and Lienhardt, Michael and Paolini, Luca},
title = {On Slicing Software Product Line Signatures},
year = {2020},
isbn = {978-3-030-61361-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61362-4_5},
doi = {10.1007/978-3-030-61362-4_5},
abstract = {A Software Product Line (SPL) is a family of similar programs (called variants) generated from a common artifact base. Variability in an SPL can be documented in terms of abstract description of functionalities (called features): a feature model (FM) identifies each variant by a set of features (called a product). Delta-orientation is a flexible approach to implement SPLs. An SPL Signature (SPLS) is a variability-aware Application Programming Interface (API), i.e., an SPL where each variant is the API of a program. In this paper we introduce and formalize, by abstracting from SPL implementation approaches, the notion of slice of an SPLS K for a set of features F (i.e., an SPLS obtained from by K by hiding the features that are not in F). Moreover, we formulate the challenge of defining an efficient algorithm that, given a delta-oriented SPLS K and a set of features F, sreturns a delta-oriented SPLS that is an slice of K for F. Thus paving the way for further research on devising such an algorithm. The proposed notions are formalized for SPLs of programs written in an imperative version of Featherweight Java.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Verification Principles: 9th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2020, Rhodes, Greece, October 20–30, 2020, Proceedings, Part I},
pages = {81–102},
numpages = {22},
location = {Rhodes, Greece}
}

@article{10.1287/opre.2018.1825,
author = {Bertsimas, Dimitris and Mišić, Velibor V.},
title = {Exact First-Choice Product Line Optimization},
year = {2019},
issue_date = {May-June 2019},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {67},
number = {3},
issn = {0030-364X},
url = {https://doi.org/10.1287/opre.2018.1825},
doi = {10.1287/opre.2018.1825},
abstract = {Which products should a firm offer based on its customers’ preferences? This is the question posed in the problem of product line design, a well-studied and notoriously difficult problem that is central in marketing science. In “Exact First-Choice Product Line Optimization” by Dimitris Bertsimas and Velibor V. Mišić, the authors propose a new approach for solving this problem when segments of customers choose products according to a ranking. They propose a new mixed-integer optimization model of the problem, which they show to be tighter than prior formulations, and a solution approach based on Benders decomposition, which exploits the surprising fact that the subproblem can be solved efficiently for both integer and fractional master solutions. A well-known product line instance based on a conjoint data set of over 3,000 products and 300 respondents, which required a week of computation time to solve in prior work, is solved by the authors’ approach in just over 10 minutes.A fundamental problem faced by firms is that of product line design: given a set of candidate products that may be offered to a collection of customers, what subset of those products should be offered to maximize the profit that is realized when customers make purchases according to their preferences? In this paper, we consider the product line design problem when customers choose according to a first-choice rule and present a new mixed-integer optimization formulation of the problem. We theoretically analyze the strength of our formulation and show that it is stronger than alternative formulations that have been proposed in the literature, thus contributing to a unified understanding of the different formulations for this problem. We also present a novel solution approach for solving our formulation at scale, based on Benders decomposition, which exploits the surprising fact that Benders cuts for both the relaxation and the integer problem can be generated in a computationally efficient manner. We demonstrate the value of our formulation and Benders decomposition approach through two sets of experiments. In the first, we use synthetic instances to show that our formulation is computationally tractable and can be solved an order of magnitude faster for small- to medium-scale instances than the alternate, previously proposed formulations. In the second, we consider a previously studied product line design instance based on a real conjoint data set, involving over 3,000 candidate products and over 300 respondents. We show that this problem, which required a week of computation time to solve in prior work, is solved by our approach to full optimality in approximately 10 minutes.The e-companion is available at .},
journal = {Oper. Res.},
month = may,
pages = {651–670},
numpages = {20},
keywords = {product line design, first-choice models, mixed-integer optimization, Benders decomposition}
}

@article{10.1016/j.jss.2019.06.002,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Clémentine},
title = {Towards complex product line variability modelling: Mining relationships from non-boolean descriptions},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.002},
doi = {10.1016/j.jss.2019.06.002},
journal = {J. Syst. Softw.},
month = oct,
pages = {341–360},
numpages = {20},
keywords = {Complex software product line, Reverse engineering, Variability modelling, Extended feature models, Formal concept analysis, Pattern structures}
}

@inproceedings{10.1145/3382026.3425769,
author = {Nieke, Michael and Sampaio, Gabriela and Thüm, Thomas and Seidl, Christoph and Teixeira, Leopoldo and Schaefer, Ina},
title = {GuyDance: Guiding Configuration Updates for Product-Line Evolution},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425769},
doi = {10.1145/3382026.3425769},
abstract = {A product line is an approach for systematically managing configuration options of customizable systems, usually by means of features. Products are generated by utilizing configurations consisting of selected features. Product-line evolution can lead to unintended changes to product behavior. We illustrate that updating configurations after product-line evolution requires decisions of both, domain engineers responsible for product-line evolution as well as application engineers responsible for configurations. The challenge is that domain and application engineers might not be able to talk to each other. We propose a formal foundation and a methodology that enables domain engineers to guide application engineers through configuration evolution by sharing knowledge on product-line evolution and by defining configuration update operations. As an effect, we enable knowledge transfer between those engineers without the need to talk to each other. We evaluate our method by providing formal proofs that show product behavior of configurations can be preserved for typical evolution scenarios.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {56–64},
numpages = {9},
keywords = {configuration, evolution, software product line},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Clarisó, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Feature Model Validation, Machine Learning, Natural Language Processing, Requirements Engineering, Software Product Line},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/2934466.2934471,
author = {Thüm, Thomas and Ribeiro, Márcio and Schröter, Reimar and Siegmund, Janet and Dalton, Francisco},
title = {Product-line maintenance with emergent contract interfaces},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934471},
doi = {10.1145/2934466.2934471},
abstract = {A software product line evolves whenever one of its products need to evolve. Maintenance of preprocessor-based product lines is a difficult task, as changes to the code base may unintentionally influence the behavior of uninvolved products. Hence, developers should be supported during maintenance. We present emergent contract interfaces to make product-line development more efficient and less error-prone. The key idea is that for a given maintenance point (i.e., an assignment), we calculate (a) features in the source code that may be affected and (b) assertions based on contracts defined in the code base. By means of a controlled experiment, we provide empirical evidence regarding efficiency and error-avoidance with emergent contract interfaces.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {134–143},
numpages = {10},
keywords = {design by contract, evolution, maintenance, preprocessor variability, software product lines, weakest precondition},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {configuration, machine learning, multi-objective search, product line, rule mining},
location = {Berlin, Germany},
series = {GECCO '17}
}

@article{10.1287/mnsc.2019.3506,
author = {Zou, Tianxin and Zhou, Bo and Jiang, Baojun},
title = {Product-Line Design in the Presence of Consumers’ Anticipated Regret},
year = {2020},
issue_date = {December 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {66},
number = {12},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2019.3506},
doi = {10.1287/mnsc.2019.3506},
abstract = {Consumers are often uncertain about their valuations for product quality when choosing among different products and will learn their valuations only after buying and using a product. Some consumers may thus experience overpurchase or underpurchase regret, depending on whether they have purchased a higher- or lower-quality level than what they would have chosen had they known their true valuations. When consumers anticipate their potential postpurchase regret, their purchase decisions may be affected. Our analysis shows that overpurchase regret lowers the firm’s profit, but underpurchase regret can benefit the firm if consumers’ overpurchase regret is not strong. When the firm optimally designs its product line, the quality difference between its offerings will be larger (smaller) if consumers’ anticipated regret increases (reduces) its profit. Surprisingly, although anticipated regret tends to reduce consumers’ utility, in equilibrium, the presence of anticipated regret can increase consumers’ expected surplus. We further examine when the firm should allow consumers to return their products by paying a restocking fee and how the optimal restocking fee will change with consumers’ propensities for the two types of regret. We also experimentally show that consumers’ propensities of underpurchase and overpurchase regret are different and can be influenced by the firm’s messages.This paper was accepted by Juanjuan Zhang, marketing.},
journal = {Manage. Sci.},
month = dec,
pages = {5665–5682},
numpages = {18},
keywords = {anticipated regret, product line design, behavioral economics, price discrimination, product return}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {asset health, extensibility, industrial analytics, interoperability, knowledge, performance, reusability, software product line},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3233028,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {A study and comparison of industrial vs. academic software product line research published at SPLC},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233028},
doi = {10.1145/3233027.3233028},
abstract = {The study presented in this paper aims to provide evidence for the hypothesis that software product line research has been changing and that the works in industry and academia have diverged over time. We analysed a subset (140) of all (593) papers published at the Software Product Line Conference (SPLC) until 2017. The subset was randomly selected to cover all years as well as types of papers. We assessed the research type of the papers (academic or industry), the kind of evaluation (application example, empirical, etc.), and the application domain. Also, we assessed which product line life-cycle phases, development practices, and topics the papers address. We present an analysis of the topics covered by academic vs. industry research and discuss the evolution of these topics and their relation over the years. We also discuss implications for researchers and practitioners. We conclude that even though several topics have received more attention than others, academic and industry research on software product lines are actually rather in line with each other.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {14–24},
numpages = {11},
keywords = {SPLC, academia, industry, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2934466.2934483,
author = {Richenhagen, Johannes and Rumpe, Bernhard and Schloßer, Axel and Schulze, Christoph and Thissen, Kevin and von Wenckstern, Michael},
title = {Test-driven semantical similarity analysis for software product line extraction},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934483},
doi = {10.1145/2934466.2934483},
abstract = {Software product line engineering rests upon the assumption that a set of products share a common base of similar functionality. The correct identification of similarities between different products can be a time-intensive task. Hence, this paper proposes an automated semantical similarity analysis supporting software product line extraction and maintenance. Under the assumption of an already identified compatible interface, the degree of semantical similarity is identified based on provided test cases. Therefore, the analysis can also be applied in a test-driven development. This is done by translating available test sequences for both components into two I/O extended finite automata and performing an abstraction of the defined behavior until a simulation relation is established. The test-based approach avoids complexity issues regarding the state space explosion problem, a common issue in model checking. The proposed approach is applied on different variants and versions of industrially used software components provided by an automotive supplier to demonstrate the method's applicability.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {174–183},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1287/opre.2016.1546,
author = {Bertsimas, Dimitris and Mišić, Velibor V.},
title = {Robust Product Line Design},
year = {2017},
issue_date = {January-February 2017},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {65},
number = {1},
issn = {0030-364X},
url = {https://doi.org/10.1287/opre.2016.1546},
doi = {10.1287/opre.2016.1546},
abstract = {The majority of approaches to product line design that have been proposed by marketing scientists assume that the underlying choice model that describes how the customer population will respond to a new product line is known precisely. In reality, however, marketers do not precisely know how the customer population will respond and can only obtain an estimate of the choice model from limited conjoint data. In this paper, we propose a new type of optimization approach for product line design under uncertainty. Our approach is based on the paradigm of robust optimization where, rather than optimizing the expected revenue with respect to a single model, one optimizes the worst-case expected revenue with respect to an uncertainty set of models. This framework allows us to account for parameter uncertainty, when we may be confident about the type of model structure but not about the values of the parameters, and structural uncertainty, when we may not even be confident about the right model structure to use to describe the customer population. Through computational experiments with a real conjoint data set, we demonstrate the benefits of our approach in addressing parameter and structural uncertainty. With regard to parameter uncertainty, we show that product lines designed without accounting for parameter uncertainty are fragile and can experience worst-case revenue losses as high as 23%, and that the robust product line can significantly outperform the nominal product line in the worst case, with relative improvements of up to 14%. With regard to structural uncertainty, we similarly show that product lines that are designed for a single model structure can be highly suboptimal under other structures (worst-case losses of up to 37%), while a product line that optimizes against the worst of a set of structurally distinct models can outperform single model product lines by as much as 55% in the worst case and can guarantee good aggregate performance over structurally distinct models.},
journal = {Oper. Res.},
month = feb,
pages = {19–37},
numpages = {19},
keywords = {product line design, robust optimization, parameter uncertainty, structural uncertainty, model uncertainty}
}

@inproceedings{10.1145/1629716.1629720,
author = {Chae, Wonseok and Blume, Matthias},
title = {Language support for feature-oriented product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629720},
doi = {10.1145/1629716.1629720},
abstract = {Product line engineering is an emerging paradigm of developing a family of products. While product line analysis and design mainly focus on reasoning about commonality and variability of family members, product line implementation gives its attention to mechanisms of managing variability. In many cases, however, product line methods do not impose any specific synthesis mechanisms on product line implementation, so implementation details are left to developers. In our previous work, we adopted feature-oriented product line engineering to build a family of compilers and managed variations using the Standard ML module system. We demonstrated the applicability of this module system to product line implementation. Although we have benefited from the product line engineering paradigm, it mostly served us as a design paradigm to change the way we think about a set of closely related compilers, not to change the way we build them. The problem was that Standard ML did not fully realize this paradigm at the code level, which caused some difficulties when we were developing a set of compilers.In this paper, we address such issues with a language-based solution. MLPolyR is our choice of an implementation language. It supports three different programming styles. First, its first-class cases facilitate composable extensions at the expression levels. Second, its module language provides extensible and parameterized modules, which make large-scale extensible programming possible. Third, its macro system simplifies specification and composition of feature related code. We will show how the combination of these language features work together to facilitate the product line engineering paradigm.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {3–10},
numpages = {8},
keywords = {feature-oriented programming, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.1145/1183236.1183266,
author = {Lee, Jaejoon and Muthig, Dirk},
title = {Feature-oriented variability management in product line engineering},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183266},
doi = {10.1145/1183236.1183266},
abstract = {Implementing feature-oriented variability modeling throughout the life cycle.},
journal = {Commun. ACM},
month = dec,
pages = {55–59},
numpages = {5}
}

@inproceedings{10.1007/978-3-642-34032-1_22,
author = {Ferrari, Alessio and Spagnolo, Giorgio Oronzo and Martelli, Giacomo and Menabeni, Simone},
title = {Product line engineering applied to CBTC systems development},
year = {2012},
isbn = {9783642340314},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34032-1_22},
doi = {10.1007/978-3-642-34032-1_22},
abstract = {Communications-based Train Control (CBTC) systems are the new frontier of automated train control and operation. Currently developed CBTC platforms are actually very complex systems including several functionalities, and every installed system, developed by a different company, varies in extent, scope, number, and even names of the implemented functionalities. International standards have emerged, but they remain at a quite abstract level, mostly setting terminology.This paper reports intermediate results in an effort aimed at defining a global model of CBTC, by mixing semi-formal modelling and product line engineering. The effort has been based on an in-depth market analysis, not limiting to particular aspects but considering as far as possible the whole picture. The adopted methodology is discussed and a preliminary model is presented.},
booktitle = {Proceedings of the 5th International Conference on Leveraging Applications of Formal Methods, Verification and Validation: Applications and Case Studies - Volume Part II},
pages = {216–230},
numpages = {15},
location = {Heraklion, Crete, Greece},
series = {ISoLA'12}
}

@inproceedings{10.1145/2791060.2791074,
author = {Reuling, Dennis and Bürdek, Johannes and Rotärmel, Serge and Lochau, Malte and Kelter, Udo},
title = {Fault-based product-line testing: effective sample generation based on feature-diagram mutation},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791074},
doi = {10.1145/2791060.2791074},
abstract = {Testing every member of a product line individually is often impracticable due to large number of possible product configurations. Thus, feature models are frequently used to generate samples, i.e., subsets of product configurations under test. Besides the extensively studied combinatorial interaction testing (CIT) approach for coverage-driven sample generation, only few approaches exist so far adopting mutation testing to emulate faults in feature models to be detected by a sample. In this paper, we present a mutation-based sampling framework for fault-based product-line testing. We define a comprehensive catalog of atomic mutation operators on the graphical representation of feature models. This way, we are able (1) to also define complex mutation operators emulating more subtle faults, and (2) to classify operators semantically, e.g., to avoid redundant and equivalent mutants. We further introduce similarity-based mutant selection and higher order mutation strategies to reduce testing efforts. Our implementation is based on the graph transformation engine Henshin and is evaluated concerning effectiveness/efficiency trade-offs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {131–140},
numpages = {10},
keywords = {combinatorial interaction testing, mutation testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2362536.2362545,
author = {Lee, Jihyun and Kang, Sungwon and Lee, Danhyung},
title = {A survey on software product line testing},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362545},
doi = {10.1145/2362536.2362545},
abstract = {Software product line (SPL) testing consists of two separate but closely related test engineering activities: domain testing and application testing. Various software product line testing approaches have been developed over the last decade, and surveys have been conducted on them. However, thus far none of them deeply addressed the questions of what researches have been conducted in order to overcome the challenges posed by the two separate testing activities and their relationships. Thus, this paper surveys the current software product line testing approaches by defining a reference SPL testing processes and identifying, based on them, key research perspectives that are important in SPL testing. Through this survey, we identify the researches that addressed the challenges and also derive open research opportunities from each perspective.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {31–40},
numpages = {10},
keywords = {software product line engineering, software product line testing, software testing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.infsof.2013.05.006,
author = {Mohabbati, Bardia and Asadi, Mohsen and Gašević, Dragan and Hatala, Marek and Müller, Hausi A.},
title = {Combining service-orientation and software product line engineering: A systematic mapping study},
year = {2013},
issue_date = {November, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.006},
doi = {10.1016/j.infsof.2013.05.006},
abstract = {Context: Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective: This study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE. Method: We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000-2011 and classified them with respect to research focus, types of research and contribution. Result: The mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches. In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion: Our study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1845–1859},
numpages = {15},
keywords = {Service-oriented architecture, Software product lines, Systematic mapping}
}

@article{10.1007/s10796-010-9230-8,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {A business maturity model of software product line engineering},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {4},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-010-9230-8},
doi = {10.1007/s10796-010-9230-8},
abstract = {In the recent past, software product line engineering has become one of the most promising practices in software industry with the potential to substantially increase the software development productivity. Software product line engineering approach spans the dimensions of business, architecture, software engineering process and organization. The increasing popularity of software product line engineering in the software industry necessitates a process maturity evaluation methodology. Accordingly, this paper presents a business maturity model of software product line, which is a methodology to evaluate the current maturity of the business dimension of a software product line in an organization. This model examines the coordination between product line engineering and the business aspects of software product line. It evaluates the maturity of the business dimension of software product line as a function of how a set of business practices are aligned with product line engineering in an organization. Using the model presented in this paper, we conducted two case studies and reported the assessment results. This research contributes towards establishing a comprehensive and unified strategy for a process maturity evaluation of software product lines.},
journal = {Information Systems Frontiers},
month = sep,
pages = {543–560},
numpages = {18},
keywords = {Business process, Maturity evaluation, Organizational management, Software process assessment, Software process model, Software product line}
}

@article{10.1016/j.infsof.2010.03.007,
author = {Heider, Wolfgang and Froschauer, Roman and Grünbacher, Paul and Rabiser, Rick and Dhungana, Deepak},
title = {Simulating evolution in model-based product line engineering},
year = {2010},
issue_date = {July, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.03.007},
doi = {10.1016/j.infsof.2010.03.007},
abstract = {Context: Numerous approaches are available for modeling product lines and their variability. However, the long-term impacts of model-based development on maintenance effort and model complexity can hardly be investigated due to a lack of empirical data. Conducting empirical research in product line engineering is difficult as companies are typically reluctant to provide access to data from their product lines. Also, many benefits of product lines can be measured only in longitudinal studies, which are difficult to perform in most environments. Objective: In this paper, we thus aim to explore the benefit of simulation to investigate the evolution of model-based product lines. Method: We present a simulation approach for exploring the effects of product line evolution on model complexity and maintenance effort. Our simulation considers characteristics of product lines (e.g., size, dependencies in models) and we experiment with different evolution profiles (e.g., technical refactoring vs. placement of new products). Results: We apply the approach in a simulation experiment that uses data from real-world product lines from the domain of industrial automation systems to demonstrate its feasibility. Conclusion: Our results demonstrate that simulation contributes to understanding the effects of maintenance and evolution in model-based product lines.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {758–769},
numpages = {12},
keywords = {Industrial automation systems, Maintenance and evolution, Model-based development, Product line engineering, Simulation}
}

@article{10.1016/j.cl.2018.01.003,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {Personalized recommender systems for product-line configuration processes},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.003},
doi = {10.1016/j.cl.2018.01.003},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {451–471},
numpages = {21},
keywords = {Product lines, Feature model, Product-line configuration, Recommender systems, Personalized recommendations}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2647908.2655967,
author = {Assunção, Wesley Klewerton Guez and Vergilio, Silvia Regina},
title = {Feature location for software product line migration: a mapping study},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655967},
doi = {10.1145/2647908.2655967},
abstract = {Developing software from scratch is a high cost and error-prone activity. A possible solution to reduce time-to-market and produce high quality software is the reuse of existing software. But when the number of features in the system grows, the maintenance becomes more complex. In such cases, to adopt a systematic approach, such as Software Product Line Engineering, is necessary. Existing systems are generally migrated to a product line, allowing systematic reuse of artefacts and easing maintenance. To this end, some approaches have been proposed in the literature in the last years. A mapping of works on this subject and the identification of some research gaps can lead to an improvement of such approaches. This paper describes the main outcomes of a systematic mapping study on the evolution and migration of systems to SPL. The main works found are presented and classified according to adopted strategy, artefacts used, and evaluation conducted. Analysis of the evolution along the past years are also presented. At the end, we summarize some trends and open issues to serve as reference to new researches.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {52–59},
numpages = {8},
keywords = {evolution, reengineering, reuse, software product line},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1007/s11334-011-0159-y,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {An architecture process maturity model of software product line engineering},
year = {2011},
issue_date = {September 2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {7},
number = {3},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-011-0159-y},
doi = {10.1007/s11334-011-0159-y},
abstract = {Software architecture has been a key research area in the software engineering community due to its significant role in creating high-quality software. The trend of developing product lines rather than single products has made the software product line a viable option in the industry. Software product line architecture (SPLA) is regarded as one of the crucial components in the product lines, since all of the resulting products share this common architecture. The increased popularity of software product lines demands a process maturity evaluation methodology. Consequently, this paper presents an architecture process maturity model for software product line engineering to evaluate the current maturity of the product line architecture development process in an organization. Assessment questionnaires and a rating methodology comprise the framework of this model. The objective of the questionnaires is to collect information about the SPLA development process. Thus, in general this work contributes towards the establishment of a comprehensive and unified strategy for the process maturity evaluation of software product line engineering. Furthermore, we conducted two case studies and reported the assessment results, which show the maturity of the architecture development process in two organizations.},
journal = {Innov. Syst. Softw. Eng.},
month = sep,
pages = {191–207},
numpages = {17},
keywords = {Application engineering, Domain engineering, Process assessment, Software architecture, Software product line}
}

@article{10.1016/j.infsof.2012.06.014,
author = {Andersson, Henric and Herzog, Erik and ÖLvander, Johan},
title = {Experience from model and software reuse in aircraft simulator product line engineering},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.06.014},
doi = {10.1016/j.infsof.2012.06.014},
abstract = {Context: ''Reuse'' and ''Model Based Development'' are two prominent trends for improving industrial development efficiency. Product lines are used to reduce the time to create product variants by reusing components. The model based approach provides the opportunity to enhance knowledge capture for a system in the early stages in order to be reused throughout its lifecycle. This paper describes how these two trends are combined to support development and support of a simulator product line for the SAAB 39 Gripen fighter aircraft. Objective: The work aims at improving the support (in terms of efficiency and quality) when creating simulation model configurations. Software based simulators are flexible so variants and versions of included models may easily be exchanged. The objective is to increase the reuse when combining models for usage in a range of development and training simulators. Method: The research has been conducted with an interactive approach using prototyping and demonstrations, and the evaluation is based on an iterative and a retrospective method. Results: A product line of simulator models for the SAAB 39 Gripen aircraft has been analyzed and defined in a Product Variant Master. A configurator system has been implemented for creation, integration, and customization of stringent simulator model configurations. The system is currently under incorporation in the standard development process at SAAB Aeronautics. Conclusion: The explicit and visual description of products and their variability through a configurator system enables better insights and a common understanding so that collaboration on possible product configurations improves and the potential of software reuse increases. The combination of application fields imposes constraints on how traditional tools and methods may be utilized. Solutions for Design Automation and Knowledge Based Engineering are available, but their application has limitations for Software Product Line engineering and the reuse of simulation models.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {595–606},
numpages = {12},
keywords = {Configurator, Knowledge Based Engineering, Model Based Development, PDM, SPL, Software Product Line}
}

@article{10.1287/mksc.2019.1160,
author = {Xu, Zibin and Dukes, Anthony},
title = {Product Line Design Under Preference Uncertainty Using Aggregate Consumer Data},
year = {2019},
issue_date = {July-August 2019},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {38},
number = {4},
issn = {1526-548X},
url = {https://doi.org/10.1287/mksc.2019.1160},
doi = {10.1287/mksc.2019.1160},
abstract = {This research studies the product line design problem when consumers are subject to perceptual errors in assessing their intrinsic preferences.This research studies the product line design problem when consumers are subject to perceptual errors in assessing their intrinsic preferences. If perceptual errors are driven by common variables, then a firm may use aggregate consumer data (e.g., conjoint studies or anonymous usage data) to deduce the errors and infer the consumer preferences. In this way, we develop microfoundations necessary to show when and how the firm can understand consumer preferences better than consumers themselves, a situation we call superior knowledge. But is superior knowledge ever unprofitable? How should the firm with superior knowledge design its product line? Do consumers receive more-relevant products or simply have more surplus extracted? Can data collection help consumers make better choices? Our results suggest that consumers’ rational suspicions may prevent the firm from exploiting its superior knowledge. In addition, the burden of signaling may force the firm to offer efficient quality for its products. Therefore, allowing the firm to collect aggregate consumer data may be strictly Pareto improving.},
journal = {Marketing Science},
month = jul,
pages = {669–689},
numpages = {21},
keywords = {consumer data collection, product line design, superior knowledge, uninformed preference, perceptual error, signaling model}
}

@article{10.1016/j.jss.2018.02.021,
author = {Hajri, Ines and Goknil, Arda and Briand, Lionel C. and Stephany, Thierry},
title = {Change impact analysis for evolving configuration decisions in product line use case models},
year = {2018},
issue_date = {May 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {139},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.02.021},
doi = {10.1016/j.jss.2018.02.021},
journal = {J. Syst. Softw.},
month = may,
pages = {211–237},
numpages = {27},
keywords = {Change impact analysis, Product line engineering, Use case driven development, Use case configurator, Evolving decisions, Incremental reconfiguration}
}

@inproceedings{10.5555/2666064.2666079,
author = {Saratxaga, C. L. and Alonso-Montes, C. and Haugen, O. and Ekelin, C. and Mitschke, A.},
title = {Product line tool-chain: variability in critical systems},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {Competitiveness has thrown industries towards adding more features to existent products increasing their inherent complexity. One of the main challenges is to define mechanisms and tools to control the propagation of the dependencies through the different engineering phases, keeping consistency among requirements and the final system design. SPL provide mechanisms to control the evolution and design of product families, based on an exhaustive variant analysis. However, the critical system industry does not adopt them due to the lack of tool support for the complete life-cycle. In this paper, a product line tool chain is presented based on the analysis of current SPL tools and approaches in order to fit the specific needs within industry partners in the CESAR project. The main goal is to show the benefits of a combination of SPL tools in an industrial scenario.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {57–60},
numpages = {4},
keywords = {CESAR, CVL, PLUM, SPL, critical systems, product line tool chain, variability management},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@article{10.1007/s11219-009-9088-5,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {An organizational maturity model of software product line engineering},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-009-9088-5},
doi = {10.1007/s11219-009-9088-5},
abstract = {Software product line engineering is an inter-disciplinary concept. It spans the dimensions of business, architecture, process, and the organization. Some of the potential benefits of this approach include cost reduction, improvements in product quality and a decrease in product development time. The increasing popularity of software product line engineering in the software industry necessitates a process maturity evaluation methodology. Accordingly, this paper presents an organizational maturity model of software product line engineering for evaluating the maturity of organizational dimension. The model assumes that organizational theories, behavior, and management play a critical role in the institutionalization of software product line engineering within an organization. Assessment questionnaires and a rating methodology comprise the framework of this model. The objective and design of the questionnaires are to collect information about the software product line engineering process from the dual perspectives of organizational behavior and management. Furthermore, we conducted two case studies and reported the assessment results using the organizational maturity model presented in this paper.},
journal = {Software Quality Journal},
month = jun,
pages = {195–225},
numpages = {31},
keywords = {Product Line, Software, Software engineering, Software process assessment, Software process improvement, Software process maturity}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Artificial intelligence, Automated feature selection, Inconsistencies, Industrial SPL tools, Literature review, Predictive analytics, Software product line}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, Ángel Jesús and Galindo, José A. and Ramos-Gutiérrez, Belén and Gómez-López, María Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {clustering, configuration workflow, process discovery, process mining, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2362536.2362573,
author = {Bartholdt, Jörg and Becker, Detlef},
title = {Scope extension of an existing product line},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362573},
doi = {10.1145/2362536.2362573},
abstract = {At the beginning, creating a product line needs a well defined and narrow scope to meet short time to market demands. When established, there is a tendency to broaden the scope and to cover more domains and products.We have undergone a scope extension of our medical diagnostic platform that was implemented while the platform and (existing) products were evolving. In this paper, we list best practices for the migration process and how to come to a sustainable solution without cannibalizing the existing platform and products.In particular, we describe our way of identification beneficial sub-domains using C/V analysis and give an example scenario with alignments in order to increase commonality. We explain the maturity considerations for deciding on reuse of existing implementations and a carve-out strategy to split existing assets into common modules and product-line specific extensions. Furthermore, we describe our best practices for making the scope extension sustainable in a long term, using various types of governance means. We briefly complement these experiences with further insights gained during execution of this endeavor.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {275–282},
numpages = {8},
keywords = {C/V analysis, governance, hierarchical product-line, scope extension},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791099,
author = {Filho, João Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
title = {Assessing product line derivation operators applied to Java source code: an empirical study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791099},
doi = {10.1145/2791060.2791099},
abstract = {Product Derivation is a key activity in Software Product Line Engineering. During this process, derivation operators modify or create core assets (e.g., model elements, source code instructions, components) by adding, removing or substituting them according to a given configuration. The result is a derived product that generally needs to conform to a programming or modeling language. Some operators lead to invalid products when applied to certain assets, some others do not; knowing this in advance can help to better use them, however this is challenging, specially if we consider assets expressed in extensive and complex languages such as Java. In this paper, we empirically answer the following question: which product line operators, applied to which program elements, can synthesize variants of programs that are incorrect, correct or perhaps even conforming to test suites? We implement source code transformations, based on the derivation operators of the Common Variability Language. We automatically synthesize more than 370,000 program variants from a set of 8 real large Java projects (up to 85,000 lines of code), obtaining an extensive panorama of the sanity of the operations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {36–45},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1007/s10270-015-0462-4,
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud and Liaaen, Marius},
title = {Automated product line test case selection: industrial case study and controlled experiment},
year = {2017},
issue_date = {May       2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0462-4},
doi = {10.1007/s10270-015-0462-4},
abstract = {Automated test case selection for a new product in a product line is challenging due to several reasons. First, the variability within the product line needs to be captured in a systematic way; second, the reusable test cases from the repository are required to be identified for testing a new product. The objective of such automated process is to reduce the overall effort for selection (e.g., selection time), while achieving an acceptable level of the coverage of testing functionalities. In this paper, we propose a systematic and automated methodology using a feature model for testing (FM_T) to capture commonalities and variabilities of a product line and a component family model for testing (CFM_T) to capture the overall structure of test cases in the repository. With our methodology, a test engineer does not need to manually go through the repository to select a relevant set of test cases for a new product. Instead, a test engineer only needs to select a set of relevant features using FM_T at a higher level of abstraction for a product and a set of relevant test cases will be selected automatically. We evaluated our methodology via three different ways: (1) We applied our methodology to a product line of video conferencing systems called Saturn developed by Cisco, and the results show that our methodology can reduce the selection effort significantly; (2) we conducted a questionnaire-based study to solicit the views of test engineers who were involved in developing FM_T and CFM_T. The results show that test engineers are positive about adapting our methodology and models (FM_T and CFM_T) in their current practice; (3) we conducted a controlled experiment with 20 graduate students to assess the performance (i.e., cost, effectiveness and efficiency) of our automated methodology as compared to the manual approach. The results showed that our methodology is cost-effective as compared to the manual approach, and at the same time, its efficiency is not affected by the increased complexity of products.},
journal = {Softw. Syst. Model.},
month = may,
pages = {417–441},
numpages = {25},
keywords = {Component family model, Feature model, Product line, Test case selection}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {configurable systems, machine learning, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1287/mksc.2016.1004,
author = {Shugan, Steven M. and Moon, Jihwan and Shi, Qiaoni and Kumar, Nanda S.},
title = {Product Line Bundling: Why Airlines Bundle High-End While Hotels Bundle Low-End},
year = {2017},
issue_date = {January 2017},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {36},
number = {1},
issn = {1526-548X},
url = {https://doi.org/10.1287/mksc.2016.1004},
doi = {10.1287/mksc.2016.1004},
abstract = {Product lines are ubiquitous. For example, Marriott International manages high-end ultra-luxury hotels e.g., Ritz-Carlton and low-end economy hotels e.g., Fairfield Inn. Firms often bundle core products with ancillary services or add-ons. Interestingly, empirical observations reveal that industries with ostensibly similar characteristics e.g., customer types, costs, competition, distribution channels, etc. employ different bundling strategies. For example, airlines bundle high-end first class with ancillary services e.g., breakfast, entertainment while hotel chains bundle ancillary services e.g., breakfast, entertainment at the low-end. We observe, unlike hotel lines that are highly differentiated at different geographic locations, airlines suffer low core differentiation because all passengers first-class and economy are at the same location i.e., same plane, weather, delays, cancellations, etc.. In general, we find product lines with low core differentiation e.g., airlines, amusement parks routinely bundle high-end while product lines with highly differentiated cores e.g., hotels, restaurants routinely bundle low-end. High-end bundling makes the high-end more attractive, increasing line differentiation less intraline competition while low-end bundling decreases line differentiation. Therefore, bundling allows optimal differentiation given a differentiation constraint complex costs. Last, firms may use strategic bundling for targeting in their core products; e.g., low-end hotels bundle targeted add-ons unattractive to high-end consumers such as lower-quality breakfasts and slower Internet.Data, as supplemental material, are available at &lt;ext-link ext-link-type="uri" href="https://doi.org/10.1287/mksc.2016.1004"&gt;https://doi.org/10.1287/mksc.2016.1004&lt;/ext-link&gt;.},
journal = {Marketing Science},
month = jan,
pages = {124–139},
numpages = {16},
keywords = {analytic model, ancillary services, fees, pricing, product line, strategic bundling, targeted add-ons}
}

@inproceedings{10.1145/2362536.2362569,
author = {Patzke, Thomas and Becker, Martin and Steffens, Michaela and Sierszecki, Krzysztof and Savolainen, Juha Erik and Fogdal, Thomas},
title = {Identifying improvement potential in evolving product line infrastructures: 3 case studies},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362569},
doi = {10.1145/2362536.2362569},
abstract = {Successful software products evolve continuously to meet the changing stakeholder requirements. For software product lines, an additional challenge is that variabilities, characteristics that vary among products, change as well over time. That challenge must be carefully tackled during the evolution of the product line infrastructure. This is a significant problem for many software development organizations, as practical guidelines on how to evolve core assets, and especially source code, are missing.This paper investigates how to achieve "good enough" variability management during the evolution of variation in software design and implementation assets. As a first contribution, we present a customizable goal-based approach which helps to identify improvement potential in existing core assets to ease evolution. To find concrete ways to improve the product line infrastructure, we list the typical symptoms of variability "code smells" and show how to refine them to root causes, questions, and finally to metrics that can be extracted from large code bases.As a second main contribution, we show how this method was applied to evaluate the reuse quality of three industrial embedded systems. These systems are implemented in C or C++ and use Conditional Compilation as the main variability mechanism. We also introduce the analysis and refactoring tool set that was used in the case studies and discuss the lessons learnt.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {239–248},
numpages = {10},
keywords = {PuLSE-E, goal-based product line measurement, industrial case study, product line code evolution, variability code smells},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3289402.3289504,
author = {Sebbaq, Hanane and Retbi, Asmaa and Idrissi, Mohammed Khalidi and Bennani, Samir},
title = {Software Product Line to overcome the variability issue in E-Learning: Systematic literature review},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289504},
doi = {10.1145/3289402.3289504},
abstract = {The disparity of educational technologies, pedagogies and learning styles implies a problem of variability when modeling E-learning systems. Furthermore, the current learning context, which has become very open and heterogeneous, raises the problem of automating the modeling, development and maintenance of personalized E-learning systems based on various pedagogies. For its part, the "Software Product Line" is a paradigm that aims to produce product families based on the principles of reuse, configuration and derivation. The main purpose of this literature review is to explore the different potential applications of "SPL" in the E-learning domain to figure out the problem of variability. We will adopt a protocol for a systematic review of literature, after which we will draw up an analysis report.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {4},
numpages = {8},
keywords = {E-learning, Software Product line, Variability, heterogeneity, scale, systematic literature review, variety},
location = {Rabat, Morocco},
series = {SITA'18}
}

@article{10.1016/j.jss.2019.05.001,
author = {Kicsi, András and Csuvik, Viktor and Vidács, László and Horváth, Ferenc and Beszédes, Árpád and Gyimóthy, Tibor and Kocsis, Ferenc},
title = {Feature analysis using information retrieval, community detection and structural analysis methods in product line adoption},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.05.001},
doi = {10.1016/j.jss.2019.05.001},
journal = {J. Syst. Softw.},
month = sep,
pages = {70–90},
numpages = {21},
keywords = {Software product line, Feature extraction, Information retrieval, Community detection}
}

@inproceedings{10.1145/2364412.2364451,
author = {Machado, Ivan do Carmo},
title = {Towards a reasoning framework for software product line testing},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364451},
doi = {10.1145/2364412.2364451},
abstract = {Testing can still be considered a bottleneck for software product line engineering. The variability implemented in the source artifacts increases its complexity. Due to its key role for product line quality, testing requires cost-effective practices, such as techniques for test selection should be produced to enable companies to experience the substantial production cost savings. In this paper, we present the outline of a Ph.D. research aimed at developing a reasoning framework to improve SPL testing practices. Based on multiple sources of evidence, the framework intends to provide testers with an automated reasoner for determining which techniques may be suitable for a given variability implementation mechanism, and how these should be employed in order to makes testing in a SPL a more effective and efficient practice. We plan to perform empirical evaluations in order to assess the proposal effectiveness.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {229–232},
numpages = {4},
keywords = {fault models, software product lines, software testing, variability management},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1007/s00766-014-0201-3,
author = {Lung, Chung-Horng and Balasubramaniam, Balasangar and Selvarajah, Kamalachelva and Elankeswaran, Poopalasingham and Gopalasundaram, Umatharan},
title = {On building architecture-centric product line architecture},
year = {2015},
issue_date = {September 2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0201-3},
doi = {10.1007/s00766-014-0201-3},
abstract = {Software architects typically spend a great deal of time and effort exploring uncertainties, evaluating alternatives, and balancing the concerns of stakeholders. Selecting the best architecture to meet both the functional and non-functional requirements is a critical but difficult task, especially at the early stage of software development when there may be many uncertainties. For example, how will a technology match the operational or performance expectations in reality? This paper presents an approach to building architecture-centric product line. The main objective of the proposed approach is to support effective requirements validation and architectural prototyping for the application-level software. Architectural prototyping is practically essential to architecture design and evaluation. However, architectural prototyping practiced in the field mostly is not used to explore alternatives. Effective construction and evaluation of multiple architecture alternatives is one of the critically challenging tasks. The product line architecture advocated in this paper consists of multiple software architecture alternatives, from which the architect can select and rapidly generate a working application prototype. The paper presents a case study of developing a framework that is primarily built with robust architecture patterns in distributed and concurrent computing and includes variation mechanisms to support various applications even in different domains. The development process of the framework is an application of software product line engineering with an aim to effectively facilitate upfront requirements analysis for an application and rapid architectural prototyping to explore and evaluate architecture alternatives.},
journal = {Requir. Eng.},
month = sep,
pages = {301–321},
numpages = {21},
keywords = {Architectural prototyping, Architecture evaluation, Patterns, Requirements validation, Software performance, Software product line}
}

@inproceedings{10.1145/3350768.3351993,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Towards the support of user preferences in search-based product line architecture design: an exploratory study},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351993},
doi = {10.1145/3350768.3351993},
abstract = {Software Product Lines (SPLs) is a reuse approach in which a family of products is generalized in a common architecture that can be adapted to different clients. The Product Line Architecture (PLA) is one of the most important artifacts of a SPL. PLA design requires great human effort as it involves several factors that are usually in conflict. To ease this task, PLA design can be formulated as an optimization problem with many factors, i.e, as a multi-objective optimization problem. In this context, the MOA4PLA approach was proposed to optimize PLA design using search algorithms and metrics specific to the context. This approach supported by OPLA-Tool has already been used in several works demonstrating its applicability. However, MOA4PLA does not take into account aspects that are subjective, such as the preferences of a particular Decision Maker (DM). To do so, this paper presents a proposal to incorporate the user preferences in the optimization process performed by MOA4PLA, through an interactive process in which the DM subjectively evaluates the solutions in processing time. Thus, the solutions generated can be better suited to the DM's needs or preferences. In order to allow the user interaction, modifications were made in MOA4PLA and implemented in the OPLA-Tool. Aiming at an initial validation of the proposal, an exploratory study was carried out, composed of two experiments: a qualitative and a quantitative. These experiments were realized with the participation of a software architect. Empirical results pointed out that the proposed interactive process enables the generation of PLAs that are in accordance with the architect's preferences. Another significant contribution are the lessons learned on how to improve the interactive process.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {387–396},
numpages = {10},
keywords = {Human-computer interaction, Multi-Objective Optimization, Product Line Architecture},
location = {Salvador, Brazil},
series = {SBES '19}
}

@article{10.1007/s10270-017-0614-9,
author = {Guizzo, Giovani and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {Applying design patterns in the search-based optimization of software product line architectures},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0614-9},
doi = {10.1007/s10270-017-0614-9},
abstract = {The design of the product line architecture (PLA) is a difficult activity that can benefit from the application of design patterns and from the use of a search-based optimization approach, which is generally guided by different objectives related, for instance, to cohesion, coupling and PLA extensibility. The use of design patterns for PLAs is a recent research field, not completely explored yet. Some works apply the patterns manually and for a specific domain. Approaches to search-based PLA design do not consider the usage of these patterns. To allow such use, this paper introduces a mutation operator named “Pattern-Driven Mutation Operator” that includes methods to automatically identify suitable scopes and apply the patterns Strategy, Bridge and Mediator with the search-based approach multi-objective optimization approach for PLA. A metamodel is proposed to represent and identify suitable scopes to receive each one of the patterns, avoiding the introduction of architectural anomalies. Empirical results are also presented, showing evidences that the use of the proposed operator produces a greater diversity of solutions and improves the quality of the PLAs obtained in the search-based optimization process, regarding the values of software metrics.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1487–1512},
numpages = {26},
keywords = {Design pattern, Search-based software engineering, Software product line architecture}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {configuration management, microservice platforms, product families, technologies and concepts, variability management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/2384716.2384733,
author = {Asaithambi, Suriya Priya R. and Jarzabek, Stan},
title = {Generic adaptable test cases for software product line testing: software product line},
year = {2012},
isbn = {9781450315630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384716.2384733},
doi = {10.1145/2384716.2384733},
abstract = {This research study is about constructing "generic adaptable test cases" to counter test case libraries explosion problem. Our work focuses on effort reduction via systematic reuse of generic test assets by taking advantage of common aspects and predicted variability in test cases. We envision that the proposed approach to organizing test case libraries will be particularly useful in the context of Software Product Line Testing (SPLT). By exploring strategies for generic test cases, I hope to address problems of domain-level testing. Our work will investigate existing testing (SPLT) practices in variability management context by conducting empirical studies. We plan to synthesize principles for "generic test case" design, identify gaps between required and exiting techniques, and finally propose new approach for generic adaptive test case construction.},
booktitle = {Proceedings of the 3rd Annual Conference on Systems, Programming, and Applications: Software for Humanity},
pages = {33–36},
numpages = {4},
keywords = {software product line testing},
location = {Tucson, Arizona, USA},
series = {SPLASH '12}
}

@article{10.1016/j.jss.2019.110419,
author = {Jung, Pilsu and Kang, Sungwon and Lee, Jihyun},
title = {Automated code-based test selection for software product line regression testing},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110419},
doi = {10.1016/j.jss.2019.110419},
journal = {J. Syst. Softw.},
month = dec,
numpages = {19},
keywords = {Product lines testing, Regression test selection, Software maintenance, Software evolution}
}

@inproceedings{10.1145/2791060.2791077,
author = {Rumpe, Bernhard and Schulze, Christoph and von Wenckstern, Michael and Ringert, Jan Oliver and Manhart, Peter},
title = {Behavioral compatibility of simulink models for product line maintenance and evolution},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791077},
doi = {10.1145/2791060.2791077},
abstract = {Embedded software systems, e.g. automotive, robotic or automation systems are highly configurable and consist of many software components being available in different variants and versions. To identify the degree of reusability between these different occurrences of a component, it is necessary to determine the functional backward and forward compatibility between them. Based on this information it is possible to identify in which system context a component can be replaced safely by another version, e.g. exchanging an older component, or variant, e.g. introducing new features, to achieve the same functionality.This paper presents a model checking approach to determine behavioral compatibility of Simulink models, obtained from different component variants or during evolution. A prototype for automated compatibility checking demonstrates its feasibility. In addition implemented optimizations make the analysis more efficient, when the compared variants or versions are structurally similar.A case study on a driver assistance system provided by Daimler AG shows the effectiveness of the approach to automatically compare Simulink components.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {141–150},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1007/s10664-012-9234-8,
author = {Reinhartz-Berger, Iris and Sturm, Arnon},
title = {Comprehensibility of UML-based software product line specifications},
year = {2014},
issue_date = {June      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-012-9234-8},
doi = {10.1007/s10664-012-9234-8},
abstract = {Software Product Line Engineering (SPLE) deals with developing artifacts that capture the common and variable aspects of software product families. Domain models are one kind of such artifacts. Being developed in early stages, domain models need to specify commonality and variability and guide the reuse of the artifacts in particular software products. Although different modeling methods have been proposed to manage and support these activities, the assessment of these methods is still in an inceptive stage. In this work, we examined the comprehensibility of domain models specified in ADOM, a UML-based SPLE method. In particular, we conducted a controlled experiment in which 116 undergraduate students were required to answer comprehension questions regarding a domain model that was equipped with explicit reuse guidance and/or variability specification. We found that explicit specification of reuse guidance within the domain model helped understand the model, whereas explicit specification of variability increased comprehensibility only to a limited extent. Explicit specification of both reuse guidance and variability often provided intermediate results, namely, results that were better than specification of variability without reuse guidance, but worse than specification of reuse guidance without variability. All these results were perceived in different UML diagram types, namely, use case, class, and sequence diagrams and for different commonality-, variability-, and reuse-related aspects.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {678–713},
numpages = {36},
keywords = {Domain models, Empirical evaluation, Software product line engineering, UML, Variability management}
}

@inproceedings{10.1007/978-3-642-33678-2_30,
author = {Braga, Rosana T. Vaccare and Trindade Junior, Onofre and Castelo Branco, Kalinka Regina and Neris, Luciano De Oliveira and Lee, Jaejoon},
title = {Adapting a software product line engineering process for certifying safety critical embedded systems},
year = {2012},
isbn = {9783642336775},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33678-2_30},
doi = {10.1007/978-3-642-33678-2_30},
abstract = {Software Product Line Engineering (SPLE) is a software development paradigm that aims at reducing the development effort and shorting time-to-market through systematic software reuse. While this paradigm has been successfully applied for the development of embedded systems in various domains, new challenges have emerged from the development of safety critical systems that require certification against a specific standard. Existing SPLE approaches do not explicitly consider the various certification standards or levels that products should satisfy. In this paper, we focus on several practical issues involved in the SPLE process, establishing an infrastructure of a product line engineering for certified products. A metamodel is proposed to capture the entities involved in SPL certification and the relationships among them. ProLiCES, which is a model-driven process for the development of SPLs, was modified to serve as an example of our approach, in the context of the UAV (Unmanned Aerial Vehicle) domain.},
booktitle = {Proceedings of the 31st International Conference on Computer Safety, Reliability, and Security},
pages = {352–363},
numpages = {12},
keywords = {development process, safety-critical embedded systems, software certification},
location = {Magdeburg, Germany},
series = {SAFECOMP'12}
}

@inproceedings{10.1145/3289402.3289508,
author = {Meftah, Chaimae and Retbi, Asmaâ and Bennani, Samir and Idrissi, Mohammed Khalidi},
title = {Exploration of Software Product Line to Enrich the Modeling of Mobile Serious Games},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289508},
doi = {10.1145/3289402.3289508},
abstract = {The marked interest of educational environments for the use of Serious Games (SG) promotes learning. The connected generation nowadays is influenced by the new practices of their use. Hence the integration of mobility that facilitates access to games. A learner focuses on training through the accessibility at all times, the quality of content, and the particularity and difference of the scenarios. The valorization of the use of Mobile Serious Games (MSG) requires the adoption of an approach. We have started our study by detecting the characteristics influencing the attachment of a learner, thereafter, we have studied the advantages of mobility in our connected society, then, we have explored the Software Product Line (SPL) modeling approach that presents the most appropriate solution to solve our problematic. Finally, we have treated the notion of variability. This concept allows the detection of common and variable points between the products of the set. This will then facilitate the modeling of serious games.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {8},
numpages = {5},
keywords = {Learning, Mobile Serious Games, Software Product Line, connected generation, mobility, modeling, scenario, variability},
location = {Rabat, Morocco},
series = {SITA'18}
}

@article{10.4018/ijkss.2014100101,
author = {Tekinerdogan, Bedir and Erdoğan, Özgü Özköse and Aktuğ, Onur},
title = {Supporting Incremental Product Development using Multiple Product Line Architecture},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100101},
doi = {10.4018/ijkss.2014100101},
abstract = {Software product line engineering SPLE has been successfully applied in various application domains to support systematic reuse. Besides of its benefits it is also acknowledged that the SPLE process can in practice be considered too time consuming and heavyweight due to the required planning and development of the asset base. For this reason more lightweight SPLE processes are required that can be integrated in the ongoing product development of the organization. In this context, the authors share their experiences in adopting a multiple product line architecture to support the incremental product development of Aselsan REHIS, a leading high technology company in Turkey. The authors first discuss the important business needs for defining a more lightweight multiple product line engineering MPLE process. Then they discuss the multiple product line architecture and how it has been used to guide the incremental product development.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {1–16},
numpages = {16},
keywords = {Incremental Process, Industrial Case, Introduction, Multiple Product Line Engineering, Software architecture design}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {configurable systems, machine learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1109/sst.2019.00011,
author = {Hajri, Ines and Goknil, Arda and Briand, Lionel C. and Stephany, Thierry},
title = {Change impact analysis for evolving configuration decisions in product line use case models},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/sst.2019.00011},
doi = {10.1109/sst.2019.00011},
abstract = {Product Line Engineering is a key practice in many software development environments where systems are complex and developed for multiple customers with varying needs. At the same time, many business contexts are use case-driven where use cases are the main artifacts driving requirements elicitation and many other development activities [1], [2]. In such contexts, variability information is often not explicitly represented, which leads to the ad-hoc change management of development artifacts such as use cases, domain models and test cases in product families. This work is about achieving automated and effective change management in a product family within the context of use case-driven development and system testing. To this end, we provide the following:A modeling methodology for capturing variability information in Product Line (PL) use case and domain models. The modeling methodology enables analysts to capture and document variability in PL use case diagrams, use case specifications, and domain models, without further requiring a feature model. For PL use case diagrams and domain models, it relies on extensions already discussed in the literature. Further, for PL use case specifications, we build on Restricted Use Case Modeling (RUCM), which includes a template and restriction rules to reduce imprecision and incompleteness in use cases [3].An approach for automated configuration of Product Specific (PS) use case and domain models. The use case-driven configuration approach is built on top of our modeling methodology. Based on a tool (PUMConf), it supports automated configuration that enables effective product-line management in use case-driven development, without requiring additional modeling artifacts and traceability to external models such as feature models [4].A change impact analysis approach for evolving configuration decisions in PL use case models. Our approach automatically identifies decisions impacted by changes in configuration decisions on PL use case models. It supports three activities. First, the analyst proposes a change but does not apply it to the corresponding configuration decision. Second, the impact of the proposed change on other configuration decisions are automatically identified. To this end, we developed an algorithm, which enables reasoning on subsequent decisions as part of our impact analysis approach. The analyst is informed about the change impact on decisions for the PL use case model. Based on this, the analyst should decide whether the proposed change is to be applied to the corresponding decision. Third, the PS use case models are incrementally regenerated only for the impacted decisions after the analyst actually makes all the required changes. To support these three activities, we extended our configurator, PUMConf [5], [6].An approach for automated classification and prioritization of system test cases in a family of products. Our approach automatically identify system test cases impacted by changes in configuration decisions in PL use case models, when a new product is configured in the product family. The initial product is tested individually and the following products are tested using techniques inspired by regression testing, i.e., test case classification and prioritization, based on configuration decision changes between the previous product(s) and the new product to be tested.},
booktitle = {Proceedings of the 10th International Workshop on Software and Systems Traceability},
pages = {11},
numpages = {1},
location = {Montreal, Quebec, Canada},
series = {SST '19}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Krüger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@article{10.1007/s10270-015-0479-8,
author = {Devroey, Xavier and Perrouin, Gilles and Cordy, Maxime and Samih, Hamza and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Statistical prioritization for software product line testing: an experience report},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0479-8},
doi = {10.1007/s10270-015-0479-8},
abstract = {Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {153–171},
numpages = {19},
keywords = {D.2.5, D.2.7, Prioritization, Software product line testing, Statistical testing}
}

@inproceedings{10.1145/2791060.2791105,
author = {Teixeira, Leopoldo and Alves, Vander and Borba, Paulo and Gheyi, Rohit},
title = {A product line of theories for reasoning about safe evolution of product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791105},
doi = {10.1145/2791060.2791105},
abstract = {A product line refinement theory formalizes safe evolution in terms of a refinement notion, which does not rely on particular languages for the elements that constitute a product line. Based on this theory, we can derive refinement templates to support safe evolution scenarios. To do so, we need to provide formalizations for particular languages, to specify and prove the templates. Without a systematic approach, this leads to many similar templates and thus repetitive verification tasks. We investigate and explore similarities between these concrete languages, which ultimately results in a product line of theories, where different languages correspond to features, and products correspond to theory instantiations. This also leads to specifying refinement templates at a higher abstraction level, which, in the long run, reduces the specification and proof effort, and also provides the benefits of reusing such templates for additional languages plugged into the theory. We use the Prototype Verification System to encode and prove soundness of the theories and their instantiations. Moreover, we also use the refinement theory to reason about safe evolution of the proposed product line of theories.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {161–170},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.5555/1753235.1753257,
author = {Carbon, Ralf and Adam, Sebastian and Uchida, Takayuki},
title = {Towards a product line approach for office devices: facilitating customization of office devices at Ricoh Co. Ltd.},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Like in many other domains, customization is one of the major challenges for providers of office devices like Ricoh Co. Ltd. The specific challenge is the integration of office devices with the workflows and existing office infrastructures of customers. Hence, an approach to support customization of office devices has been developed by Ricoh and Fraunhofer IESE. The approach builds upon product line engineering and focuses on improving application engineering to support customization based on the workflows and office infrastructures of specific customers. The key ideas of the approach are flexibility concepts on architecture level that support recurring types of customizations and a requirements engineering process that is driven by the workflows of individual customers. In this paper, the approach developed in cooperation with Ricoh is presented. A case study illustrates the applicability of the concepts and the overall approach.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {151–160},
numpages = {10},
keywords = {application engineering, flexibility, product line architecture, product line engineering, service orientation},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Configuration Process, Decision Process, Economic Value, Software Product Line, Utility Elicitation}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {preference-based multi-objective algorithms, search-based software engineering, software product line testing},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@article{10.1016/j.scico.2012.05.003,
author = {Laguna, Miguel A. and Crespo, Yania},
title = {A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {8},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.05.003},
doi = {10.1016/j.scico.2012.05.003},
abstract = {Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities.},
journal = {Sci. Comput. Program.},
month = aug,
pages = {1010–1034},
numpages = {25},
keywords = {Evolution, Legacy system, Reengineering, Refactoring, Software product line}
}

@inproceedings{10.1145/3357765.3359515,
author = {Hinterreiter, Daniel and Nieke, Michael and Linsbauer, Lukas and Seidl, Christoph and Prähofer, Herbert and Grünbacher, Paul},
title = {Harmonized temporal feature modeling to uniformly perform, track, analyze, and replay software product line evolution},
year = {2019},
isbn = {9781450369800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357765.3359515},
doi = {10.1145/3357765.3359515},
abstract = {A feature model (FM) describes commonalities and variability within a software product line (SPL) and represents the configuration options at one point in time. A temporal feature model (TFM) additionally represents FM evolution, e.g., the change history or the planning of future releases. The increasing number of different TFM notations hampers research collaborations due to a lack of interoperability regarding notations, editors, and analyses. We present a common API for TFMs, which provides the core of a TFM ecosystem, to harmonize notations. We identified the requirements for the API based on systematically classifying and comparing the capabilities of existing TFM approaches. Our approach allows to work seamlessly with different TFM notations to perform, track, analyze and replay evolution. Our evaluation investigates two research questions on the expressiveness (RQ1) and utility (RQ2) of our approach by presenting implementations for several existing FM and TFM notations and replaying evolution histories from two case study systems.},
booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {115–128},
numpages = {14},
keywords = {evolution, software product lines},
location = {Athens, Greece},
series = {GPCE 2019}
}

@inproceedings{10.1145/2993236.2993249,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {A feature-based personalized recommender system for product-line configuration},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993249},
doi = {10.1145/2993236.2993249},
abstract = {Today’s competitive marketplace requires the industry to understand unique and particular needs of their customers. Product line practices enable companies to create individual products for every customer by providing an interdependent set of features. Users configure personalized products by consecutively selecting desired features based on their individual needs. However, as most features are interdependent, users must understand the impact of their gradual selections in order to make valid decisions. Thus, especially when dealing with large feature models, specialized assistance is needed to guide the users in configuring their product. Recently, recommender systems have proved to be an appropriate mean to assist users in finding information and making decisions. In this paper, we propose an advanced feature recommender system that provides personalized recommendations to users. In detail, we offer four main contributions: (i) We provide a recommender system that suggests relevant features to ease the decision-making process. (ii) Based on this system, we provide visual support to users that guides them through the decision-making process and allows them to focus on valid and relevant parts of the configuration space. (iii) We provide an interactive open-source configurator tool encompassing all those features. (iv) In order to demonstrate the performance of our approach, we compare three different recommender algorithms in two real case studies derived from business experience.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {120–131},
numpages = {12},
keywords = {Personalized Recommendations, Product-Line Configuration, Recommenders, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@article{10.1016/j.jss.2016.07.039,
author = {Shatnawi, Anas and Seriai, Abdelhak-Djamel and Sahraoui, Houari},
title = {Recovering software product line architecture of a family of object-oriented product variants},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.07.039},
doi = {10.1016/j.jss.2016.07.039},
abstract = {Automatic architecture recovery from a set of software product variants.Recovering variability concerning component, configuration, and dependencies.Experimented on two families of product variants. Software Product Line Engineering (SPLE) aims at applying a pre-planned systematic reuse of large-grained software artifacts to increase the software productivity and reduce the development cost. The idea of SPLE is to analyze the business domain of a family of products to identify the common and the variable parts between the products. However, it is common for companies to develop, in an ad-hoc manner (e.g. clone and own), a set of products that share common services and differ in terms of others. Thus, many recent research contributions are proposed to re-engineer existing product variants to a software product line. These contributions are mostly focused on managing the variability at the requirement level. Very few contributions address the variability at the architectural level despite its major importance. Starting from this observation, we propose an approach to reverse engineer the architecture of a set of product variants. Our goal is to identify the variability and dependencies among architectural-element variants. Our work relies on formal concept analysis to analyze the variability. To validate the proposed approach, we evaluated on two families of open-source product variants; Mobile Media and Health Watcher. The results of precision and recall metrics of the recovered architectural variability and dependencies are 81%, 91%, 67% and 100%, respectively.},
journal = {J. Syst. Softw.},
month = sep,
pages = {325–346},
numpages = {22},
keywords = {Formal concept analysis, Object-oriented product variants, Software architecture recovery, Software component, Software product line, Software reuse}
}

@inproceedings{10.1145/2973839.2973842,
author = {Lima, Crescencio and Chavez, Christina},
title = {A Systematic Review on Metamodels to Support Product Line Architecture Design},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973842},
doi = {10.1145/2973839.2973842},
abstract = {Product Line Architecture (PLA) design is a key activity for developing successful Software Product Line (SPL) projects. PLA design is a difficult task, mostly due to the complexity of the software systems that SPLs deal with, and their variabilities. Metamodels have been used to support the representation of assets that compose a PLA, SPL variability and the relationships among them. The goal of this study is to characterize the use of metamodeling on PLA design, aiming to identify the main characteristics of metamodels, the elements used for PLA and variability representation and trace the evolution of metamodels. We conducted a systematic literature review to identify the primary studies on the use of metamodels in PLA Design. Thirty-five studies that proposed metamodels to support PLA design were selected. The review main findings are: (i) it is difficult to identify the existence of research trends because the number of publication varies and metamodels lack standardization; (ii) several metamodels support feature representation; (iii) the majority of studies addressed variability representation with variation points in UML diagrams; and, (iv) five evolution lines that describe how metamodels evolved over the years were identified.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {13–22},
numpages = {10},
keywords = {Metamodels, Product Line Architecture, Software Product Lines, Systematic Literature Review, Variability},
location = {Maringá, Brazil},
series = {SBES '16}
}

@article{10.1007/s10664-014-9358-0,
author = {Koziolek, Heiko and Goldschmidt, Thomas and Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan and Gamer, Thomas and Aleksy, Markus},
title = {Assessing software product line potential: an exploratory industrial case study},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9358-0},
doi = {10.1007/s10664-014-9358-0},
abstract = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {411–448},
numpages = {38},
keywords = {Business case, Domain analysis, Software product lines}
}

@inproceedings{10.1145/3178298.3178300,
author = {Elmoniem, Mohamed A. Abd and Nasr, Eman S. and Gheith, Mervat H.},
title = {A Requirements Elicitation Tool for Cloud-Based ERP Software Product Line},
year = {2017},
isbn = {9781450355124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178298.3178300},
doi = {10.1145/3178298.3178300},
abstract = {Software Product Line (SPL) 1 is a very promising trend of software reusability. It could be applied in different fields in order to enhance and facilitate the software development process. In the last years, SPLs have broken into Enterprise Resource Planning (ERP) systems. An increasing need showed up for techniques, approaches and tools that combine SPL with ERP. Cloud ERP systems offer many benefits for Small and Medium Enterprises (SME). Managing the requirements elicitation process for Cloud ERP SPLs is a challenging process that faces many difficulties, such as the indirect interaction and the larger context of the target consumers. Facilitating the requirements elicitation process for cloud-based ERP SPLs by using automated tools will help to solve the faced difficulties. To the best of our knowledge, in the context of ERP SPLs, there are no tools for eliciting the requirements of cloud--based ERP SPLs nor even for ERP SPL. This paper exploits the advantages of the Form-Based Model (FBM) as a conceptual model to integrate it with cloud based ERP SPL. In addition, based on this integration, the paper presents a tool for eliciting the requirements of cloud-based ERP SPLs},
booktitle = {Proceedings of the 3rd Africa and Middle East Conference on Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {Cloud SPLs, ERP, SaaS ERP, SaaS ERP tool, SaaS SPLs, extended feature model, feature model, form-based model, mapping functional requirements, requirements engineering, requirements engineering tool, software product line},
location = {Cairo, Egypt},
series = {AMECSE '17}
}

@article{10.1007/s00766-014-0203-1,
author = {Díaz, Jessica and Pérez, Jennifer and Garbajosa, Juan},
title = {A model for tracing variability from features to product-line architectures: a case study in smart grids},
year = {2015},
issue_date = {September 2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0203-1},
doi = {10.1007/s00766-014-0203-1},
abstract = {In current software systems with highly volatile requirements, traceability plays a key role to maintain the consistency between requirements and code. Traceability between artifacts involved in the development of software product line (SPL) is still more critical because it is necessary to guarantee that the selection of variants that realize the different SPL products meet the requirements. Current SPL traceability mechanisms trace from variability in features to variations in the configuration of product-line architecture (PLA) in terms of adding and removing components. However, it is not always possible to materialize the variable features of a SPL through adding or removing components, since sometimes they are materialized inside components, i.e., in part of their functionality: a class, a service, and/or an interface. Additionally, variations that happen inside components may crosscut several components of architecture. These kinds of variations are still challenging and their traceability is not currently well supported. Therefore, it is not possible to guarantee that those SPL products with these kinds of variations meet the requirements. This paper presents a solution for tracing variability from features to PLA by taking these kinds of variations into account. This solution is based on models and traceability between models in order to automate SPL configuration by selecting the variants and realizing the product application. The FPLA modeling framework supports this solution which has been deployed in a software factory. Validation has consisted in putting the solution into practice to develop a product line of power metering management applications for smart grids.},
journal = {Requir. Eng.},
month = sep,
pages = {323–343},
numpages = {21},
keywords = {Product-line architecture, Software product line engineering, Traceability modeling, Variability}
}

@inproceedings{10.5555/1753235.1753250,
author = {Jepsen, Hans Peter and Beuche, Danilo},
title = {Running a software product line: standing still is going backwards},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Danfoss Drives - one of the largest producers of frequency converters in the world - has been doing Software Product Line development for its frequency converter products for about 3 years. This paper describes the approach used and the experiences with it. It discusses processes, ways to convince the unconvinced and arising tool issues when doing product line development.This paper is a follow-up on a previous article which described the product line migration process in detail.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {101–110},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2362536.2362570,
author = {Braga, Rosana T. V. and Trindade, Onofre and Branco, Kalinka R. L. J. Castelo and Lee, Jaejoon},
title = {Incorporating certification in feature modelling of an unmanned aerial vehicle product line},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362570},
doi = {10.1145/2362536.2362570},
abstract = {Safety critical systems (e.g., an avionics control system for safe flight) are often required to achieve certification under pre-established standards (e.g., DO-178B for software considerations in airborne systems and equipment certification). We have been working with our industrial partner for the last three years to develop product line assets for their avionics software product line (SPL) and, recently, we encountered two major challenges regarding certification. Firstly, an individual product must be certified, but each may require a different certification level: there might be variations in the certification requirements according to specific system usage contexts. Secondly, certification involves not only product but also process, as standards such as DO-178B also assess the quality of the development process. In this paper, we propose to include a certification view during feature modelling to provide a better understanding of the relationships between features and a certification level required for each product. The experience of introducing certification into the design model of an Unmanned Aerial Vehicle (UAV) SPL is presented to illustrate some key ideas. We also describe the lessons we have learned from this experience.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {249–258},
numpages = {10},
keywords = {critical software development, feature modelling, software certification, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3461001.3471147,
author = {Kenner, Andy and May, Richard and Krüger, Jacob and Saake, Gunter and Leich, Thomas},
title = {Safety, security, and configurable software systems: a systematic mapping study},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471147},
doi = {10.1145/3461001.3471147},
abstract = {Safety and security are important properties of any software system, particularly in safety-critical domains, such as embedded, automotive, or cyber-physical systems. Moreover, particularly those domains also employ highly-configurable systems to customize variants, for example, to different customer requirements or regulations. Unfortunately, we are missing an overview understanding of what research has been conducted on the intersection of safety and security with configurable systems. To address this gap, we conducted a systematic mapping study based on an automated search, covering ten years (2011--2020) and 65 relevant (out of 367) publications. We classified each publication based on established security and safety concerns (e.g., CIA triad) as well as the connection to configurable systems (e.g., ensuring security of such a system). In the end, we found that considerably more research has been conducted on safety concerns, but both properties seem under-explored in the context of configurable systems. Moreover, existing research focuses on two directions: Ensuring safety and security properties in product-line engineering; and applying product-line techniques to ensure safety and security properties. Our mapping study provides an overview of the current state-of-the-art as well as open issues, helping practitioners identify existing solutions and researchers define directions for future research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {148–159},
numpages = {12},
keywords = {configurable systems, mapping study, safety, security, software product line engineering},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1016/j.infsof.2006.08.008,
author = {Her, Jin Sun and Kim, Ji Hyeok and Oh, Sang Hun and Rhew, Sung Yul and Kim, Soo Dong},
title = {A framework for evaluating reusability of core asset in product line engineering},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.008},
doi = {10.1016/j.infsof.2006.08.008},
abstract = {Product line engineering (PLE) is a new effective approach to software reuse, where applications are generated by instantiating a core asset which is a large-grained reuse unit. Hence, a core asset is a key element of PLE, and therefore the reusability of the core asset largely determines the success of PLE projects. However, current quality models to evaluate reusability do not adequately address the unique characteristics of core assets in PLE. This paper proposes a comprehensive framework for evaluating the reusability of core assets. We first identify the key characteristics of core assets, and derive a set of quality attributes that characterizes the reusability of core assets. Then, we define metrics for each quality attribute and finally present practical guidelines for applying the evaluation framework in PLE projects. Using the proposed framework, the reusability of core assets can be more effectively and precisely evaluated.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {740–760},
numpages = {21},
keywords = {Core asset, Metric, Product line engineering, Quality model, Reusability}
}

@inproceedings{10.1145/3422392.3422498,
author = {Freire, Willian Marques and Massago, Mamoru and Zavadski, Arthur Cattaneo and Malachini, Aline Maria and Amaral, Miotto and Colanzi, Thelma Elita},
title = {OPLA-Tool v2.0: a Tool for Product Line Architecture Design Optimization},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422498},
doi = {10.1145/3422392.3422498},
abstract = {The Multi-objective Optimization Approach for Product Line Architecture Design (MOA4PLA) is the seminal approach that successfully optimizes Product Line Architecture (PLA) design using search algorithms. The tool named OPLA-Tool was developed in order to automate the use of MOA4PLA. Over time, the customization of the tool to suit the needs of new research and application scenarios led to several problems. The main problems identified in the original version of OPLA-Tool are environment configuration, maintainability and usability problems, and PLA design modeling and visualization. Such problems motivated the development of a new version of this tool: OPLA-Tool v2.0, presented in this work. In this version, those problems were solved by the source code refactoring, migration to a web-based graphical user interface (GUI) and inclusion of a new support tool for PLA modeling and visualization. Furthermore, OPLA-Tool v2.0 has new functionalities, such as new objective functions, new search operators, intelligent interaction with users during the optimization process, multi-user authentication and simultaneous execution of several experiments to PLA optimization. Such a new version of OPLA-Tool is an important achievement to PLA design optimization as it provides an easier and more complete way to automate this task.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {818–823},
numpages = {6},
keywords = {Software product line, multi-objective evolutionary algorithms, product line architecture},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/1062455.1062551,
author = {Verlage, Martin and Kiesgen, Thomas},
title = {Five years of product line engineering in a small company},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062551},
doi = {10.1145/1062455.1062551},
abstract = {In 1999, a new team at MARKET MAKER Software AG began to develop a software product line for managing and displaying stock market data and financial market news. The basic idea was to use web technology in all applications for delivering services to customers. It soon turned out that the company had to change both the processes and the organization. This report summarizes the changes made and the lessons learned over the past five years, when the product line idea was introduced into a small company which faced the pressure to quickly market the first product line instances.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {534–543},
numpages = {10},
keywords = {SME, experience report, product line engineering, project management},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@inproceedings{10.1145/2993412.3003392,
author = {Boss, Birgit and Tischer, Christian and Krishnan, Sreejith and Nutakki, Arun and Gopinath, Vinod},
title = {Setting up architectural SW health builds in a new product line generation},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3003392},
doi = {10.1145/2993412.3003392},
abstract = {Setting up a new product line generation in a mature domain, typically does not start from scratch but takes into consideration the architecture and assets of the former product line generation. Being able to accommodate legacy and 3rd party code is one of the major product line qualities to be met. On the other side, product line qualities like reusability, maintainability and alterability, i.e. being able to cope up with a large amount of variability, with configurability and fast integratability are major drivers.While setting up a new product line generation and thus a new corresponding architecture, we this time focused on architectural software (SW) health and tracking of architectural metrics from the very beginning. Taking the definition of "architecture being a set of design decisions" [18] literally, we attempt to implement an architectural check for every design decision taken. Architectural design decisions in our understanding do not only - and even not mainly - deal with the definition of components and their interaction but with patterns and rules or anti-patterns. The rules and anti-patterns, "what not to do" or more often also "what not to do &lt;u&gt;any more&lt;/u&gt;", is even more important in setting up a new product line generation because developers are not only used to the old style of developing and the old architecture, but also still have to develop assets for both generations.In this article we describe selected architectural checks that we have implemented, the layered architecture check and the check for usage of obsolete services. Additionally we discuss selected architectural metrics: the coupling coefficient metrics and the instability metrics. In the summary and outlook we describe our experiences and still open topics in setting up architectural SW health checks for a large-scale product line.The real-world examples are taken from the domain of Engine Control Unit development at Robert Bosch GmbH.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {16},
numpages = {7},
keywords = {architectural checks, architectural technical debt, embedded software, product line development, software architecture, software erosion, technical debt},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@inproceedings{10.5555/776816.776934,
author = {Klein, John and Price, Barry and Weiss, David},
title = {Industrial-strength software product-line engineering},
year = {2003},
isbn = {076951877X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Software product-line engineering is one of the few approaches to software engineering that shows promise of improving software productivity by factors of 5 to 10. There are still few examples of its successful application on a large scale, partly because of the complexity of initiating a product-line engineering project and the many factors that must be addressed for such a project to be successful.This tutorial draws on experiences in introducing and sustaining product-line engineering in Lucent Technologies and in Avaya. The objective is to convey to participants the obstacles involved in transitioning to product line engineering and how to overcome such obstacles, particularly in large software development organizations. Participants will learn both technical and organizational aspects of the problem. Participants will leave the tutorial with many ideas on how to introduce product line engineering into an organization in a systematic way.},
booktitle = {Proceedings of the 25th International Conference on Software Engineering},
pages = {751–752},
numpages = {2},
location = {Portland, Oregon},
series = {ICSE '03}
}

@inproceedings{10.1145/2648511.2648541,
author = {Gregg, Susan P. and Scharadin, Rick and LeGore, Eric and Clements, Paul},
title = {Lessons from AEGIS: organizational and governance aspects of a major product line in a multi-program environment},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648541},
doi = {10.1145/2648511.2648541},
abstract = {This paper tells the story of the AEGIS Weapon System product line and how it evolved from a series of standalone software programs with no sharing into a true systems and software product line. The paper focuses on the strong internal and external governance of the product line. The need for strong governance is brought about by the strong role that the AEGIS customer community plays in oversight of design, development, and procurement. The paper recounts the product line's beginnings, and describes how the product line is operated today. Organizational issues, measurement issues, and governance issues are covered, along with a summary of important lessons learned about operating a product line in an environment of strong competing interests.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {264–273},
numpages = {10},
keywords = {AEGIS, Navy, bill-of-features, combat systems, command and control, feature modeling, feature profiles, hierarchical product lines, product audit, product baselines, product configurator, product derivation, product line engineering, product line governance, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2019136.2019184,
author = {Holl, Gerald},
title = {Product line bundles to support product derivation in multi product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019184},
doi = {10.1145/2019136.2019184},
abstract = {A multi product line comprises several heterogeneous product lines that are part of a large-scale system. Typically, multi product lines (MPLs) cannot be managed centrally as the involved product lines are developed and evolved by multiple teams that often work independently. Support for modularity and the management of dependencies are thus essential in MPLs. We aim at developing a tool-supported approach for modularizing MPLs to facilitate product derivation in MPLs. We elicit requirements for MPL tool support based on a literature survey and by involving experts from our industry partners. Our approach is based on product line bundles which allow to organize and deploy product line models and specific tool support. We also support the distributed configuration of a MPL based on different types of dependencies between product lines. We will evaluate our research in the laboratory and industrial case studies. We will also conduct experiments to assess the usefulness of the developed tool features.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {41},
numpages = {6},
keywords = {multi product lines, product derivation, tool support},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, José A. and Acher, Mathieu and Jézéquel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {constraints and variability mining, machine learning, software product lines, software testing, variability modeling},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2430502.2430531,
author = {Wulf-Hadash, Ora and Reinhartz-Berger, Iris},
title = {Cross product line analysis},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430531},
doi = {10.1145/2430502.2430531},
abstract = {Due to increase in market competition and merger and acquisition of companies, different software product lines (SPLs) may exist under the same roof. These SPLs may be developed applying different domain analysis processes, but are likely not disjoint. Cross product line analysis aims to examine the common and variable aspects of different SPLs for improving maintenance and future development of related SPLs. Currently different SPL artifacts, or more accurately feature models, are compared, matched, and merged for supporting scalability, increasing modularity and reuse, synchronizing feature model versions, and modeling multiple SPLs for software supply chains. However, in all these cases the focus is on creating valid merged models from the input feature models. Furthermore, the terminology used in all the input feature models is assumed to be the same, namely similar features are named the same. As a result these methods cannot be simply applied to feature models that represent different SPLs. In this work we offer adapting similarity metrics and text clustering techniques in order to enable cross product line analysis. This way analysis of feature models that use different terminologies in the same domain can be done in order to improve the management of the involved SPLs. Preliminary results reveal that the suggested method helps systematically analyze the commonality and variability between related SPLs, potentially suggesting improvements to existing SPLs and to the maintenance of sets of SPLs.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {21},
numpages = {8},
keywords = {empirical evaluation, feature clustering, feature diagram matching, feature diagram merging, feature similarity},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@article{10.1007/s10515-015-0185-3,
author = {Bürdek, Johannes and Kehrer, Timo and Lochau, Malte and Reuling, Dennis and Kelter, Udo and Schürr, Andy},
title = {Reasoning about product-line evolution using complex feature model differences},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-015-0185-3},
doi = {10.1007/s10515-015-0185-3},
abstract = {Features define common and variable parts of the members of a (software) product line. Feature models are used to specify the set of all valid feature combinations. Feature models not only enjoy an intuitive tree-like graphical syntax, but also a precise formal semantics, which can be denoted as propositional formulae over Boolean feature variables. A product line usually constitutes a long-term investment and, therefore, has to undergo continuous evolution to meet ever-changing requirements. First of all, product-line evolution leads to changes of the feature model due to its central role in the product-line paradigm. As a result, product-line engineers are often faced with the problems that (1) feature models are changed in an ad-hoc manner without proper documentation, and (2) the semantic impact of feature diagram changes is unclear. In this article, we propose a comprehensive approach to tackle both challenges. For (1), our approach compares the old and new version of the diagram representation of a feature model and specifies the changes using complex edit operations on feature diagrams. In this way, feature model changes are automatically detected and formally documented. For (2), we propose an approach for reasoning about the semantic impact of diagram changes. We present a set of edit operations on feature diagrams, where complex operations are primarily derived from evolution scenarios observed in a real-world case study, i.e., a product line from the automation engineering domain. We evaluated our approach to demonstrate its applicability with respect to the case study, as well as its scalability concerning experimental data sets.},
journal = {Automated Software Engg.},
month = dec,
pages = {687–733},
numpages = {47},
keywords = {Feature models, Model-driven engineering, Software evolution, Software product lines}
}

@article{10.1287/deca.2013.0273,
author = {Xiong, Hui and Chen, Ying-Ju},
title = {Product Line Design with Deliberation Costs: A Two-Stage Process},
year = {2013},
issue_date = {September 2013},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {10},
number = {3},
issn = {1545-8490},
url = {https://doi.org/10.1287/deca.2013.0273},
doi = {10.1287/deca.2013.0273},
abstract = {Motivated by the current practice of some service industries, we study a two-stage product line design problem in the presence of private consumer deliberation costs. In this problem, consumers pay an upfront payment before accessing the products. A consumer may incur a cost to introspect about her preferences, and the product line design enables the seller to encourage or discourage introspection. Getting consumers to deliberate about their preferences benefits them and the seller, because it facilitates a tailored product line. However, in so doing, the seller is forced to compensate the consumers for their cognitive cost of deliberation. We show that this compensation causes the seller an additional expense to avoid cannibalization across product lines. If the heterogeneity in deliberation costs is high, the seller is better off by offering a compromise product to consumers with high deliberation costs to discourage them from deliberation. The seller does not intentionally downward distort the qualities and optimally designs the product line that is ex-post socially efficient instead. This is because the seller is able to counterbalance the loss of second-stage information asymmetry by charging the upfront payment. Nonetheless, heterogeneity on consumer deliberation costs leads to an unambiguous reduction of consumer deliberation.},
journal = {Decision Analysis},
month = sep,
pages = {225–244},
numpages = {20},
keywords = {adverse selection, deliberation cost, dynamic mechanism design, product line design}
}

@article{10.1016/j.asoc.2016.08.024,
author = {dos Santos Neto, Pedro de Alcntara and Britto, Ricardo and Rablo, Ricardo de Andrade Lira and Cruz, Jonathas Jivago de Almeida and Lira, Werney Ayala Luz},
title = {A hybrid approach to suggest software product line portfolios},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.024},
doi = {10.1016/j.asoc.2016.08.024},
abstract = {Graphical abstractDisplay Omitted HighlightsThe work proposes a hybrid approach to deal with the Product Portfolio Scope Problem.The approach is composed by a solution to deploy the feature relevance indicated by the customers into code assets of a SPL, based on a systematic method (SQFD).The approach includes a method to estimate the cost of an asset based on common and relevant measures related to source code, together with a fuzzy system to deal with the imprecision to set reference values.The work presents an application of an NSGA-II to search for products minimizing the cost and maximizing the relevance of the candidate products.The approach was evaluated using different scenarios, exploring the mains aspects related to method in the practice: size, granularity of features and products search space.The previous version of our hybrid approach was dependent on the employed technologies and algorithms. Herein we reformulate our approach, detaching it from any particular technique/algorithm.The data collection process associated with our approach was improved to facilitate the hybrid approach's usage and mitigate associated construct validity threats.A more comprehensive evaluation, which focused on show the real word usefulness and scalability of our hybrid approach. To validate the usefulness of our approach, it was used the SPL associated with a tool broadly employed in both industrial and academic contexts (ArgoUML-SPL). The scalability of our approach was evaluated using a synthetic SPL.All the experiments were based on the guidelines defined by Arcuri and Briand in order to evaluate the statistical significance of this kind of work. Software product line (SPL) development is a new approach to software engineering which aims at the development of a whole range of products. However, as long as SPL can be useful, there are many challenges regarding the use of that approach. One of the main problems which hinders the adoption of software product line (SPL) is the complexity regarding product management. In that context, we can remark the scoping problem. One of the existent ways to deal with scoping is the product portfolio scoping (PPS). PPS aims to define the products that should be developed as well as their key features. In general, that approach is driven by marketing aspects, like cost of the product and customer satisfaction. Defining a product portfolio by using the many different available aspects is a NP-hard problem. This work presents an improved hybrid approach to solve the feature model selection problem, aiming at supporting product portfolio scoping. The proposal is based in a hybrid approach not dependent on any particular algorithm/technology. We have evaluated the usefulness and scalability of our approach using one real SPL (ArgoUML-SPL) and synthetic SPLs. As per the evaluation results, our approach is both useful from a practitioner's perspective and scalable.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1243–1255},
numpages = {13},
keywords = {Feature model selection problem, Fuzzy inference systems, NSGA-II, Product portfolio scoping, Search based feature model selection, Search based software engineering, Software product lines}
}

@inproceedings{10.1145/2556624.2556643,
author = {Fenske, Wolfram and Thüm, Thomas and Saake, Gunter},
title = {A taxonomy of software product line reengineering},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556643},
doi = {10.1145/2556624.2556643},
abstract = {In the context of single software systems, refactoring is commonly accepted to be the process of restructuring an existing body of code in order to improve its internal structure without changing its external behavior. This process is vital to the maintenance and evolution of software systems.Software product line engineering is a paradigm for the construction and customization of large-scale software systems. As systems grow in complexity and size, maintaining a clean structure becomes arguably more important. However, product line literature uses the term "refactoring" for such a wide range of reengineering activities that it has become difficult to see how these activities pertain to maintenance and evolution and how they are related.We improve this situation in the following way: i) We identify the dimensions along which product line reengineering occurs. ii) We derive a taxonomy that distinguishes and relates these reengineering activities. iii) We propose definitions for the three main branches of this taxonomy. iv) We classify a corpus of existing work.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {4},
numpages = {8},
keywords = {reengineering, refactoring, software product lines, taxonomy},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Coverage, Debugging/fault localization, EXAM score, Genetic algorithm, Software product line, Test generation}
}

@article{10.1007/s00165-013-0276-5,
author = {Sampath, Prahladavaradan},
title = {An elementary theory of product-line variations},
year = {2014},
issue_date = {Jul 2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {4},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-013-0276-5},
doi = {10.1007/s00165-013-0276-5},
abstract = {The primary aim of a software product-line is to maximise reuse of software components by managing the variability in component functionalities and product configurations. Feature oriented domain analysis (FODA) diagrams are a formalism for modelling the variability in a software product-line, and are used as a tool for managing a product-line and planning its evolution. This paper presents an elementary theory of variations in a product-line, leading up to a technique for extracting FODA diagrams from legacy product-lines. The theory is elementary in the sense that it is built using very simple mathematical structures, making minimal assumptions on the structure of product-lines. Examples drawn from the automotive domain are used to illustrate the theoretical developments.},
journal = {Form. Asp. Comput.},
month = jul,
pages = {695–727},
numpages = {33},
keywords = {SPLE, FODA, Formal concept analysis, Lattice theory}
}

@article{10.1287/mksc.1120.0736,
author = {Guo, Liang and Zhang, Juanjuan},
title = {Consumer Deliberation and Product Line Design},
year = {2012},
issue_date = {11-12 2012},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {31},
number = {6},
issn = {1526-548X},
url = {https://doi.org/10.1287/mksc.1120.0736},
doi = {10.1287/mksc.1120.0736},
abstract = {This paper studies optimal product line design when consumers need to incur costly deliberation to uncover their valuations for quality. To induce deliberation, a firm must maintain quality dispersion and cut the price of the high-end product so that consumers are motivated to deliberate in the hope that high-end consumption fits their needs. To prevent deliberation, the firm may have to offer downgraded quality at a low price so that an impulsive purchase will not appear too wasteful. Whether the firm should induce deliberation depends on how much surplus it creates by aligning the supply of quality with heterogeneous demand for quality and how much surplus it captures during this process. Interestingly, equilibrium firm profit, consumer surplus, and social welfare can all increase with the cost of deliberation. We extend the model to accommodate consumers' heterogeneous prior beliefs of their valuations for quality. We also discuss how market research could benefit from taking into account the endogeneity of consumer deliberation.},
journal = {Marketing Science},
month = nov,
pages = {995–1007},
numpages = {13},
keywords = {agency theory, consumer deliberation, information acquisition, preference construction, price discrimination, product line design}
}

@article{10.1287/mnsc.2013.1789,
author = {Xiong, Hui and Chen, Ying-Ju},
title = {Product Line Design with Seller-Induced Learning},
year = {2014},
issue_date = {March 2014},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {60},
number = {3},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2013.1789},
doi = {10.1287/mnsc.2013.1789},
abstract = {In practice, some well-established service providers sellers offer one-time experiences or product demonstrations for the services that have been introduced to the market for years. Such activities, labeled as seller-induced learning, not only help the consumers learn more about themselves but also exploit the consumers by elaborating on the consumer heterogeneity. When the seller-induced learning completely resolves the consumers' valuation uncertainty, it can facilitate a more sophisticated price discrimination scheme and may give rise to a relatively more efficient allocation. Nevertheless, if there is residual valuation uncertainty, the seller may abandon the seller-induced learning to avoid the exacerbated ex post cannibalization. We show that an exploding offer shall sometimes be offered in conjunction with the seller-induced learning to encourage immediate purchases when uncertainty arises in only some consumers. We identify regimes under which the seller-induced learning is charged at a strictly positive price. Under these regimes, the seller need not sacrifice the ex post efficiency upon inducing consumer learning. Therefore, our result indicates that the seller-induced learning may eliminate the conflict between rent extraction and efficiency initiatives. However, quality distortion prevails when the seller provides an identical menu for all the consumers or the free seller-induced learning. This paper was accepted by J. Miguel Villas-Boas, marketing.},
journal = {Manage. Sci.},
month = mar,
pages = {784–795},
numpages = {12},
keywords = {consumer uncertainty, dynamic mechanism design, product line design, seller-induced learning}
}

@inproceedings{10.1145/2816839.2816850,
author = {Lahiani, Nesrine and Bennouar, Djamal},
title = {A Model Driven Approach to Derive e-Learning Applications in Software Product Line},
year = {2015},
isbn = {9781450334587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2816839.2816850},
doi = {10.1145/2816839.2816850},
abstract = {Platforms such as Moodle aims to ease and improve the teaching-learning process by means of taking advantage of internet technologies. All existing e-learning platforms are pretty similar the concepts of activity, assignment, deliverable or grade. But also a wide range of differences among them exists. Software Product Line (SPL) has as goal the effective production of similar software systems.. Product derivation represents a fundamental aspect in SPL. It is also the main challenge that SPL faces. Despite its importance, there is only a little research on product derivation compared to the large work on developing product lines. In addition, the few available research reports guidance about how to derive a product from a product line. In this paper we describe a combination of SPL and MDA which both fit perfectly together in order to build applications in cost effective way. We proposed an approach for product derivation that adopts MDA with its organized layers of models to achieve SPL goals.},
booktitle = {Proceedings of the International Conference on Intelligent Information Processing, Security and Advanced Communication},
articleno = {78},
numpages = {6},
keywords = {Model Driven Architecture, Product Derivation, Software Product Line, e-learning},
location = {Batna, Algeria},
series = {IPAC '15}
}

@inproceedings{10.1145/3467707.3467773,
author = {Bai, Zhaoyang and Wang, Yamin and Zhai, Xiang and Ran, Lilin},
title = {Product Family Design BOM Model Based on the Cost Perspective},
year = {2021},
isbn = {9781450389501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3467707.3467773},
doi = {10.1145/3467707.3467773},
abstract = {The rationality of product family design BOM is related to production efficiency and cost. Aiming at the product family design BOM, with a certain degree of customization, aiming at minimizing the sum of design cost, conversion cost and parts cost, the BOM model of product family design is established, and the particle swarm optimization algorithm is used to solve the model, and the design scheme of product family design BOM is obtained. Finally, taking the manufacturer of small banknote sorter as an example, it is verified that this method can effectively reduce the data redundancy in the conventional product configuration and improve the management efficiency.},
booktitle = {Proceedings of the 2021 7th International Conference on Computing and Artificial Intelligence},
pages = {437–445},
numpages = {9},
keywords = {Cost, Design BOM, Particle Swarm Optimization, Product Family},
location = {Tianjin, China},
series = {ICCAI '21}
}

@inproceedings{10.1145/2695664.2695797,
author = {Tizzei, Leonardo P. and Azevedo, Leonardo G. and de Bayser, Maximilien and Cerqueira, Renato F. G.},
title = {Architecting cloud tools using software product line techniques: an exploratory study},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695797},
doi = {10.1145/2695664.2695797},
abstract = {Multitenant cloud computing tools are usually complex and have to manage variabilities to support customization. Software Product Line (SPL) techniques have been successfully applied in the industry to manage variability in complex systems. However, few works in the literature discuss the application of SPL techniques to architect industry cloud computing tools, resulting in a lack of support to cloud architects on how to apply such techniques. This work presents how software product line techniques can be applied for architecting cloud tools, and discusses the benefits, drawbacks, and some challenges of applying such techniques to develop a real industry cloud tool, named as Installation Service.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1441–1448},
numpages = {8},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1007/s10009-015-0386-x,
author = {Gnesi, Stefania and Jarzabek, Stan},
title = {Special section on the 17th International Software Product Line Conference},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-015-0386-x},
doi = {10.1007/s10009-015-0386-x},
abstract = {Today, companies develop, maintain and deploy families of similar software products (e.g., games for different models of smartphones) rather than a single product. Software product lines engineering refers to software engineering methods, tools and techniques for creating a collection of similar software systems from a shared set of software assets using a common means of production. Software Product Line Conferences started in 1996, as the premier forum for practitioners, researchers and educators to present and discuss the most recent ideas, innovations, trends, experiences, and concerns in the area of software product lines, software product family engineering and, more recently, systems family engineering, managing families of software products as a whole rather than each family member individually. This special section stems from the 17th SPL Conference held in Tokyo, Japan, in August 2013. The contributions to this special section are further elaborations of the papers presented at the conference.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {555–557},
numpages = {3},
keywords = {Software Engineering, Software Product Lines}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, Márcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/1629716.1629737,
author = {Botterweck, Goetz and Pleuss, Andreas and Polzer, Andreas and Kowalewski, Stefan},
title = {Towards feature-driven planning of product-line evolution},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629737},
doi = {10.1145/1629716.1629737},
abstract = {Industries that successfully apply product line approaches often operate in markets that are well established and have a strategic perspective. Consequently, such organizations have a tendency towards long-term planning of products and product lines. Although there are numerous approaches for efficient product line engineering, there is surprisingly little support for a long-term, strategic perspective and an evolution of product lines. To address these challenges, we aim to integrate evolution into model-driven product line engineering. In particular, we explore how feature models can be applied to describe the evolution of product lines. The paper contributes (i) concepts for describing the evolution of product lines with feature models, (ii) a corresponding framework, which puts this into a bigger context and (iii) three scenarios that show how this framework can be applied. The concepts are motivated with examples from automotive software engineering and embedded systems, which are industries with a strong affinity to product lines, where long term planning of the product portfolio are common strategies.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {109–116},
numpages = {8},
keywords = {evolution, feature modeling, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.jss.2016.01.039,
author = {Mariani, Thainá and Elita Colanzi, Thelma and Regina Vergilio, Silvia},
title = {Preserving architectural styles in the search based design of software product line architectures},
year = {2016},
issue_date = {May 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {115},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.01.039},
doi = {10.1016/j.jss.2016.01.039},
abstract = {This paper presents search operators for layered and client/server architectures.The goal is to preserve the style of a PLA in a search based optimization approach.A representation for layered and client/server PLAs in a class diagram is presented.Rules for each style based on their representation are defined.The operators preserved the styles and contributed to improve quantitative results. Architectural styles help to improve the Product Line Architecture (PLA) design by providing a better organization of its elements, which results in some benefits, like flexibility, extensibility and maintainability. The PLA design can also be improved by using a search based optimization approach, taking into account different metrics, such as cohesion, coupling and feature modularization. However, the application of search operators changes the PLA organization, and consequently may violate the architectural styles rules, impacting negatively in the architecture understanding. To overcome such limitation, this work introduces a set of search operators to be used in the search based design with the goal of preserving the architectural styles during the optimization process. Such operators consider rules of the layered and client/server architectural styles, generally used in the search based design of conventional architectures and PLAs. The operators are implemented and evaluated in the context of MOA4PLA, a Multi-objective Optimization Approach for PLA Design. Results from an empirical evaluation show that the proposed operators contribute to obtain better solutions, preserving the adopted style and also improving some software metric values.},
journal = {J. Syst. Softw.},
month = may,
pages = {157–173},
numpages = {17},
keywords = {Architectural style, Search based design, Software product line}
}

@article{10.1007/s10270-015-0471-3,
author = {Bonifácio, Rodrigo and Borba, Paulo and Ferraz, Cristiano and Accioly, Paola},
title = {Empirical assessment of two approaches for specifying software product line use case scenarios},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0471-3},
doi = {10.1007/s10270-015-0471-3},
abstract = {Modularity benefits, including the independent maintenance and comprehension of individual modules, have been widely advocated. However, empirical assessments to investigate those benefits have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). In this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the benefits of modularity in the context of two approaches for specifying product line use case scenarios: PLUSS and MSVCM. The first uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario specifications. After evaluating these approaches through the specifications of several systems, we find out that MSVCM reduces feature scattering and improves scenario cohesion. These results suggest that evolving a product line specification using MSVCM requires only localized changes. On the other hand, the results of six experiments reveal that MSVCM requires more time to derive the product line specifications and, contrasting with the modularity results, reduces the time to evolve a product line specification only when the subjects have been well trained and are used to the task of evolving product line specifications.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {97–123},
numpages = {27},
keywords = {Experimentation in software engineering, Requirements engineering, Software modularity, Software product lines, Usage scenarios}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Delta Modeling, Software Evolution, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.5555/3106050.3106053,
author = {Hänsel, Joachim and Giese, Holger},
title = {Towards collective online and offline testing for dynamic software product line systems},
year = {2017},
isbn = {9781538628034},
publisher = {IEEE Press},
abstract = {Dynamic Software Product Line (DSPLs) based Systems are capable of adapting in response to changes concerning their observations at runtime in order to exhibit appropriate behavior. The observation space and the variability in the configuration space is usually known at design time. However, running a set of tests with all combinations of configuration and observation from these spaces is likely to be infeasible. We propose to make use of monitoring results from multiple instances of systems derived from a DSPL at runtime collecting their observations and the employed configurations. The collective of systems is enabled to profit from an operational profile with regard to proper coverage by systematic tests. The systematic tests are carried out offline. Additional online testing further improves the confidence in the system.},
booktitle = {Proceedings of the 2nd International Workshop on Variability and Complexity in Software Design},
pages = {9–12},
numpages = {4},
keywords = {dynamic software product lines, offline testing, online testing, self-adaptive software systems},
location = {Buenos Aires, Argentina},
series = {VACE '17}
}

@inproceedings{10.1145/3461002.3473948,
author = {Xu, Hao and Baarir, Souheib and Ziadi, Tewfik and Hillah, Lom Messan and Essodaigui, Siham and Bossu, Yves},
title = {Optimisation for the product configuration system of Renault: towards an integration of symmetries},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473948},
doi = {10.1145/3461002.3473948},
abstract = {The problem of configuring model variability is widespread in many different domains. Renault, a leading french automobile manufacturer, has developed its technology internally to model vehicle diversity. This technology relies on the approach known as knowledge compilation. Since its inception, continuous progress has been made in the tool while monitoring the latest developments from the software field and academia. However, the growing number of vehicle models brings potential risks and higher requirements for the tool. This paper presents a short reminder of Renault's technology principles and the improvements we intend to achieve by analyzing and leveraging notable data features of Renault problem instances. In particular, the aim is to exploit symmetry properties.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {86–90},
numpages = {5},
keywords = {SAT, knowledge compilation, product line, symmetries},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Steghöfer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Variability management, Software product lines, Multiple-case study, Challenges}
}

@inproceedings{10.1007/978-3-642-33666-9_46,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel and Walawege, Suneth},
title = {A product line modeling and configuration methodology to support model-based testing: an industrial case study},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_46},
doi = {10.1007/978-3-642-33666-9_46},
abstract = {Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behavioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {726–742},
numpages = {17},
keywords = {UML state machine, aspect-oriented modeling, behavioral variability, model-based testing, product line engineering},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.5555/1885639.1885657,
author = {Elsner, Christoph and Ulbrich, Peter and Lohmann, Daniel and Schröder-Preikschat, Wolfgang},
title = {Consistent product line configuration across file type and product line boundaries},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Creating a valid software configuration of a product line can require laborious customizations involving multiple configuration file types, such as feature models, domain-specific languages, or preprocessor defines in C header files. Using configurable off-the-shelf components causes additional complexity. Without checking of constraints across file types boundaries already at configuration time, intricate inconsistencies are likely to be introduced--resulting in product defects, which are costly to discover and resolve later on.Up to now, at best ad-hoc solutions have been applied. To tackle this problem in a general way, we have developed an approach and a corresponding plug-in infrastructure. It allows for convenient definition and checking of constraints across configuration file types and product line boundaries. Internally, all configuration files are converted to models, facilitating the use of model-based constraint languages (e.g., OCL). Converter plug-ins for arbitrary configuration file types may be integrated and hide a large amount of complexity usually associated with modeling. We have validated our approach using a quadrotor helicopter product line comprising three sub-product-lines and four different configuration file formats. The results give evidence that our approach is practically applicable, reduces time and effort for product derivation (by avoiding repeated compiling, testing, and reconfiguration cycles), and prevents faulty software deployment.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {181–195},
numpages = {15},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.infsof.2012.09.007,
author = {Guana, Victor and Correal, Dario},
title = {Improving software product line configuration: A quality attribute-driven approach},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.09.007},
doi = {10.1016/j.infsof.2012.09.007},
abstract = {Context: During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product's intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task. Objective: Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets. Method: We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product's functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years. Results: Through our SPL experiment, we established that our approach improved in nearly 98% the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools. Conclusion: We can conclude that our domain-specific modeling approach significantly improves the software architect's decision making when selecting the most suitable combinations of reusable components in the context of a SPL.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {541–562},
numpages = {22},
keywords = {Domain specific modeling, Model driven - software product lines, Quality evaluation, Sensitivity points, Software architecture, Variability management}
}

@inproceedings{10.5555/2662572.2662582,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {Representation of software product line architectures for search-based design},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {The Product-Line Architecture (PLA) is the main artifact of a Software Product Line (SPL). Search-based approaches can provide automated discovery of near-optimal PLAs and make its design less dependent on human architects. To do this, it is necessary to adopt a suitable PLA representation to apply the search operators. In this sense, we review existing architecture representations proposed by related work, but all of them need to be extended to encompass specific characteristics of SPL. Then, the use of such representations for PLA is discussed and, based on the performed analysis, we introduce a novel direct PLA representation for search-based optimization. Some implementation aspects are discussed involving implementation details about the proposed PLA representation, constraints and impact on specific search operators. Ongoing work addresses the application of specific search operators for the proposed representation and the definition of a fitness function to be applied in a multi-objective search-based approach for the PLA design.},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {28–33},
numpages = {6},
keywords = {architecture modelling, multi-objective search-based approach, software product line},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@article{10.4018/ijkss.2014100104,
author = {Ripon, Shamim H and Hossain, Sk. Jahir and Piash, Moshiur Mahamud},
title = {Logic-Based Analysis and Verification of Software Product Line Variant Requirement Model},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100104},
doi = {10.4018/ijkss.2014100104},
abstract = {Software Product Line SPL provides the facility to systematically reuse of software improving the efficiency of software development regarding time, cost and quality. The main idea of SPL is to identify the common core functionality that can be implemented once and reused afterwards. A variant model has also to be developed to manage the variants of the SPL. Usually, a domain model consisting of the common and variant requirements is developed during domain engineering phase to alleviate the reuse opportunity. The authors present a product line model comprising of a variant part for the management of variant and a decision table to depict the customization of decision regarding each variant. Feature diagrams are widely used to model SPL variants. Both feature diagram and our variant model, which is based on tabular method, lacks logically sound formal representation and hence, not amenable to formal verification. Formal representation and verification of SPL has gained much interest in recent years. This chapter presents a logical representation of the variant model by using first order logic. With this representation, the table based variant model as well as the graphical feature diagram can now be verified logically. Besides applying first-order-logic to model the features, the authors also present an approach to model and analyze SPL model by using semantic web approach using OWL-DL. The OWL-DL representation also facilitates the search and maintenance of feature models and support knowledge sharing within a reusable engineering context. Reasoning tools are used to verify the consistency of the feature configuration for both logic-based and semantic web-based approaches.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {52–76},
numpages = {25},
keywords = {Domain Model, Feature Diagrams, OWL-DL, Software Product Line SPL, Web-Based Approaches}
}

@inproceedings{10.1007/11767718_28,
author = {Chang, Soo Ho and Kim, Soo Dong and Rhew, Sung Yul},
title = {A variability-centric approach to instantiating core assets in product line engineering},
year = {2006},
isbn = {3540346821},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11767718_28},
doi = {10.1007/11767718_28},
abstract = {As a key activity in product line engineering (PLE), instantiation is a task to generate target applications by resolving variability embedded in core assets. However, instantiation is often conducted in manual and ad-hoc fashion, largely replying on domain knowledge and experience. Hence, it can easily lead to technical problems in precisely specifying decision model consisting of product-specific variation points and variants, and in handling inter-variant conflicts/dependency. To overcome this difficulty, it is desirable to develop a systematic process which includes a set of systematic activities, detailed instructions, and concrete specification of artifacts. In this paper, we first propose a meta-model of a core asset to specify its key elements. Then, we represent a comprehensive process that defines key instantiation activities, representations of artifacts, and work instructions. With the proposed process, one can instantiate core assets more effectively and systematically.},
booktitle = {Proceedings of the 7th International Conference on Product-Focused Software Process Improvement},
pages = {334–347},
numpages = {14},
location = {Amsterdam, The Netherlands},
series = {PROFES'06}
}

@inproceedings{10.1109/ASE.2013.6693104,
author = {Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
title = {Scalable product line configuration: a straw to break the camel's back},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693104},
doi = {10.1109/ASE.2013.6693104},
abstract = {Software product lines are hard to configure. Techniques that work for medium sized product lines fail for much larger product lines such as the Linux kernel with 6000+ features. This paper presents simple heuristics that help the Indicator-Based Evolutionary Algorithm (IBEA) in finding sound and optimum configurations of very large variability models in the presence of competing objectives. We employ a combination of static and evolutionary learning of model structure, in addition to utilizing a pre-computed solution used as a "seed" in the midst of a randomly-generated initial population. The seed solution works like a single straw that is enough to break the camel's back -given that it is a feature-rich seed. We show promising results where we can find 30 sound solutions for configuring upward of 6000 features within 30 minutes.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {465–474},
numpages = {10},
keywords = {SMT solvers, automated configuration, evolutionary algorithms, multiobjective optimization, variability models},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1145/1842752.1842773,
author = {McGregor, John D.},
title = {A method for analyzing software product line ecosystems},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842773},
doi = {10.1145/1842752.1842773},
abstract = {The ecosystem for a software product line includes all of the entities with which the software product line organization interacts. Information, artifacts, customers, money and products move among these entities as a part of the planning, development, and deployment processes. In this paper we present an analysis technique that uses the economic notion of a transaction to examine the transfers between the entities. The result of the analysis is data that is used to evaluate and structure the organization. We illustrate with an example.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {73–80},
numpages = {8},
keywords = {software ecosystem, software product line},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@article{10.1145/2180921.2180941,
author = {Ripon, Shamim H.},
title = {A unified tabular method for modeling variants of software product line},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/2180921.2180941},
doi = {10.1145/2180921.2180941},
abstract = {Reuse of software is a promising approach to improving the efficiency of software development regarding time, cost and quality. Reuse requires a systematic approach. The best results are achieved if we focus on systems in a specific domain, so-called product line. The key difference between the conventional software engineering and software product line engineering is variant management. The main idea of software product line is to identify the common core functionality which can be implemented once and reused afterwards for all members of the product line. To facilitate this reuse opportunity the domain engineering phase makes the domain model comprising the common as well as variant requirements. In principle, common requirements among systems in a family are easy to handle. However, problem arises during handling variants. Different variants have dependencies on each other; a single variant can affect several variants of the domain model. These problems become complex when the volume of information grows in a domain and there are a lot of variants with several interdependencies. Hence, a separate model is required for handling the variants. This paper presents a mechanism, which we call, Unified Tabular Method to facilitate the management of variant dependencies in product lines. The tabular method consists of a variant part to model the variants and their dependencies, and a decision table to depict the customization decision regarding each variant while deriving customized products. Tabular method alleviates the problem of possible explosion of variant combinations and facilitates the tracing of variant information in the domain model},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {1–7},
numpages = {7},
keywords = {modeling variants, software product line, unified tabular method}
}

@inproceedings{10.5555/1860875.1860879,
author = {Elsner, Christoph and Schwanninger, Christa and Schröder-Preikschat, Wolgang and Lohmann, Daniel},
title = {Multi-Level Product Line Customization},
year = {2010},
isbn = {9781607506287},
publisher = {IOS Press},
address = {NLD},
abstract = {Managing and developing a set of software products jointly using a software product line approach has achieved significant productivity and quality gain in the last decade. More and more, product lines now are becoming themselves entities that are sold and bought in the software supply chain. Customers build more specialized product lines on top of them or derive themselves the concrete products. As customers have different requirements, whole product lines now may vary depending on customer needs---they need to be customized. Current approaches going beyond the scope of one product line do not provide appropriate means for customization. They either are tailored to specific implementation techniques, only regard customization on few levels (e.g., only source code level), or imply a lot of manual effort for performing the customization.The PLiC Approach tackles this challenge by providing a generic, reusable reference architecture and methodology for implementing such customizable product lines. In the reference architecture, a product line consists of so-called product line components (PLiCs), which are flexibly recombinable slices of a formerly monolithic product line, thereby maintaining strict separation of concerns. The approach furthermore comprises a tool-supported methodology for recombination of PLiCs based on customer needs and thus minimizes manual intervention when customizing. We implemented the PLiC Approach for a complex model-driven product line, where it facilitates comprehensive customization on various levels in the models, the model transformation chain, and in the source code with reasonable effort. This gives evidence that our approach can be applied in various other contexts where the same or fewer customization levels need to be considered.},
booktitle = {Proceedings of the 2010 Conference on New Trends in Software Methodologies, Tools and Techniques: Proceedings of the 9th SoMeT_10},
pages = {37–58},
numpages = {22}
}

@article{10.1504/ijcsm.2021.120688,
author = {Cheng, Xianfu and You, Minhua and Ma, Xiaotian},
title = {Bi-level optimisation model of modular product family with adaptability consideration},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {14},
number = {4},
issn = {1752-5055},
url = {https://doi.org/10.1504/ijcsm.2021.120688},
doi = {10.1504/ijcsm.2021.120688},
abstract = {Considering the adaptability of product platform in product family optimisation, the product module can be divided into common module, adaptable module and customisation module. Some of the design parameters in adaptable module have bi-level relationship with leader-follower characteristics. The objective function of the upper-level is to optimise the product family design, and that of the lower-level is to optimise the single product design. The bi-level optimisation algorithm is designed, the genetic algorithm is designed to solve the upper-level model and the Fmincon function is designed to solve the lower-level model. For the overall optimisation of modular product family with bi-level relationship, the upper-level module is to reflect the universality and adaptability of the product, and the lower-level module is more to meet the customisation needs of customers. The upper-level and lower-level models can be solved by genetic algorithm. Finally, the feasibility of the method is verified by the example of the hoisting system.},
journal = {Int. J. Comput. Sci. Math.},
month = jan,
pages = {357–368},
numpages = {11},
keywords = {modular design, product family, adaptable design, bi-level optimisation, parametric design, genetic algorithm, hoisting system, adaptability}
}

@article{10.1016/j.infsof.2012.11.008,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Goseva-Popstojanova, Katerina and Dorman, Karin S.},
title = {Predicting failure-proneness in an evolving software product line},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.008},
doi = {10.1016/j.infsof.2012.11.008},
abstract = {ContextPrevious work by researchers on 3years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. ObjectiveThe work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. MethodThis investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. ResultsOur experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. ConclusionAs the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1479–1495},
numpages = {17},
keywords = {Change metrics, Failure-prone files, Post-release defects, Prediction, Reuse, Software product lines}
}

@inproceedings{10.1145/3267183.3267188,
author = {Barbosa, Jefferson and Andrade, Rossana M. C. and Filho, João Bosco F. and Bezerra, Carla I. M. and Barreto, Isaac and Capilla, Rafael},
title = {Cloning in Customization Classes: A Case of a Worldwide Software Product Line},
year = {2018},
isbn = {9781450365543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267183.3267188},
doi = {10.1145/3267183.3267188},
abstract = {Cloning-and-owning, in the long run, can severely affect evolution, as changes in cloned fragments may require modifications in various parts of the system. This problem scales if cloning is used in classes that derive products in a Software Product Line, because these classes can impact in several features and products. However, it is hard to know to which extent cloning in customization classes can impact in a project. We conduct a study, within an SPL that generates mobile software for over 150 countries, to analyze cloning practices and how cloned parts relate to the maintainability of customization classes. We collect and identify clones inside customization classes during a period of 13 months, involving 70 customization classes and 5 branches. In parallel, we collect the respective issues from the issue tracking tool of the SPL project, obtaining over 140 issues related to customization classes. We then confront the time spent to solve each issue with its nature (i.e., if it relates to cloned code or not). As first result, we verify that issues related to cloning take in average 136% more time to be solved. Our study helps to understand how cloning relates to maintainability in the context of mass customization, giving insights about cloned code evolution and its impacts in a software product line project.},
booktitle = {Proceedings of the VII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {43–52},
numpages = {10},
keywords = {Clone, Customization, Software Product Line},
location = {Sao Carlos, Brazil},
series = {SBCARS '18}
}

@article{10.5555/2747015.2747184,
author = {da Silva, Ivonei Freitas and da Mota Silveira Neto, Paulo Anselmo and O'Leary, Pádraig and de Almeida, Eduardo Santana and Meira, Silvio Romero de Lemos},
title = {Software product line scoping and requirements engineering in a small and medium-sized enterprise},
year = {2014},
issue_date = {February 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0164-1212},
abstract = {HighlightsWe described a detailed qualitative study on software product line scoping and requirements engineering.We examine weaknesses regarding the iterativeness, adaptability, and communication.Agile methods can mitigate the iterativeness, adaptability, and communication weaknesses. Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods.},
journal = {J. Syst. Softw.},
month = feb,
pages = {189–206},
numpages = {18},
keywords = {Agile methods, Requirements engineering, Software product line scoping}
}

@inproceedings{10.1145/3461001.3472729,
author = {Abbas, Muhammad and Saadatmand, Mehrdad and Enoiu, Eduard Paul},
title = {Requirements-driven reuse recommendation},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3472729},
doi = {10.1145/3461001.3472729},
abstract = {This tutorial explores requirements-based reuse recommendation for product line assets in the context of clone-and-own product lines.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {210},
numpages = {1},
keywords = {SPL adoption, similarity, software reuse},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1007/s10515-019-00266-2,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong},
title = {Using multi-objective search and machine learning to infer rules constraining product configurations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00266-2},
doi = {10.1007/s10515-019-00266-2},
abstract = {Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.},
journal = {Automated Software Engg.},
month = jun,
pages = {1–62},
numpages = {62},
keywords = {Product line, Configuration, Rule mining, Multi-objective search, Machine learning, Interacting products}
}

@article{10.1007/s10845-020-01572-3,
author = {Gauss, Leandro and Lacerda, Daniel P. and Cauchick Miguel, Paulo A.},
title = {Module-based product family design: systematic literature review and meta-synthesis},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {1},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-020-01572-3},
doi = {10.1007/s10845-020-01572-3},
abstract = {Increased demand for a greater variety of products has forced many companies to rethink their strategies to offer more product variants without sacrificing production efficiency. In this context, research has found that such a trade-off can be properly managed by exploiting the module-based product family (MBPF) design. Over the years, active work in developing methods to design MBPFs has been carried out. Nevertheless, many of them have been created, and consequently exist, in isolation from one other. As a result, the adoption of these methods in industry and academy alike is inhibited by the seemingly broad array of material without a coherent organizing structure. To bridge this gap, this paper performs a systematic literature review and a meta-synthesis, wherein 72 methods to design MBPFs and their respective instances are connected in the form of a functional model and structured classes of design problems. These entities together serve as a meta-method for organizing the research on MBPF design, from which it was possible to identify the common underlying structure among the methods developed over the past 20&nbsp;years. The main contributions of this work include: (1) constructing a functional model that connects the design methods for MBPFs; (2) suggesting structured classes of design problems that complement the functional model by cataloging the techniques meant to execute each sub-function of the model; (3) proposing a construction heuristic to build and assess functional models and classes of design problems.},
journal = {J. Intell. Manuf.},
month = jan,
pages = {265–312},
numpages = {48},
keywords = {Modularity, Product family design, Systematic literature review, Meta-synthesis, Functional model, Design science}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1007/978-3-642-33176-3_7,
author = {ter Beek, Maurice H. and Muccini, Henry and Pelliccione, Patrizio},
title = {Assume-guarantee testing of evolving software product line architectures},
year = {2012},
isbn = {9783642331756},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33176-3_7},
doi = {10.1007/978-3-642-33176-3_7},
abstract = {Despite some work on testing software product lines, maintaining the quality of products when a software product line evolves is still an open problem. In this paper, we propose a novel assume-guarantee testing approach as a solution to the following research question: how can we verify the correct functioning of products of an software product line when core components evolve? The underlying idea is to retest only some of the products that conform to the software product line architecture and to infer, using assume-guarantee reasoning, the correctness of the other products. Assume-guarantee reasoning moreover permits the retesting of only those components that are affected by the changes.},
booktitle = {Proceedings of the 4th International Conference on Software Engineering for Resilient Systems},
pages = {91–105},
numpages = {15},
keywords = {assume-guarantee testing, compositional verification, evolving software product lines, software testing},
location = {Pisa, Italy},
series = {SERENE'12}
}

@article{10.1016/j.infsof.2007.10.013,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {The software product line architecture: An empirical investigation of key process activities},
year = {2008},
issue_date = {October, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {50},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.10.013},
doi = {10.1016/j.infsof.2007.10.013},
abstract = {Software architecture has been a key area of concern in software industry due to its profound impact on the productivity and quality of software products. This is even more crucial in case of software product line, because it deals with the development of a line of products sharing common architecture and having controlled variability. The main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting a comprehensive empirical investigation covering a broad range of organizations currently involved in the business of software product lines. This is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge. The results of this investigation provide empirical evidence that software product line architecture process activities play a significant role in successfully developing and managing a software product line.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1098–1113},
numpages = {16},
keywords = {Domain engineering, Empirical study, Software architecture, Software engineering, Software product line}
}

@inproceedings{10.1007/978-3-642-37422-7_26,
author = {Adam, Sebastian and Schmid, Klaus},
title = {Effective requirements elicitation in product line application engineering: an experiment},
year = {2013},
isbn = {9783642374210},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-37422-7_26},
doi = {10.1007/978-3-642-37422-7_26},
abstract = {[Context &amp; Motivation] Developing new software systems based on a software product line (SPL) is still a time-consuming task and the benefits of using such an approach are often smaller than expected. One important reason for this are difficulties in systematically mapping customer requirements to characteristics of the SPL. [Question/problem] Even though it has been recognized that the success of reuse strongly depends on how requirements are treated, it remains unclear how to perform this in an optimal way. [Principal ideas/results] In this paper, we present a controlled experiment performed with 26 students that compared two requirements elicitation approaches when instantiating a given SPL. [Contribution] Our findings indicate that a novel, problem-oriented requirements approach that explicitly integrates the reuse of SPL requirements into the elicitation of customer-specific requirements is more effective than a traditional SPL requirements approach, which distinguishes requirements reuse and additional elicitation customer-specific requirements.},
booktitle = {Proceedings of the 19th International Conference on Requirements Engineering: Foundation for Software Quality},
pages = {362–378},
numpages = {17},
location = {Essen, Germany},
series = {REFSQ'13}
}

@inproceedings{10.5555/2022115.2022131,
author = {Wu, Yijian and Peng, Xin and Zhao, Wenyun},
title = {Architecture evolution in software product line: an industrial case study},
year = {2011},
isbn = {9783642213465},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A software product line (SPL) usually involves a shared set of core assets and a series of application products. To ensure consistency, the evolution of the core assets and all the application products should be coordinated and synchronized under a unified evolution process. Therefore, SPL evolution often involves cross-product propagation and synchronization besides application derivation based on core assets, presenting quite different characteristic from the evolution of individual software products. As software architectures, including the product line architecture (PLA) and application architectures, play a central role in SPL engineering and evolution, architecture-based evolution analysis is a natural way for analyzing and managing SPL evolution. In this paper, we explore common practices of architecture evolution and the rationale behind in industrial SPL development. To this end, we conduct a case study with Wingsoft examination system product line (WES-PL), an industrial product line with an evolution history of eight years and more than 10 application products. In the case study, we reviewed the evolution history of WES-PL architecture and analyzed several typical evolution cases. Based on the historical analysis, we identify some special problems in industrial SPL practice from the aspect of architecture evolution and summarize some useful experiences about SPL evolution decisions to complement classical SPL methodology. On the other hand, we also propose some possible improvements for the evolution management in WES-PL.},
booktitle = {Proceedings of the 12th International Conference on Top Productivity through Software Reuse},
pages = {135–150},
numpages = {16},
location = {Pohang, South Korea},
series = {ICSR'11}
}

@article{10.1016/j.infsof.2011.01.001,
author = {Peng, Xin and Yu, Yijun and Zhao, Wenyun},
title = {Analyzing evolution of variability in a software product line: From contexts and requirements to features},
year = {2011},
issue_date = {July, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.01.001},
doi = {10.1016/j.infsof.2011.01.001},
abstract = {Context: In the long run, features of a software product line (SPL) evolve with respect to changes in stakeholder requirements and system contexts. Neither domain engineering nor requirements engineering handles such co-evolution of requirements and contexts explicitly, making it especially hard to reason about the impact of co-changes in complex scenarios. Objective: In this paper, we propose a problem-oriented and value-based analysis method for variability evolution analysis. The method takes into account both kinds of changes (requirements and contexts) during the life of an evolving software product line. Method: The proposed method extends the core requirements engineering ontology with the notions to represent variability-intensive problem decomposition and evolution. On the basis of problemorientation, the analysis method identifies candidate changes, detects influenced features, and evaluates their contributions to the value of the SPL. Results and Conclusion: The process of applying the analysis method is illustrated using a concrete case study of an evolving enterprise software system, which has confirmed that tracing back to requirements and contextual changes is an effective way to understand the evolution of variability in the software product line.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {707–721},
numpages = {15},
keywords = {Context, Evolution, Feature, Requirements, Software product line, Variability}
}

@inproceedings{10.5555/2026187.2026224,
author = {Sharma, Sugam and Yang, Hen-I. and Wong, Johnny and Chang, Carl K},
title = {Wrenching: transient migration from commonality to variability in product line engineering of smart homes},
year = {2011},
isbn = {9783642215346},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Currently smart home are created and deployed on each user's specifications, which guarantees their complete satisfaction, but incurs a very high deployment cost and non-usable resources. Techniques from product line engineering provide a framework to promote resource reusability by identifying reusable common features. They help to reduce the cost but can compromise users' satisfaction, especially when they need special accommodations or have unique preferences. In this paper, we propose wrenching, a transient relaxation of common features into variability, so uncommon, ad hoc request can be satisfied. Based on wrenching, we further devise Smart Variability Model, that can accommodate situations where existing models are not applicable.},
booktitle = {Proceedings of the 9th International Conference on Toward Useful Services for Elderly and People with Disabilities: Smart Homes and Health Telematics},
pages = {230–235},
numpages = {6},
keywords = {commonality, customization, deployment, product line engineering, smart home, smart variability model, variability, wrenching},
location = {Montreal, Canada},
series = {ICOST'11}
}

@inproceedings{10.5555/3351736.3351778,
author = {Hajri, Ines and Goknil, Arda and Briand, Lionel C. and Stephany, Thierry},
title = {Applying product line use case modeling in an industrial automotive embedded system: lessons learned and a refined approach},
year = {2015},
isbn = {9781467369084},
publisher = {IEEE Press},
abstract = {In this paper, we propose, apply, and assess Product line Use case modeling Method (PUM), an approach that supports modeling variability at different levels of granularity in use cases and domain models. Our motivation is that, in many software development environments, use case modeling drives interactions among stakeholders and, therefore, use cases and domain models are common practice for requirements elicitation and analysis. In PUM, we integrate and adapt existing product line extensions for use cases and introduce some template extensions for use case specifications. Variability is captured in use case diagrams while it is reflected at a greater level of detail in use case specifications. Variability in domain concepts is captured in domain models. PUM is supported by a tool relying on Natural Language Processing (NLP). We applied PUM to an industrial automotive embedded system and report lessons learned and results from structured interviews with experienced engineers.},
booktitle = {Proceedings of the 18th International Conference on Model Driven Engineering Languages and Systems},
pages = {338–347},
numpages = {10},
location = {Ottawa, Ontario, Canada},
series = {MODELS '15}
}

@inproceedings{10.5555/1753235.1753247,
author = {Chen, Lianping and Ali Babar, Muhammad and Ali, Nour},
title = {Variability management in software product lines: a systematic review},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Variability Management (VM) in Software Product Line (SPL) is a key activity that usually affects the degree to which a SPL is successful. SPL community has spent huge amount of resources on developing various approaches to dealing with variability related challenges over the last decade. To provide an overview of different aspects of the proposed VM approaches, we carried out a systematic literature review of the papers reporting VM in SPL. This paper presents and discusses the findings from this systematic literature review. The results reveal the chronological backgrounds of various approaches over the history of VM research, and summarize the key issues that drove the evolution of different approaches. This study has also identified several gaps that need to be filled by future efforts in this line of research.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {81–90},
numpages = {10},
keywords = {software product lines, systematic reviews, variability management},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1007/s10515-011-0099-7,
author = {Bagheri, Ebrahim and Ensan, Faezeh and Gasevic, Dragan},
title = {Decision support for the software product line domain engineering lifecycle},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0099-7},
doi = {10.1007/s10515-011-0099-7},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of the target domain and through the development of comprehensive and variability-covering feature models. The feature models developed within the software product line development process need to cover the relevant features and aspects of the target domain. In other words, the feature models should be elaborate representations of the feature space of that domain. Given that feature models, i.e., software product line feature models, are developed mostly by domain analysts by sifting through domain documentation, corporate records and transcribed interviews, the process is a cumbersome and error-prone one. In this paper, we propose a decision support platform that assists domain analysts throughout the domain engineering lifecycle by: (1) automatically performing natural language processing tasks over domain documents and identifying important information for the domain analysts such as the features and integrity constraints that exist in the domain documents; (2) providing a collaboration platform around the domain documents such that multiple domain analysts can collaborate with each other during the process using a Wiki; (3) formulating semantic links between domain terminology with external widely used ontologies such as WordNet in order to disambiguate the terms used in domain documents; and (4) developing traceability links between the unstructured information available in the domain documents and their formal counterparts within the formal feature model representations. Results obtained from our controlled experimentations show that the decision support platform is effective in increasing the performance of the domain analysts during the domain engineering lifecycle in terms of both the coverage and accuracy measures.},
journal = {Automated Software Engg.},
month = sep,
pages = {335–377},
numpages = {43},
keywords = {Domain engineering, Feature models, NLP model inference, Software product lines}
}

@article{10.1287/mnsc.1090.1133,
author = {Amaldoss, Wilfred and Jain, Sanjay},
title = {Reference Groups and Product Line Decisions: An Experimental Investigation of Limited Editions and Product Proliferation},
year = {2010},
issue_date = {April 2010},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {56},
number = {4},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.1090.1133},
doi = {10.1287/mnsc.1090.1133},
abstract = {Some luxury goods manufacturers offer limited editions of their products, whereas some others market multiple product lines. Researchers have found that reference groups shape consumer evaluations of these product categories. Yet little empirical research has examined how reference groups affect the product line decisions of firms. Indeed, in a field setting it is quite a challenge to isolate reference group effects from contextual effects and correlated effects. In this paper, we propose a parsimonious model that allows us to study how reference groups influence firm behavior and that lends itself to experimental analysis. With the aid of the model, we investigate the behavior of consumers in a laboratory setting where we can focus on the reference group effects after controlling for the contextual and correlated effects. The experimental results show that in the presence of strong reference group effects, limited editions and multiple products can help improve firms' profits. Furthermore, the trends in the purchase decisions of our participants point to the possibility that they are capable of introspecting close to two steps of thinking at the outset of the game and then learning through reinforcement mechanisms.},
journal = {Manage. Sci.},
month = apr,
pages = {621–644},
numpages = {24},
keywords = {experimental economics, game theory, product line, reference groups}
}

@inproceedings{10.1145/3461001.3473059,
author = {Azanza, Maider and Montalvillo, Leticia and Díaz, Oscar},
title = {20 years of industrial experience at SPLC: a systematic mapping study},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473059},
doi = {10.1145/3461001.3473059},
abstract = {Software Product Lines (SPLs) have been around since the late 1970s and have established themselves as a way to deal with product variability. Tens of companies around the globe can pay testament to their advantages. Practitioners, however, have lamented the lack of data on other practitioners' experiences that would help them in the SPL journey. This work intends to analyze the application of SPLs in industry in the last 20 years. We departed from 194 industrial studies that were published at the Software Product Line Conference, the premier venue for SPL research. After the filtering process we selected 66 primary studies, from 43 different companies and 15 countries. The studies were classified to answer three research questions: (i) which contexts have SPLs been applied in?, (ii) what phenomena have been reported? and, (iii) what evidences have been collected in terms of obtained benefits, encountered issues and lessons learned? Regarding the context, SPLs have mainly been reported in USA and Germany (50%) and are used to develop embedded systems (76%). The most cited reason to adopt SPLs is the need to increase product variants (42.42%). As for the phenomena, the most reported problem area is adoption (39.39%). Last, as for evidences the most cited benefit is a cost reduction (53.03%), the issue is evolution (13.13%) and the learned lesson is that architecture is essential (24.24%). We believe the findings will be of interest to the community as a whole in quest to bridge the gap between industry and academia while balancing rigor, authenticity and relevance.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {172–183},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00014,
author = {Idowu, Samuel and Strüber, Daniel and Berger, Thorsten},
title = {Asset management in machine learning: a survey},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00014},
doi = {10.1109/ICSE-SEIP52600.2021.00014},
abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {51–60},
numpages = {10},
keywords = {SE4AI, asset management, machine learning},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@article{10.1016/j.jss.2013.12.038,
author = {Capilla, Rafael and Bosch, Jan and Trinidad, Pablo and Ruiz-Cortés, Antonio and Hinchey, Mike},
title = {An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.12.038},
doi = {10.1016/j.jss.2013.12.038},
abstract = {Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures.},
journal = {J. Syst. Softw.},
month = may,
pages = {3–23},
numpages = {21},
keywords = {Dynamic Software Product Lines, Dynamic variability, Feature models, Software architecture}
}

@inproceedings{10.1145/3307630.3342412,
author = {Ananieva, Sofia and Kehrer, Timo and Klare, Heiko and Koziolek, Anne and Lönn, Henrik and Ramesh, S. and Burger, Andreas and Taentzer, Gabriele and Westfechtel, Bernhard},
title = {Towards a Conceptual Model for Unifying Variability in Space and Time},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342412},
doi = {10.1145/3307630.3342412},
abstract = {Effectively managing variability in space and time is among the main challenges of developing and maintaining large-scale yet long-living software-intensive systems. Over the last decades, two large research fields, Software Configuration Management (SCM) and Software Product Line Engineering (SPLE), have focused on version management and the systematic handling of variability, respectively. However, neither research community has been successful in producing unified management techniques that are effective in practice, and both communities have developed largely independently of each other. As a step towards overcoming this unfortunate situation, in this paper, we report on ongoing work on conceiving a conceptual yet integrated model of SCM and SPLE concepts, originating from a recent Dagstuhl seminar on the unification of version and variant management. Our goal is to provide discussion grounds for a wider exploration of a unified methodology supporting software evolution in both space and time.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {44–48},
numpages = {5},
keywords = {product lines, revision management, variability, version control},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1287/mnsc.1090.0997,
author = {Boulding, William and Christen, Markus},
title = {Pioneering Plus a Broad Product Line Strategy: Higher Profits or Deeper Losses?},
year = {2009},
issue_date = {June 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {55},
number = {6},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.1090.0997},
doi = {10.1287/mnsc.1090.0997},
abstract = {Previous research suggests firms can build a market share advantage by preempting later entrants with a broad product line and expanding rapidly into related markets. Whether such a strategy leads to a pioneering profit advantage relative to followers also depends on its cost effects. In this paper, we examine when the market share advantage of a pioneering firm with a broad product line strategy translates into a profit advantage by examining the cost effects of this strategy. Using the profit impact of marketing strategies data and an estimation method that controls for various unobserved factors, we find significant differences between different industry settings. From these contrasting findings, we generate an emerging theoretical framework that we subject to empirical testing. We conjecture, and empirically verify, that creating a broad product line with a versioning strategy---creating variety from a standard product in anticipating customer demand---does not increase the pioneering cost disadvantage, and thus results in a pioneering profit advantage. On the other hand, with a tailoring strategy---creating variety by customizing a product to actual customer demand---a broad product line substantially increases the pioneering cost disadvantage, thereby making a preemption strategy counterproductive.},
journal = {Manage. Sci.},
month = jun,
pages = {958–967},
numpages = {10},
keywords = {IV estimation, business unit profitability, pioneering, preemption, product line strategy}
}

@article{10.1016/j.infsof.2007.10.010,
author = {Snook, Colin and Poppleton, Michael and Johnson, Ian},
title = {Rigorous engineering of product-line requirements: A case study in failure management},
year = {2008},
issue_date = {January, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {50},
number = {1–2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.10.010},
doi = {10.1016/j.infsof.2007.10.010},
abstract = {We consider the failure detection and management function for engine control systems as an application domain where product line engineering is indicated. The need to develop a generic requirement set - for subsequent system instantiation - is complicated by the addition of the high levels of verification demanded by this safety-critical domain, subject to avionics industry standards. We present our case study experience in this area as a candidate method for the engineering, validation and verification of generic requirements using domain engineering and Formal Methods techniques and tools. For a defined class of systems, the case study produces a generic requirement set in UML and an example system instance. Domain analysis and engineering produce a validated model which is integrated with the formal specification/verification method B by the use of our UML-B profile. The formal verification both of the generic requirement set, and of a simple system instance, is demonstrated using our U2B, ProB and prototype Requirements Manager tools. This work is a demonstrator for a tool-supported method which will be an output of EU project RODIN (This work is conducted in the setting of the EU funded Research Project: IST 511599 RODIN (Rigorous Open Development Environment for Complex Systems) http://rodin.cs.ncl.ac.uk/). The use of existing and prototype formal verification and support tools is discussed. The method, developed in application to this novel combination of product line, failure management and safety-critical engineering, is evaluated and considered to be applicable to a wide range of domains.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {112–129},
numpages = {18},
keywords = {Formal specification, Generic requirements, Product line, Refinement, Tools, UML-B, Verification}
}

@article{10.1007/s10845-017-1332-4,
author = {Du, Gang and Xia, Yi and Jiao, Roger J. and Liu, Xiaojie},
title = {Leader-follower joint optimization problems in product family design},
year = {2019},
issue_date = {Mar 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {3},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-017-1332-4},
doi = {10.1007/s10845-017-1332-4},
abstract = {Product family design (PFD) has been traditionally tackled as a single-level multi-objective optimization problem. This paper reveals a complex type of leader-follower joint optimization (LFJO) problems that are widely observed for PFD. Leader-follower decision making is inherent in product family optimization that involves multiple decision makers and encompasses different levels of decision hierarchy, in which many conflicting goals compete to arrive at equilibrium solutions. It is important for PFD to explicitly model such leader-follower decisions in line with a Stackelberg game. Consistent with multiple decision makers across different stages of the PFD process and multiple levels of the PFD decision hierarchy, this paper classifies the leader-follower decisions of PFD using a quartet grid, which serves as a reference model for conceptualization of diverse types of LFJO problems associated with PFD. Coinciding with the bilevel decision structure of game theoretic optimization, each LFJO problem formulation defined from the quartet grid can be quantitatively mapped to a bilevel programming mathematical model to be solved effectively by nested genetic algorithms. A case study of gear reducer PFD is presented to demonstrate the rational and potential of the LFJO quartet grid for dealing with game-theoretic optimization problems underpinning PFD decisions.},
journal = {J. Intell. Manuf.},
month = mar,
pages = {1387–1405},
numpages = {19},
keywords = {Bilevel programming, Game theoretic decision making, Leader-follower joint optimization, Product family design, Quartet grid}
}

@inproceedings{10.5555/2022115.2022133,
author = {Zhu, Jiayi and Peng, Xin and Jarzabek, Stan and Xing, Zhenchang and Xue, Yinxing and Zhao, Wenyun},
title = {Improving product line architecture design and customization by raising the level of variability modeling},
year = {2011},
isbn = {9783642213465},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product Line Architecture (PLA) plays a central role in software product line development. In order to support architecture-level variability modeling, most architecture description languages (ADLs) introduce architectural variation elements, such as optional component, connector and interface, which must be customized during product derivation. Variation elements are many, and design and customization of PLA at the level of individual variation elements are difficult and error-prone. We observed that developers usually perceive architecture variability from the perspective of variant features or variant design decisions that are mapped into groups of architecture variation elements. In the paper, we describe heuristics to identify configurations of variation elements that typically form such groups. We call them variation constructs. We developed an architecture variability management method and a tool that allow developers to work at the variation construct level rather than at the level of individual variation elements. We have applied and evaluated the proposed method in the development and maintenance of a medium-size financial product line. Our experience indicates that by raising variability modeling from variation element to construct level, architecture design and customizations become more intuitive. Not only does our method reduce the design and customization effort, but also better ensures consistent configuration of architectural variation elements, avoiding errors.},
booktitle = {Proceedings of the 12th International Conference on Top Productivity through Software Reuse},
pages = {151–166},
numpages = {16},
keywords = {ADL, architecture, software product line, variability},
location = {Pohang, South Korea},
series = {ICSR'11}
}

@inproceedings{10.1145/3336294.3336311,
author = {Tërnava, Xhevahire and Mortara, Johann and Collet, Philippe},
title = {Identifying and Visualizing Variability in Object-Oriented Variability-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336311},
doi = {10.1145/3336294.3336311},
abstract = {In many variability-intensive systems, variability is implemented in code units provided by a host language, such as classes or functions, which do not align well with the domain features. Annotating or creating an orthogonal decomposition of code in terms of features implies extra effort, as well as massive and cumbersome refactoring activities. In this paper, we introduce an approach for identifying and visualizing the variability implementation places within the main decomposition structure of object-oriented code assets in a single variability-rich system. First, we propose to use symmetry, as a common property of some main implementation techniques, such as inheritance or overloading, to identify uniformly these places. We study symmetry in different constructs (e.g., classes), techniques (e.g., subtyping, overloading) and design patterns (e.g., strategy, factory), and we also show how we can use such symmetries to find variation points with variants. We then report on the implementation and application of a toolchain, symfinder, which automatically identifies and visualizes places with symmetry. The publicly available application to several large open-source systems shows that symfinder can help in characterizing code bases that are variability-rich or not, as well as in discerning zones of interest w.r.t. variability.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {231–243},
numpages = {13},
keywords = {identifying software variability, object-oriented variability-rich systems, software product line engineering, tool support for understanding software variability, visualizing software variability},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Controlled experimentation, Feature model, Maintainability, Quality attributes, Software prediction model, Software product line, Structural complexity}
}

@inproceedings{10.5555/2486788.2487011,
author = {Gonzalez-Sanchez, Javier},
title = {Toward a software product line for affective-driven self-adaptive systems},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {One expected characteristic in modern systems is self-adaptation, the capability of monitoring and reacting to changes into the environment. A particular case of self-adaptation is affective-driven self-adaptation. Affective-driven self-adaptation is about having consciousness of user’s affects (emotions) and drive self-adaptation reacting to changes in those affects. Most of the previous work around self-adaptive systems deals with performance, resources, and error recovery as variables that trigger a system reaction. Moreover, most effort around affect recognition has been put towards offline analysis of affect, and to date only few applications exist that are able to infer user’s affect in real-time and trigger self-adaptation mechanisms. In response to this deficit, this work proposes a software product line approach to jump-start the development of affect-driven self-adaptive systems by offering the definition of a domain-specific architecture, a set of components (organized as a framework), and guidelines to tailor those components. Study cases with systems for learning and gaming will confirm the capability of the software product line to provide desired functionalities and qualities.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1381–1384},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1016/j.jss.2010.01.048,
author = {Lee, Jaejoon and Muthig, Dirk and Naab, Matthias},
title = {A feature-oriented approach for developing reusable product line assets of service-based systems},
year = {2010},
issue_date = {July, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.01.048},
doi = {10.1016/j.jss.2010.01.048},
abstract = {Service orientation (SO) is a relevant promising candidate for accommodating rapidly changing user needs and expectations. One of the goals of adopting SO is the improvement of reusability, however, the development of service-based system in practice has uncovered several challenging issues, such as how to identify reusable services, how to determine configurations of services that are relevant to users' current product configuration and context, and how to maintain service validity after configuration changes. In this paper, we propose a method that addresses these issues by adapting a feature-oriented product line engineering approach. The method is notable in that it guides developers to identify reusable services at the right level of granularity and to map users' context to relevant service configuration, and it also provides a means to check the validity of services at runtime in terms of invariants and pre/post-conditions of services. Moreover, we propose a heterogeneous style based architecture model for developing such systems.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1123–1136},
numpages = {14},
keywords = {Feature-oriented, Service-based systems, Software architecture, Software architecture styles, Software product line engineering}
}

@inproceedings{10.1145/3382026.3425767,
author = {Marchezan, Luciano and Carbonell, João and Rodrigues, Elder and Bernardino, Maicon and Basso, Fábio Paulo and Assunção, Wesley K. G.},
title = {Enhancing the Feature Retrieval Process with Scoping and Tool Support: PAxSPL_v2},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425767},
doi = {10.1145/3382026.3425767},
abstract = {Software Product Lines (SPLs) are commonly adopted with an extractive approach, by performing a reengineering process in legacy systems, when dealing with variability and reuse became challenging. As a starting activity of the process, the legacy systems are analyzed to retrieve, categorize, and group their features in terms of commonality and variability. Due to the importance of this feature retrieving, we proposed the Prepare, Assemble, and Execute framework for SPL reengineering (PAxSPL). PAxSPL aims at guiding users to customize the feature retrieval for their scenario. In an initial evaluation of the PAxSPL in a real-world scenario, we could observe the need for including scoping activities and implementing a tool to make the framework more adoptable in practice. In this paper, we describe how we performed these improvements. We performed the evolution of PAxSPL by including SPL scoping concepts and activities into our framework as well as developing a supporting tool. We also conducted a pilot study to evaluate how PAxSPL allows instantiating a scenario where the SPL reengineering were conducted. The results show that all artifacts, activities, and techniques from the scenario could be properly represented. However, we also identified a potential limitation during the assembly of techniques regarding parallel activities. The main contribution is PAxSPL_v2 that makes the framework more adherent to industries performing the reengineering of legacy systems into SPLs.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {29–36},
numpages = {8},
keywords = {automated support, software product lines, variability management},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@article{10.1016/j.jss.2007.10.031,
author = {Karam, Marcel and Dascalu, Sergiu and Safa, Haidar and Santina, Rami and Koteich, Zeina},
title = {A product-line architecture for web service-based visual composition of web applications},
year = {2008},
issue_date = {June, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {6},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.10.031},
doi = {10.1016/j.jss.2007.10.031},
abstract = {A web service-based web application (WSbWA) is a collection of web services or reusable proven software parts that can be discovered and invoked using standard Internet protocols. The use of these web services in the development process of WSbWAs can help overcome many problems of software use, deployment and evolution. Although the cost-effective software engineering of WSbWAs is potentially a very rewarding area, not much work has been done to accomplish short time to market conditions by viewing and dealing with WSbWAs as software products that can be derived from a common infrastructure and assets with a captured specific abstraction in the domain. Both Product Line Engineering (PLE) and Agile Methods (AMs), albeit with different philosophies, are software engineering approaches that can significantly shorten the time to market and increase the quality of products. Using the PLE approach we built, at the domain engineering level, a WSbWA-specific lightweight product-line architecture and combined it, at the application engineering level, with an Agile Method that uses a domain-specific visual language with direct manipulation and extraction capabilities of web services to perform customization and calibration of a product or WSBWA for a specific customer. To assess the effectiveness of our approach we designed and implemented a tool that we used to investigate the return on investment of the activities related to PLE and AMs. Details of our proposed approach, the related tool developed, and the experimental study performed are presented in this article together with a discussion of planned directions of future work.},
journal = {J. Syst. Softw.},
month = jun,
pages = {855–867},
numpages = {13},
keywords = {Agile methods, Product line architecture, Product line engineering, Visual languages, Web services}
}

@article{10.1016/j.jss.2007.10.025,
author = {Hanssen, Geir K. and Fígri, Tor E.},
title = {Process fusion: An industrial case study on agile software product line engineering},
year = {2008},
issue_date = {June, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {6},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.10.025},
doi = {10.1016/j.jss.2007.10.025},
abstract = {This paper presents a case study of a software product company that has successfully integrated practices from software product line engineering and agile software development. We show how practices from the two fields support the company's strategic and tactical ambitions, respectively. We also discuss how the company integrates strategic, tactical and operational processes to optimize collaboration and consequently improve its ability to meet market needs, opportunities and challenges. The findings from this study are relevant to software product companies seeking ways to balance agility and product management. The findings also contribute to research on industrializing software engineering.},
journal = {J. Syst. Softw.},
month = jun,
pages = {843–854},
numpages = {12},
keywords = {Agile software development, Software product development, Software product line engineering, Software product management}
}

@inproceedings{10.5555/2022115.2022130,
author = {Wu, Yijian and Yang, Yiming and Peng, Xin and Qiu, Cheng and Zhao, Wenyun},
title = {Recovering object-oriented framework for software product line reengineering},
year = {2011},
isbn = {9783642213465},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A large number of software product lines (SPL) in practice are not constructed from scratch, but reengineered from legacy variant products. In order to transfer legacy products to SPL core assets, reverse variability analysis should be involved to find commonality and differences among variant artifacts. In this paper we concentrate on the recovery of SPL framework which can be represented by an object-oriented design model with variation points. We propose a semi-automatic SPL framework recovery approach with the assumption that involved legacy products have similar designs and implementations. In this approach, we adopt a bottom-up process based on clone detection and context analysis to identify corresponding mappings among design elements in different products. Then we use a top-down process from class level to method level with some heuristic rules to determine the commonality/variability classification and the variability type for each design element. In order to evaluate the effectiveness of our approach, we conduct a case study on an industrial product line and present comprehensive analysis and discussions on the results.},
booktitle = {Proceedings of the 12th International Conference on Top Productivity through Software Reuse},
pages = {119–134},
numpages = {16},
location = {Pohang, South Korea},
series = {ICSR'11}
}

@inproceedings{10.5555/1885639.1885671,
author = {Marinho, Fabiana G. and Lima, Fabrício and Filho, João B. Ferreira and Rocha, Lincoln and Maia, Marcio E. F. and de Aguiar, Saulo B. and Dantas, Valéria L. L. and Viana, Windson and Andrade, Rossana M. C. and Teixeira, Eldânae and Werner, Cláudia},
title = {A software product line for the mobile and context-aware applications domain},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The mobile and context-aware application domain presents challenging requirements to software development. Although several solutions have been proposed for this type of application, reuse is not systematically used throughout the software development lifecycle. Then, in this paper we propose an approach for the development of a mobile and context-aware Software Product Line (SPL). A SPL for the mobile and context-ware mobile guide domain is presented in order to illustrate the steps of the proposed approach. Furthermore, the lessons learned in the SPL development are discussed. Both approach and SPL are the main contributions of this paper.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {346–360},
numpages = {15},
keywords = {context-awareness, mobility, software product line},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/3307630.3342407,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Clémentine},
title = {Exploring the Variability of Interconnected Product Families with Relational Concept Analysis},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342407},
doi = {10.1145/3307630.3342407},
abstract = {Among the various directions that SPLE promotes, extractive adoption of complex product lines is especially valuable, provided that appropriate approaches are made available. Complex variability can be encoded in different ways, including the feature model (FM) formalism extended with multivalued attributes, UML-like cardinalities, and references connecting separate FMs. In this paper, we address the extraction of variability relationships depicting connections between systems from separate families. Because Formal Concept Analysis provides suitable knowledge structures to represent the variability of a given system family, we explore the relevance of Relational Concept Analysis, an FCA extension to take into account relationships between different families, to tackle this issue. We investigate a method to extract variability information from descriptions representing several inter-connected product families. It aims to be used to assist the design of inter-connected FMs, and to provide recommendations during product selection.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {199–206},
numpages = {8},
keywords = {complex software product line, relational concept analysis, reverse engineering, variability extraction},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/1176617.1176733,
author = {Trask, Bruce and Paniscotti, Dominick and Roman, Angel and Bhanot, Vikram},
title = {Using model-driven engineering to complement software product line engineering in developing software defined radio components and applications},
year = {2006},
isbn = {159593491X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1176617.1176733},
doi = {10.1145/1176617.1176733},
abstract = {This paper details the application of Software Product Lines (SPL)16 and Model-Driven Engineering (MDE)15 to the software defined radio domain. More specifically it is an experience report emphasizing the synergy 17 resulting from combining MDE and SPL technologies. The software defined radio domain has very unique characteristics as its systems typically are a confluence of a number of typically challenging aspects of software development. To name a few, these systems are usually described by modifiers such as, embedded, real-time, distributed, object-oriented, portable, heterogeneous, multithreaded, high performance, dynamic, resource-constrained, safety-critical, secure, networked, component based and fault-tolerant. Each one of these modifiers by themselves carries with it a set of unique challenges, but building systems characterized by all of these modifiers all at the same time makes for a daunting task in software development. In addition to all of these, it is quite common in these embedded systems for components to have multiple implementations that must run on disparate processing elements. With all of this taken into account, it stands to reason that these systems could and should benefit greatly from advances in software technology such as product line engineering, domain-specific modeling and model-driven engineering. It is our experience that one big benefit to the software development industry is the combination of the Software Product Lines and Model Driven Engineering technologies.},
booktitle = {Companion to the 21st ACM SIGPLAN Symposium on Object-Oriented Programming Systems, Languages, and Applications},
pages = {846–853},
numpages = {8},
keywords = {development, domain, generation, language, model},
location = {Portland, Oregon, USA},
series = {OOPSLA '06}
}

@inproceedings{10.1145/1858996.1859021,
author = {Kim, Chang Hwan Peter and Batory, Don and Khurshid, Sarfraz},
title = {Eliminating products to test in a software product line},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859021},
doi = {10.1145/1858996.1859021},
abstract = {A Software Product Line (SPL) is a family of programs where each program is defined by a unique combination of features. Developing a set of programs with commonalities and variabilities in this way can significantly reduce both the time and cost of software development. However, as the number of programs may be exponential in the number of features, testing an SPL, the phase to which the majority of software development is dedicated, becomes especially challenging [12].Indeed, scale is the biggest challenge in testing or checking the properties of programs in a product line. Even a product line with just 10 optional features has over a thousand (210) distinct programs. As an example of a situation where every program must be considered, suppose that every program of an SPL outputs a String that each feature might modify.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {139–142},
numpages = {4},
keywords = {feature oriented programming, software product lines, static analysis, testing},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1145/3307630.3342403,
author = {Berger, Thorsten and Collet, Philippe},
title = {Usage Scenarios for a Common Feature Modeling Language},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342403},
doi = {10.1145/3307630.3342403},
abstract = {Feature models are recognized as a de facto standard for variability modeling. Presented almost three decades ago, dozens of different variations and extensions to the original feature-modeling notation have been proposed, together with hundreds of variability management techniques building upon feature models. Unfortunately, despite several attempts to establish a unified language, there is still no emerging consensus on a feature-modeling language that is both intuitive and simple, but also expressive enough to cover a range of important usage scenarios. There is not even a documented and commonly agreed set of such scenarios.Following an initiative among product-line engineering researchers in September 2018, we present 14 usage scenarios together with examples and requirements detailing each scenario. The scenario descriptions are the result of a systematic process, where members of the initiative authored original descriptions, which received feedback via a survey, and which we then refined and extended based on the survey results, reviewers' comments, and our own expertise. We also report the relevance of supporting each usage scenario for the language, as perceived by the initiative's members, prioritizing each scenario. We present a roadmap to build and implement a first version of the envisaged common language.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {174–181},
numpages = {8},
keywords = {feature models, software product lines, unified language},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/3453444,
author = {Ashmore, Rob and Calinescu, Radu and Paterson, Colin},
title = {Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453444},
doi = {10.1145/3453444},
abstract = {Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic, and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence, and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our article provides a comprehensive survey of the state of the art in the assurance of ML, i.e., in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the machine learning lifecycle, i.e., of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The article begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {111},
numpages = {39},
keywords = {Machine learning lifecycle, assurance, assurance evidence, machine learning workflow, safety-critical systems}
}

@inproceedings{10.1109/ICSE.2019.00090,
author = {Lillack, Max and Stănciulescu, Ştefan and Hedman, Wilhelm and Berger, Thorsten and Wąsowski, Andrzej},
title = {Intention-based integration of software variants},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00090},
doi = {10.1109/ICSE.2019.00090},
abstract = {Cloning is a simple way to create new variants of a system. While cheap at first, it increases maintenance cost in the long term. Eventually, the cloned variants need to be integrated into a configurable platform. Such an integration is challenging: it involves merging the usual code improvements between the variants, and also integrating the variable code (features) into the platform. Thus, variant integration differs from traditional software merging, which does not produce or organize configurable code, but creates a single system that cannot be configured into variants. In practice, variant integration requires fine-grained code edits, performed in an exploratory manner, in multiple iterations. Unfortunately, little tool support exists for integrating cloned variants.In this work, we show that fine-grained code edits needed for integration can be alleviated by a small set of integration intentions---domain-specific actions declared over code snippets controlling the integration. Developers can interactively explore the integration space by declaring (or revoking) intentions on code elements. We contribute the intentions (e.g., 'keep functionality' or 'keep as a configurable feature') and the IDE tool INCLINE, which implements the intentions and five editable views that visualize the integration process and allow declaring intentions producing a configurable integrated platform. In a series of experiments, we evaluated the completeness of the proposed intentions, the correctness and performance of INCLINE, and the benefits of using intentions for variant integration. The experiments show that INCLINE can handle complex integration tasks, that views help to navigate the code, and that it consistently reduces mistakes made by developers during variant integration.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {831–842},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.5555/1887899.1887951,
author = {Lopez-Herrejon, Roberto E.},
title = {On the need of safe software product line architectures},
year = {2010},
isbn = {3642151132},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A Software Product Line (SPL) is a family of related software systems distinguished by the different sets of features each system provides. Over the last decade, the substantial benefits of SPL practices have been extensively documented and corroborated both in academia and industry. Several architecture methods have been proposed that employ different artifacts for expressing the components of a SPL, their properties and relationships. Of crucial importance for any SPL architecture method is to guarantee that the variability, for instance as expressed in feature models, is not only preserved but also kept consistent across all artifacts used. In this research challenge paper we argue that Safe Composition - the guarantee that all programs of a product line are type safe - can be leveraged to address this guarantee for structural properties of SPL architectures and the challenges that that entails.},
booktitle = {Proceedings of the 4th European Conference on Software Architecture},
pages = {493–496},
numpages = {4},
location = {Copenhagen, Denmark},
series = {ECSA'10}
}

@article{10.1016/j.mcm.2005.02.006,
author = {Nichols, K. B. and Venkataramanan, M. A. and Ernstberger, K. W.},
title = {Product line selection and pricing analysis: Impact of genetic relaxations},
year = {2005},
issue_date = {December, 2005},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {42},
number = {13},
issn = {0895-7177},
url = {https://doi.org/10.1016/j.mcm.2005.02.006},
doi = {10.1016/j.mcm.2005.02.006},
abstract = {A model for the product line selection and pricing problem (PLSP) is presented andthree solution procedures based on a genetic algorithm are developed to analyze the results based on consumer preference patterns. Since the PLSP model is nonlinear and integer, two of the solution procedures use genetic encoding to ''relax'' the NP hard model. The relaxations result in linear integer and shortest path models for the fitness evaluation which are solved using branch and bound and labeling algorithms, respectively. Performance of the quality of solutions generated by the procedures is evaluated for various problem sizes and customer preference structures. The results show that the genetic relaxations provide efficient and effective solution methodologies for the problem, when compared to the pure artificial intelligence technique of genetic search. The impact of the preference structure on the product line and the managerial implications of the solution characteristics generated by the genetic relaxations are also discussed. The models can be used to explicitly consider tradeoffs between marketing and operations concerns in designing a product line.},
journal = {Math. Comput. Model.},
month = dec,
pages = {1397–1410},
numpages = {14},
keywords = {Genetic algorithms, Heuristics, Pricing, Product line}
}

@inproceedings{10.5555/2022115.2022129,
author = {Gamez, Nadia and Fuentes, Lidia},
title = {Software product line evolution with cardinality-based feature models},
year = {2011},
isbn = {9783642213465},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature models are widely used for modelling variability present in a Software Product Line family. We propose using cardinality-based feature models and clonable features to model and manage the evolution of the structural variability present in pervasive systems, composed by a large variety of heterogeneous devices. The use of clonable features increases the expressiveness of feature models, but also greatly increases the complexity of the resulting configurations. So, supporting the evolution of product configurations becomes an intractable task to do it manually. In this paper, we propose a model driven development process to propagate changes made in an evolved feature model, into existing configurations. Furthermore, our process allows us to calculate the effort needed to perform the evolution changes in the customized products. To do this, we have defined two operators, one to calculate the differences between two configurations and another to create a new configuration from a previous one. Finally, we validate our approach, showing that by using our tool support we can generate new configurations for a family of products with thousands of cloned features.},
booktitle = {Proceedings of the 12th International Conference on Top Productivity through Software Reuse},
pages = {102–118},
numpages = {17},
keywords = {evolution, feature models, software product lines},
location = {Pohang, South Korea},
series = {ICSR'11}
}

@article{10.4018/IJWSR.2019010103,
author = {Sun, Chang-ai and Wang, Zhen and Wang, Ke and Xue, Tieheng and Aiello, Marco},
title = {Adaptive BPEL Service Compositions via Variability Management: A Methodology and Supporting Platform},
year = {2019},
issue_date = {January 2019},
publisher = {IGI Global},
address = {USA},
volume = {16},
number = {1},
issn = {1545-7362},
url = {https://doi.org/10.4018/IJWSR.2019010103},
doi = {10.4018/IJWSR.2019010103},
abstract = {Service-Oriented Architectures are a popular development paradigm to enable distributed applications constructed from independent web services. When coordinated, web services are an infrastructure to fulfill dynamic and vertical integration of business. They may face frequent changes of both requirements and execution environments. Static and predefined service compositions using business process execution language BPEL are not able to cater for such rapid and unpredictable context shifts. The authors propose a variability management-based adaptive and configurable service composition approach that treats changes as first-class citizens and consists of identifying, expressing, realizing, and managing changes of service compositions. The proposed approach is realized with a language called VxBPEL to support variability in service compositions and a platform for design, execution, analysis, and maintenance of VxBPEL-based service compositions. Four case studies validate the feasibility of the proposed approach while exhibiting good performance of the supporting platform.},
journal = {Int. J. Web Serv. Res.},
month = jan,
pages = {37–69},
numpages = {33},
keywords = {Adaptive Systems, Business Process Execution Language, Service Composition, Service Oriented Architectures, Variability Management}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {generating feature models, inductive generalization from examples, machine learning},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and Jézéquel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2491627.2491629,
author = {Clements, Paul and Krueger, Charles and Shepherd, James and Winkler, Andrew},
title = {A PLE-based auditing method for protecting restricted content in derived products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491629},
doi = {10.1145/2491627.2491629},
abstract = {Many organizations that produce a portfolio of products for different customers need to ensure that sensitive or restricted content that may appear in some products must not appear in others. Examples of this need include complying with statutes in different countries of sale, protection of intellectual property developed specifically for one customer, and more. For organizations operating under these requirements and producing their products under a product line engineering paradigm that relies on automation in product derivation, there is a need for a method to ensure that the content restrictions have been met in the derived products. This paper describes an auditing method that meets this need. It was created for use in the Second Generation Product Line Engineering approach that is being applied by Lockheed Martin in their AEGIS ship combat system product line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {218–226},
numpages = {9},
keywords = {bill-of-features, feature modeling, feature profiles, hierarchical product lines, product audit, product baselines, product configurator, product derivation, product line engineering, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@article{10.1016/j.eswa.2007.01.036,
author = {Liao, Shu-Hsien and Chen, Chyuan-Meei and Wu, Chung-Hsin},
title = {Mining customer knowledge for product line and brand extension in retailing},
year = {2008},
issue_date = {April, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {34},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.01.036},
doi = {10.1016/j.eswa.2007.01.036},
abstract = {Retailing consists of the final activities and steps needed to place a product in the hands of the consumer or to provide services to the consumer. In fact, retailing is actually the last step in a supply chain that may stretch from Europe or Asia to the customer's hometown. Therefore, any firm that sells a product or provides a service to the final consumer is performing the retailing function. On the other hand, product line extension, which adds depth to an existing product line by introducing new products in the same product category, can give customers greater choice and help to protect the firm from flanking attack by a competitor. In addition, a product line extension is marketed under the same general brand as a previous item or items. Thus, to distinguish the brand extension from the other item(s) under the primary brand, the retailer can either add secondary brand identification or add a generic brand. This paper investigates product line and brand extension issues in the Taiwan branch of a leading international retailing company, Carrefour, which is a hypermarket retailer. This paper develops a relational database and proposes Apriori algorithm and K-means as methodologies for association rule and cluster analysis for data mining, which is then implemented to mine customer knowledge from household customers. Knowledge extraction by data mining results is illustrated as knowledge patterns/rules and clusters in order to propose suggestions and solutions to the case firm for product line and brand extensions and knowledge management.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {1763–1776},
numpages = {14},
keywords = {Association rules, Brand extension, Cluster analysis, Data mining, Knowledge extraction, Product line extension, Retailing}
}

@inproceedings{10.1145/3382025.3414952,
author = {Varela-Vaca, Ángel Jesús and Gasca, Rafael M. and Carmona-Fombella, Jose Antonio and Gómez-López, María Teresa},
title = {AMADEUS: towards the AutoMAteD secUrity teSting},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414952},
doi = {10.1145/3382025.3414952},
abstract = {The proper configuration of systems has become a fundamental factor to avoid cybersecurity risks. Thereby, the analysis of cybersecurity vulnerabilities is a mandatory task, but the number of vulnerabilities and system configurations that can be threatened is extremely high. In this paper, we propose a method that uses software product line techniques to analyse the vulnerable configuration of the systems. We propose a solution, entitled AMADEUS, to enable and support the automatic analysis and testing of cybersecurity vulnerabilities of configuration systems based on feature models. AMADEUS is a holistic solution that is able to automate the analysis of the specific infrastructures in the organisations, the existing vulnerabilities, and the possible configurations extracted from the vulnerability repositories. By using this information, AMADEUS generates automatically the feature models, that are used for reasoning capabilities to extract knowledge, such as to determine attack vectors with certain features. AMADEUS has been validated by demonstrating the capacities of feature models to support the threat scenario, in which a wide variety of vulnerabilities extracted from a real repository are involved. Furthermore, we open the door to new applications where software product line engineering and cybersecurity can be empowered.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {11},
numpages = {12},
keywords = {cybersecurity, feature model, pentesting, reasoning, testing, vulnerabilities, vulnerable configuration},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2695664.2695991,
author = {Labib, A. Ezzat and Penadés, M. Carmen and Canós, José H. and Gómez, Abel},
title = {Enforcing reuse and customization in the development of learning objects: a product line approach},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695991},
doi = {10.1145/2695664.2695991},
abstract = {The growing use of information technologies in the educational cycles has raised new requirements for the development of Interactive Learning Materials in terms of content reuse, customization, and ease of creation and efficiency of production. In practical terms, the goal is the development of tools for creating reusable, granular, durable, and interoperable learning objects, and to compose such objects into meaningful courseware pieces. Current learning object development tools require special technical skills in the instructors to exploit reuse and customization features, leading sometimes to unsatisfactory user experiences.In this paper, we explore a new way to reuse and customization following Product Line Engineering principles and tools. We have applied product line-based document engineering tools to create the so-called Learning Object Authoring Tool (LOAT), which supports the development of learning materials following the Cisco's Reusable Information Object strategy. We describe the principles behind LOAT, outline its design, and give clues about how it may be used by instructors to create learning objects in their own disciplines.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {261–263},
numpages = {3},
keywords = {authoring tool, content model, document product line, e-learning, learning object},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1504/IJIIDS.2008.021444,
author = {Bubak, Oldrich and Gomaa, Hassan},
title = {Applying software product line concepts in service orientation},
year = {2008},
issue_date = {November 2008},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {2},
number = {4},
issn = {1751-5858},
url = {https://doi.org/10.1504/IJIIDS.2008.021444},
doi = {10.1504/IJIIDS.2008.021444},
abstract = {Today's competitive business environment commands innovation, increasingly shorter time-to-market and efficiency. Product line technology, pioneered in manufacturing, is increasingly finding its way to the software sector allowing companies to sustain growth and achieve market success. Regardless of the domain, however, information systems have been behind all facets of business operations. Here, the emerging service oriented architectures can help provide the answers to the need for agility, versatility and economies. This effort introduces the concepts of software product lines and service orientation and explores their parallels. Next, it attempts to show the applicability of software product line methods to service orientation. Finally, the work discusses the main obstacles on the road to realising the synergy between these cutting-edge technologies.},
journal = {Int. J. Intell. Inf. Database Syst.},
month = nov,
pages = {383–396},
numpages = {14},
keywords = {SOA, SPL, enterprise architecture, reusable architecture, service orientation, service oriented architecture, software product lines}
}

@inproceedings{10.1145/2791060.2791103,
author = {Mazo, Raúl and Muñoz-Fernández, Juan C. and Rincón, Luisa and Salinesi, Camille and Tamura, Gabriel},
title = {VariaMos: an extensible tool for engineering (dynamic) product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791103},
doi = {10.1145/2791060.2791103},
abstract = {This paper presents the new release of VariaMos, a Java-based tool for defining variability modeling languages, modeling (dynamic) product lines and cyber-physical self-adaptive systems, and supporting automated verification, analysis, configuration and simulation of these models. In particular, we describe the characteristics of this new version regarding its first release: (1) the capability to create languages for modeling systems with variability, even with different views; (2) the capability to use the created language to model (dynamic) product lines; (3) the capability to analyze and configure these models according to the changing context and requirements; and (4) the capability to execute them over several simulation scenarios. Finally, we show how to use VariaMos with an example, and we compare it with other tools found in the literature.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {374–379},
numpages = {6},
keywords = {constraints, dynamic product line models, product line engineering, simulation, tool, variability},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.infsof.2006.05.004,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {Managing the business of software product line: An empirical investigation of key business factors},
year = {2007},
issue_date = {February, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.05.004},
doi = {10.1016/j.infsof.2006.05.004},
abstract = {Business has been highlighted as a one of the critical dimensions of software product line engineering. This paper's main contribution is to increase the understanding of the influence of key business factors by showing empirically that they play an imperative role in managing a successful software product line. A quantitative survey of software organizations currently involved in the business of developing software product lines over a wide range of operations, including consumer electronics, telecommunications, avionics, and information technology, was designed to test the conceptual model and hypotheses of the study. This is the first study to demonstrate the relationships between the key business factors and software product lines. The results provide evidence that organizations in the business of software product line development have to cope with multiple key business factors to improve the overall performance of the business, in addition to their efforts in software development. The conclusions of this investigation reinforce current perceptions of the significance of key business factors in successful software product line business.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {194–208},
numpages = {15},
keywords = {Key business factor, Management, Marketing strategy, Software engineering economics, Software product line, Strategic planning}
}

@inproceedings{10.5555/1885639.1885650,
author = {Svendsen, Andreas and Zhang, Xiaorui and Lind-Tviberg, Roy and Fleurey, Franck and Haugen, Øystein and Møller-Pedersen, Birger and Olsen, Gøran K.},
title = {Developing a software product line for train control: a case study of CVL},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents a case study of creating a software product line for the train signaling domain. The Train Control Language (TCL) is a DSL which automates the production of source code for computers controlling train stations. By applying the Common Variability Language (CVL), which is a separate and generic language to define variability on base models, we form a software product line of stations. We discuss the process and experience of using CVL to automate the production of three real train stations. A brief discussion about the verification needed for the generated products is also included.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {106–120},
numpages = {15},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/3387904.3389257,
author = {Raab, Markus and Denner, Bernhard and Hahnenberg, Stefan and Cito, Jürgen},
title = {Unified Configuration Setting Access in Configuration Management Systems},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389257},
doi = {10.1145/3387904.3389257},
abstract = {The behavior of software is often governed by a large set of configuration settings, distributed over several stacks in the software system. These settings are often manifested as plain text files that exhibit different formats and syntax. Configuration management systems are introduced to manage the complexity of provisioning and distributing configuration in large scale software. Globally patching configuration settings in these systems requires, however, introducing text manipulation or external templating mechanisms, that paradoxically lead to increased complexity and, eventually, to misconfigurations. These issues manifest through crashes or bugs that are often only discovered at runtime. We introduce a framework called Elektra, which integrates a centralized configuration space into configuration management systems to avoid syntax errors and avert the overriding of default values, to increase developer productivity. Elektra enables mounting different configuration files into a common, globally shared data structure to abstract away from the intricate details of file formats and configuration syntax and introduce a unified way to specify and patch configuration settings as key/value pairs. In this work, we integrate Elektra in the configuration management tool Puppet. Additionally, we present a user study with 14 developers showing that Elektra enables significant productivity improvements over existing configuration management concepts. Our study participants performed significantly faster using Elektra in solving three representative scenarios that involve configuration manipulation, compared to other general-purpose configuration manipulation methods.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {331–341},
numpages = {11},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1145/3461001.3471145,
author = {Sundermann, Chico and Feichtinger, Kevin and Engelhardt, Dominik and Rabiser, Rick and Thüm, Thomas},
title = {Yet another textual variability language? a community effort towards a unified language},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471145},
doi = {10.1145/3461001.3471145},
abstract = {Variability models are commonly used to model commonalities and variability in a product line. There is a large variety of textual formats to represent and store variability models. This variety causes overhead to researchers and practitioners as they frequently need to translate models. The MODEVAR initiative consists of dozens of researchers and aims to find a unified language for variability modeling. In this work, we describe the cooperative development of a textual variability language. We evaluate preferences of the community regarding properties of existing formats and applications for an initial design of a unified variability language. Then, we examine the acceptance of the community for our proposal. The results indicate that our proposal is a promising start towards a unified variability language instead of yet another language. We envision that the community applies our language proposal in teaching, research prototypes, and industrial applications to further evolve the design and then ultimately reach a unified language.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {136–147},
numpages = {12},
keywords = {exchange format, software product lines, unified language, variability language, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {automatic repair, fault, system evolution, variability model},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3236405.3236426,
author = {Belarbi, Maouaheb},
title = {A methodological framework to enable the generation of code from DSML in SPL},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236426},
doi = {10.1145/3236405.3236426},
abstract = {Software Product Line has acquired a significant momentum at the end of the 1990ies since it allows the production of variable software systems corresponding to the same domain portfolio. The effectiveness of the derivation process depends on how well variability is defined and implemented which is a crucial topic area that was addressed among two essential trends: On the one hand, starting from Domain Specific Modelling Language to express domain requirements and automate the code generation with Model-Driven Engineering techniques and on the second hand, exploiting the soar of variability mechanisms.In this context, the current research presents a method that unifies the two aforementioned approaches to cover the overall strategies by defining a framework that allows a better code generation in terms of documentation, maintainability, rapidity,etc. The starting point is the usage of the Domain Specific Modelling Language to represent the stakeholders requirements. Then, the resulting meta-model will be converted into one our several Feature Diagrams on which variability mechanisms can be applied to generate all the family products.A preliminary experiment has been undertaken to design the methodology of the proposed software factory in a meta-model. The validation task was evaluated with an academic use case called HandiWeb developed to facilitate handicap persons access to the internet. The first results allow us to put the hand on the key challenges that must be resolved by the proposed methodology.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {64–71},
numpages = {8},
keywords = {DSML, SPL, methodology, software factory, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3236405.3237205,
author = {Ziadi, Tewfik and Martinez, Jabier and Tërnava, Xhevahire},
title = {Teaching projects and research objectives in SPL extraction},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3237205},
doi = {10.1145/3236405.3237205},
abstract = {This year at SPLC we present a teaching and research project where a group of master students analysed a variability-rich domain and extracted an SPL (The Robocode SPL). We present the results of such extraction augmented with an analysis and a quantification regarding the time and effort spent. The research objective was to get and share data about an end-to-end SPL extraction which is usually unavailable in industrial cases because of their large size, complexity, and duration. We provide all the material to replicate, reproduce or extend the case study so it can be easily reused for teaching by anyone in our community. However, we were asking ourselves how can we leverage such case study for teaching to pursue research objectives. In this position paper, we aim to outline our initial ideas that we want to enrich with the others' viewpoints during SPLTea. Towards planning the settings of future teaching projects around this Robocode SPL case study, which can be the timely research objectives that we can identify? Can we involve others in planning this project in their institutions to get further relevant results?},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {44–45},
numpages = {2},
keywords = {extractive software product line adoption, reverse-engineering, software product lines, teaching},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414960,
author = {Strüder, Stefan and Mukelabai, Mukelabai and Strüber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {classification, defect, feature, prediction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.5555/1885639.1885663,
author = {Ganesan, Dharmalingam and Lindvall, Mikael and McComas, David and Bartholomew, Maureen and Slegel, Steve and Medina, Barbara},
title = {Architecture-based unit testing of the flight software product line},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents an analysis of the unit testing approach developed and used by the Core Flight Software (CFS) product line team at the NASA GSFC. The goal of the analysis is to understand, review, and recommend strategies for improving the existing unit testing infrastructure as well as to capture lessons learned and best practices that can be used by other product line teams for their unit testing. The CFS unit testing framework is designed and implemented as a set of variation points, and thus testing support is built into the product line architecture. The analysis found that the CFS unit testing approach has many practical and good solutions that are worth considering when deciding how to design the testing architecture for a product line, which are documented in this paper along with some suggested improvements.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {256–270},
numpages = {15},
keywords = {coverage, flight software, function hook, implemented architecture, mock, unit testing},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/1159733.1159762,
author = {Denger, Christian and Kolb, Ronny},
title = {Testing and inspecting reusable product line components: first empirical results},
year = {2006},
isbn = {1595932186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1159733.1159762},
doi = {10.1145/1159733.1159762},
abstract = {In recent years, product line development has increasingly received attention in industry as it enables software-developing organizations to reduce both cost and time of developing and maintaining increasingly complex systems as well as to address the demands for individually customized products. Successful product line development requires high quality of reusable artifacts in order to achieve the promised benefits. The unique issues of quality assurance in the context of systematic reuse, however, have not been quantitatively investigated so far. This paper describes a first empirical study comparing the two defect detection techniques, code inspections and functional testing, in the context of product line development. The primary goal of the study was to initially investigate the defect finding potential of the techniques on reusable software components with common and variant features. The major findings of the study are that the two techniques identified different types of defects on variants of a reusable component. Inspections are on average 66.39% more effective and need on average 36.84% less effort to detect a defect We found that both the testing and inspection techniques applied in the experiment were ineffective in identifying variant-specific defects. Overall, the results indicate that the standard quality assurance techniques seem to be insufficient to address special characteristics of reusable components.},
booktitle = {Proceedings of the 2006 ACM/IEEE International Symposium on Empirical Software Engineering},
pages = {184–193},
numpages = {10},
keywords = {controlled experiment, functional testing, inspection, quality assurance, reusable components, software product line},
location = {Rio de Janeiro, Brazil},
series = {ISESE '06}
}

@inproceedings{10.1145/3106195.3106217,
author = {Krüger, Jacob and Nielebock, Sebastian and Krieter, Sebastian and Diedrich, Christian and Leich, Thomas and Saake, Gunter and Zug, Sebastian and Ortmeier, Frank},
title = {Beyond Software Product Lines: Variability Modeling in Cyber-Physical Systems},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106217},
doi = {10.1145/3106195.3106217},
abstract = {Smart IT has an increasing influence on the control of daily life. For instance, smart grids manage power supply, autonomous automobiles take part in traffic, and assistive robotics support humans in production cells. We denote such systems as Cyber-physical Systems (CPSs), where cyber addresses the controlling software, while physical describes the controlled hardware. One key aspect of CPSs is their capability to adapt to new situations autonomously or with minimal human intervention. To achieve this, CPSs reuse, reorganize and reconfigure their components during runtime. Some components may even serve in different CPSs and different situations simultaneously. The hardware of a CPS usually consists of a heterogeneous set of variable components. While each component can be designed as a software product line (SPL), which is a well established approach to describe software and hardware variability, it is not possible to describe CPSs' variability solely on a set of separate, non-interacting product lines. To properly manage variability, a CPS must specify dependencies and interactions of its separate components and cope with variable environments, changing requirements, and differing safety properties. In this paper, we i) propose a classification of variability aspects, ii) point out current challenges in variability modeling, and iii) sketch open research questions. Overall, we aim to initiate new research directions for variable CPSs based on existing product-line techniques.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {237–241},
numpages = {5},
keywords = {Cyber-physical system, Modeling, Software product line},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.cie.2019.06.039,
author = {Okpoti, Evans Sowah and Jeong, In-Jae and Moon, Seung Ki},
title = {Decentralized determination of design variables among cooperative designers for product platform design in a product family},
year = {2019},
issue_date = {Sep 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2019.06.039},
doi = {10.1016/j.cie.2019.06.039},
journal = {Comput. Ind. Eng.},
month = sep,
pages = {601–614},
numpages = {14},
keywords = {Product family, Product platform design, Decentralized coordination, Collaborative design, Multi-agent}
}

@inproceedings{10.1145/3357419.3357437,
author = {Khan, Khansa and Azam, Farooque and Anwar, Muhammad Waseem and Kiran, Ayesha},
title = {A Meta-model For Software Project Change and Configuration Management},
year = {2019},
isbn = {9781450371889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357419.3357437},
doi = {10.1145/3357419.3357437},
abstract = {Project change and configuration management refers to the process of controlling and managing of change in project development process. Change can occur in any of the items and phases. This paper has focused on change management i.e. when change occurs in requirements of the software project. Several approaches have been proposed for change/configuration management. But, a metamodel is required that accommodates the tracking of change in three of the requirements i.e. Functional Requirement (FR), Non-Functional Requirements (NFR) and Domain Requirements (DR). In this paper, we have proposed a metamodel for tracking that how change in one requirement impacts change in other requirement so that it can later manage the development process of the project. It provides the identification of hierarchal relation b/w FRs, NFRs and DRs and helps to authorize the changes in order to ensure the requirements consistency in the project.},
booktitle = {Proceedings of the 9th International Conference on Information Communication and Management},
pages = {12–16},
numpages = {5},
keywords = {Metamodel, configuration management, domain requirements (DR), functional requirements (FR), non-functional requirements (NFR)},
location = {Prague, Czech Republic},
series = {ICICM '19}
}

@inproceedings{10.1145/3106195.3106212,
author = {Marimuthu, C. and Chandrasekaran, K.},
title = {Systematic Studies in Software Product Lines: A Tertiary Study},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106212},
doi = {10.1145/3106195.3106212},
abstract = {Software product lines are widely used in the software industries to increase the re-usability and to decrease maintenance cost. On the other hand, systematic reviews are widely used in the software engineering research community to provide the overview of the research field and practitioners guidelines. Researchers have conducted many systematic studies on the different aspects of SPLs. To the best of our knowledge, till now there is no tertiary study conducted on systematic studies of SPL related research topics. In this paper, we aim at conducting a systematic mapping study of existing systematic studies to report the overview of the findings for researchers and practitioners. We performed snowballing and automated search to find out the relevant systematic studies. As a result, we analyzed 60 relevant studies to answer 5 research questions. The main focus of this tertiary study is to highlight the research topics, type of published reviews, active researchers and publication forums. Additionally, we highlight some of the limitations of the systematic studies. The important finding of this study is that the research field is well matured as the systematic studies covered a wide range of research topics. Another important finding is that many studies provided information for practitioners as well as researchers which is a notable improvement in the systematic reviews. However, many studies failed to assess the quality of the primary studies which is the major limitation of the existing systematic studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {143–152},
numpages = {10},
keywords = {software product line, systematic review, tertiary study},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.5555/645882.672247,
author = {Muthig, Dirk and Atkinson, Colin},
title = {Model-Driven Product Line Architectures},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {It has long been recognized that successful product line engineering revolves around the creation of a coherent and flexible product line architecture that consolidates the common parts of a product family for reuse and captures the variant parts for simple adaptation. However, it has been less clear what form such architectures should take and how they should be represented. One promising approach is offered by the new Model-Driven Architecture (MDA) paradigm of the Object Management Group (OMG). This paradigm holds that an organization's key architectural assets should be represented in an abstract "platform-independent" way, in terms of Unified Modeling Language (UML) models, and thereby be shielded from the idiosyncrasies and volatility of specific implementation technologies. In this paper, we discuss the opportunities and challenges involved in using the MDA paradigm for product line engineering and explain how model-driven, product line architectures can be developed, maintained and applied. After first outlining the core concepts of product line engineering and the ad hoc strategies currently used to support it, the paper provides a detailed metamodel of the information that needs to be stored within a product line architecture.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {110–129},
numpages = {20},
series = {SPLC 2}
}

@inproceedings{10.1145/3382025.3414967,
author = {Lima, Jackson A. Prado and Mendonça, Willian D. F. and Vergilio, Silvia R. and Assunção, Wesley K. G.},
title = {Learning-based prioritization of test cases in continuous integration of highly-configurable software},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414967},
doi = {10.1145/3382025.3414967},
abstract = {Continuous Integration (CI) is a practice widely adopted in the industry to allow frequent integration of code changes. During the CI process, many test cases are executed multiple times a day, subject to time constraints. In this scenario, a learning-based approach, named COLEMAN, has been successfully applied. COLEMAN allows earlier execution of the most promising test cases to reveal faults. This approach considers CI particularities such as time budget and volatility of test cases, related to the fact that test cases can be added/removed along the CI cycles. In the CI of Highly Configuration System (HCS), many product variants must be tested, each one with different configuration options, but having test cases that are common to or reused from other variants. In this context, we found, by analogy, another particularity, the volatility of variants, that is, some variants can be included/discontinued along CI cycles. Considering this context, this work introduces two strategies for the application of COLEMAN in the CI of HCS: the Variant Test Set Strategy (VTS) that relies on the test set specific for each variant, and the Whole Test Set Strategy (WST) that prioritizes the test set composed by the union of the test cases of all variants. Both strategies are evaluated in a real-world HCS, considering three test budgets. The results show that the proposed strategies are applicable regarding the time spent for prioritization. They perform similarly regarding early fault detection, but WTS better mitigates the problem of beginning without knowledge, and is more suitable when a new variant to be tested is added.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {31},
numpages = {11},
keywords = {continuous integration, family of products, software product line, test case prioritization},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3336294.3336315,
author = {Wolschke, Christian and Becker, Martin and Schneickert, Sören and Adler, Rasmus and MacGregor, John},
title = {Industrial Perspective on Reuse of Safety Artifacts in Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336315},
doi = {10.1145/3336294.3336315},
abstract = {In the future, safety-critical industrial products will have to be maintained and variants will have to be produced. In order to do this economically, the safety artifacts of the components should also be reused. At present, however, it is still unclear how this reuse could take place. Moreover this reuse is complicated, by the different situations in the various industries involved and by the corresponding standards applied.Current industrial practice for certification processes relies on a component-based view of reuse. We investigate the possibilities of product lines with managed processes for reuse also across multiple domains.In order to identify the challenges and possible solutions, we conducted interviews with industry partners from the domains of ICT, Rail, Automotive, and Industrial Automation, and from small- and medium-sized enterprises to large organizations. The semi-structured interviews identified the characteristics of current safety engineering processes, the handling of general variety and reuse, the approach followed for safety artifacts, and the need for improvement.In addition, a detailed literature survey summarizes existing approaches. We investigate which modularity concepts exist for dealing with safety, how variability concepts integrate safety, by which means process models can consider safety, and how safety cases are evolved while maintenance takes place. An overview of similar research projects complements the analysis.The identified challenges and potential solution proposals show how safety is related to Software Product Lines.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {143–154},
numpages = {12},
keywords = {modular safety, open source certification, product line certification, safety reuse, safety standards},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414968,
author = {Li, Yang and Schulze, Sandro and Scherrebeck, Helene Hvidegaard and Fogdal, Thomas Sorensen},
title = {Automated extraction of domain knowledge in practice: the case of feature extraction from requirements at danfoss},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414968},
doi = {10.1145/3382025.3414968},
abstract = {Software product line supports structured reuse of software artifacts in order to realize the maintenance and evolution of the typically large number of variants, which promotes the industrialization of software development, especially for software-intensive products. However, for a legacy system, it is non-trivial to gain information about commonalities and differences of the variants. Meanwhile, software requirements specifications as the initial artifacts can be used to achieve this information to generate a domain model. Unfortunately, manually analyzing these requirements is time-consuming and inefficient. To address this problem, we explored the usage of feature extraction techniques to automatically extract domain knowledge from requirements to assist domain engineers. In detail, we applied Doc2Vec and a clustering algorithm to process the requirements for achieving the initial feature tree. Moreover, we utilized key words/phrases extraction techniques to provide key information to domain engineers for further analyzing the extraction results. In particular, we developed a GUI to support the extraction process. The empirical evaluation indicates that most of the extracted features and terms are beneficial to improve the process of feature extraction.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {4},
numpages = {11},
keywords = {feature extraction, requirement documents, reverse engineering, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.cosrev.2020.100341,
author = {Kotsiopoulos, Thanasis and Sarigiannidis, Panagiotis and Ioannidis, Dimosthenis and Tzovaras, Dimitrios},
title = {Machine Learning and Deep Learning in smart manufacturing: The Smart Grid paradigm},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2020.100341},
doi = {10.1016/j.cosrev.2020.100341},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {25},
keywords = {Industry 4.0, Machine Learning, Deep Learning, Industrial AI, Smart Grid}
}

@inproceedings{10.1145/3382025.3414973,
author = {Schlie, Alexander and Knüppel, Alexander and Seidl, Christoph and Schaefer, Ina},
title = {Incremental feature model synthesis for clone-and-own software systems in MATLAB/Simulink},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414973},
doi = {10.1145/3382025.3414973},
abstract = {Families of related MATLAB/Simulink systems commonly emerge ad hoc using clone-and-own practices. Extractively migrating systems towards a software product line (SPL) can be a remedy. A feature model (FM) represents all potential configurations of an SPL, ideally, in non-technical domain terms. However, yielding a sensible FM from automated synthesis remains a major challenge due to domain knowledge being a prerequisite for features to be adequate abstractions. In incremental reverse engineering, subsequent generation of FMs may further overwrite changes and design decisions made during previous manual FM refinement.In this paper, we propose an approach to largely automate the synthesis of a suitable FM from a set of cloned MATLAB/Simulink models as part of reverse engineering an SPL. We fully automate the extraction of an initial, i.e., a technical, FM that closely aligns with realization artifacts and their variability, and further provide operations to manually refine it to incorporate domain knowledge. Most importantly, we provide concepts to capture such operations and to replay them on a structurally different technical FM stemming from a subsequent reverse engineering increment that included further systems of the portfolio. We further provide an implementation and demonstrate the feasibility of our approach using two MATLAB/Simulink data sets from the automotive domain.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {7},
numpages = {12},
keywords = {150% model, MATLAB/Simulink, clone-and-own, feature model, incremental, individual, mapping, refinement, synthesis, variability},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselböck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {product and production configuration, product line of factories, smart factory, smart product, smart production},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1007/978-3-031-08421-8_41,
author = {Giorgio, Lazzarinetti and Nicola, Massarenti and Fabio, Sgrò and Andrea, Salafia},
title = {Continuous Defect Prediction in CI/CD Pipelines: A Machine Learning-Based Framework},
year = {2021},
isbn = {978-3-031-08420-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08421-8_41},
doi = {10.1007/978-3-031-08421-8_41},
abstract = {Recent advances in information technology has led to an increasing number of applications to be developed and maintained daily by product teams. Ensuring that a software application works as expected and that it is absent of bugs requires a lot of time and resources. Thanks to the recent adoption of DevOps methodologies, it is often the case where code commits and application builds are centralized and standardized. Thanks to this new approach, it is now possible to retrieve log and build data to ease the development and management operations of product teams. However, even if such approaches include code control to detect unit or integration errors, they do not check for the presence of logical bugs that can raise after code builds. For such reasons in this work we propose a framework for continuous defect prediction based on machine learning algorithms trained on a publicly available dataset. The framework is composed of a machine learning model for detecting the presence of logical bugs in code on the basis of the available data generated by DevOps tools and a dashboard to monitor the software projects status. We also describe the serverless architecture we designed for hosting the aforementioned framework.},
booktitle = {AIxIA 2021 – Advances in Artificial Intelligence: 20th International Conference of the Italian Association for Artificial Intelligence, Virtual Event, December 1–3, 2021, Revised Selected Papers},
pages = {591–606},
numpages = {16},
keywords = {Continuous defect prediction, Machine learning, DevOps, Continuous integration}
}

@inproceedings{10.1145/3185089.3185117,
author = {Fahmy, Syahrul and Deraman, Aziz and Yahaya, Jamaiah H.},
title = {The Role of Human in Software Configuration Management},
year = {2018},
isbn = {9781450354141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185089.3185117},
doi = {10.1145/3185089.3185117},
abstract = {Two common problems in software development projects are falling behind schedule and software that does not fulfil its purpose. These problems can be attributed to on-going changes made to software products especially during development and maintenance, leading to more work than initially anticipated, diminishing quality as new changes are implemented. One approach for addressing these problems is through a systematic Software Configuration Management (SCM) process. However, after more than 50 years after its inception, these problems are still prevalent in software development, questioning the effectiveness of SCM implementation by software organizations. Although guided by international standards, industry best practices, and array of tools to support its implementation, the role of human has received little attention (if any), in mainstream SCM research, compared to other areas in software engineering. As such, this research project challenges the traditional view of SCM and puts forth a notion of People-Centric SCM, a holistic approach for managing changes to software products, focused on human rather than tools, and that is based on existing standards and best practices. The model and assessment framework were validated by means of Subject Matter Expert reviews and 5 case studies involving 9 software practitioners from the public sector, higher education institutions and the private sector in Malaysia, in addition to 2 international experts. Results of the validation demonstrated the soundness of the model, the plausibility of the assessment framework, and the practically of the People-Centric SCM approach to software organizations.},
booktitle = {Proceedings of the 2018 7th International Conference on Software and Computer Applications},
pages = {56–60},
numpages = {5},
keywords = {Software Configuration Management, Software Engineering, Software Quality},
location = {Kuantan, Malaysia},
series = {ICSCA '18}
}

@inproceedings{10.1145/2000259.2000264,
author = {Kavimandan, Amogh and Gokhale, Aniruddha and Karsai, Gabor and Gray, Jeff},
title = {Managing the quality of software product line architectures through reusable model transformations},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000264},
doi = {10.1145/2000259.2000264},
abstract = {In model-driven engineering of applications, the quality of the software architecture is realized and preserved in the successive stages of its lifecycle through model transformations. However, limited support for reuse in contemporary model transformation techniques forces developers of product line architectures to reinvent transformation rules for every variant of the product line, which can adversely impact developer productivity and in turn degrade the quality of the resulting software architecture for the variant. To overcome these challenges, this paper presents the MTS (Model-transformation Templatization and Specialization generative transformation process, which promotes reuse in model transformations through parameterization and specialization of transformation rules. MTS defines two higher order transformations to capture the variability in transformation rules and to specialize them across product variants. The core idea behind MTS is realized within a graphical model transformation tool in a way that is minimally intrusive to the underlying tool's implementation. The paper uses two product line case studies to evaluate MTS in terms of reduction in efforts to define model transformation rules as new variants are added to the product line, and the overhead in executing the higher order transformations. These metrics provide an indirect measure of how potential degradation in the quality of software architectures of product lines caused due to lack of reuse can be alleviated by MTS.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {13–22},
numpages = {10},
keywords = {model transformations, reuse, software quality},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1145/1404946.1404950,
author = {Hubaux, A. and Heymans, P. and Unphon, H.},
title = {Separating variability concerns in a product line re-engineering project},
year = {2008},
isbn = {9781605581439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1404946.1404950},
doi = {10.1145/1404946.1404950},
abstract = {Feature diagrams have now become common variability models in software product lines engineering literature. Whereas ongoing research keeps improving their expressiveness, formalisation, and automation, open studies of their usage in real projects are still missing. This paper intends to (1) present the process we followed to elicit the variability of PloneMeeting, an Open Source project, and (2) report on the initial results obtained when applying variability modelling techniques promoting separation of concerns between software variability and product line variability.},
booktitle = {Proceedings of the 2008 AOSD Workshop on Early Aspects},
articleno = {4},
numpages = {8},
keywords = {feature diagram, open source, separation of concerns, software product lines, variability management, variability model},
location = {Brussels, Belgium},
series = {EA '08}
}

@inproceedings{10.1145/2791060.2791086,
author = {Martinez, Jabier and Ziadi, Tewfik and Bissyandé, Tegawendé F. and Klein, Jacques and Le Traon, Yves},
title = {Bottom-up adoption of software product lines: a generic and extensible approach},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791086},
doi = {10.1145/2791060.2791086},
abstract = {Although Software Product Lines are recurrently praised as an efficient paradigm for systematic reuse, practical adoption remains challenging. For bottom-up Software Product Line adoption, where a set of artefact variants already exists, practitioners lack end-to-end support for chaining (1) feature identification, (2) feature location, (3) feature constraints discovery, as well as (4) reengineering approaches. This challenge can be overcome if there exists a set of principles for building a framework to integrate various algorithms and to support different artefact types. In this paper, we propose the principles of such a framework and we provide insights on how it can be extended with adapters, algorithms and visualisations enabling their use in different scenarios. We describe its realization in BUT4Reuse (Bottom--Up Technologies for Reuse) and we assess its generic and extensible properties by implementing a variety of extensions. We further empirically assess the complexity of integration by reproducing case studies from the literature. Finally, we present an experiment where users realize a bottom-up Software Product Line adoption building on the case study of Eclipse variants.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {101–110},
numpages = {10},
keywords = {mining existing assets, reverse engineering, software product line engineering},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3307630.3342415,
author = {Chavarriaga, Jaime and Casallas, Rubby and Parra, Carlos and Henao-Mejía, Martha Cecilia and Calle-Archila, Carlos Ricardo},
title = {Nine Years of Courses on Software Product Lines at Universidad de los Andes, Colombia},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342415},
doi = {10.1145/3307630.3342415},
abstract = {Software Product Lines has been taught in Universidad de los Andes, Colombia, since 2011. The content, activities and evaluation in these courses have changed during this period of time. For instance, while topics such as the processes to engineer product lines, feature models to specify domain variability, and design patterns to implement the variability are common to all these courses, other topics such as the product line maturity levels, some techniques to implement variability and recent automation practices for testing, continuous integration and delivery have varied with the time. In addition, topics and activities, such as the course project that has been present in all the courses, had also been modified. This paper (1) describes the evolution of our courses on Software Product Lines, presenting commonalities and variabilities in their topics, activities and evaluation techniques and (2) discusses some lessons learned during its recent design as a Blended Learning course.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {130–133},
numpages = {4},
keywords = {software product lines, teaching, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2648511.2648546,
author = {Van Landuyt, Dimitri and Op de beeck, Steven and Hovsepyan, Aram and Michiels, Sam and Joosen, Wouter and Meynckens, Sven and de Jong, Gjalt and Barais, Olivier and Acher, Mathieu},
title = {Towards managing variability in the safety design of an automotive hall effect sensor},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648546},
doi = {10.1145/2648511.2648546},
abstract = {This paper discusses the merits and challenges of adopting software product line engineering (SPLE) as the main development process for an automotive Hall Effect sensor. This versatile component is integrated into a number of automotive applications with varying safety requirements (e.g., windshield wipers and brake pedals).This paper provides a detailed explanation as to why the process of safety assessment and verification of the Hall Effect sensor is currently cumbersome and repetitive: it must be repeated entirely for every automotive application in which the sensor is to be used. In addition, no support is given to the engineer to select and configure the appropriate safety solutions and to explain the safety implications of his decisions.To address these problems, we present a tailored SPLE-based approach that combines model-driven development with advanced model composition techniques for applying and reasoning about specific safety solutions. In addition, we provide insights about how this approach can reduce the overall complexity, improve reusability, and facilitate safety assessment of the Hall Effect sensor.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {304–309},
numpages = {6},
keywords = {ASIL validation, automotive, hardware/software co-design, safety patterns, software product line engineering},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3382025.3414959,
author = {Horcas, Jose-Miguel and Pinto, Mónica and Fuentes, Lidia},
title = {Extensible and modular abstract syntax for feature modeling based on language constructs},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414959},
doi = {10.1145/3382025.3414959},
abstract = {Since the definition of feature models in 1990, a large number of language constructs have emerged. Each language construct usually comes with its own abstract and concrete syntax, its semantics, and even its complete language dialect and tool support. Nowadays, there is a consensus in the Software Product Line community about a need for defining a common variability modeling language. But the fact of the matter is that it is very complex to achieve a good compromise between how expressive the language should be and the effort of developing practical tools for a language with all possible language constructs. In this paper, we propose an extensible model-driven engineering approach for defining the abstract syntax of feature modeling language constructs that could be tailored to different needs and domains. We formalize our approach as a set of modular and reusable metamodels that allows practitioners to decide which subset of language constructs to use through: (1) generating a new variability language; and (2) managing feature models with different level of expressiveness. We provide an instantiation and implementation of our approach.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {10},
numpages = {7},
keywords = {SPL, abstract syntax, feature modeling, language construct, language level, metamodeling, model-driven engineering},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461002.3473074,
author = {Fantechi, Alessandro and Gnesi, Stefania and Livi, Samuele and Semini, Laura},
title = {A spaCy-based tool for extracting variability from NL requirements},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473074},
doi = {10.1145/3461002.3473074},
abstract = {In previous work, we have shown that ambiguity detection in requirements can also be used as a way to capture latent aspects of variability. Natural Language Processing (NLP) tools have been used for a lexical analysis aimed at ambiguity indicators detection, and we have studied the necessary adaptations to those tools for pointing at potential variability, essentially by adding specific dictionaries for variability. We have identified also some syntactic rules able to detect potential variability, such as disjunction between nouns or pairs of indicators in a subordinate proposition. This paper describes a new prototype NLP tool, based on the spaCy library, specifically designed to detect variability. The prototype is shown to preserve the same recall exhibited by previously used lexical tools, with a higher precision.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {32–35},
numpages = {4},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1287/deca.2013.0276,
author = {Sarin, Rakesh K.},
title = {From the Editor---Optimal Betting, Reducing Unnecessary Mammography in Breast Cancer Diagnosis, Product Line Design, and Value of Information},
year = {2013},
issue_date = {September 2013},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {10},
number = {3},
issn = {1545-8490},
url = {https://doi.org/10.1287/deca.2013.0276},
doi = {10.1287/deca.2013.0276},
abstract = {A brief summary of the five papers published in this issue is provided. These papers deal with diverse topics that include: optimal betting behavior in the presence of ambiguity; public policy on screening for breast cancer; product line design; and value of information.},
journal = {Decision Analysis},
month = sep,
pages = {187–188},
numpages = {2}
}

@inproceedings{10.1145/2934466.2934482,
author = {Sampaio, Gabriela and Borba, Paulo and Teixeira, Leopoldo},
title = {Partially safe evolution of software product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934482},
doi = {10.1145/2934466.2934482},
abstract = {A key challenge developers might face when evolving a product line is not to inadvertently affect users of existing products. In refactoring and conservative extension scenarios, we can avoid this problem by checking for behavior preservation, either by testing the generated products or by using formal theories. Product line refinement theories support that by requiring behavior preservation for all existing products. However, in many evolution scenarios, such as bug fixing, there is a high chance that only some of the products are refined. To support developers in these and other non full-refinement situations, we define a theory of partial product line refinement that helps to precisely understand which products should not be affected by an evolution scenario. This provides a kind of impact analysis that could, for example, reduce test effort, since products not affected do not need to be tested. Additionally, we formally derive a catalog of eight partial refinement templates that capture evolution scenarios, and associated preconditions, not covered before. Finally, by analyzing 79218 commits from the Linux repository, we find evidence that the proposed templates could cover a number of practical evolution scenarios.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {124–133},
numpages = {10},
keywords = {product line evolution, product line maintenance, product line refinement},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.infsof.2019.05.003,
author = {Espinosa, Edison and Acuña, Silvia Teresita and Vegas, Sira and Juristo, Natalia},
title = {Adopting configuration management principles for managing experiment materials in families of experiments},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.003},
doi = {10.1016/j.infsof.2019.05.003},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {39–67},
numpages = {29},
keywords = {Experimental software engineering, Experiment replication, Experimental material, Experimental software configuration management}
}

@article{10.5555/3447080.3447091,
author = {Border, Charles},
title = {Development of a configuration management course for computing operations students},
year = {2020},
issue_date = {October 2020},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {36},
number = {3},
issn = {1937-4771},
abstract = {The Operations side of deploying a modern computing application necessarily involves multiple groups working in concert to develop the application and the server side configuration that will support that application. This paper reports on efforts to develop a course that encourages students to dig into issues related to configuration management, security policy development, application auditing, business control issues, and most importantly, team work. While the course is entitled "Configuration Management" it is much more about students creating a process for secure iterative application deployment that borrows extensively from the DevOps movement.Ansible, our chosen configuration management tool, is relatively easy to work with at the level of complexity that can be reached in an undergraduate class. What made this class different was the attempt made to create a process that would more closely mimic the Operations side of a DevOps workflow. Initial results from the class were encouraging and many lessons were learned.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {89–101},
numpages = {13}
}

@article{10.1016/j.infsof.2021.106573,
author = {Zhang, Fanlong and Khoo, Siau-cheng},
title = {An empirical study on clone consistency prediction based on machine learning},
year = {2021},
issue_date = {Aug 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {136},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106573},
doi = {10.1016/j.infsof.2021.106573},
journal = {Inf. Softw. Technol.},
month = aug,
numpages = {16},
keywords = {Code clones, Clone consistent change, Clone consistency prediction, Software maintenance, Machine learning}
}

@inproceedings{10.1145/3461002.3473944,
author = {Ballesteros, Joaquín and Fuentes, Lidia},
title = {Transfer learning for multiobjective optimization algorithms supporting dynamic software product lines},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473944},
doi = {10.1145/3461002.3473944},
abstract = {Dynamic Software Product Lines (DSPLs) are a well-accepted approach for self-adapting Cyber-Physical Systems (CPSs) at run-time. The DSPL approaches make decisions supported by performance models, which capture system features' contribution to one or more optimization goals. Combining performance models with Multi-Objectives Evolutionary Algorithms (MOEAs) as decision-making mechanisms is common in DSPLs. However, MOEAs algorithms start solving the optimization problem from a randomly selected population, not finding good configurations fast enough after a context change, requiring too many resources so scarce in CPSs. Also, the DSPL engineer must deal with the hardware and software particularities of the target platform in each CPS deployment. And although each system instantiation has to solve a similar optimization problem of the DSPL, it does not take advantage of experiences gained in similar CPS. Transfer learning aims at improving the efficiency of systems by sharing the previously acquired knowledge and applying it to similar systems. In this work, we analyze the benefits of transfer learning in the context of DSPL and MOEAs testing on 8 feature models with synthetic performance models. Results are good enough, showing that transfer learning solutions dominate up to 71% of the non-transfer learning ones for similar DSPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {51–59},
numpages = {9},
keywords = {cyber-physical systems, dynamic software product lines, multiobjective optimization algorithms, self-adaptation, transfer learning},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {SAT, benchmark, configurable systems, sampling, software product lines, variability model},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/1882291.1882309,
author = {Ramasubbu, Narayan and Balan, Rajesh Krishna},
title = {Evolution of a bluetooth test application product line: a case study},
year = {2010},
isbn = {9781605587912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882291.1882309},
doi = {10.1145/1882291.1882309},
abstract = {In this paper, we study the decision making process involved in the five year lifecycle of a Bluetooth software product produced by a large, multi-national test and measurement firm. In this environment, customer change requests either have to be added as a standard feature in the product, or developed as a special customized version of the product. We first discuss the influential factors, such as evolving standards, market share, installed-base, and complexity, which collectively determined how the firm responded to product change requests. We then develop a predictive decision model to test the collective impact of these factors on determining whether to standardize or customize a customer's change request. Finally, we develop and test a customization cost estimation model, for use by software product teams, which specifically accounts for factors unique to the customization stage of a product lifecycle.},
booktitle = {Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {107–116},
numpages = {10},
keywords = {complexity, product development, product life cycle, software engineering economics, software evolution, software process},
location = {Santa Fe, New Mexico, USA},
series = {FSE '10}
}

@inproceedings{10.1145/3461001.3471144,
author = {Uta, Mathias and Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Tran, Thi Ngoc Trang and Helic, Denis},
title = {Evaluating recommender systems in feature model configuration},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471144},
doi = {10.1145/3461001.3471144},
abstract = {Configurators can be evaluated in various ways such as efficiency and completeness of solution search, optimality of the proposed solutions, usability of configurator user interfaces, and configuration consistency. Due to the increasing size and complexity of feature models, the integration of recommendation algorithms with feature model configurators becomes relevant. In this paper, we show how the output of a recommender system can be evaluated within the scope of feature model configuration scenarios. Overall, we argue that the discussed ways of measuring recommendation quality help developers to gain a broader view on evaluation techniques in constraint-based recommendation domains.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {58–63},
numpages = {6},
keywords = {configuration, evaluation, feature models, recommender systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1007/978-3-642-28714-5_13,
author = {Adam, Sebastian},
title = {Providing software product line knowledge to requirements engineers --- a template for elicitation instructions},
year = {2012},
isbn = {9783642287138},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-28714-5_13},
doi = {10.1007/978-3-642-28714-5_13},
abstract = {[Context &amp; Motivation] Developing new software systems based on a software product line (SPL) in so-called application engineering (AE) projects is still a time-consuming and expensive task. Especially when a large number of customer-specific requirements exists, there is still no systematic support for efficiently aligning these non-anticipated requirements with SPL characteristics early on. [Question/problem] In order to improve this process significantly, sound knowledge about an SPL must be available when guiding the requirements elicitation during AE. Thus, an appropriate reflection of SPL characteristics in process-supporting artifacts is indispensable for actually supporting a requirements engineer in this task. [Principal ideas/results] In this paper, a validated template for elicitation instructions that aims at providing a requirements engineer with knowledge about an underlying SPL in an appropriate manner is presented. This template consists of predefined text blocks and algorithms that explain how SPL-relevant product and process knowledge can be systematically reflected into capability-aware elicitation instructions. [Contribution] By using such elicitation instructions, requirements engineers are enabled to elicit requirements in an AE project more effectively.},
booktitle = {Proceedings of the 18th International Conference on Requirements Engineering: Foundation for Software Quality},
pages = {147–164},
numpages = {18},
location = {Essen, Germany},
series = {REFSQ'12}
}

@article{10.1007/s10845-017-1362-y,
author = {Wang, Chih-Hsuan},
title = {Association rule mining and cognitive pairwise rating based portfolio analysis for product family design},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {4},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-017-1362-y},
doi = {10.1007/s10845-017-1362-y},
abstract = {Changing customer needs coupled with rapid technology advances has boosted stronger requirements for a greater variety of consumer electronics. This trend has forced global companies to reconsider their product-positioning strategies. To reduce design cost and shorten the time to market, portfolio analysis for product family design is usually adopted to acquire diverse but related market applications. This study presents a novel framework to implement product differentiation and product configuration. Initially, association rule mining is used to capture user perceptions to identify the significant portfolios of hedonic attributes. Secondly, cognitive pairwise rating is conducted to elicit user preferences for utilitarian attributes (UAs). Finally, the Technique for Order Preference by Similarity to Ideal Solution is used to prioritize the optimal portfolios of UAs. Experiment results show that "keyboard interface", "body material", and "screen size" are the most concerned HAs for differentiating the product family while "CPU performance" is the most important UA for configuring padbooks, ultrabooks and notebooks. In summary, this research allows companies to effectively and efficiently incorporate user perceptions or preferences into the entire decision-making process.},
journal = {J. Intell. Manuf.},
month = apr,
pages = {1911–1922},
numpages = {12},
keywords = {Association rule mining, Cognitive pairwise rating, Portfolio analysis, Product family design, TOPSIS ranking}
}

@article{10.1016/j.eswa.2020.114161,
author = {Houssein, Essam H. and Emam, Marwa M. and Ali, Abdelmgeid A. and Suganthan, Ponnuthurai Nagaratnam},
title = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114161},
doi = {10.1016/j.eswa.2020.114161},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {20},
keywords = {Breast cancer classification, Convolutional neural network, Computer-aided diagnosis system (CAD), Deep learning, Histological images, Machine learning, Magnetic resonance imaging (MRI), Medical imaging modalities, Mammogram images, Ultrasound images, Thermography images}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Thüm, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {product lines, sampling, sampling evalutaion},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1016/j.rcim.2016.02.001,
author = {Wang, Qi and Tang, Dunbing and Yin, Leilei and Salido, Miguel A. and Giret, Adriana and Xu, Yuchun},
title = {Bi-objective optimization for low-carbon product family design},
year = {2016},
issue_date = {October 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2016.02.001},
doi = {10.1016/j.rcim.2016.02.001},
abstract = {Consumers, industry, and government entities are becoming increasingly concerned about the issue of global warming. With this in mind, manufacturers have begun to develop products with consideration of low-carbon. In recent years, many companies are utilizing product families to satisfy various customer needs with lower costs. However, little research has been conducted on the development of a product family that considers environmental factors. In this paper, a low-carbon product family design that integrates environmental concerns is proposed. To this end, a new method of platform planning is investigated with considerations of cost and greenhouse gas (GHG) emission of a product family simultaneously. In this research, a low-carbon product family design problem is described at first, and then a GHG emission model of product family is established. Furthermore, to support low-carbon product family design, an optimization method is applied to make a significant trade-off between cost and GHG emission to implement a feasible platform planning. Finally, the effectiveness of the proposed method is illustrated through a case study. We present a new planning method of product platform, called low-carbon product family design.We establish a bi-objective optimization model that minimizes costs and greenhouse gas emission of a product family.An optimization method is proposed to solve the bi-objective optimization problem.Our case study shows that there is a tradeoff between cost and GHG emission in product platform planning.},
journal = {Robot. Comput.-Integr. Manuf.},
month = oct,
pages = {53–65},
numpages = {13},
keywords = {Low carbon design, Multiple objective programming, Platform planning, Product family design}
}

@inproceedings{10.1145/3382025.3414964,
author = {Hoff, Adrian and Nieke, Michael and Seidl, Christoph and Sæther, Eirik Halvard and Motzfeldt, Ida Sandberg and Din, Crystal Chang and Yu, Ingrid Chieh and Schaefer, Ina},
title = {Consistency-preserving evolution planning on feature models},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414964},
doi = {10.1145/3382025.3414964},
abstract = {A software product line (SPL) enables large-scale reuse in a family of related software systems through configurable features. SPLs represent a long-term investment so that their ongoing evolution becomes paramount and requires careful planning. While existing approaches enable to create an evolution plan for an SPL on feature-model (FM) level, they assume the plan to be rigid and do not support retroactive changes. In this paper, we present a method that enables to create and retroactively adapt an FM evolution plan while preventing undesired impacts on its structural and logical consistency. This method is founded in structural operational semantics and linear temporal logic. We implement our method using rewriting logic, integrate it within an FM tool suite and perform an evaluation using a collection of existing FM evolution scenarios.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {8},
numpages = {12},
keywords = {feature model evolution, feature models, formal semantics, linear temporal logic, rewriting logic, software evolution, software product lines, structural operational semantics},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3336294.3336302,
author = {Strüber, Daniel and Mukelabai, Mukelabai and Krüger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {benchmark, product lines, software evolution, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2791060.2791095,
author = {Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire},
title = {Test control algorithms for the validation of cyber-physical systems product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791095},
doi = {10.1145/2791060.2791095},
abstract = {Cyber-Physical Systems (CPSs) product lines appear in a wide range of applications of different domains (e.g., car's doors' windows, doors of a lift, etc.). The variability of these systems is large and as a result they can be configured into plenty of configurations. Testing each of the configurations can be time consuming as not only software has to be simulated, but also the hardware and the physical layer of the CPS, which is often modelled with complex mathematical models. Choosing the adequate test control strategy is critical when testing CPSs product lines. This paper presents a set of test control algorithms organized in an architecture of three layers (domain, application and simulation) for testing CPSs product lines. An illustrative example of a CPS product line is presented and three experiments are conducted to measure the performance of the proposed test control algorithms. We conclude that test scheduling and test suite minimization significantly help to reduce the overall test costs while preserving the test quality in CPSs product lines. In addition, we conclude that knowing the results of the previously tested configurations permits reducing the time for the detection of anomalous designs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {273–282},
numpages = {10},
keywords = {cyber-physical systems product lines, product line engineering, testing, validation},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/3229048,
author = {Zheng, Yongjie and Cu, Cuong and Taylor, Richard N.},
title = {Maintaining Architecture-Implementation Conformance to Support Architecture Centrality: From Single System to Product Line Development},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3229048},
doi = {10.1145/3229048},
abstract = {Architecture-centric development addresses the increasing complexity and variability of software systems by focusing on architectural models, which are generally easier to understand and manipulate than source code. It requires a mechanism that can maintain architecture-implementation conformance during architectural development and evolution. The challenge is twofold. There is an abstraction gap between software architecture and implementation, and both may evolve. Existing approaches are deficient in support for both change mapping and product line architecture. This article presents a novel approach named 1.x-way mapping and its extension, 1.x-line mapping to support architecture-implementation mapping in single system development and in product line development, respectively. They specifically address mapping architecture changes to code, maintaining variability conformance between product line architecture and code, and tracing architectural implementation. We built software tools named xMapper and xLineMapper to realize the two approaches, and conducted case studies with two existing open-source systems to evaluate the approaches. The result shows that our approaches are applicable to the implementation of a real software system and are capable of maintaining architecture-implementation conformance during system evolution.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {8},
numpages = {52},
keywords = {Architecture-implementation mapping, architectural evolution, architecture-centric development, architecture-centric feature traceability, variability conformance}
}

@inproceedings{10.1145/2019136.2019161,
author = {Pleuss, Andreas and Rabiser, Rick and Botterweck, Goetz},
title = {Visualization techniques for application in interactive product configuration},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019161},
doi = {10.1145/2019136.2019161},
abstract = {In product line engineering (PLE) a major challenge is the complexity of artifacts that have to be handled. In real-world product lines, variability models can become large and complex comprising thousands of elements with hundreds of non-trivial dependencies. Visual and interactive techniques aim to reduce the (cognitive) complexity and support the user during challenging PLE tasks like product configuration. There are many visualization techniques described in the literature -- e.g., in Software Visualization -- and some isolated techniques have been applied in PLE tools. Nevertheless, the full potential of visualization in the context of PLE has not been exploited so far. This paper provides an overview of (1) available visualization techniques and criteria to judge their benefits and drawbacks for product configuration, (2) which have been applied in product configuration in PLE, and (3) which could be beneficial to support product configuration. We propose a research agenda for future work in visual and interactive PLE techniques.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {22},
numpages = {8},
keywords = {product configuration, product line engineering, software visualization},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2791060.2791075,
author = {Fang, Miao and Leyh, Georg and Doerr, Joerg and Elsner, Christoph and Zhao, Jingjing},
title = {Towards model-based derivation of systems in the industrial automation domain},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791075},
doi = {10.1145/2791060.2791075},
abstract = {Many systems in the industrial automation domain include information systems. They manage manufacturing processes and control numerous distributed hardware and software components. In current practice, the development and reuse of such systems is costly and time-consuming, due to the variability of systems' topology and processes. Up to now, product line approaches for systematic modeling and management of variability have not been well established for such complex domains.In this paper, we present a model-based approach to support the derivation of systems in the target domain. The proposed architecture of the derivation infrastructure enables feature-, topology- and process configuration to be integrated into the multi-staged derivation process. We have developed a prototype to prove feasibility and improvement of derivation efficiency. We report the evaluation results that we collected through semi-structured interviews from domain stakeholders. The results show high potential to improve derivation efficiency by adopting the approach in practice. Finally, we report the lessons learned that raise the opportunities and challenges for future research.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {283–292},
numpages = {10},
keywords = {derivation, model-based engineering, product line, variability modeling},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/1176887.1176897,
author = {Yoshimura, Kentaro and Ganesan, Dharmalingam and Muthig, Dirk},
title = {Defining a strategy to introduce a software product line using existing embedded systems},
year = {2006},
isbn = {1595935428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1176887.1176897},
doi = {10.1145/1176887.1176897},
abstract = {Engine Control Systems (ECS) for automobiles have numerous variants for many manufactures and different markets. To improve development efficiency, exploiting ECS commonalities and predicting their variability are mandatory. The concept of software product line engineering meets the business background of ECS. However, we should carefully investigate the expected technical, economical, and organizational effects of introducing this strategy into existing products.This paper explains an approach for assessing the potential of merging existing embedded software into a product line approach. The definition of an economically useful product line approach requires two things: analyzing return on investment (ROI) expectations of a product line and understanding the effort required for building reusable assets. We did a clone analysis to provide the basis for effort estimation for merge potential assessment of existing variants. We also report on a case study with ECS. We package the lessons learned and open issues that arose during the case study.},
booktitle = {Proceedings of the 6th ACM &amp; IEEE International Conference on Embedded Software},
pages = {63–72},
numpages = {10},
keywords = {clone detection and classification, economics, engine control systems, reverse rngineering, software, software product line},
location = {Seoul, Korea},
series = {EMSOFT '06}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: learning featured transition systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour of a whole family of software-based systems. Reasoning at the family level yields important economies of scale and quality improvements for a broad range of systems such as software product lines, adaptive and configurable systems. Yet, to fully benefit from the above advantages, a model of the system family's behaviour is necessary. Such a model is often prohibitively expensive to create manually due to the number of variants. For large long-lived systems with outdated specifications or for systems that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes to automate the learning of such models from existing artefacts. To advance research at a fundamental level, our learning target are Featured Transition Systems (FTS), an abstract formalism that can be used to provide a pivot semantics to a range of variability-aware state-based modelling languages. The main research questions addressed by this PhD project are: (1) Can we learn variability-aware models efficiently? (2) Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e., with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {active automata learning, featured transition systems, model learning, software product lines, variability mining},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382026.3431252,
author = {Michelon, Gabriela Karoline},
title = {Evolving System Families in Space and Time},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431252},
doi = {10.1145/3382026.3431252},
abstract = {Managing the evolution of system families in space and time, i.e., system variants and their revisions is still an open challenge. The software product line (SPL) approach can support the management of product variants in space by reusing a common set of features. However, feature changes over time are often necessary due to adaptations and/or bug fixes, leading to different product versions. Such changes are commonly tracked in version control systems (VCSs). However, VCSs only deal with the change history of source code, and, even though their branching mechanisms allow to develop features in isolation, VCS does not allow propagating changes across variants. Variation control systems have been developed to support more fine-grained management of variants and to allow tracking of changes at the level of files or features. However, these systems are also limited regarding the types and granularity of artifacts. Also, they are cognitively very demanding with increasing numbers of revisions and variants. Furthermore, propagating specific changes over variants of a system is still a complex task that also depends on the variability-aware change impacts. Based on these existing limitations, the goal of this doctoral work is to investigate and define a flexible and unified approach to allow an easy and scalable evolution of SPLs in space and time. The expected contributions will aid the management of SPL products and support engineers to reason about the potential impact of changes during SPL evolution. To evaluate the approach, we plan to conduct case studies with real-world SPLs.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {104–111},
numpages = {8},
keywords = {feature-oriented software development, software evolution, software product lines, version control systems},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2815782.2815799,
author = {Schaefer, Ina and Seidl, Christoph and Cleophas, Loek and Watson, Bruce W.},
title = {SPLicing TABASCO: Custom-Tailored Software Product Line Variants from Taxonomy-Based Toolkits},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815799},
doi = {10.1145/2815782.2815799},
abstract = {Taxonomy-Based Software Construction (TABASCO) applies extensive domain analyses to create conceptual hierarchies of algorithmic domains. Those are used as basis for the implementation of software toolkits. The monolithic structure of TABASCO-based toolkits restricts their adoption on resource-constrained or special-purpose devices. In this paper, we address this problem by applying Software Product Line (SPL) techniques to TABASCO-based toolkits: We use software taxonomies as input to creating a conceptual representation of variability as feature models of an SPL. We apply the variability realization mechanism delta modeling to transform realization artifacts, such as source code, to only contain elements for a particular selection of features. Our method is suitable for proactive, reactive and extractive SPL development so that it supports a seamless adoption and evolution of an SPL approach for TABASCO-based toolkits. We demonstrate the feasibility of the method with three case studies by proactively, reactively and extractively transforming TABASCO-based toolkits to SPLs, which allow derivation of variants with custom-tailored functionality.},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {34},
numpages = {10},
keywords = {Software Product Line (SPL) adoption, Taxonomy-Based Software Construction (TABASCO) toolkit},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@article{10.1287/mnsc.1090.1058,
author = {Wang, Xinfang (Jocelyn) and Camm, Jeffrey D. and Curry, David J.},
title = {A Branch-and-Price Approach to the Share-of-Choice Product Line Design Problem},
year = {2009},
issue_date = {October 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {55},
number = {10},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.1090.1058},
doi = {10.1287/mnsc.1090.1058},
abstract = {We develop a branch-and-price algorithm for constructing an optimal product line using partworth estimates from choice-based conjoint analysis. The algorithm determines the specific attribute levels for each multiattribute product in a set of products to maximize the resulting product line's share of choice, i.e., the number of respondents for whom at least one new product's utility exceeds the respondent's reservation utility. Computational results using large commercial and simulated data sets demonstrate that the algorithm can identify provably optimal, robust solutions to realistically sized problems.},
journal = {Manage. Sci.},
month = oct,
pages = {1718–1728},
numpages = {11},
keywords = {branch and price, column generation, combinatorial optimization, conjoint analysis, integer programming, marketing, optimization, product line design, share of choice}
}

@inproceedings{10.1145/2145204.2145402,
author = {Liu, Xiaoqing (Frank) and Barnes, Eric Christopher and Savolainen, Juha Erik},
title = {Conflict detection and resolution for product line design in a collaborative decision making environment},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145402},
doi = {10.1145/2145204.2145402},
abstract = {Ensuring that the non-functional requirements (NFRs), of a system are satisfied is an essential task in software development. However, this task is complicated by the fact that many NFRs conflict with each other from multiple perspectives. It is essential to resolve conflicts collectively in a collaborative decision making process since stakeholders often disagree on how conflicts should be resolved. In this paper, we describe a method for dividing high-level NFR conflicts within a product line into more manageable sub-problems. Stakeholders make use of an argumentation based collaborative decision support system to determine which design alternatives provide the best trade-offs between NFRs. Finally, we present an empirical study in which the aforementioned system was used to resolve a single instance of an NFR conflict across 3 members of a product line. It shows that the system is effective in resolving conflicts in a collaborative decision process.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {1327–1336},
numpages = {10},
keywords = {Collaboration architectures, collaborative software development, computer-mediated communication, participatory/cooperative design},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.5555/648033.744208,
author = {Muthig, Dirk and Patzke, Thomas},
title = {Generic Implementation of Product Line Components},
year = {2002},
isbn = {3540007377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {An argument pro component-based software development is the idea of constructing software systems by assembling preexisting components instead of redeveloping similar or identical functionality always from scratch. Unfortunately, integrating existing components practically means adaptation and use rather than use only, which makes an ideal component-based development hard to realize in practice. Product line engineering, however, tackles this problem by making components as generic as needed for a particular product family and thus allows component reuse. Such a component covers variabilities and thus its implementation must consider variabilities as well.In this paper, we describe a process for implementing generic product line components and give an overview of variability mechanisms at the implementation level, illustrated by a running example, a generic test component.},
booktitle = {Revised Papers from the International Conference NetObjectDays on Objects, Components, Architectures, Services, and Applications for a Networked World},
pages = {313–329},
numpages = {17},
series = {NODe '02}
}

@inproceedings{10.1145/1138474.1138485,
author = {Yoshimura, Kentaro and Ganesan, Dharmalingam and Muthig, Dirk},
title = {Assessing merge potential of existing engine control systems into a product line},
year = {2006},
isbn = {1595934022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1138474.1138485},
doi = {10.1145/1138474.1138485},
abstract = {Engine Control Systems (ECS) for automobiles have many variants for many manufactures and several markets. To improve their development efficiency, exploiting ECS commonalities and predicting their variability are mandatory. The concept of software product line engineering meets this ECS business background. However, we should carefully investigate the expected technical, economical, and organizational effects of introducing the strategy into existing products. Thereafter, a strategy will be derived systematically and realize the desired benefits.This paper reports an experience with the up-front investigation performed for Hitachi's ECS. We focus on the approach to plan the migration of the existing family of individual systems into a future product line. The approach assesses potential ways of merging software from existing variants and eventually defines a procedure for performing the migration. To get a high quality strategy, we integrate the approach of software measurement, the expertise of software architects, and reverse engineering techniques.},
booktitle = {Proceedings of the 2006 International Workshop on Software Engineering for Automotive Systems},
pages = {61–67},
numpages = {7},
keywords = {clone detection and classification, engine control systems, reverse engineering, software product line},
location = {Shanghai, China},
series = {SEAS '06}
}

@article{10.1007/s10270-020-00839-w,
author = {Pol’la, Matias and Buccella, Agustina and Cechich, Alejandra},
title = {Analysis of variability models: a systematic literature review},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00839-w},
doi = {10.1007/s10270-020-00839-w},
abstract = {Dealing with variability, during Software Product Line Engineering (SPLE), means trying to allow software engineers to develop a set of similar applications based on a manageable range of variable functionalities according to expert users’ needs. Particularly, variability management (VM) is an activity that allows flexibility and a high level of reuse during software development. In the last years, we have witnessed a proliferation of methods, techniques and supporting tools for VM in general, and for its analysis in particular. More precisely, a specific field has emerged, named (automated) variability analysis, focusing on verifying variability models across the SPLE’s phases. In this paper, we introduce a systematic literature review of existing proposals (as primary studies) focused on analyzing variability models. We define a classification framework, which is composed of 20 sub-characteristics addressing general aspects, such as scope and validation, as well as model-specific aspects, such as variability primitives, reasoner type. The framework allows to look at the analysis of variability models during its whole life cycle—from design to derivation—according to the activities involved during an SPL development. Also, the framework helps us answer three research questions defined for showing the state of the art and drawing challenges for the near future. Among the more interesting challenges, we can highlight the needs of more applications in industry, the existence of more mature tools, and the needs of providing more semantics in the way of variability primitives for identifying inconsistencies in the models.},
journal = {Softw. Syst. Model.},
month = aug,
pages = {1043–1077},
numpages = {35},
keywords = {Variability analysis, Software Product Line, Variability management, Supporting tools}
}

@article{10.1016/j.advengsoft.2021.103029,
author = {Nagy, Enikő and Lovas, Róbert and Pintye, István and Hajnal, Ákos and Kacsuk, Péter},
title = {Cloud-agnostic architectures for machine learning based on Apache Spark},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {159},
number = {C},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2021.103029},
doi = {10.1016/j.advengsoft.2021.103029},
journal = {Adv. Eng. Softw.},
month = sep,
numpages = {9},
keywords = {Reference architectures, Big data, Artificial intelligence, Machine learning, Cloud computing, Orchestration, Distributed computing, Stream processing, Spark}
}

@article{10.1016/j.infsof.2012.09.006,
author = {Behjati, Razieh and Yue, Tao and Briand, Lionel and Selic, Bran},
title = {SimPL: A product-line modeling methodology for families of integrated control systems},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.09.006},
doi = {10.1016/j.infsof.2012.09.006},
abstract = {Context: Integrated control systems (ICSs) are heterogeneous systems where software and hardware components are integrated to control and monitor physical devices and processes. A family of ICSs share the same software code base, which is configured differently for each product to form a unique installation. Due to the complexity of ICSs and inadequate automation support, product configuration in this context is typically error-prone and costly. Objective: As a first step to overcome these challenges, we propose a UML-based product-line modeling methodology that provides a foundation for semi-automated product configuration in the specific context of ICSs. Method: We performed a comprehensive domain analysis to identify characteristics of ICS families, and their configuration challenges. Based on this, we formulated the characteristics of an adequate configuration solution, and derived from them a set of modeling requirements for a model-based solution to configuration. The SimPL methodology is proposed to fulfill these requirements. Results: To evaluate the ability of SimPL to fulfill the modeling requirements, we applied it to a large-scale industrial case study. Our experience with the case study shows that SimPL is adequate to provide a model of the product family that meets the modeling requirements. Further evaluation is still required to assess the applicability and scalability of SimPL in practice. Doing this requires conducting field studies with human subjects and is left for future work. Conclusion: We conclude that configuration in ICSs requires better automation support, and UML-based approaches to product family modeling can be tailored to provide the required foundation.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {607–629},
numpages = {23},
keywords = {Integrated control systems, MARTE, Product-line engineering, UML, Variability modeling}
}

@inproceedings{10.1145/3307630.3342704,
author = {Cañete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {energy efficiency, latency, mobile cloud computing, mobile edge computing, optimisation, software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {feature identification, requirement documents, reverse engineering, software product lines, variability extraction},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.5555/1768029.1768057,
author = {Djebbi, Olfa and Salinesi, Camille},
title = {RED-PL, a method for deriving product requirements from a product line requirements model},
year = {2007},
isbn = {9783540729877},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product lines (SPL) modeling has proven to be an effective approach to reuse in software development. Several variability approaches were developed to plan requirements reuse, but only little of them actually address the issue of deriving product requirements. Indeed, while the modeling approaches sell on requirements reuse, the associated derivation techniques actually focus on deriving and reusing technical product data.This paper presents a method that intends to support requirements derivation. Its underlying principle is to take advantage of approaches made for reuse PL requirements and to complete them by a requirements development process by reuse for single products. The proposed approach matches users' product requirements with PL requirements models and derives a collection of requirements that is (i) consistent, and (ii) optimal with respect to users' priorities and company's constraints. The proposed methodological process was validated in an industrial setting by considering the requirement engineering phase of a product line of blood analyzers.},
booktitle = {Proceedings of the 19th International Conference on Advanced Information Systems Engineering},
pages = {279–293},
numpages = {15},
keywords = {derivation, product line, requirements},
location = {Trondheim, Norway},
series = {CAiSE'07}
}

@inproceedings{10.5555/3507788.3507798,
author = {Khan, Md Asif and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Garcon, Sylvain and Tauseef, Qasim},
title = {Failure prediction using machine learning in IBM WebSphere liberty continuous integration environment},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {The growing complexity and dependencies of software have increased the importance of testing to ensure that frequent changes do not adversely affect existing functionality. Moreover, continuous integration comes with unique challenges associated with maintaining a stable build environment. Several studies have shown that the testing environment becomes more efficient with proper test case prioritization techniques. However, an application's dynamic behavior makes it challenging to derive test case prioritization techniques for achieving optimal results. With the advance of machine learning, the context of an application execution can be analyzed to select and prioritize test suites more efficiently.Test suite prioritization techniques aim to reorder test suites' execution to deliver high quality, maintainable software at lower costs to meet specific objectives such as revealing failures earlier. The state-of-the-art techniques on test prioritization in a continuous integration environment focus on relatively small, single-language, unit-tested projects. This paper compares and analyzes Machine learning-based test suite prioritization technique on two large-scale dataset collected from a continuous integration environment Google and IBM respectively. We optimize hyperparameters and report on experiments' findings by using different machine learning algorithms for test suite prioritization. Our optimized algorithms prioritize test suites with 93% accuracy on average and require 20% fewer test suites to detect 80% of the failures than the test suites prioritized randomly.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {CI, continuous integration, machine learning, test prioritization},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1145/3383219.3383229,
author = {Li, Yang and Schulze, Sandro and Xu, Jiahua},
title = {Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383229},
doi = {10.1145/3383219.3383229},
abstract = {In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Feature Extraction, Feature Terms Identification, Requirement Documents, Software Product Lines},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/1964138.1964141,
author = {Tang, Antony and Couwenberg, Wim and Scheppink, Erik and de Burgh, Niels Aan and Deelstra, Sybren and van Vliet, Hans},
title = {SPL migration tensions: an industry experience},
year = {2010},
isbn = {9781450305426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964138.1964141},
doi = {10.1145/1964138.1964141},
abstract = {In a software development environment where legacy software systems have been successfully deployed, there are tensions that deter the organization from moving towards software product line engineering (SPLE). An example is the effort required to develop a product line architecture versus time-to-market pressure or the lack of evidence to justify the benefits of SPLE. In this report we discuss the tensions that exist in Océ Technologies. A reactive software reuse approach has not yielded the desired long-term benefits of reusability. A proactive approach requires knowledge exchange and coordination between software management and technical staff. We describe how such knowledge sharing can ease the tensions and facilitate a SPLE migration process.},
booktitle = {Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering},
articleno = {3},
numpages = {6},
keywords = {agile development process, architecture management, industry case study, software product line engineering},
location = {Reno, Nevada},
series = {KOPLE '10}
}

@article{10.1016/j.jss.2021.111031,
author = {Giray, Görkem},
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111031},
doi = {10.1016/j.jss.2021.111031},
journal = {J. Syst. Softw.},
month = oct,
numpages = {35},
keywords = {Software engineering, Software development, Software process, Machine learning, Deep learning, Systematic literature review}
}

@inproceedings{10.1145/3324884.3415281,
author = {Abdelkader, Hala},
title = {Towards robust production machine learning systems: managing dataset shift},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3415281},
doi = {10.1145/3324884.3415281},
abstract = {The advances in machine learning (ML) have stimulated the integration of their capabilities into software systems. However, there is a tangible gap between software engineering and machine learning practices, that is delaying the progress of intelligent services development. Software organisations are devoting effort to adjust the software engineering processes and practices to facilitate the integration of machine learning models. Machine learning researchers as well are focusing on improving the interpretability of machine learning models to support overall system robustness. Our research focuses on bridging this gap through a methodology that evaluates the robustness of machine learning-enabled software engineering systems. In particular, this methodology will automate the evaluation of the robustness properties of software systems against dataset shift problems in ML. It will also feature a notification mechanism that facilitates the debugging of ML components.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1164–1166},
numpages = {3},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1016/S0164-1212(03)00013-X,
author = {Zelkowitz, Marvin V. and Rus, Ioana},
title = {Defect evolution in a product line environment},
year = {2004},
issue_date = {February, 2004},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {70},
number = {1–2},
issn = {0164-1212},
url = {https://doi.org/10.1016/S0164-1212(03)00013-X},
doi = {10.1016/S0164-1212(03)00013-X},
abstract = {One mechanism used for monitoring the development of the Space Shuttle flight control software, in order to minimize any risks to the missions, is the independent verification and validation (IV&amp;V) process. Using data provided by both the Shuttle software developer and the IV&amp;V contractor, in this paper we describe the overall IV&amp;V process as used on the Space Shuttle program and provide an analysis of the use of metrics to document and control this process over multiple releases of this software. Our findings reaffirm the value of IV&amp;V, show the impact of IV&amp;V on multiple releases of a large complex software system, and indicate that some of the traditional measures of defect detection and repair are not applicable in a multiple-release environment such as this one.},
journal = {J. Syst. Softw.},
month = feb,
pages = {143–154},
numpages = {12},
keywords = {Evolutionary software, Life and mission critical software, Metrics, Process characterization, Product line development, Software independent verification and validation, Software safety and reliability, Space Shuttle program}
}

@inproceedings{10.1145/2430502.2430522,
author = {von Rhein, Alexander and Apel, Sven and Kästner, Christian and Thüm, Thomas and Schaefer, Ina},
title = {The PLA model: on the combination of product-line analyses},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430522},
doi = {10.1145/2430502.2430522},
abstract = {Product-line analysis has received considerable attention in the last decade. As it is often infeasible to analyze each product of a product line individually, researchers have developed analyses, called variability-aware analyses, that consider and exploit variability manifested in a code base. Variability-aware analyses are often significantly more efficient than traditional analyses, but each of them has certain weaknesses regarding applicability or scalability. We present the Product-Line-Analysis model, a formal model for the classification and comparison of existing analyses, including traditional and variability-aware analyses, and lay a foundation for formulating and exploring further, combined analyses. As a proof of concept, we discuss different examples of analyses in the light of our model, and demonstrate its benefits for systematic comparison and exploration of product-line analyses.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {14},
numpages = {8},
keywords = {PLA model, product-line analysis, software product lines},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {AutoML, NAS, configuration search, feature model, neural architecture search},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/1964138.1964140,
author = {Leitner, Andrea and Kreiner, Christian},
title = {Managing ERP configuration variants: an experience report},
year = {2010},
isbn = {9781450305426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964138.1964140},
doi = {10.1145/1964138.1964140},
abstract = {The concepts of Software Product Line Engineering (SPLE) have been adapted and applied to enterprise IT systems, in particular the ERP systems of a production company. Based on a 2-layer feature model for the domain of the company's business processes, individual, albeit similar division's ERP system configurations can be derived by feature selection forming a variant description model. It is indicated that regular release upgrades can also benefit from the SPLE approach.The customization capabilities of the ERP platform are captured in another model; building up this model is automated according to information extracted online. As well, customizing an ERP system -- based on the models mentioned - is performed online with the help of a connector developed in this project.Quantitative analysis and lessons learned during the project conclude this experience report.},
booktitle = {Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering},
articleno = {2},
numpages = {6},
keywords = {IT management, enterprise resource planning, experience report, software product line engineering},
location = {Reno, Nevada},
series = {KOPLE '10}
}

@inproceedings{10.1145/2934466.2962727,
author = {Zhang, Bo and Becker, Martin},
title = {Supporting product configuration in application engineering using EXConfig},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2962727},
doi = {10.1145/2934466.2962727},
abstract = {Nowadays systems are often developed following the product line approach in order to reduce time to market, achieve lower cost, and ensure high quality. To this end, common and variable requirements of product variants are specified as features in a feature model, so that each product variant can be configured and customized along the development lifecycle. While developers in family engineering tend to use variability management tools (e.g., pure::variants) for feature modeling and developing core assets for reuse, such sophisticated tools might be too complicated and inappropriate for product configuration in application engineering. In order to solve this challenge, this paper introduces an Excel-based product configurator called EXConfig, which focuses on product line features in the problem space and supports staged product configuration in application engineering. This tool can be easily customized and integrated with other system design tools or variability management tools, which connects application engineering and family engineering in development. The usage of this tool and its integration have been validated several times in industry.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {324–327},
numpages = {4},
keywords = {enterprise architect, excel, feature model, variability configuration},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and Térnava, Xhevahire and Blouin, Arnaud and Jézéquel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3236405.3236425,
author = {Hinterreiter, Daniel},
title = {Supporting feature-oriented development and evolution in industrial software ecosystems},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236425},
doi = {10.1145/3236405.3236425},
abstract = {Companies nowadays need to serve a mass market while at the same time customers request highly individual solutions. To handle this problem, development is frequently organized in software ecosystems (SECOs), i.e., interrelated software product lines involving internal and external developers. Individual products for customers are derived and adapted by adding new features or creating new versions of existing features to meet the customer-specific requirements. Development teams typically use version control systems to track fine-grained, implementation-level changes to product lines and products. However, it is difficult to relate such low-level changes to features and their evolution in the SECO. State-of-the-art approaches addressing this issue are variation control systems, which allow tracking of changes at the level of features. However, these systems have not found their way into mainstream development so far. In this thesis we will describe which workflows and additions to variation control systems are required to support feature-oriented development in an industrial SECO environment. We will further investigate mechanisms that support feature-based monitoring to guide the evolution in SECOs.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {79–86},
numpages = {8},
keywords = {configuration management, software evolution, software product lines, variation control systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/375212.375271,
author = {Niemelä, Eila and Ihme, Tuomas},
title = {Product line software engineering of embedded systems},
year = {2001},
isbn = {1581133588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375212.375271},
doi = {10.1145/375212.375271},
abstract = {In order to be able to determine whether the product line approach is suitable, a company needs to analyse its business drivers, commonality of existing products, domain knowledge owned by the engineering staff, and quality of the representations of existing software artefacts. In this paper we present evaluation criteria for the development of a product line and give an overview of the current state of practices in the embedded software area. Evaluation criteria are divided into three classes. Business drivers of a product line are defined by analysing product assortment and business manners. Domains and personnel are considered in the analysis of the preconditions and targets of a product line. In the development of core assets, elements that affect assets engineering are considered as  well as the mechanisms needed in their maintenance. A product line architecture that brings about a balance between sub- domains and their most important properties is an investment that must be looked after. However, the subdomains need flexibility to use, change and manage their own technologies, and evolve separately, but in a controlled way.},
booktitle = {Proceedings of the 2001 Symposium on Software Reusability: Putting Software Reuse in Context},
pages = {118–125},
numpages = {8},
keywords = {domain engineering, product features, product line architecture, quality attributes},
location = {Toronto, Ontario, Canada},
series = {SSR '01}
}

@inproceedings{10.1145/2491627.2491631,
author = {Myllärniemi, Varvana and Savolainen, Juha and Männistö, Tomi},
title = {Performance variability in software product lines: a case study in the telecommunication domain},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491631},
doi = {10.1145/2491627.2491631},
abstract = {In the research on software product lines, product variants typically differ by their functionality, and quality attributes are more or less similar across products. To accumulate empirical evidence, this paper presents a descriptive case study of performance variability in a software product line of mobile network base stations. The goal is to study the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The results highlight that the evolution of customer needs motivates performance variability; performance variability can be realized either with software or hardware variability strategy, with the latter often being prevailing; and the software strategy can be kept focused by downgrading performance.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {32–41},
numpages = {10},
keywords = {architecture, case study, software product line, variability},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inbook{10.5555/3454287.3455252,
author = {Jeong, Jisoo and Lee, Seungeui and Kim, Jeesoo and Kwak, Nojun},
title = {Consistency-based semi-supervised learning for object detection},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Specifically, the consistency constraint is applied not only for object classification but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage and two-stage detectors and the results show the effectiveness of our method.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {965},
numpages = {10}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, José A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {configurable systems, feature models, monte carlo tree search, software product lines, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inbook{10.5555/1980562.1980565,
author = {Heidenreich, Florian and Sánchez, Pablo and Santos, João and Zschaler, Steffen and Alférez, Mauricio and Araújo, João and Fuentes, Lidia and Kulesza, Uirá and Moreira, Ana and Rashid, Awais},
title = {Relating feature models to other models of a software product line: a comparative study of featuremapper and VML},
year = {2010},
isbn = {3642160859},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product lines using feature models often require the relation between feature models in problem space and the models used to describe the details of the product line to be expressed explicitly. This is particularly important, where automatic product derivation is required. Different approaches for modelling this mapping have been proposed in the literature. However, a discussion of their relative benefits and drawbacks is currently missing. As a first step towards a better understanding of this field, this paper applies two of these approaches-- FeatureMapper as a representative of declarative approaches and VML* as a representative of operational approaches--to the case study. We show in detail how the case study can be expressed using these approaches and discuss strengths and weaknesses of the two approaches with regard to the case study.},
booktitle = {Transactions on Aspect-Oriented Software Development VII: A Common Case Study for Aspect-Oriented Modeling},
pages = {69–114},
numpages = {46}
}

@inproceedings{10.5555/648114.748898,
author = {Svahnberg, Mikael and Mattsson, Michael},
title = {Conditions and Restrictions for Product Line Generation Migration},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we describe a case study of a company in the domain of automatic guided vehicles (AGVs) that is in the process of migrating from a previous generation of software product line, which has mainly been centered around hardware, into a new product line generation, which will be software-centered. We describe the issues motivating this transition, and the factors that complicate it. Moreover, we present a three stage process for migrating into a new software product line. This process is currently initiated in collaboration with the aforementioned company.},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {143–154},
numpages = {12},
series = {PFE '01}
}

@article{10.1504/IJCSM.2017.083753,
title = {Adaptability-oriented hierarchical correlation optimisation in product family design},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {8},
number = {2},
issn = {1752-5055},
url = {https://doi.org/10.1504/IJCSM.2017.083753},
doi = {10.1504/IJCSM.2017.083753},
abstract = {For leader-follower characteristic between platform parameters and customisation parameters in product family design, a bi-level model and leaderfollower correlation optimisation model are developed. The adaptability of platform parameters is considered, and they are regarded as association parameters to transfer between the upper and lower in the model. After the values of platform variables in the upper level model are assigned, each sub-plan problem in the lower level model can be solved without relying on platform parameters under certain conditions. Simultaneously, incidence relation of platform parameters could be taken as relaxation constraints, and genetic algorithm is applied to solve the leader-follower correlation optimisation model of the product family. Finally, an example on optimal design of product family for drum group is used to demonstrate the feasibility of the established model and proposed method.},
journal = {Int. J. Comput. Sci. Math.},
month = jan,
pages = {146–156},
numpages = {11}
}

@inproceedings{10.1145/3382026.3425776,
author = {Michelon, Gabriela Karoline and Obermann, David and Assunção, Wesley Klewerton Guez and Linsbauer, Lukas and Grünbacher, Paul and Egyed, Alexander},
title = {Mining Feature Revisions in Highly-Configurable Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425776},
doi = {10.1145/3382026.3425776},
abstract = {Highly-Configurable Software Systems (HCSSs) support the systematic evolution of systems in space, i.e., the inclusion of new features, which then allow users to configure software products according to their needs. However, HCSSs also change over time, e.g., when adapting existing features to new hardware or platforms. In practice, HCSSs are thus developed using both version control systems (VCSs) and preprocessor directives (#ifdefs). However, the use of a preprocessor as variability mechanism has been criticized regarding the separation of concerns and code obfuscation, which complicates the analysis of HCSS evolution in VCSs. For instance, a single commit may contain changes of totally unrelated features, which may be scattered over many variation points (#ifdefs), thus making the evolution history hard to understand. This complexity often leads to error-prone changes and high costs for maintenance and evolution. In this paper, we propose an automated approach to mine HCSS features taking into account evolution in space and time. Our approach uses constraint satisfaction problem solving to mine newly introduced, removed and changed features. It finds a configuration containing the feature revisions which are needed to activate a specific program location. Furthermore, it increments the revision number of each changed feature. Thus, our approach enables to analyze when and which features often change over time, as well as their interactions, for every single commit of a HCSS. Our approach can contribute to future research on understanding the characteristics of HCSS and supporting developers during maintenance and evolution tasks.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {74–78},
numpages = {5},
keywords = {feature evolution, preprocessors, repository mining, software product lines, system evolution, version control systems},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342398,
author = {Beek, Maurice H. ter and Schmid, Klaus and Eichelberger, Holger},
title = {Textual Variability Modeling Languages: An Overview and Considerations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342398},
doi = {10.1145/3307630.3342398},
abstract = {During the three decades since the invention of the first variability modeling approach [28], there have been multiple attempts to introduce advanced variability modeling capabilities. More recently, we have seen increased attention on textual variability modeling languages. In this paper, we summarize the main capabilities of state of the art textual variability modeling languages, based on [23], including updates regarding more recent work. Based on this integrated characterization, we provide a discussion of additional concerns, opportunities and challenges that are relevant for designing future (textual) variability modeling languages. The paper also summarizes relevant contributions by the authors as input to further discussions on future (textual) variability modeling languages.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {151–157},
numpages = {7},
keywords = {software product lines, textual specification languages, variability modeling},
location = {Paris, France},
series = {SPLC '19}
}

@inbook{10.5555/1986548.1986551,
author = {Heidenreich, Florian and Sánchez, Pablo and Santos, João and Zschaler, Steffen and Alférez, Mauricio and Araújo, João and Fuentes, Lidia and Kulesza, Uirá and Moreira, Ana and Rashid, Awais},
title = {Relating feature models to other models of a software product line: a comparative study of featuremapper and VML},
year = {2010},
isbn = {3642160859},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product lines using feature models often require the relation between feature models in problem space and the models used to describe the details of the product line to be expressed explicitly. This is particularly important, where automatic product derivation is required. Different approaches for modelling this mapping have been proposed in the literature. However, a discussion of their relative benefits and drawbacks is currently missing. As a first step towards a better understanding of this field, this paper applies two of these approaches-- FeatureMapper as a representative of declarative approaches and VML* as a representative of operational approaches--to the case study. We show in detail how the case study can be expressed using these approaches and discuss strengths and weaknesses of the two approaches with regard to the case study.},
booktitle = {Transactions on Aspect-Oriented Software Development VII: A Common Case Study for Aspect-Oriented Modeling},
pages = {69–114},
numpages = {46}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannemüller, Martin and Matthé, Michael and Schürr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {dynamic software product lines, machine learning, performance-influence models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.2478/acss-2014-0003,
author = {Bartusevics, Arturs and Novickis, Leonids and Bluemel, Eberhard},
title = {Intellectual Model-Based Configuration Management Conception},
year = {2014},
issue_date = {7 2014},
publisher = {Walter de Gruyter GmbH},
address = {Berlin, DEU},
volume = {15},
number = {1},
issn = {2255-8691},
url = {https://doi.org/10.2478/acss-2014-0003},
doi = {10.2478/acss-2014-0003},
abstract = {Abstract Software configuration management is one of the most important disciplines within the software development project, which helps control the software evolution process and allows including into the end project only tested and validated changes. To achieve this, software management completes certain tasks. Concrete tools are used for technical implementation of tasks, such as version control systems, servers of continuous integration, compilers, etc. A correct configuration management process usually requires several tools, which mutually exchange information by generating various kinds of transfers. When it comes to introducing the configuration management process, often there are situations when tool installation is started, yet at that given moment there is no general picture of the total process. The article offers a model-based configuration management concept, which foresees the development of an abstract model for the configuration management process that later is transformed to lower abstraction level models and tools are indicated to support the technical process. A solution of this kind allows a more rational introduction and configuration of tools},
journal = {Appl. Comput. Syst.},
month = jul,
pages = {22–27},
numpages = {6},
keywords = {Configuration management, configuration management model, model-based approach}
}

@inproceedings{10.1145/3307630.3342419,
author = {Ghofrani, Javad and Kozegar, Ehsan and Bozorgmehr, Arezoo and Soorati, Mohammad Divband},
title = {Reusability in Artificial Neural Networks: An Empirical Study},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342419},
doi = {10.1145/3307630.3342419},
abstract = {Machine learning, especially deep learning has aroused interests of researchers and practitioners for the last few years in development of intelligent systems such as speech, natural language, and image processing. Software solutions based on machine learning techniques attract more attention as alternatives to conventional software systems. In this paper, we investigate how reusability techniques are applied in implementation of artificial neural networks (ANNs). We conducted an empirical study with an online survey among experts with experience in developing solutions with ANNs. We analyze the feedback of more than 100 experts to our survey. The results show existing challenges and some of the applied solutions in an intersection between reusability and ANNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {122–129},
numpages = {8},
keywords = {artificial neural networks, empirical study, reusability, survey, systematic reuse},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233049,
author = {Horcas, Jose-Miguel and Cortiñas, Alejandro and Fuentes, Lidia and Luaces, Miguel R.},
title = {Integrating the common variability language with multilanguage annotations for web engineering},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233049},
doi = {10.1145/3233027.3233049},
abstract = {Web applications development involves managing a high diversity of files and resources like code, pages or style sheets, implemented in different languages. To deal with the automatic generation of custom-made configurations of web applications, industry usually adopts annotation-based approaches even though the majority of studies encourage the use of composition-based approaches to implement Software Product Lines. Recent work tries to combine both approaches to get the complementary benefits. However, technological companies are reticent to adopt new development paradigms such as feature-oriented programming or aspect-oriented programming. Moreover, it is extremely difficult, or even impossible, to apply these programming models to web applications, mainly because of their multilingual nature, since their development involves multiple types of source code (Java, Groovy, JavaScript), templates (HTML, Markdown, XML), style sheet files (CSS and its variants, such as SCSS), and other files (JSON, YML, shell scripts). We propose to use the Common Variability Language as a composition-based approach and integrate annotations to manage fine grained variability of a Software Product Line for web applications. In this paper, we (i) show that existing composition and annotation-based approaches, including some well-known combinations, are not appropriate to model and implement the variability of web applications; and (ii) present a combined approach that effectively integrates annotations into a composition-based approach for web applications. We implement our approach and show its applicability with an industrial real-world system.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {196–207},
numpages = {12},
keywords = {CVL, SPL, annotations, automation, composition, variability, web engineering},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.compind.2015.07.006,
author = {Bonev, Martin and Hvam, Lars and Clarkson, John and Maier, Anja},
title = {Formal computer-aided product family architecture design for mass customization},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {74},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2015.07.006},
doi = {10.1016/j.compind.2015.07.006},
abstract = {Conventional methods for product family architecture design are informal and limit the support for architecture communication, synthesis and documentation.A formal computational approach requires the integration to configuration systems, improved generic models and extended structural analysis methods.Product family architectures can be modelled interchangeably with generic design-structure matrixes, generic node-link diagrams and product variant masters.Supportive tools automate the documentation and formalize the synthesis of architectures, thereby making any decision about a preferred solution explicit and transparent. With product customization companies aim at creating higher customer value and stronger economic benefits. The profitability of the offered variety relies on the quality of the developed product family architectures and their consistent implementation in configuration systems. Yet existing methods are informal, providing limited support for domain experts to communicate, synthesize and document architectures effectively. In single product design explicit visual models such as design structure matrices and node-link diagrams have been used in combination with structural analysis methods to overcome the limitation of the informal approach. Drawing on thereto established best practises, this paper evaluates and extends the relevant methods and modelling techniques, to create a consistent and formal approach for the design and customization of entire product families. To validate it's applicability, the approach is tested on a case study at a manufacturing company offering bespoke industrial applications. A generic modelling method termed the integrated design model (IDM) is developed and complemented with a computational structural analysis method, to assist domain experts in their daily work. When combined with a configuration system, the presented IDM tool automates the documentation and formalizes the synthesis of architectures, thereby making any decision about a preferred solution explicit and transparent.},
journal = {Comput. Ind.},
month = dec,
pages = {58–70},
numpages = {13},
keywords = {Design synthesis, Generic modelling, Mass customization, Product family architecture, Structural analysis}
}

@article{10.1155/2021/9976306,
author = {Wang, Wei and Wu, Wenqing},
title = {Using Machine Learning Algorithms to Recognize Shuttlecock Movements},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9976306},
doi = {10.1155/2021/9976306},
abstract = {Shuttlecock is an excellent traditional national sport in China. Because of its simplicity, convenience, and fun, it is loved by the broad masses of people, especially teenagers and children. The development of shuttlecock sports into a confrontational event is not long, and it takes a period of research to master the tactics and strategies of shuttlecock sports. Based on this, this article proposes the use of machine learning algorithms to recognize the movement of shuttlecock movements, aiming to provide more theoretical and technical support for shuttlecock competitions by identifying features through actions with the assistance of technical algorithms. This paper uses literature research methods, model methods, comparative analysis methods, and other methods to deeply study the motion characteristics of shuttlecock motion, the key algorithms of machine learning algorithms, and other theories and construct the shuttlecock motion recognition based on multiview clustering algorithm. The model analyzes the robustness and accuracy of the machine learning algorithm and other algorithms, such as a variety of performance comparisons, and the results of the shuttlecock motion recognition image. For the key movements of shuttlecock movement, disk, stretch, hook, wipe, knock, and abduction, the algorithm proposed in this paper has a good movement recognition rate, which can reach 91.2%. Although several similar actions can be recognized well, the average recognition accuracy rate can exceed 75%, and even through continuous image capture, the number of occurrences of the action can be automatically analyzed, which is beneficial to athletes. And the coach can better analyze tactics and research strategies.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/3336294.3336307,
author = {Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Simao, Adenilso},
title = {Learning from Difference: An Automated Approach for Learning Family Models from Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336307},
doi = {10.1145/3336294.3336307},
abstract = {Substantial effort has been spent on extending specification notations and their associated reasoning techniques to software product lines (SPLs). Family-based analysis techniques operate on a single artifact, referred to as a family model, that is annotated with variability constraints. This modeling approach paves the way for efficient model-based testing and model checking for SPLs. Albeit reasonably efficient, the creation and maintenance of family models tend to be time consuming and error-prone, especially if there are crosscutting features. To tackle this issue, we introduce FFSMDiff, a fully automated technique to learn featured finite state machines (FFSM), a family-based formalism that unifies Mealy Machines from SPLs into a single representation. Our technique incorporates variability to compare and merge Mealy machines and annotate states and transitions with feature constraints. We evaluate our technique using 34 products derived from three different SPLs. Our results support the hypothesis that families of Mealy machines can be effectively merged into succinct FFSMs with fewer states, especially if there is high feature sharing among products. These indicate that FFSMDiff is an efficient family-based model learning technique.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {52–63},
numpages = {12},
keywords = {150% model, family model, model learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {Software engineering, computer-aided software engineering, software variability}
}

@inproceedings{10.1145/3307630.3342388,
author = {Fantechi, Alessandro and Gnesi, Stefania and Semini, Laura},
title = {Applying the QuARS Tool to Detect Variability},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342388},
doi = {10.1145/3307630.3342388},
abstract = {In this demo paper we present how to use the QuARS tool to extract variability information from requirements documents. The main functionality of QuARS is to detect ambiguity in Natural Language (NL) requirement documents.Ambiguity in requirements may be due to intentional or unintentional indication of possible variability; an ambiguity detecting tool can hence be useful to analysts and clients to figure the potential of a requirements document to describe a family of different products.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {29–32},
numpages = {4},
keywords = {ambiguity, natural language requirements, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/1147249.1147254,
author = {Fischbein, Dario and Uchitel, Sebastian and Braberman, Victor},
title = {A foundation for behavioural conformance in software product line architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147254},
doi = {10.1145/1147249.1147254},
abstract = {Software product lines or families represent an emerging paradigm that is enabling companies to engineer applications with similar functionality and user requirements more effectively. Behaviour modelling at the architecture level has the potential for supporting behaviour analysis of entire product lines, as well as defining optional and variable behaviour for different products of a family. However, to do so rigorously, a well defined notion of behavioural conformance of a product to its product line must exist. In this paper we provide a discussion on the shortcomings of traditional behaviour modelling formalisms such as Labelled Transition Systems for characterising conformance and propose Modal Transition Systems as an alternative. We discuss existing semantics for such models, exposing their limitations and finally propose a novel semantics for Modal Transition Systems, branching semantics, that can provide the formal underpinning for a notion of behaviour conformance for software product line architectures.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {39–48},
numpages = {10},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1007/11763864_9,
author = {Moon, Mikyeong and Chae, Heung Seok and Yeom, Keunhyuk},
title = {A metamodel approach to architecture variability in a product line},
year = {2006},
isbn = {3540346066},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11763864_9},
doi = {10.1007/11763864_9},
abstract = {Architecture describes the organizational structure of a system including components, interactions, and constraints. Reusable components, units of software systems, have been considered to support a considerable improvement in reducing development costs and time to market because their interfaces and functionality are explicitly defined. Instead of reusing an individual component, however, it is much more advantageous to reuse a whole design or architecture. A domain architecture, sharing a common software architecture across a product line, includes common components shared by all products and optional components exploited by a subset of the products. Variability, one of the key concepts in the development of domain architectures indicates the ability to derive various products from the product line. Clearly, we need to support variability during domain architecture development. In this paper, we suggest a metamodeling concept that enables a common under-standing of architecture variability. The domain architecture metamodel reflects the Object Management Group's (OMGTM) Reusable Asset Specification (RAS) which addresses the engineering elements of reuse. We describe a domain architecture in which commonality and variability are explicitly considered.},
booktitle = {Proceedings of the 9th International Conference on Reuse of Off-the-Shelf Components},
pages = {115–126},
numpages = {12},
location = {Turin, Italy},
series = {ICSR'06}
}

@article{10.1016/j.knosys.2017.02.020,
author = {Prez-Ortiz, M. and Gutirrez, P.A. and Aylln-Tern, M.D. and Heaton, N. and Ciria, R. and Briceo, J. and Hervs-Martnez, C.},
title = {Synthetic semi-supervised learning in imbalanced domains},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.020},
doi = {10.1016/j.knosys.2017.02.020},
abstract = {Liver transplantation is a promising and widely-accepted treatment for patients with terminal liver disease. However, transplantation is restricted by the lack of suitable donors, resulting in significant waiting list deaths. This paper proposes a novel donor-recipient allocation system that uses machine learning to predict graft survival after transplantation using a dataset comprised of donor-recipient pairs from the Kings College Hospital (United Kingdom). The main novelty of the system is that it tackles the imbalanced nature of the dataset by considering semi-supervised learning, analysing its potential for obtaining more robust and equitable models in liver transplantation. We propose two different sources of unsupervised data for this specific problem (recent transplants and virtual donor-recipient pairs) and two methods for using these data during model construction (a semi-supervised algorithm and a label propagation scheme). The virtual pairs and the label propagation method are shown to alleviate the imbalanced distribution. The results of our experiments show that the use of synthetic and real unsupervised information helps to improve and stabilise the performance of the model and leads to fairer decisions with respect to the use of only supervised data. Moreover, the best model is combined with the Model for End-stage Liver Disease score (MELD), which is at the moment the most popular assignation methodology worldwide. By doing this, our decision-support system considers both the compatibility of the donor and the recipient (by our prediction system) and the recipient severity (via the MELD score), supporting then the principles of fairness and benefit.},
journal = {Know.-Based Syst.},
month = may,
pages = {75–87},
numpages = {13},
keywords = {Imbalanced classification, Liver transplantation, Machine learning, Semi-supervised learning, Support vector machines, Survival analysis, Transplant recipient}
}

@inproceedings{10.1145/2019136.2019186,
author = {Corrêa, Chessman K. F.},
title = {Towards automatic consistency preservation for model-driven software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019186},
doi = {10.1145/2019136.2019186},
abstract = {Model-Driven Software Product Line Engineering (MD-SPLE) is the combination of Model-Driven Software Development and Software Product Line Engineering. In this paradigm, there is a strong dependency relationship between meta-models, models, transformation specifications and traceability links. Moreover, there are also dependency relationships between core assets and product specific artifacts, which raise dependency complexity. Due to the pressure to release new product versions and the quantity of dependent elements, there is a probability that models and related artifacts are not updated at all and become inconsistent with each other. The proposal of this thesis is to keep MD-SPLE models, meta-models, transformation specifications and traceability links consistent.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {43},
numpages = {7},
keywords = {consistency, evolution, maintenance, model-driven, software product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1109/WAIN52551.2021.00028,
author = {Lewis, Grace A. and Bellomo, Stephany and Ozkaya, Ipek},
title = {Characterizing and Detecting Mismatch in Machine-Learning-Enabled Systems},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00028},
doi = {10.1109/WAIN52551.2021.00028},
abstract = {Increasing availability of machine learning (ML) frameworks and tools, as well as their promise to improve solutions to data-driven decision problems, has resulted in popularity of using ML techniques in software systems. However, end-to-end development of ML-enabled systems, as well as their seamless deployment and operations, remain a challenge. One reason is that development and deployment of ML-enabled systems involves three distinct workflows, perspectives, and roles, which include data science, software engineering, and operations. These three distinct perspectives, when misaligned due to incorrect assumptions, cause ML mismatches which can result in failed systems. We conducted an interview and survey study where we collected and validated common types of mismatches that occur in end-to-end development of ML-enabled systems. Our analysis shows that how each role prioritizes the importance of relevant mismatches varies, potentially contributing to these mismatched assumptions. In addition, the mismatch categories we identified can be specified as machine readable descriptors contributing to improved ML-enabled system development. In this paper, we report our findings and their implications for improving end-to-end ML-enabled system development.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {133–140},
numpages = {8},
location = {Madrid, Spain}
}

@inproceedings{10.1145/3307630.3342384,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {MetricHaven: More than 23,000 Metrics for Measuring Quality Attributes of Software Product Lines},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342384},
doi = {10.1145/3307630.3342384},
abstract = {Variability-aware metrics are designed to measure qualitative aspects of software product lines. As we identified in a prior SLR [6], there exist already many metrics that address code or variability separately, while the combination of both has been less researched. MetricHaven fills this gap, as it extensively supports combining information from code files and variability models. Further, we also enable the combination of well established single system metrics with novel variability-aware metrics, going beyond existing variability-aware metrics. Our tool supports most prominent single system and variability-aware code metrics. We provide configuration support for already implemented metrics, resulting in 23,342 metric variations. Further, we present an abstract syntax tree developed for MetricHaven, that allows the realization of additional code metrics.Tool: https://github.com/KernelHaven/MetricHavenVideo: https://youtu.be/vPEmD5Sr6gM},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {25–28},
numpages = {4},
keywords = {SPL, feature models, implementation, metrics, software product lines, variability models},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Exploit, Feature Model, Variability Model, Vulnerability, Vulnerability Analysis and Management},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342400,
author = {Kamali, Seiede Reyhane and Kasaei, Shirin and Lopez-Herrejon, Roberto E.},
title = {Answering the Call of the Wild? Thoughts on the Elusive Quest for Ecological Validity in Variability Modeling},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342400},
doi = {10.1145/3307630.3342400},
abstract = {Ecological validity is a term commonly used in several disciplines to refer to the fact that in a research study, the methods, the materials, and the settings must approximate the real world, i.e. what happens in everyday life. Variability modeling is no exception, it has striven for this form of validity by looking at two main sources, industrial projects and open source projects. Despite their unquestionable value, industrial projects inherently pose limitations; for instance, in terms of open access or results replication, which are two important tenets for any scientific endeavor. In this paper, we present our first findings on the use of open source projects in variability modeling research, and identify trends and avenues for further research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {143–150},
numpages = {8},
keywords = {feature models, open source projects, variability models},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/566493.1148027,
author = {García, Francisco José and Barras, Juan-Antonio and Laguna, Miguel Ángel and Marqués, Jos´e Manuel},
title = {Product line variability support by FORM and Mecano model integration},
year = {2002},
issue_date = {January 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/566493.1148027},
doi = {10.1145/566493.1148027},
abstract = {A product line definition must cover several systems, for this reason additional requirements are included as product line assets during domain engineering. Generic assets are presented to cover all components the product line instances are built from, and their corresponding composition rules. These generic assets embrace common and variable product aspects supporting the variability in product line definition and instantiation.This paper is devoted to present the problem of handling product line variability in every life-cycle stage by the integration of the ideas of the domain engineering method FORM (Feature-Oriented Reuse Method) and the Mecano Model, which defines a coarse-grained reusable element structure.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {35–38},
numpages = {4},
keywords = {FORM, Mecano model, coarse-grained asset, domain engineering, product line variability, reusability, software product line, traceability}
}

@inproceedings{10.1145/2815400.2815401,
author = {Tang, Chunqiang and Kooburat, Thawan and Venkatachalam, Pradeep and Chander, Akshay and Wen, Zhe and Narayanan, Aravind and Dowell, Patrick and Karl, Robert},
title = {Holistic configuration management at Facebook},
year = {2015},
isbn = {9781450338349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815400.2815401},
doi = {10.1145/2815400.2815401},
abstract = {Facebook's web site and mobile apps are very dynamic. Every day, they undergo thousands of online configuration changes, and execute trillions of configuration checks to personalize the product features experienced by hundreds of million of daily active users. For example, configuration changes help manage the rollouts of new product features, perform A/B testing experiments on mobile devices to identify the best echo-canceling parameters for VoIP, rebalance the load across global regions, and deploy the latest machine learning models to improve News Feed ranking. This paper gives a comprehensive description of the use cases, design, implementation, and usage statistics of a suite of tools that manage Facebook's configuration end-to-end, including the frontend products, backend systems, and mobile apps.},
booktitle = {Proceedings of the 25th Symposium on Operating Systems Principles},
pages = {328–343},
numpages = {16},
location = {Monterey, California},
series = {SOSP '15}
}

@inproceedings{10.1145/2934466.2934486,
author = {Santos, Alcemir Rodrigues and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {RiPLE-HC: javascript systems meets spl composition},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934486},
doi = {10.1145/2934466.2934486},
abstract = {Context. Software Product Lines (SPL) engineering is increasingly being applied to handle variability in industrial software systems. Problem. The research community has pointed out a series of benefits which modularity brings to software composition, a key aspect in SPL engineering. However, in practice, the reuse in Javascript-based systems relies on the use of package managers (e.g., npm, jam, bower, requireJS), but these approaches do not allow the management of project features. Method. This paper presents the RiPLE-HC, a strategy aimed at blending compositional and annotative approaches to implement variability in Javascript-based systems. Results. We applied the approach in an industrial environment and conducted an academic case study with six open-source systems to evaluate its robustness and scalability. Additionally, we carried a controlled experiment to analyze the impact of the RiPLE-HC code organization on the feature location maintenance tasks. Conclusion. The empirical evaluations yielded evidence of reduced effort in feature location, and positive benefits when introducing systematic reuse aspects in Javascript-based systems.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {154–163},
numpages = {10},
keywords = {eclipse plugin, feature composition, featureIDE, software product line engineering, web systems domain},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1109/ICSE43902.2021.00100,
author = {Velez, Miguel and Jamshidi, Pooyan and Siegmund, Norbert and Apel, Sven and Kästner, Christian},
title = {White-Box Analysis over Machine Learning: Modeling Performance of Configurable Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00100},
doi = {10.1109/ICSE43902.2021.00100},
abstract = {Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly accurate performance-influence models to the most accurate and expensive black-box approach, but at a reduced cost and with additional benefits from interpretable and local models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1072–1084},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3382025.3414965,
author = {Young, Jeffrey M. and Walkingshaw, Eric and Thüm, Thomas},
title = {Variational satisfiability solving},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414965},
doi = {10.1145/3382025.3414965},
abstract = {Incremental satisfiability (SAT) solving is an extension of classic SAT solving that allows users to efficiently solve a set of related SAT problems by identifying and exploiting shared terms. However, using incremental solvers effectively is hard since performance is sensitive to a problem's structure and the order sub-terms are fed to the solver, and the burden to track results is placed on the end user. For analyses that generate sets of related SAT problems, such as those in software product lines, incremental SAT solvers are either not used at all, used but not explicitly stated so in the literature, or used but suffer from the aforementioned usability problems. This paper translates the ordering problem to an encoding problem and automates the use of incremental SAT solving. We introduce variational SAT solving, which differs from incremental SAT solving by accepting all related problems as a single variational input and returning all results as a single variational output. Our central idea is to make explicit the operations of incremental SAT solving, thereby encoding differences between related SAT problems as local points of variation. Our approach automates the interaction with the incremental solver and enables methods to automatically optimize sharing of the input. To evaluate our methods we construct a prototype variational SAT solver and perform an empirical analysis on two real-world datasets that applied incremental solvers to software evolution scenarios. We show, assuming a variational input, that the prototype solver scales better for these problems than naive incremental solving while also removing the need to track individual results.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {18},
numpages = {12},
keywords = {choice calculus, satisfiability solving, software product lines, variation},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1007/978-3-030-26619-6_16,
author = {Tello, Ghalia and Gianini, Gabriele and Mizouni, Rabeb and Damiani, Ernesto},
title = {Machine Learning-Based Framework for Log-Lifting in Business Process Mining Applications},
year = {2019},
isbn = {978-3-030-26618-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26619-6_16},
doi = {10.1007/978-3-030-26619-6_16},
abstract = {Real-life event logs are typically much less structured and more complex than the predefined business activities they refer to. Most of the existing process mining techniques assume that there is a one-to-one mapping between process model activities and events recorded during process execution. Unfortunately, event logs and process model activities are defined at different levels of granularity. The challenges posed by this discrepancy can be addressed by means of log-lifting. In this work we develop a machine-learning-based framework aimed at bridging the abstraction level gap between logs and process models. The proposed framework operates of two main phases: log segmentation and machine-learning-based classification. The purpose of the segmentation phase is to identify the potential segment separators in a flow of low-level events, in which each segment corresponds to an unknown high-level activity. For this, we propose a segmentation algorithm based on maximum likelihood with n-gram analysis. In the second phase, event segments are mapped into their corresponding high-level activities using a supervised machine learning technique. Several machine learning classification methods are explored including ANNs, SVMs, and random forest. We demonstrate the applicability of our framework using a real-life event log provided by the SAP company. The results obtained show that a machine learning approach based on the random forest algorithm outperforms the other methods with an accuracy of 96.4%. The testing time was found to be around 0.01s, which makes the algorithm a good candidate for real-time deployment scenarios.},
booktitle = {Business Process Management: 17th International Conference, BPM 2019, Vienna, Austria, September 1–6, 2019, Proceedings},
pages = {232–249},
numpages = {18},
keywords = {Process mining, Segmentation, Log lifting, Machine learning},
location = {Vienna, Austria}
}

@inproceedings{10.1145/3233027.3236404,
author = {Gazzillo, Paul and Koc, Ugur and Nguyen, ThanhVu and Wei, Shiyi},
title = {Localizing configurations in highly-configurable systems},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236404},
doi = {10.1145/3233027.3236404},
abstract = {The complexity of configurable systems has grown immensely, and it is only getting more complex. Such systems are a challenge for software testing and maintenance, because bugs and other defects can and do appear in any configuration. One common requirement for many development tasks is to identify the configurations that lead to a given defect or some other program behavior. We distill this requirement down to a challenge question: given a program location in a source file, what are valid configurations that include the location? The key obstacle is scalability. When there are thousands of configuration options, enumerating all combinations is exponential and infeasible. We provide a set of target programs of increasing difficulty and variations on the challenge question so that submitters of all experience levels can try out solutions. Our hope is to engage the community and stimulate new and interesting approaches to the problem of analyzing configurations.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {269–273},
numpages = {5},
keywords = {configurations, program analysis, testing, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2019136.2019173,
author = {Fukuda, Takeshi and Atarashi, Yoshitaka and Yoshimura, Kentaro},
title = {An approach to evaluate time-dependent changes in feature constraints},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019173},
doi = {10.1145/2019136.2019173},
abstract = {Feature selections mining is the process of discovering potentially feature associations and constraints in data. Especially, mining from time-series data obtains feature constraint trends. In this paper, we describe an approach to evaluate feature constraint trends and present results of two case studies. Feature selections mining was applied to a product transactions database at Hitachi. The product transactions had 148 optional features, and 8,372 products were derived from the product line. Both case studies focus on transaction-time periods: time series and time intervals. Feature selections mining discovered feature constraints around 100 rules in each study, and determined they constantly change.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {33},
numpages = {5},
keywords = {embedded systems, feature modeling, industry case study, software product line engineering},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.procs.2018.08.100,
author = {Zykov, Sergey V. and Shumsky, Leonid D. and Tykushin, Anatoly V. and Tormasov, Alexander G.},
title = {Applicative-based automatic configuration management for virtual machines},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {126},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.08.100},
doi = {10.1016/j.procs.2018.08.100},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {1771–1778},
numpages = {8},
keywords = {virtual machine, automatic configuration, applicative computing}
}

@article{10.5555/2871950.2871961,
author = {Chen, Ying-Ju and Tomlin, Brian and Wang, Yimin},
title = {Coproduct Technologies: Product Line Design and Process Innovation},
year = {2013},
issue_date = {December 2013},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {59},
number = {12},
issn = {0025-1909},
abstract = {The simultaneous production of different outputs coproducts is observed in the chemical, material, mineral, and semiconductor industries among others. Often, as with microprocessors, the outputs differ in quality in the vertical sense and firms classify the output into different grades products. We analyze product line design and production for a firm operating a vertical coproduct technology. We examine how the product line and profit are influenced by the production cost and output distribution of the technology. We prove that production cost influences product line design in a fundamentally different manner for coproduct technologies than for uniproduct technologies where the firm can produce products independently. For example, with coproducts, the size and length of the product line can both increase in the production cost. Contrary to the oft-held view that variability is bad, we prove the firm benefits from a more variable output distribution if the production or classification cost is low enough.Data, as supplemental material, are available at &lt;ext-link ext-link-type="uri" xlink="http://dx.doi.org/10.1287/mnsc.2013.1738"&gt;http://dx.doi.org/10.1287/mnsc.2013.1738&lt;/ext-link&gt;. This paper was accepted by Serguei Netessine, operations management.},
journal = {Manage. Sci.},
month = dec,
pages = {2772–2789},
numpages = {18},
keywords = {pricing, process technology, quality segmentation}
}

@inproceedings{10.1145/3233027.3233036,
author = {Hamza, Mostafa and Walker, Robert J. and Elaasar, Maged},
title = {CIAhelper: towards change impact analysis in delta-oriented software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233036},
doi = {10.1145/3233027.3233036},
abstract = {Change is inevitable for software systems to deal with the evolving environment surrounding them, and applying changes requires careful design and implementation not to break existing functionalities. Evolution in software product lines (SPLs) is more complex compared to evolution for individual products: a change applied to a single feature might affect all the products in the whole product family. In this paper we present an approach for change impact analysis in delta-oriented programming (DOP), an existing language aimed at supporting SPLs. We propose the CIAHelper tool to identify dependencies within a DOP program, by analyzing the semantics of both the code artifacts and variability models to construct a directed dependency graph. We also consider how the source code history could be used to enhance the recall of detecting the affected artifacts given a change proposal. We evaluate our approach by means of five case studies on two different DOP SPLs.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {31–42},
numpages = {12},
keywords = {change impact analysis, code assets, delta-oriented programming, feature model, variability model},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414966,
author = {Basile, Davide and Beek, Maurice H. ter and Cordy, Maxime and Legay, Axel},
title = {Tackling the equivalent mutant problem in real-time systems: the 12 commandments of model-based mutation testing},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414966},
doi = {10.1145/3382025.3414966},
abstract = {Mutation testing can effectively drive test generation to reveal faults in software systems. However, it faces a typical efficiency issue as it can produce many mutants that are equivalent to the original system, making it impossible to generate test cases from them.We consider this problem when model-based mutation testing is applied to real-time system product lines, represented as timed automata. We define novel, time-specific mutation operators and formulate the equivalent mutant problem in the frame of timed refinement relations.Further, we study in which cases a mutation yields an equivalent mutant. Our theoretical results provide guidance to system engineers, allowing them to eliminate mutations from which no test case can be produced. Our evaluation, based on a proof-of-concept tool and an industrial case from the automotive domain, confirms the validity of our theory and demonstrates that our approach can eliminate many of the equivalent mutants (88% in our case study).},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {30},
numpages = {11},
keywords = {mutation-based testing, real-time systems, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3109729.3109745,
author = {Markiegi, Urtzi},
title = {Test optimisation for Highly-Configurable Cyber-Physical Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109745},
doi = {10.1145/3109729.3109745},
abstract = {Cyber-Physical Systems (CPS) have become one of the core-enabling technologies for multiple domains, such as manufacturing, healthcare, energy and transportation. Furthermore, these domains are demanding CPS to be highly-configurable in order to respond to multiple and changing market requirements. Testing these Highly-Configurable Cyber-Physical Systems (HCCPS) is challenging. First, when working with CPSs, considerable time is required in order to tackle physical processes during testing. And secondly, in highly-configurable systems, a large number of system variants need to be tested. Consequently, reducing HCCPS testing time is essential.In this context, a research work is presented to reduce the overall testing time of HCCPS, focusing on a merged strategy of product and test cases optimisation. In particular, two approaches are proposed in order to achieve the testing time reduction. The first approach aims to reduce the HCCPS testing time by an iterative allocation of products and test cases. The second approach aims to reduce the HCCPS testing time by a feedback driven dynamic and iterative allocation of products and test cases.A preliminary experiment has been undertaken to test the iterative allocation approach. In this experiment, products to be tested are selected and prioritised. Next, multiple testing iterations are perform until the time-budget is consumed. In each iteration a small number of test cases are allocated for each of the products to be tested. The experiment was evaluated with an academic HCCPS and preliminary results suggest that the proposed approach reduces the fault detection time when compared with traditional approaches.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {139–144},
numpages = {6},
keywords = {Cyber-Physical Systems, Fault Detection, Highly-Configurable Systems, Product Line Testing, Search-Based Software Engineering, Software Engineering},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.infsof.2010.12.006,
author = {Chen, Lianping and Ali Babar, Muhammad},
title = {A systematic review of evaluation of variability management approaches in software product lines},
year = {2011},
issue_date = {April, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.12.006},
doi = {10.1016/j.infsof.2010.12.006},
abstract = {ContextVariability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated. ObjectiveThe objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches. MethodWe carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007. ResultsWe selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects. ConclusionThe findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {344–362},
numpages = {19},
keywords = {Empirical studies, Software product line, Systematic literature reviews, Variability management}
}

@article{10.1016/j.scico.2012.04.009,
author = {Marinho, Fabiana G. and Andrade, Rossana M. C. and Werner, Cláudia and Viana, Windson and Maia, Marcio E. F. and Rocha, Lincoln S. and Teixeira, Eldínae and Filho, João B. Ferreira and Dantas, Valéria L. L. and Lima, Fabrício and Aguiar, Saulo},
title = {MobiLine: A Nested Software Product Line for the domain of mobile and context-aware applications},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.04.009},
doi = {10.1016/j.scico.2012.04.009},
abstract = {Mobile devices are multipurpose and multi-sensor equipments supporting applications able to adapt their behavior according to changes in the user's context (device, location, time, etc.). Meanwhile, the development of mobile and context-aware software is not a simple task, mostly due to the peculiar characteristics of these devices. Although several solutions have been proposed to facilitate their development, reuse is not systematically used throughout the software development life-cycle. In this paper, we discuss an approach for the development of mobile and context-aware software using the Software Product Line (SPL) paradigm. Furthermore, a Nested SPL for the domain of mobile and context-aware applications is presented, lessons learned in the SPL development are discussed and a product for a context-aware visit guide is shown.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {2381–2398},
numpages = {18},
keywords = {Context-awareness, Mobility, Software product line}
}

@article{10.1016/j.infsof.2019.01.008,
author = {Meqdadi, Omar and Alhindawi, Nouh and Alsakran, Jamal and Saifan, Ahmad and Migdadi, Hatim},
title = {Mining software repositories for adaptive change commits using machine learning techniques},
year = {2019},
issue_date = {May 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {109},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.01.008},
doi = {10.1016/j.infsof.2019.01.008},
journal = {Inf. Softw. Technol.},
month = may,
pages = {80–91},
numpages = {12},
keywords = {Code change metrics, Adaptive maintenance, Commit types, Maintenance classification, Machine learning}
}

@inproceedings{10.1007/978-3-030-64148-1_12,
author = {Lwakatare, Lucy Ellen and Crnkovic, Ivica and Rånge, Ellinor and Bosch, Jan},
title = {From a Data Science Driven Process to a Continuous Delivery Process for Machine Learning Systems},
year = {2020},
isbn = {978-3-030-64147-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64148-1_12},
doi = {10.1007/978-3-030-64148-1_12},
abstract = {Development of machine learning (ML) enabled applications in real-world settings is challenging and requires the consideration of sound software engineering (SE) principles and practices. A large body of knowledge exists on the use of modern approaches to developing traditional software components, but not ML components. Using exploratory case study approach, this study investigates the adoption and use of existing software development approaches, specifically continuous delivery (CD), to development of ML components. Research data was collected using a multivocal literature review (MLR) and focus group technique with ten practitioners involved in developing ML-enabled systems at a large telecommunication company. The results of our MLR show that companies do not outright apply CD to the development of ML components rather as a result of improving their development practices and infrastructure over time. A process improvement conceptual model, that includes the description of CD application to ML components is developed and initially validated in the study.},
booktitle = {Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings},
pages = {185–201},
numpages = {17},
keywords = {Machine learning system, Software process, Continuous delivery},
location = {Turin, Italy}
}

@inproceedings{10.1145/2791060.2791108,
author = {Berger, Thorsten and Lettner, Daniela and Rubin, Julia and Grünbacher, Paul and Silva, Adeline and Becker, Martin and Chechik, Marsha and Czarnecki, Krzysztof},
title = {What is a feature? a qualitative study of features in industrial software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791108},
doi = {10.1145/2791060.2791108},
abstract = {The notion of features is commonly used to describe the functional and non-functional characteristics of a system. In software product line engineering, features often become the prime entities of software reuse and are used to distinguish the individual products of a product line. Properly decomposing a product line into features, and correctly using features in all engineering phases, is core to the immediate and long-term success of such a system. Yet, although more than ten different definitions of the term feature exist, it is still a very abstract concept. Definitions lack concrete guidelines on how to use the notion of features in practice.To address this gap, we present a qualitative empirical study on actual feature usage in industry. Our study covers three large companies and an in-depth, contextualized analysis of 23 features, perceived by the interviewees as typical, atypical (outlier), good, or bad representatives of features. Using structured interviews, we investigate the rationales that lead to a feature's perception, and identify and analyze core characteristics (facets) of these features. Among others, we find that good features precisely describe customer-relevant functionality, while bad features primarily arise from rashly executed processes. Outlier features, serving unusual purposes, are necessary, but do not require the full engineering process of typical features.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {16–25},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.5555/646863.707954,
author = {Márkus, András and Váncza, József},
title = {Product Line Design with Customer Preferences},
year = {2001},
isbn = {3540422196},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {When customizing their product lines, manufacturers attempt to fulfill the requirements of the customers within the technical and economical constraints of the manufacturing environment. Product line design is a recurrent process that aims at finding the proper balance between the exploration of new product alternatives and the exploitation of the known selling potential of the available variants. This paper offers a framework where, driven by the interaction of customer preferences and the reallocation of manufacturing resources, product families emerge from technically feasible product alternatives.},
booktitle = {Proceedings of the 14th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems: Engineering of Intelligent Systems},
pages = {846–855},
numpages = {10},
series = {IEA/AIE '01}
}

@inproceedings{10.1145/3318396.3318448,
author = {Babaagba, Kehinde Oluwatoyin and Adesanya, Samuel Olumide},
title = {A Study on the Effect of Feature Selection on Malware Analysis using Machine Learning},
year = {2019},
isbn = {9781450362672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318396.3318448},
doi = {10.1145/3318396.3318448},
abstract = {In this paper, the effect of feature selection in malware detection using machine learning techniques is studied. We employ supervised and unsupervised machine learning algorithms with and without feature selection. These include both classification and clustering algorithms. The algorithms are compared for effectiveness and efficiency using their predictive accuracy, among others, as performance metric. From the studies, we observe that the best detection rate was attained for supervised learning with feature selection. The supervised learning algorithm used was Multilayer Perceptron (MLP) algorithm. The analysis also reveals that our system can detect viruses from varying sources.},
booktitle = {Proceedings of the 2019 8th International Conference on Educational and Information Technology},
pages = {51–55},
numpages = {5},
keywords = {Feature Selection and Machine Learning, Malware Detection},
location = {Cambridge, United Kingdom},
series = {ICEIT 2019}
}

@article{10.1007/s10664-020-09853-4,
author = {Hajri, Ines and Goknil, Arda and Pastore, Fabrizio and Briand, Lionel C.},
title = {Automating system test case classification and prioritization for use case-driven testing in product lines},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09853-4},
doi = {10.1007/s10664-020-09853-4},
abstract = {Product Line Engineering (PLE) is a crucial practice in many software development environments where software systems are complex and developed for multiple customers with varying needs. At the same time, many development processes are use case-driven and this strongly influences their requirements engineering and system testing practices. In this paper, we propose, apply, and assess an automated system test case classification and prioritization approach specifically targeting system testing in the context of use case-driven development of product families. Our approach provides: (i) automated support to classify, for a new product in a product family, relevant and valid system test cases associated with previous products, and (ii) automated prioritization of system test cases using multiple risk factors such as fault-proneness of requirements and requirements volatility in a product family. Our evaluation was performed in the context of an industrial product family in the automotive domain. Results provide empirical evidence that we propose a practical and beneficial way to classify and prioritize system test cases for industrial product lines.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3711–3769},
numpages = {59},
keywords = {Product Line Engineering, Use case driven development, Regression testing, Test case selection and prioritization, Automotive, Requirements engineering}
}

@inproceedings{10.1145/302405.302690,
author = {Bosch, Jan},
title = {Product-line architectures in industry: a case study},
year = {1999},
isbn = {1581130740},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/302405.302690},
doi = {10.1145/302405.302690},
booktitle = {Proceedings of the 21st International Conference on Software Engineering},
pages = {544–554},
numpages = {11},
keywords = {case study, experiences, product-line architectures},
location = {Los Angeles, California, USA},
series = {ICSE '99}
}

@inproceedings{10.1145/2791060.2791069,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Empirical comparison of regression methods for variability-aware performance prediction},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791069},
doi = {10.1145/2791060.2791069},
abstract = {Product line engineering derives product variants by selecting features. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable product variant. We infer such a correlation using four regression methods based on small samples of measured configurations, without additional effort to detect feature interactions. We conduct experiments on six real-world case studies to evaluate the prediction accuracy of the regression methods. A key finding in our empirical study is that one regression method, called Bagging, is identified as the best to make accurate and robust predictions for the studied systems.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {186–190},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2934466.2934487,
author = {Rabiser, Daniela and Grünbacher, Paul and Prähofer, Herbert and Angerer, Florian},
title = {A prototype-based approach for managing clones in clone-and-own product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934487},
doi = {10.1145/2934466.2934487},
abstract = {Feature models are commonly used in industrial contexts to guide and automate the derivation of product variants. However, in real-world product lines the derivation process goes beyond selecting and composing product features. Specifically, developers often perform clone-and-own reuse, i.e., they copy, modify, and extend existing code to provide the functionality required by customers. Clones are created at different levels of granularity, ranging from individual features to entire systems. Refactoring and reverse engineering approaches have been proposed for dealing with cloned product variants. However, managing clones has not been addressed in the context of feature models. For instance, if clones are created to address customer requirements in specific product variants, the connection to the original feature models is frequently lost. We thus present a modeling approach based on prototypes, i.e., prefabricated objects from which clones are created. Our approach allows to manage prototypes and their clones at the levels of products, components, and features. We use compliance levels to define the required level of consistency between prototypes and clones. We further adapt an existing consistency checking framework for detecting inconsistent clones when the product line evolves. Our approach uses feature-to-code mappings to determine the impact of changes on code elements. We present a case study illustrating prototypes, clones, and compliance levels in selected development scenarios of our industry partner's product line. We also discuss the use of static code analysis techniques to support engineers in determining the impact of changed prototypes on affected clones, an area we plan to address in our future work.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {35–44},
numpages = {10},
keywords = {cloning, co-evolution, feature modeling, industrial systems},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1007/s10845-012-0699-5,
author = {Okudan, Gül E. and Chiu, Ming-Chuan and Kim, Tae-Hyun},
title = {Perceived feature utility-based product family design: a mobile phone case study},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {5},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-012-0699-5},
doi = {10.1007/s10845-012-0699-5},
abstract = {To assure profit maximization through mass customization and personalization, effectively eliciting consumer needs across different market segments is critical. Although functional performance specifications and adequacy of various design forms can be measured directly and objectively, many designers and engineers struggle with clearly evaluating product criteria requiring subjective consumer input; the fact that these inputs change over time further complicates the process. To appropriately evaluate product criteria, an effective design decision-making analysis is required. In this study, we propose a methodology to assure effective elicitation of needs and their inclusion in design decision making and illustrate it using a mobile phone product family design scenario. First, consumer perceived utility of design features is gathered using a questionnaire (500+ responses) and then modeled using multi- attribute utility theory to facilitate the evaluation of a product family while responding to needs across customer clusters shaped by demographics. The methodology goal is to determine the relative goodness of a product family in comparison to its competition. We also compare and evaluate the application of the proposed method to conjoint analysis.},
journal = {J. Intell. Manuf.},
month = oct,
pages = {935–949},
numpages = {15},
keywords = {Conjoint analysis (CA), Design decision making, Mass customization and personalization, Mobile phones, Multi-attribute utility theory}
}

@inproceedings{10.1145/3233027.3233035,
author = {Varshosaz, Mahsa and Al-Hajjaji, Mustafa and Thüm, Thomas and Runge, Tobias and Mousavi, Mohammad Reza and Schaefer, Ina},
title = {A classification of product sampling for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233035},
doi = {10.1145/3233027.3233035},
abstract = {The analysis of software product lines is challenging due to the potentially large number of products, which grow exponentially in terms of the number of features. Product sampling is a technique used to avoid exhaustive testing, which is often infeasible. In this paper, we propose a classification for product sampling techniques and classify the existing literature accordingly. We distinguish the important characteristics of such approaches based on the information used for sampling, the kind of algorithm, and the achieved coverage criteria. Furthermore, we give an overview on existing tools and evaluations of product sampling techniques. We share our insights on the state-of-the-art of product sampling and discuss potential future work.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {1–13},
numpages = {13},
keywords = {domain models, feature interaction, sampling algorithms, software product lines, testing},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1007/s00766-013-0185-4,
author = {Derakhshanmanesh, Mahdi and Fox, Joachim and Ebert, Jürgen},
title = {Requirements-driven incremental adoption of variability management techniques and tools: an industrial experience report},
year = {2014},
issue_date = {November  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0185-4},
doi = {10.1007/s00766-013-0185-4},
abstract = {In theory, software product line engineering has reached a mature state. In practice though, implementing a variability management approach remains a tough case-by-case challenge for any organization. To tame the complexity of this undertaking, it is inevitable to handle variability from multiple perspectives and to manage variability consistently across artifacts, tools, and workflows. Especially, a solid understanding and management of the requirements to be met by the products is an inevitable prerequisite. In this article, we share experiences from the ongoing incremental adoption of explicit variability management at TRW Automotive's department for automotive slip control systems--located in Koblenz, Germany. On the technical side, the three key drivers of this adoption effort are (a) domain modeling and scoping, (b) handling of variability in requirements and (c) tighter integration of software engineering focus areas (e.g., domain modeling, requirements engineering, architectural modeling) to make use of variability-related data. In addition to implementation challenges with using and integrating concrete third-party tools, social and workflow-related issues are covered as well. The lessons learned are presented, discussed, and thoroughly compared with the state of the art in research.},
journal = {Requir. Eng.},
month = nov,
pages = {333–354},
numpages = {22},
keywords = {Features, Incremental adoption, Requirements, Reuse, Software product lines, Tool integration}
}

@inproceedings{10.5555/318773.319262,
author = {Bayer, Joachim and Girard, Jean-François and Würthner, Martin and DeBaud, Jean-Marc and Apel, Martin},
title = {Transitioning legacy assets to a product line architecture},
year = {1999},
isbn = {3540665382},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A successful software system evolves over time, but this evolution often occurs in an ad-hoc fashion. One approach to structure system evolution is the concept of software product lines where a core architecture supports a variety of application contexts. However, in practice, the high cost and high risks of redevelopment as well as the substantial investments made to develop the existing systems most often mandate significant leverage of the legacy assets. Yet, there is little guidance in the literature on how to transition legacy assets into a product line set-up.In this paper, we present RE-PLACE, an approach developed to support the transition of existing software assets towards a product line architecture while taking into account anticipated new system variants. We illustrate this approach with its application in an industrial setting.},
booktitle = {Proceedings of the 7th European Software Engineering Conference Held Jointly with the 7th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {446–463},
numpages = {18},
keywords = {architecture recovery, domain-specific software architecture, reengineering, reuse, software product line},
location = {Toulouse, France},
series = {ESEC/FSE-7}
}

@inproceedings{10.5555/1758463.1758480,
author = {Peña, Joaquin and Hinchey, Michael G. and Ruiz-Cortés, Antonio and Trinidad, Pablo},
title = {Building the core architecture of a NASA multiagent system product line},
year = {2006},
isbn = {9783540709442},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The field of Software Product Lines (SPL) emphasizes building a family of software products from which concrete products can be derived rapidly. This helps to reduce time-to-market, costs, etc., and can result in improved software quality and safety. Current Agent-Oriented Software Engineering (AOSE) methodologies are concerned with developing a single Multiagent System. The main contribution of this paper is a proposal to developing the core architecture of a Multiagent Systems Product Line (MAS-PL), exemplifying our approach with reference to a concept NASA mission based on multiagent technology.},
booktitle = {Proceedings of the 7th International Conference on Agent-Oriented Software Engineering VII},
pages = {208–224},
numpages = {17},
location = {Hakodate, Japan},
series = {AOSE'06}
}

@article{10.1007/s10845-010-0465-5,
author = {Agard, Bruno and Barajas, Marco},
title = {The use of fuzzy logic in product family development: literature review and opportunities},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {5},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-010-0465-5},
doi = {10.1007/s10845-010-0465-5},
abstract = {Over the past few years, a number of key issues related to the product family design have been addressed, and a great deal of work has been done to improve it. Many different tools have been employed in this effort, such as mass customization, modularity, delayed differentiation, commonality, platforms, product families, and so on. The purpose of this paper is to analyze how fuzzy logic has been applied and how it can help to improve the entire process of product family development. Given its powerful capability to represent aspects that binary variables cannot, we show how fuzzy logic has been used to take advantage by considering the vague parameters related to the human character in different processes. Our aim is to contribute to the understanding and improvement of product family development process by identifying essential applications of fuzzy logic. An extended overview of the product family development process is provided, and also this work highlights the role of fuzzy logic in it. Fourteen fuzzy logic tools and thirteen topics into the product family development process are identified and summarized as a framework to analyze the role of fuzzy logic and at the same time to identify further application opportunities.},
journal = {J. Intell. Manuf.},
month = oct,
pages = {1445–1462},
numpages = {18},
keywords = {Fuzzy logic, Literature review, Opportunities, Product family development, Shortcomings}
}

@inproceedings{10.1145/2755567.2755568,
author = {Oliveira, Edson and Allian, Ana P.},
title = {Do Reference Architectures can Contribute to Standardizing Variability Management Tools?},
year = {2015},
isbn = {9781450334457},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2755567.2755568},
doi = {10.1145/2755567.2755568},
abstract = {Variability Management (VM) is one of the core activities for the success of software reuse. Several VM tools developed in academia and industry support mass customization of new software products and decrease time to market. Despite of a significant number of VM tools, in most cases, industry has adopted different techniques for managing variability, including producing their own tools. Such a heterogeneity provides difficulties in establishing VM, product customization and derivation, and standardization. From another perspective, reference architectures (RA) are a special type of software architecture as it encompasses specific domain knowledge, making it easier the development, standardization and evolution of software systems. Concepts from reference architectures can mitigate the lacking of VM tools standardization. Therefore, this position paper presents a vision towards supporting architectural standardization of VM tools, through reference architectures, for achieving a well-recognized understanding of such a domain and promoting reuse of design expertise. In this context, the main contribution of this paper is providing a discussion with regard to reference architectures and variability management tools towards supporting answering the following research question: "Do reference architectures can contribute to standardizing VM tools"?. Such standardization is useful as it fosters interoperability and reuse.},
booktitle = {Proceedings of the 1st International Workshop on Exploring Component-Based Techniques for Constructing Reference Architectures},
pages = {9–12},
numpages = {4},
keywords = {reference architectures, standardization, variability management tools},
location = {Montréal, QC, Canada},
series = {CobRA '15}
}

@inproceedings{10.1145/2019136.2019163,
author = {Corrêa, Chessman K. F. and Oliveira, Toacy C. and Werner, Claudia M. L.},
title = {An analysis of change operations to achieve consistency in model-driven software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019163},
doi = {10.1145/2019136.2019163},
abstract = {Model-Driven Software Product Line (MD-SPL) is the combination of Model-Driven Software Development and Software Product Line. In this paradigm, there are complex dependency relationships between different software development artefacts. These artefacts have to be changed during development and maintenance activities. However, the quantity of elements involved and the complex dependencies may imply that not all artefacts are updated appropriately, making these artefacts inconsistent with each other and with variability rules. This article discusses the change impact on interrelated MD-SPL artefacts and what has to be done do keep consistency.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {24},
numpages = {4},
keywords = {consistency, model-driven software product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3177540.3177555,
author = {Wang, Li-C.},
title = {Machine Learning for Feature-Based Analytics},
year = {2018},
isbn = {9781450356268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177540.3177555},
doi = {10.1145/3177540.3177555},
abstract = {Applying machine learning in Electronic Design Automation (EDA) has received growing interests in recent years. One approach to analyze data in EDA applications can be called feature-based analytics. In this context, the paper explains the inadequacy of adopting a traditional machine learning problem formulation view. Then, an alternative machine learning view is suggested where learning from data is treated as an iterative search process. The theoretical and practical considerations for implementing such a search process are discussed in the context of various applications.},
booktitle = {Proceedings of the 2018 International Symposium on Physical Design},
pages = {74–81},
numpages = {8},
keywords = {Occam's razor, design automation, feature-based analytics, learnable, machine learning, version space},
location = {Monterey, California, USA},
series = {ISPD '18}
}

@inproceedings{10.1145/3338906.3342484,
author = {Moghadam, Mahshid Helali},
title = {Machine learning-assisted performance testing},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342484},
doi = {10.1145/3338906.3342484},
abstract = {Automated testing activities like automated test case generation imply a reduction in human effort and cost, with the potential to impact the test coverage positively. If the optimal policy, i.e., the course of actions adopted, for performing the intended test activity could be learnt by the testing system, i.e., a smart tester agent, then the learnt policy could be reused in analogous situations which leads to even more efficiency in terms of required efforts. Performance testing under stress execution conditions, i.e., stress testing, which involves providing extreme test conditions to find the performance breaking points, remains a challenge, particularly for complex software systems. Some common approaches for generating stress test conditions are based on source code or system model analysis, or use-case based design approaches. However, source code or precise system models might not be easily available for testing. Moreover, drawing a precise performance model is often difficult, particularly for complex systems. In this research, I have used model-free reinforcement learning to build a self-adaptive autonomous stress testing framework which is able to learn the optimal policy for stress test case generation without having a model of the system under test. The conducted experimental analysis shows that the proposed smart framework is able to generate the stress test conditions for different software systems efficiently and adaptively without access to performance models.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1187–1189},
numpages = {3},
keywords = {Autonomous testing, Performance testing, Reinforcement learning, Stress testing, Test case generation},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1155/2021/4767388,
author = {Soleymani, Ali and Arabgol, Fatemeh and Shojae Chaeikar, Saman},
title = {A Novel Approach for Detecting DGA-Based Botnets in DNS Queries Using Machine Learning Techniques},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {2090-7141},
url = {https://doi.org/10.1155/2021/4767388},
doi = {10.1155/2021/4767388},
abstract = {In today’s security landscape, advanced threats are becoming increasingly difficult to detect as the pattern of attacks expands. Classical approaches that rely heavily on static matching, such as blacklisting or regular expression patterns, may be limited in flexibility or uncertainty in detecting malicious data in system data. This is where machine learning techniques can show their value and provide new insights and higher detection rates. The behavior of botnets that use domain-flux techniques to hide command and control channels was investigated in this research. The machine learning algorithm and text mining used to analyze the network DNS protocol and identify botnets were also described. For this purpose, extracted and labeled domain name datasets containing healthy and infected DGA botnet data were used. Data preprocessing techniques based on a text-mining approach were applied to explore domain name strings with n-gram analysis and PCA. Its performance is improved by extracting statistical features by principal component analysis. The performance of the proposed model has been evaluated using different classifiers of machine learning algorithms such as decision tree, support vector machine, random forest, and logistic regression. Experimental results show that the random forest algorithm can be used effectively in botnet detection and has the best botnet detection accuracy.},
journal = {J. Comput. Netw. Commun.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/2491627.2491646,
author = {Marijan, Dusica and Gotlieb, Arnaud and Sen, Sagar and Hervieu, Aymeric},
title = {Practical pairwise testing for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491646},
doi = {10.1145/2491627.2491646},
abstract = {One key challenge for software product lines is efficiently managing variability throughout their lifecycle. In this paper, we address the problem of variability in software product lines testing. We (1) identify a set of issues that must be addressed to make software product line testing work in practice and (2) provide a framework that combines a set of techniques to solve these issues. The framework integrates feature modelling, combinatorial interaction testing and constraint programming techniques. First, we extract variability in a software product line as a feature model with specified feature interdependencies. We then employ an algorithm that generates a minimal set of valid test cases covering all 2-way feature interactions for a given time interval. Furthermore, we evaluate the framework on an industrial SPL and show that using the framework saves time and provides better test coverage. In particular, our experiments show that the framework improves industrial testing practice in terms of (i) 17% smaller set of test cases that are (a) valid and (b) guarantee all 2-way feature coverage (as opposite to 19.2% 2-way feature coverage in the hand made test set), and (ii) full flexibility and adjustment of test generation to available testing time.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {227–235},
numpages = {9},
keywords = {feature modelling, software product lines, variability management},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2491627.2491645,
author = {Zhang, Bo and Becker, Martin and Patzke, Thomas and Sierszecki, Krzysztof and Savolainen, Juha Erik},
title = {Variability evolution and erosion in industrial product lines: a case study},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491645},
doi = {10.1145/2491627.2491645},
abstract = {Successful software products evolve continuously to meet the changing stakeholder requirements. For software product lines, modifying variability is an additional challenge that must be carefully tackled during the evolution of the product line. This bears considerable challenges for industry as understanding on how variability realizations advance over time is not trivial. Moreover, it may lead to an erosion of variability, which needs an investigation of techniques on how to identify the variability erosion in practice, especially in the source code. To address various erosion symptoms, we have investigated the evolution of a large-scale industrial product line over a period of four years. Along improvement goals, we have researched a set of appropriate metrics and measurement approaches in a goal-oriented way, applied them in this case study with tool support, and interpreted the results including identified erosion symptoms.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {168–177},
numpages = {10},
keywords = {industrial case study, product line evolution, static code analysis, variability erosion},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1109/SEAMS.2019.00015,
author = {Jamshidi, Pooyan and Cámara, Javier and Schmerl, Bradley and Kästner, Christian and Garlan, David},
title = {Machine learning meets quantitative planning: enabling self-adaptation in autonomous robots},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2019.00015},
doi = {10.1109/SEAMS.2019.00015},
abstract = {Modern cyber-physical systems (e.g., robotics systems) are typically composed of physical and software components, the characteristics of which are likely to change over time. Assumptions about parts of the system made at design time may not hold at run time, especially when a system is deployed for long periods (e.g., over decades). Self-adaptation is designed to find reconfigurations of systems to handle such run-time inconsistencies. Planners can be used to find and enact optimal reconfigurations in such an evolving context. However, for systems that are highly configurable, such planning becomes intractable due to the size of the adaptation space. To overcome this challenge, in this paper we explore an approach that (a) uses machine learning to find Pareto-optimal configurations without needing to explore every configuration and (b) restricts the search space to such configurations to make planning tractable. We explore this in the context of robot missions that need to consider task timeliness and energy consumption. An independent evaluation shows that our approach results in high-quality adaptation plans in uncertain and adversarial environments.},
booktitle = {Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {39–50},
numpages = {12},
keywords = {artificial intelligence, machine learning, quantitative planning, robotics systems, self-adaptive systems},
location = {Montreal, Quebec, Canada},
series = {SEAMS '19}
}

@article{10.1515/acss-2014-0010,
author = {Bartusevics, Arturs and Novickis, Leonids and Leye, Stefan},
title = {Implementation of Software Configuration Management Process by Models},
year = {2014},
issue_date = {12 2014},
publisher = {Walter de Gruyter GmbH},
address = {Berlin, DEU},
volume = {16},
number = {1},
issn = {2255-8691},
url = {https://doi.org/10.1515/acss-2014-0010},
doi = {10.1515/acss-2014-0010},
abstract = {Abstract Nowadays software configuration management process is not only dilemma which system should be used for version control or how to merge changes from one source code branch to other. There are multiple tasks such as version control, build management, deploy management, status accounting, bug tracking and many others that should be solved to support full configuration management process according to most popular quality standards. The main scope of the mentioned process is to include only valid and tested software items to final version of product and prepare a new version as soon as possible. To implement different tasks of software configuration management process, a set of different tools, scripts and utilities should be used. The current paper provides a new model-based approach to implementation of configuration management. Using different models, a new approach helps to organize existing solutions and develop new ones by a parameterized way, thus increasing reuse of solutions. The study provides a general description of new model-based conception and definitions of all models needed to implement a new approach. The second part of the paper contains an overview of criteria, practical experiments and lessons learned from using new models in software configuration management. Finally, further works are defined based on results of practical experiments and lessons learned.},
journal = {Appl. Comput. Syst.},
month = dec,
pages = {26–32},
numpages = {7},
keywords = {Model-Driven Approach, Software Configuration Management}
}

@inproceedings{10.1145/3109729.3109758,
author = {Ben Snaiba, Ziad and de Vink, Erik P. and Willemse, Tim A.C.},
title = {Family-Based Model Checking of SPL based on mCRL2},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109758},
doi = {10.1145/3109729.3109758},
abstract = {We discuss how the general-purpose model checker mCRL2 can be used for family-based verification of behavioral properties of software product lines. This is achieved by exploiting a feature-oriented extension of the modal μ-calculus for the specification of SPL properties, and for its model checking by encoding it back into the logic of mCRL2. Using the example of the well-known minepump SPL an illustration of the possibilities of the approach is given.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {Family-based model checking, Software Product Lines, mCRL2},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1109/WAIN52551.2021.00016,
author = {Staron, Miroslaw and Hergès, Helena Odenstedt and Naredi, Silvana and Block, Linda and El-Merhi, Ali and Vithal, Richard and Elam, Mikael},
title = {Robust Machine Learning in Critical Care &amp;#x2014; Software Engineering and Medical Perspectives},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00016},
doi = {10.1109/WAIN52551.2021.00016},
abstract = {Using machine learning in clinical practice poses hard requirements on explainability, reliability, replicability and robustness of these systems. Therefore, developing reliable software for monitoring critically ill patients requires close collaboration between physicians and software engineers. However, these two different disciplines need to find own research perspectives in order to contribute to both the medical and the software engineering domain. In this paper, we address the problem of how to establish a collaboration where software engineering and medicine meets to design robust machine learning systems to be used in patient care. We describe how we designed software systems for monitoring patients under carotid endarterectomy, in particular focusing on the process of knowledge building in the research team. Our results show what to consider when setting up such a collaboration, how it develops over time and what kind of systems can be constructed based on it. We conclude that the main challenge is to find a good research team, where different competences are committed to a common goal.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {62–69},
numpages = {8},
location = {Madrid, Spain}
}

@inproceedings{10.1145/2491627.2491642,
author = {Martini, Antonio and Pareto, Lars and Bosch, Jan},
title = {Communication factors for speed and reuse in large-scale agile software development},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491642},
doi = {10.1145/2491627.2491642},
abstract = {An open issue in industry is the combination of software reuse in the context of large scale Agile Software Development. The speed offered by Agile Software Development is needed for short time to market, while reuse strategies such as Software Product Line Engineering are needed for long-term productivity, efficiency, and profit. The paper investigates, through a survey, communication factors affecting both speed and reuse in 3 large companies developing embedded systems and employing Agile Software Development and Software Product Line Engineering. Our results include a prioritized list of communication related factors obtained by statistical analysis and the recognition and spread of the factors in the companies. We have recognized 5 interfaces with the Agile development team that need to be improved: system engineers (architects), product management, distributed teams, inter-project teams and sales unit. Few factors (involving inter-project communication) depend on the business drivers for the company. We also reveal that Agile teams need strategic and architectural inputs in order to be implanted in a large company employing Software Product Line Engineering. Academic and industrial training as well as different tactics for co-location would improve the communication skills of engineers. There is also a need for solutions, in the reference architecture, for fostering Agile Software Development: the goal is the combination of the focus on customer value of the teams, reusability, system requirements and avoidance of organizational dependencies.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {42–51},
numpages = {10},
keywords = {agile software development, communication, development speed, embedded systems, factors, software process improvement (SPI), software reuse, speed},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2019136.2019149,
author = {Murugesupillai, Esan and Mohabbati, Bardia and Gašević, Dragan},
title = {A preliminary mapping study of approaches bridging software product lines and service-oriented architectures},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019149},
doi = {10.1145/2019136.2019149},
abstract = {Service Oriented Architectures (SOA) and Software Product Lines (SPL) have individually proven to be software engineering concepts that create added value to the development of software systems. Recently, the research community has recognized and investigated potentials for combining these two concepts. However, there have been no mapping study and literature surveys that systematically review the present research results in combining the two. This paper presents results of a preliminary work on a systematic mapping study of research papers that report on combining SOA and SPL. The main goal of a systematic mapping study is to provide a breath overview, classification of approaches and the quantity and type of research as well as available research results, which is complimentary step toward further systematic literature review. This paper, based on selected papers published from 2002 to mid-2010, reports on various aspects of the analyzed literature, including the motivations for combining the two concepts; contributions to specific stages of software engineering lifecycles; types of synergies and characteristics that are accomplished through combinations of the two concepts; and the methods used for and the rigor of the evaluations of the research conducted on the studied topic.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {11},
numpages = {8},
keywords = {service-oriented architecture, service-oriented product line, software product line, software variability, variability management},
location = {Munich, Germany},
series = {SPLC '11}
}

@phdthesis{10.5555/AAI28022522,
author = {Islam, Md Johirul and Ciardo, Gianfranco and Prabhu, Gurpur and Tian, Jin and Sharma, Anuj},
advisor = {Hridesh, Rajan,},
title = {Towards Understanding the Challenges Faced by Machine Learning Software Developers and Enabling Automated Solutions},
year = {2020},
isbn = {9798672106496},
publisher = {Iowa State University},
address = {USA},
abstract = {Modern software systems are increasingly including machine learning (ML) as an integral component. However, we do not yet understand the difficulties faced by software developers when learning about ML libraries and using them within their systems. To fill that gap this thesis reports on a detailed (manual) examination of 3,243 highly-rated Q&amp;A posts related to ten ML libraries, namely Tensorflow, Keras, scikitlearn, Weka, Caffe, Theano, MLlib, Torch, Mahout, and H2O, on Stack Overflow, a popular online technical Q&amp;A forum. Our findings reveal the urgent need for software engineering (SE) research in this area.The second part of the thesis particularly focuses on understanding the Deep Neural Network (DNN) bug characteristics. We study 2,716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, their root causes and impacts, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software.While exploring the bug characteristics, our findings imply that repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. So, the third part of this thesis presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from Github for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns and the most common bug fix patterns are fixing data dimension and neural network connectivity.Finally, we propose an automatic technique to detect ML Application Programming Interface (API) misuses. We started with an empirical study to understand ML API misuses. Our study shows that ML API misuse is prevalent and distinct compared to non-ML API misuses. Inspired by these findings, we contributed Amimla (Api Misuse In Machine Learning Apis) an approach and a tool for ML API misuse detection. Amimla relies on several technical innovations. First, we proposed an abstract representation of ML pipelines to use in misuse detection. Second, we proposed an abstract representation of neural networks for deep learning related APIs. Third, we have developed a representation strategy for constraints on ML APIs. Finally, we have developed a misuse detection strategy for both single and multi-APIs. Our experimental evaluation shows that Amimla achieves a high average accuracy of ∼80% on two benchmarks of misuses from Stack Overflow and Github.},
note = {AAI28022522}
}

@article{10.1016/j.asoc.2018.12.012,
author = {Arevalillo, Jorge M.},
title = {A machine learning approach to assess price sensitivity with application to automobile loan segmentation},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {76},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2018.12.012},
doi = {10.1016/j.asoc.2018.12.012},
journal = {Appl. Soft Comput.},
month = mar,
pages = {390–399},
numpages = {10},
keywords = {Machine learning, Conditional inference trees, Random forests, Model based recursive partitioning, Price sensitivity}
}

@inproceedings{10.1145/2491627.2491633,
author = {Tsuchiya, Ryosuke and Kato, Tadahisa and Washizaki, Hironori and Kawakami, Masumi and Fukazawa, Yoshiaki and Yoshimura, Kentaro},
title = {Recovering traceability links between requirements and source code in the same series of software products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491633},
doi = {10.1145/2491627.2491633},
abstract = {If traceability links between requirements and source code are not clarified when conducting maintenance and enhancements for the same series of software products, engineers cannot immediately find the correction location in the source code for requirement changes. However, manually recovering links in a large group of products requires significant costs and some links may be overlooked. Here, we propose a semi-automatic method to recover traceability links between requirements and source code in the same series of large software products. In order to support differences in representation between requirements and source code, we recover links by using the configuration management log as an intermediary. We refine the links by classifying requirements and code elements in terms of whether they are common or specific to the products. As a result of applying our method to real products that have 60KLOC, we have recovered valid traceability links within a reasonable amount of time. Automatic parts have taken 13 minutes 36 seconds, and non-automatic parts have taken about 3 hours, with a recall of 76.2% and a precision of 94.1%. Moreover, we recovered some links that were unknown to engineers. By recovering traceability links, software reusability will be improved, and software product line introduction will be facilitated.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {121–130},
numpages = {10},
keywords = {commonality and variability analysis, configuration management log, traceability recovery},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2491627.2491641,
author = {Koziolek, Heiko and Goldschmidt, Thomas and de Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan},
title = {Experiences from identifying software reuse opportunities by domain analysis},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491641},
doi = {10.1145/2491627.2491641},
abstract = {In a large corporate organization there are sometimes similar software products in certain subdomains with a perceived functional overlap. This promises to be an opportunity for systematic reuse to reduce software development and maintenance costs. In such situations companies have used different domain analysis approaches (e.g., SEI Technical Probe) that helped to assess technical and organizational potential for a software product line approach. We applied existing domain analysis approaches for software product line engineering and tailored them to include a feature analysis as well as architecture evaluation. In this paper, we report our experiences from applying the approach in two subdomains of industrial automation.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {208–217},
numpages = {10},
keywords = {business case, domain analysis, software product lines},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1007/978-3-319-26844-6_31,
author = {Teppola, Susanna and Parviainen, Päivi and Partanen, Jari and Kettunen, Petri},
title = {Variability Management Strategies to Support Efficient Delivery and Maintenance of Embedded Systems},
year = {2015},
isbn = {9783319268439},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26844-6_31},
doi = {10.1007/978-3-319-26844-6_31},
abstract = {Software intensive organisations that are able to efficiently handle product variability can reach competitive advantage by shorter development lead times, improved customer satisfaction, and reduced costs of product management. Improved reusability and flexibility, combined with variability strategies can provide companies mechanisms to offer new product variants fast to customers. Especially for long living embedded systems, it is essential to effectively maintain the delivered systems and keep maintenance costs at reasonable level. This paper describes a case study in which three industrial product development projects were studied in order to understand which variability strategies were implemented in their specific variability context. Results indicate that variability challenges and selected variability strategies depend both on the product platform maturity, as well as, the project development model. However, variability strategy needs continuous evaluation during the product lifecycle.},
booktitle = {Proceedings of the 16th International Conference on Product-Focused Software Process Improvement - Volume 9459},
pages = {431–438},
numpages = {8},
keywords = {Variability management, Variability strategy},
location = {Bolzano, Italy},
series = {PROFES 2015}
}

@inproceedings{10.1145/3106195.3106207,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106207},
doi = {10.1145/3106195.3106207},
abstract = {Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering &amp; machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {133–142},
numpages = {10},
keywords = {Feature Identification, Natural Language Documents, Reverse Engineering, Software Product Lines, Systematic Literature Review, Variability Extraction},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.knosys.2018.04.006,
author = {Lee, Gichang and Jeong, Jaeyun and Seo, Seungwan and Kim, CzangYeob and Kang, Pilsung},
title = {Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network},
year = {2018},
issue_date = {July 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {152},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.04.006},
doi = {10.1016/j.knosys.2018.04.006},
abstract = {In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment (positive/negative) of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is a lack of information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network (CNN). In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. To verify the proposed methodology, we evaluated the classification accuracy and the rate of polarity words among high scoring words using two movie review datasets. Experimental results show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores.},
journal = {Know.-Based Syst.},
month = jul,
pages = {70–82},
numpages = {13},
keywords = {Class activation mapping, Convolutional neural network, Sentiment analysis, Weakly supervised learning, Word localization}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {knowledge sharing, online learning, product-line management, self-adaptation, software design, software product-lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.cosrev.2021.100376,
author = {Amutha, J. and Sharma, Sandeep and Sharma, Sanjay Kumar},
title = {Strategies based on various aspects of clustering in wireless sensor networks using classical, optimization and machine learning techniques: Review, taxonomy, research findings, challenges and future directions},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2021.100376},
doi = {10.1016/j.cosrev.2021.100376},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {43},
keywords = {Wireless Sensor Networks, Optimization, Machine learning, Routing, Security, Reliability}
}

@article{10.1016/j.future.2019.06.022,
author = {Raza, Muhammad and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Zhao, Ming and Rehman, Zia ur},
title = {A comparative analysis of machine learning models for quality pillar assessment of SaaS services by multi-class text classification of users’ reviews},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.06.022},
doi = {10.1016/j.future.2019.06.022},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {341–371},
numpages = {31},
keywords = {SaaS, Quality pillars, User reviews, Text classification, Machine learning approaches}
}

@inproceedings{10.1145/1082960.1082981,
author = {Dehlinger, Josh and Lutz, Robyn R.},
title = {A product-line requirements approach to safe reuse in multi-agent systems},
year = {2005},
isbn = {1595931163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1082960.1082981},
doi = {10.1145/1082960.1082981},
abstract = {The dynamic nature of highly autonomous agents within distributed systems is difficult to specify with existing requirements techniques. However, capturing the possibly shifting configurations of agents in the requirements specification is essential for safe reuse of agents. The contribution of this work is an extensible agent-oriented requirements specification template for distributed systems that supports safe reuse. We make two basic claims for this idea. First, by adopting a product-line-like approach, it exploits component reuse during system evolution. Second, the template allows ready integration with an existing tool-supported, safety analysis technique sensitive to dynamic variations within the components (i.e., agents) of a system. To illustrate these claims, we apply the requirements specification template and safety analysis to a real-world context-aware, distributed satellite system.},
booktitle = {Proceedings of the Fourth International Workshop on Software Engineering for Large-Scale Multi-Agent Systems},
pages = {1–7},
numpages = {7},
location = {St. Louis, Missouri},
series = {SELMAS '05}
}

@inproceedings{10.1145/3430895.3460135,
author = {Bernius, Jan Philip and Krusche, Stephan and Bruegge, Bernd},
title = {A Machine Learning Approach for Suggesting Feedback in Textual Exercises in Large Courses},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460135},
doi = {10.1145/3430895.3460135},
abstract = {Open-ended textual exercises facilitate the comprehension of problem-solving skills. Students can learn from their mistakes when teachers provide individual feedback. However, courses with hundreds of students cause a heavy workload for teachers: providing individual feedback is mostly a manual, repetitive, and time-consuming activity.This paper presents CoFee, a machine learning approach designed to suggest computer-aided feedback in open-ended textual exercises. The approach uses topic modeling to split student answers into text segments and language embeddings to transform these segments. It then applies clustering to group the text segments by similarity so that the same feedback can be applied to all segments within the same cluster.We implemented this approach in a reference implementation called Athene and integrated it into Artemis. We used Athene to review 17 textual exercises in two large courses at the Technical University of Munich with 2,300 registered students and 53 teachers. On average, Athene suggested feedback for 26% of the submissions. Accordingly, 85% of these suggestions were accepted by the teachers, 5% were extended with a comment and then accepted, and 10% were changed.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {173–182},
numpages = {10},
keywords = {assessment support system, automatic assessment, education, feedback, grading, interactive learning, learning, software engineering},
location = {Virtual Event, Germany},
series = {L@S '21}
}

@inproceedings{10.1145/3233027.3233041,
author = {Montalvillo, Leticia and Díaz, Oscar and Fogdal, Thomas},
title = {Reducing coordination overhead in SPLs: peering in on peers},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233041},
doi = {10.1145/3233027.3233041},
abstract = {SPL product customers might not always wait for the next core asset release. When an organization aims to react to market events, quick bug fixes or urgent customer requests, strategies are needed to support fast adaptation, e.g. with product-specific extensions, which are later propagated into the SPL. This leads to the grow-and-prune model where quick reaction to changes often requires copying and specialization (grow) to be later cleaned up by merging and refactoring (prune). This paper focuses on the grow stage. Here, application engineers branch off the core-asset Master branch to account for their products' specifics within the times and priorities of their customers without having to wait for the next release of the core assets. However, this practice might end up in the so-called "integration hell". When long-living branches are merged back into the Master, the amount of code to be integrated might cause build failures or requires complex troubleshooting. On these premises, we advocate for making application engineers aware of potential coordination problems right during coding rather than deferring it until merge time. To this end, we introduce the notion of "peering bar" for Version Control Systems, i.e. visual bars that reflect whether your product's features are being upgraded in other product branches. In this way, engineers are aware of what their peers are doing on the other SPL's products. Being products from the same SPL, they are based on the very same core assets, and hence, bug ixes or functional enhancements undertaken for a product might well serve other products. This work introduces design principles for peering bars. These principles are fleshed out for GitHub as the Version Control System, and pure::variants as the SPL framework.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {110–120},
numpages = {11},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2019136.2019181,
author = {Weiss, Michael},
title = {Economics of collectives},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019181},
doi = {10.1145/2019136.2019181},
abstract = {The transition from a software product line to a software ecosystem, as reported by Bosch [5], takes place, when the product line company makes its platform available to developers outside the company. A similar transition takes place from a software ecosystem to a collective, when the platform is jointly created and owned by a group of members. Building on the literature on software product line economics, this research identifies three factors affecting the economics of collectives (level of contribution, number of members, and diversity of use), and develops a model linking those factors to three economic outcomes (time, quality, and cost).},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {39},
numpages = {8},
keywords = {collectives, open source software, product line economics, software ecosystems, software product lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.imavis.2016.11.013,
author = {Yue, Zongsheng and Meng, Deyu and He, Juan and Zhang, Gemeng},
title = {Semi-supervised learning through adaptive Laplacian graph trimming},
year = {2017},
issue_date = {April 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {60},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.11.013},
doi = {10.1016/j.imavis.2016.11.013},
abstract = {Graph-based semi-supervised learning (GSSL) attracts considerable attention in recent years. The performance of a general GSSL method relies on the quality of Laplacian weighted graph (LWR) composed of the similarity imposed on input examples. A key for constructing an effective LWR is on the proper selection of the neighborhood size K or on the construction of KNN graph or -neighbor graph on training samples, which constitutes the fundamental elements in LWR. Specifically, too large K or will result in shortcut phenomenon while too small ones cannot guarantee to represent a complete manifold structure underlying data. To this issue, this study attempts to propose a method, called adaptive Laplacian graph trimming (ALGT), to make an automatic tuning to cut improper inter-cluster shortcut edges while enhance the connection between intra-cluster samples, so as to adaptively fit a proper LWR from data. The superiority of the proposed method is substantiated by experimental results implemented on synthetic and UCI data sets. A method which can adaptively fit a proper Laplacian weighted graph from data.A penalty helping cut inter-cluster shortcuts and enhance intra-cluster connections.A graph-based SSL model is less sensitive to neighborhood size by integrating ALGT.Superiority of ALGT is verified by experimental results on synthetic and UCI data.},
journal = {Image Vision Comput.},
month = apr,
pages = {38–47},
numpages = {10},
keywords = {Graph Laplacian, Nearest neighborhood graph, Self-paced learning, Semi-supervised learning}
}

@inproceedings{10.1007/978-3-030-30244-3_10,
author = {Khoza, Sibusiso C. and Grobler, Jacomine},
title = {Comparing Machine Learning and Statistical Process Control for Predicting Manufacturing Performance},
year = {2019},
isbn = {978-3-030-30243-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30244-3_10},
doi = {10.1007/978-3-030-30244-3_10},
abstract = {Quality has become one of the most important factors in the success of manufacturing companies. In this paper, the use of machine learning algorithms in quality control is compared to the use of statistical process monitoring, a classical quality management technique. The test dataset has a large number of features, which requires the use of principal component analysis and clustering to isolate the data into potential process groups. A Random Forest, Support Vector Machine and Naive Bayes algorithms were used to predict when the manufacturing process is out of control. The Random Forest algorithm performed significantly better than both the Naive Bayes and SVM algorithms in all 3 clusters of the dataset. The results were benchmarked against Hotelling’s  control charts which were trained using 80% of each cluster dataset and tested on the remaining 20%. In comparison with Hotelling’s  multivariate statistical process monitoring charts, the Random Forest algorithm still emerges as the better quality control method.},
booktitle = {Progress in Artificial Intelligence: 19th EPIA Conference on Artificial Intelligence, EPIA 2019, Vila Real, Portugal, September 3–6, 2019, Proceedings, Part II},
pages = {108–119},
numpages = {12},
location = {Vila Real, Portugal}
}

@inproceedings{10.1145/2556610.2556615,
author = {Bendix, Lars and Pendleton, Christian},
title = {The role of configuration management in outsourcing and distributed development},
year = {2013},
isbn = {9781450326414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556610.2556615},
doi = {10.1145/2556610.2556615},
abstract = {The use of distributed development teams is becoming more common and for many good reasons. Some of the advantages are that it makes it possible to outsource parts of the development effort, gives access to a larger pool of talents and specialists, facilitates the integration of mergers and acquisitions, and allows for more flexibility in scaling up and down projects. However, distributed development also brings many new problems to be dealt with on a project. It is more complex to manage, tends to create silos between groups, and there is a risk of loss of control over remote groups or people. Traditionally Configuration Management is seen as the infrastructure that allows for the coordination of the various activities on a project and it makes sure that work products flow smoothly through different stages of the development process.In this paper, we want to investigate to what degree Configuration Management concepts and principles can provide an infrastructure for distributed development teams too and help address some of their special challenges. We first look at what challenges distributed development teams face and categorize them according to how closely related they are to the Configuration Management domain. Then we sketch Configuration Management solutions to some of the related challenges. It turns out that surprisingly many distributed development challenges can be dealt with or made less problematic by simply applying well-known concepts and principles from Configuration Management. Furthermore, a number of other challenges can be alleviated by creative thinking in the implementation of Configuration Management and/or the collaboration between the Configuration Management activity and other activities.},
booktitle = {Proceedings of the 9th Central &amp; Eastern European Software Engineering Conference in Russia},
articleno = {8},
numpages = {10},
keywords = {best practices, categorization, challenges, configuration management, distributed development, ontology, outsourcing, virtual teams},
location = {Moscow, Russia},
series = {CEE-SECR '13}
}

@article{10.1007/s10664-019-09769-8,
author = {Ochodek, Miroslaw and Hebig, Regina and Meding, Wilhelm and Frost, Gert and Staron, Miroslaw},
title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09769-8},
doi = {10.1007/s10664-019-09769-8},
abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {220–265},
numpages = {46},
keywords = {Measurement, Machine learning, Action research, Code reviews}
}

@article{10.1016/j.procs.2019.11.105,
author = {Komarudin, Oman and Adianto, Daya and Azurat, Ade},
title = {Modeling Requirements of Multiple Single Products to Feature Model},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {161},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.11.105},
doi = {10.1016/j.procs.2019.11.105},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {107–114},
numpages = {8},
keywords = {software product line engineering, feature model, domain engineering}
}

@inproceedings{10.1145/2934466.2946045,
author = {Noir, Jérome Le and Madelénat, Sébastien and Gailliard, Grégory and Labreuche, Christophe and Acher, Mathieu and Barais, Olivier and Constant, Olivier},
title = {A decision-making process for exploring architectural variants in systems engineering},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946045},
doi = {10.1145/2934466.2946045},
abstract = {In systems engineering, practitioners shall explore numerous architectural alternatives until choosing the most adequate variant. The decision-making process is most of the time a manual, time-consuming, and error-prone activity. The exploration and justification of architectural solutions is ad-hoc and mainly consists in a series of tries and errors on the modeling assets. In this paper, we report on an industrial case study in which we apply variability modeling techniques to automate the assessment and comparison of several candidate architectures (variants). We first describe how we can use a model-based approach such as the Common Variability Language (CVL) to specify the architectural variability. We show that the selection of an architectural variant is a multi-criteria decision problem in which there are numerous interactions (veto, favor, complementary) between criteria.We present a tooled process for exploring architectural variants integrating both CVL and the MYRIAD method for assessing and comparing variants based on an explicit preference model coming from the elicitation of stakeholders' concerns. This solution allows understanding differences among variants and their satisfactions with respect to criteria. Beyond variant selection automation improvement, this experiment results highlight that the approach improves rationality in the assessment and provides decision arguments when selecting the preferred variants.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {277–286},
numpages = {10},
keywords = {architecture, decision-making, design exploration, model-driven engineering, multi-criteria decision analysis, systems engineering},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.eswa.2020.114176,
author = {AlOmar, Eman Abdullah and Peruma, Anthony and Mkaouer, Mohamed Wiem and Newman, Christian and Ouni, Ali and Kessentini, Marouane},
title = {How we refactor and how we document it? On the use of supervised machine learning algorithms to classify refactoring documentation},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114176},
doi = {10.1016/j.eswa.2020.114176},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {26},
keywords = {Refactoring, Software quality, Software engineering, Machine learning}
}

@inproceedings{10.1145/376503.376658,
author = {Hamilton, James R. and Hawley, Harold G. and Lalum, Clinton J.},
title = {Product-line reuse for Ada systems},
year = {1995},
isbn = {0897917057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/376503.376658},
doi = {10.1145/376503.376658},
booktitle = {Proceedings of the Conference on TRI-Ada '95: Ada's Role in Global Markets: Solutions for a Changing Complex World},
pages = {341–350},
numpages = {10},
keywords = {application engineering, application modeling, asset retrieval and adaption, domain engineering, domain-specific, object-oriented, product-line, rule-based adaption, software reuse},
location = {Anaheim, California, USA},
series = {TRI-Ada '95}
}

@inproceedings{10.1007/11552413_161,
author = {Kim, Haeng-Kon},
title = {Knowledge acqusition for mobile embedded software development based on product line},
year = {2005},
isbn = {3540288945},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11552413_161},
doi = {10.1007/11552413_161},
abstract = {To achieve the goal of assuring short development cycles for new products, they are developing a process-based knowledge-driven product development environment by employing information technology. The success of a KBS critically depends on the amount of knowledge embedded in the system. Expert knowledge, which results from an individual's extensive problem-solving experience, has been described as  unconscious knowledge.  Besides shortening development time, properly handled reuse will also improve the reliability since code is executed for longer time and in different contexts. In this paper, We propose a design process suitable for developing product lines for mobile embedded systems with KBS. The process starts in a requirement-capturing phase where the requirements from all products in the line are collected. The contributions of this work with respect to mobile embedded systems are an outline of a development process, focusing on the special considerations that must be taken into account when designing a PLA for the systems.},
booktitle = {Proceedings of the 9th International Conference on Knowledge-Based Intelligent Information and Engineering Systems - Volume Part I},
pages = {1131–1138},
numpages = {8},
keywords = {CBD, domain engineering, intelligent agents, knowledge based systems, mobile embedded systems},
location = {Melbourne, Australia},
series = {KES'05}
}

@article{10.1016/j.aei.2012.10.008,
author = {Hsiao, Shih-Wen and Ko, Ya-Chuan and Lo, Chi-Hung and Chen, Shih-Ho},
title = {An ISM, DEI, and ANP based approach for product family development},
year = {2013},
issue_date = {January, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {27},
number = {1},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2012.10.008},
doi = {10.1016/j.aei.2012.10.008},
abstract = {Product customization has been used for many years, and with this concept, product families can be designed that are better matched to the needs of customers through the use of market segmentation and modular architecture. This study is composed of two phases: the first phase establishes the modular architecture, and taking a bicycle as the case study, applies the Interpretive Structural Model (ISM) to modularize and cluster parts, and then models the connecting relations between the parts numerically using a Disassembly Effort Index (DEI). The second step involves the development of a product family using cluster analysis to divide the experimental samples and the employment of an Analytic Network Process (ANP) to obtain the optimal weight performance of the modules after establishing the market segmentation model. Results of the case study establish four product families and address construction performance of both the common and independent modules between the product families.},
journal = {Adv. Eng. Inform.},
month = jan,
pages = {131–148},
numpages = {18},
keywords = {Analytic Network Process, Interpretive Structural Model, Modular design, Product design, Product family}
}

@inproceedings{10.5555/645547.658853,
author = {Lee, Kwanwoo and Kang, Kyo Chul and Lee, Jaejoon},
title = {Concepts and Guidelines of Feature Modeling for Product Line Software Engineering},
year = {2002},
isbn = {3540434836},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product line software engineering (PLSE) is an emerging software engineering paradigm, which guides organizations toward the development of products from core assets rather than the development of products one by one from scratch. In order to develop highly reusable core assets, PLSE must have the ability to exploit commonality and manage variability among products from a domain perspective. Feature modeling is one of the most popular domain analysis techniques, which analyzes commonality and variability in a domain to develop highly reusable core assets for a product line. Various attempts have been made to extend and apply it to the development of software product lines. However, feature modeling can be difficult and time-consuming without a precise understanding of the goals of feature modeling and the aid of practical guidelines. In this paper, we clarify the concept of features and the goals of feature modeling, and provide practical guidelines for successful product line software engineering. The authors have extensively used feature modeling in several industrial product line projects and the guidelines described in this paper are based on these experiences.},
booktitle = {Proceedings of the 7th International Conference on Software Reuse: Methods, Techniques, and Tools},
pages = {62–77},
numpages = {16},
series = {ICSR-7}
}

@inproceedings{10.1145/2791060.2791092,
author = {Kühn, Thomas and Cazzola, Walter and Olivares, Diego Mathias},
title = {Choosy and picky: configuration of language product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791092},
doi = {10.1145/2791060.2791092},
abstract = {Although most programming languages naturally share several language features, they are typically implemented as a monolithic product. Language features cannot be plugged and unplugged from a language and reused in another language. Some modular approaches to language construction do exist but composing language features requires a deep understanding of its implementation hampering their use. The choose and pick approach from software product lines provides an easy way to compose a language out of a set of language features. However, current approaches to language product lines are not sufficient enough to cope with the complexity and evolution of real world programming languages. In this work, we propose a general light-weight bottom-up approach to automatically extract a feature model from a set of tagged language components. We applied this approach to the Neverlang language development framework and developed the AiDE tool to guide language developers towards a valid language composition. The approach has been evaluated on a decomposed version of Javascript to highlight the benefits of such a language product line.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {71–80},
numpages = {10},
keywords = {language composition, language product lines},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3109729.3109734,
author = {Marcén, Ana C. and Font, Jaime and Pastor, Óscar and Cetina, Carlos},
title = {Towards Feature Location in Models through a Learning to Rank Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109734},
doi = {10.1145/3109729.3109734},
abstract = {In this work, we propose a feature location approach to discover software artifacts that implement the feature functionality in a model. Given a model and a feature description, model fragments extracted from the model and the feature description are encoded based on a domain ontology. Then, a Learning to Rank algorithm is used to train a classifier that is based on the model fragments and feature description encoded. Finally, the classifier assesses the similarity between a population of model fragments and the target feature being located to find the set of most suitable feature realizations. We have evaluated the approach with an industrial case study, locating features with mean precision and recall values of around 73.75% and 73.31%, respectively (the sanity check obtains less than 35%).},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/1081706.1081758,
author = {Pettersson, Ulf and Jarzabek, Stan},
title = {Industrial experience with building a web portal product line using a lightweight, reactive approach},
year = {2005},
isbn = {1595930140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081706.1081758},
doi = {10.1145/1081706.1081758},
abstract = {Imprecise, frequently changing requirements and short time-to-market create challenges for application of conventional software methods in Web Portal engineering. To address these challenges, ST Electronics (Info-Software Systems) Pte. Ltd. applied a lightweight, reactive approach to support a Web Portal product line. Unique characteristics of the approach were fast, low-cost migration from a single conventional Web Portal towards a reusable "generic Web Portal" solution, effective handling of large number of functional variants and their dependencies, the ability to rapidly develop new Web Portals from the generic one, and to independently evolve multiple Web Portals without ever losing a connection between them and the "generic Web Portal". The initial Web Portal was built using state-of-the-art conventional methods. The Web Portal was not flexible enough to reap the benefits of new business opportunities that required the company to rapidly develop and further maintain many similar Web Portals. To overcome the limitations of the conventional solution, a reuse technique called XVCL was applied incrementally. Over a period of three weeks, the conventional solution was converted into a Web architecture capable of handling nine Web Portals from a base of code smaller than the original Web Portal. In the paper, we describe the process that led to building the above Web Portal product line. We explain the difficulties in building an effective generic Web solution using conventional techniques. We analyze our reuse-based solution in qualitative and quantitative ways.},
booktitle = {Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {326–335},
numpages = {10},
keywords = {maintenance, program synthesis, reuse, software product lines, static meta-programming, web engineering},
location = {Lisbon, Portugal},
series = {ESEC/FSE-13}
}

@inproceedings{10.1145/2934466.2934485,
author = {Lapeña, Raúl and Ballarin, Manuel and Cetina, Carlos},
title = {Towards clone-and-own support: locating relevant methods in legacy products},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934485},
doi = {10.1145/2934466.2934485},
abstract = {Clone-and-Own (CAO) is a common practice in families of software products consisting of reusing code from methods in legacy products in new developments. In industrial scenarios, CAO consumes high amounts of time and effort without guaranteeing good results. We propose a novel approach, Computer Assisted CAO (CACAO), that given the natural language requirements of a new product, and the legacy products from that family, ranks the legacy methods in the family for each of the new product requirements according to their relevancy to the new development. We evaluated our approach in the industrial domain of train control software. Without CACAO, software engineers tasked with the development of a new product had to manually review a total of 2200 methods in the family. Results show that CACAO can reduce the number of methods to be reviewed, and guide software engineers towards the identification of relevant legacy methods to be reused in the new product.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {194–203},
numpages = {10},
keywords = {clone and own, families of software products, software reuse},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2362536.2362552,
author = {Gómez, Abel and Penadés, M. Carmen and Canós, José H. and Borges, Marcos R. S. and Llavador, Manuel},
title = {DPLfw: a framework for variable content document generation},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362552},
doi = {10.1145/2362536.2362552},
abstract = {Variable Data Printing solutions provide means to generate documents whose content varies according to some criteria. Since the early Mail Merge-like applications that generated letters with destination data taken from databases, different languages and frameworks have been developed with increasing levels of sophistication. Current tools allow the generation of highly customized documents that are variable not only in content, but also in layout. However, most frameworks are technology-oriented, and their use requires high skills in implementation-related tools (XML, XPATH, and others), which do not include support for domain-related tasks like identification of document content variability.In this paper, we introduce DPLfw, a framework for variable content document generation based on Software Product Line Engineering principles. It is an implementation of the Document Product Lines (DPL) approach, which was defined with the aim of supporting variable content document generation from a domain-oriented point of view. DPL models document content variability in terms of features, and product line-like processes support the generation of documents. We define the DPLfw architecture, and illustrate its use in the definition of variable-content emergency plans.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {96–105},
numpages = {10},
keywords = {DITA, document product line, feature modeling, model driven engineering, variable data printing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2647908.2655964,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Using similarity metrics for mining variability from software repositories},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655964},
doi = {10.1145/2647908.2655964},
abstract = {Much activity within software product line engineering has been concerned with explicitly representing and exploiting commonality and variability at the feature level for the purpose of a particular engineering task e.g. requirements specification, design, coding, verification, product derivation process, but not for comparing how similar products in the product line are with each other. In contrast, a case-based approach to software development is concerned with descriptions and models as a set of software cases stored in a repository for the purpose of searching at a product level, typically as a foundation for new product development. New products are derived by finding the most similar product descriptions in the repository using similarity metrics.The new idea is to use such similarity metrics for mining variability from software repositories. In this sense, software product line engineering could be informed by the case-based approach. This approach requires defining and implementing such similarity metrics based on the representations used for the software cases in such a repository. It provides complementary benefits to the ones given through feature-based representations of variability and may help mining such variability.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {32–35},
numpages = {4},
keywords = {case-based reasoning, commonality and variability, feature-based representation, product lines, similarity metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3442391.3442399,
author = {Greiner, Sandra and Westfechtel, Bernhard},
title = {On Preserving Variability Consistency in Multiple Models},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442399},
doi = {10.1145/3442391.3442399},
abstract = {Model-driven software product line engineering (MDPLE) is a holistic approach to realize variability-intensive systems by using models. In MDPLE the usage of models aims to increase the level of automation by reducing the product derivation to a pure code derivation step. Since models are present at different development phases, they have to be kept consistent all over these phases, for example by storing information about corresponding elements in model transformations. Reasons why to use model transformations or similar automated mechanisms are manifold. For instance, if the product line is built in a forward-engineering process, model transformations will be beneficial to propagate the coarse-grained information of an early phase to the subsequent phase automatically. In contrast to single-variant engineering, in MDPLE there is not only the challenge to keep multiple models consistent but also their presence conditions. Since variability mechanisms and the ways how presence conditions across different models are maintained vary, this contribution categorizes the consistency maintenance of presence conditions in MDPLE approaches to give an overview of already existing techniques. As a result, we find that while several automated solutions to keep presence conditions across models consistent exist, they are not employed in the MDPLE tool landscape.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {7},
numpages = {10},
keywords = {Model-driven Software Product Line Engineering, multi-variant model transformations, multi-view modeling, software evolution},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1007/s10270-020-00803-8,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat and Nie, Kunming},
title = {A framework for automated multi-stage and multi-step product configuration of cyber-physical systems},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00803-8},
doi = {10.1007/s10270-020-00803-8},
abstract = {Product line engineering (PLE) has been employed to large-scale cyber-physical systems (CPSs) to provide customization based on users’ needs. A PLE methodology can be characterized by its support for capturing and managing the abstractions as commonalities and variabilities and the automation of the configuration process for effective selection and customization of reusable artifacts. The automation of a configuration process heavily relies on the captured abstractions and formally specified constraints using a well-defined modeling methodology. Based on the results of our previous work and a thorough literature review, in this paper, we propose a conceptual framework to support multi-stage and multi-step automated product configuration of CPSs, including a comprehensive classification of constraints and a list of automated functionalities of a CPS configuration solution. Such a framework can serve as a guide for researchers and practitioners to evaluate an existing CPS PLE solution or devise a novel CPS PLE solution. To validate the framework, we conducted three real-world case studies. Results show that the framework fulfills all the requirements of the case studies in terms of capturing and managing variabilities and constraints. Results of the literature review indicate that the framework covers all the functionalities concerned by the literature, suggesting that the framework is complete for enabling the maximum automation of configuration in CPS PLE.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {211–265},
numpages = {55},
keywords = {Cyber-physical systems, Product line engineering, Automated configuration, Multi-stage and multi-step configuration process, Constraint classification, Variability modeling, Real-world case studies}
}

@article{10.5555/3322706.3362023,
author = {Chew, Rob and Wenger, Michael and Kery, Caroline and Nance, Jason and Richards, Keith and Hadley, Emily and Baumgartner, Peter},
title = {SMART: an open source data labeling platform for supervised learning},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {SMART is an open source web application designed to help data scientists and research teams efficiently build labeled training data sets for supervised machine learning tasks. SMART provides users with an intuitive interface for creating labeled data sets, supports active learning to help reduce the required amount of labeled data, and incorporates interrater reliability statistics to provide insight into label quality. SMART is designed to be platform agnostic and easily deployable to meet the needs of as many different research teams as possible. The project website contains links to the code repository and extensive user documentation.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2999–3003},
numpages = {5},
keywords = {active learning, data labeling, open source, software, supervised learning}
}

@inproceedings{10.1145/2648511.2648522,
author = {Schröter, Reimar and Siegmund, Norbert and Thüm, Thomas and Saake, Gunter},
title = {Feature-context interfaces: tailored programming interfaces for software product lines},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648522},
doi = {10.1145/2648511.2648522},
abstract = {Despite the wide use of software product lines, their implementation and evolution is a challenging task. When implementing a feature, a developer has to know which code fragments of other (already implemented) features are accessible in each program variant in which the feature is included. Especially for composition-based implementation techniques, in which the code is implemented in separated modules, it is an exhausting and error-prone task to find safely accessible code fragments of other modules. State-of-the-art tool support, such as product-line type checkers, detect errors a posteriori (i.e., during compilation), but fails to prevent errors during the implementation. To overcome this problem, we propose feature-context interfaces, which provide a modular and non-variable programming interface to the variable source code of a product line. These interfaces ease changes, extensions, and the maintainability of product lines. To demonstrate applicability, we implemented a content assist and an outline view in Eclipse based on feature-context interfaces. We evaluate the potential of our method by analyzing the number of potential type errors we prevent compared to state-of-the-art techniques.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {102–111},
numpages = {10},
keywords = {modularity, software product lines, syntactic interface},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {recommender systems, runtime decision-making, self-configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1155/2019/4368036,
author = {Delić, Vlado and Perić, Zoran and Sečujski, Milan and Jakovljević, Nikša and Nikolić, Jelena and Mišković, Dragiša and Simić, Nikola and Suzić, Siniša and Delić, Tijana and Gastaldo, Paolo},
title = {Speech Technology Progress Based on New Machine Learning Paradigm},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1687-5265},
url = {https://doi.org/10.1155/2019/4368036},
doi = {10.1155/2019/4368036},
abstract = {Speech technologies have been developed for decades as a typical signal processing area, while the last decade has brought a huge progress based on new machine learning paradigms. Owing not only to their intrinsic complexity but also to their relation with cognitive sciences, speech technologies are now viewed as a prime example of interdisciplinary knowledge area. This review article on speech signal analysis and processing, corresponding machine learning algorithms, and applied computational intelligence aims to give an insight into several fields, covering speech production and auditory perception, cognitive aspects of speech communication and language understanding, both speech recognition and text-to-speech synthesis in more details, and consequently the main directions in development of spoken dialogue systems. Additionally, the article discusses the concepts and recent advances in speech signal compression, coding, and transmission, including cognitive speech coding. To conclude, the main intention of this article is to highlight recent achievements and challenges based on new machine learning paradigms that, over the last decade, had an immense impact in the field of speech signal processing.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@inproceedings{10.1145/3486609.3487196,
author = {Bragança, Alexandre and Azevedo, Isabel and Bettencourt, Nuno and Morais, Carlos and Teixeira, Diogo and Caetano, David},
title = {Towards supporting SPL engineering in low-code platforms using a DSL approach},
year = {2021},
isbn = {9781450391122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486609.3487196},
doi = {10.1145/3486609.3487196},
abstract = {Low-code application platforms enable citizen developers to autonomously build complete applications, such as web applications or mobile applications. Some of these platforms also offer support for reuse to facilitate the development of similar applications. The offered mechanisms are usually elementary, they allow module reuse or building a new application from a template. However, they are insufficient to achieve the industrial level reuse necessary for software product lines (SPL). In fact, these platforms were conceived to help build standalone applications, not software families and even fewer software product lines. In this paper, we argue that the major limitation is that these platforms seldom provide access to their metamodel, the access to applications’ models and code is also limited and, therefore, makes it harder to analyze commonality and variability and construct models based on it. An approach is proposed to surpass these limitations: firstly, a metamodel of the applications built with the platform is obtained, and then, based on the metamodel, a domain-specific language (DSL) that can express the models of the applications, including variability, is constructed. With this DSL, users can combine and reuse models from different applications to explore and build similar applications. The solution is illustrated with an industrial case study. A discussion of the results is presented as well as its limitations and related work. The authors hope that this work provides inspiration and some ideas that the community can explore to facilitate the adoption and implementation of SPLs in the context, and supported by, low-code platforms.},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {16–28},
numpages = {13},
keywords = {domain specific languages, low-code platforms, software product line engineering},
location = {Chicago, IL, USA},
series = {GPCE 2021}
}

@inproceedings{10.5555/3045390.3045466,
author = {Patrini, Giorgio and Nielsen, Frank and Nock, Richard and Carioni, Marcello},
title = {Loss factorization, weakly supervised learning and label noise robustness},
year = {2016},
publisher = {JMLR.org},
abstract = {We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the same loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator -- the focal quantity of this work -- which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation.Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like SGD and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {708–717},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@article{10.1016/j.procs.2019.12.133,
author = {Rahouma, Kamel H. and Ali, Ayman},
title = {Applying Machine Learning Technology to Optimize the Operational Cost of the Egyptian Optical Network},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {163},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.133},
doi = {10.1016/j.procs.2019.12.133},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {502–517},
numpages = {16},
keywords = {Machine Learning Technology, Optical Network, Operational Cost, Intelligent Universal Platform}
}

@inproceedings{10.1145/2791060.2791089,
author = {Abbas, Nadeem and Andersson, Jesper},
title = {Harnessing variability in product-lines of self-adaptive software systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791089},
doi = {10.1145/2791060.2791089},
abstract = {This work studies systematic reuse in the context of self-adaptive software systems. In our work, we realized that managing variability for such platforms is different compared to traditional platforms, primarily due to the run-time variability and system uncertainties. Motivated by the fact that recent trends show that self-adaptation will be used more often in future system generation and that software reuse state-of-practice or research do not provide sufficient support, we have investigated the problems and possibly resolutions in this context. We have analyzed variability for these systems, using a systematic reuse prism, and identified a research gap in variability management. The analysis divides variability handling into four activities: (1) identify variability, (2) constrain variability, (3) implement variability, and (4) manage variability. Based on the findings we envision a reuse framework for the specific domain and present an example framework that addresses some of the identified challenges. We argue that it provides basic support for engineering self-adaptive software systems with systematic reuse. We discuss some important avenues of research for achieving the vision.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {191–200},
numpages = {10},
keywords = {self-adaptive software systems, software reuse, variability analysis},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1109/ICSE-C.2017.15,
author = {Martinez, Jabier and Ziadi, Tewfik and Bissyandé, Tegawendé F. and Klein, Jacques and Traon, Yves Le},
title = {Bottom-up technologies for reuse: automated extractive adoption of software product lines},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.15},
doi = {10.1109/ICSE-C.2017.15},
abstract = {Adopting Software Product Line (SPL) engineering principles demands a high up-front investment. Bottom-Up Technologies for Reuse (BUT4Reuse) is a generic and extensible tool aimed to leverage existing similar software products in order to help in extractive SPL adoption. The envisioned users are 1) SPL adopters and 2) Integrators of techniques and algorithms to provide automation in SPL adoption activities. We present the methodology it implies for both types of users and we present the validation studies that were already conducted. BUT4Reuse tool and source code are publicly available under the EPL license.Website: http://but4reuse.github.ioVideo: https://www.youtube.com/watch?v=pa62Yc9LWyk},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {67–70},
numpages = {4},
keywords = {extractive software product line adoption, reverse engineering, software product line engineering, variability management},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@inproceedings{10.1145/2245276.2245344,
author = {Calhau, Rodrigo Fernandes and de Almeida Falbo, Ricardo},
title = {A Configuration Management task ontology for semantic integration},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2245344},
doi = {10.1145/2245276.2245344},
abstract = {Configuration Management (CM) is an important task for developing complex products. It is a complex task and there are many CM systems that aim to support it. However, generally, these systems work in isolation and there is a need for integrating them. In this context, ontologies have an important role, acting as an inter-lingua to help achieving a shared conceptualization that allows semantic integration. This paper presents an ontology of the CM task. This ontology was built with the purpose of supporting semantic integration of CM systems, mainly in service and process layers of integration.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {348–353},
numpages = {6},
keywords = {configuration management, semantic integration, task ontology},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/3368089.3417063,
author = {Peng, Zi and Yang, Jinqiu and Chen, Tse-Hsun (Peter) and Ma, Lei},
title = {A first look at the integration of machine learning models in complex autonomous driving systems: a case study on Apollo},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417063},
doi = {10.1145/3368089.3417063},
abstract = {Autonomous Driving System (ADS) is one of the most promising and valuable large-scale machine learning (ML) powered systems. Hence, ADS has attracted much attention from academia and practitioners in recent years. Despite extensive study on ML models, it still lacks a comprehensive empirical study towards understanding the ML model roles, peculiar architecture, and complexity of ADS (i.e., various ML models and their relationship with non-trivial code logic). In this paper, we conduct an in-depth case study on Apollo, which is one of the state-of-the-art ADS, widely adopted by major automakers worldwide. We took the first step to reveal the integration of the underlying ML models and code logic in Apollo. In particular, we study the Apollo source code and present the underlying ML model system architecture. We present our findings on how the ML models interact with each other, and how the ML models are integrated with code logic to form a complex system. Finally, we inspect Apollo in a dynamic view and notice the heavy use of model-relevant components and the lack of adequate tests in general. Our study reveals potential maintenance challenges of complex ML-powered systems and identifies future directions to improve the quality assurance of ADS and general ML systems.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1240–1250},
numpages = {11},
keywords = {Autonomous driving systems, Empirical study, Machine learning, Model testing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3106195.3106206,
author = {Arcaini, Paolo and Gargantini, Angelo and Vavassori, Paolo},
title = {Automated Repairing of Variability Models},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106206},
doi = {10.1145/3106195.3106206},
abstract = {Variability models are a common means for describing the commonalities and differences in Software Product Lines (SPL); configurations of the SPL that respect the constraints imposed by the variability model define the problem space. The same variability is usually also captured in the final implementation through implementation constraints, defined in terms of preprocessor directives, build files, build-time errors, etc. Configurations satisfying the implementation constraints and producing correct (compilable) programs define the solution space. Since sometimes the variability model is defined after the implementation exists, it could wrongly assess the validity of some system configurations, i.e., it could consider acceptable some configurations (not belonging to the solution space) that do not permit to obtain a correct program. We here propose an approach that automatically repairs variability models such that the configurations they consider valid are also part of the solution space. Experiments show that some existing variability models are indeed faulty and can be repaired by our approach.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {9–18},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2364412.2364416,
author = {Wanderley, Fernando and da Silveira, Denis Silva and Araujo, João and Lencastre, Maria},
title = {Generating feature model from creative requirements using model driven design},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364416},
doi = {10.1145/2364412.2364416},
abstract = {Software Product Lines (SPL) have largely been taken on board by industry. Several reports from large companies such as Bosch, Nokia, Philips and Siemens witness gains and benefits achieved with their use, especially with respect to the reduction on time to market. In SPL development, domain analysis plays a central role where the relevant features are identified. Feature-Oriented Domain Analysis is a method which uses a feature model to specify variabilities and commonalities of an SPL. However, activities related to the Domain Analysis process (managing commonalities and variabilities, with users visibility and relevant stakeholders) in most cases, do not seem to be a simple and easy activity, since to represent these analyzes in modelling domain tools with a certain degree of formality, requires a technical knowledge that domain experts do not always have it prior to use. But creative requirements techniques have been suggested to facilitate the elicitation activity by filling the gap the communications problems between domain experts and software engineers, making the domain analysis more agile. Thus, to improve the domain analysis process, this paper seeks to set out the use of a creative and agile technique for modelling requirements by means of mind maps for cognitive and effective support when building feature models.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {18–25},
numpages = {8},
keywords = {feature model, mind mapping modeling, model-driven engineering, software product line},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1504/IJAC.2016.082030,
title = {Distributed system configuration management using Markov logic networks},
year = {2016},
issue_date = {January 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {2},
number = {2},
issn = {1741-8569},
url = {https://doi.org/10.1504/IJAC.2016.082030},
doi = {10.1504/IJAC.2016.082030},
abstract = {Large-scale distributed systems are playing an ever increasing role in computational research, modelling and simulation, information processing, and application hosting, via traditional high performance computing, cloud and data intensive systems. The continuous management of such systems is a critical consideration when focusing on reliability, availability, and security. As the size and complexity of these systems continue to grow, it becomes increasingly difficult to track the multitude of parameters required to ensure optimal performance from the system, especially in those systems that have been built through expansion over time as heterogeneous architectures and not as a single homogeneous platform. In this paper, we discuss the use of Markov logic networks, in a distributed multi-agent system to provide an effective means of managing these parameters. We showcase an example for analysis, providing a comparative illustration of how this approach is resolving differences between various system nodes after identifying potential configuration issues.},
journal = {Int. J. Autonomic Comput.},
month = jan,
pages = {137–154},
numpages = {18}
}

@article{10.1007/s10270-020-00791-9,
author = {Westfechtel, Bernhard and Greiner, Sandra},
title = {Extending single- to multi-variant model transformations by trace-based propagation of variability annotations},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00791-9},
doi = {10.1007/s10270-020-00791-9},
abstract = {Model-driven engineering involves the construction of models on different levels of abstraction. Software engineers are supported by model transformations, which automate the transition from high- to low-level models. Product line engineering denotes a systematic process that aims at developing different product variants from a set of reusable assets. When model-driven engineering is combined with product line engineering, engineers have to deal with multi-variant models. In annotative approaches to product line engineering, model elements are decorated with annotations, i.e., Boolean expressions that define the product variants in which model elements are to be included. In model-driven product line engineering, domain engineers require multi-variant transformations, which create multi-variant target models from multi-variant source models. We propose a reuse-based gray-box approach to realizing multi-variant model transformations. We assume that single-variant transformations already exist, which have been developed for model-driven engineering, without considering product lines. Furthermore, we assume that single-variant transformations create traces, which comprise the steps executed in order to derive target models from source models. Single-variant transformations are extended into multi-variant transformations by trace-based propagation: after executing a single-variant transformation, the resulting single-variant target model is enriched with annotations that are calculated with the help of the transformation’s trace. This approach may be applied to single-variant transformations written in different languages and requires only access to the trace, not to the respective transformation definition. We also provide a correctness criterion for trace-based propagation, and a proof that this criterion is satisfied under the prerequisites of a formal computational model.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {853–888},
numpages = {36},
keywords = {Model transformation, Software product line, Annotative variability}
}

@inproceedings{10.1145/2019136.2019187,
author = {Abbas, Nadeem},
title = {Towards autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019187},
doi = {10.1145/2019136.2019187},
abstract = {We envision an Autonomic Software Product Line (ASPL). The ASPL is a dynamic software product line that supports self adaptable products. We plan to use reflective architecture to model and develop ASPL. To evaluate the approach, we have implemented three autonomic product lines which show promising results. The ASPL approach is at initial stages, and require additional work. We plan to exploit online learning to realize more dynamic software product lines to cope with the problem of product line evolution. We propose on-line knowledge sharing among products in a product line to achieve continuous improvement of quality in product line products.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {44},
numpages = {8},
keywords = {knowledge, on-line learning, self-adaptation},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2019136.2019158,
author = {Guana, Victor and Correal, Dario},
title = {Variability quality evaluation on component-based software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019158},
doi = {10.1145/2019136.2019158},
abstract = {Quality assurance and evaluation in Model Driven Software Product Lines (MD-SPLs) are pivotal points for the growing and solidification of the generative software factories. They are framed as one of the future fact methodologies for the construction of software systems. Although several approximations address the problem of generative environments, software product line scope expression, and core asset definition, not many of them try to solve, as a fundamental step, the automation of the quality attribute evaluation in the MD-SPL development cycle. This paper presents a model-driven engineering method and a tool for the quality evaluation of product line configurations through a cross architectural view analysis.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {19},
numpages = {8},
keywords = {domain specific modeling, model composition, model-driven software product line, quality attribute, sensitivity point},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.5555/2666064.2666076,
author = {Hu, Jie and Yang, Ye and Wang, Qing and Ruhe, Guenther and Wang, Haitao},
title = {Value-based portfolio scoping: an industrial case study},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {Customization is considered as a promising way for better satisfying diversity of customer needs. In organizations short of resources, it is a frequent challenge to get balance between development and customization workload in order to ensure product success as well as customer satisfaction. In this paper, we proposed a value-based product portfolio scoping approach to determine optimal product scale for planning software product line adoption. The approach blends existing methods in domain analysis, requirements clustering, and valuation theory. An industrial case study is presented to demonstrate the application of the approach and its effectiveness.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {45–48},
numpages = {4},
keywords = {cost benefit, customization, product line, product portfolio, requirements analysis, scoping},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and Jézéquel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Software product line, Configurable system, Software variability, Software testing, Machine learning, Quality assurance}
}

@inproceedings{10.1145/2791060.2791101,
author = {Quinton, Clément and Rabiser, Rick and Vierhauser, Michael and Grünbacher, Paul and Baresi, Luciano},
title = {Evolution in dynamic software product lines: challenges and perspectives},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791101},
doi = {10.1145/2791060.2791101},
abstract = {In many domains systems need to run continuously and cannot be shut down for reconfiguration or maintenance tasks. Cyber-physical or cloud-based systems, for instance, thus often provide means to support their adaptation at runtime. The required flexibility and adaptability of systems suggests the application of Software Product Line (spl) principles to manage their variability and to support their reconfiguration. Specifically, Dynamic Software Product Lines (dspl) have been proposed to support the management and binding of variability at runtime. While spl evolution has been widely studied, it has so far not been investigated in detail in a dspl context. Variability models that are used in a dspl have to co-evolve and be kept consistent with the systems they represent to support reconfiguration even after changes to the systems at runtime. In this short paper we present a classification of the required operations for jointly evolving problem and solution space in a dspl. We analyze the impact of such operations on the consistency of a dspl and propose an approach to deal with the described issues. We describe a runtime monitoring system used in the domain of industrial automation software as an example of a dspl evolving at runtime to motivate and explain our work.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {126–130},
numpages = {5},
keywords = {consistency, dynamic software product lines, evolution},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2364412.2364439,
author = {Vale, Tassio and Figueiredo, Gustavo Bittencourt and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {A study on service identification methods for software product lines},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364439},
doi = {10.1145/2364412.2364439},
abstract = {The combination of service-orientation and software product line engineering, called Service-Oriented Product Line Engineering (SOPLE) have received attention by researchers and practitioners in the last years, and these areas can address issues of each other. One service-orientation issue is service identification. It consists of determining candidate services to a service-oriented environment based on pre-existing software artifacts, e.g., business process, source code, and so on. In order to provide a systematic identification of services, there are many available service identification methods in the literature, regarding different understanding of services, goals, and techniques. Due to this heterogeneity, this paper presents an in-depth comparison of service identification methods as well as a recommendation of the most suitable ones in the SOPLE context. This work can help the decision making of the most suitable method according to stakeholders' needs.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {156–163},
numpages = {8},
keywords = {service identification, service-oriented computing, service-oriented product lines, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2647648.2647649,
author = {Raschke, Wolfgang and Zilli, Massimiliano and Loinig, Johannes and Weiss, Reinhold and Steger, Christian and Kreiner, Christian},
title = {Embedding research in the industrial field: a case of a transition to a software product line},
year = {2014},
isbn = {9781450330459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647648.2647649},
doi = {10.1145/2647648.2647649},
abstract = {Java Cards [4, 5] are small resource-constrained embedded systems that have to fulfill rigorous security requirements. Multiple application scenarios demand diverse product performance profiles which are targeted towards markets such as banking applications and mobile applications. In order to tailor the products to the customer's needs we implemented a Software Product Line (SPL). This paper reports on the industrial case of an adoption to a SPL during the development of a highly-secure software system. In order to provide a scientific method which allows the description of research in the field, we apply Action Research (AR). The rationale of AR is to foster the transition of knowledge from a mature research field to practical problems encountered in the daily routine. Thus, AR is capable of providing insights which might be overlooked in a traditional research approach. In this paper we follow the iterative AR process, and report on the successful transfer of knowledge from a research project to a real industrial application.},
booktitle = {Proceedings of the 2014 International Workshop on Long-Term Industrial Collaboration on Software Engineering},
pages = {3–8},
numpages = {6},
keywords = {action research, knowledge transfer, software reuse},
location = {Vasteras, Sweden},
series = {WISE '14}
}

@inproceedings{10.1145/2791060.2791093,
author = {Souto, Sabrina and Gopinath, Divya and d'Amorim, Marcelo and Marinov, Darko and Khurshid, Sarfraz and Batory, Don},
title = {Faster bug detection for software product lines with incomplete feature models},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791093},
doi = {10.1145/2791060.2791093},
abstract = {A software product line (SPL) is a family of programs that are differentiated by features --- increments in functionality. Systematically testing an SPL is challenging because it requires running each test of a test suite against a combinatorial number of programs. Feature models capture dependencies among features and can (1) reduce the space of programs to test and (2) enable accurate categorization of failing tests as failures of programs or the tests themselves, not as failures due to illegal combinations of features. In practice, sadly, feature models are not always available.We introduce SPLif, the first approach for testing SPLs that does not require the a priori availability of feature models. Our insight is to use a profile of passing and failing test runs to quickly identify failures that are indicative of real problems in test or code rather than specious failures due to illegal feature combinations.Experimental results on five SPLs and one large configurable system (GCC) demonstrate the effectiveness of our approach. SPLif enabled the discovery of five news bugs in GCC, three of which have already been fixed.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {151–160},
numpages = {10},
keywords = {GCC, feature models, software testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.5555/2331762.2331766,
author = {Sekiguchi, Atsuji and Shimada, Kuniaki and Wada, Yuji and Ooba, Akio and Yoshimi, Ryouji and Matsumoto, Akiko},
title = {Configuration management technology using tree structures of ICT systems},
year = {2012},
isbn = {9781618397850},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {In large-scale ICT (Information and Communication Technology) systems, such as cloud computing, a reduction of operation management workloads and stabilization of the systems are requested. Configuration management copes with these problems. Configuration management is a process of the design, verification, and deployment of configurations of ICT systems. Configuration management can reduce the workloads and improper settings of design by describing the rules for the design and verifying the configurations. However, the problem is that it is too hard for non-experts to describe the rules correctly. We thus focused on the characteristics of the configurations that have relationships (such as same/different) between two devices or software that corresponds to the tree structure of the ICT system (such as servers, racks, and data centers). By using the relationships as verification rules that should be satisfied by the configurations, we developed configuration management technology that does not require rules written by operation managers. This technology reduces the workloads on design and verification by consolidating the same configurations and reducing the number of managed configurations. Moreover, the technology discovers improper settings by verifying the configurations based on the relationships. We implemented a prototype of our technology and applied it to real systems. As a result, the following effects were confirmed. 1) 94 percent (117,632/125,286) of the configurations converged under an environment of servers with uniform configurations, and 94 percent of the workloads were also reduced. 2) Improper settings were successfully discovered under two different systems.},
booktitle = {Proceedings of the 15th Communications and Networking Simulation Symposium},
articleno = {4},
numpages = {7},
keywords = {cloud computing, configuration management, data center, system operation management, verification},
location = {Orlando, Florida},
series = {CNS '12}
}

@inproceedings{10.1145/2648511.2648544,
author = {Nöbauer, Markus and Seyff, Norbert and Groher, Iris},
title = {Inferring variability from customized standard software products},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648544},
doi = {10.1145/2648511.2648544},
abstract = {Systematic variability management is an important prerequisite for successful software reuse. However, it requires significant effort and extensive domain knowledge to document and maintain information on variability. In this paper we present a tool-supported approach which supports semi-automatically inferring variability information from customized standard software products. The approach does not only enable the identification and documentation of variability information based on existing products, it is also capable of incrementally updating this information. To guarantee quick access to reusable code artifacts (e.g. requirements, features or software components), the presented solution stores these artifacts together with related requirements and a generated variability model in an asset repository. The tool-supported approach has been applied to customizations of Microsoft Dynamics AX ERP systems. Our experiences highlight the potential and benefits of our approach compared to manually gathering information on software variability.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {284–293},
numpages = {10},
keywords = {ERP systems, reuse, standard software product customizations, variability inference},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2491627.2491652,
author = {Eichelberger, Holger and Schmid, Klaus},
title = {A systematic analysis of textual variability modeling languages},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491652},
doi = {10.1145/2491627.2491652},
abstract = {Industrial variability models tend to grow in size and complexity due to ever-increasing functionality and complexity of software systems. Some authors report on variability models specifying several thousands of variabilities. However, traditional variability modeling approaches do not seem to scale adequately to cope with size and complexity of such models. Recently, textual variability modeling languages have been advocated as one scalable solution.In this paper, we provide a systematic analysis of the capabilities of current textual variability modeling languages, in particular regarding variability management in the large. Towards this aim, we define a classification schema consisting of five dimensions, classify ten different textual variability modeling languages using the classification schema and provide an analysis. In summary, some textual variability modeling languages go beyond textual representations of traditional variability modeling approaches and provide sophisticated modeling concepts and constraint languages. Three textual variability modeling approaches already support mechanisms for large-scale variability modeling such as model composition, modularization, or evolution support.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {12–21},
numpages = {10},
keywords = {scalability, software product lines, textual variability modeling, variability management, variability modeling in the large},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.5555/1753235.1753241,
author = {John, Isabel and Eisenbarth, Michael},
title = {A decade of scoping: a survey},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Scoping can be defined as the process of deciding in which parts of an organization's products, features and domains systematic reuse is economically useful. It generally is the first phase in product line engineering. For a decade now scoping has been recognized as a discipline of it's own in product line engineering. So it's time to look at what has been done in scoping in the last years and what is still to be done. In this survey, we identify and characterize existing scoping approaches with the main goal to derive open areas and research questions for further research in scoping. We analyze and compare existing approaches and derive open and partially addressed research questions that can be tackled by researchers in product line engineering in the next years.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {31–40},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.cor.2004.07.011,
author = {(Sundar) Balakrishnan, P. V. and Gupta, Rakesh and Jacob, Varghese S.},
title = {An investigation of mating and population maintenance strategies in hybrid genetic heuristics for product line designs},
year = {2006},
issue_date = {March 2006},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {33},
number = {3},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2004.07.011},
doi = {10.1016/j.cor.2004.07.011},
abstract = {This research builds on prior work on developing near optimal solutions to the product line design problems within the conjoint analysis framework. In this research, we investigate and compare different genetic algorithm operators; in particular, we examine systematically the impact of employing alternative population maintenance strategies and mutation techniques within our problem context. Two alternative population maintenance methods, that we term ''Emigration'' and ''Malthusian'' strategies, are deployed to govern how individual product lines in one generation are carried over to the next generation. We also allow for two different types of reproduction methods termed ''Equal Opportunity'' in which the parents to be paired for mating are selected with equal opportunity and a second based on always choosing the best string in the current generation as one of the parents which is referred to as the ''Queen bee'', while the other parent is randomly selected from the set of parent strings. We also look at the impact of integrating the artificial intelligence approach with a traditional optimization approach by seeding the GA with solutions obtained from a Dynamic Programming heuristic proposed by others. A detailed statistical analysis is also carried out to determine the impact of various problem and technique aspects on multiple measures of performance through means of a Monte Carlo simulation study. Our results indicate that such proposed procedures are able to provide multiple ''good'' solutions. This provides more flexibility for the decision makers as they now have the opportunity to select from a number of very good product lines. The results obtained using our approaches are encouraging, with statistically significant improvements averaging 5% or more, when compared to the traditional benchmark of the heuristic dynamic programming technique.},
journal = {Comput. Oper. Res.},
month = mar,
pages = {639–659},
numpages = {21},
keywords = {Attribute importance, Conjoint analysis, Decision support, Dynamic programming, Genetic algorithms, Hybrid heuristics, Malthusian, Marketing}
}

@article{10.1016/j.jss.2019.04.051,
author = {Sampaio, Gabriela and Borba, Paulo and Teixeira, Leopoldo},
title = {Partially safe evolution of software product lines},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.051},
doi = {10.1016/j.jss.2019.04.051},
journal = {J. Syst. Softw.},
month = sep,
pages = {17–42},
numpages = {26},
keywords = {Product line evolution, Product line maintenance, Product line refinement}
}

@inproceedings{10.1145/3109729.3109746,
author = {Estrada-Torres, Bedilia},
title = {Improve Performance Management in Flexible Business Processes},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109746},
doi = {10.1145/3109729.3109746},
abstract = {The performance of business processes is evaluated and monitored with the aim of identifying whether strategic and operational goals are being achieved. Most approaches about performance measurement have been defined over traditional highly repetitive and well-structured processes. However, current organizational and business needs have encouraged the appearance of customizable processes to manage collections of process variants derived from a process, and loosely specified processes to manage non-repeatable and unpredictable processes. However, current techniques of performance measurement have not evolved to the same pace that business processes, thus generating a gap between processes and the measurement of their performance. The thesis introduced in this paper, is focused on enhancing the performance measurement of business processes by means of the improvement of existing techniques for the definition of process performance indicators and their applicability to different types of processes. With this purpose a set of artifacts, including a metamodel, notations, tools and methodologies will be developed. They will be validated by means of case studies based on real scenarios.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {145–149},
numpages = {5},
keywords = {Business process management, flexible processes, performance indicators},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2934466.2934469,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof and Yu, Huiqun},
title = {A mathematical model of performance-relevant feature interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934469},
doi = {10.1145/2934466.2934469},
abstract = {Modern software systems have grown significantly in their size and complexity, therefore understanding how software systems behave when there are many configuration options, also called features, is no longer a trivial task. This is primarily due to the potentially complex interactions among the features. In this paper, we propose a novel mathematical model for performance-relevant, or quantitative in general, feature interactions, based on the theory of Boolean functions. Moreover, we provide two algorithms for detecting all such interactions with little measurement effort and potentially guaranteed accuracy and confidence level. Empirical results on real-world configurable systems demonstrated the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {25–34},
numpages = {10},
keywords = {boolean functions, feature interactions, fourier transform, performance},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.jss.2018.07.054,
author = {Ochoa, Lina and González-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.054},
doi = {10.1016/j.jss.2018.07.054},
journal = {J. Syst. Softw.},
month = oct,
pages = {511–532},
numpages = {22},
keywords = {Extended product line, Product configuration, Systematic literature review}
}

@inproceedings{10.1145/2362536.2362553,
author = {Andersen, Nele and Czarnecki, Krzysztof and She, Steven and Wąsowski, Andrzej},
title = {Efficient synthesis of feature models},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362553},
doi = {10.1145/2362536.2362553},
abstract = {Variability modeling, and in particular feature modeling, is a central element of model-driven software product line architectures. Such architectures often emerge from legacy code, but, unfortunately creating feature models from large, legacy systems is a long and arduous task.We address the problem of automatic synthesis of feature models from propositional constraints. We show that this problem is NP-hard. We design efficient techniques for synthesis of models from respectively CNF and DNF formulas, showing a 10- to 1000-fold performance improvement over known techniques for realistic benchmarks.Our algorithms are the first known techniques that are efficient enough to be applied to dependencies extracted from real systems, opening new possibilities of creating reverse engineering and model management tools for variability models. We discuss several such scenarios in the paper.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {106–115},
numpages = {10},
keywords = {feature models, software product lines, variability models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2362536.2362546,
author = {Myllärniemi, Varvana and Raatikainen, Mikko and Männistö, Tomi},
title = {A systematically conducted literature review: quality attribute variability in software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362546},
doi = {10.1145/2362536.2362546},
abstract = {Typically, products in a software product line differ by their functionality, and quality attributes are not intentionally varied. Why, how, and which quality attributes to vary has remained an open issue. A systematically conducted literature review on quality attribute variability is presented, where primary studies are selected by reading all content of full studies in Software Product Line Conference. The results indicate that the success of feature modeling influences the proposed approaches, different approaches suit specific quality attributes differently, and empirical evidence on industrial quality variability is lacking.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {41–45},
numpages = {5},
keywords = {quality attribute, systematic literature review, variability},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1007/978-3-030-38085-4_19,
author = {Christodoulopoulos, Konstantinos and Sartzetakis, Ippokratis and Soumplis, Polizois and Varvarigos, Emmanouel (Manos)},
title = {Machine Learning Assisted Quality of Transmission Estimation and Planning with Reduced Margins},
year = {2019},
isbn = {978-3-030-38084-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38085-4_19},
doi = {10.1007/978-3-030-38085-4_19},
abstract = {In optical transport networks, the Quality of Transmission (QoT) using a physical layer model (PLM) is estimated before establishing new or reconfiguring established optical connections. Traditionally, high margins are added to account for the model’s inaccuracy and the uncertainty in the current and evolving physical layer conditions, targeting uninterrupted operation for several years, until the end-of-life (EOL). Reducing the margins increases network efficiency but requires accurate QoT estimation. We present two machine learning (ML) assisted QoT estimators that leverage monitoring data of existing connections to understand the actual physical layer conditions and achieve high estimation accuracy. We then quantify the benefits of planning/upgrading a network over multiple periods with accurate QoT estimation as opposed to planning with EOL margins.},
booktitle = {Optical Network Design and Modeling: 23rd IFIP WG 6.10 International Conference, ONDM 2019, Athens, Greece, May 13–16, 2019, Proceedings},
pages = {211–222},
numpages = {12},
keywords = {Overprovisioning, Static network planning, End-of-life margins, Physical layer impairments, Monitoring, Cross-layer optimization, Incremental multi-period planning, Marginless},
location = {Athens, Greece}
}

@techreport{10.5555/2206214,
author = {Johnson, L. Arnold and Dempsey, Kelley L. and Ross, Ronald S. and Gupta, Sarbari and Bailey, Dennis},
title = {SP 800-128. Guide for Security-Focused Configuration Management of Information Systems},
year = {2011},
publisher = {National Institute of Standards &amp; Technology},
address = {Gaithersburg, MD, USA},
abstract = {The purpose of Special Publication 800-128, Guide for Security-Focused Configuration Management of Information Systems, is to provide guidelines for organizations responsible for managing and administering the security of federal information systems and associated environments of operation. Configuration management concepts and principles described in NIST SP 800-128, provide supporting information for NIST SP 800-53, Recommended Security Controls for Federal Information Systems and Organizations. NIST SP 800-128 assumes that information security is an integral part of an organization s overall configuration management. The focus of this document is on implementation of the information system security aspects of configuration management, and as such the term security-focused configuration management (SecCM) is used to emphasize the concentration on information security. In addition to the fundamental concepts associated with SecCM, the process of applying SecCM practices to information systems is described. The goal of SecCM activities is to manage and monitor the configurations of information systems to achieve adequate security and minimize organizational risk while supporting the desired business functionality and services.}
}

@inproceedings{10.1145/3377812.3381399,
author = {Abbas, Muhammad},
title = {Variability aware requirements reuse analysis},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381399},
doi = {10.1145/3377812.3381399},
abstract = {Problem: The goal of a software product line is to aid quick and quality delivery of software products, sharing common features. Effectively achieving the above-mentioned goals requires reuse analysis of the product line features. Existing requirements reuse analysis approaches are not focused on recommending product line features, that can be reused to realize new customer requirements. Hypothesis: Given that the customer requirements are linked to product line features' description satisfying them: then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customer requirements can be evaluated against existing customer requirements and reuse of product line features can be recommended. Contributions: We treated the problem of feature reuse analysis as a text classification problem at the requirements-level. We use Natural Language Processing and clustering to recommend reuse of features based on similarities and historic reuse information. The recommendations can be used to realize new customer requirements.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {190–193},
numpages = {4},
keywords = {product line, requirements, similarities, software reuse, variability},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2934466.2934488,
author = {Plakidas, Konstantinos and Stevanetic, Srdjan and Schall, Daniel and Ionescu, Tudor B. and Zdun, Uwe},
title = {How do software ecosystems evolve? a quantitative assessment of the r ecosystem.},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934488},
doi = {10.1145/2934466.2934488},
abstract = {In this work we advance the understanding of software eco-systems research by examining the structure and evolution of the R statistical computing open-source ecosystem. Our research attempts to shed light on the following intriguing question: what makes software ecosystems successful? The approach we follow is to perform a quantitative analysis of the R ecosystem. R is a well-established and popular ecosystem, whose community and marketplace are steadily growing. We assess and quantify the ecosystem throughout its history, and derive metrics on its core software components, the marketplace as well as its community. We use our insights to make observations that are applicable to ecosystems in general, validate existing theories from the literature, and propose a predictive model for the evolution of software packages. Our results show that the success of the ecosystem relies on a strong commitment by a small core of users who support a large and growing community.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {89–98},
numpages = {10},
keywords = {R, empirical study, predictive model, software ecosystems},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791072,
author = {Patel, Sachin and Shah, Vipul},
title = {Automated testing of software-as-a-service configurations using a variability language},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791072},
doi = {10.1145/2791060.2791072},
abstract = {The benefits offered by cloud technologies have compelled enterprises to adopt the Software-as-a-Service (SaaS) model for their enterprise software needs. A SaaS has to be configured or customized to suit the specific requirements of every enterprise that subscribes to it. IT service providers have to deal with the problem of testing many such configurations created for different enterprises. The software gets upgraded periodically and the configurations need to be tested on an ongoing basis to ensure business continuity. In order to run the testing organization efficiently, it is imperative that the test cycle is automated. Developing automated test scripts for a large number of configurations is a non-trivial task because differences across them may range from a few user interface changes to business process level changes. We propose an approach that combines the benefits of model driven engineering and variability modeling to address this issue. The approach comprises of the Enterprise Software Test Modeling Language to model the test cases. We use the Common Variability Language to model variability in the test cases and apply model transformations on a base model to generate a test model for each configuration. These models are used to generate automated test scripts for all the configurations. We describe the test modelling language and an experiment which shows that the approach can be used to automatically generate variations in automated test scripts.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {253–262},
numpages = {10},
keywords = {enterprise software testing, model based testing, software-as-a-service, test automation, variability specification},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1007/s10270-016-0539-8,
author = {Hajri, Ines and Goknil, Arda and Briand, Lionel C. and Stephany, Thierry},
title = {Configuring use case models in product families},
year = {2018},
issue_date = {July      2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0539-8},
doi = {10.1007/s10270-016-0539-8},
abstract = {In many domains such as automotive and avionics, the size and complexity of software systems is quickly increasing. At the same time, many stakeholders tend to be involved in the development of such systems, which typically must also be configured for multiple customers with varying needs. Product Line Engineering (PLE) is therefore an inevitable practice for such systems. Furthermore, because in many areas requirements must be explicit and traceability to them is required by standards, use cases and domain models are common practice for requirements elicitation and analysis. In this paper, based on the above observations, we aim at supporting PLE in the context of use case-centric development. Therefore, we propose, apply, and assess a use case-driven configuration approach which interactively receives configuration decisions from the analysts to generate product-specific (PS) use case and domain models. Our approach provides the following: (1) a use case-centric product line modeling method (PUM), (2) automated, interactive configuration support based on PUM, and (3) an automatic generation of PS use case and domain models from Product Line (PL) models and configuration decisions. The approach is supported by a tool relying on Natural Language Processing (NLP) and integrated with an industrial requirements management tool, i.e., IBM DOORS. We successfully applied and evaluated our approach to an industrial case study in the automotive domain, thus showing evidence that the approach is practical and beneficial to capture variability at the appropriate level of granularity and to configure PS use case and domain models in industrial settings.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {939–971},
numpages = {33},
keywords = {Configuration, Consistency checking, Natural language processing, Product line engineering, Use case-driven development}
}

@article{10.1007/s10845-012-0641-x,
author = {Fujita, Kikuo and Amaya, Hirofumi and Akai, Ryota},
title = {Mathematical model for simultaneous design of module commonalization and supply chain configuration toward global product family},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {5},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-012-0641-x},
doi = {10.1007/s10845-012-0641-x},
abstract = {Manufacturing today has become global in all aspects marketing, design, production, distribution, etc. While product family design has been an essential viewpoint for meeting the demand for product variety, its interaction with the issues of supply chain, market systems, etc. makes the meaning of product family both broad and more complicated. In this paper we call such situation `global product family,' and first characterizes its components and complexity. Following this, we proposes a mathematical model for the simultaneous design problem of module commonalization strategies under the given product architecture and supply chain configuration through selection of manufacturing sites for module production, assembly and final distribution as an instance of the problems. In the model, the choice of modules and various sites are represented with 0-1 design variables with the volume of production and transportation represented with non-negative continuous design variables, and the objective defined on total cost. An optimization method is configured with a genetic algorithm and a simplex method for such a mixed integer programming problem. Some numerical case studies are included to determine the validity and promise of the developed mathematical model and algorithm. Finally, we conclude with some discussion of future work.},
journal = {J. Intell. Manuf.},
month = oct,
pages = {991–1004},
numpages = {14},
keywords = {Commonalization, Globalization, Market systems, Optimal design, Product family and platform, Product variety, Supply chain configuration}
}

@inproceedings{10.1145/1985484.1985490,
author = {Stallinger, Fritz and Neumann, Robert and Schossleitner, Robert and Kriener, Stephan},
title = {Migrating towards evolving software product lines: challenges of an SME in a core customer-driven industrial systems engineering context},
year = {2011},
isbn = {9781450305846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985484.1985490},
doi = {10.1145/1985484.1985490},
abstract = {In this paper we identify key challenges a medium-sized software organization is facing in migrating towards Software Product Line Engineering (SPLE). The software engineering context of the company is characterized by a two-fold access to the market - core customer driven product enhancement and product development for a broader, anonymous market - and the embedding of software engineering in multi-disciplinary systems and solutions engineering.Based on a characterization of the business, the software product subject to migration towards SPLE, and the goals and background of the SPLE initiative, seven key challenges with respect to the migration are identified. These challenges relate to process diversity in the face of multiple reuse approaches; the management of requirements and variability; the integration of requirements traceability and variability management; legacy software and discipline vs. software-specific modularization; integration with systems engineering; costing and pricing models; and project vs. product documentation.},
booktitle = {Proceedings of the 2nd International Workshop on Product Line Approaches in Software Engineering},
pages = {20–24},
numpages = {5},
keywords = {sme, software engineering, software product line, software product migration, systems engineering},
location = {Waikiki, Honolulu, HI, USA},
series = {PLEASE '11}
}

@inproceedings{10.1145/1414558.1414606,
author = {Jiang, Keyuan and Kamali, Reza},
title = {Integration of configuration management into the IT curriculum},
year = {2008},
isbn = {9781605583297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414558.1414606},
doi = {10.1145/1414558.1414606},
abstract = {The discipline of information technology deals with the selection, creation, application, integration and administration of computing technologies. With the prevailing adoption of information technology, growing sophistication and complexity of today's technology solutions, and increased security threats to the existing systems, it becomes more and more challenging to maintain and support IT operations with the highest availability possible. One of the important and useful practices is to design and implement the appropriate configuration management strategies in the operations, administration and development of IT applications and systems.Configuration management (CM) is a discipline of technical management that ensures the consistency of a product's performance, its quality, functional and physical attributes with its intended requirements, design, and operational information throughout its life. The practice of configuration management applies to every aspect of information technology, ranging from application development, to operating system setup, and to security management.Despite its importance, many practitioners, especially those fresh out of college, consider configuration management as an after-thought. Among all the computing related disciplines defined by ACM Computing Curricula, software engineering is the only one that offers configuration management in its curriculum but with a limited scope.In developing our new IT curriculum based upon the IT Curriculum 2005, we integrated configuration management into our curriculum by developing a course titled "Application Configuration and Management". In the course, students are exposed to various configuration management standards, best practices, techniques, and tools. A series of laboratory exercises were also developed to enhance students' understanding and learning of what is actually involved in the practice of configuration management.},
booktitle = {Proceedings of the 9th ACM SIGITE Conference on Information Technology Education},
pages = {183–186},
numpages = {4},
keywords = {configuration management, it curriculum development},
location = {Cincinnati, OH, USA},
series = {SIGITE '08}
}

@inproceedings{10.1145/2648511.2648516,
author = {Reinhartz-Berger, Iris and Figl, Kathrin},
title = {Comprehensibility of orthogonal variability modeling languages: the cases of CVL and OVM},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648516},
doi = {10.1145/2648511.2648516},
abstract = {As the complexity and variety of systems and software products have increased, the ability to manage their variability effectively and efficiently became crucial. To this end, variability can be specified either as an integral part of the development artifacts or in a separate orthogonal variability model. Lately, orthogonal variability models attract a lot of attention due to the fact that they do not require changing the complexity of the development artifacts and can be used in conjunction with different development artifacts. Despite this attention and to the best of our knowledge, no empirical study examined the comprehensibility of orthogonal variability models.In this work, we conducted an exploratory experiment to examine potential comprehension problems in two common orthogonal variability modeling languages, namely, Common Variability Language (CVL) and Orthogonal Variability Model (OVM). We examined the comprehensibility of the variability models and their relations to the development artifacts for novice users. To measure comprehensibility we used comprehension score (i.e., percentage of correct solution), time spent to complete tasks, and participants' perception of difficulty of different model constructs. The results showed high comprehensibility of the variability models, but low comprehensibility of the relations between the variability models and the development artifacts. Although the comprehensibility of CVL and OVM was similar in terms of comprehension score and time spent to complete tasks, novice users perceived OVM as more difficult to comprehend.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {42–51},
numpages = {10},
keywords = {CVL, OVM, empirical study, model comprehension, variability analysis},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1007/s10586-020-03210-2,
author = {Chiang, Ron C.},
title = {Contention-aware container placement strategy for docker swarm with machine learning based clustering algorithms},
year = {2020},
issue_date = {Feb 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-020-03210-2},
doi = {10.1007/s10586-020-03210-2},
abstract = {Containerization technology utilizes operating system level virtualization to package applications to run with required libraries and are isolated from other processes on the same host. Lightweight and quick deployment make containers popular in many data centers. Running distributed applications in data centers usually involves multiple clusters of machines. Docker Swarm is a container orchestration tool for managing a cluster of Docker containers and their hosts. However, Docker Swarm’s scheduler does not consider resource utilization when placing containers in a cluster. This paper first investigated performance interference in container clusters. Our experimental study showed that distributed applications’ performance can be degraded when co-located with other containers which aggressively consume resources. A new scheduler is proposed to improve performance while keeping high resource utilization. The experimental results demonstrated that the proposed prototype with machine learning based clustering algorithms could effectively improve distributed applications’ performance by up to 14.5% with an average at around 12%. This work also provides theoretical bounds for the container placement problem.},
journal = {Cluster Computing},
month = nov,
pages = {13–23},
numpages = {11},
keywords = {Cloud computing, Container, Virtualization, Serverless Computing, Microservices}
}

@inproceedings{10.1145/2934466.2934467,
author = {Ferreira, Gabriel and Malik, Momin and Kästner, Christian and Pfeffer, Jürgen and Apel, Sven},
title = {Do #ifdefs influence the occurrence of vulnerabilities? an empirical study of the linux kernel},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934467},
doi = {10.1145/2934466.2934467},
abstract = {Preprocessors support the diversification of software products with #ifdefs, but also require additional effort from developers to maintain and understand variable code. We conjecture that #ifdefs cause developers to produce more vulnerable code because they are required to reason about multiple features simultaneously and maintain complex mental models of dependencies of configurable code.We extracted a variational call graph across all configurations of the Linux kernel, and used configuration complexity metrics to compare vulnerable and non-vulnerable functions considering their vulnerability history. Our goal was to learn about whether we can observe a measurable influence of configuration complexity on the occurrence of vulnerabilities.Our results suggest, among others, that vulnerable functions have higher variability than non-vulnerable ones and are also constrained by fewer configuration options. This suggests that developers are inclined to notice functions appear in frequently-compiled product variants. We aim to raise developers' awareness to address variability more systematically, since configuration complexity is an important, but often ignored aspect of software product lines.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {65–73},
numpages = {9},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.ins.2008.03.002,
author = {Ahmed, F. and Capretz, L. F. and Samarabandu, J.},
title = {Fuzzy inference system for software product family process evaluation},
year = {2008},
issue_date = {July, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {178},
number = {13},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2008.03.002},
doi = {10.1016/j.ins.2008.03.002},
abstract = {When developing multiple products within a common application domain, systematic use of a software product family process can yield increased productivity in cost, quality, effort and schedule. Such a process provides the means for the reuse of software assets which can considerably reduce the development time and the cost of software products. A comprehensive strategy for the evaluating the maturity of a software product family process is needed due to growing popularity of this concept in the software industry. In this paper, we propose a five-level maturity scale for software product family process. We also present a fuzzy inference system for evaluating maturity of software product family process using the proposed maturity scale. This research is aimed at establishing a comprehensive and unified strategy for process evaluation of a software product family. Such a process evaluation strategy will enable an organization to discover and monitor the strengths and weaknesses of the various activities performed during development of multiple products within a common application domain.},
journal = {Inf. Sci.},
month = jul,
pages = {2780–2793},
numpages = {14},
keywords = {Adaptive neuro-fuzzy inference system, Fuzzy logic, Process maturity, Software engineering, Software process assessment, Software product family}
}

@inproceedings{10.1145/2791060.2791102,
author = {Muñoz-Fernández, Juan C. and Tamura, Gabriel and Raicu, Irina and Mazo, Raúl and Salinesi, Camille},
title = {REFAS: a PLE approach for simulation of self-adaptive systems requirements},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791102},
doi = {10.1145/2791060.2791102},
abstract = {Model simulation has demonstrated its usefulness in evaluation and decision-making for improving preliminary versions of artefacts before production. Particularly, one of the main goals of simulation is to verify model properties based on data collected from its execution. In this paper, we present the simulation capabilities of our REFAS framework for specifying requirements models for dynamic software products lines and self-adaptive systems. The simulation is controlled by a feedback loop and a reasoning engine that operates on the functional and non-functional requirements. The paper contribution is threefold. First, REFAS allows developers to evaluate and improve requirements models through their simulation capabilities. Second, REFAS provides rich feedback in its interactive simulations for the human modeller to make informed decisions to improve her model. Third, REFAS automates the generation of simulation scenarios required to verify the model adequacy and correctness. We evaluate our contribution by comparing the application of REFAS to a case study used in other approaches.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {121–125},
numpages = {5},
keywords = {MAPE-K loops, dynamic adaptation, dynamic software product lines, requirements engineering, simulation},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791070,
author = {Liang, Jia Hui and Ganesh, Vijay and Czarnecki, Krzysztof and Raman, Venkatesh},
title = {SAT-based analysis of large real-world feature models is easy},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791070},
doi = {10.1145/2791060.2791070},
abstract = {Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide efficient automatic analysis of real-world feature models (FM) of systems ranging from cars to operating systems. It is well-known that solver-based analysis of real-world FMs scale very well even though SAT instances obtained from such FMs are large, and the corresponding analysis problems are known to be NP-complete. To better understand why SAT solvers are so effective, we systematically studied many syntactic and semantic characteristics of a representative set of large real-world FMs. We discovered that a key reason why large real-world FMs are easy-to-analyze is that the vast majority of the variables in these models are unrestricted, i.e., the models are satisfiable for both true and false assignments to such variables under the current partial assignment. Given this discovery and our understanding of CDCL SAT solvers, we show that solvers can easily find satisfying assignments for such models without too many backtracks relative to the model size, explaining why solvers scale so well. Further analysis showed that the presence of unrestricted variables in these real-world models can be attributed to their high-degree of variability. Additionally, we experimented with a series of well-known nonbacktracking simplifications that are particularly effective in solving FMs. The remaining variables/clauses after simplifications, called the core, are so few that they are easily solved even with backtracking, further strengthening our conclusions. We explain the connection between our findings and backdoors, an idea posited by theorists to explain the power of SAT solvers. This connection strengthens our hypothesis that SAT-based analysis of FMs is easy. In contrast to our findings, previous research characterizes the difficulty of analyzing randomly-generated FMs in terms of treewidth. Our experiments suggest that the difficulty of analyzing real-world FMs cannot be explained in terms of treewidth.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {91–100},
numpages = {10},
keywords = {SAT-based analysis, feature model},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.ipm.2009.09.001,
author = {Johnson Lim, Soon Chong and Liu, Ying and Lee, Wing Bun},
title = {Multi-facet product information search and retrieval using semantically annotated product family ontology},
year = {2010},
issue_date = {July, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {46},
number = {4},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2009.09.001},
doi = {10.1016/j.ipm.2009.09.001},
abstract = {With the advent of various services and applications of Semantic Web, semantic annotation has emerged as an important research topic. The application of semantically annotated ontology had been evident in numerous information processing and retrieval tasks. One of such tasks is utilizing the semantically annotated ontology in product design which is able to suggest many important applications that are critical to aid various design related tasks. However, ontology development in design engineering remains a time consuming and tedious task that demands considerable human efforts. In the context of product family design, management of different product information that features efficient indexing, update, navigation, search and retrieval across product families is both desirable and challenging. For instance, an efficient way of retrieving timely information on product family can be useful for tasks such as product family redesign and new product variant derivation when requirements change. However, the current research and application of information search and navigation in product family is mostly limited to its structural aspect which is insufficient to handle advanced information search especially when the query targets at multiple aspects of a product. This paper attempts to address this problem by proposing an information search and retrieval framework based on the semantically annotated multi-facet product family ontology. Particularly, we propose a document profile (DP) model to suggest semantic tags for annotation purpose. Using a case study of digital camera families, we illustrate how the faceted search and retrieval of product information can be accomplished. We also exemplify how we can derive new product variants based on the designer's query of requirements via the faceted search and retrieval of product family information. Lastly, in order to highlight the value of our current work, we briefly discuss some further research and applications in design decision support, e.g. commonality analysis and variety comparison, based on the semantically annotated multi-facet product family ontology.},
journal = {Inf. Process. Manage.},
month = jul,
pages = {479–493},
numpages = {15},
keywords = {Information management and retrieval, Multi-facet, Ontology, Product family, Semantic annotation}
}

@inproceedings{10.1145/1370152.1370158,
author = {Kögel, Maximilian},
title = {Towards software configuration management for unified models},
year = {2008},
isbn = {9781605580456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370152.1370158},
doi = {10.1145/1370152.1370158},
abstract = {Change occurs throughout the software lifecycle. Software Configuration Management tools and techniques provide the foundation to effectively control change. With a growing number of approaches combining models from different domains into one unified, integrated model, ([15], [12]), there is also an emerging demand for SCM techniques and methods that are able to support these unified models. Traditional SCM systems operating on the abstraction of a filesystem and managing change at the granularity of textual lines are not adequate for these requirements. We propose a novel approach to SCM for unified models combining product versioning, operation-based deltas and change packages. To demonstrate feasibility we have implemented our approach in Sysiphus a suite of tools for collaborating over Software Engineering artifacts represented in a unified model.},
booktitle = {Proceedings of the 2008 International Workshop on Comparison and Versioning of Software Models},
pages = {19–24},
numpages = {6},
keywords = {configuration management, operation-based, scm, unified model, versioning},
location = {Leipzig, Germany},
series = {CVSM '08}
}

@article{10.1155/2021/2658090,
author = {Shi, Xiaorui and Cui, Wei and Zhu, Ping and Yang, Yanhua and Souri, Alireza},
title = {Research on Automobile Assembly Line Optimization Based on Industrial Engineering Technology and Machine Learning Algorithm},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/2658090},
doi = {10.1155/2021/2658090},
abstract = {Aiming at the lack of search depth of traditional genetic algorithm in automobile assembly line balance optimization, an improved genetic algorithm based on bagging integrated clustering is proposed for balance optimization. Through the integrated learning of several K-means algorithm based learners through bagging, a population clustering analysis method based on bagging integrated clustering algorithm is established, and then, a dual objective automobile assembly line balance optimization model is established. The population clustering analysis method is used to improve the intersection link of genetic algorithm to improve the search depth. The effectiveness and search performance of the improved genetic algorithm in solving the double objective assembly line balance problem are verified in an example.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {9}
}

@inproceedings{10.1145/1506250.1506254,
author = {Lim, Soon Chong Johnson and Liu, Ying and Lee, Wing Bun},
title = {Faceted search and retrieval based on semantically annotated product family ontology},
year = {2009},
isbn = {9781605584300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1506250.1506254},
doi = {10.1145/1506250.1506254},
abstract = {With the advent of various services and applications of Semantic Web, semantic annotation had emerged as an important research area. The use of semantically annotated ontology had been evident in numerous information processing and retrieval tasks. One of such tasks is utilizing the semantically annotated ontology in product design which is able to suggest many important applications that are critical to aid various design related tasks. However, ontology development in design engineering remains a time consuming and tedious task that demands tremendous human efforts. In the context of product family design, management of different product information that features efficient indexing, update, navigation, search and retrieval across product families is both desirable and challenging. This paper attempts to address this issue by proposing an information management and retrieval framework based on the semantically annotated product family ontology. Particularly, we propose a document profile (DP) model to suggest semantic tags for annotation purpose. Using a case study of digital camera families, we illustrate how the faceted search and retrieval of product information can be accomplished based on the semantically annotated camera family ontology. Lastly, we briefly discuss some further research and application in design decision support, e.g. commonality and variety, based on the semantically annotated product family ontology.},
booktitle = {Proceedings of the WSDM '09 Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {15–24},
numpages = {10},
keywords = {information management and retrieval, ontology, product family, semantic annotation},
location = {Barcelona, Spain},
series = {ESAIR '09}
}

@article{10.1504/ijkesdp.2020.112630,
author = {Moses, Beulah and Singhal, Shyam},
title = {Effort estimation in software development using story point: a machine learning approach},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {1},
issn = {1755-3210},
url = {https://doi.org/10.1504/ijkesdp.2020.112630},
doi = {10.1504/ijkesdp.2020.112630},
abstract = {Agile methodologies are besieged with problems and potential solutions around predictive insights on a project. These problems range from estimation, quality, to effort and duration requirements. Despite of having innumerable predictive models not a single reliable solution is available to estimate the duration and effort required to complete an agile project on an ongoing basis. This is due subjective nature of 'story point', and progressive elaboration of 'user story'. This paper analyses the relationship between story points and effort, across a sample of software development projects, in an organisation. A novel machine learning predictive model has been developed and is implemented across agile projects that infer relationship between 'effort' and 'story points', directly in contrast with agile literature. This predictive model was tested and worked accurately, reliably and effectively across various agile projects. This research can be extended to agile projects having sprints of less than fifty story points.},
journal = {Int. J. Knowl. Eng. Soft Data Paradigm.},
month = jan,
pages = {25–44},
numpages = {19},
keywords = {forecasting, estimation model, machine learning, regression, agile, scrum, story points, effort estimation}
}

@inproceedings{10.1145/2362536.2362547,
author = {Johansen, Martin Fagereng and Haugen, Øystein and Fleurey, Franck},
title = {An algorithm for generating t-wise covering arrays from large feature models},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362547},
doi = {10.1145/2362536.2362547},
abstract = {A scalable approach for software product line testing is required due to the size and complexity of industrial product lines. In this paper, we present a specialized algorithm (called ICPL) for generating covering arrays from feature models. ICPL makes it possible to apply combinatorial interaction testing to software product lines of the size and complexity found in industry. For example, ICPL allows pair-wise testing to be readily applied to projects of about 7,000 features and 200,000 constraints, the Linux Kernel, one of the largest product lines where the feature model is available. ICPL is compared to three of the leading algorithms for t-wise covering array generation. Based on a corpus of 19 feature models, data was collected for each algorithm and feature model when the algorithm could finish 100 runs within three days. These data are used for comparing the four algorithms. In addition to supporting large feature models, ICPL is quick, produces small covering arrays and, even though it is non-deterministic, produces a covering array of a similar size within approximately the same time each time it is run with the same feature model.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {46–55},
numpages = {10},
keywords = {combinatorial interaction testing, feature models, product lines, testing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/1080173.1080191,
author = {Jeon, Sung-eok and Ji, Chuanyi},
title = {Role of machine learning in configuration management of ad hoc wireless networks},
year = {2005},
isbn = {1595930264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1080173.1080191},
doi = {10.1145/1080173.1080191},
abstract = {In this work, we show that machine learning, e.g., graphical models, plays an important role for the self-configuration of ad hoc wireless network. The role of such a learning approach includes a simple representation of complex dependencies in the network and a distributed algorithm which can adaptively find a nearly optimal configuration.},
booktitle = {Proceedings of the 2005 ACM SIGCOMM Workshop on Mining Network Data},
pages = {223–224},
numpages = {2},
location = {Philadelphia, Pennsylvania, USA},
series = {MineNet '05}
}

@inproceedings{10.1145/2019136.2019146,
author = {Gerlach, Simon},
title = {Improving efficiency when deriving numerous products from software product lines simultaneously},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019146},
doi = {10.1145/2019136.2019146},
abstract = {In-car infotainment systems must allow for product differentiation and the adaption to the needs of different markets. Product line approaches are applied because large numbers of different product variants need to be developed simultaneously. During development, updated versions of each projected product variant need to be derived from the product line assets repeatedly. Current build tools create each of the numerous product variants one after another. Accordingly, the creation process can take much time. This paper presents an approach to abbreviate this creation process based on the fact that multiple product variants created at once can have parts in common. To benefit from this optimization potential the workflow that creates an individual product variant is subdivided into multiple fragments. Whenever a set of such product variants needs to be created, an optimization algorithm then calculates an individual execution order of the fragments for this set. This order minimizes the total execution time by a systematic reuse of workflow fragment's results for the creation of multiple different product variants.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {9},
numpages = {4},
keywords = {application engineering, automotive, product configuration, product derivation, software product lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.5555/1753235.1753266,
author = {Hubaux, Arnaud and Classen, Andreas and Heymans, Patrick},
title = {Formal modelling of feature configuration workflows},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {In software product line engineering, the configuration process can be a long and complex undertaking that involves many participants. When configuration is supported by feature diagrams, two challenges are to modularise the feature diagram into related chunks, and to schedule them as part of the configuration process. Existing work has only focused on the first of these challenges and, for the rest, assumes that feature diagram modules are configured sequentially. This paper addresses the second challenge. It suggests using YAWL, a state-of-the-art workflow language, to represent the configuration workflow while feature diagrams model the available configuration options. The principal contribution of the paper is a new combined formalism: feature configuration workflows. A formal semantics is provided so as to pave the way for unambiguous tool specification and safer reasoning about of the configuration process. The work is motivated and illustrated through a configuration scenario taken from the space industry.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {221–230},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3375627.3375858,
author = {Zucker, Julian and d'Leeuwen, Myraeka},
title = {Arbiter: A Domain-Specific Language for Ethical Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375858},
doi = {10.1145/3375627.3375858},
abstract = {The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {421–425},
numpages = {5},
keywords = {domain-specific languages, ethical machine learning},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/2499777.2500714,
author = {Huang, Changyun and Kamei, Yasutaka and Yamashita, Kazuhiro and Ubayashi, Naoyasu},
title = {Using alloy to support feature-based DSL construction for mining software repositories},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500714},
doi = {10.1145/2499777.2500714},
abstract = {The Mining Software Repositories (MSR) field reveals knowledge for software development by analyzing data stored in repositories such as source control and bug trace systems. In order to reveal the knowledge, MSR researchers need to perform complicated procedures iteratively. To help the complex work of MSR practitioners, we study the construction of domain specific languages (DSLs) for MSR. We have conducted feature-oriented domain analysis (FODA) on MSR and developed a DSL based on the feature model. In this paper, we expand our previous work and propose to construct not a single DSL but a DSL family. A DSL family consists of a series of DSLs with commonality in their domain but suitable to specific applications of MSR. To readily construct these DSLs, we use Alloy to encode the feature model. Our encoding includes not only the DSL features and their relations but also some composition rules that can be used to generate the syntax of DSLs. Based on this, we can automatically derive the language elements to construct DSLs suitable to specific purposes of MSR.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {86–89},
numpages = {4},
keywords = {DSL, FODA, SPL, mining software repositories},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2362536.2362554,
author = {Martini, Antonio and Pareto, Lars and Bosch, Jan},
title = {Enablers and inhibitors for speed with reuse},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362554},
doi = {10.1145/2362536.2362554},
abstract = {An open issue in industry is software reuse in the context of large scale Agile product development. The speed offered by agile practices is needed to hit the market, while reuse is needed for long-term productivity, efficiency, and profit. The paper presents an empirical investigation of factors influencing speed and reuse in three large product developing organizations seeking to implement Agile practices. The paper identifies, through a multiple case study with 3 organizations, 114 business-, process-, organizational-, architecture-, knowledge- and communication factors with positive or negative influences on reuse, speed or both. Contributions are a categorized inventory of influencing factors, a display for organizing factors for the purpose of process improvement work, and a list of key improvement areas to address when implementing reuse in organizations striving to become more Agile. Categories identified include good factors with positive influences on reuse or speed, harmful factors with negative influences, and complex factors involving inverse or ambiguous relationships. Key improvement areas in the studied organizations are intra-organizational communication practices, reuse awareness and practices, architectural integration and variability management. Results are intended to support process improvement work in the direction of Agile product development. Feedback on results from the studied organizations has been that the inventory captures current situations, and is useful for software process improvement work.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {116–125},
numpages = {10},
keywords = {agile software development, embedded systems, enablers, inhibitors, software process improvement (SPI), software reuse, speed},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1007/978-3-319-38921-9_12,
author = {Velez, Gorka and Quartulli, Marco and Martin, Angel and Otaegui, Oihana and Assem, Haytham},
title = {Machine Learning for Autonomic Network Management in a Connected Cars Scenario},
year = {2016},
isbn = {9783319389202},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-38921-9_12},
doi = {10.1007/978-3-319-38921-9_12},
abstract = {Current 4G networks are approaching the limits of what is possible with this generation of radio technology. Future 5G networks will be highly based on software, with the ultimate goal of being self-managed. Machine Learning is a key technology to reach the vision of a 5G self-managing network. This new paradigm will significantly impact on connected vehicles, fostering a new wave of possibilities. This paper presents a preliminary approach towards Autonomic Network Management on a connected cars scenario. The focus is on the machine learning part, which will allow forecasting resource demand requirements, detecting errors, attacks and outlier events, and responding and taking corrective actions.},
booktitle = {Proceedings of the 10th International Workshop on Communication Technologies for Vehicles - Volume 9669},
pages = {111–120},
numpages = {10},
keywords = {5G, Connected cars, Machine learning, Network management}
}

@inproceedings{10.1145/2934466.2934470,
author = {Kühn, Thomas and Cazzola, Walter},
title = {Apples and oranges: comparing top-down and bottom-up language product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934470},
doi = {10.1145/2934466.2934470},
abstract = {Over the past decade language development tools have been significantly improved. This permitted both practitioners and researchers to design a wide variety of domain-specific languages (DSL) and extensions to programming languages. Moreover, multiple researchers have combined different language variants to form families of DSLs as well as programming languages. Unfortunately, current language development tools cannot directly support the development of these families. To overcome this limitation, researchers have recently applied ideas from software product lines (SPL) to create product lines of compilers/interpreters for language families, denoted language product lines (LPL). Similar to SPLs, however, these product lines can be created either using a top-down or a bottom-up approach. Yet, there exist no case study comparing the suitability of both approaches to the development of LPLs, making it unclear how language development tools should evolve. Accordingly, this paper compares both feature modeling approaches by applying them to the development of an LPL for the family of role-based programming languages and discussing their applicability, feasibility and overall suitability for the development of LPLs. Although one might argue that this compares apples and oranges, we believe that this case still provides crucial insights into the requirements, assumptions, and challenges of each approach.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {50–59},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791078,
author = {Vale, Gustavo and Albuquerque, Danyllo and Figueiredo, Eduardo and Garcia, Alessandro},
title = {Defining metric thresholds for software product lines: a comparative study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791078},
doi = {10.1145/2791060.2791078},
abstract = {A software product line (SPL) is a set of software systems that share a common and variable set of features. Software metrics provide basic means to quantify several modularity aspects of SPLs. However, the effectiveness of the SPL measurement process is directly dependent on the definition of reliable thresholds. If thresholds are not properly defined, it is difficult to actually know whether a given metric value indicates a potential problem in the feature implementation. There are several methods to derive thresholds for software metrics. However, there is little understanding about their appropriateness for the SPL context. This paper aims at comparing three methods to derive thresholds based on a benchmark of 33 SPLs. We assess to what extent these methods derive appropriate values for four metrics used in product-line engineering. These thresholds were used for guiding the identification of a typical anomaly found in features' implementation, named God Class. We also discuss the lessons learned on using such methods to derive thresholds for SPLs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {176–185},
numpages = {10},
keywords = {metrics, software product lines, thresholds},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2364412.2364414,
author = {Derakhshanmanesh, Mahdi and Fox, Joachim and Ebert, Jürgen},
title = {Adopting feature-centric reuse of requirements assets: an industrial experience report},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364414},
doi = {10.1145/2364412.2364414},
abstract = {In this paper, we share practical experiences from an ongoing effort towards adopting a feature-centric method that enhances reuse of requirements at TRW Automotive's slip control system department (based in Koblenz, Germany). After introducing identified challenges in detail, key solution factors and a technical reuse concept for managing and deriving product-specific requirements are presented. Then, we demonstrate one way of implementing this solution approach based on industry-standard tools. In addition, identified pitfalls and lessons learned are discussed.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {2–9},
numpages = {8},
keywords = {features, requirements, reuse, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.jss.2007.07.006,
author = {Del Rosso, Christian},
title = {Software performance tuning of software product family architectures: Two case studies in the real-time embedded systems domain},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.07.006},
doi = {10.1016/j.jss.2007.07.006},
abstract = {Software performance is an important non-functional quality attribute and software performance evaluation is an essential activity in the software development process. Especially in embedded real-time systems, software design and evaluation are driven by the needs to optimize the limited resources, to respect time deadlines and, at the same time, to produce the best experience for end-users. Software product family architectures add additional requirements to the evaluation process. In this case, the evaluation includes the analysis of the optimizations and tradeoffs for the whole products in the family. Performance evaluation of software product family architectures requires knowledge and a clear understanding of different domains: software architecture assessments, software performance and software product family architecture. We have used a scenario-driven approach to evaluate performance and dynamic memory management efficiency in one Nokia software product family architecture. In this paper we present two case studies. Furthermore, we discuss the implications and tradeoffs of software performance against evolvability and maintenability in software product family architectures.},
journal = {J. Syst. Softw.},
month = jan,
pages = {1–19},
numpages = {19},
keywords = {Dynamic memory management, Embedded real-time systems, Software architecture assessments, Software performance, Software product family}
}

@inproceedings{10.1145/2647908.2655966,
author = {Itzik, Nili and Reinhartz-Berger, Iris},
title = {Generating feature models from requirements: structural vs. functional perspectives},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655966},
doi = {10.1145/2647908.2655966},
abstract = {Adoption of SPLE techniques is challenging and expensive. Hence, automation in the adoption process is desirable, especially with respect to variability management. Different methods have been suggested for (semi-)automatically generating feature models from requirements or textual descriptions of products. However, while there are different ways to represent the same SPL in feature models, addressing different stakeholders' needs and preferences, existing methods usually follow fixed, predefined ways to generate feature models. As a result, the generated feature models may represent perspectives less relevant to the given tasks.In this paper we suggest an ontological approach that measures the semantic similarity, extracts variability, and automatically generates feature models that represent structural (objects-related) or functional (actions-related) perspectives. The stakeholders are able to control the perspective of the generated feature models, considering their needs and preferences for given tasks.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {44–51},
numpages = {8},
keywords = {feature models, mining, ontology, reverse engineering, semantic similarity},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3483899.3483907,
author = {Uchôa, Anderson and Assunção, Wesley Klewerton Guez and Garcia, Alessandro},
title = {Do Critical Components Smell Bad? An Empirical Study with Component-based Software Product Lines},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483907},
doi = {10.1145/3483899.3483907},
abstract = {Component-based software product line (SPL) consists of a set of software products that share common components. For a proper SPL product composition, each component has to follow three principles: encapsulating a single feature, restricting data access, and be replaceable. However, it is known that developers usually introduce anomalous structures, i.e., code smells, along the implementation of components. These code smells might violate one or more component principles and hinder the SPL product composition. Thus, developers should identify code smells in component-based SPLs, especially those affecting highly interconnected components, which are called critical components. Nevertheless, there is limited evidence of how smelly these critical components tend to be in component-based SPLs. To address this limitation, this paper presents a survey with developers of three SPLs. We inquire these developers about their perceptions of a critical component. Then, we characterize critical components per SPL, and identify nine recurring types of code smells. Finally, we quantitatively assess the smelliness of the critical components. Our results suggest that: (i) critical components are ten times more prone to have code smells than non-critical ones; (ii) the most frequent code smell types affecting critical components violate several component principles together; and (iii) these smell types affect multiple SPL components.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {21–30},
numpages = {10},
keywords = {Component-based software product line, empirical study, smell},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@article{10.1016/j.future.2018.09.053,
author = {Cecchinel, Cyril and Fouquet, François and Mosser, Sébastien and Collet, Philippe},
title = {Leveraging live machine learning and deep sleep to support a self-adaptive efficient configuration of battery powered sensors},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.053},
doi = {10.1016/j.future.2018.09.053},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {225–240},
numpages = {16}
}

@phdthesis{10.5555/1970793,
author = {Lee, Sihyung},
title = {Reducing complexity of large-scale network configuration management},
year = {2010},
isbn = {9781124101866},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {The configuration of large-scale networks is known to be difficult and error-prone. It is a low-level device-specific task and has to deal with subtle dependencies between multiple devices across a network. Network misconfiguration is a key cause of network disruptions and may also lead to security problems in networks. The complexity of network configuration is rapidly increasing as configurations change over time; as a result, there are more human errors that greatly degrade the connectivity of networks and increase management costs. To reduce the complexity of network configuration, we propose four network management modules: Verification, Simplification, Correlation/Visualization, and Classification. The Verification module consists of a complete configuration model and an automatic policy inference system. Using the model and the policy inference system, the Verification module evaluates a variety of network-wide policies, both within a single technology and across multiple technologies (e.g., packet filtering and routing policies). The Simplification module streamlines policies in a configuration so that the configurations are easier to understand and update; in this manner, it demonstrates the potential for improving comprehensibility of network configurations. The Correlation/Visualization module visualizes high-level, intended policies by correlating low-level configurations. This module helps operators to understand distributed low-level configurations more quickly and accurately. Finally, the Classification module identifies critical elements in a network. This identification allows operators to focus their time on higher-priority problems, thus reducing the complexity of network management. We implement the four network management modules and evaluate their effectiveness with configurations from four production networks. The Verification module discovers more than a hundred errors that are confirmed and corrected by the network administrators. Some of these misconfigurations can result in loss of connectivity, access to protected networks, and financial implications by providing free transit services. The Simplification module reduces up to 70% of commands related to routing policies. We also go over a few reduction types and show that such simplification does improve the manageability of the configuration. The Correlation/Visualization module decreases operation and service deployment time from hours to minutes and increased its accuracy from 70% to nearly 100%. The Classification module identifies configurations that impact route advertisements to more than 100 peers. This module also finds routing sessions that result in more than 100 GB of loss within a few seconds if not properly protected. We believe that our systems significantly improve accuracy and reduce time of network configuration. The proposed ideas can benefit many different types of networks, especially in large installations, such as service provider networks, enterprise networks, data-center networks, and power grids.},
note = {AAI3415822}
}

@inproceedings{10.5555/1753235.1753267,
author = {Mendonca, Marcilio and Wąsowski, Andrzej and Czarnecki, Krzysztof},
title = {SAT-based analysis of feature models is easy},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Feature models are a popular variability modeling notation used in product line engineering. Automated analyses of feature models, such as consistency checking and interactive or offline product selection, often rely on translating models to propositional logic and using satisfiability (SAT) solvers.Efficiency of individual satisfiability-based analyses has been reported previously. We generalize and quantify these studies with a series of independent experiments. We show that previously reported efficiency is not incidental. Unlike with the general SAT instances, which fall into easy and hard classes, the instances induced by feature modeling are easy throughout the spectrum of realistic models. In particular, the phenomenon of phase transition is not observed for realistic feature models.Our main practical conclusion is a general encouragement for researchers to continued development of SAT-based methods to further exploit this efficiency in future.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {231–240},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3168365.3168378,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Clémentine},
title = {Towards the Extraction of Variability Information to Assist Variability Modelling of Complex Product Lines},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168378},
doi = {10.1145/3168365.3168378},
abstract = {Software product line engineering gathers a set of methods that rely on systematic reuse and mass customisation to reduce the development time and cost of a set of similar software systems. Boolean feature models are the de facto standard used to represent product line variability in terms of features, a feature being a distinguishable characteristic of one or several softwares. The extractive adoption of a product line from a set of individually developed softwares requires to extract variability information from a collection of software descriptions to model their variability. With the appearance of more and more complex software systems, software product line engineering faces new challenges including variability extraction and modelling. Extensions of boolean feature models, as multi-valued attributes or UML-like cardinalities have since been proposed to support variability modelling in complex product lines. In this paper, we propose research directions to address the issue of extracting more complex variability information, as a part of extended feature models synthesis from software descriptions. We consider the capabilities of Formal Concept Analysis, a mathematical framework for knowledge discovery, along with two of its extensions called Pattern Structures and Relational Concept Analysis, to answer this problematic. These frameworks bring theoretical foundations to complex variability extraction algorithms.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {113–120},
numpages = {8},
keywords = {Reverse Engineering, Software Product Line, Variability Extraction},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1007/978-3-642-41533-3_23,
author = {Nie, Kunming and Yue, Tao and Ali, Shaukat and Zhang, Li and Fan, Zhiqiang},
title = {Constraints: The Core of Supporting Automated Product Configuration of Cyber-Physical Systems},
year = {2013},
isbn = {9783642415326},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41533-3_23},
doi = {10.1007/978-3-642-41533-3_23},
abstract = {In the context of product line engineering of cyber-physical systems, there exists a large number of constraints to support, for example, consistency checking of design decisions made in hardware and software components during configuration. Manual configuration is not feasible in this context considering that managing and manipulating all these constraints in a real industrial context is very complicated and thus warrants an automated solution. Typical automation activities in this context include automated configuration value inference, optimizing configuration steps and consistency checking. However, to this end, relevant constraints have to be well-specified and characterized in the way such that automated configuration can be enabled. In this paper, we classify and characterize constraints that are required to be specified to support most of the key functionalities of any automated product configuration solution, based on our experience of studying three industrial product lines.},
booktitle = {Proceedings of the 16th International Conference on Model-Driven Engineering Languages and Systems - Volume 8107},
pages = {370–387},
numpages = {18},
keywords = {Classification, Configuration, Constraints, Cyber-Physical Systems, Industrial Case Studies, Product Line Engineering}
}

@article{10.1016/j.jss.2014.08.034,
author = {Alsawalqah, Hamad I. and Kang, Sungwon and Lee, Jihyun},
title = {A method to optimize the scope of a software product platform based on end-user features},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.08.034},
doi = {10.1016/j.jss.2014.08.034},
abstract = {A novel method to optimize the scope of a software product platform is proposed.The method is supported with a mathematical formulation and an optimization solver.Depending on the input parameters and the objectives, competing scopes can exist.The method shows how trade-off analysis can be performed among competing scopes.The results of the method were validated as "satisfiable" to "very satisfiable". ContextDue to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development. AimThis paper proposes a novel method to find the optimized scope of a software product platform based on end-user features. MethodThe proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives. ResultsIn a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope.},
journal = {J. Syst. Softw.},
month = dec,
pages = {79–106},
numpages = {28},
keywords = {Commonality decision, Product platform scope, Software product line engineering}
}

@inproceedings{10.1145/2362536.2362556,
author = {Nunes, Camila and Garcia, Alessandro and Lucena, Carlos and Lee, Jaejoon},
title = {History-sensitive heuristics for recovery of features in code of evolving program families},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362556},
doi = {10.1145/2362536.2362556},
abstract = {A program family might degenerate due to unplanned changes in its implementation, thus hindering the maintenance of family members. This degeneration is often induced by feature code that is changed individually in each member without considering other family members. Hence, as a program family evolves over time, it might no longer be possible to distinguish between common and variable features. One of the imminent activities to address this problem is the history-sensitive recovery of program family's features in the code. This recovery process encompasses the analysis of the evolution history of each family member in order to classify the implementation elements according to their variability nature. In this context, this paper proposes history-sensitive heuristics for the recovery of features in code of degenerate program families. Once the analysis of the family history is carried out, the feature elements are structured as Java project packages; they are intended to separate those elements in terms of their variability degree. The proposed heuristics are supported by a prototype tool called RecFeat. We evaluated the accuracy of the heuristics in the context of 33 versions of 2 industry program families. They presented encouraging results regarding recall measures that ranged from 85% to 100%; whereas the precision measures ranged from 71% to 99%.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {136–145},
numpages = {10},
keywords = {feature recovery, heuristics, program families, software evolution},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1007/s10270-017-0641-6,
author = {Li, Yan and Yue, Tao and Ali, Shaukat and Zhang, Li},
title = {Enabling automated requirements reuse and configuration},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0641-6},
doi = {10.1007/s10270-017-0641-6},
abstract = {A system product line (PL) often has a large number of reusable and configurable requirements, which in practice are organized hierarchically based on the architecture of the PL. However, the current literature lacks approaches that can help practitioners to systematically and automatically develop structured and configuration-ready PL requirements repositories. In the context of product line engineering and model-based engineering, automatic requirements structuring can benefit from models. Such a structured PL requirements repository can greatly facilitate the development of product-specific requirements repository, the product configuration at the requirements level, and the smooth transition to downstream product configuration phases (e.g., at the architecture design phase). In this paper, we propose a methodology with tool support, named as Zen-ReqConfig, to tackle the above challenge. Zen-ReqConfig is built on existing model-based technologies, natural language processing, and similarity measure techniques. It automatically devises a hierarchical structure for a PL requirements repository, automatically identifies variabilities in textual requirements, and facilitates the configuration of products at the requirements level, based on two types of variability modeling techniques [i.e., cardinality-based feature modeling (CBFM) and a UML-based variability modeling methodology (named as SimPL)]. We evaluated Zen-ReqConfig with five case studies. Results show that Zen-ReqConfig can achieve a better performance based on the character-based similarity measure Jaro than the term-based similarity measure Jaccard. With Jaro, Zen-ReqConfig can allocate textual requirements with high precision and recall, both over 95% on average and identify variabilities in textual requirements with high precision (over 97% on average) and recall (over 94% on average). Zen-ReqConfig achieved very good time performance: with less than a second for generating a hierarchical structure and less than 2 s on average for allocating a requirement. When comparing SimPL and CBFM, no practically significant difference was observed, and they both performed well when integrated with Zen-ReqConfig.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2177–2211},
numpages = {35},
keywords = {Configuration, Feature model, Product line, Requirements, Reuse}
}

@inproceedings{10.1007/978-3-030-33246-4_45,
author = {González-Rojas, Oscar and Tafurth, Juan},
title = {Multi-cloud Services Configuration Based on Risk Optimization},
year = {2019},
isbn = {978-3-030-33245-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33246-4_45},
doi = {10.1007/978-3-030-33246-4_45},
abstract = {Nowadays risk analysis becomes critical in the Cloud Computing domain due to the increasing number of threats affecting applications running on cloud infrastructures. Multi-cloud environments allow connecting and migrating services from multiple cloud providers to manage risks. This paper addresses the question of how to model and configure multi-cloud services that can adapt to changes in user preferences and threats on individual and composite services. We propose an approach that combines Product Line (PL) and Machine Learning (ML) techniques to model and timely find optimal configurations of large adaptive systems such as multi-cloud services. A three-layer variability modeling on domain, user preferences, and adaptation constraints is proposed to configure multi-cloud solutions. ML regression algorithms are used to quantify the risk of resulting configurations by analyzing how a service was affected by incremental threats over time. An experimental evaluation on a real life electronic identification and trust multi-cloud service shows the applicability of the proposed approach to predict the risk for alternative re-configurations on autonomous and decentralized services that continuously change their availability and provision attributes.},
booktitle = {On the Move to Meaningful Internet Systems: OTM 2019 Conferences: Confederated International Conferences: CoopIS, ODBASE, C&amp;TC 2019, Rhodes, Greece, October 21–25, 2019, Proceedings},
pages = {733–749},
numpages = {17},
keywords = {Multi-cloud services, Variability modeling, Product line configuration, Risk optimization, Machine learning},
location = {Rhodes, Greece}
}

@inproceedings{10.5555/1105634.1105651,
author = {de Oliveira, Edson Alves and Gimenes, Itana M. S. and Huzita, Elisa Hatsue Moriya and Maldonado, José Carlos},
title = {A variability management process for software product lines},
year = {2005},
publisher = {IBM Press},
abstract = {The software product line approach (PL) promotes the generation of specific products from a set of core assets for a given domain. This approach is applicable to domains in which products have well-defined commonalities and variation points. Variability management is concerned with the management of the differences between products throughout the PL lifecycle. This paper presents a UML-based process for variability management that allows identification, representation and delimitation of variabilities as well as identification of mechanisms for variability implementation. The process is illustrated with excerpts of a case study carried out within the context of an existing PL for the Workflow Management System (WfMS) domain. The case study was carried out based on the experimental software engineering concepts. The results have shown that the proposed process has made explicit a higher number of variabilities than does the existing PL process, and it offers better support for variability tracing.},
booktitle = {Proceedings of the 2005 Conference of the Centre for Advanced Studies on Collaborative Research},
pages = {225–241},
numpages = {17},
location = {Toranto, Ontario, Canada},
series = {CASCON '05}
}

@inproceedings{10.5555/1030818.1030848,
author = {Bapat, Vivek and Sturrock, David T.},
title = {The arena product family: enterprise modeling solutions: the arena product family: enterprise modeling solutions},
year = {2003},
isbn = {0780381327},
publisher = {Winter Simulation Conference},
abstract = {This paper introduces the Arena suite of products for modeling, simulation, and optimization highlighting product architecture and technology features that are targeted toward successful deployment of simulation and Arena throughout an enterprise.},
booktitle = {Proceedings of the 35th Conference on Winter Simulation: Driving Innovation},
pages = {210–217},
numpages = {8},
location = {New Orleans, Louisiana},
series = {WSC '03}
}

@article{10.1145/2049656.2049658,
author = {Albrecht, Jeannie and Tuttle, Christopher and Braud, Ryan and Dao, Darren and Topilski, Nikolay and Snoeren, Alex C. and Vahdat, Amin},
title = {Distributed application configuration, management, and visualization with plush},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/2049656.2049658},
doi = {10.1145/2049656.2049658},
abstract = {Support for distributed application management in large-scale networked environments remains in its early stages. Although a number of solutions exist for subtasks of application deployment, monitoring, and maintenance in distributed environments, few tools provide a unified framework for application management. Many of the existing tools address the management needs of a single type of application or service that runs in a specific environment, and these tools are not adaptable enough to be used for other applications or platforms. To this end, we present the design and implementation of Plush, a fully configurable application management infrastructure designed to meet the general requirements of several different classes of distributed applications. Plush allows developers to specifically define the flow of control needed by their computations using application building blocks. Through an extensible resource management interface, Plush supports execution in a variety of environments, including both live deployment platforms and emulated clusters. Plush also uses relaxed synchronization primitives for improving fault tolerance and liveness in failure-prone environments. To gain an understanding of how Plush manages different classes of distributed applications, we take a closer look at specific applications and evaluate how Plush provides support for each.},
journal = {ACM Trans. Internet Technol.},
month = dec,
articleno = {6},
numpages = {41},
keywords = {Application management, PlanetLab}
}

@article{10.1145/2579281.2579294,
author = {Castelluccia, Daniela and Boffoli, Nicola},
title = {Service-oriented product lines: a systematic mapping study},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579294},
doi = {10.1145/2579281.2579294},
abstract = {Software product line engineering and service-oriented architectures both enable organizations to capitalize on reuse of existing software assets and capabilities and improve competitive advantage in terms of development savings, product flexibility, time-to-market. Both approaches accommodate variation of assets, including services, by changing the software being reused or composing services according a new orchestration. Therefore, variability management in Service-oriented Product Lines (SoPL) is one of the main challenges today. In order to highlight the emerging evidence-based results from the research community, we apply the well-defined method of systematic mapping in order to populate a classification scheme for the SoPL field of interest. The analysis of results throws light on the current open issues. Moreover, different facets of the scheme can be combined to answer more specific research questions. The report reveals the need for more empirical research able to provide new metrics measuring efficiency and efficacy of the proposed models, new methods and tools supporting variability management in SoPL, especially during maintenance and verification and validation. The mapping study about SoPL opens further investigations by means of a complete systematic review to select and validate the most efficient solutions to variability management in SoPL.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {1–6},
numpages = {6},
keywords = {empirical study, mapping study, product line development, service-oriented architecture, service-oriented computing, software product line, variability management}
}

@article{10.1504/IJSN.2012.048493,
author = {Alsubhi, K. and Alhazmi, Y. and Bouabdallah, N. and Boutaba, R.},
title = {Security configuration management in intrusion detection and prevention systems},
year = {2012},
issue_date = {August 2012},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {1},
issn = {1747-8405},
url = {https://doi.org/10.1504/IJSN.2012.048493},
doi = {10.1504/IJSN.2012.048493},
abstract = {This paper aims to study the impact of security enforcement levels on the performance and usability of an enterprise information system. We develop a new analytical model to investigate the relationship between the Intrusion Detection and Prevention System performance and the rules mode selection. In particular, we analyze the IDPS rule-checking process along with its consequent action on the resulting security of the network and on the average service time per event. Simulation was conducted to validate our performance analysis study. The results demonstrate that it is desirable to strike a balance between system security and network performance.},
journal = {Int. J. Secur. Netw.},
month = aug,
pages = {30–39},
numpages = {10}
}

@article{10.1145/279437.279454,
author = {Addy, Edward A.},
title = {Report from the first annual workshop on software architectures in product line acquisitions},
year = {1998},
issue_date = {May 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/279437.279454},
doi = {10.1145/279437.279454},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {32–39},
numpages = {8}
}

@inproceedings{10.1145/2791060.2791091,
author = {Chavarriaga, Jaime and Rangel, Carlos and Noguera, Carlos and Casallas, Rubby and Jonckers, Viviane},
title = {Using multiple feature models to specify configuration options for electrical transformers: an experience report},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791091},
doi = {10.1145/2791060.2791091},
abstract = {Electrical Transformers are complex devices that exhibit an enormous variability depending on the intended power transformation, environmental conditions, standards imposed and customer particularities. Incomplete information or inconsistencies in the specifications can lead to re-processes and higher bid times. This paper presents our experience on using multiple feature models to specify custom Electrical Transformer as a Configuration Process. This process facilitates the elicitation of knowledge from multidisciplinary experts using several feature models, one per domain and per standard and defining relationships among them. This separation of domains eases the analysis and validation of the models. To support the process, we have developed some tools to separate, merge and analyze these models. The final feature models are tested configuring and comparing products from existing company catalogs. We consider that the same strategy can be used in other contexts where experts on multiple disciplines participate.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {216–224},
numpages = {9},
keywords = {electrical transformers, product configuration, variability models},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.eswa.2007.07.011,
author = {Chung, Shu-Hsing and Lee, Amy Hsin-I and Kang, He-Yau and Lai, Chih-Wei},
title = {A DEA window analysis on the product family mix selection for a semiconductor fabricator},
year = {2008},
issue_date = {July, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {35},
number = {1–2},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.07.011},
doi = {10.1016/j.eswa.2007.07.011},
abstract = {In a competitive market, semiconductor fabricator must face an environment with multi-product types, multi-priority orders and demand changes in time. Since semiconductor fabrication has a very complicated production process, the above-stated characteristics make the production planning even more difficult. This paper applies data envelopment analysis (DEA) to find a set of product family mix that is efficient for the company to produce. To ensure long-term effectiveness in productivity and in profit gaining, window analysis is adopted to seek the most recommended set of product family mixes for manufacturing by measuring the performance changes over time. With this method, the performance of a mix in one period is compared not only with the performance of other mixes but also with its own performance in other periods. The proposed mechanism can provide guidance to the fabricator regarding strategies for aggregate planning so as to improve manufacturing efficiency.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {379–388},
numpages = {10},
keywords = {DEA, Data envelopment analysis, Product family mix, Window analysis}
}

@inproceedings{10.1145/3362789.3362923,
author = {Vázquez-Ingelmo, Andrea and García-Peñalvo, Francisco J. and Therón, Roberto},
title = {Automatic generation of software interfaces for supporting decision-making processes. An application of domain engineering and machine learning},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362923},
doi = {10.1145/3362789.3362923},
abstract = {Information dashboards are sophisticated tools. Although they enable users to reach useful insights and support their decision-making challenges, a good design process is essential to obtain powerful tools. Users need to be part of these design processes, as they will be the consumers of the information displayed. But users are very diverse and can have different goals, beliefs, preferences, etc., and creating a new dashboard for each potential user is not viable. There exist several tools that allow users to configure their displays without requiring programming skills. However, users might not exactly know what they want to visualize or explore, also becoming the configuration process a tedious task. This research project aims to explore the automatic generation of user interfaces for supporting these decision-making processes. To tackle these challenges, a domain engineering, and machine learning approach is taken. The main goal is to automatize the design process of dashboards by learning from the context, including the end-users and the target data to be displayed.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1007–1011},
numpages = {5},
keywords = {Automatic generation, Domain engineering, High-level requirements, Information Dashboards, Meta-modeling},
location = {León, Spain},
series = {TEEM'19}
}

@inproceedings{10.1145/2364412.2364444,
author = {Filho, João Bosco Ferreira and Barais, Olivier and Baudry, Benoit and Viana, Windson and Andrade, Rossana M. C.},
title = {An approach for semantic enrichment of software product lines},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364444},
doi = {10.1145/2364412.2364444},
abstract = {Software Product Lines (SPLs) have evolved and gained attention as one of the most promising approaches for software reuse. Feature models are the main technique to represent domain variability in SPLs. However, there are other domain aspects, besides variability, which cannot be expressed in a feature model. Also, these diagrams were not designed to facilitate information retrieval, interoperability and inference. In contrast, ontologies seem to be the best solution to meet these requirements. Therefore, this work presents an approach for semantic enrichment of SPLs using ontologies. Our proposal provides methods to add domain information besides variability description, and a top-ontology that specifies generic concepts and relations in an SPL, working as a guide model for information addition. The proposed approach reuses the existing SPL feature model, adding semantic descriptions in a less intrusive way than modifying the feature model notation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {188–195},
numpages = {8},
keywords = {knowledge, ontology, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.infsof.2015.12.004,
author = {Lu, Hong and Yue, Tao and Ali, Shaukat and Zhang, Li},
title = {Model-based incremental conformance checking to enable interactive product configuration},
year = {2016},
issue_date = {April 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {72},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.12.004},
doi = {10.1016/j.infsof.2015.12.004},
abstract = {ContextModel-based product line engineering (PLE) is a paradigm that can enable automated product configuration of large-scale software systems, in which models are used as an abstract specification of commonalities and variabilities of products of a product line. ObjectiveIn the context of PLE, providing immediate feedback on the correctness of a manual configuration step to users has a practical impact on whether a configuration process with tool support can be successfully adopted in practice. MethodIn an existing work, a UML-based variability modeling methodology named as SimPL and an interactive configuration process was proposed. Based on the existing work, we propose an automated, incremental and efficient conformance checking approach to ensure that the manual configuration of a variation point conforms to a set of pre-defined conformance rules specified in the Object Constraint Language (OCL). The proposed approach, named as Zen-CC, has been implemented as an integrated part of our product configuration and derivation tool: Zen-Configurator. ResultsThe performance and scalability of Zen-CC have been evaluated with a real-world case study. Results show that Zen-CC significantly outperformed two baseline engines in terms of performance. Besides, the performance of Zen-CC remains stable during the configuration of all the 10 products of the product line and its efficiency also remains un-impacted even with the growing product complexity, which is not the case for both of the baseline engines. ConclusionThe results suggest that Zen-CC performs practically well and is much more scalable than the two baseline engines and is scalable for configuring products with a larger number of variation points.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {68–89},
numpages = {22},
keywords = {Incremental conformance checking, Interactive product configuration, Model based engineering, Product line engineering, Variation point}
}

@inproceedings{10.1145/1453101.1453118,
author = {Sarma, Anita and Redmiles, David and van der Hoek, André},
title = {Empirical evidence of the benefits of workspace awareness in software configuration management},
year = {2008},
isbn = {9781595939951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1453101.1453118},
doi = {10.1145/1453101.1453118},
abstract = {In this paper, we present results from our empirical evaluations of a workspace awareness tool that we designed and implemented to augment the functionality of software configuration management systems. Particularly, we performed two user experiments directed at understanding the effectiveness of a workspace awareness tool in improving coordination and reducing conflicts. In the first experiment, we evaluated the tool through text-based assignments to avoid interference from the well-documented impact of individual differences among participants, as these differences are known to lessen the observable effect of proposed tools or to lead to them having no observable effect at all. This strategy of evaluating an application in a domain that is known to have less individual differences is novel and in our case particularly helpful in providing baseline quantifiable results. Upon this baseline, we performed a second experiment, with code-based assignments, to validate that the tool's beneficial effects also occur in the case of programming. Together, our results provide quantitative evidence of the benefits of workspace awareness in software configuration management, as we demonstrate that it improves coordination and conflict resolution without inducing significant overhead in monitoring awareness cues.},
booktitle = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {113–123},
numpages = {11},
keywords = {conflicts, evaluation, parallel work, software configuration management, user experiments, workspace awareness},
location = {Atlanta, Georgia},
series = {SIGSOFT '08/FSE-16}
}

@inproceedings{10.5555/2666064.2666065,
author = {Baumgart, Stephan and Fröberg, Joakim and Punnekkat, Sasikumar},
title = {Towards efficient functional safety certification of construction machinery using a component-based approach},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {Electronic systems in the automotive domain implement safety critical functionality in vehicles and the safety certification process according to a functional safety standard is time consuming and a big part of the expenses of a development project. We describe the functional safety certification of electronic automotive systems by presenting a use case from the construction equipment industry. In this context, we highlight some of the major challenges we foresee, while using a product line approach to achieve efficient functional safety certification of vehicle variants. We further elaborate on the impact of functional safety certification when applying the component-based approach on developing safety critical product variants and discuss the implications by cost modeling and analysis.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {1–4},
numpages = {4},
keywords = {component based development, cost modeling, functional safety, safety certification},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@inproceedings{10.1145/1111449.1111536,
author = {Stumptner, Markus and Thomas, Bruce},
title = {Constraint-based livespaces configuration management},
year = {2006},
isbn = {1595932879},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1111449.1111536},
doi = {10.1145/1111449.1111536},
abstract = {In this paper, we describe use of constraint-based methods for configuring ubiquitous workspaces. A declarative representation allows succinct, easily maintainable definitions of the dependencies inherent in setting up a meeting, and permits the use of general constraint reasoners for various standard tasks such as setting up meeting interfaces, switching between setting for different meetings, and saving and restoring settings. Personalisation techniques can be used for intelligently adapting the workspace to individual user needs.},
booktitle = {Proceedings of the 11th International Conference on Intelligent User Interfaces},
pages = {357–359},
numpages = {3},
keywords = {configuration, constraint satisfaction, liveSpaces, ubiquitous workspaces},
location = {Sydney, Australia},
series = {IUI '06}
}

@inproceedings{10.5555/1892801.1892826,
author = {Asklund, Ulf and Bendix, Lars},
title = {A software configuration management course},
year = {2003},
isbn = {3540140360},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software Configuration Management has been a big success in research and creation of tools. There are also many vendors in the market of selling courses to companies. However, in the education sector Software Configuration Management has still not quite made it at least not into the university curriculum. It is either not taught at all or is just a minor part of a general course in software engineering. In this paper, we report on our experience with giving a full course entirely dedicated to Software Configuration Management topics and start a discussion of what ideally should be the goal and contents of such a course.},
booktitle = {Proceedings of the 2001 ICSE Workshops on SCM 2001, and SCM 2003 Conference on Software Configuration Management},
pages = {245–258},
numpages = {14},
location = {Toronto, Canada},
series = {SCM'01/SCM'03}
}

@article{10.1145/2629395,
author = {Jain, Radhika and Cao, Lan and Mohan, Kannan and Ramesh, Balasubramaniam},
title = {Situated Boundary Spanning: An Empirical Investigation of Requirements Engineering Practices in Product Family Development},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629395},
doi = {10.1145/2629395},
abstract = {Requirements Engineering (RE) faces considerable challenges that are often related to boundaries between various stakeholders involved in the software development process. These challenges may be addressed by boundary spanning practices. We examine how boundary spanning can be adapted to address RE challenges in Product Family Development (PFD), a context that involves complex RE. We study two different development approaches, namely, conventional and agile PFD, because these present considerably different challenges. Our findings from a multisite case study present boundary spanning as a solution to improve the quality of RE processes and highlight interesting differences in how boundary spanner roles and boundary objects are adapted in conventional and agile PFD.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {16},
numpages = {29},
keywords = {Boundary spanning, agile development, product family development, requirements engineering}
}

@article{10.1007/s00766-015-0237-z,
author = {Oliinyk, Olesia and Petersen, Kai and Schoelzke, Manfred and Becker, Martin and Schneickert, Soeren},
title = {Structuring automotive product lines and feature models: an exploratory study at Opel},
year = {2017},
issue_date = {March     2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-015-0237-z},
doi = {10.1007/s00766-015-0237-z},
abstract = {Automotive systems are highly complex and customized systems containing a vast amount of variability. 
Feature modeling plays a key role in customization. Empirical evidence through industry application, and in particular methodological guidance of how to structure automotive product lines and their feature models is needed. The overall aim of this work is to provide guidance to practitioners how to structure automotive product lines and their feature models, understanding strengths and weaknesses of alternative structures. The research was conducted in three phases. In the first phase, the context situation was understood using interviews and workshops. In the second phase, possible structures of product lines and feature models were evaluated based on industry feedback collected in workshops. In the third phase, the structures were implemented in the tool GEARS and practitioner feedback was collected. One key challenge was the unavailability of structuring guidelines, which was the focus of this research. The structures considered most suitable for the automotive product line were multiple product lines with modular decomposition. The structures most suitable for the feature model were functional decomposition, using context variability, models corresponding to assets, and feature categories. Other structures have been discarded, and the rationales have been presented. It was possible to support the most suitable structures with the commercial tool GEARS. The implementation in GEARS and the feedback from the practitioners provide early indications for the potential usefulness of the structures and the tool implementation.},
journal = {Requir. Eng.},
month = mar,
pages = {105–135},
numpages = {31},
keywords = {Automotive, Case study, Empirical, Feature modeling, Product line engineering, Variability modeling}
}

@inproceedings{10.5555/645882.672250,
author = {Ferber, Stefan and Haag, Jürgen and Savolainen, Juha},
title = {Feature Interaction and Dependencies: Modeling Features for Reengineering a Legacy Product Line},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Reengineering a legacy product line has been addressed very little by current product line research activities. This paper introduces a method to investigate feature dependencies and interactions, which restricts the variants that can be derived from the legacy product line assets. Reorganizing the product line assets with respect to new requirements requires more knowledge than what is easily provided by the classical feature-modeling approaches. Hence, adding all the feature dependencies and interactions into the feature tree results in unreadable and unmanageable feature models that fail to achieve their original goals.We therefore propose two complementary views to represent the feature model. One view shows the hierarchical refinement of features similar to common feature-modeling approaches in a feature tree. The second view describes what kind of dependencies and interactions there are between various features.We show two examples of feature dependencies and interactions in the context of an engine-control software product line, and we demonstrate how our approach helps to define correct product configurations from product line variants.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {235–256},
numpages = {22},
series = {SPLC 2}
}

@inproceedings{10.5555/1364385.1364391,
author = {Enck, William and McDaniel, Patrick and Sen, Subhabrata and Sebos, Panagiotis and Spoerel, Sylke and Greenberg, Albert and Rao, Sanjay and Aiello, William},
title = {Configuration management at massive scale: system design and experience},
year = {2007},
isbn = {9998888776},
publisher = {USENIX Association},
address = {USA},
abstract = {The development and maintenance of network device configurations is one of the central challenges faced by large network providers. Current network management systems fail to meet this challenge primarily because of their inability to adapt to rapidly evolving customer and provider-network needs, and because of mismatches between the conceptual models of the tools and the services they must support. In this paper, we present the PRESTO configuration management system that attempts to address these failings in a comprehensive and flexible way. Developed for and deployed over the last 4 years within a large ISP network, PRESTO constructs device-native configurations based on the composition of configlets representing different services or service options. Configlets are compiled by extracting and manipulating data from external systems as directed by the PRESTO configuration scripting and template language. We outline the configuration management needs of large-scale network providers, introduce the PRESTO system and configuration language, and demonstrate the use, workflows, and ultimately the platform's flexibility via an example of VPN service. We conclude by considering future work and reflect on the operators' experiences with PRESTO.},
booktitle = {2007 USENIX Annual Technical Conference on Proceedings of the USENIX Annual Technical Conference},
articleno = {6},
numpages = {14},
location = {Santa Clara, CA},
series = {ATC'07}
}

@inproceedings{10.1145/2491627.2491628,
author = {Passos, Leonardo and Guo, Jianmei and Teixeira, Leopoldo and Czarnecki, Krzysztof and Wąsowski, Andrzej and Borba, Paulo},
title = {Coevolution of variability models and related artifacts: a case study from the Linux kernel},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491628},
doi = {10.1145/2491627.2491628},
abstract = {Variability-aware systems are subject to the coevolution of variability models and related artifacts. Surprisingly, little knowledge exists to understand such coevolution in practice. This shortage is directly reflected in existing approaches and tools for variability management, as they fail to provide effective support for such a coevolution. To understand how variability models and related artifacts coevolve in a large and complex real-world variability-aware system, we inspect over 500 Linux kernel commits spanning almost four years of development. We collect a catalog of evolution patterns, capturing the coevolution of the Linux kernel variability model, Makefiles, and C source code. Further, we extract general findings to guide further research and tool development.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {91–100},
numpages = {10},
keywords = {Linux, catalog, evolution, patterns, variability},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1007/11590019_12,
author = {Fægri, Tor Erlend and Decker, Björn and Dingsøyr, Torgeir and Jaccheri, Letizia and Lago, Patricia and Muthig, Dirk and van Vliet, Hans},
title = {Exploring communities of practice for product family engineering},
year = {2005},
isbn = {3540304657},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11590019_12},
doi = {10.1007/11590019_12},
abstract = {Product Family Engineering (PFE) is an approach to software engineering that seeks to reduce the global effort in producing multiple software products by actively promoting and governing the reuse of assets between the family members. However, PFE is highly demanding, putting stringent demands on careful planning and management in the organization. To leverage the full opportunities offered by PFE, its introduction and use requires effective coordination of people and organizational units. This paper presents a solution to support this coordination by exploring concepts from the area of communities of practices for PFE. The expected benefit is that communities offer a more adaptive approach supporting the transition towards PFE compared to formal organizational restructuring. In this paper, we will describe our considerations about the building principles for organizations using a shared platform for the family members and a study proposal that will examine the effects of knowledge brokering among Communities of Practice as a means to assist PFE.},
booktitle = {Proceedings of the Third Biennial Conference on Professional Knowledge Management},
pages = {96–105},
numpages = {10},
location = {Kaiserslautern, Germany},
series = {WM'05}
}

@article{10.1145/3464939,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat},
title = {Recommending Faulty Configurations for Interacting Systems Under Test Using Multi-objective Search},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3464939},
doi = {10.1145/3464939},
abstract = {Modern systems, such as cyber-physical systems, often consist of multiple products within/across product lines communicating with each other through information networks. Consequently, their runtime behaviors are influenced by product configurations and networks. Such systems play a vital role in our daily life; thus, ensuring their correctness by thorough testing becomes essential. However, testing these systems is particularly challenging due to a large number of possible configurations and limited available resources. Therefore, it is important and practically useful to test these systems with specific configurations under which products will most likely fail to communicate with each other. Motivated by this, we present a search-based configuration recommendation (SBCR) approach to recommend faulty configurations for the system under test (SUT) based on cross-product line (CPL) rules. CPL rules are soft constraints, constraining product configurations while indicating the most probable system states with a certain degree of confidence. In SBCR, we defined four search objectives based on CPL rules and combined them with six commonly applied search algorithms. To evaluate SBCR (i.e., SBCRNSGA-II, SBCRIBEA, SBCRMoCell, SBCRSPEA2, SBCRPAES, and SBCRSMPSO), we performed two case studies (Cisco and Jitsi) and conducted difference analyses. Results show that for both of the case studies, SBCR significantly outperformed random search-based configuration recommendation (RBCR) for 86% of the total comparisons based on six quality indicators, and 100% of the total comparisons based on the percentage of faulty configurations (PFC). Among the six variants of SBCR, SBCRSPEA2 outperformed the others in 85% of the total comparisons based on six quality indicators and 100% of the total comparisons based on PFC.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {53},
numpages = {36},
keywords = {Product line, configuration recommendation, interacting products, mined rules, multi-objective search, testing}
}

@inproceedings{10.1145/1921168.1921176,
author = {Chen, Xu and Mao, Yun and Mao, Z. Morley and Van der Merwe, Jacobus},
title = {Declarative configuration management for complex and dynamic networks},
year = {2010},
isbn = {9781450304481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1921168.1921176},
doi = {10.1145/1921168.1921176},
abstract = {Network management and operations are complicated, tedious, and error-prone, requiring signifcant human involvement and domain knowledge. As the complexity involved inevitably grows due to larger scale networks and more complex protocol features, human operators are increasingly short-handed, despite the best effort from existing support systems to make it otherwise. This paper presents coolaid, a system under which the domain knowledge of device vendors and service providers is formally captured by a declarative language. Through effcient and powerful rule-based reasoning on top of a database-like abstraction over a network of devices, coolaid enables new management primitives to perform network-wide reasoning, prevent misconfguration, and automate network confguration, while requiring minimum operator effort. We describe the design and prototype implementation of coolaid, and demonstrate its effectiveness and scalability through various realistic network management tasks.},
booktitle = {Proceedings of the 6th International COnference},
articleno = {6},
numpages = {12},
location = {Philadelphia, Pennsylvania},
series = {Co-NEXT '10}
}

@article{10.1145/1143489.1143493,
author = {Burgess, Mark},
title = {A control theory perspective on configuration management and Cfengine},
year = {2006},
issue_date = {April 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/1143489.1143493},
doi = {10.1145/1143489.1143493},
abstract = {Cfengine is an autonomous agent for the configuration of Unix-like operating systems. It works by implementing a hybrid feedback loop, with both disrcete and continuous elements.},
journal = {SIGBED Rev.},
month = apr,
pages = {12–16},
numpages = {5},
keywords = {configuration management, control theory}
}

@inproceedings{10.5555/1251150.1251155,
author = {Desai, Narayan and Bradshaw, Rick and Matott, Scott and Bittner, Sandra and Coghlan, Susan and Evard, Rémy and Lueninghoener, Cory and Leggett, Ti and Navarro, John-Paul and Rackow, Gene and Stacey, Craig and Stacey, Tisha},
title = {A case study in configuration management tool deployment},
year = {2005},
publisher = {USENIX Association},
address = {USA},
abstract = {While configuration management systems are generally regarded as useful, their deployment process is not well understood or documented. In this paper, we present a case study in configuration management tool deployment. We describe the motivating factors and both the technical considerations and the social issues involved in this process. Our discussion includes an analysis of the overall effect on the system management model and the tasks performed by administrators.},
booktitle = {Proceedings of the 19th Conference on Large Installation System Administration Conference - Volume 19},
pages = {5},
numpages = {1},
location = {San Diego, CA},
series = {LISA '05}
}

@inproceedings{10.1145/2491627.2491650,
author = {Xu, Zhihong and Cohen, Myra B. and Motycka, Wayne and Rothermel, Gregg},
title = {Continuous test suite augmentation in software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491650},
doi = {10.1145/2491627.2491650},
abstract = {Software Product Line (SPL) engineering offers several advantages in the development of families of software products. There is still a need, however, to generate test cases for individual products in product lines more efficiently. In this paper we propose an approach, CONTESA, for generating test cases for SPLs using test suite augmentation. Instead of generating test cases for products independently, our approach generates new test cases for products in an order that allows it to build on test cases created for products tested earlier. In this work, we use a genetic algorithm to generate test cases, targeting branches not yet covered in each product, although other algorithms and coverage criteria could be utilized. We have evaluated CONTESA on two non-trivial SPLs, and have shown that CONTESA is more efficient and effective than an approach that generates test cases for products independently. A further evaluation shows that CONTESA is more effective at achieving coverage, and reveals as many faults as an existing feature-based testing approach.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {52–61},
numpages = {10},
keywords = {software product lines, software testing, test generation},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@article{10.1017/S0890060405050043,
author = {Yeh, Jinn-Yi and Wu, Tai-Hsi},
title = {Solutions for product configuration management: An empirical study},
year = {2005},
issue_date = {January 2005},
publisher = {Cambridge University Press},
address = {USA},
volume = {19},
number = {1},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060405050043},
doi = {10.1017/S0890060405050043},
abstract = {Customers can directly express their preferences on many options when ordering products today. Mass customization manufacturing thus has emerged as a new trend for its aiming to satisfy the needs of individual customers. This process of offering a wide product variety often induces an exponential growth in the volume of information and redundancy for data storage. Thus, a technique for managing product configuration is necessary, on the one hand, to provide customers faster configured and lower priced products, and on the other hand, to translate customers' needs into the product information needed for tendering and manufacturing. This paper presents a decision-making scheme through constructing a product family model (PFM) first, in which the relationship between product, modules, and components are defined. The PFM is then transformed into a product configuration network. A product configuration problem assuming that customers would like to have a minimum-cost and customized product can be easily solved by finding the shortest path in the corresponding product configuration network. Genetic algorithms (GAs), mathematical programming, and tree-searching methods such as uniform-cost search and iterative deepening A* are applied to obtain solutions to this problem. An empirical case is studied in this work as an example. Computational results show that the solution quality of GAs retains 93.89% for a complicated configuration problem. However, the running time of GAs outperforms the running time of other methods with a minimum speed factor of 25. This feature is very useful for a real-time system.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = jan,
pages = {39–47},
numpages = {9},
keywords = {Configuration, Genetic Algorithm, Mathematical Programming, Product Family Model, Tree Searching}
}

@article{10.1145/2580950,
author = {Thüm, Thomas and Apel, Sven and Kästner, Christian and Schaefer, Ina and Saake, Gunter},
title = {A Classification and Survey of Analysis Strategies for Software Product Lines},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2580950},
doi = {10.1145/2580950},
abstract = {Software-product-line engineering has gained considerable momentum in recent years, both in industry and in academia. A software product line is a family of software products that share a common set of features. Software product lines challenge traditional analysis techniques, such as type checking, model checking, and theorem proving, in their quest of ensuring correctness and reliability of software. Simply creating and analyzing all products of a product line is usually not feasible, due to the potentially exponential number of valid feature combinations. Recently, researchers began to develop analysis techniques that take the distinguishing properties of software product lines into account, for example, by checking feature-related code in isolation or by exploiting variability information during analysis. The emerging field of product-line analyses is both broad and diverse, so it is difficult for researchers and practitioners to understand their similarities and differences. We propose a classification of product-line analyses to enable systematic research and application. Based on our insights with classifying and comparing a corpus of 123 research articles, we develop a research agenda to guide future research on product-line analyses.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {6},
numpages = {45},
keywords = {Product-line analysis, model checking, program family, software analysis, software product line, static analysis, theorem proving, type checking}
}

@inproceedings{10.1145/3097983.3098186,
author = {Sharma, Ashlesh and Srinivasan, Vidyuth and Kanchan, Vishal and Subramanian, Lakshminarayanan},
title = {The Fake vs Real Goods Problem: Microscopy and Machine Learning to the Rescue},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098186},
doi = {10.1145/3097983.3098186},
abstract = {Counterfeiting of physical goods is a global problem amounting to nearly 7% of world trade. While there have been a variety of overt technologies like holograms and specialized barcodes and covert technologies like taggants and PUFs, these solutions have had a limited impact on the counterfeit market due to a variety of factors - clonability, cost or adoption barriers. In this paper, we introduce a new mechanism that uses machine learning algorithms on microscopic images of physical objects to distinguish between genuine and counterfeit versions of the same product. The underlying principle of our system stems from the idea that microscopic characteristics in a genuine product or a class of products (corresponding to the same larger product line), exhibit inherent similarities that can be used to distinguish these products from their corresponding counterfeit versions. A key building block for our system is a wide-angle microscopy device compatible with a mobile device that enables a user to easily capture the microscopic image of a large area of a physical object. Based on the captured microscopic images, we show that using machine learning algorithms (ConvNets and bag of words), one can generate a highly accurate classification engine for separating the genuine versions of a product from the counterfeit ones; this property also holds for "super-fake" counterfeits observed in the marketplace that are not easily discernible from the human eye. We describe the design of an end-to-end physical authentication system leveraging mobile devices, portable hardware and a cloud-based object verification ecosystem. We evaluate our system using a large dataset of 3 million images across various objects and materials such as fabrics, leather, pills, electronics, toys and shoes. The classification accuracy is more than 98% and we show how our system works with a cellphone to verify the authenticity of everyday objects.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {2011–2019},
numpages = {9},
keywords = {computer vision, conventional neural networks, microscopy, physical authentication},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@article{10.1016/j.eswa.2010.07.119,
author = {Kankar, P. K. and Sharma, Satish C. and Harsha, S. P.},
title = {Fault diagnosis of ball bearings using machine learning methods},
year = {2011},
issue_date = {March, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.07.119},
doi = {10.1016/j.eswa.2010.07.119},
abstract = {Ball bearings faults are one of the main causes of breakdown of rotating machines. Thus, detection and diagnosis of mechanical faults in ball bearings is very crucial for the reliable operation. This study is focused on fault diagnosis of ball bearings using artificial neural network (ANN) and support vector machine (SVM). A test rig of high speed rotor supported on rolling bearings is used. The vibration response are obtained and analyzed for the various defects of ball bearings. The specific defects are considered as crack in outer race, inner race with rough surface and corrosion pitting in balls. Statistical methods are used to extract features and to reduce the dimensionality of original vibration features. A comparative experimental study of the effectiveness of ANN and SVM is carried out. The results show that the machine learning algorithms mentioned above can be used for automated diagnosis of bearing faults. It is also observed that the severe (chaotic) vibrations occur under bearings with rough inner race surface and ball with corrosion pitting.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1876–1886},
numpages = {11},
keywords = {Artificial neural network, Fault diagnosis, Support vector machine}
}

@inproceedings{10.1145/3136040.3136054,
author = {Linsbauer, Lukas and Berger, Thorsten and Grünbacher, Paul},
title = {A classification of variation control systems},
year = {2017},
isbn = {9781450355247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136040.3136054},
doi = {10.1145/3136040.3136054},
abstract = {Version control systems are an integral part of today's software and systems development processes. They facilitate the management of revisions (sequential versions) and variants (concurrent versions) of a system under development and enable collaboration between developers. Revisions are commonly maintained either per file or for the whole system. Variants are supported via branching or forking mechanisms that conceptually clone the whole system under development. It is known that such cloning practices come with disadvantages. In fact, while short-lived branches for isolated development of new functionality (a.k.a. feature branches) are well supported, dealing with long-term and fine-grained system variants currently requires employing additional mechanisms, such as preprocessors, build systems or custom configuration tools. Interestingly, the literature describes a number of variation control systems, which provide a richer set of capabilities for handling fine-grained system variants compared to the version control systems widely used today. In this paper we present a classification and comparison of selected variation control systems to get an understanding of their capabilities and the advantages they can offer. We discuss problems of variation control systems, which may explain their comparably low popularity. We also propose research activities we regard as important to change this situation.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {49–62},
numpages = {14},
keywords = {Variability management, configuration management, software product lines, software repositories},
location = {Vancouver, BC, Canada},
series = {GPCE 2017}
}

@article{10.1007/s10664-014-9353-5,
author = {Asadi, Mohsen and Soltani, Samaneh and Gašević, Dragan and Hatala, Marek},
title = {The effects of visualization and interaction techniques on feature model configuration},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9353-5},
doi = {10.1007/s10664-014-9353-5},
abstract = {A Software Product Line is a set of software systems of a domain, which share some common features but also have significant variability. A feature model is a variability modeling artifact which represents differences among software products with respect to variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by selecting features in the feature model (referred to as the configuration process) and by instantiating the reference model. However, feature model configuration can be a cumbersome task because: 1) feature models may consist of a large number of features, which are hard to comprehend and maintain; and 2) many factors including technical limitations, implementation costs, stakeholders' requirements and expectations must be considered in the configuration process. Recognizing these issues, a significant amount of research efforts has been dedicated to different aspects of feature model configuration such as automating the configuration process. Several approaches have been proposed to alleviate the feature model configuration challenges through applying visualization and interaction techniques. However, there have been limited empirical insights available into the impact of visualization and interaction techniques on the feature model configuration process. In this paper, we present a set of visualization and interaction interventions for representing and configuring feature models, which are then empirically validated to measure the impact of the proposed interventions. An empirical study was conducted by following the principles of control experiments in software engineering and by applying the well-known software quality standard ISO 9126 to operationalize the variables investigated in the experiment. The results of the empirical study revealed that the employed visualization and interaction interventions significantly improved completion time of comprehension and changing of the feature model configuration. Additionally, according to results, the proposed interventions are easy-to-use and easy-to-learn for the participants.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1706–1743},
numpages = {38},
keywords = {Controlled experiment, Software product line engineering, Tools}
}

@article{10.1016/j.infsof.2009.11.001,
author = {Rabiser, Rick and Grünbacher, Paul and Dhungana, Deepak},
title = {Requirements for product derivation support: Results from a systematic literature review and an expert survey},
year = {2010},
issue_date = {March, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.001},
doi = {10.1016/j.infsof.2009.11.001},
abstract = {Context: An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support. Objective: Our aim is to identify and validate requirements for tool-supported product derivation. Method: We identify the requirements through a systematic literature review and validate them with an expert survey. Results: We discuss the resulting requirements and provide implementation examples from existing product derivation approaches. Conclusions: We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {324–346},
numpages = {23},
keywords = {Product derivation, Product line engineering, Software product line, Systematic literature review}
}

@article{10.1007/s10009-013-0298-6,
author = {Ferrari, Alessio and Spagnolo, Giorgio O. and Martelli, Giacomo and Menabeni, Simone},
title = {From commercial documents to system requirements: an approach for the engineering of novel CBTC solutions},
year = {2014},
issue_date = {November  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {6},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-013-0298-6},
doi = {10.1007/s10009-013-0298-6},
abstract = {Communications-based train control (CBTC) systems are the new frontier of automated train control and operation. Currently developed CBTC platforms are actually very complex systems including several functionalities, and every installed system, developed by a different company, varies in extent, scope, number, and even names of the implemented functionalities. International standards have emerged, but they remain at a quite abstract level, mostly setting terminology. This paper presents the results of an experience in defining a global model of CBTC, by mixing semi-formal modelling and product line engineering. The effort has been based on an in-depth market analysis, not limiting to particular aspects but considering as far as possible the whole picture. The paper also describes a methodology to derive novel CBTC products from the global model, and to define system requirements for the individual CBTC components. To this end, the proposed methodology employs scenario-based requirements elicitation aided with rapid prototyping. To enhance the quality of the requirements, these are written in a constrained natural language (CNL), and evaluated with natural language processing (NLP) techniques. The final goal is to go toward a formal representation of the requirements for CBTC systems. The overall approach is discussed, and the current experience with the implementation of the method is presented. In particular, we show how the presented methodology has been used in practice to derive a novel CBTC architecture.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = nov,
pages = {647–667},
numpages = {21},
keywords = {CBTC, CENELEC, Constrained natural language, Experience report, Formal methods, Product line engineering}
}

@inproceedings{10.1145/3218585.3218670,
author = {Martins, Luana Almeida and Parreira, Paulo Afonso and Freire, André Pimenta and Costa, Heitor},
title = {Exploratory Study on the Use of Software Product Lines in the Development of Quality Assistive Technology Software},
year = {2018},
isbn = {9781450364676},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3218585.3218670},
doi = {10.1145/3218585.3218670},
abstract = {The use of Software Product Line for the development of Assistive Technologies has not been widely explored yet. However, some studies point to the viability of using this approach to develop Assistive Technology software. Through this approach, important limiting factors to use Assistive Technologies can be overcome. These factors are related to the acquisition costs and difficulty to find products corresponding to specific and varying user needs. Considering that Software Product Line approach provides mass customization of software products, the specific needs of each user can be more easily satisfied by software developers. Furthermore, the reuse of code artifacts to development provides a fall in the acquisition cost of these software products. We present in this paper a literature review that aims to investigate how this approach has been applied to the development of Assistive Technology software. Also, we present some quality factors that should be considered to develop Assistive Technologies using Software Product Lines. Thus, the main findings of the review are grouped in order to find the main gaps to be explored in future work.},
booktitle = {Proceedings of the 8th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {262–269},
numpages = {8},
keywords = {Assistive Technology, Software Product Line, Software Quality},
location = {Thessaloniki, Greece},
series = {DSAI '18}
}

@article{10.1504/IJCAT.2008.018155,
author = {Yadav, S. R. and Dashora, Yogesh and Shankar, Ravi and Chan, Felix. T. S. and Tiwari, M. K.},
title = {An interactive particle swarm optimisation for selecting a product family and designing its supply chain},
year = {2008},
issue_date = {May 2008},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {31},
number = {3/4},
issn = {0952-8091},
url = {https://doi.org/10.1504/IJCAT.2008.018155},
doi = {10.1504/IJCAT.2008.018155},
abstract = {A platform-based product development with mixed market-modular strategy can maintain product differentiation and help trade-off the cost and price premium drawing capability. This paper formulates a multi-objective problem to select a product family and design its supply chain and uses an Interactive Particle Swarm Optimisation (IPSO) approach. A case study for a wiring harness supplier of an Automated Guided Vehicle (AGV) manufacturer is considered and IPSO is implemented to solve it. The results establish that the platform-based product development serves the purpose of maintaining market diversity with near optimal cost and profits; more explorative insights are concern of future research.},
journal = {Int. J. Comput. Appl. Technol.},
month = may,
pages = {168–186},
numpages = {19},
keywords = {AGVs, SCM, automated guided vehicles, interactive PSO, market strategy, modular products, particle swarm optimisation, platform-based product development, process flexibility, product differentiation, product families, product family selection, supply chain design, supply chain management, wiring harness suppliers}
}

@article{10.1007/s10664-014-9345-5,
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud and Liaaen, Marius},
title = {A systematic test case selection methodology for product lines: results and insights from an industrial case study},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9345-5},
doi = {10.1007/s10664-014-9345-5},
abstract = {In the context of product lines, test case selection aims at obtaining a set of relevant test cases for a product from the entire set of test cases available for a product line. While working on a research-based innovation project on automated testing of product lines of Video Conferencing Systems (VCSs) developed by Cisco, we felt the need to devise a cost-effective way of selecting relevant test cases for a product. To fulfill such need, we propose a systematic and automated test selection methodology using: 1) Feature Model for Testing (FM_T) to capture commonalities and variabilities of a product line; 2) Component Family Model for Testing (CFM_T) to model the structure of test case repository; 3) A tool to automatically build restrictions from CFM_T to FM_T and traces from CFM_T to the actual test cases. Using our methodology, a test engineer is only required to select relevant features through FM_T at a higher level of abstraction for a product and the corresponding test cases will be obtained automatically. We evaluate our methodology by applying it to a VCS product line called Saturn with seven commercial products and the results show that our methodology can significantly reduce cost measured as test selection time and at the same time achieves higher effectiveness (feature coverage, feature pairwise coverage and fault detection) as compared with the current manual process. Moreover, we conduct a questionnaire-based study to solicit the views of test engineers who are involved in developing FM_T and CFM_T. The results show that test engineers are positive about adapting our methodology in their current practice. Finally, we present a set of lessons learnt while applying product line engineering at Cisco for test case selection.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1586–1622},
numpages = {37},
keywords = {Component family model, Feature model, Product line, Test case selection}
}

@article{10.1016/j.scico.2017.10.013,
author = {Castro, Thiago and Lanna, Andr and Alves, Vander and Teixeira, Leopoldo and Apel, Sven and Schobbens, Pierre-Yves},
title = {All roads lead to Rome},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2017.10.013},
doi = {10.1016/j.scico.2017.10.013},
abstract = {The formalization of seven strategies for product-line reliability analysis.The first feature-family-product-based strategy for product-line model checking.A general principle for lifting analyses to product lines using ADDs.Proofs that the formalized strategies commute.All strategies proven sound with respect to single-product reliability analysis. Software product line engineering is a means to systematically manage variability and commonality in software systems, enabling the automated synthesis of related programs (products) from a set of reusable assets. However, the number of products in a software product line may grow exponentially with the number of features, so it is practically infeasible to quality-check each of these products in isolation. There is a number of variability-aware approaches to product-line analysis that adapt single-product analysis techniques to cope with variability in an efficient way. Such approaches can be classified along three analysis dimensions (product-based, family-based, and feature-based), but, particularly in the context of reliability analysis, there is no theory comprising both (a) a formal specification of the three dimensions and resulting analysis strategies and (b) proof that such analyses are equivalent to one another. The lack of such a theory hinders formal reasoning on the relationship between the analysis dimensions and derived analysis techniques. We formalize seven approaches to reliability analysis of product lines, including the first instance of a feature-family-product-based analysis in the literature. We prove the formalized analysis strategies to be sound with respect to the probabilistic approach to reliability analysis of a single product. Furthermore, we present a commuting diagram of intermediate analysis steps, which relates different strategies and enables the reuse of soundness proofs between them.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {116–160},
numpages = {45},
keywords = {Model checking, Product-line analysis, Reliability analysis, Software product lines, Verification}
}

@inproceedings{10.5555/646972.713540,
author = {Kandt, Ronald Kirk},
title = {Software Configuration Management Principles and Best Practices},
year = {2002},
isbn = {3540002340},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper identifies fundamental principles and practices essential to the successful performance of a configuration management system. Practices are grouped into four categories that govern the management process, ensure product quality, protect software artifacts, and guide tool use. In addition, the practices are prioritized according to their effect on software products and processes and the coverage of the identified principles. When these practices should be applied in the software development lifecycle is discussed, as is the potential for automating and validating practices.},
booktitle = {Proceedings of the 4th International Conference on Product Focused Software Process Improvement},
pages = {300–313},
numpages = {14},
series = {PROFES '02}
}

@inproceedings{10.1145/3202710.3203146,
author = {Klünder, Jil and Hohl, Philipp and Schneider, Kurt},
title = {Becoming Agile while preserving software product lines: an Agile transformation model for large companies},
year = {2018},
isbn = {9781450364591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3202710.3203146},
doi = {10.1145/3202710.3203146},
abstract = {Software process improvement has always been an essential part of software projects. Current market trends and the rapid pace of changing requirements demand fast development and adaptability. Agile software development is a popular possibility to react on these trends. Implementing agile practices promises for example a shorter time-to-market, satisfied customers and increased software quality. Consequently many companies strive for an integration of agile methods or for an agile transformation.High-technological environments such as the automotive domain also want to benefit from the advantages promised by agile software development. Even more than smaller companies, these large ones have to deal with software systems getting more and more complex. One established solution facing this problem is an effective and managed way to reuse software at least partially. Software product lines provide an efficient way to manage software reuse and to handle the high complexity. Consequently, they are widely distributed in large and high-technological environments.In most companies in the automotive domain, software product lines are already present and agile development methods should be introduced. Hence, there is a need for a transformation model preserving the benefits of software product lines. We conducted a literature review to achieve an overview and a better understanding of agile transformation models in large companies. In total, we analyzed 367 papers. None of them addresses the agile transformation in large companies with existing software product lines. In consideration of this research gap, we propose a transformation model preserving the benefits of already existing models and extending aspects which are important for existing software product lines.},
booktitle = {Proceedings of the 2018 International Conference on Software and System Process},
pages = {1–10},
numpages = {10},
keywords = {Agile transformation, software product line engineering, transformation process},
location = {Gothenburg, Sweden},
series = {ICSSP '18}
}

@inproceedings{10.1145/2647908.2655961,
author = {Seidl, Christoph and Domachowska, Irena},
title = {Teaching variability engineering to cognitive psychologists},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655961},
doi = {10.1145/2647908.2655961},
abstract = {In research of cognitive psychology, experiments to measure cognitive processes may be run in many similar yet slightly different configurations. Variability engineering offers techniques to handle variable configurations both conceptually and technically. However, these techniques are largely unknown to cognitive psychologists so that experiment configurations are specified informally or too coarse grain. This is problematic, because it becomes difficult to get an overview of paradigm configurations used in the so far conducted experiments. Variability engineering techniques provide, i.a., concise notations for capturing variability in software and can also be used to express the configurable nature of a wide range of experiments in cognitive psychology. Furthermore, it enables cognitive psychologists to structure configuration knowledge, to identify suitably similar experiment setups and to more efficiently identify individual configuration options as relevant reasons for a particular effect in the outcome of an experiment. In this paper, we present experiences with teaching variability engineering to cognitive psychologists along with a suitable curriculum.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {16–23},
numpages = {8},
keywords = {cognitive psychology, feature model, teaching, variability engineering},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/1868688.1868695,
author = {Voelter, Markus},
title = {Implementing feature variability for models and code with projectional language workbenches},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868695},
doi = {10.1145/1868688.1868695},
abstract = {Product line engineering deals with managing and implementing the variability among a set of related products. We distinguish between two kinds of variability: configuration and customization. Customization variability can be described using programming language code or creative construction DSLs, whereas configuration variability is described using configuration based approaches, such as feature models. Many product lines have both kinds of variability, and they need to be integrated efficiently. This paper describes an approach for product line engineering using projectional language workbenches. These represent code and models with the same fundamental technology, enabling the mixing of models and code. They make the tight integration between several domain-specific languages possible and simple. Since they can store arbitrary information in models, it is possible to overlay configuration variability over customization variability (i.e. apply feature model-based configuration to code and models). Because of the projectional approach to editing, programs can be shown with or without the dependencies on feature models, they can even be rendered (and edited) for a specific variant. This approach leads to highly integrated and productive tools for product line development. The paper explains the approach, outlines the implementation of a prototype tool based on Jetbrains MPS and illustrates the benefits using a small product line for embedded systems.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {41–48},
numpages = {8},
keywords = {domain-specific languages, feature modeling, language composition, product line engineering},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@inproceedings{10.1007/978-3-030-21290-2_42,
author = {Reinhartz-Berger, Iris and Shimshoni, Ilan and Abdal, Aviva},
title = {Behavior-Derived Variability Analysis: Mining Views for Comparison and Evaluation},
year = {2019},
isbn = {978-3-030-21289-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-21290-2_42},
doi = {10.1007/978-3-030-21290-2_42},
abstract = {The large variety of computerized solutions (software and information systems) calls for a systematic approach to their comparison and evaluation. Different methods have been proposed over the years for analyzing the similarity and variability of systems. These methods get artifacts, such as requirements, design models, or code, of different systems (commonly in the same domain), identify and calculate their similarities, and represent the variability in models, such as feature diagrams. Most methods rely on implementation considerations of the input systems and generate outcomes based on predefined, fixed strategies of comparison (referred to as variability views). In this paper, we introduce an approach for mining relevant views for comparison and evaluation, based on the input artifacts. Particularly, we equip SOVA – a Semantic and Ontological Variability Analysis method – with data mining techniques in order to identify relevant views that highlight variability or similarity of the input artifacts (natural language requirement documents). The comparison is done using entropy and Rand index measures. The method and its outcomes are evaluated on a case of three photo sharing applications.},
booktitle = {Advanced Information Systems Engineering: 31st International Conference, CAiSE 2019, Rome, Italy, June 3–7, 2019, Proceedings},
pages = {675–690},
numpages = {16},
keywords = {Software Product Line Engineering, Variability analysis, Requirements specifications, Feature diagrams},
location = {Rome, Italy}
}

@inproceedings{10.1145/1852786.1852794,
author = {Murgia, Alessandro and Concas, Giulio and Marchesi, Michele and Tonelli, Roberto},
title = {A machine learning approach for text categorization of fixing-issue commits on CVS},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852794},
doi = {10.1145/1852786.1852794},
abstract = {We studied data mining from CVS repositories of two large OO projects, Eclipse and Netbeans, focusing on "fixing-issue" commits.We highlight common characteristics of issue reporting, and problems related to the identification of these messages, and compare static traditional approaches, like Knowledge Engineering, to dynamic approaches based on Machine Learning techniques. We compare for the first time performances of Machine Learning (ML) techniques to automatic classify "fixing-issues" among message commits. Our study calculates precision and recall of different Machine Learning Classifiers for the correct classification of issue-reporting commits. Our results show that some ML classifiers can correctly classify up to 99.9% of such commits.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {6},
numpages = {10},
keywords = {classifier, data mining, machine learning},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@article{10.5555/965697.965701,
author = {Du, Xuehong and Jiao, Jianxin and Tseng, Mitchell M.},
title = {Product family modeling and design support: An approach based on graph rewriting systems},
year = {2002},
issue_date = {April 2002},
publisher = {Cambridge University Press},
address = {USA},
volume = {16},
number = {2},
issn = {0890-0604},
abstract = {Earlier research on product family design (PFD) often highlights isolated and successful empirical studies with a limited attempt to explore the modeling and design support issues surrounding this economically important class of engineering design problems. This paper proposes a graph rewriting system to organize product family data according to the underpinning logic and to model product derivation mechanisms for PFD. It represents the structural and behavioral aspects of product families as family graphs and related graph operations, respectively. The derivation of product variants becomes a graph rewriting process, in which family graphs are transformed to variant graphs by applying appropriate graph rewriting rules. The system is developed in the language of programmed graph rewriting systems or PROGRES, which supports the specification of hierarchical graph schema and parametric rewriting rules. A meta model is defined for family graphs to factor out those entities common to all product families. A generic model is defined to describe all specific entities relevant to particular families. An instance model describes all product variants for individual customer orders. A prototype of a graph-based PFD system for office chairs is also developed. The system can provide an interactive environment for customers to make choices among product offerings. It also facilitates design automation of product families and enhances interactions and negotiations among sales, design, and manufacturing.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = apr,
pages = {103–120},
numpages = {18},
keywords = {Design Automation, Graph Grammar, Mass Customization Systems, PROGRES, Product Family}
}

@article{10.1287/mksc.2019.1204,
author = {Huang, Xiao and Zhang, Dan},
title = {Service Product Design and Consumer Refund Policies},
year = {2020},
issue_date = {March-April 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {39},
number = {2},
issn = {1526-548X},
url = {https://doi.org/10.1287/mksc.2019.1204},
doi = {10.1287/mksc.2019.1204},
abstract = {This paper studies joint quality and refund policy design for service products in the presence of customer valuation uncertainty.Customers often exhibit considerable uncertainty in their service valuations. In response, firms may tailor their products and allow service cancellations. We consider the joint product customization and refund policy decisions of a monopolistic firm selling to a heterogeneous customer population with imperfect signals on their valuations. Our results shed light on how customers’ valuation uncertainty, characterized by the valuation heterogeneity and signal quality, drives the interaction between product line and refund policy designs. In particular, when the valuation heterogeneity is high, the firm may choose to offer a single quality level with a full refund, leading to a variety reduction in the product line. In contrast, when the valuation heterogeneity is low, the firm will always offer a full product line without any refund. At moderate valuation heterogeneity, both qualities and refunds are subject to more customization, and a partial refund can be optimal when the signal quality is high, even though our setup does not involve aggregate demand uncertainty, capacity limitations, competition, or channel conflicts. Interestingly, despite its appeal, generous refund terms do not increase aggregate customer surplus. Furthermore, the firm may not have incentives to reduce customers’ valuation uncertainty even if doing so is costless. We verify the robustness of our results and discuss their practical implications.},
journal = {Marketing Science},
month = mar,
pages = {366–381},
numpages = {16},
keywords = {product line design, cancellations, return policy, partial refunds, consumer uncertainty, services, quality}
}

@inproceedings{10.5555/1924976.1925001,
author = {Bass, Janet and Pullman, David},
title = {Configuration management for mac OS X, it's just unix right?},
year = {2010},
publisher = {USENIX Association},
address = {USA},
booktitle = {Proceedings of the 24th International Conference on Large Installation System Administration},
pages = {1–6},
numpages = {6},
location = {San Jose, CA},
series = {LISA'10}
}

@article{10.1007/s10664-021-09944-w,
author = {Riom, Timothé and Sawadogo, Arthur and Allix, Kevin and Bissyandé, Tegawendé F. and Moha, Naouel and Klein, Jacques},
title = {Revisiting the VCCFinder approach for the identification of vulnerability-contributing commits},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09944-w},
doi = {10.1007/s10664-021-09944-w},
abstract = {Detecting vulnerabilities in software is a constant race between development teams and potential attackers. While many static and dynamic approaches have focused on regularly analyzing the software in its entirety, a recent research direction has focused on the analysis of changes that are applied to the code. VCCFinder is a seminal approach in the literature that builds on machine learning to automatically detect whether an incoming commit will introduce some vulnerabilities. Given the influence of VCCFinder in the literature, we undertake an investigation into its performance as a state-of-the-art system. To that end, we propose to attempt a replication study on the VCCFinder supervised learning approach. The insights of our failure to replicate the results reported in the original publication informed the design of a new approach to identify vulnerability-contributing commits based on a semi-supervised learning technique with an alternate feature set. We provide all artefacts and a clear description of this approach as a new reproducible baseline for advancing research on machine learning-based identification of vulnerability-introducing commits.},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {30},
keywords = {Vulnerability detection, Machine learning, Replication, Software engineering}
}

@article{10.4018/jismd.2012040102,
author = {Mazo, Raúl and Salinesi, Camille and Diaz, Daniel and Djebbi, Olfa and Lora-Michiels, Alberto},
title = {Constraints: The Heart of Domain and Application Engineering in the Product Lines Engineering Strategy},
year = {2012},
issue_date = {April 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {2},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012040102},
doi = {10.4018/jismd.2012040102},
abstract = {Drawing from an analogy between features based Product Line PL models and Constraint Programming CP, this paper explores the use of CP in the Domain Engineering and Application Engineering activities that are put in motion in a Product Line Engineering strategy. Specifying a PL as a constraint program instead of a feature model carries out two important qualities of CP: expressiveness and direct automation. On the one hand, variables in CP can take values over boolean, integer, real or even complex domains and not only boolean values as in most PL languages such as the Feature-Oriented Domain Analysis FODA. Specifying boolean, arithmetic, symbolic and reified constraint, provides a power of expression that spans beyond that provided by the boolean dependencies in FODA models. On the other hand, PL models expressed as constraint programs can directly be executed and analyzed by off-the-shelf solvers. This paper explores the issues of a how to specify a PL model using CP, including in the presence of multi-model representation, b how to verify PL specifications, c how to specify configuration requirements, and d how to support the product configuration activity. Tests performed on a benchmark of 50 PL models show that the approach is efficient and scales up easily to very large and complex PL specifications.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = apr,
pages = {33–68},
numpages = {36},
keywords = {Computer Science, Constraint-Based Product Lines, Information Systems, Product Line Analysis, Product Line Configuration, Product Line Integration, Product Line Reasoning, Product Line Specification, Product Line Verification}
}

@inproceedings{10.5555/2666064.2666071,
author = {Acher, Mathieu and Michel, Raphaël and Heymans, Patrick and Collet, Philippe and Lahire, Philippe},
title = {Languages and tools for managing feature models},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {Feature models (FMs) are the de facto standard for modeling variability of software product lines. The research effort is still intensive and aims at increasing the adoption of FMs in practice. Integrated solutions that combine state-of-the-art techniques, languages, and tools are emerging. We give an overview of complementary languages, TVL and FAMILIAR, and tools for the purpose of supporting the management (e.g., configuration) of FMs. We report on practical applications of the languages and tools in different domains and for different purposes. Still, we are interested in applying our solutions to other contexts (e.g., industrial) in order to determine their applicability and possible adoption by practitioners.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {25–28},
numpages = {4},
keywords = {automated reasoning, feature models, languages, model management, software product lines, variability},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {Software product line testing, dimensionality reduction, many-objective problems},
location = {Natal, Brazil},
series = {SAST '20}
}

@inproceedings{10.1007/11554844_7,
author = {Zhang, Weishan and Jarzabek, Stan},
title = {Reuse without compromising performance: industrial experience from RPG software product line for mobile devices},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_7},
doi = {10.1007/11554844_7},
abstract = {It is often believed that reusable solutions, being generic, must necessarily compromise performance. In this paper, we consider a family of Role-Playing Games (RPGs). We analyzed similarities and differences among four RPGs. By applying a reuse technique of XVCL, we built an RPG product line architecture (RPG-PLA) from which we could derive any of the four RPGs. We built into the RPG-PLA a number of performance optimization strategies that could benefit any of the four (and possibly other similar) RPGs. By comparing the original vs. the new RPGs derived from the RPG-PLA, we demonstrated that reuse allowed us to achieve improved performance, both speed and memory utilization, as compared to each game developed individually. At the same time, our solution facilitated rapid development of new games, for new mobile devices, as well as ease of evolving with new features the RPG-PLA and custom games already in use.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {57–69},
numpages = {13},
location = {Rennes, France},
series = {SPLC'05}
}

@inproceedings{10.1145/324138.324190,
author = {Sadowski, Deborah and Bapat, Vivek},
title = {The Arena product family: enterprise modeling solutions},
year = {1999},
isbn = {0780357809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/324138.324190},
doi = {10.1145/324138.324190},
booktitle = {Proceedings of the 31st Conference on Winter Simulation: Simulation---a Bridge to the Future - Volume 1},
pages = {159–166},
numpages = {8},
location = {Phoenix, Arizona, USA},
series = {WSC '99}
}

@inproceedings{10.5555/2820656.2820658,
author = {Lago, Patricia},
title = {Challenges and opportunities for sustainable software},
year = {2015},
publisher = {IEEE Press},
abstract = {With the increasing role played by software in supporting our society, its sustainability and environmental impact have become major factors in the development and operation of software-intensive systems. Myths and beliefs hide the real truth behind Green IT: IT is energy-inefficient because software is developed to make it so -- intentionally or not. But how far are we from being able to control software energy-efficiency? What makes software greener? How can we transform measuring software energy consumption in a general practice? What architectural design decisions will result in more sustainable systems? How can we ensure that new-generation software will be both cloud-ready and environmental-friendly? and How can we make evident the economic and social impact of developing software with 'energy in mind'? These are a few of the challenges ahead for a more sustainable digital society. This talk will discuss them, hence drawing directions for exciting challenges, promising opportunities, and ultimately inspiring research.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {1–2},
numpages = {2},
location = {Florence, Italy},
series = {PLEASE '15}
}

@article{10.1007/s11063-020-10286-9,
author = {Li, Li and Zhao, Kaiyi and Li, Sicong and Sun, Ruizhi and Cai, Saihua},
title = {Extreme Learning Machine for Supervised Classification with Self-paced Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10286-9},
doi = {10.1007/s11063-020-10286-9},
abstract = {The extreme learning machine (ELM), a typical machine learning algorithm based on feedforward neural network, has been widely used in classification, clustering, regression and feature learning. However, the traditional ELM learns all samples at once, and sample weights of traditional methods are defined before the learning process and they will not change during the learning process. So, its performance is vulnerable to noisy data and outliers, finding a way to solve this problem is meaningful. In this work, we propose a model of self-paced ELM named SP-ELM for binary classification and multi-classification originated from the self-paced learning paradigm. Concretely, the algorithm takes the importance of samples into account according to the loss of predicted value and real value, and it establishes the model from the simple samples to complex samples. By setting certain restrictions, the influence of complex data on the model is reduced. Four different self-paced regularization terms are adopted in the paper to select the instances. Experimental results demonstrate the effectiveness and of the proposed method by comparing it with other improved ELMs.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1723–1744},
numpages = {22},
keywords = {Classification, Extreme learning machine, Self-paced learning, Accuracy}
}

@inproceedings{10.1145/2364412.2364442,
author = {Cavalcante, Everton and Almeida, André and Batista, Thais and Cacho, Nélio and Lopes, Frederico and Delicato, Flavia C. and Sena, Thiago and Pires, Paulo F.},
title = {Exploiting software product lines to develop cloud computing applications},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364442},
doi = {10.1145/2364412.2364442},
abstract = {With the advance of the Cloud Computing paradigm, new challenges in terms of models, tools, and techniques to support developers to design, build and deploy complex software systems that make full use of the cloud technology arise. In the heterogeneous scenario of this new paradigm, the development of applications using cloud services becomes hard, and the software product lines (SPL) approach is potentially promising for this context since specificities of the cloud platforms, such as services heterogeneity, pricing model, and other aspects can be catered as variabilities to core features. In this perspective, this paper (i) proposes a seamless adaptation of the SPL-based development to include important features of cloud-based applications, and (ii) reports the experience of developing HW-CSPL, a SPL for the Health Watcher (HW) System, which allows citizens to register complaints and consult information regarding the public health system of a city. Several functionalities of this system were implemented using different Cloud Computing platforms, and run time specificities of this application deployed on the cloud were analyzed, as well as other information such as change impact and pricing.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {179–187},
numpages = {9},
keywords = {cloud computing, cloud platforms, health watcher system, services, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1007/s10664-021-09940-0,
author = {Cashman, Mikaela and Firestone, Justin and Cohen, Myra B. and Thianniwet, Thammasak and Niu, Wei},
title = {An empirical investigation of organic software product lines},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09940-0},
doi = {10.1007/s10664-021-09940-0},
abstract = {Software product line engineering is a best practice for managing reuse in families of software systems that is increasingly being applied to novel and emerging domains. In this work we investigate the use of software product line engineering in one of these new domains, synthetic biology. In synthetic biology living organisms are programmed to perform new functions or improve existing functions. These programs are designed and constructed using small building blocks made out of DNA. We conjecture that there are families of products that consist of common and variable DNA parts, and we can leverage product line engineering to help synthetic biologists build, evolve, and reuse DNA parts. In this paper we perform an investigation of domain engineering that leverages an open-source repository of more than 45,000 reusable DNA parts. We show the feasibility of these new types of product line models by identifying features and related artifacts in up to 93.5% of products, and that there is indeed both commonality and variability. We then construct feature models for four commonly engineered functions leading to product lines ranging from 10 to 7.5 × 1020 products. In a case study we demonstrate how we can use the feature models to help guide new experimentation in aspects of application engineering. Finally, in an empirical study we demonstrate the effectiveness and efficiency of automated reverse engineering on both complete and incomplete sets of products. In the process of these studies, we highlight key challenges and uncovered limitations of existing SPL techniques and tools which provide a roadmap for making SPL engineering applicable to new and emerging domains.},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {43},
keywords = {Software product lines, Synthetic biology, Reverse engineering, BioBricks}
}

@article{10.1145/3088440,
author = {Acher, Mathieu and Lopez-Herrejon, Roberto E. and Rabiser, Rick},
title = {Teaching Software Product Lines: A Snapshot of Current Practices and Challenges},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
url = {https://doi.org/10.1145/3088440},
doi = {10.1145/3088440},
abstract = {Software Product Line (SPL) engineering has emerged to provide the means to efficiently model, produce, and maintain multiple similar software variants, exploiting their common properties, and managing their variabilities (differences). With over two decades of existence, the community of SPL researchers and practitioners is thriving, as can be attested by the extensive research output and the numerous successful industrial projects. Education has a key role to support the next generation of practitioners to build highly complex, variability-intensive systems. Yet, it is unclear how the concepts of variability and SPLs are taught, what are the possible missing gaps and difficulties faced, what are the benefits, and what is the material available. Also, it remains unclear whether scholars teach what is actually needed by industry. In this article, we report on three initiatives we have conducted with scholars, educators, industry practitioners, and students to further understand the connection between SPLs and education, that is, an online survey on teaching SPLs we performed with 35 scholars, another survey on learning SPLs we conducted with 25 students, as well as two workshops held at the International Software Product Line Conference in 2014 and 2015 with both researchers and industry practitioners participating. We build upon the two surveys and the workshops to derive recommendations for educators to continue improving the state of practice of teaching SPLs, aimed at both individual educators as well as the wider community.},
journal = {ACM Trans. Comput. Educ.},
month = oct,
articleno = {2},
numpages = {31},
keywords = {Software product lines, software engineering teaching, software product line teaching, variability modeling}
}

@inproceedings{10.5555/2663370.2663379,
author = {Remmel, Hanna and Paech, Barbara and Engwer, Christian and Bastian, Peter},
title = {Design and rationale of a quality assurance process for a scientific framework},
year = {2013},
isbn = {9781467362610},
publisher = {IEEE Press},
abstract = {The testing of scientific frameworks is a challenging task. The special characteristics of scientific software e.g. missing test oracle, the need for high performance parallel computing, and high priority of non-functional requirements, need to be accounted for as well as the large variability in a framework. In our previous research, we have shown how software product line engineering can be applied to support the testing of scientific frameworks. We developed a process for handling the variability of a framework using software product line (SPL) variability modeling. From the variability models, we derive test applications and use them for system tests for the framework. In this paper we examine the overall quality assurance for a scientific framework. First, we propose a SPL test strategy for scientific frameworks called Variable test Application strategy for Frameworks (VAF). This test strategy tests both, commonality and variability, of the framework and supports the framework's users in testing their applications by creating reusable test artifacts. We operationalize VAF with test activities that are combined with other quality assurance activities to form the design of a quality assurance process for scientific frameworks. We introduce a list of special characteristics for scientific software that we use as rationale for the design of this process.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering for Computational Science and Engineering},
pages = {58–67},
numpages = {10},
keywords = {quality assurance process, scientific software development, software product line engineering, test strategy},
location = {San Francisco, California},
series = {SE-CSE '13}
}

@inproceedings{10.1007/978-3-030-58545-7_45,
author = {Li, Junbing and Zhang, Changqing and Zhu, Pengfei and Wu, Baoyuan and Chen, Lei and Hu, Qinghua},
title = {SPL-MLL: Selecting Predictable Landmarks for Multi-label Learning},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_45},
doi = {10.1007/978-3-030-58545-7_45},
abstract = {Although significant progress achieved, multi-label classification is still challenging due to the complexity of correlations among different labels. Furthermore, modeling the relationships between input and some (dull) classes further increases the difficulty of accurately predicting all possible labels. In this work, we propose to select a small subset of labels as landmarks which are easy to predict according to input (predictable) and can well recover the other possible labels (representative). Different from existing methods which separate the landmark selection and landmark prediction in the 2-step manner, the proposed algorithm, termed Selecting Predictable Landmarks for Multi-Label Learning (SPL-MLL), jointly conducts landmark selection, landmark prediction, and label recovery in a unified framework, to ensure both the representativeness and predictableness for selected landmarks. We employ the Alternating Direction Method (ADM) to solve our problem. Empirical studies on real-world datasets show that our method achieves superior classification performance over other state-of-the-art methods.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {783–799},
numpages = {17},
keywords = {Multi-label learning, Predictable landmarks, A unified framework},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.jss.2011.04.020,
author = {Hanssen, Geir K.},
title = {A longitudinal case study of an emerging software ecosystem: Implications for practice and theory},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.04.020},
doi = {10.1016/j.jss.2011.04.020},
abstract = {Software ecosystems is an emerging trend within the software industry, implying a shift from closed organizations and processes towards open structures, where actors external to the software development organization are becoming increasingly involved in development. This forms an ecosystem of organizations that are related through the shared interest in a software product, leading to new opportunities and new challenges to the industry and its organizational environment. To understand why and how this change occurs, we have followed the development of a software product line organization for a period of approximately five years. We have studied their change from a waterfall-like approach, via agile software product line engineering, towards an emerging software ecosystem. We discuss implications for practice, and propose a nascent theory on software ecosystems. We conclude that the observed change has led to an increase in collaboration across (previously closed) organizational borders, and to the development of a shared value consisting of two components: the technology (the product line, as an extensible platform), and the business domain it supports. Opening up both the technical interface of the product and the organizational interfaces are key enablers of such a change.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1455–1466},
numpages = {12},
keywords = {Agile software development, Longitudinal case study, Software ecosystems, Software product line engineering}
}

@inproceedings{10.5555/1753235.1753249,
author = {Montagud, Sonia and Abrahão, Silvia},
title = {Gathering current knowledge about quality evaluation in software product lines},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Recently, a number of methods and techniques for assessing the quality of software product lines have been proposed. However, to the best of our knowledge, there is no study which summarizes all the existing evidence about them. This paper presents a systematic review that investigates what methods and techniques have been employed (in the last 10 years) to evaluate the quality of software product lines and how they were employed. A total of 39 research papers have been reviewed from an initial set of 1388 papers. The results show that 25% of the papers reported evaluations at the Design phase of the Domain Engineering phase. The most widely used mechanism for modeling quality attributes was extended feature models and the most evaluated artifact was the base architecture. In addition, the results of the review have identified several research gaps. Specifically, 77% of the papers employed case studies as a "proof of concept" whereas 23% of the papers did not perform any type of validation. Our results are particularly relevant in positioning new research activities and in the selection of quality evaluation methods or techniques that best fit a given purpose.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {91–100},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.5555/2075144.2075172,
author = {Reinhartz-Berger, Iris and Sturm, Arnon and Wand, Yair},
title = {External variability of software: classification and ontological foundations},
year = {2011},
isbn = {9783642246050},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software variability management deals with the adaptation of reusable artifacts, such as models, specifications, and code, for particular requirements. External variability, which refers to software functionality as visible to users, deserves a special attention as it is closely linked to requirements and hence to early development stages. Errors or inaccuracies in these stages are relatively inexpensive to detect and easy to correct, yet can lead to expensive outcomes if not corrected. Nevertheless, the analysis of external variability in the literature is done ad-hoc and requires improvement.In this paper we introduce a framework for classifying external variability types based on ontological principles. The framework defines the external view of software in terms of the behavior of the application domain. Behavior is formalized as state changes in response to external stimuli. Based on this view we classify the possible similarities and differences among applications and identify an integrated similarity measurement. We demonstrate the usage of this classification framework for feasibility studies in system development.},
booktitle = {Proceedings of the 30th International Conference on Conceptual Modeling},
pages = {275–289},
numpages = {15},
keywords = {domain analysis, domain engineering, software product line engineering, variability management},
location = {Brussels, Belgium},
series = {ER'11}
}

@inproceedings{10.1145/3427423.3427450,
author = {Haris, M Syauqi and Kurniawan, Tri Astoto},
title = {Automated requirement sentences extraction from software requirement specification document},
year = {2021},
isbn = {9781450376051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427423.3427450},
doi = {10.1145/3427423.3427450},
abstract = {In the requirement reuse and natural language document-based Software Product Line (SPL) domain analysis, requirement sentences of the requirement document are the primary concern. Most studies conducted in this research area have document preprocessing stage in their methods that is a manual process to separate requirement sentences and non-requirement sentences from the document. This manual labor process might be tedious and error-prone since it will need much time and expert intervention to make this process completely done. In this paper, we present a method to automate requirement sentence extraction from the Software Requirement Specification (SRS) document by leveraging Natural Language Processing (NLP) approach and requirement boilerplate sentence patterns. Conducted experiments in this research show this method has such accuracy from 64% to 100% on precision value and recall value in the range of 64% to 89%.},
booktitle = {Proceedings of the 5th International Conference on Sustainable Information Engineering and Technology},
pages = {142–147},
numpages = {6},
keywords = {domain analysis, natural language processing, requirement boilerplate, software product line, software requirement reuse},
location = {Malang, Indonesia},
series = {SIET '20}
}

@inproceedings{10.1145/581339.581414,
author = {Maccari, Alessandro},
title = {Experiences in assessing product family software architecture for evolution},
year = {2002},
isbn = {158113472X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/581339.581414},
doi = {10.1145/581339.581414},
abstract = {Software architecture assessments are a means to detect architectural problems before the bulk of development work is done. They facilitate planning of improvement activities early in the lifecycle and allow limiting the changes on any existing software. This is particularly beneficial when the architecture has been planned to (or already does) support a whole product family, or a set of products that share common requirements, architecture, components or code. As the family requirements evolve and new products are added, the need to assess the evolvability of the existing architecture is vital. I illustrate two assessment case studies I have recently worked on in the mobile telephone software domain: the Symbian operating system platform and the network resource access control software system. The former assessment has been carried out as a task within the European project ESAPS, while the latter has been performed solely by Nokia. By means of simple experimental data, I show evidence of the usefulness of architectural assessment as rated by the participating stakeholders. Both assessments have led to the identification of previously unknown architectural defects, and to the consequent planning of improvement initiatives. In both cases, stakeholders noted that a number of side benefits, including improvement of communication and architectural documentation, were also of considerable importance. I illustrate the lessons we have learned, and outline suggestions for future research and experimentation.},
booktitle = {Proceedings of the 24th International Conference on Software Engineering},
pages = {585–592},
numpages = {8},
location = {Orlando, Florida},
series = {ICSE '02}
}

@article{10.1504/IJMLO.2016.076187,
author = {Siew, Jit-Ping and Low, Heng-Chin and Teoh, Ping-Chow},
title = {An interactive mobile learning application using machine learning framework in a flexible manufacturing environment},
year = {2016},
issue_date = {April 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {1/2},
issn = {1746-725X},
url = {https://doi.org/10.1504/IJMLO.2016.076187},
doi = {10.1504/IJMLO.2016.076187},
abstract = {A mobile learning program was initiated in a manufacturing facility 'MF' that supports customer orders from regions of Asia Pacific, Europe, and Middle East. The area of focus is in stencil printing process, with the objective of achieving zero defects. The challenge is to deliver clear, concise, and actionable information in order to empower three crews that work 24 hours every working day for continuous improvement. Using R statistical software to process information downloaded from Oracle database and an artificial neural network algorithm to learn the characteristics of the stencil printing process, an Android app was created as user interface in a mobile device which delivers the information in a form of a colour coded square matrix, introduced as ISN inferior-superior-neutral matrix. This approach resulted in dramatic process improvements in crews that are empowered with knowledge towards achieving the goal of zero defect manufacturing.},
journal = {Int. J. Mob. Learn. Organ.},
month = apr,
pages = {1–24},
numpages = {24}
}

@inproceedings{10.1007/978-3-319-23781-7_27,
author = {Bouarar, Selma and Jean, Stéphane and Siegmund, Norbert},
title = {SPL Driven Approach for Variability in Database Design},
year = {2015},
isbn = {9783319237800},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23781-7_27},
doi = {10.1007/978-3-319-23781-7_27},
abstract = {The evolution of computer technology has strongly impacted the database design. No phase was spared: several conceptual formalisms e.g. ER, UML, ontological, various logical models e.g. relational, object, key-value, a wide panoply of physical optimization structures and deployment platforms have been proposed. As a result, the database design process has become more complex involving more tasks and even more actors as database architect or analyst. Getting inspired from software engineering in dealing with variable similar systems, we propose a methodological framework for a variability-aware design of databases, whereby this latter is henceforth devised as a Software Product Line. Doing so guarantees a high reuse, automation, and customizability in generating ready-to-be implemented databases. We also propose a solution to help users make a suitable choice among the wide panoply. Finally, a case study is presented.},
booktitle = {Proceedings of the 5th International Conference on Model and Data Engineering - Volume 9344},
pages = {332–342},
numpages = {11},
keywords = {Database design, Software Product Line, Variability},
location = {Rhodes, Greece},
series = {MEDI 2015}
}

@inproceedings{10.1145/2019136.2019142,
author = {Kästner, Christian and Apel, Sven and Ostermann, Klaus},
title = {The road to feature modularity?},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019142},
doi = {10.1145/2019136.2019142},
abstract = {Modularity of feature representations has been a long standing goal of feature-oriented software development. While some researchers regard feature modules and corresponding composition mechanisms as a modular solution, other researchers have challenged the notion of feature modularity and pointed out that most feature-oriented implementation mechanisms lack proper interfaces and support neither modular type checking nor separate compilation. We step back and reflect on the feature-modularity discussion. We distinguish two notions of modularity, cohesion without interfaces and information hiding with interfaces, and point out the different expectations that, we believe, are the root of many heated discussions. We discuss whether feature interfaces should be desired and weigh their potential benefits and costs, specifically regarding crosscutting, granularity, feature interactions, and the distinction between closed-world and open-world reasoning. Because existing evidence for and against feature modularity and feature interfaces is shaky and inconclusive, more research is needed, for which we outline possible directions.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {5},
numpages = {8},
keywords = {crosscutting, feature interactions, feature models, feature modules, granularity, interfaces, modularity, module systems, variability},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1109/ICPC.2017.21,
author = {Tang, Yutian and Leung, Hareton},
title = {Constructing feature model by identifying variability-aware modules},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.21},
doi = {10.1109/ICPC.2017.21},
abstract = {Modeling variability, known as building feature models, should be an essential step in the whole process of product line development, maintenance and testing. The work on feature model recovery serves as a foundation and further contributes to product line development and variability-aware analysis. Different from the architecture recovery process even though they somewhat share the same process, the variability is not considered in all architecture recovery techniques. In this paper, we proposed a feature model recovery technique VMS, which gives a variability-aware analysis on the program and further constructs modules for feature model mining. With our work, we bring the variability information into architecture and build the feature model directly from the source base. Our experimental results suggest that our approach performs competitively and outperforms six other representative approaches for architecture recovery.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {263–274},
numpages = {12},
keywords = {configuration, feature model recovery, feature modules, product line, variability-aware modularity},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/2600428.2609601,
author = {Cormack, Gordon V. and Grossman, Maura R.},
title = {Evaluation of machine-learning protocols for technology-assisted review in electronic discovery},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609601},
doi = {10.1145/2600428.2609601},
abstract = {Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P&lt;0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P&lt;0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of "stabilization" -- determining when training is adequate, and therefore may stop.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {153–162},
numpages = {10},
keywords = {e-discovery, electronic discovery, predictive coding, technology-assisted review},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@article{10.1016/j.eswa.2017.07.021,
author = {Zhou, Feng and Jiao, Jianxin Roger and Yang, Xi Jessie and Lei, Baiying},
title = {Augmenting feature model through customer preference mining by hybrid sentiment analysis},
year = {2017},
issue_date = {December 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {89},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.07.021},
doi = {10.1016/j.eswa.2017.07.021},
abstract = {We use sentiment analysis of online product reviewers to extract customer preference information.The proposed sentiment analysis method is a hybrid combination of various affective lexicons.We adopt the commented features from product users to enhance the basic feature.We incorporate the customer preference information as attribute into the model.We demonstrate the feasibility and potential of the proposed method via an application case. A feature model is an essential tool to identify variability and commonality within a product line of an enterprise, assisting stakeholders to configure product lines and to discover opportunities for reuse. However, the number of product variants needed to satisfy individual customer needs is still an open question, as feature models do not incorporate any direct customer preference information. In this paper, we propose to incorporate customer preference information into feature models using sentiment analysis of user-generated online product reviews. The proposed sentiment analysis method is a hybrid combination of affective lexicons and a rough-set technique. It is able to predict sentence sentiments for individual product features with acceptable accuracy, and thus augment a feature model by integrating positive and negative opinions of the customers. Such opinionated customer preference information is regarded as one attribute of the features, which helps to decide the number of variants needed within a product line. Finally, we demonstrate the feasibility and potential of the proposed method via an application case of Kindle Fire HD tablets.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {306–317},
numpages = {12},
keywords = {Customer preference mining, Feature model, Product line planning, Sentiment analysis}
}

@inproceedings{10.5555/1892801.1892806,
author = {Chu-Carroll, Mark C. and Wright, James},
title = {Supporting distributed collaboration through multidimensional software configuration management},
year = {2003},
isbn = {3540140360},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In recent years, new software development methodologies and styles have become popular. In particular, many applications are being developed in the open-source community by groups of loosely coordinated programmers scattered across the globe.This style of widely distributed collaboration creates a suite of new problems for software development. Instead of being able to knock on the door of a collaborator, all communication between programmers working together on a system must be mediated through the computer. But at the same time, the bandwidth available for communication is dramatically more limited than those available to local collaborators.In this paper, we present a new SCM system called Stellation which is specifically designed to address the limits of current SCM systems, particularly when those systems are applied to large projects developed in a geographically distributed environment. Stellation attempts to enhance communication and collaboration between programmers by providing a mechanism called multidimensionality that allows them to share viewpoints on the structure and organization of the system; by providing a hierarchical branching mechanism that allows the granularity of coordination to be varied for different purposes; and by providing a mechanism for integrating programming language knowledge into the system, allowing it to be used for organizational and coordination purposes.},
booktitle = {Proceedings of the 2001 ICSE Workshops on SCM 2001, and SCM 2003 Conference on Software Configuration Management},
pages = {40–53},
numpages = {14},
location = {Toronto, Canada},
series = {SCM'01/SCM'03}
}

@article{10.1016/j.infsof.2017.01.012,
author = {Reinhartz-Berger, Iris and Figl, Kathrin and Haugen, ystein},
title = {Investigating styles in variability modeling},
year = {2017},
issue_date = {July 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {87},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.01.012},
doi = {10.1016/j.infsof.2017.01.012},
abstract = {ContextA common way to represent product lines is with variability modeling. Yet, there are different ways to extract and organize relevant characteristics of variability. Comprehensibility of these models and the ease of creating models are important for the efficiency of any variability management approach. ObjectiveThe goal of this paper is to investigate the comprehensibility of two common styles to organize variability into models hierarchical and constrained where the dependencies between choices are specified either through the hierarchy of the model or as cross-cutting constraints, respectively. MethodWe conducted a controlled experiment with a sample of 90 participants who were students with prior training in modeling. Each participant was provided with two variability models specified in Common Variability Language (CVL) and was asked to answer questions requiring interpretation of provided models. The models included 920 nodes and 819 edges and used the main variability elements. After answering the questions, the participants were asked to create a model based on a textual description. ResultsThe results indicate that the hierarchical modeling style was easier to comprehend from a subjective point of view, but there was also a significant interaction effect with the degree of dependency in the models, that influenced objective comprehension. With respect to model creation, we found that the use of a constrained modeling style resulted in higher correctness of variability models. ConclusionsPrior exposure to modeling style and the degree of dependency among elements in the model determine what modeling style a participant chose when creating the model from natural language descriptions. Participants tended to choose a hierarchical style for modeling situations with high dependency and a constrained style for situations with low dependency. Furthermore, the degree of dependency also influences the comprehension of the variability model.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {81–102},
numpages = {22},
keywords = {Cognitive aspects, Comprehensibility, Empirical research, Feature modeling, Hierarchical modeling, Product line engineering, Textual constraints, Variability modeling}
}

@article{10.1007/s10664-015-9401-9,
author = {Jonsson, Leif and Borg, Markus and Broman, David and Sandahl, Kristian and Eldh, Sigrid and Runeson, Per},
title = {Automated bug assignment: Ensemble-based machine learning in large scale industrial contexts},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9401-9},
doi = {10.1007/s10664-015-9401-9},
abstract = {Bug report assignment is an important part of software maintenance. In particular, incorrect assignments of bug reports to development teams can be very expensive in large software development projects. Several studies propose automating bug assignment techniques using machine learning in open source software contexts, but no study exists for large-scale proprietary projects in industry. The goal of this study is to evaluate automated bug assignment techniques that are based on machine learning classification. In particular, we study the state-of-the-art ensemble learner Stacked Generalization (SG) that combines several classifiers. We collect more than 50,000 bug reports from five development projects from two companies in different domains. We implement automated bug assignment and evaluate the performance in a set of controlled experiments. We show that SG scales to large scale industrial application and that it outperforms the use of individual classifiers for bug assignment, reaching prediction accuracies from 50 % to 89 % when large training sets are used. In addition, we show how old training data can decrease the prediction accuracy of bug assignment. We advice industry to use SG for bug assignment in proprietary contexts, using at least 2,000 bug reports for training. Finally, we highlight the importance of not solely relying on results from cross-validation when evaluating automated bug assignment.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1533–1578},
numpages = {46},
keywords = {Bug assignment, Bug reports, Classification, Ensemble learning, Industrial scale; Large scale, Machine learning}
}

@article{10.1023/A:1018682001890,
author = {Thompson, S. M.},
title = {Configuration management — keeping it all together},
year = {1997},
issue_date = {July 1997},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1358-3948},
url = {https://doi.org/10.1023/A:1018682001890},
doi = {10.1023/A:1018682001890},
abstract = {Any project, programme or organisation, working in any environment with released information, managing its change, maintaining traceability, and ensuring results always meet expectations, needs configuration management. Software projects introduce additional complexities — multiple developers working on the same item at the same time, the need for compatibility with other products and systems, targeting releases for multiple platforms, and supporting multiple versions (for example development and released versions). This paper describes the configuration management processes that support and manage products through their entire life cycle as they change and evolve.},
journal = {BT Technology Journal},
month = jul,
pages = {48–60},
numpages = {13}
}

@article{10.1016/j.jss.2019.110422,
author = {Edded, Sabrine and Sassi, Sihem Ben and Mazo, Raúl and Salinesi, Camille and Ghezala, Henda Ben},
title = {Collaborative configuration approaches in software product lines engineering: A systematic mapping study},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110422},
doi = {10.1016/j.jss.2019.110422},
journal = {J. Syst. Softw.},
month = dec,
numpages = {17},
keywords = {Product lines, Collaborative configuration, Systematic mapping study, Framework}
}

@inproceedings{10.1145/3474624.3476016,
author = {Bezerra, Carla and Lima, Rafael and Silva, Publio},
title = {DyMMer 2.0: A Tool for Dynamic Modeling and Evaluation of Feature Model},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476016},
doi = {10.1145/3474624.3476016},
abstract = {Managing dynamic variability has motivated several researchers to combine Dynamic Software Product Lines (DSPLs) practices with runtime variability mechanisms. By combining these approaches, a DSPL acquires important features, ranging from the ability to reconfigure by changing the context, adding or removing features, crash recovery, and re-adaptation based on changes in the model’s features. Feature model (FM) is an important artifact of a DPSL and there is a lack of tools that support the modeling of this artifact. We have extended the DyMMer tool for modeling FM of DSPLs from an adaptation mechanism based on MAPE-K to solve this problem. We migrated the DyMMer tool to a web version and incorporated new features: (i) modeling of FMs from SPLs and DSPLs, (ii) development of an adaptation mechanism for FM of DSPLs, (iii) repository of FMs, (iv) inclusion of thresholds for measures, and (v) user authentication. We believe that this tool is useful for research in the area of DSPLs, and also for dynamic domain modeling and evaluation. Video: https://youtu.be/WVHW6bI8ois},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {121–126},
numpages = {6},
keywords = {Dynamic Software Product Line, Feature Model, Modeling},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1007/s10270-011-0220-1,
author = {Hubaux, Arnaud and Heymans, Patrick and Schobbens, Pierre-Yves and Deridder, Dirk and Abbasi, Ebrahim Khalil},
title = {Supporting multiple perspectives in feature-based configuration},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-011-0220-1},
doi = {10.1007/s10270-011-0220-1},
abstract = {Feature diagrams have become commonplace in software product line engineering as a means to document variability early in the life cycle. Over the years, their application has also been extended to assist stakeholders in the configuration of software products. However, existing feature-based configuration techniques offer little support for tailoring configuration views to the profiles of the various stakeholders. In this paper, we propose a lightweight, yet formal and flexible, mechanism to leverage multidimensional separation of concerns in feature-based configuration. We propose a technique to specify concerns in feature diagrams and to generate automatically concern-specific configuration views. Three alternative visualisations are proposed. Our contributions are motivated and illustrated through excerpts from a real web-based meeting management application which was also used for a preliminary evaluation. We also report on the progress made in the development of a tool supporting multi-view feature-based configuration.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {641–663},
numpages = {23},
keywords = {Feature diagram, Feature-based configuration, Multi-view, Separation of concerns, Software product line engineering}
}

@article{10.5555/2503308.2503354,
author = {Gould, Stephen},
title = {DARWIN: a framework for machine learning and computer vision research and development},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
issn = {1532-4435},
abstract = {We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data flows.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {3533–3537},
numpages = {5},
keywords = {computer vision, graphical models, machine learning, open-source software}
}

@inproceedings{10.5555/2820656.2820664,
author = {de Jesus Souza, Magno Luã and Santos, Alcemir Rodrigues and de Almeida, Eduardo Santana},
title = {Towards the selection of modeling techniques for dynamic software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {Emerging domains such as smart homes and more recently smart cities represent a big challenge to software engineering. In such context, the need of runtime self-adaptations to cope with both user needs and environmental changes brings Dynamic Software Product Lines (DSPL) as a suitable solution. However, DSPL implementation itself is challenging, which demands a proper modeling. In this sense, the literature still lacks of means of choosing the modeling technique that best fits a given domain. This paper tackles such problem by defining a criteria for rank such techniques, which is used for ranking a set DSPL modeling techniques found in the literature.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {19–22},
numpages = {4},
keywords = {dynamic software product lines, dynamic variability, modeling techniques},
location = {Florence, Italy},
series = {PLEASE '15}
}

@inproceedings{10.5555/645789.667792,
author = {Habela, Piotr and Subieta, Kazimierz},
title = {OODBMS Metamodel Supporting Configuration Management of Large Applications},
year = {2002},
isbn = {3540440879},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Many practical cases of database schema evolution require an effective support from configuration management. Although DBMS construction and software configuration management (SCM) constitute the well established areas of research, they are usually considered in separation from each other. In this paper different issues of SCM are summarized and their relevance to DBMS is investigated. We suggest to extend the OODBMS metamodel to allow recording certain aspects of application-database dependencies in a database schema repository. The extended metamodel contains both typical database metamodel information as well as software configuration information. Such a solution we consider necessary for solving some of schema evolution problems.},
booktitle = {Proceedings of the 8th International Conference on Object-Oriented. Information Systems},
pages = {40–52},
numpages = {13},
series = {OOIS '02}
}

@inproceedings{10.1145/3302333.3302348,
author = {Krüger, Jacob},
title = {Are You Talking about Software Product Lines? An Analysis of Developer Communities},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302348},
doi = {10.1145/3302333.3302348},
abstract = {Community-question-answering systems, such as Stack Overflow, provide a platform for various communities to ask questions, discuss topics, and find knowledge. Especially software developers are heavily relying on such systems to identify solutions for their problems. While the content of community-question-answering systems may be less scientific, it usually represents practical knowledge from various perspectives and backgrounds. Thus, analyzing this content can be valuable for the scientific community to understand previous and current (i.e., open questions) needs of practitioners. In this paper, we report a systematic analysis of two websites that comprise communities with a focus on software development: Stack Exchange and Quora. We extract questions, answers, comments, and discussions on software product lines in general and feature modeling in particular. The results provide a historical perspective, an overview on commonly addressed scopes, and a classification of discussed topics and problems. Moreover, our findings are interesting to understand the practical impact of software-product-line techniques outside of well-analyzed case studies, to support lectures by identifying regularly asked questions, and to scope tool development based on reported technical problems.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {11},
numpages = {9},
keywords = {Quora, Software product line, Stack Overflow, community question answering, feature modeling, variability modeling},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/1509239.1509258,
author = {Bonifácio, Rodrigo and Borba, Paulo},
title = {Modeling scenario variability as crosscutting mechanisms},
year = {2009},
isbn = {9781605584423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1509239.1509258},
doi = {10.1145/1509239.1509258},
abstract = {Variability management is a common challenge for Software Product Line (SPL) adoption, since developers need suitable mechanisms for specifying and implementing variability that occurs at different SPL artifacts (requirements, design, implementation, and test). In this paper, we present a novel approach for use case scenario variability management, enabling a better separation of concerns between languages used to manage variabilities and languages used to specify use case scenarios. The result is that both representations can be understood and evolved in a separate way. We achieve such a goal by modeling variability management as a crosscutting phenomenon, for the reason that artifacts such as feature models, product configurations, and configuration knowledge crosscut each other with respect to each specific SPL member. After applying our approach to different case studies, we achieved a better feature modularity and scenario cohesion.},
booktitle = {Proceedings of the 8th ACM International Conference on Aspect-Oriented Software Development},
pages = {125–136},
numpages = {12},
keywords = {requirements models, software product line, variability management},
location = {Charlottesville, Virginia, USA},
series = {AOSD '09}
}

@article{10.1023/A:1008686916910,
author = {Tellioğlu, Hilda and Wagner, Ina},
title = {Negotiating Boundaries. Configuration Management in Software DevelopmentTeams},
year = {1997},
issue_date = {1997},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {6},
number = {4},
issn = {0925-9724},
url = {https://doi.org/10.1023/A:1008686916910},
doi = {10.1023/A:1008686916910},
abstract = {Using case-study material from three
small software development teams, this paper analyses
the regionalisation of ’design spaces‘. Its main
purpose is to understand problems and practices of
cooperative work in such spaces. ’Configuration
management‘ is used to denote both a practice and
supporting software tools and their relationship. A
major concern is how to develop practices and tools
that support cooperation across multiple
organisational and social boundaries while
simultaneously being ’respectful of regionalisations‘.},
journal = {Comput. Supported Coop. Work},
month = dec,
pages = {251–274},
numpages = {24},
keywords = {Articulation work, CSCW, Configuration Management (CM), Empirical studies}
}

@article{10.1016/j.cl.2016.09.004,
author = {Méndez-Acuña, David and Galindo, José A. and Degueule, Thomas and Combemale, Benoît and Baudry, Benoît},
title = {Leveraging Software Product Lines Engineering in the development of external DSLs},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2016.09.004},
doi = {10.1016/j.cl.2016.09.004},
abstract = {The use of domain-specific languages (DSLs) has become a successful technique in the development of complex systems. Consequently, nowadays we can find a large variety of DSLs for diverse purposes. However, not all these DSLs are completely different; many of them share certain commonalities coming from similar modeling patterns - such as state machines or petri nets - used for several purposes. In this scenario, the challenge for language designers is to take advantage of the commonalities existing among similar DSLs by reusing, as much as possible, formerly defined language constructs. The objective is to leverage previous engineering efforts to minimize implementation from scratch. To this end, recent research in software language engineering proposes the use of product line engineering, thus introducing the notion of language product lines. Nowadays, there are several approaches that result useful in the construction of language product lines. In this article, we report on an effort for organizing the literature on language product line engineering. More precisely, we propose a definition for the life-cycle of language product lines, and we use it to analyze the capabilities of current approaches. In addition, we provide a mapping between each approach and the technological space it supports. HighlightsSurvey on the applicability of software product lines in the construction of DSLs.General life-cycle for language product lines.Mapping current approaches on language product lines and technological spaces.Research map in language product lines engineering.},
journal = {Comput. Lang. Syst. Struct.},
month = nov,
pages = {206–235},
numpages = {30},
keywords = {Domain-specific languages, Software Product Lines Engineering, Software language engineering, Variability management}
}

@inproceedings{10.1145/3302333.3302346,
author = {Gomes, Karine and Teixeira, Leopoldo and Alves, Thayonara and Ribeiro, Márcio and Gheyi, Rohit},
title = {Characterizing safe and partially safe evolution scenarios in product lines: An Empirical Study},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302346},
doi = {10.1145/3302333.3302346},
abstract = {Evolving software product lines is often error-prone. Previous works have proposed classifying product line evolution into safe or partially safe, depending on the number of products that have their behavior preserved after evolution. Based on these notions, it is possible to derive transformation templates that abstract common evolution scenarios, such as adding an optional feature. However, existing works are focused on evaluating either safe or partially safe templates. Hence, in this work we aim to characterize product line evolution as a whole, measuring to what extent the evolution history is safe compared to partially safe, to better understand how product lines evolve. We measure how often existing templates happen using 2,300 commits from an open-source product line. According to our study, 91.7% of the commits represent partially safe evolution scenarios. Our results also show that 1,800 of these commits can automatically be classified as instances of existing templates. Among these, commits that do not modify other variability-aware models, are the most frequent, accounting for 72.3% out of the total of commits. For the remaining 500 commits, we identify that 24.4% are related to changes in the configuration knowledge, that is, the file responsible for the mapping between features and code.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {9},
keywords = {Configurable Systems, Empirical Study, Partially Safe Evolution, Product Line Evolution, Safe Evolution, Software Product Lines},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1145/1842713.1842717,
author = {Robinson, William N. and Ding, Yi},
title = {A survey of customization support in agent-based business process simulation tools},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/1842713.1842717},
doi = {10.1145/1842713.1842717},
abstract = {Agent-based business process simulation has grown in popularity, in part because of its analysis capabilities. The analyses depend on the kinds of simulations that can be built, adapted, and extended, which in turn depend on the underlying simulation framework. We report the results of our analysis of 19 agent-based process simulation tools and their simulation frameworks. We conclude that a growing number of simulation tools are using component-based software techniques. Nevertheless, most simulation tools do not directly support requirements models, their transformation into executable simulations, or the management of model variants over time. Such practices are becoming more widely applied in software engineering under the term software product line engineering (SPLE). Based on our analysis, agent-based process simulation tools may improve their customization capacity by: (1) supporting object modeling more completely and (2) supporting software product line engineering issues.},
journal = {ACM Trans. Model. Comput. Simul.},
month = oct,
articleno = {14},
numpages = {29},
keywords = {Agent-based modeling, application frameworks, encapsulation, event-driven simulation, modularity, software product line engineering}
}

@inproceedings{10.1145/111062.111082,
author = {Lubkin, David},
title = {Heterogeneous configuration management with DSEE},
year = {1991},
isbn = {0897914295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/111062.111082},
doi = {10.1145/111062.111082},
booktitle = {Proceedings of the 3rd International Workshop on Software Configuration Management},
pages = {153–160},
numpages = {8},
location = {Trondheim, Norway},
series = {SCM '91}
}

@article{10.4018/ijsds.2014040103,
author = {Durrani, Usman Khan and Pita, Zijad and Richardson, Joan},
title = {The Tetrad Influences: A Case Study of an Adaptable Software Configuration Management Process},
year = {2014},
issue_date = {April 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {2},
issn = {1947-8569},
url = {https://doi.org/10.4018/ijsds.2014040103},
doi = {10.4018/ijsds.2014040103},
abstract = {The Software Configuration Management (SCM) process with its origin in hardware development was first applied to the software development environment in the 1950's. Since then many IT dynamics associated with this process have evolved, such as, software development methodologies, software process improvement standards, computing environments, and organizational needs. Through the observation of these IT dynamics, which the researchers called "the tetrad influences", it is now apparent that there is a need to look into new adaptable approaches to apply the SCM process for traceability and governance. In this paper, we will present a conceptual framework highlighting the tetrad influences on the SCM process and will propose a Software configuration Adaptable Lean Agile Management "SALAM" model as a solution. We contribute a case study of a large Australian IT project where hybrid project teams delivered a consolidated software product in a hybrid cloud computing environment.},
journal = {Int. J. Strateg. Decis. Sci.},
month = apr,
pages = {30–42},
numpages = {13},
keywords = {Adaptable Environment, Agile Software Development, Cloud Computing, Governance, Lean, Lean Thinking, Model, Software Configuration Management, Tetrad, Traceability}
}

@article{10.5555/3455716.3455773,
author = {Ma, Fan and Meng, Deyu and Dong, Xuanyi and Yang, Yi},
title = {Self-paced multi-view co-training},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a "draw without replacement" strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {57},
numpages = {38},
keywords = {co-training, self-paced learning, multi-view learning, semi-supervised learning, ε-expansion theory, probably approximately correct learnable}
}

@article{10.3103/S1060992X19020048,
author = {Yakovenko, A. A.},
title = {A Hybrid Learning Approach for Adaptive Classification of Acoustic Signals Using the Simulated Responses of Auditory Nerve Fibers},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {2},
issn = {1060-992X},
url = {https://doi.org/10.3103/S1060992X19020048},
doi = {10.3103/S1060992X19020048},
journal = {Opt. Mem. Neural Netw.},
month = apr,
pages = {118–128},
numpages = {11},
keywords = {adaptive pattern classification, auditory periphery model, machine perception, neural responses, radial basis functions, self-organizing maps, unsupervised learning}
}

@inproceedings{10.5555/3495724.3496169,
author = {Parvaneh, Amin and Abbasnejad, Ehsan and Teney, Damien and Shi, Javen Qinfeng and van den Hengel, Anton},
title = {Counterfactual vision-and-language navigation: unravelling the unseen},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The task of vision-and-language navigation (VLN) requires an agent to follow text instructions to find its way through simulated household environments. A prominent challenge is to train an agent capable of generalising to new environments at test time, rather than one that simply memorises trajectories and visual details observed during training. We propose a new learning strategy that learns both from observations and generated counterfactual environments. We describe an effective algorithm to generate counterfactual observations on the fly for VLN, as linear combinations of existing environments. Simultaneously, we encourage the agent's actions to remain stable between original and counterfactual environments through our novel training objective – effectively removing spurious features that would otherwise bias the agent. Our experiments show that this technique provides significant improvements in generalisation on benchmarks for Room-to-Room navigation and Embodied Question Answering.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {445},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.1145/3350768.3351299,
author = {de Oliveira, Davi Cedraz S. and Bezerra, Carla I. M.},
title = {Development of the Maintainability Index for SPLs Feature Models Using Fuzzy Logic},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351299},
doi = {10.1145/3350768.3351299},
abstract = {The variability of the common features in an Software Product Line (SPL) can be managed by an feature model, an artifact that consist of a tree-shaped diagram, that describe the features identified in the products and the possible relationships between them. Guarantee the quality of the feature model may be essential to ensure that errors do not propagate across all products. The process of evaluating the quality of a product or artifact can be done using measures, which may reflect the characteristics, sub-characteristics or attributes of quality. However, the isolated values of each measure do not allow access to a whole quality of the feature model, since most of the measures cover several specific aspects that are not correlated. In this context, this paper proposes the aggregation of measures in order to evaluate the maintainability of the feature model in SPL. We aim to investigate how to aggregate these measures and access the respective sub-characteristics by means of a single aggregate value that has the same available information as a set of measures. For this, we have used the theory of Fuzzy Logic as a technique for aggregation of these measures. The new aggregate measure represents the maintainability index of a feature models (MIFM) was obtained. Moreover, to evaluate the MIFM, we applied it to a set of models. It was verified that the aggregate measure obtained allows to measure if a feature models has a high or low maintainability index, supporting the domain engineer in the evaluation of the maintenance of the feature model in a faster and more precise way.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {357–366},
numpages = {10},
keywords = {Feature Models, Fuzzy Logic, Measures, Quality Evaluation, Software Product Line},
location = {Salvador, Brazil},
series = {SBES '19}
}

@article{10.1007/s10994-021-05965-0,
author = {Giffon, Luc and Emiya, Valentin and Kadri, Hachem and Ralaivola, Liva},
title = {QuicK-means: accelerating inference for K-means by learning fast transforms},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {5},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-021-05965-0},
doi = {10.1007/s10994-021-05965-0},
abstract = {K-means—and the celebrated Lloyd’s algorithm—is more than the clustering method it was originally designed to be. It has indeed proven pivotal to help increase the speed of many machine learning, data analysis techniques such as indexing, nearest-neighbor search and prediction, data compression and, lately, inference with kernel machines. Here, we introduce an efficient extension of K-means, dubbed QuicK-means, that rests on the idea of expressing the matrix of the K cluster centroids as a product of sparse matrices, a feat made possible by recent results devoted to find approximations of matrices as a product of sparse factors. Using such a decomposition squashes the complexity of the matrix-vector product between the factorized K×D centroid matrix U and any vector from OKD to OAlogB+B, with A=minK,D and B=maxK,D, where D is the dimension of the data. This drastic computational saving has a direct impact in the assignment process of a point to a cluster. We propose to learn such a factorization during the Lloyd’s training procedure. We show that resorting to a factorization step at each iteration does not impair the convergence of the optimization scheme, and demonstrate the benefits of our approach experimentally.},
journal = {Mach. Learn.},
month = may,
pages = {881–905},
numpages = {25},
keywords = {k-means, Clustering, Fast transforms, Machine learning}
}

@article{10.1287/mnsc.2018.3136,
author = {Stamatopoulos, Ioannis and Tzamos, Christos},
title = {Design and Dynamic Pricing of Vertically Differentiated Inventories},
year = {2019},
issue_date = {September 2019},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {65},
number = {9},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2018.3136},
doi = {10.1287/mnsc.2018.3136},
abstract = {We study a model in which a monopoly firm designs the quality profile of its inventory and then dynamically updates its pricing menu for a finite selling horizon to maximize revenue. In a counterfactual scenario, a social planner goes through the same process to maximize total welfare. We show that in both scenarios the problem of dynamically pricing heterogeneous-quality (vertically differentiated) inventories is equivalent to that of dynamically pricing homogeneous-quality inventories, in the sense that a solution to one implies a solution to the other. Moreover, we prove a strong scarcity result, which suggests that the sale of a product drives up the prices on all remaining products, whether of higher or lower quality. We then consider product line design under a production technology that utilizes costly and potentially limited resources. We show that with unlimited (but costly) resources, the revenue maximizer undersupplies quality to all products compared with the social planner. With limited resources, we show that the revenue maximizer exhibits elitism: he overallocates (underallocates) resources on the production of high-quality (low-quality) products. However, as the volume of expected consumer arrivals increases to infinity, both the revenue maximizer and the welfare maximizer allocate resources equally across products.},
journal = {Manage. Sci.},
month = sep,
pages = {4222–4241},
numpages = {20},
keywords = {revenue management, welfare analysis, product line design, resource allocation}
}

@inproceedings{10.1145/2745802.2745806,
author = {Santos, Alcemir Rodrigues and de Oliveira, Raphael Pereira and de Almeida, Eduardo Santana},
title = {Strategies for consistency checking on software product lines: a mapping study},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745806},
doi = {10.1145/2745802.2745806},
abstract = {Context. Software Product Lines (SPL) has become one of the most prominents way to promote the systematic reuse of software artifacts. Like any other piece of software, with the SPL aging, it becomes necessary to manage their evolution. However, in this process, engineers might introduce divergences among the SPL artifacts. Thus, a number of initiatives address the management of such inconsistencies. Objective. In this paper, we mapped the existing approaches to inconsistency management within SPL. Method. We used the systematic mapping study methodology. Results. We classified and performed a characterization of the approaches found, which we mangaged to arrange in three main categories. Most papers selected proposed new methods as solution research. Besides, there is still a need for validation and evaluation studies. Conclusion. We identified a lack of support for a number of activities of consistency assurance. For instance, no paper addressed the tracking of findings, decisions, and actions, as well as, few papers describing either the handling or a management policy for identified inconsistencies.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {5},
numpages = {14},
keywords = {consistency checking, literature review, mapping study, software product line engineering},
location = {Nanjing, China},
series = {EASE '15}
}

@article{10.5555/3546258.3546440,
author = {Klink, Pascal and Abdulsamad, Hany and Belousov, Boris and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {A probabilistic interpretation of self-paced learning with applications to reinforcement learning},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {182},
numpages = {52},
keywords = {curriculum learning, reinforcement learning, self-paced learning, tempered inference, rl-as-inference}
}

@inproceedings{10.1145/72910.73353,
author = {Thomson, R. and Sommerville, I.},
title = {Configuration management using SySL},
year = {1989},
isbn = {0897913345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/72910.73353},
doi = {10.1145/72910.73353},
booktitle = {Proceedings of the 2nd International Workshop on Software Configuration Management},
pages = {106–109},
numpages = {4},
location = {Princeton, New Jersey, USA},
series = {SCM '89}
}

@article{10.1007/s11219-017-9400-8,
author = {Alférez, Mauricio and Acher, Mathieu and Galindo, José A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Automated reasoning, Configuration, Domain-specific languages, Feature modeling, Software product line engineering, Variability modeling, Video testing}
}

@article{10.1016/j.dss.2008.03.003,
author = {Mohan, Kannan and Xu, Peng and Cao, Lan and Ramesh, Balasubramaniam},
title = {Improving change management in software development: Integrating traceability and software configuration management},
year = {2008},
issue_date = {November, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {45},
number = {4},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2008.03.003},
doi = {10.1016/j.dss.2008.03.003},
abstract = {Software configuration management (SCM) and traceability are two prominent practices that support change management in software development. While SCM helps manage the evolution of software artifacts and their documentation, traceability helps manage knowledge about the process of the development of software artifacts. In this paper, we present the integration of traceability and SCM to help change management during the development and evolution of software artifacts. We developed a traceability model using a case study conducted in a software development organization. This model represents knowledge elements that are essential to comprehensively manage changes tracked within the change management function of SCM tools. A tool that supports the integrated practice of SCM and traceability is also presented. We illustrate the usefulness of our model and tool using a change management scenario that was drawn from our case study. We also present a qualitative study towards empirically evaluating the usefulness of our approach.},
journal = {Decis. Support Syst.},
month = nov,
pages = {922–936},
numpages = {15},
keywords = {Change management, Process knowledge, Software configuration management, Traceability}
}

@inproceedings{10.1145/1294261.1294284,
author = {Su, Ya-Yunn and Attariyan, Mona and Flinn, Jason},
title = {AutoBash: improving configuration management with operating system causality analysis},
year = {2007},
isbn = {9781595935915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294261.1294284},
doi = {10.1145/1294261.1294284},
abstract = {AutoBash is a set of interactive tools that helps users and system administrators manage configurations. AutoBash leverages causal tracking support implemented within our modified Linux kernel to understand the inputs (causal dependencies) and outputs (causal effects) of configuration actions. It uses OS-level speculative execution to try possible actions, examine their effects, and roll them back when necessary. AutoBash automates many of the tedious parts of trying to fix a misconfiguration, including searching through possible solutions, testing whether a particular solution fixes a problem, and undoing changes to persistent and transient state when a solution fails. Our results show that AutoBash correctly identifies the solution to several CVS, gcc cross-compiler, and Apache configuration errors. We also show that causal analysis reduces AutoBash's search time by an average of 35% and solution verification time by an average of 70%.},
booktitle = {Proceedings of Twenty-First ACM SIGOPS Symposium on Operating Systems Principles},
pages = {237–250},
numpages = {14},
keywords = {causality, configuration management, speculative execution},
location = {Stevenson, Washington, USA},
series = {SOSP '07}
}

@article{10.1145/280277.280280,
author = {Conradi, Reidar and Westfechtel, Bernhard},
title = {Version models for software configuration management},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/280277.280280},
doi = {10.1145/280277.280280},
abstract = {After more than 20 years of research and practice in software configuration management (SCM), constructing consistent configurations of versioned software products still remains a challenge. This article focuses on the version models underlying both commercial systems and research prototypes. It provides an overview and classification of different versioning paradigms and defines and relates fundamental concepts such as revisions, variants, configurations, and changes. In particular, we focus on intensional versioning, that is, construction of versions based on configuration rules. Finally, we provide an overview of systems that have had significant impact on the development of the SCM discipline and classify them according to a detailed taxonomy.},
journal = {ACM Comput. Surv.},
month = jun,
pages = {232–282},
numpages = {51},
keywords = {changes, configuration rules, configurations, revisions, variants, versions}
}

@inproceedings{10.1007/978-3-030-23502-4_14,
author = {Sondur, Sanjeev and Kant, Krishna},
title = {Towards Automated Configuration of Cloud Storage Gateways: A Data Driven Approach},
year = {2019},
isbn = {978-3-030-23501-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-23502-4_14},
doi = {10.1007/978-3-030-23502-4_14},
abstract = {Cloud storage gateways (CSGs) are an essential part of enterprises to take advantage of the scale and flexibility of cloud object store. A CSG provides clients the impression of a locally configured large size block-based storage device, which needs to be mapped to remote cloud storage which is invariably object based. Proper configuration of the cloud storage gateway is extremely challenging because of numerous parameters involved and interactions among them. In this paper, we study this problem for a commercial CSG product that is typical of offerings in the market. We explore how machine learning techniques can be exploited both for the forward problem (i.e. predicting performance from the configuration parameters) and backward problem (i.e. predicting configuration parameter values from the target performance). Based on extensive testing with real world customer workloads, we show that it is possible to achieve excellent prediction accuracy while ensuring that the model is not overfitted to the data.},
booktitle = {Cloud Computing – CLOUD 2019: 12th International Conference, Held as Part of the Services Conference Federation, SCF 2019, San Diego, CA, USA, June 25–30, 2019, Proceedings},
pages = {192–207},
numpages = {16},
keywords = {Cloud storage gateway, Object store, Performance, Configuration management, Machine learning},
location = {San Diego, CA, USA}
}

@article{10.1016/j.scico.2012.04.004,
author = {Hartmann, Herman and Keren, Mila and Matsinger, Aart and Rubin, Julia and Trew, Tim and Yatzkar-Haham, Tali},
title = {Using MDA for integration of heterogeneous components in software supply chains},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.04.004},
doi = {10.1016/j.scico.2012.04.004},
abstract = {Software product lines are increasingly built using components from specialized suppliers. A company that is in the middle of a supply chain has to integrate components from its suppliers and offer (partially configured) products to its customers. To satisfy both the variability required by each customer and the variability required to satisfy different customers' needs, it may be necessary for such a company to use components from different suppliers, partly offering the same feature set. This leads to a product line with alternative components, possibly using different mechanisms for interfacing, binding and variability, which commonly occurs in embedded software development. In this paper, we describe the limitations of the current practice of combining heterogeneous components in a product line and describe the challenges that arise from software supply chains. We introduce a model-driven approach for automating the integration between components that can generate a partially or fully configured variant, including glue between mismatched components. We analyze the consequences of using this approach in an industrial context, using a case study derived from an existing supply chain and describe the process and roles associated with this approach.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {2313–2330},
numpages = {18},
keywords = {Component technology, Model driven engineering, Resource constrained products, Software integration, Software product line engineering, Software supply chains}
}

@article{10.1023/A:1018686102799,
author = {Moor, S. R. and Gleen, K. E. and Gunne-Braden, J.},
title = {A BT enterprise configuration management system},
year = {1997},
issue_date = {July 1997},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1358-3948},
url = {https://doi.org/10.1023/A:1018686102799},
doi = {10.1023/A:1018686102799},
abstract = {Enterprise configuration management provides a framework within which to control the growth and complexity of modern large-scale integrated systems. BT is a leader in the field. This paper describes the company‘s design, implementation, and deployment of the Configuration Management Environment — a suite of whole-life, whole organisation, processes and tools needed to support a competitive and dynamic Networks and Systems division.},
journal = {BT Technology Journal},
month = jul,
pages = {73–85},
numpages = {13}
}

@article{10.1007/s10664-020-09892-x,
author = {Kuiter, Elias and Krieter, Sebastian and Krüger, Jacob and Saake, Gunter and Leich, Thomas},
title = {variED: an editor for collaborative, real-time feature modeling},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09892-x},
doi = {10.1007/s10664-020-09892-x},
abstract = {Feature models are a helpful means to document, manage, maintain, and configure the variability of a software system, and thus are a core artifact in software product-line engineering. Due to the various purposes of feature models, they can be a cross-cutting concern in an organization, integrating technical and business aspects. For this reason, various stakeholders (e.g., developers and consultants) may get involved into modeling the features of a software product line. Currently, collaboration in such a scenario can only be done with face-to-face meetings or by combining single-user feature-model editors with additional communication and version-control systems. While face-to-face meetings are often costly and impractical, using version-control systems can cause merge conflicts and inconsistency within a model, due to the different intentions of the involved stakeholders. Advanced tools that solve these problems by enabling collaborative, real-time feature modeling, analogous to Google Docs or Overleaf for text editing, are missing. In this article, we build on a previous paper and describe (1) the extended formal foundations of collaborative, real-time feature modeling, (2) our conflict resolution algorithm in more detail, (3) proofs that our formalization converges and preserves causality as well as user intentions, (4) the implementation of our prototype, and (5) the results of an empirical evaluation to assess the prototype’s usability. Our contributions provide the basis for advancing existing feature-modeling tools and practices to support collaborative feature modeling. The results of our evaluation show that our prototype is considered helpful and valuable by 17 users, also indicating potential for extending our tool and opportunities for new research directions.},
journal = {Empirical Softw. Engg.},
month = mar,
numpages = {47},
keywords = {Software product lines, Groupware, Feature modeling, Variability, Consistency maintenance, Collaboration}
}

@inproceedings{10.1007/978-3-662-44857-1_6,
author = {Samih, Hamza and Le Guen, Hélène and Bogusch, Ralf and Acher, Mathieu and Baudry, Benoit},
title = {An Approach to Derive Usage Models Variants for Model-Based Testing},
year = {2014},
isbn = {9783662448564},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-44857-1_6},
doi = {10.1007/978-3-662-44857-1_6},
abstract = {Testing techniques in industry are not yet adapted for product line engineering (PLE). In particular, Model-based Testing (MBT), a technique that allows to automatically generate test cases from requirements, lacks support for managing variability (differences) among a set of related product. In this paper, we present an approach to equip usage models, a widely used formalism in MBT, with variability capabilities. Formal correspondences are established between a variability model, a set of functional requirements, and a usage model. An algorithm then exploits the traceability links to automatically derive a usage model variant from a desired set of selected features. The approach is integrated into the professional MBT tool MaTeLo and is currently used in industry.},
booktitle = {Proceedings of the 26th IFIP WG 6.1 International Conference on Testing Software and Systems - Volume 8763},
pages = {80–96},
numpages = {17},
keywords = {Model-based Testing, Orthogonal Variability Model, Product Line, Requirements, Usage Model, Usage Model Variant},
location = {Madrid, Spain},
series = {ICTSS 2014}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and Jézéquel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Systematic literature review, Software product lines, Machine learning, Configurable systems}
}

@article{10.1016/j.neunet.2021.03.022,
author = {Zhong, Yongjian and Du, Bo and Xu, Chang},
title = {Learning to reweight examples in multi-label classification},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.03.022},
doi = {10.1016/j.neunet.2021.03.022},
journal = {Neural Netw.},
month = oct,
pages = {428–436},
numpages = {9},
keywords = {Multi-label classification, Self-paced learning, Reweight instance}
}

@article{10.1145/571681.571689,
author = {Estublier, Jacky and Leblang, David and Clemm, Geoff and Conradi, Reidar and Tichy, Walter and van der Hoek, André and Wiborg-Weber, Darcy},
title = {Impact of the research community on the field of software configuration management: summary of an impact project report},
year = {2002},
issue_date = {September 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/571681.571689},
doi = {10.1145/571681.571689},
abstract = {Software Configuration Management (SCM) is an important discipline in professional software development and maintenance. The importance of SCM has increased as programs have become larger and more complex and mission/life-critical. This paper discusses the evolution of SCM technology from the early days of software development to present and the impact university and industrial research has had along the way. It also includes a survey of the industrial state-of-the-practice and research directions.The paper published here is not intended to be a definitive assessment. Rather, our intention is to solicit comments and corrections from the community to help refine the work. If you would like to provide further information, please contact the first author. A longer version of this report can be found at http://wwwadele.imag.fr/SCMImpact.pdf.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {31–39},
numpages = {9},
keywords = {industrial impact, software configuration management, software engineering, software quality}
}

@inproceedings{10.1145/3141848.3141851,
author = {Assis, Guilherme and Vale, Gustavo and Figueiredo, Eduardo},
title = {Feature oriented programming in Groovy},
year = {2017},
isbn = {9781450355186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141848.3141851},
doi = {10.1145/3141848.3141851},
abstract = {Software Product Line (SPL) aims to reuse code and other artifacts in order to reduce costs and gain agility. Feature Oriented Programming (FOP) is a technique to develop SPLs that aims to improve the modularity and flexibility of feature code. The basic idea of FOP is to decompose software into smaller pieces, called features, so they can be composed according to the needs of each customer. Currently, Groovy programming language has no tool or framework that supports the implementation of FOP-SPLs. Groovy is a programming language that has been growing in popularity in recent years. Given this scenario, this work proposes and evaluates G4FOP, a light way to develop SPLs using Groovy. G4FOP extends FeatureHouse which is a framework for software composition and FOP. A preliminary evaluation shows that G4FOP covers Groovy grammar. We also demonstrate by an example that G4FOP is suitable to develop SPLs. G4FOP is currently integrated to the official FeatureHouse repository.},
booktitle = {Proceedings of the 8th ACM SIGPLAN International Workshop on Feature-Oriented Software Development},
pages = {21–30},
numpages = {10},
keywords = {Feature Oriented Programming, FeatureHouse, Groovy, Software Product Line},
location = {Vancouver, BC, Canada},
series = {FOSD 2017}
}

@article{10.1007/s10270-019-00722-3,
author = {Schwägerl, Felix and Westfechtel, Bernhard},
title = {Integrated revision and variation control for evolving model-driven software product lines},
year = {2019},
issue_date = {Dec 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {6},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-019-00722-3},
doi = {10.1007/s10270-019-00722-3},
abstract = {Software engineering projects are faced with abstraction, which is achieved by software models, historical evolution, which is addressed by revision control, and variability, which is managed with the help of software product line engineering. Addressing these phenomena by separate tools ignores obvious overlaps and therefore fails at exploiting synergies between revision and variation control for models. In this article, we present a conceptual framework for integrated revision and variation control of model-driven software projects. The framework reuses the abstractions of revision graphs and feature models and follows an iterative, revision-control-like approach to software product line engineering called product-based product line development. A single version (i.e., a variant of a selected revision) is made available in a workspace, where the user may apply arbitrary modifications. Based on a user-provided specification of the affected variants, the changes are automatically written back to a transparent repository that relies on an internal multi-version storage. The uniform handling of revisions and variants of models is achieved by transparently mapping version concepts to a semantic base layer, which is defined upon propositional logic. At the heart of the conceptual framework is a dynamic filtered editing model, which allows that the versioned artifacts and the feature model co-evolve. We contribute algorithms for checkout and commit, which satisfy a set of consistency constraints referring to variant specifications in an evolving feature model. This article furthermore addresses the orchestration of collaborative development by distributed replication and the well formedness of text and model artifacts to be checked out into the workspace. The Eclipse-based tool SuperMod demonstrates the feasibility of the conceptual framework. It allows the user to reuse arbitrary editing tools for text-based programming and/or Ecore-based modeling languages. An evaluation based on three case studies investigates the properties of SuperMod with a specific focus on filtered editing. The evaluation demonstrates that the dynamic filtered editing model reduces the cognitive complexity and the amount of user interaction necessary for variation control when compared to unfiltered model-driven approaches to software product line engineering.},
journal = {Softw. Syst. Model.},
month = dec,
pages = {3373–3420},
numpages = {48},
keywords = {Model versioning, Model-driven product lines, Variation control systems, Tool integration, Integrated historical and logical versioning}
}

@inproceedings{10.5555/1892801.1892813,
author = {Barkstrom, Bruce R.},
title = {Data product configuration management and versioning in large-scale production of satellite scientific data},
year = {2003},
isbn = {3540140360},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes a formal structure for keeping track of files, source code, scripts, and related material for large-scale Earth science data production. We first describe the environment and processes that govern this configuration management problem. Then, we show that a graph with typed nodes and arcs can describe the derivation of production design and of the produced files and their metadata. The graph provides three useful by-products: • a hierarchical data file inventory structure that can help system users find particular files, • methods for creating production graphs that govern job scheduling and provenance graphs that track all of the sources and transformations between raw data input and a particular output file, •a systematic relationship between different elements of the structure and development documentation.},
booktitle = {Proceedings of the 2001 ICSE Workshops on SCM 2001, and SCM 2003 Conference on Software Configuration Management},
pages = {118–133},
numpages = {16},
location = {Toronto, Canada},
series = {SCM'01/SCM'03}
}

@inproceedings{10.5555/302163.302187,
author = {Jézéquel, J.-M.},
title = {Reifying configuration management for object-oriented software},
year = {1998},
isbn = {0818683686},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 20th International Conference on Software Engineering},
pages = {240–249},
numpages = {10},
location = {Kyoto, Japan},
series = {ICSE '98}
}

@inproceedings{10.5555/3524938.3525920,
author = {Xu, Ning and Shu, Jun and Liu, Yun-Peng and Geng, Xin},
title = {Variational label enhancement},
year = {2020},
publisher = {JMLR.org},
abstract = {Label distribution covers a certain number of labels, representing the degree to which each label describes the instance. When dealing with label ambiguity, label distribution could describe the supervised information in a fine-grained way. Unfortunately, many training sets only contain simple logical labels rather than label distributions due to the difficulty of obtaining label distributions directly. To solve this problem, we consider the label distributions as the latent vectors and infer them from the logical labels in the training datasets by using variational inference. After that, we induce a predictive model to train the label distribution data by employing the multi-output regression technique. The recovery experiment on fourteen label distribution datasets and the predictive experiment on ten multilabel learning datasets validate the advantage of our approach over the state-of-the-art approaches.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {982},
numpages = {10},
series = {ICML'20}
}

@article{10.1016/j.asoc.2016.07.048,
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah and Jalab, Hamid A.},
title = {Extracting features from online software reviews to aid requirements reuse},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.048},
doi = {10.1016/j.asoc.2016.07.048},
abstract = {Display Omitted The extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have the access.Online reviews for software products can be used as input for features extraction to assist requirements reuse.Techniques from unsupervised learning and Natural Language Processing is employed as a propose solutions to Requirements Reuse problem.The approach obtained a precision of 87% (62% average) and a recall of 86% (82% average), when evaluated against the truth data set created manually. Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. In Requirements Reuse (RR), the extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have access to these software artefacts. Due to organisational privacy, SRS are always kept confidential and not easily available to the public. As alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the RR process. The aim of this paper is to propose a semi-automated approach, known as Feature Extraction for Reuse of Natural Language requirements (FENL), to extract phrases that can represent software features from software reviews in the absence of SRS as a way to initiate the RR process. FENL is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. In the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (IR) area. As a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. As for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. The performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and F-Measure.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1297–1315},
numpages = {19},
keywords = {Latent semantic analysis, Natural language processing, Requirements reuse, Software engineering, Unsupervised learning}
}

@article{10.1016/j.dss.2017.01.002,
author = {Carneiro, Nuno and Figueira, Gonalo and Costa, Miguel},
title = {A data mining based system for credit-card fraud detection in e-tail},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {95},
number = {C},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2017.01.002},
doi = {10.1016/j.dss.2017.01.002},
abstract = {Credit-card fraud leads to billions of dollars in losses for online merchants. With the development of machine learning algorithms, researchers have been finding increasingly sophisticated ways to detect fraud, but practical implementations are rarely reported. We describe the development and deployment of a fraud detection system in a large e-tail merchant. The paper explores the combination of manual and automatic classification, gives insights into the complete development process and compares different machine learning methods. The paper can thus help researchers and practitioners to design and implement data mining based systems for fraud detection or similar problems. This project has contributed not only with an automatic system, but also with insights to the fraud analysts for improving their manual revision process, which resulted in an overall superior performance. A case study of credit-card fraud detection in an e-tail company is presented.The design and implementation of a fraud detection system is reported.A practical perspective on the complete development process is given.The combination of an automatic classifier with manual revision is explored.Different supervised learning methods are compared.},
journal = {Decis. Support Syst.},
month = mar,
pages = {91–101},
numpages = {11},
keywords = {00-01, 99-00, Credit-card, Fraud detection, Online retail, Practical implementation, Supervised learning}
}

@inproceedings{10.1145/2499777.2500721,
author = {Seidl, Christoph and Schaefer, Ina and Aßmann, Uwe},
title = {Variability-aware safety analysis using delta component fault diagrams},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500721},
doi = {10.1145/2499777.2500721},
abstract = {Component Fault Diagrams (CFD) allow the specification of fault propagation paths, which is employed for the design of safety-critical systems as well as their certification. Even though families of safety-critical systems exist with many similar, yet not equal, variants there is no dedicated variability mechanism for CFDs to reuse commonalities of all family members and to alter only variable parts. In this paper, we present a variability representation approach for CFDs based on delta modeling that allows to transform an initial CFD within a closed or open variant space. Furthermore, we provide delta-aware analysis techniques for CFDs in order to analyse multiple variants efficiently. We show the feasibility of our approach by means of an example scenario based on the personal home robot TurtleBot using a prototypical implementation of our concepts.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {2–9},
numpages = {8},
keywords = {component fault diagrams, delta modeling, minimum cut set, safety, software fault trees, variability},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@article{10.1504/IJCAT.1999.004739,
author = {Chan, Stephen C. and Shek, Stanley C. and Lee, John W.},
title = {Modelling and applying constraint relations in a product family data model},
year = {1999},
issue_date = {May 1999},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {12},
number = {1},
issn = {0952-8091},
url = {https://doi.org/10.1504/IJCAT.1999.004739},
doi = {10.1504/IJCAT.1999.004739},
abstract = {A customer purchasing a product may select one variant among the several alternatives available for a component, and may select one among the many options available for a feature. The variants and options selected, in turn, may determine the available variants and options for other components and features, because of dependency relations among them. This paper describes and classifies common dependency relations among features and components, extends the ODMG data model to support such a data model, describes a graphical notation for representing the data model, and discusses several useful applications based on the data model that assist the customer in selecting these options and variants.},
journal = {Int. J. Comput. Appl. Technol.},
month = may,
pages = {27–38},
numpages = {12},
keywords = {constraint relations, data model, dependency relations, feature selection, product families, product features, product variants, variant selection}
}

@article{10.1016/j.dsp.2021.103205,
author = {Pourebrahim, Yousef and Razzazi, Farbod and Sameti, Hossein},
title = {Semi-supervised parallel shared encoders for speech emotion recognition},
year = {2021},
issue_date = {Nov 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {118},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103205},
doi = {10.1016/j.dsp.2021.103205},
journal = {Digit. Signal Process.},
month = nov,
numpages = {11},
keywords = {Semi-supervised learning, Speech emotion recognition, Domain adaptation, Deep neural networks}
}

@inproceedings{10.1145/2425415.2425420,
author = {Eyal-Salman, Hamzeh and Seriai, Abdelhak-Djamel and Dony, Christophe and Al-msie'deen, Ra'fat},
title = {Recovering traceability links between feature models and source code of product variants},
year = {2012},
isbn = {9781450318099},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2425415.2425420},
doi = {10.1145/2425415.2425420},
abstract = {Usually software product variants, developed by clone-and-own approach, form often a starting point for building Software Product Line (SPL). To migrate software products that deemed similar into a product line, it is essential to trace variability among software artifacts because the distinguishing factor between traditional software engineering and software product line engineering is the variability. Variability tracing is used to support conversion from traditional software development into software product line development and automate products derivation process such that core assets can be automatically configured for a product according to the features selection from the feature model. Tracing and maintaining interrelationships between artifacts within a software system also are needed to facilitate program comprehension, make the process of maintaining the system less dependent on individual experts. This paper presents a method based on information retrieval approach namely, latent semantic indexing, to establish traceability links between object-oriented source code of product variants and their feature model as representative of variability model.},
booktitle = {Proceedings of the VARiability for You Workshop: Variability Modeling Made Useful for Everyone},
pages = {21–25},
numpages = {5},
keywords = {feature models, latent semantic indexing, software product line, source code, traceability links, variability},
location = {Innsbruck, Austria},
series = {VARY '12}
}

@inproceedings{10.1145/1985484.1985498,
author = {Sun, Yu and Cho, Hyun and Gray, Jeff and White, Jules},
title = {Supporting feature model configuration using a demonstration-based approach},
year = {2011},
isbn = {9781450305846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985484.1985498},
doi = {10.1145/1985484.1985498},
abstract = {Configuration of feature models in software product-lines typically involves manipulating a model to modify the feature selections and analyzing the model to ensure that no configuration constraints are violated. In order to capture and reuse configuration knowledge from different users, model transformation and constraint languages can be used to specify and automate the constraint checking and model manipulation processes. However, this approach presents challenges to general end-users (e.g., domain experts who may not be programmers) who do not have experience using these languages. This paper presents a demonstration-based technique to support the capture and reuse of feature model configurations.},
booktitle = {Proceedings of the 2nd International Workshop on Product Line Approaches in Software Engineering},
pages = {55–59},
numpages = {5},
keywords = {feature model, model transformation by demonstration},
location = {Waikiki, Honolulu, HI, USA},
series = {PLEASE '11}
}

@article{10.1016/j.jss.2014.08.024,
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud},
title = {Cost-effective test suite minimization in product lines using search techniques},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.08.024},
doi = {10.1016/j.jss.2014.08.024},
abstract = {We define five cost-effectiveness measures for product line test minimization.We define a sound fitness function based on the cost-effectiveness measures.Ten search algorithms are empirically evaluated using an industrial case study.The scalability of the search algorithms is assessed using 500 artificial problems.Random-Weighted GA performs the best and can solve a wide range of problems. Cost-effective testing of a product in a product line requires obtaining a set of relevant test cases from the entire test suite via test selection and minimization techniques. In this paper, we particularly focus on test minimization for product lines, which identifies and eliminates redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. However, such minimization may result in the minimized test suite with low test coverage, low fault revealing capability, low priority test cases, and require more time than the allowed testing budget (e.g., time) as compared to the original test suite. To deal with the above issues, we formulated the minimization problem as a search problem and defined a fitness function considering various optimization objectives based on the above issues. To assess the performance of our fitness function, we conducted an extensive empirical evaluation by investigating the fitness function with three weight-based Genetic Algorithms (GAs) and seven multi-objective search algorithms using an industrial case study and 500 artificial problems inspired from the industrial case study. The results show that Random-Weighted Genetic Algorithm (RWGA) significantly outperforms the other algorithms since RWGA can balance all the objectives together by dynamically updating weights during each generation. Based on the results of our empirical evaluation, we also implemented a tool called TEst Minimization using Search Algorithms (TEMSA) to support test minimization using various search algorithms in the context of product lines.},
journal = {J. Syst. Softw.},
month = may,
pages = {370–391},
numpages = {22},
keywords = {Product line, Search algorithm, Test suite minimization}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacitúa, Ricardo and Sepúlveda, Samuel and Mazo, Raúl},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Feature model, Software product lines, Framework, Classification, Models}
}

@inproceedings{10.5555/1051937.1051956,
author = {Couch, Alva and Hart, John and Idhaw, Elizabeth G. and Kallas, Dominic},
title = {Seeking Closure in an Open World: A Behavioral Agent Approach to Configuration Management},
year = {2003},
publisher = {USENIX Association},
address = {USA},
abstract = {We present a new model of configuration management based upon a hierarchy of simple communicating autonomous agents. Each of these agents is responsible for a "closure": a domain of "semantic predictability" in which declarative commands to the agent have a simple, persistent, portable, and documented effect upon subsequent observable behavior. Closures are built bottom-up to form a management hierarchy based upon the pre-existing dependencies between subsystems in a complex system. Closure agents decompose configuration management via a modularity of effect and behavior that promises to eventually lead to self-organizing systems driven entirely by behavioral specifications, where a system's configuration is free of details that have no observable effect upon system.},
booktitle = {Proceedings of the 17th USENIX Conference on System Administration},
pages = {125–148},
numpages = {24},
location = {San Diego, CA},
series = {LISA '03}
}

@inproceedings{10.1109/SPLC.2011.47,
author = {Chen, Sheng and Erwig, Martin},
title = {Optimizing the Product Derivation Process},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.47},
doi = {10.1109/SPLC.2011.47},
abstract = {Feature modeling is widely used in software product-line engineering to capture the commonalities and variabilities within an application domain. As feature models evolve, they can become very complex with respect to the number of features and the dependencies among them, which can cause the product derivation based on feature selection to become quite time consuming and error prone. We address this problem by presenting techniques to find good feature selection sequences that are based on the number of products that contain a particular feature and the impact of a selected feature on the selection of other features. Specifically, we identify a feature selection strategy, which brings up highly selective features early for selection. By prioritizing feature selection based on the selectivity of features our technique makes the feature selection process more efficient. Moreover, our approach helps with the problem of unexpected side effects of feature selection in later stages of the selection process, which is commonly considered a difficult problem. We have run our algorithm on the e-Shop and Berkeley DB feature models and also on some automatically generated feature models. The evaluation results demonstrate that our techniques can shorten the product derivation processes significantly.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {35–44},
numpages = {10},
keywords = {Decision Sequence, Feature Model, Feature Selection},
series = {SPLC '11}
}

@inproceedings{10.1145/2556624.2556637,
author = {Machado, Ivan do Carmo and Santos, Alcemir Rodrigues and Cavalcanti, Yguaratã Cerqueira and Trzan, Eduardo Gomes and de Souza, Marcio Magalhães and de Almeida, Eduardo Santana},
title = {Low-level variability support for web-based software product lines},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556637},
doi = {10.1145/2556624.2556637},
abstract = {The Web systems domain has faced an increasing number of devices, browsers, and platforms to cope with, driving software systems to be more flexible to accomodate them. Software product line (SPL) engineering can be used as a strategy to implement systems capable of handling such a diversity. To this end, automated tool support is almost indispensable. However, current tool support gives more emphasis to modeling variability in the problem domain, over the support of variability at the solution domain. There is a need for mapping the variability between both abstraction levels, so as to determine what implementation impact a certain variability has. In this paper, we propose the FeatureJS, a FeatureIDE extension aiming at Javascript and HTML support for SPL engineering. The tool combines feature-oriented programming and preprocessors, as a strategy to map variability at source code with the variability modeled at a higher level of abstraction. We carried out a preliminary evaluation with an industrial project, aiming to characterize the capability of the tool to handle SPL engineering in the Web systems domain.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {8},
keywords = {Eclipse plugin, FeatureIDE, feature composition, feature oriented software development, software product line engineering, web systems domain},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1007/s10664-013-9254-z,
author = {Chen, Ning and Hoi, Steven C. and Xiao, Xiaokui},
title = {Software process evaluation: a machine learning framework with application to defect management process},
year = {2014},
issue_date = {December  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9254-z},
doi = {10.1007/s10664-013-9254-z},
abstract = {Software process evaluation is important to improve software development and the quality of software products in a software organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they usually suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In this study, we mainly focus on the procedure aspect of software processes, and formulate the problem as a sequence (with additional information, e.g., time, roles, etc.) classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to evaluate the execution of a software process more objectively. To validate the efficacy of our approach, we apply it to evaluate the execution of a defect management (DM) process in nine real industrial software projects. Our empirical results show that our approach is effective and promising in providing a more objective and quantitative measurement for the DM process evaluation task. Furthermore, we conduct a comprehensive empirical study to compare our proposed machine learning approach with an existing conventional approach (i.e., artifacts inspection). Finally, we analyze the advantages and disadvantages of both approaches in detail.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {1531–1564},
numpages = {34},
keywords = {Defect management process, Machine learning, Sequence classification, Software process evaluation}
}

@inproceedings{10.1145/377435.377707,
author = {Xizhe, Jin},
title = {Evaluation technique of software configuration management (poster session)},
year = {2001},
isbn = {1581133308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/377435.377707},
doi = {10.1145/377435.377707},
booktitle = {Proceedings of the 6th Annual Conference on Innovation and Technology in Computer Science Education},
pages = {186},
location = {Canterbury, United Kingdom},
series = {ITiCSE '01}
}

@article{10.1016/j.procs.2016.08.238,
author = {Kobayashi, Nobuhide and Yamada, Hikari and Utsunomiya, Hiroyuki and Morisaki, Shuji and Yamamoto, Shuichiro},
title = {The Evaluation Knowledge of Standard Software Asset using The Seven Samurai Framework},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {96},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2016.08.238},
doi = {10.1016/j.procs.2016.08.238},
abstract = {The knowledge which is needed on automotive software development, increases significantly according to large-scale, complexity of automotive software. Therefore, it is very difficult for an engineer to understand the whole software development. This paper introduces a way of constructing a meta-model, which visualizes the knowledge of expert engineers, based on The Seven Samurai framework. It can solve issues of system development by considering the seven types of elements which are defined in this framework. And then, its name was defined based on the famous Japanese cinema. Additionally, this paper shows the evaluation results of applying the meta-model to the evaluation of actual standard software assets in the product line, and then the effectiveness of the proposed approach is confirmed based on the results.},
journal = {Procedia Comput. Sci.},
month = oct,
pages = {782–790},
numpages = {9},
keywords = {NFR framework, The Seven Samurai framework, meta-model, product line, variants}
}

@phdthesis{10.5555/101563,
author = {Wiebe, Douglas J.},
title = {Generic software configuration management: theory and design},
year = {1990},
publisher = {University of Washington},
address = {USA},
abstract = {This dissertation addresses the software configuration management (SCM) issues of classification, versioning, validation and building. Existing SCM systems usually hardwire particular design decisions on these issues into their implementations. This hardwired approach leads to inflexible and non-adaptive SCM tools. Generic SCM is proposed here as a solution to the problems of hardwired SCM systems. A generic SCM system lets users make decisions on the basic SCM issues and thereby tailor the behavior of the generic SCM system. The dissertation describes a particular generic SCM system called Jason. Jason users specify classes of objects and configurations, families of versions, configuration consistency constraints and build plans. These specifications act as schemas for software objectbases that contain the actual software objects and configurations. Thus Jason users control the way in which Jason organizes and manages software objects.The dissertation defines an algebraic model of generic SCM that provides the theoretical basis for Jason. This model gives precise mathematical definitions for the SCM concepts of software objects, classes, configurations, objectbases, families, versions, version selection, consistency checking and building. The algebraic model applies and extends concepts from the fields of universal algebra and model theory.A prototype Jason implementation compiles user-written schemas into programs that implement classes, families and constraints. Experience with the prototype indicates that the Jason design performs well in practical applications.},
note = {UMI Order No. GAX90-26033}
}

@article{10.1145/2581376,
author = {Behjati, Razieh and Nejati, Shiva and Briand, Lionel C.},
title = {Architecture-Level Configuration of Large-Scale Embedded Software Systems},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2581376},
doi = {10.1145/2581376},
abstract = {Configuration in the domain of Integrated Control Systems (ICS) is largely manual, laborious, and error prone. In this article, we propose a model-based configuration approach that provides automation support for reducing configuration effort and the likelihood of configuration errors in the ICS domain. We ground our approach on component-based specifications of ICS families. We then develop a configuration algorithm using constraint satisfaction techniques over finite domains to generate products that are consistent with respect to their ICS family specifications. We reason about the termination and consistency of our configuration algorithm analytically. We evaluate the effectiveness of our configuration approach by applying it to a real subsea oil production system. Specifically, we have rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {25},
numpages = {43},
keywords = {Model-based product-line engineering, UML/OCL, consistent configuration, constraint satisfaction techniques, formal specification, product configuration}
}

@article{10.1145/232069.232083,
author = {Sommerville, Ian},
title = {Sixth international workshop on software configuration management},
year = {1996},
issue_date = {July 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/232069.232083},
doi = {10.1145/232069.232083},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {54–57},
numpages = {4}
}

@inproceedings{10.1145/111062.111067,
author = {Berrada, K. and Lopez, F. and Minot, R.},
title = {VMCM, a PCTE based version and configuration management system},
year = {1991},
isbn = {0897914295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/111062.111067},
doi = {10.1145/111062.111067},
booktitle = {Proceedings of the 3rd International Workshop on Software Configuration Management},
pages = {43–52},
numpages = {10},
location = {Trondheim, Norway},
series = {SCM '91}
}

@article{10.1016/j.scico.2006.05.011,
author = {Murta, Leonardo and Oliveira, Hamilton and Dantas, Cristine and Lopes, Luiz Gustavo and Werner, Cláudia},
title = {Odyssey-SCM: An integrated software configuration management infrastructure for UML models},
year = {2007},
issue_date = {March 2007},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {65},
number = {3},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2006.05.011},
doi = {10.1016/j.scico.2006.05.011},
abstract = {Model-driven development is becoming a reality. Different CASE tool vendors support this paradigm, allowing developers to define high-level models and helping to transform them into refined models or source code. However, current software configuration management tools use a file-based data model that is barely sufficient to manipulate source code. This file-based data model is not adequate to provide versioning capabilities for software modeling environments, which are strongly focused on analysis and architectural design artifacts. The existence of a versioned repository of high-level artifacts integrated with a customized change control process could help in the development and maintenance of such model-based systems. In this work, we introduce Odyssey-SCM, an integrated software configuration management infrastructure for UML models. This infrastructure is composed of a flexible version control system for fine-grained UML model elements, named Odyssey-VCS, and two complementary components: a customizable change control system tightly integrated with the version control system, and a traceability link detection tool that uses data mining to discover change traces among versioned UML model elements and provides the rationale of change traces, automatically collected from the integrated software configuration management infrastructure.},
journal = {Sci. Comput. Program.},
month = mar,
pages = {249–274},
numpages = {26},
keywords = {Change control system, Data mining, Model-driven development, Software configuration management, Version control system}
}

@article{10.4018/jismd.2012100101,
author = {Asadi, Mohsen and Mohabbati, Bardia and Gaševic, Dragan and Bagheri, Ebrahim and Hatala, Marek},
title = {Developing Semantically-Enabled Families of Method-Oriented Architectures},
year = {2012},
issue_date = {October 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012100101},
doi = {10.4018/jismd.2012100101},
abstract = {Method Engineering ME aims to improve software development methods by creating and proposing adaptation frameworks whereby methods are created to provide suitable matches with the requirements of the organization and address project concerns and fit specific situations. Therefore, methods are defined and modularized into components stored in method repositories. The assembly of appropriate methods depends on the particularities of each project, and rapid method construction is inevitable in the reuse and management of existing methods. The ME discipline aims at providing engineering capability for optimizing, reusing, and ensuring flexibility and adaptability of methods; there are three key research challenges which can be observed in the literature: 1 the lack of standards and tooling support for defining, publishing, discovering, and retrieving methods which are only locally used by their providers without been largely adapted by other organizations; 2 dynamic adaptation and assembly of methods with respect to imposed continuous changes or evolutions of the project lifecycle; and 3 variability management in software methods in order to enable rapid and effective construction, assembly and adaptation of existing methods with respect to particular situations. The authors propose semantically-enabled families of method-oriented architecture by applying service-oriented product line engineering principles and employing Semantic Web technologies.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {1–26},
numpages = {26},
keywords = {Method Engineering, Method Oriented Architecture MOA, Semantic Web, Software Development, Software Product Line}
}

@article{10.1016/j.infsof.2006.08.001,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Classifying variability modeling techniques},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.001},
doi = {10.1016/j.infsof.2006.08.001},
abstract = {Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {717–739},
numpages = {23},
keywords = {Classification, Software product family, Variability management, Variability modeling}
}

@article{10.1016/j.jksuci.2016.01.005,
author = {Maâzoun, Jihen and Bouassida, Nadia and Ben-Abdallah, Hanêne},
title = {Change impact analysis for software product lines},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {28},
number = {4},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2016.01.005},
doi = {10.1016/j.jksuci.2016.01.005},
abstract = {A software product line (SPL) represents a family of products in a given application domain. Each SPL is constructed to provide for the derivation of new products by covering a wide range of features in its domain. Nevertheless, over time, some domain features may become obsolete with the apparition of new features while others may become refined. Accordingly, the SPL must be maintained to account for the domain evolution. Such evolution requires a means for managing the impact of changes on the SPL models, including the feature model and design. This paper presents an automated method that analyzes feature model evolution, traces their impact on the SPL design, and offers a set of recommendations to ensure the consistency of both models. The proposed method defines a set of new metrics adapted to SPL evolution to identify the effort needed to maintain the SPL models consistently and with a quality as good as the original models. The method and its tool are illustrated through an example of an SPL in the Text Editing domain. In addition, they are experimentally evaluated in terms of both the quality of the maintained SPL models and the precision of the impact change management.},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = oct,
pages = {364–380},
numpages = {17},
keywords = {Change impact management, Feature model, Model evolution, Software product line}
}

@inproceedings{10.1145/1062455.1062552,
author = {Schmid, Klaus and John, Isabel and Kolb, Ronny and Meier, Gerald},
title = {Introducing the puLSE approach to an embedded system population at testo AG},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062552},
doi = {10.1145/1062455.1062552},
abstract = {Over the last few years, product line engineering has become a major theme in software engineering research, and is increasingly becoming a central topic of software engineering practice in the embedded domain.Migrating towards a product line approach is not an easy feat. It is even less so, if it is done under tight technology constraints in an embedded environment. It becomes even more difficult if the transition directly aims at integrating two product families into a single product population. In this paper, we discuss our experiences with a project where we successfully dealt with these difficulties and achieved a successful product line transition. In our paper we strongly emphasize the role of technology transfer, as many facets of product line know-how had to be transferred to guarantee a complete transition to product line engineering. From the experiences of this project many lessons learned can be deduced, which can be transferred to different environments.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {544–552},
numpages = {9},
keywords = {product line introduction, software product line, systematic software reuse, technology transfer},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@article{10.1007/s00521-018-3478-1,
author = {Gu, Nannan and Fan, Pengying and Fan, Mingyu and Wang, Di},
title = {Structure regularized self-paced learning for robust semi-supervised pattern classification},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3478-1},
doi = {10.1007/s00521-018-3478-1},
abstract = {Semi-supervised classification is a hot topic in pattern recognition and machine learning. However, in presence of heavy noise and outliers, the unlabeled training data could be very challenging or even misleading for the semi-supervised classifier. In this paper, we propose a novel structure regularized self-paced learning method for semi-supervised classification problems, which can efficiently learn partially labeled training data sequentially from the simple to the complex ones. The proposed formulation consists of three components: a cost function defined by a mixture of losses, a functional complexity regularizer, and a self-paced regularizer; and the corresponding optimization algorithm involves three iterative steps: classifier updating, sample importance calculating, and pseudo-labeling. In the proposed method, the cost function for classifier updating and sample importance calculating is defined as a combination of the label fitting loss and manifold smoothness loss. Then, the importance of the pseudo-labeled and unlabeled samples is adaptively calculated by the novel cost. Unlabeled samples with high importance values are pseudo-labeled with their current predictions. In this way, labels are efficiently propagated from the labeled samples to the unlabeled ones in the robust self-paced manner. Experimental results on several benchmark data sets are provided to show the effectiveness of the proposed method.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {6559–6574},
numpages = {16},
keywords = {Semi-supervised classification, Pattern classification, Self-paced learning, Manifold learning, Locally linear coding}
}

@article{10.5555/3455716.3455897,
author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
title = {Curriculum learning for reinforcement learning domains: a framework and survey},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {181},
numpages = {50},
keywords = {curriculum learning, reinforcement learning, transfer learning}
}

@article{10.1016/j.neunet.2021.05.021,
author = {Dias, Leonardo A. and Damasceno, Augusto M.P. and Gaura, Elena and Fernandes, Marcelo A.C.},
title = {A full-parallel implementation of Self-Organizing Maps on hardware},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {143},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.05.021},
doi = {10.1016/j.neunet.2021.05.021},
journal = {Neural Netw.},
month = nov,
pages = {818–827},
numpages = {10},
keywords = {Self-Organizing Map, Parallel design, Hardware, FPGA}
}

@inproceedings{10.1145/3416505.3423564,
author = {Borovits, Nemania and Kumara, Indika and Krishnan, Parvathy and Palma, Stefano Dalla and Di Nucci, Dario and Palomba, Fabio and Tamburri, Damian A. and van den Heuvel, Willem-Jan},
title = {DeepIaC: deep learning-based linguistic anti-pattern detection in IaC},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423564},
doi = {10.1145/3416505.3423564},
abstract = {Linguistic anti-patterns are recurring poor practices concerning inconsistencies among the naming, documentation, and implementation of an entity. They impede readability, understandability, and maintainability of source code. This paper attempts to detect linguistic anti-patterns in infrastructure as code (IaC) scripts used to provision and manage computing environments. In particular, we consider inconsistencies between the logic/body of IaC code units and their names. To this end, we propose a novel automated approach that employs word embeddings and deep learning techniques. We build and use the abstract syntax tree of IaC code units to create their code embedments. Our experiments with a dataset systematically extracted from open source repositories show that our approach yields an accuracy between 0.785 and 0.915 in detecting inconsistencies.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {Code Embedding, Deep Learning, Defects, IaC, Infrastructure Code, Linguistic Anti-patterns, Word2Vec},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@inproceedings{10.5555/800164.805199,
author = {Beged-Dov, Aharon Gavriel},
title = {Production control system for a product line with diverse demand patterns},
year = {1969},
publisher = {Winter Simulation Conference},
abstract = {This paper describes a production control plan which may be used in a shop whose product line consists of numerous items which vary extensively in their demand pattern.The plan is capable of (a) Stabilizing production by allowing inventories to fluctuate with moderate demand fluctuations; (b) Minimizing inventories of finished goods while providing protection again serious stockout; (c) Detecting significant changes in demand patterns and revising production schedules accordingly; (d) Scheduling production economically.Employing several well-known concepts, in particular the control limits principle used in quality control, the plan considers simultaneously forecasted demand, forecast error, inventory position, and shop capacity to arrive at production decisions. For this purpose, control limits are applied to both demand and inventory, and production is stabilized with a simple production smoothing procedure. Simulation results, based on data taken from a medium size production shop, demonstrate the plan features and effectiveness.Overall, the paper emphasizes operational principles rather than theoretical concepts. However, the development clearly shows that the plan is amenable to generalized treatment.},
booktitle = {Proceedings of the Third Conference on Applications of Simulation},
pages = {86–99},
numpages = {14},
location = {Los Angeles, California, USA}
}

@article{10.1007/s10270-009-0127-2,
author = {Höfner, Peter and Khedri, Ridha and Möller, Bernhard},
title = {An algebra of product families},
year = {2011},
issue_date = {May       2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-009-0127-2},
doi = {10.1007/s10270-009-0127-2},
abstract = {Experience from recent years has shown that it is often advantageous not to build a single product but rather a family of similar products that share at least one common functionality while having well-identified variabilities. Such product families are built from elementary features that reach from hardware parts to software artefacts such as requirements, architectural elements or patterns, components, middleware, or code. We use the well established mathematical structure of idempotent semirings as the basis for a product family algebra that allows a formal treatment of the above notions. A particular application of the algebra concerns the multi-view reconciliation problem that arises when complex systems are modelled. We use algebraic integration constraints linking features in one view to features in the same or a different view and show in several examples the suitability of this approach for a wide class of integration constraint formulations. Our approach is illustrated with a Haskell prototype implementation of one particular model of product family algebra.},
journal = {Softw. Syst. Model.},
month = may,
pages = {161–182},
numpages = {22},
keywords = {Feature modelling, Formal family specification, Idempotent semiring, Multi-view reconciliation, Product family, Product line}
}

@inproceedings{10.1145/3474085.3481541,
author = {Huang, Lianghua and Liu, Yu and Zhou, Xiangzeng and You, Ansheng and Li, Ming and Wang, Bin and Zhang, Yingya and Pan, Pan and Yinghui, Xu},
title = {Once and for All: Self-supervised Multi-modal Co-training on One-billion Videos at Alibaba},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481541},
doi = {10.1145/3474085.3481541},
abstract = {Videos grow to be one of the largest mediums on the Internet. E-commerce platforms like Alibaba need to process millions of video data across multimedia (e.g., visual, audio, image, and text) and on a variety of tasks (e.g., retrieval, tagging, and summary) every day. In this work, we aim to develop a once and for all pretraining technique for diverse modalities and downstream tasks. To achieve this, we make the following contributions: (1) We propose a self-supervised multi-modal co-training framework. It takes cross-modal pseudo-label consistency as the supervision and can jointly learn representations of multiple modalities. (2) We introduce several novel techniques (e.g., sliding-window subset sampling, coarse-to-fine clustering, fast spatial-temporal convolution and parallel data transmission and processing) to optimize the training process, making billion-scale stable training feasible. (3) We construct a large-scale multi-modal dataset consisting of 1.4 billion videos (~0.5 PB) and train our framework on it. The training takes only 4.6 days on an in-house 256 GPUs cluster, and it simultaneously produces pretrained video, audio, image, motion, and text networks. (4) Finetuning from our pretrained models, we obtain significant performance gains and faster convergence on diverse multimedia tasks at Alibaba. Furthermore, we also validate the learned representation on public datasets. Despite the domain gap between our commodity-centric pretraining and the action-centric evaluation data, we show superior results against state-of-the-arts.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1148–1156},
numpages = {9},
keywords = {co-training, self-supervised learning, once and for all, multi-modal},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.5555/3216238.3216241,
author = {Hu, Ming and Li, Xi and Shi, Mengze},
title = {Product and Pricing Decisions in Crowdfunding},
year = {2015},
issue_date = {May 2015},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {34},
number = {3},
issn = {1526-548X},
abstract = {This paper studies the optimal product and pricing decisions in a crowdfunding mechanism by which a project between a creator and many buyers will be realized only if the total funds committed by the buyers reach a specified goal. When the buyers are sufficiently heterogeneous in their product valuations, the creator should offer a line of products with different levels of product quality. Compared to the traditional situation where orders are placed and fulfilled individually, with the crowdfunding mechanism, a product line is more likely than a single product to be optimal and the quality gap between products is smaller. This paper also shows the effect of the crowdfunding mechanism on pricing dynamics over time. Together, these results underscore the substantial influence of the emerging crowdfunding mechanisms on common marketing decisions.},
journal = {Marketing Science},
month = may,
pages = {331–345},
numpages = {15},
keywords = {product line design, price discrimination, crowdfunding}
}

@inproceedings{10.5555/1927229.1927288,
author = {Daramola, Olawande J.},
title = {A process framework for semantics-aware tourism information systems},
year = {2010},
isbn = {3642169848},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The growing sophistication of user requirements in tourism due to the advent of new technologies such as the Semantic Web and mobile computing has imposed new possibilities for improved intelligence in Tourism Information Systems (TIS). Traditional software engineering and web engineering approaches cannot suffice, hence the need to find new product development approaches that would sufficiently enable the next generation of TIS. The next generation of TIS are expected among other things to: enable semantics-based information processing, exhibit natural language capabilities, facilitate inter-organization exchange of information in a seamless way, and evolve proactively in tandem with dynamic user requirements. In this paper, a product development approach called Product Line for Ontology-based Semantics-Aware Tourism Information Systems (PLOSATIS) which is a novel hybridization of software product line engineering, and Semantic Web engineering concepts is proposed. PLOSATIS is presented as potentially effective, predictable and amenable to software process improvement initiatives.},
booktitle = {Proceedings of the 10th International Conference on Current Trends in Web Engineering},
pages = {521–532},
numpages = {12},
keywords = {web engineering, tourism information system, software product development, software process improvement, semantic web, product line},
location = {Vienna, Austria},
series = {ICWE'10}
}

@inproceedings{10.1145/3468264.3468603,
author = {Ding, Yi and Pervaiz, Ahsan and Carbin, Michael and Hoffmann, Henry},
title = {Generalizable and interpretable learning for configuration extrapolation},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468603},
doi = {10.1145/3468264.3468603},
abstract = {Modern software applications are increasingly configurable, which puts a burden on users to tune these configurations for their target hardware and workloads. To help users, machine learning techniques can model the complex relationships between software configuration parameters and performance. While powerful, these learners have two major drawbacks: (1) they rarely incorporate prior knowledge and (2) they produce outputs that are not interpretable by users. These limitations make it difficult to (1) leverage information a user has already collected (e.g., tuning for new hardware using the best configurations from old hardware) and (2) gain insights into the learner’s behavior (e.g., understanding why the learner chose different configurations on different hardware or for different workloads). To address these issues, this paper presents two configuration optimization tools, GIL and GIL+, using the proposed generalizable and interpretable learning approaches. To incorporate prior knowledge, the proposed tools (1) start from known configurations, (2) iteratively construct a new linear model, (3) extrapolate better performance configurations from that model, and (4) repeat. Since the base learners are linear models, these tools are inherently interpretable. We enhance this property with a graphical representation of how they arrived at the highest performance configuration. We evaluate GIL and GIL+ by using them to configure Apache Spark workloads on different hardware platforms and find that, compared to prior work, GIL and GIL+ produce comparable, and sometimes even better performance configurations, but with interpretable results.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {728–740},
numpages = {13},
keywords = {machine learning, interpretability, generalizability, Configuration},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1016/j.sigpro.2019.107332,
author = {Shi, Caijuan and Gu, Zhibin and Duan, Changyu and Tian, Qi},
title = {Multi-view adaptive semi-supervised feature selection with the self-paced learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {168},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2019.107332},
doi = {10.1016/j.sigpro.2019.107332},
journal = {Signal Process.},
month = mar,
numpages = {11},
keywords = {Multi-view learning, Semi-supervised feature selection, Self-paced learning, Graph-based semi-supervised learning}
}

@inproceedings{10.1145/224019.224036,
author = {Grinter, Rebecca E.},
title = {Using a configuration management tool to coordinate software development},
year = {1995},
isbn = {0897917065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/224019.224036},
doi = {10.1145/224019.224036},
abstract = {I describe a naturalistic study of one organization's use of a configuration management tool to coordinate the development of a software product. In this organization, the developers use the tool routinely to reduce the complexities of coordinating their development efforts. I examine how the tool provides mechanisms of interaction that let the developers work with each other. I identify four aspects of these mechanisms: difficulties of representing work, the multiple levels that they operate at, the possibilities for coordination they provide, and their role in supporting a model of work.},
booktitle = {Proceedings of Conference on Organizational Computing Systems},
pages = {168–177},
numpages = {10},
location = {Milpitas, California, USA},
series = {COCS '95}
}

@inproceedings{10.1145/318372.318563,
author = {Utt, Mary Hunter and Mathews, Robert},
title = {Developing a user information architecture for Rational's ClearCase product family documentation set},
year = {1999},
isbn = {1581130724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/318372.318563},
doi = {10.1145/318372.318563},
abstract = {Information architecture, like information development and delivery, has much in common with its software counterpart. This paper describes how the Rational ClearCase® documentation group developed an information architecture to meet changing industry, corporate, and product requirements. During this work, it became clear that our architecture development process mapped closely to the Rational Unified Process, an iterative and incremental approach to software architecture and development. The common features are noted and extensions are proposed to enhance both user-information processes and the Rational Unified Process.},
booktitle = {Proceedings of the 17th Annual International Conference on Computer Documentation},
pages = {86–92},
numpages = {7},
keywords = {information architecture, Rational Unified Process, RUP, ClearCase documentation},
location = {New Orleans, Louisiana, USA},
series = {SIGDOC '99}
}

@article{10.1007/s00034-021-01657-1,
author = {Pravin, Sheena Christabel and Palanivelan, M.},
title = {A Hybrid Deep Ensemble for Speech Disfluency Classification},
year = {2021},
issue_date = {Aug 2021},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {40},
number = {8},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-021-01657-1},
doi = {10.1007/s00034-021-01657-1},
abstract = {In this paper, a novel Hybrid Deep Ensemble (HDE) is proposed for automatic speech disfluency classification on a sparse speech dataset. Categorizations of speech disfluencies for diagnosis of speech disorders have so long relied on sophisticated deep learning models. Such a task can be accomplished by a straightforward approach with high accuracy by the proposed model which is an optimal combination of diverse machine learning and deep learning algorithms in a hierarchical arrangement which includes a deep autoencoder that yields the compressed latent features. The proposed model has shown considerable improvement in downgrading processing time overcoming the issues of cumbersome hyper-parameter tuning and huge data demand of the deep learning algorithms with high classification accuracy. Experimental results show that the proposed Hybrid Deep Ensemble has superior performance compared to the individual base learners, and the deep neural network as well. The proposed model and the baseline models were evaluated in terms of Cohen’s kappa coefficient, Hamming loss, Jaccard score, F-score and classification accuracy.},
journal = {Circuits Syst. Signal Process.},
month = aug,
pages = {3968–3995},
numpages = {28},
keywords = {Latent features, Deep autoencoder, Sparse speech dataset, Speech disfluency classification, Hybrid Deep Ensemble}
}

@inproceedings{10.1145/3375998.3376006,
author = {Song, Xiaoming and Fan, Bo and Song, Kexing and Huo, Hua and Hou, Wenwu},
title = {Complex Process Module Partitioning Method For Mass Customization},
year = {2020},
isbn = {9781450377027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375998.3376006},
doi = {10.1145/3375998.3376006},
abstract = {Aiming at the problem of module planning in product family design of mass customization, the limitations of module partitioning of product components in the past are analyzed, and the method for module division at complex processing levels is proposed. Based on the modular theory of the product family core system, the design relationship between product performance and process is comprehensively analyzed, the correlation matrix between process modules is established according to the feature-associated mathematical model, the correlation function is calibrated by the range conversion method, and the membership function is calculated by the transitive closure method, combine the principle of threshold cutting to obtain the best module division scheme through matlab operation., program to achieve a fast, low-cost design of personalized products that adapt to market and technology changes. Finally, taking the aluminum/copper strip process module as an example, the effectiveness and feasibility of the method are verified, the production efficiency of the product is improved, and the processing cost of the enterprise is reduced.},
booktitle = {Proceedings of the 2019 8th International Conference on Networks, Communication and Computing},
pages = {1–5},
numpages = {5},
keywords = {Product family, Process module, Mass customization, Fuzzy cluster},
location = {Luoyang, China},
series = {ICNCC '19}
}

@inproceedings{10.1145/1621607.1621633,
author = {Sanen, Frans and Truyen, Eddy and Joosen, Wouter},
title = {Mapping problem-space to solution-space features: a feature interaction approach},
year = {2009},
isbn = {9781605584942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1621607.1621633},
doi = {10.1145/1621607.1621633},
abstract = {Mapping problem-space features into solution-space features is a fundamental configuration problem in software product line engineering. A configuration problem is defined as generating the most optimal combination of software features given a requirements specification and given a set of configuration rules. Current approaches however provide little support for expressing complex configuration rules between problem and solution space that support incomplete requirements specifications. In this paper, we propose an approach to model complex configuration rules based on a generalization of the concept of problem-solution feature interactions. These are interactions between solution-space features that only arise in specific problem contexts. The use of an existing tool to support our approach is also discussed: we use the DLV answer set solver to express a particular configuration problem as a logic program whose answer set corresponds to the optimal combinations of solution-space features. We motivate and illustrate our approach with a case study in the field of managing dynamic adaptations in distributed software, where the goal is to generate an optimal protocol for accommodating a given adaptation.},
booktitle = {Proceedings of the Eighth International Conference on Generative Programming and Component Engineering},
pages = {167–176},
numpages = {10},
keywords = {software product line engineering, problem-solution feature interactions, distributed runtime adaptation, default logic, configuration knowledge, DLV},
location = {Denver, Colorado, USA},
series = {GPCE '09}
}

@inproceedings{10.1145/267895.267907,
author = {Joeris, Gregor},
title = {Change management needs integrated process and configuration management},
year = {1997},
isbn = {3540635319},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1145/267895.267907},
doi = {10.1145/267895.267907},
booktitle = {Proceedings of the 6th European SOFTWARE ENGINEERING Conference Held Jointly with the 5th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {125–141},
numpages = {17},
location = {Zurich, Switzerland},
series = {ESEC '97/FSE-5}
}

@article{10.1016/j.infsof.2019.08.007,
author = {Nogueira Teixeira, Eldânae and Aleixo, Fellipe Araújo and Amâncio, Francisco Dione de Sousa and OliveiraJr, Edson and Kulesza, Uirá and Werner, Cláudia},
title = {Software process line as an approach to support software process reuse: A systematic literature review},
year = {2019},
issue_date = {Dec 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {116},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.08.007},
doi = {10.1016/j.infsof.2019.08.007},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {22},
keywords = {Process variability management, Software process line, Process reuse, Software process, Systematic review}
}

@article{10.1016/j.ijar.2007.03.006,
author = {Peterson, Leif E. and Coleman, Matthew A.},
title = {Machine learning-based receiver operating characteristic (ROC) curves for crisp and fuzzy classification of DNA microarrays in cancer research},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {47},
number = {1},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2007.03.006},
doi = {10.1016/j.ijar.2007.03.006},
abstract = {Receiver operating characteristic (ROC) curves were generated to obtain classification area under the curve (AUC) as a function of feature standardization, fuzzification, and sample size from nine large sets of cancer-related DNA microarrays. Classifiers used included k-nearest neighbor (kNN), naive Bayes classifier (NBC), linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), learning vector quantization (LVQ1), logistic regression (LOG), polytomous logistic regression (PLOG), artificial neural networks (ANN), particle swarm optimization (PSO), constricted particle swarm optimization (CPSO), kernel regression (RBF), radial basis function networks (RBFN), gradient descent support vector machines (SVMGD), and least squares support vector machines (SVMLS). For each data set, AUC was determined for a number of combinations of sample size, total sum[-log(p)] of feature t-tests, with and without feature standardization and with (fuzzy) and without (crisp) fuzzification of features. Altogether, a total of 2,123,530 classification runs were made. At the greatest level of sample size, ANN resulted in a fitted AUC of 90%, while PSO resulted in the lowest fitted AUC of 72.1%. AUC values derived from 4NN were the most dependent on sample size, while PSO was the least. ANN depended the most on total statistical significance of features used based on sum[-log(p)], whereas PSO was the least dependent. Standardization of features increased AUC by 8.1% for PSO and -0.2% for QDA, while fuzzification increased AUC by 9.4% for PSO and reduced AUC by 3.8% for QDA. AUC determination in planned microarray experiments without standardization and fuzzification of features will benefit the most if CPSO is used for lower levels of feature significance (i.e., sum[-log(p)]~50) and ANN is used for greater levels of significance (i.e., sum[-log(p)]~500). When only standardization of features is performed, studies are likely to benefit most by using CPSO for low levels of feature statistical significance and LVQ1 for greater levels of significance. Studies involving only fuzzification of features should employ LVQ1 because of the substantial gain in AUC observed and low expense of LVQ1. Lastly, PSO resulted in significantly greater levels of AUC (89.5% average) when feature standardization and fuzzification were performed. In consideration of the data sets used and factors influencing AUC which were investigated, if low-expense computation is desired then LVQ1 is recommended. However, if computational expense is of less concern, then PSO or CPSO is recommended.},
journal = {Int. J. Approx. Reasoning},
month = jan,
pages = {17–36},
numpages = {20},
keywords = {Soft computing, Receiver operator characteristic (ROC) curve, Gene expression, Fuzzy classification, DNA microarrays, Area under the curve (AUC)}
}

@article{10.1016/j.eswa.2021.115218,
author = {Serrano-Pérez, Jonathan and Sucar, L. Enrique},
title = {Artificial datasets for hierarchical classification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115218},
doi = {10.1016/j.eswa.2021.115218},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {13},
keywords = {Evaluation, Hierarchical classification, Artificial datasets}
}

@inproceedings{10.1145/222124.222151,
author = {Zeller, Andreas},
title = {A unified version model for configuration management},
year = {1995},
isbn = {0897917162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/222124.222151},
doi = {10.1145/222124.222151},
booktitle = {Proceedings of the 3rd ACM SIGSOFT Symposium on Foundations of Software Engineering},
pages = {151–160},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGSOFT '95}
}

@article{10.1007/s00500-021-05766-6,
author = {Agudelo, Oscar Esneider Acosta and Marín, Carlos Enrique Montenegro and Crespo, Rubén González},
title = {Sound measurement and automatic vehicle classification and counting applied to road traffic noise characterization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {18},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05766-6},
doi = {10.1007/s00500-021-05766-6},
abstract = {Increase in population density in large cities has increased the environmental noise present in these environments, causing negative effects on human health. There are different sources of environmental noise; however, noise from road traffic is the most prevalent in cities. Therefore, it is necessary to have tools that allow noise characterization to establish strategies that permit obtaining levels that do not affect the quality of life of people. This research discusses the implementation of a system that allows the acquisition of data to characterize the noise generated by road traffic. First, the methodology for obtaining acoustic indicators with an electret measurement microphone is described, so that it adjusts to the data collection needs for road traffic noise analyses. Then, an approach for the classification and counting of automatic vehicular traffic through deep learning is presented. Results showed that there were differences of 0.2 dBA in terms of RMSE between a type 1 sound level meter and the measurement microphone used. With reference to vehicle classification and counting for four categories, the approximate error is between 3.3% and -15.5%.},
journal = {Soft Comput.},
month = sep,
pages = {12075–12087},
numpages = {13},
keywords = {Deep learning, Classification, Vehicle, Road traffic, Environmental noise}
}

@inproceedings{10.1145/3167132.3167351,
author = {Krüger, Jacob and Ludwig, Kai and Zimmermann, Bernhard and Leich, Thomas},
title = {Physical separation of features: a survey with CPP developers},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167351},
doi = {10.1145/3167132.3167351},
abstract = {Several implementation techniques for software product lines have emerged over time. A common distinction of these techniques is whether features are annotated in the code base (virtually separated) or composed from modules (physically separated). While each approach promises different pros and cons, mainly annotations and especially the C PreProcessor (CPP) are established in practice. Thus, the question arises, which barriers prevent the adoption of composition-based approaches. In this paper, we report an empirical study among C and C++ developers in which we investigate this issue. Therefore, we ask our participants to describe how they use the CPP and how they assess the idea of moving annotated code into modules. More precisely, we use small examples based on our Feature Compositional PreProcessor (FeatureCoPP) that enables this separation while keeping annotations---avoiding divergences from the preprocessor concept. Overall, we identify different characteristics that indicate when physical separation can be useful. While most responses are skeptical towards the approach, they also emphasize its usability for source code analysis and for implementing specific use cases.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2042–2049},
numpages = {8},
keywords = {separation of concerns, product line, empirical study},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/2602576.2602590,
author = {Olsson, Tobias and Toll, Daniel and Wingkvist, Anna and Ericsson, Morgan},
title = {Evaluation of a static architectural conformance checking method in a line of computer games},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602590},
doi = {10.1145/2602576.2602590},
abstract = {We present an evaluation of a simple method to find architectural problems in a product line of computer games. The method uses dependencies (direct, indirect, or no) to automatically classify types in the implementation to high-level components in the product line architecture. We use a commercially available tool to analyse dependencies in the source code. The automatic classification of types is compared to a manual classification by the developer, and all mismatches are reported. To evaluate the method, we inspect the source code and look for a pre-defined set of architectural problems in all types. We compare the set of types that contained problems to the set of types where the manual and automatic classification disagreed to determine precision and recall. We also investigate what changes are needed to correct the found mismatches by either designing and implementing changes in the source code or refining the automatic classification. Our evaluation shows that the simple method is effective at detecting architectural problems in a product line of four games. The method is lightweight, customisable and easy to implement early in the development cycle.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {113–118},
numpages = {6},
keywords = {static conformance checking, product line architecture, model-view-controller, computer game, MVC},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@article{10.1016/j.infsof.2013.02.007,
author = {Santos Rocha, Roberto dos and Fantinato, Marcelo},
title = {The use of software product lines for business process management},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.02.007},
doi = {10.1016/j.infsof.2013.02.007},
abstract = {ContextBusiness Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT. ObjectivePresenting the results of a study undertaken to seek and assess PL approaches for BPM through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic PL approaches for BPM. MethodA SLR was conducted with four research questions formulated to evaluate PL approaches for BPM. Results63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that PLs only partially address the BPM lifecycle since the last business process phase is not a current concern on the found approaches. ConclusionsThe found PL approaches for BPM only cover partially the BPM lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic PL proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that PL approaches for BPM are still at an early stage and gaining maturity.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1355–1373},
numpages = {19},
keywords = {Software product line, PL, Business process management, BPM}
}

@inproceedings{10.1007/978-3-030-65310-1_20,
author = {Metzger, Andreas and Quinton, Clément and Mann, Zoltán Ádám and Baresi, Luciano and Pohl, Klaus},
title = {Feature Model-Guided Online Reinforcement Learning for Self-Adaptive Services},
year = {2020},
isbn = {978-3-030-65309-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65310-1_20},
doi = {10.1007/978-3-030-65310-1_20},
abstract = {A self-adaptive service can maintain its QoS requirements in the presence of dynamic environment changes. To develop a self-adaptive service, service engineers have to create self-adaptation logic encoding when the service should execute which adaptation actions. However, developing self-adaptation logic may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. Online reinforcement learning addresses design time uncertainty by learning suitable adaptation actions through interactions with the environment at runtime. To learn more about its environment, reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens has an impact on the performance of the learning process. We focus on two problems related to how a service’s adaptation actions are explored: (1) Existing solutions randomly explore adaptation actions and thus may exhibit slow learning if there are many possible adaptation actions to choose from. (2) Existing solutions are unaware of service evolution, and thus may explore new adaptation actions introduced during such evolution rather late. We propose novel exploration strategies that use feature models (from software product line engineering) to guide exploration in the presence of many adaptation actions and in the presence of service evolution. Experimental results for a self-adaptive cloud management service indicate an average speed-up of the learning process of 58.8% in the presence of many adaptation actions, and of 61.3% in the presence of service evolution. The improved learning performance in turn led to an average QoS improvement of 7.8% and 23.7% respectively
.},
booktitle = {Service-Oriented Computing: 18th International Conference, ICSOC 2020, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {269–286},
numpages = {18},
keywords = {Cloud service, Feature model, Reinforcement learning, Adaptation},
location = {Dubai, United Arab Emirates}
}

@article{10.5555/2946645.3053434,
author = {Szabó, Zoltán and Sriperumbudur, Bharath K. and Póczos, Barnabás and Gretton, Arthur},
title = {Learning theory for distribution regression},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax optimal rate (Caponnetto and De Vito, 2007; Steinwart et al., 2009). This result answers a 17-year-old open question, establishing the consistency of the classical set kernel (Haussler, 1999; Gärtner et al., 2002) in regression. We also cover consistency for more recent kernels on distributions, including those due to Christmann and Steinwart (2010).},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5272–5311},
numpages = {40},
keywords = {two-Stage sampled distribution regression, multi-instance learning, minimax optimality, mean embedding, Kernel ridge regression}
}

@inproceedings{10.1145/3321707.3321755,
author = {Lee, Nian-Ze and Arcaini, Paolo and Ali, Shaukat and Ishikawa, Fuyuki},
title = {Stability analysis for safety of automotive multi-product lines: a search-based approach},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321755},
doi = {10.1145/3321707.3321755},
abstract = {Safety assurance for automotive products is crucial and challenging. It becomes even more difficult when the variability in automotive products is considered. Recently, the notion of automotive multi-product lines (multi-PL) is proposed as a unified framework to accommodate different sources of variability in automotive products. In the context of automotive multi-PL, we propose a stability analysis for safety, motivated by our industrial collaboration, where we observed that under certain operation scenarios, safety varies drastically with small fluctuations in production parameters, environmental conditions, or driving inputs. To characterize instability, we formulate a multi-objective optimization problem, and solve it with a search-based approach. The proposed technique is applied to an industrial automotive multi-PL, and experimental results show its effectiveness to spot instability. Moreover, based on information gathered during the search, we provide some insights on both testing and quality engineering of automotive products.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1241–1249},
numpages = {9},
keywords = {stability analysis, simulink, search, safety, robustness, product line, automotive domain},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@article{10.1007/s10664-020-09911-x,
author = {Ramos-Gutiérrez, Belén and Varela-Vaca, Ángel Jesús and Galindo, José A. and Gómez-López, María Teresa and Benavides, David},
title = {Discovering configuration workflows from existing logs using process mining},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09911-x},
doi = {10.1007/s10664-020-09911-x},
abstract = {Variability models are used to build configurators, for guiding users through the configuration process to reach the desired setting that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the design options that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suits stakeholders according to previous configurations. For example, when configuring a Linux distribution the configuration process starts by choosing the network or the graphic card and then, other packages concerning a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), a framework that can automatically assist determining the configuration workflow that better fits the configuration logs generated by user activities given a set of logs of previous configurations and a variability model. COLOSSI is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Derived from the possible complexity of both logs and the discovered processes, often, it is necessary to divide the traces into small ones. This provides an easier configuration workflow to be understood and followed by the user during the configuration process. In this paper, we apply and compare four different techniques for the traces clustering: greedy, backtracking, genetic and hierarchical algorithms. Our proposal is validated in three different scenarios, to show its feasibility, an ERP configuration, a Smart Farming, and a Computer Configuration. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering along with the necessity to apply clustering techniques for the trace preparation in the context of configuration workflows.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {41},
keywords = {Clustering, Process discovery, Process mining, Configuration workflow, Variability}
}

@article{10.5555/3455716.3455938,
author = {Weinshall, Daphna and Amir, Dan},
title = {Theory of curriculum learning, with convex loss functions},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Curriculum Learning is motivated by human cognition, where teaching often involves gradually exposing the learner to examples in a meaningful order, from easy to hard. Although methods based on this concept have been empirically shown to improve performance of several machine learning algorithms, no theoretical analysis has been provided even for simple cases. To address this shortfall, we start by formulating an ideal definition of difficulty score - the loss of the optimal hypothesis at a given datapoint. We analyze the possible contribution of curriculum learning based on this score in two convex problems - linear regression, and binary classification by hinge loss minimization. We show that in both cases, the convergence rate of SGD optimization decreases monotonically with the difficulty score, in accordance with earlier empirical results. We also prove that when the difficulty score is fixed, the convergence rate of SGD optimization is monotonically increasing with respect to the loss of the current hypothesis at each point. We discuss how these results settle some confusion in the literature where two apparently opposing heuristics are reported to improve performance: curriculum learning in which easier points are given priority, vs hard data mining where the more difficult points are sought out.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {222},
numpages = {19},
keywords = {hinge loss minimization, linear regression, curriculum learning}
}

@article{10.5555/3322706.3361988,
author = {Zhou, Zhixin and Amini, Arash A.},
title = {Analysis of spectral clustering algorithms for community detection: the general bipartite setting},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a k-means type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1774–1820},
numpages = {47},
keywords = {sub-Gaussian biclustering, stochastic block model, spectral clustering, regularization of random graphs, graphon clustering, community detection, bipartite networks}
}

@article{10.1016/j.infsof.2009.03.005,
author = {Reinhartz-Berger, Iris and Sturm, Arnon},
title = {Utilizing domain models for application design and validation},
year = {2009},
issue_date = {August, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.03.005},
doi = {10.1016/j.infsof.2009.03.005},
abstract = {Domain analysis enables identifying families of applications and capturing their terminology in order to assist and guide system developers to design valid applications in the domain. One major way of carrying out the domain analysis is modeling. Several studies suggest using metamodeling techniques, feature-oriented approaches, or architectural-based methods for modeling domains and specifying applications in those domains. However, these methods mainly focus on representing the domain knowledge, providing insufficient guidelines (if any) for creating application models that satisfy the domain rules and constraints. In particular, validation of the application models which include application-specific knowledge is insufficiently dealt. In order to fill these lacks, we propose a general approach, called Application-based DOmain Modeling (ADOM), which enables specifying domains and applications similarly, (re)using domain knowledge in application models, and validating the application models against the relevant domain models. In this paper we present the ADOM approach, demonstrating its application to UML 2.0 class and sequence diagrams.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1275–1289},
numpages = {15},
keywords = {Variability management, Software product line engineering, Metamodeling, Feature oriented, Domain engineering, Domain analysis}
}

@inproceedings{10.1145/3468264.3468531,
author = {Bittner, Paul Maximilian and Schultheiß, Alexander and Thüm, Thomas and Kehrer, Timo and Young, Jeffrey M. and Linsbauer, Lukas},
title = {Feature trace recording},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468531},
doi = {10.1145/3468264.3468531},
abstract = {Tracing requirements to their implementation is crucial to all stakeholders of a software development process. When managing software variability, requirements are typically expressed in terms of features, a feature being a user-visible characteristic of the software. While feature traces are fully documented in software product lines, ad-hoc branching and forking, known as clone-and-own, is still the dominant way for developing multi-variant software systems in practice. Retroactive migration to product lines suffers from uncertainties and high effort because knowledge of feature traces must be recovered but is scattered across teams or even lost. We propose a semi-automated methodology for recording feature traces proactively, during software development when the necessary knowledge is present. To support the ongoing development of previously unmanaged clone-and-own projects, we explicitly deal with the absence of domain knowledge for both existing and new source code. We evaluate feature trace recording by replaying code edit patterns from the history of two real-world product lines. Our results show that feature trace recording reduces the manual effort to specify traces. Recorded feature traces could improve automation in change-propagation among cloned system variants and could reduce effort if developers decide to migrate to a product line.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1007–1020},
numpages = {14},
keywords = {software product lines, feature traceability, feature location, disciplined annotations, clone-and-own},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1016/j.neucom.2018.04.075,
author = {Xu, Wei and Liu, Wei and Huang, Xiaolin and Yang, Jie and Qiu, Song},
title = {Multi-modal self-paced learning for image classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {309},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.075},
doi = {10.1016/j.neucom.2018.04.075},
journal = {Neurocomput.},
month = oct,
pages = {134–144},
numpages = {11},
keywords = {Multi-modal, Self-paced learning, Curriculum learning, Image classification}
}

@article{10.1016/j.jss.2021.111027,
author = {Jiang, Zijian and Zhong, Hao and Meng, Na},
title = {Investigating and recommending co-changed entities for JavaScript programs},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111027},
doi = {10.1016/j.jss.2021.111027},
journal = {J. Syst. Softw.},
month = oct,
numpages = {19},
keywords = {JavaScript, Machine learning, Change suggestion, Multi-entity edit}
}

@article{10.1016/j.eswa.2015.02.020,
author = {Dermeval, Diego and Tenório, Thyago and Bittencourt, Ig Ibert and Silva, Alan and Isotani, Seiji and Ribeiro, Márcio},
title = {Ontology-based feature modeling},
year = {2015},
issue_date = {July 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {11},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.02.020},
doi = {10.1016/j.eswa.2015.02.020},
abstract = {We compare two ontology-based feature modeling styles by conducting an experiment.The results show that ontology factor has statistical significance in all metrics.The results show that the ontology based on instances is more flexible.The results show that the ontology based on instances demands less time to change. A software product line (SPL) is a set of software systems that have a particular set of common features and that satisfy the needs of a particular market segment or mission. Feature modeling is one of the key activities involved in the design of SPLs. The feature diagram produced in this activity captures the commonalities and variabilities of SPLs. In some complex domains (e.g., ubiquitous computing, autonomic systems and context-aware computing), it is difficult to foresee all functionalities and variabilities a specific SPL may require. Thus, Dynamic Software Product Lines (DSPLs) bind variation points at runtime to adapt to fluctuations in user needs as well as to adapt to changes in the environment. In this context, relying on formal representations of feature models is important to allow them to be automatically analyzed during system execution. Among the mechanisms used for representing and analyzing feature models, description logic (DL) based approaches demand to be better investigated in DSPLs since it provides capabilities, such as automated inconsistency detection, reasoning efficiency, scalability and expressivity. Ontology is the most common way to represent feature models knowledge based on DL reasoners. Previous works conceived ontologies for feature modeling either based on OWL classes and properties or based on OWL individuals. However, considering change or evolution scenarios of feature models, we need to compare whether a class-based or an individual-based feature modeling style is recommended to describe feature models to support SPLs, and especially its capabilities to deal with changes in feature models, as required by DSPLs. In this paper, we conduct a controlled experiment to empirically compare two approaches based on each one of these modeling styles in several changing scenarios (e.g., add/remove mandatory feature, add/remove optional feature and so on). We measure time to perform changes, structural impact of changes (flexibility) and correctness for performing changes in our experiment. Our results indicate that using OWL individuals requires less time to change and is more flexible than using OWL classes and properties. These results provide insightful assumptions towards the definition of an approach relying on reasoning capabilities of ontologies that can effectively support products reconfiguration in the context of DSPL.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {4950–4964},
numpages = {15},
keywords = {Software product line, Ontology, Feature modeling, Empirical software engineering}
}

@inproceedings{10.5555/1885639.1885667,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Gasevic, Dragan and Soltani, Samaneh},
title = {Stratified analytic hierarchy process: prioritization and selection of software features},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product line engineering allows for the rapid development of variants of a domain specific application by using a common set of reusable assets often known as core assets. Variability modeling is a critical issue in product line engineering, where the use of feature modeling is one of most commonly used formalisms. To support an effective and automated derivation of concrete products for a product family, staged configuration has been proposed in the research literature. In this paper, we propose the integration of well-known requirements engineering principles into stage configuration. Being inspired by the well-established Preview requirements engineering framework, we initially propose an extension of feature models with capabilities for capturing business oriented requirements. This representation enables a more effective capturing of stakeholders' preferences over the business requirements and objectives (e.g.,. implementation costs or security) in the form of fuzzy linguistic variables (e.g., high, medium, and low). On top of this extension, we propose a novel method, the Stratified Analytic Hierarchy process, which first helps to rank and select the most relevant high level business objectives for the target stakeholders (e.g., security over implementation costs), and then helps to rank and select the most relevant features from the feature model to be used as the starting point in the staged configuration process. Besides a complete formalization of the process, we define the place of our proposal in existing software product line lifecycles as well as demonstrate the use of our proposal on the widely-used e-Shop case study. Finally, we report on the results of our user study, which indicates a high appreciation of the proposed method by the participating industrial software developers. The tool support for S-AHP is also introduced.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {300–315},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Web System, Software Product Line, Machine Learning, Energy Aware},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.1109/ASE.2011.6100070,
author = {Chen, Ning and Hoi, Steven C. H. and Xiao, Xiaokui},
title = {Software process evaluation: A machine learning approach},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100070},
doi = {10.1109/ASE.2011.6100070},
abstract = {Software process evaluation is essential to improve software development and the quality of software products in an organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In particular, we formulate the problem as a sequence classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to objectively evaluate the quality and performance of a software process. To validate the efficacy of our approach, we apply it to evaluate the defect management process performed in four real industrial software projects. Our empirical results show that our approach is effective and promising in providing an objective and quantitative measurement for software process evaluation.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {333–342},
numpages = {10},
series = {ASE '11}
}

@inproceedings{10.1007/978-3-030-86380-7_23,
author = {Krysińska, Izabela and Morzy, Mikołaj and Kajdanowicz, Tomasz},
title = {Curriculum Learning Revisited: Incremental Batch Learning with Instance Typicality Ranking},
year = {2021},
isbn = {978-3-030-86379-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86380-7_23},
doi = {10.1007/978-3-030-86380-7_23},
abstract = {The technique of curriculum learning mimics cognitive mechanisms observed in human learning, where simpler concepts are presented prior to gradual introduction of more difficult concepts. Until now, the major obstacle for curriculum methods was the lack of a reliable method for estimating the difficulty of training instances. In this paper we show that, instead of trying to assess the difficulty of learning instances, a simple graph-based method of computing the typicality of instances can be used in conjunction with curriculum methods. We design new batch schedulers which organize ordered instances into batches of varying size and learning difficulty. Our method does not require any changes to the architecture of trained models, we improve the training merely by manipulating the order and frequency of instance presentation to the model.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part IV},
pages = {279–291},
numpages = {13},
keywords = {Batch training, Typicality, Curriculum learning},
location = {Bratislava, Slovakia}
}

@phdthesis{10.5555/AAI28544034,
author = {Khoshmanesh, Seyedehzahra and Samik, Basu, and Andrew, Miner, and Hridesh, Rajan, and Karin, Dorman,},
advisor = {R, Lutz, Robyn},
title = {Learning Feature Interactions with and without Specifications},
year = {2021},
isbn = {9798544278207},
publisher = {Iowa State University},
address = {USA},
abstract = {Developers of software product lines and highly configurable systems reuse and combine features (units of functionality) to build new or customize existing products. However, features can interact in ways that are contrary to developers' intent. Predicting whether a new combination of features will produce an unwanted or even hazardous feature interaction is a continuing challenge. Current techniques to detect unwanted feature interactions are costly, slow, and inadequate. In this thesis, we investigate how to detect unwanted feature interactions early in development and that are scalable to large software product lines or highly configurable systems. First, we propose a similarity-based method to identify unwanted feature interactions much earlier in the development process for early detection. It uses knowledge of prior feature interactions stored with the software product line's feature model to help find unwanted interactions between a new feature and existing features. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.Moreover, to learn and automate the detection, we show how detecting unwanted feature interactions can be effectively represented as a link prediction problem. We investigate six link-based similarity metrics and evaluate our approach on a software product line benchmark. Results show that the best machine learning algorithms achieve an accuracy of 0.75 to 1 for classifying feature interactions.Finally, we develop a new approach based on program analysis that extracts feature-relevant learning models from the source code to obtain more semantic details of unwanted feature interactions. The method is capable of learning feature interactions whether constraints on feature combinations are specified or not. If specifications of feature constraints are unavailable, as is common in real-world systems, our approach infers the constraints using feature-related data-flow dependency information. Experimental evaluation on three software product line benchmarks and a highly configurable system shows that this approach is fast and effective.The contribution is to support developers by automatically detecting those feature combinations in a new product or version that can interact in unwanted or unrecognized ways. This enables a better understanding of hidden interactions and identifies software components that should be tested together because their features interact in some configurations.},
note = {AAI28544034}
}

@inproceedings{10.1007/978-3-030-86960-1_34,
author = {Ribeiro, Diogo and Matos, Luís Miguel and Cortez, Paulo and Moreira, Guilherme and Pilastri, André},
title = {A Comparison of Anomaly Detection Methods for Industrial Screw Tightening},
year = {2021},
isbn = {978-3-030-86959-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86960-1_34},
doi = {10.1007/978-3-030-86960-1_34},
abstract = {Within the context of Industry 4.0, quality assessment procedures using data-driven techniques are becoming more critical due to the generation of massive amounts of production data. In this paper, we address the detection of abnormal screw tightening processes, which is a relevant industrial task. Since labeling is costly, requiring a manual effort, we focus on unsupervised approaches. In particular, we assume a low-dimensional input screw fastening approach that is based only on angle-torque pairs. Using such pairs, we explore three main unsupervised Machine Learning (ML) algorithms: Local Outlier Factor (LOF), Isolation Forest (iForest) and a deep learning Autoencoder (AE). For benchmarking purposes, we also explore a supervised Random Forest (RF) algorithm. Several computational experiments were held by using recent industrial data with 2.8 million angle-torque pair records and a realistic and robust rolling window evaluation. Overall, high quality anomaly discrimination results were achieved by the iForest (99%) and AE (95% and 96%) unsupervised methods, which compared well against the supervised RF (99% and 91%). When compared with iForest, the AE requires less computation effort and provides faster anomaly detection response times.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part II},
pages = {485–500},
numpages = {16},
keywords = {Unsupervised learning, Random Forest, One-class classification, Isolation Forest, Industry 4.0, Deep learning, Autoencoder},
location = {Cagliari, Italy}
}

@inproceedings{10.5555/3524938.3524952,
author = {Ahn, Sungsoo and Seo, Younggyo and Shin, Jinwoo},
title = {Learning what to defer for maximum independent sets},
year = {2020},
publisher = {JMLR.org},
abstract = {Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the number of elements in the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme, coined learning what to defer (LwD), where the agent adaptively shrinks or stretch the number of stages by learning to distribute the element-wise decisions of the solution at each stage. We apply the proposed framework to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {14},
numpages = {11},
series = {ICML'20}
}

@inproceedings{10.1145/2110147.2110161,
author = {Lienhardt, Michael and Clarke, Dave},
title = {Row types for delta-oriented programming},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110161},
doi = {10.1145/2110147.2110161},
abstract = {Delta-oriented programming (DOP) provides a technique for implementing Software Product Lines based on modifications (add, remove, modify) to a core program. Unfortunately, such modifications can introduce errors into a program, especially when type signatures of classes are modified in a non-monotonic fashion. To deal with this problem we present a type system for delta-oriented programs based on row polymorphism. This exercise elucidates the close correspondence between delta-oriented programs and row polymorphism.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {121–128},
numpages = {8},
keywords = {structural typing, software product line engineering, delta-oriented programming},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@article{10.1007/s11042-020-10443-1,
author = {Rao, Champakamala Sundar and Karunakara, K.},
title = {A comprehensive review on brain tumor segmentation and classification of MRI images},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10443-1},
doi = {10.1007/s11042-020-10443-1},
abstract = {In the analysis of medical images, one of the challenging tasks is the recognition of brain tumours via medical resonance images (MRIs). The diagnosis process is still tedious due to its complexity and considerable variety in tissues of tumor perception. Therefore, the necessities of tumor identification techniques are improving nowadays for medical applications. In the past decades, different approaches in the segmentation of various precisions and complexity degree have been accomplished, which depends on the simplicity and the benchmark of the technique. An overview of this analysis is to give out the summary of the semi-automatic techniques for brain tumor segmentation and classification utilizing MRI. An enormous amount of MRI based image data is accomplished using deep learning approaches. There are several works, dealing on the conventional approaches for MRI-based segmentation of brain tumor. Alternatively, in this review, we revealed the latest trends in the methods of deep learning. Initially, we explain the several threads in MRI pre-processing, including registration of image, rectification of bias field, and non-brain tissue dismissal. And terminally, the present state evaluation of algorithm is offered and forecasting the growths to systematise the MRI-based brain tumor into a regular cyclic routine in the clinical field are focussed.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {17611–17643},
numpages = {33},
keywords = {Image processing, Tissue, Bias field, Segmentation, Brain tumor, MRI}
}

@inproceedings{10.1145/3276604.3276609,
author = {Guerra, Esther and de Lara, Juan and Chechik, Marsha and Salay, Rick},
title = {Analysing meta-model product lines},
year = {2018},
isbn = {9781450360296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276604.3276609},
doi = {10.1145/3276604.3276609},
abstract = {Model-driven engineering advocates the use of models to describe and automate many software development tasks. The syntax of modelling languages is defined by meta-models, making them essential artefacts. A combination of product line engineering methods and meta-models has been proposed to enable specification of modelling language variants, e.g., to describe a range of systems. However, there is a lack of techniques for ensuring syntactic correctness of all meta-models within a family (including their OCL constraints), and semantic correctness related to properties of individual instances of the different variants. The absence of verification methods at the product-line level can cause synthesis of ill-formed meta-models and problematic feature combinations whose effect at the instance level may go unnoticed.  To attack this problem, we propose an approach to lifting both the meta-model syntax checking and the satisfiability checking of properties of individual meta-model instances, to the product-line level. We validate the approach via a prototype tool called Merlin, and report on several experiments that show the advantages of our method w.r.t. an enumerative analysis approach.},
booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {160–173},
numpages = {14},
keywords = {Product Lines, OCL, Model-Driven Engineering, Model Finding, Meta-Modelling},
location = {Boston, MA, USA},
series = {SLE 2018}
}

@inproceedings{10.1145/3168365.3168377,
author = {Ananieva, Sofia and Klare, Heiko and Burger, Erik and Reussner, Ralf},
title = {Variants and Versions Management for Models with Integrated Consistency Preservation},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168377},
doi = {10.1145/3168365.3168377},
abstract = {Modern software systems are often developed and maintained by describing them in several modeling and programming languages. To reduce complexity and improve understandability of such systems, models represent specific views on the system. These views have semantic interrelations (e.g., by sharing common or dependent information) that need to be kept consistent during evolution of the system. Apart from that, modern systems need to run in many different contexts and be highly configurable to satisfy the demand for fully customizable products. Such variable systems often comprise various dependencies from which inconsistencies may arise. Combining solutions for consistency management with variants and versions management, however, comes with many challenges.In this research-in-progress paper, we introduce the VaVe approach which makes variants and versions management aware of automated consistency preservation in the context of multi-view modeling. We explain core features of the approach and reason about its benefits and limitations.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {3–10},
numpages = {8},
keywords = {Variability Management, Software Product Lines, Delta-Based Consistency Preservation},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1109/WI.2006.141,
author = {Wang, Xiuli and Wang, Yongji and Zhou, Hui},
title = {QSCM: Engineering QoS in Web-Based Software Configuration Management System},
year = {2006},
isbn = {0769527477},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2006.141},
doi = {10.1109/WI.2006.141},
abstract = {The conventional Software Configuration Management (SCM) tools have shown their weakness in large and complex software systems with the development of modern software. This paper provides a better SCM tool with high availability to facilitate software development process. A novel approach named Quality of Service (QoS)-based SCM (QSCM) is presented, which introduces QoS guarantee techniques at the application level to web-based SCM system in order to improve SCM system's performance. QSCM classifies users together with SCM activities and treats with requests according to their priority. When the server is overloaded, it stops treatment with new requests so that to avoid from crash. The work described here is a first step towards this goal.},
booktitle = {Proceedings of the 2006 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {1050–1056},
numpages = {7},
series = {WI '06}
}

@inproceedings{10.1145/3426746.3434054,
author = {Ezeh, Dubem},
title = {On packet classification using a decision-tree ensemble},
year = {2020},
isbn = {9781450381833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426746.3434054},
doi = {10.1145/3426746.3434054},
abstract = {Different traffic flows get treated differently at routers, depending on the descriptions they fit as well as rules set by network administrators. Today's routers have their work cut out having to categorize these incoming flows based on preconfigured administrative policies. Purely hardware-based traffic classification solutions are expensive, of low capacity and consume a lot of power. This paper proposes the use of machine learning techniques to classify these flows for appropriate action in Software-Defined Networks.},
booktitle = {Proceedings of the Student Workshop},
pages = {17–18},
numpages = {2},
keywords = {random forest algorithm, Software-Defined Networks, Packet classification},
location = {Barcelona, Spain},
series = {CoNEXT'20}
}

@article{10.1007/s00354-021-00126-2,
author = {Li, Peipei and Wu, Man and He, Junhong and Hu, Xuegang},
title = {Recurring Drift Detection and Model Selection-Based Ensemble Classification for Data Streams with Unlabeled Data},
year = {2021},
issue_date = {Aug 2021},
publisher = {Ohmsha},
address = {JPN},
volume = {39},
number = {2},
issn = {0288-3635},
url = {https://doi.org/10.1007/s00354-021-00126-2},
doi = {10.1007/s00354-021-00126-2},
abstract = {Data stream classification is widely popular in the field of network monitoring, sensor network and electronic commerce, etc. However, in the real-world applications, recurring concept drifting and label missing in data streams seriously aggravate the difficulty on the classification solutions. And this challenge has received little attention from the research community. Motivated by this, we propose a new ensemble classification approach based on the recurring concept drifting detection and model selection for data streams with unlabeled data. First, we build an ensemble model based on the classifiers and clusters. To improve the classification accuracy, we use the ensemble model to predict each data chunk and partition clusters according to the distribution of predicted class labels. Second, we adopt a new concept drifting detection method based on the divergence of concept distributions between adjoining data chunks to distinguish recurring concept drifts. All historical new concepts will be maintained. Meanwhile, we introduce the time-stamp-based weights for base models in the ensemble model. In the selection of the base model, we consider the time-stamp-based weight and the divergence between concept distributions simultaneously. Finally, extensive experiments conducted on four benchmark data sets show that our approach can quickly adapt to data streams with recurring concept drifts, and improve the classification accuracy compared to several state-of-the-art classification algorithms for data streams with concept drifts and unlabeled data.},
journal = {New Gen. Comput.},
month = aug,
pages = {341–376},
numpages = {36},
keywords = {Unlabeled data, Recurring concept drift, Ensemble learning, Data stream classification}
}

@article{10.1145/3477428,
author = {Liu, Liu and Isaacman, Sibren and Kremer, Ulrich},
title = {An Adaptive Application Framework with Customizable Quality Metrics},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/3477428},
doi = {10.1145/3477428},
abstract = {Many embedded environments require applications to produce outcomes under different, potentially changing, resource constraints. Relaxing application semantics through approximations enables trading off resource usage for outcome quality. Although quality is a highly subjective notion, previous work assumes given, fixed low-level quality metrics that often lack a strong correlation to a user’s higher-level quality experience. Users may also change their minds with respect to their quality expectations depending on the resource budgets they are willing to dedicate to an execution. This motivates the need for an adaptive application framework where users provide execution budgets and a customized quality notion. This article presents a novel adaptive program graph representation that enables user-level, customizable quality based on basic quality aspects defined by application developers. Developers also define application configuration spaces, with possible customization to eliminate undesirable configurations. At runtime, the graph enables the dynamic selection of the configuration with maximal customized quality within the user-provided resource budget.An adaptive application framework based on our novel graph representation has been implemented on Android and Linux platforms and evaluated on eight benchmark programs, four with fully customizable quality. Using custom quality instead of the default quality, users may improve their subjective quality experience value by up to 3.59×, with 1.76× on average under different resource constraints. Developers are able to exploit their application structure knowledge to define configuration spaces that are on average 68.7% smaller as compared to existing, structure-oblivious approaches. The overhead of dynamic reconfiguration averages less than 1.84% of the overall application execution time.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = nov,
articleno = {13},
numpages = {33},
keywords = {QoS, configuration management, Approximate computing}
}

@article{10.1016/j.specom.2019.10.003,
author = {Stasak, Brian and Epps, Julien and Goecke, Roland},
title = {Automatic depression classification based on affective read sentences: Opportunities for text-dependent analysis},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.003},
doi = {10.1016/j.specom.2019.10.003},
journal = {Speech Commun.},
month = dec,
pages = {1–14},
numpages = {14},
keywords = {Valence, Speech elicitation, Machine learning, Paralinguistics, Digital medicine, Digital phenotyping}
}

@inproceedings{10.5555/1037990.1038004,
author = {Da Silva, Fabio Q. B. and Da Cunha, Juliana Silva and Franklin, Danielle M. and Varejão, Luciana S. and Belian, Rosalie},
title = {An NFS Configuration Management System and its Underlying Object-Oriented Model},
year = {1998},
publisher = {USENIX Association},
address = {USA},
booktitle = {Proceedings of the 12th USENIX Conference on System Administration},
pages = {121–130},
numpages = {10},
location = {Boston, Massachusetts},
series = {LISA '98}
}

@article{10.1007/s10270-009-0120-9,
author = {Anquetil, Nicolas and Kulesza, Uirá and Mitschke, Ralf and Moreira, Ana and Royer, Jean-Claude and Rummler, Andreas and Sousa, André},
title = {A model-driven traceability framework for software product lines},
year = {2010},
issue_date = {September 2010},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-009-0120-9},
doi = {10.1007/s10270-009-0120-9},
abstract = {Software product line (SPL) engineering is a recent approach to software development where a set of software products are derived for a well defined target application domain, from a common set of core assets using analogous means of production (for instance, through Model Driven Engineering). Therefore, such family of products are built from reuse, instead of developed individually from scratch. SPL promise to lower the costs of development, increase the quality of software, give clients more flexibility and reduce time to market. These benefits come with a set of new problems and turn some older problems possibly more complex. One of these problems is traceability management. In the European AMPLE project we are creating a common traceability framework across the various activities of the SPL development. We identified four orthogonal traceability dimensions in SPL development, one of which is an extension of what is often considered as "traceability of variability". This constitutes one of the two contributions of this paper. The second contribution is the specification of a metamodel for a repository of traceability links in the context of SPL and the implementation of a respective traceability framework. This framework enables fundamental traceability management operations, such as trace import and export, modification, query and visualization. The power of our framework is highlighted with an example scenario.},
journal = {Softw. Syst. Model.},
month = sep,
pages = {427–451},
numpages = {25},
keywords = {Traceability, Software product line, Model driven engineering}
}

@article{10.1016/j.neucom.2019.04.066,
author = {Zhu, Qi and Yuan, Ning and Huang, Jiashuang and Hao, Xiaoke and Zhang, Daoqiang},
title = {Multi-modal AD classification via self-paced latent correlation analysis},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {355},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.04.066},
doi = {10.1016/j.neucom.2019.04.066},
journal = {Neurocomput.},
month = aug,
pages = {143–154},
numpages = {12},
keywords = {Computer-aided diagnosis, Self-paced learning, Low-rank, Feature extraction, Multi-modal fusion}
}

@inproceedings{10.5555/3524938.3525259,
author = {Geng, Sinong and Nassif, Houssam and Manzanares, Carlos A. and Reppen, A. Max and Sircar, Ronnie},
title = {Deep PQR: solving inverse reinforcement learning using anchor actions},
year = {2020},
publisher = {JMLR.org},
abstract = {We propose a reward function estimation framework for inverse reinforcement learning with deep energy-based policies. We name our method PQR, as it sequentially estimates the Policy, the Q- function, and the Reward function by deep learning. PQR does not assume that the reward solely depends on the state, instead it allows for a dependency on the choice of action. Moreover, PQR allows for stochastic state transitions. To accomplish this, we assume the existence of one anchor action whose reward is known, typically the action of doing nothing, yielding no reward. We present both estimators and algorithms for the PQR method. When the environment transition is known, we prove that the PQR reward estimator uniquely recovers the true reward. With unknown transitions, we bound the estimation error of PQR. Finally, the performance of PQR is demonstrated by synthetic and real-world datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {321},
numpages = {11},
series = {ICML'20}
}

@inproceedings{10.5555/3060832.3060891,
author = {Pi, Te and Li, Xi and Zhang, Zhongfei and Meng, Deyu and Wu, Fei and Xiao, Jun and Zhuang, Yueting},
title = {Self-paced boost learning for classification},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Effectiveness and robustness are two essential aspects of supervised learning studies. For effective learning, ensemble methods are developed to build a strong effective model from ensemble of weak models. For robust learning, self-paced learning (SPL) is proposed to learn in a self-controlled pace from easy samples to complex ones. Motivated by simultaneously enhancing the learning effectiveness and robustness, we propose a unified framework, Self-Paced Boost Learning (SPBL). With an adaptive from-easy-to-hard pace in boosting process, SPBL asymptotically guides the model to focus more on the insufficiently learned samples with higher reliability. Via a max-margin boosting optimization with self-paced sample selection, SPBL is capable of capturing the intrinsic inter-class discriminative patterns while ensuring the reliability of the samples involved in learning. We formulate SPBL as a fully-corrective optimization for classification. The experiments on several real-world datasets show the superiority of SPBL in terms of both effectiveness and robustness.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1932–1938},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@inproceedings{10.1109/ICSE.2019.00047,
author = {Horton, Eric and Parnin, Chris},
title = {DockerizeMe: automatic inference of environment dependencies for python code snippets},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00047},
doi = {10.1109/ICSE.2019.00047},
abstract = {Platforms like Stack Overflow and GitHub's gist system promote the sharing of ideas and programming techniques via the distribution of code snippets designed to illustrate particular tasks. Python, a popular and fast-growing programming language, sees heavy use on both sites, with nearly one million questions asked on Stack Overflow and 400 thousand public gists on GitHub. Unfortunately, around 75% of the Python example code shared through these sites cannot be directly executed. When run in a clean environment, over 50% of public Python gists fail due to an import error for a missing library.We present DockerizeMe, a technique for inferring the dependencies needed to execute a Python code snippet without import error. DockerizeMe starts with offline knowledge acquisition of the resources and dependencies for popular Python packages from the Python Package Index (PyPI). It then builds Docker specifications using a graph-based inference procedure. Our inference procedure resolves import errors in 892 out of nearly 3,000 gists from the Gistable dataset for which Gistable's baseline approach could not find and install all dependencies.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {328–338},
numpages = {11},
keywords = {python, environment inference, docker, dependencies, configuration management},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1007/s11219-013-9197-z,
author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
title = {Quality attribute modeling and quality aware product configuration in software product lines},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9197-z},
doi = {10.1007/s11219-013-9197-z},
abstract = {In software product line engineering, the customers mostly concentrate on the functionalities of the target product during product configuration. The quality attributes of a target product, such as security and performance, are often assessed until the final product is generated. However, it might be very costly to fix the problem if it is found that the generated product cannot satisfy the customers' quality requirements. Although the quality of a generated product will be affected by all the life cycles of product development, feature-based product configuration is the first stage where the estimation or prediction of the quality attributes should be considered. As we know, the key issue of predicting the quality attributes for a product configured from feature models is to measure the interdependencies between functional features and quality attributes. The current existing approaches have several limitations on this issue, such as requiring real products for the measurement or involving domain experts' efforts. To overcome these limitations, we propose a systematic approach of modeling quality attributes in feature models based on domain experts' judgments using the analytic hierarchical process (AHP) and conducting quality aware product configuration based on the captured quality knowledge. Domain experts' judgments are adapted to avoid generating the real products for quality evaluation, and AHP is used to reduce domain experts' efforts involved in the judgments. A prototype tool is developed to implement the concepts of the proposed approach, and a formal evaluation is carried out based on a large-scale case study.},
journal = {Software Quality Journal},
month = sep,
pages = {365–401},
numpages = {37},
keywords = {Software product line, Quality attributes assessment, Product configuration, Non-functional requirement (NFR) framework, Feature model, Analytic hierarchical process (AHP)}
}

@article{10.1007/s10270-012-0289-1,
author = {Leitner, Andrea and Preschern, Christopher and Kreiner, Christian},
title = {Effective development of automation systems through domain-specific modeling in a small enterprise context},
year = {2014},
issue_date = {February  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-012-0289-1},
doi = {10.1007/s10270-012-0289-1},
abstract = {High development and maintenance costs and a high error rate are the major problems in the development of automation systems, which are mainly caused by bad communication and inefficient reuse methods. To overcome these problems, we propose a more systematic reuse approach. Though systematic reuse approaches such as software product lines are appealing, they tend to involve rather burdensome development and management processes. This paper focuses on small enterprises. Since such companies are often unable to perform a "big bang" adoption of the software product line, we suggest an incremental, more lightweight process to transition from single-system development to software product line development. Besides the components of the transition process, this paper discusses tool selection, DSL technology, stakeholder communication support, and business considerations. Although based on problems from the automation system domain, we believe the approach may be general enough to be applicable in other domains as well. The approach has proven successful in two case studies. First, we applied it to a research project for the automation of a logistics lab model, and in the second case (a real-life industry case), we investigated the approaches suitability for fish farm automation systems. Several metrics were collected throughout the evolution of each case, and this paper presents the data for single system development, clone&amp;own and software product line development. The results and observable effects are compared, discussed, and finally summarized in a list of lessons learned.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {35–54},
numpages = {20},
keywords = {System development process, Software product line, Small enterprise cost model, Domain-specific modeling, Automation system}
}

@inproceedings{10.1145/2556624.2556631,
author = {Dintzner, Nicolas and Van Deursen, Arie and Pinzger, Martin},
title = {Extracting feature model changes from the Linux kernel using FMDiff},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556631},
doi = {10.1145/2556624.2556631},
abstract = {The Linux kernel feature model has been studied as an example of large scale evolving feature model and yet details of its evolution are not known. We present here a classification of feature changes occurring on the Linux kernel feature model, as well as a tool, FMDiff, designed to automatically extract those changes. With this tool, we obtained the history of more than twenty architecture specific feature models, over ten releases and compared the recovered information with Kconfig file changes. We establish that FMDiff provides a comprehensive view of feature changes and show that the collected data contains promising information regarding the Linux feature model evolution.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {22},
numpages = {8},
keywords = {software product line, feature model, evolution},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/3180155.3180159,
author = {Krieter, Sebastian and Thüm, Thomas and Schulze, Sandro and Schröter, Reimar and Saake, Gunter},
title = {Propagating configuration decisions with modal implication graphs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180159},
doi = {10.1145/3180155.3180159},
abstract = {Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {898–909},
numpages = {12},
keywords = {configuration, decision propagation, software product line},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1007/s10836-012-5343-y,
author = {Colanzi, Thelma Elita and Assunção, Wesley Klewerton and De Freitas Guilhermino Trindade, Daniela and Zorzo, Carlos Alberto and Vergilio, Silvia Regina},
title = {Evaluating Different Strategies for Testing Software Product Lines},
year = {2013},
issue_date = {February  2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1},
issn = {0923-8174},
url = {https://doi.org/10.1007/s10836-012-5343-y},
doi = {10.1007/s10836-012-5343-y},
abstract = {The Software Product Line (SPL) approach is associated with many benefits, and with some challenges too. In the SPL testing, a great challenge is the integration of test methods and techniques with the SPL engineering. To do this, some strategies have been proposed, however, they have not been properly evaluated and compared. In this paper a case study is described comparing three different test strategies: the product by product strategy that tests each product individually; an incremental strategy that tests the products reusing test cases from products previously tested; and a strategy that instantiates test data derived in the domain engineering, considering SPL commonalities and variabilities. In the study an SPL from the games domain was used, and the test data were generated from use cases. The results show that SPL oriented strategies are associated to greater percentages of reuse, and consequently, with lower effort to write test cases.},
journal = {J. Electron. Test.},
month = feb,
pages = {9–24},
numpages = {16},
keywords = {Use case model, Testing strategies, Software product line}
}

@article{10.1145/108515.108537,
author = {Davis, Alan M. and Bersoff, Edward H.},
title = {Impacts of life cycle models on software configuration management},
year = {1991},
issue_date = {Aug. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/108515.108537},
doi = {10.1145/108515.108537},
journal = {Commun. ACM},
month = aug,
pages = {104–118},
numpages = {15}
}

@inproceedings{10.1109/ICSM.2012.6405273,
author = {Hassan, Ahmed E. and Zou, Ying and Dhaliwal, Tejinder and Khomh, Foutse},
title = {Recovering commit dependencies for selective code integration in software product lines},
year = {2012},
isbn = {9781467323130},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2012.6405273},
doi = {10.1109/ICSM.2012.6405273},
abstract = {In software product lines, multiple products of a software product family, share source code of common components. New features added to the common components of a software product family, are integrated into products following a selective code integration process. Selective code integration is a process in which developers pick the commits (i.e., code changes) related to a feature from one code branch and integrate them into another code branch. Developers often manually link the commits to the features to enable the selective integration of features. In current practice, not all dependent commits are always linked to features and developers might miss the unlinked commits during selective code integration. In this paper, we propose two grouping approaches that identify dependencies among commits and create groups of dependent commits that need to be integrated as a whole into a code branch. Our first approach is automatic and the other is developer-guided. Through a case study on data derived from a product line of mobile software applications, we show that our approaches can help to reduce by up to 94% integration failures caused by missing commit dependencies.},
booktitle = {Proceedings of the 2012 IEEE International Conference on Software Maintenance (ICSM)},
pages = {202–211},
numpages = {10},
keywords = {Software maintenance, Selective Code Integration, Product Line Development, Measurement, Learning systems, Equations, Educational institutions, Developer-guided Grouping, Conferences},
series = {ICSM '12}
}

@article{10.1007/s10009-014-0362-x,
author = {Eichelberger, Holger and Schmid, Klaus},
title = {Mapping the design-space of textual variability modeling languages: a refined analysis},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-014-0362-x},
doi = {10.1007/s10009-014-0362-x},
abstract = {Variability modeling is a major part of modern product line engineering. Graphical or table-based approaches to variability modeling are focused around abstract models and specialized tools to interact with these models. However, more recently textual variability modeling languages, comparable to some extent to programming languages, were introduced. We consider the recent trend in product line engineering towards textual variability modeling languages as a phenomenon, which deserves deeper analysis. In this article, we report on the results and approach of a literature survey combined with an expert study. In the literature survey, we identified 11 languages, which enable the textual specification of product line variability and which are sufficiently described for an in-depth analysis. We provide a classification scheme, useful to describe the range of capabilities of such languages. Initially, we identified the relevant capabilities of these languages from a literature survey. The result of this has been refined, validated and partially improved by the expert survey. A second recent phenomenon in product line variability modeling is the increasing scale of variability models. Some authors of textual variability modeling languages argue that these languages are more appropriate for large-scale models. As a consequence, we would expect specific capabilities addressing scalability in the languages. Thus, we compare the capabilities of textual variability modeling techniques, if compared to graphical variability modeling approaches and in particular to analyze their specialized capabilities for large-scale models.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {559–584},
numpages = {26},
keywords = {Variability modeling, Survey, Scalability, Domain-specific languages}
}

@article{10.1007/s10664-017-9499-z,
author = {Assunção, Wesley K. and Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Vergilio, Silvia R. and Egyed, Alexander},
title = {Reengineering legacy applications into software product lines: a systematic mapping},
year = {2017},
issue_date = {December  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9499-z},
doi = {10.1007/s10664-017-9499-z},
abstract = {Software Product Lines (SPLs) are families of systems that share common assets allowing a disciplined reuse. Rarely SPLs start from scratch, instead they usually start from a set of existing systems that undergo a reengineering process. Many approaches to conduct the reengineering process have been proposed and documented in research literature. This scenario is a clear testament to the interest in this research area. We conducted a systematic mapping study to provide an overview of the current research on reengineering of existing systems to SPLs, identify the community activity in regarding of venues and frequency of publications in this field, and point out trends and open issues that could serve as references for future research. This study identified 119 relevant publications. These primary sources were classified in six different dimensions related to reengineering phases, strategies applied, types of systems used in the evaluation, input artefacts, output artefacts, and tool support. The analysis of the results points out the existence of a consolidate community on this topic and a wide range of strategies to deal with different phases and tasks of the reengineering process, besides the availability of some tools. We identify some open issues and areas for future research such as the implementation of automation and tool support, the use of different sources of information, need for improvements in the feature management, the definition of ways to combine different strategies and methods, lack of sophisticated refactoring, need for new metrics and measures and more robust empirical evaluation. Reengineering of existing systems into SPLs is an active research topic with real benefits in practice. This mapping study motivates new research in this field as well as the adoption of systematic reuse in software companies.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {2972–3016},
numpages = {45},
keywords = {Systematic reuse, Reengineering, Product family, Legacy systems, Evolution}
}

@inproceedings{10.1145/3427921.3450243,
author = {Samoaa, Hazem and Leitner, Philipp},
title = {An Exploratory Study of the Impact of Parameterization on JMH Measurement Results in Open-Source Projects},
year = {2021},
isbn = {9781450381949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427921.3450243},
doi = {10.1145/3427921.3450243},
abstract = {The Java Microbenchmarking Harness (JMH) is a widely used tool for testing performance-critical code on a low level. One of the key features of JMH is the support for user-defined parameters, which allows executing the same benchmark with different workloads. However, a benchmark configured with n parameters with m different values each requires JMH to execute the benchmark mn times (once for each combination of configured parameter values). Consequently, even fairly modest parameterization leads to a combinatorial explosion of benchmarks that have to be executed, hence dramatically increasing execution time. However, so far no research has investigated how this type of parameterization is used in practice, and how important different parameters are to benchmarking results. In this paper, we statistically study how strongly different user parameters impact benchmark measurements for 126 JMH benchmarks from five well-known open source projects. We show that 40% of the studied metric parameters have no correlation with the resulting measurement, i.e., testing with different values in these parameters does not lead to any insights. If there is a correlation, it is often strongly predictable following a power law, linear, or step function curve. Our results provide a first understanding of practical usage of user-defined JMH parameters, and how they correlate with the measurements produced by benchmarks. We further show that a machine learning model based on Random Forest ensembles can be used to predict the measured performance of an untested metric parameter value with an accuracy of 93% or higher for all but one benchmark class, demonstrating that given sufficient training data JMH performance test results for different parameterizations are highly predictable.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {213–224},
numpages = {12},
keywords = {machine learning, java microbenchmarking harness (JMH), benchmark parametrization, benchmark measurements},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.4018/IJRSDA.2016070101,
author = {Ripon, Shamim H and Kamal, Sarwar and Hossain, Saddam and Dey, Nilanjan},
title = {Theoretical Analysis of Different Classifiers under Reduction Rough Data Set: A Brief Proposal},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {3},
issn = {2334-4598},
url = {https://doi.org/10.4018/IJRSDA.2016070101},
doi = {10.4018/IJRSDA.2016070101},
abstract = {Rough set plays vital role to overcome the complexities, vagueness, uncertainty, imprecision, and incomplete data during features analysis. Classification is tested on certain dataset that maintain an exact class and review process where key attributes decide the class positions. To assess efficient and automated learning, algorithms are used over training datasets. Generally, classification is supervised learning whereas clustering is unsupervised. Classifications under mathematical models deal with mining rules and machine learning. The Objective of this work is to establish a strong theoretical and manual analysis among three popular classifier namely K-nearest neighbor K-NN, Naive Bayes and Apriori algorithm. Hybridization with rough sets among these three classifiers enables enable to address larger datasets. Performances of three classifiers have tested in absence and presence of rough sets. This work is in the phase of implementation for DNA Deoxyribonucleic Acid datasets and it will design automated system to assess classifier under machine learning environment.},
journal = {Int. J. Rough Sets Data Anal.},
month = jul,
pages = {1–20},
numpages = {20},
keywords = {Rough Set, Naive Bayes, K-NN, DNA, Apriori Algorithm}
}

@article{10.1007/s10270-015-0472-2,
author = {Dintzner, Nicolas and Deursen, Arie and Pinzger, Martin},
title = {Analysing the Linux kernel feature model changes using FMDiff},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0472-2},
doi = {10.1007/s10270-015-0472-2},
abstract = {Evolving a large scale, highly variable system is a challenging task. For such a system, evolution operations often require to update consistently both their implementation and its feature model. In this context, the evolution of the feature model closely follows the evolution of the system. The purpose of this work is to show that fine-grained feature changes can be used to guide the evolution of the highly variable system. In this paper, we present an approach to obtain fine-grained feature model changes with its supporting tool "FMDiff". Our approach is tailored for Kconfig-based variability models and proposes a feature change classification detailing changes in features, their attributes and attribute values. We apply our approach to the Linux kernel feature model, extracting feature changes occurring in sixteen official releases. In contrast to previous studies, we found that feature modifications are responsible for most of the changes. Then, by taking advantage of the multi-platform aspect of the Linux kernel, we observe the effects of a feature change across the different architecture-specific feature models of the kernel. We found that between 10 and 50 % of feature changes impact all the architecture-specific feature models, offering a new perspective on studies of the evolution of the Linux feature model and development practices of its developers.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {55–76},
numpages = {22},
keywords = {Software product line, Feature model, Evolution}
}

@article{10.1016/j.procs.2017.08.206,
author = {Mani, Neel and Helfert, Markus and Pahl, Claus},
title = {A Domain-specific Rule Generation Using Model-Driven Architecture in Controlled Variability Model},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.08.206},
doi = {10.1016/j.procs.2017.08.206},
abstract = {The business environment changes rapidly and needs to adapt to the enterprise business systems must be considered for new types of requirements to accept changes in the business strategies and processes. This raises new challenges that the traditional development approaches cannot always provide a complete solution in an efficient way. However, most of the current proposals for automatic generation are not devised to cope with rapid integration of the changes in the business requirement of end user (stakeholders and customers) resource. Domain-specific Rules constitute a key element for domain specific enterprise application, allowing configuration of changes, and management of the domain constraint within a domain. In this paper, we propose an approach to the development of an automatic generation of the domain-specific rules by using variability feature model and ontology definition of domain model concepts coming from Software product line engineering and Model Driven Architecture. We provide a process approach to generate a domain-specific rule based on the end user requirement.},
journal = {Procedia Comput. Sci.},
month = sep,
pages = {2354–2362},
numpages = {9},
keywords = {Variability Model, Rule Generation, Model Driven Architecture, Domain-specific rules, Business Process Model}
}

@inproceedings{10.5555/3466184.3466446,
author = {Rodriguez, Brodderick and Yilmaz, Levent},
title = {Learning rule-based explanatory models from exploratory multi-simulation for decision-support under uncertainty},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Exploratory modeling and simulation is an effective strategy when there are substantial contextual uncertainty and representational ambiguity in problem formulation. However, two significant challenges impede the use of an ensemble of models in exploratory simulation. The first challenge involves streamlining the maintenance and synthesis of multiple models from plausible features that are identified from and subject to the constraints of the research hypothesis. The second challenge is making sense of the data generated by multi-simulation over a model ensemble. To address both challenges, we introduce a computational framework that integrates feature-driven variability management with an anticipatory learning classifier system to generate explanatory rules from multi-simulation data.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2293–2304},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/2517208.2517213,
author = {Kolesnikov, Sergiy and von Rhein, Alexander and Hunsen, Claus and Apel, Sven},
title = {A comparison of product-based, feature-based, and family-based type checking},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517213},
doi = {10.1145/2517208.2517213},
abstract = {Analyzing software product lines is difficult, due to their inherent variability. In the past, several strategies for product-line analysis have been proposed, in particular, product-based, feature-based, and family-based strategies. Despite recent attempts to conceptually and empirically compare different strategies, there is no work that empirically compares all of the three strategies in a controlled setting. We close this gap by extending a compiler for feature-oriented programming with support for product-based, feature-based, and family-based type checking. We present and discuss the results of a comparative performance evaluation that we conducted on a set of 12 feature-oriented, Java-based product lines. Most notably, we found that the family-based strategy is superior for all subject product lines: it is substantially faster, it detects all kinds of errors, and provides the most detailed information about them.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {115–124},
numpages = {10},
keywords = {type checking, product-line analysis, fuji, feature-oriented programming},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@inproceedings{10.1145/2897053.2897058,
author = {Sharifloo, Amir Molzam and Metzger, Andreas and Quinton, Clément and Baresi, Luciano and Pohl, Klaus},
title = {Learning and evolution in dynamic software product lines},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897058},
doi = {10.1145/2897053.2897058},
abstract = {A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {158–164},
numpages = {7},
keywords = {machine learning, evolution, dynamic software product lines, adaptation},
location = {Austin, Texas},
series = {SEAMS '16}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {deep learning, constraint solving, configurable systems, AutoML},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1007/s10586-019-03012-1,
author = {Vázquez-Ingelmo, Andrea and García-Peñalvo, Francisco José and Therón, Roberto and Amo Filvà, Daniel and Fonseca Escudero, David},
title = {Connecting domain-specific features to source code: towards the automatization of dashboard generation},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-019-03012-1},
doi = {10.1007/s10586-019-03012-1},
abstract = {Dashboards are useful tools for generating knowledge and support decision-making processes, but the extended use of technologies and the increasingly available data asks for user-friendly tools that allow any user profile to exploit their data. Building tailored dashboards for any potential user profile would involve several resources and long development times, taking into account that dashboards can be framed in very different contexts that should be studied during the design processes to provide practical tools. This situation leads to the necessity of searching for methodologies that could accelerate these processes. The software product line paradigm is one recurrent method that can decrease the time-to-market of products by reusing generic core assets that can be tuned or configured to meet specific requirements. However, although this paradigm can solve issues regarding development times, the configuration of the dashboard is still a complex challenge; users’ goals, datasets, and context must be thoroughly studied to obtain a dashboard that fulfills the users’ necessities and that fosters insight delivery. This paper outlines the benefits and a potential approach to automatically configuring information dashboards by leveraging domain commonalities and code templates. The main goal is to test the functionality of a workflow that can connect external algorithms, such as artificial intelligence algorithms, to infer dashboard features and feed a generator based on the software product line paradigm.},
journal = {Cluster Computing},
month = sep,
pages = {1803–1816},
numpages = {14},
keywords = {Automatic configuration, Artificial intelligence, Feature model, Information dashboards, Meta-model, Domain engineering, SPL}
}

@inproceedings{10.1145/3340482.3342742,
author = {Borg, Markus and Svensson, Oscar and Berg, Kristian and Hansson, Daniel},
title = {SZZ unleashed: an open implementation of the SZZ algorithm - featuring example usage in a study of just-in-time bug prediction for the Jenkins project},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342742},
doi = {10.1145/3340482.3342742},
abstract = {Machine learning applications in software engineering often rely on detailed information about bugs. While issue trackers often contain information about when bugs were fixed, details about when they were introduced to the system are often absent. As a remedy, researchers often rely on the SZZ algorithm as a heuristic approach to identify bug-introducing software changes. Unfortunately, as reported in a recent systematic literature review, few researchers have made their SZZ implementations publicly available. Consequently, there is a risk that research effort is wasted as new projects based on SZZ output need to initially reimplement the approach. Furthermore, there is a risk that newly developed (closed source) SZZ implementations have not been properly tested, thus conducting research based on their output might introduce threats to validity. We present SZZ Unleashed, an open implementation of the SZZ algorithm for git repositories. This paper describes our implementation along with a usage example for the Jenkins project, and conclude with an illustrative study on just-in-time bug prediction. We hope to continue evolving SZZ Unleashed on GitHub, and warmly invite the community to contribute.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {mining software repositories, issue tracking, defect prediction, SZZ},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@article{10.1016/j.neucom.2019.06.075,
author = {Xue, Yani and Li, Miqing and Shepperd, Martin and Lauria, Stasha and Liu, Xiaohui},
title = {A novel aggregation-based dominance for Pareto-based evolutionary algorithms to configure software product lines},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {364},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.075},
doi = {10.1016/j.neucom.2019.06.075},
journal = {Neurocomput.},
month = oct,
pages = {32–48},
numpages = {17},
keywords = {Multi-objective optimization, Evolutionary algorithm, Software product line, Optimal feature selection}
}

@inproceedings{10.1145/3397271.3401041,
author = {Chen, Fanglin and Liu, Xiao and Proserpio, Davide and Troncoso, Isamar and Xiong, Feiyu},
title = {Studying Product Competition Using Representation Learning},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401041},
doi = {10.1145/3397271.3401041},
abstract = {Studying competition and market structure at the product level instead of brand level can provide firms with insights on cannibalization and product line optimization. However, it is computationally challenging to analyze product-level competition for the millions of products available on e-commerce platforms. We introduce Product2Vec, a method based on the representation learning algorithm Word2Vec, to study product-level competition, when the number of products is large. The proposed model takes shopping baskets as inputs and, for every product, generates a low-dimensional embedding that preserves important product information. In order for the product embeddings to be useful for firm strategic decision making, we leverage economic theories and causal inference to propose two modifications to Word2Vec. First of all, we create two measures, complementarity and exchangeability, that allow us to determine whether product pairs are complements or substitutes. Second, we combine these vectors with random utility-based choice models to forecast demand. To accurately estimate price elasticities, i.e., how demand responds to changes in price, we modify Word2Vec by removing the influence of price from the product vectors. We show that, compared with state-of-the-art models, our approach is faster, and can produce more accurate demand forecasts and price elasticities.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1261–1268},
numpages = {8},
keywords = {representation learning, product2vec, product competition},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1007/s10489-020-01730-3,
author = {Zhu, Wenjie and Peng, Bo and Wu, Han and Wang, Binhao},
title = {Query set centered sparse projection learning for set based image classification},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01730-3},
doi = {10.1007/s10489-020-01730-3},
abstract = {Set based image classification technology has been developed successfully in recent decades. Previous approaches dispose set based image classification by employing all the gallery sets to learn metrics or construct the model using a typical number of parameters. However, they are based on the assumption that the global structure is consistent with the local structure, which is rigid in real applications. Additionally, the participation of all gallery sets increases the influence of outliers. This paper conducts this task via sparse projection learning by employing ℓ2,1 norm from the perspective of the query set. Instead of involving all the image sets, this work devotes to searching for a local region, which is centered with a query set and constructed by the candidates selected from different classes in the gallery sets. By maximizing the inter-class while minimizing the intra-class of the candidates from the gallery sets from the query set, this work can learn a discriminate and sparse projection for image set feature extraction. In order to learn the projection, an alternative updating algorithm to solve the optimization problem is proposed and the convergence and complexity are analyzed. Finally, the distance is measured in the discriminate low-dimensional space using Euclidean distance between the central data point of the query set and the central one of images from the same class. The proposed approach learns the projection in the local set centered with the query set with ℓ2,1 norm, which contributes to more discriminative feature. Compared with the existing algorithms, the experiments on the challenging databases demonstrate that the proposed simple yet effective approach obtains the best classification accuracy with comparable time cost.},
journal = {Applied Intelligence},
month = oct,
pages = {3400–3411},
numpages = {12},
keywords = {Discriminate subspace learning, Set based image classification, Sparse projection learning, Query set}
}

@inproceedings{10.5555/2555523.2555556,
author = {Bagheri, Ebrahim and Ensan, Faezeh},
title = {Light-weight software product lines for small and medium-sized enterprises (SMEs)},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {Product line engineering practices promote the idea of systematic reuse of core assets and have been reported to decrease time-to-market and development costs for new products. However, our recent efforts to transfer our product line engineering knowledge to several of our small and medium-size enterprise industrial partner showed that there are challenges that need to be addressed before core product line engineering ideas can be deployed in SME context. These challenges include upfront investment costs, business traceability, levels of abstraction of functional features and semantic distinction between functional and non-functional software aspects. In order to address these challenges within the context of SMEs, we adopt and extend the behavior-driven development methodology in a way to not only offer agility in practice but also to equip software developers with the means to capture and manage software variability within the behavior-driven development process. We introduce the details of the extended methodology and discuss its advantages and disadvantages in detail.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {311–324},
numpages = {14},
location = {Ontario, Canada},
series = {CASCON '13}
}

@inproceedings{10.1109/ASE.2015.58,
author = {Angerer, Florian and Grimmer, Andreas and Prähofer, Herbert and Grünbacher, Paul},
title = {Configuration-aware change impact analysis},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.58},
doi = {10.1109/ASE.2015.58},
abstract = {Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This paper presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining possibly impacted products when changing source code of a product family. The approach further supports engineers, who are adapting specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanism, it provides more precise results than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. Using an industrial product line we report evaluation results on the benefit and performance of the approach.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {385–395},
numpages = {11},
keywords = {program analysis, maintenance, configuration, change impact analysis},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1155/2021/4513610,
author = {Chen, Ling-qing and Wu, Mei-ting and Pan, Li-fang and Zheng, Ru-bin and Liu, KunHong},
title = {Grade Prediction in Blended Learning Using Multisource Data},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4513610},
doi = {10.1155/2021/4513610},
abstract = {Today, blended learning is widely carried out in many colleges. Different online learning platforms have accumulated a large number of fine granularity records of students’ learning behavior, which provides us with an excellent opportunity to analyze students’ learning behavior. In this paper, based on the behavior log data in four consecutive years of blended learning in a college’s programming course, we propose a novel multiclassification frame to predict students’ learning outcomes. First, the data obtained from diverse platforms, i.e., MOOC, Cnblogs, Programming Teaching Assistant (PTA) system, and Rain Classroom, are integrated and preprocessed. Second, a novel error-correcting output codes (ECOC) multiclassification framework, based on genetic algorithm (GA) and ternary bitwise calculator, is designed to effectively predict the grade levels of students by optimizing the code-matrix, feature subset, and binary classifiers of ECOC. Experimental results show that the proposed algorithm in this paper significantly outperforms other alternatives in predicting students’ grades. In addition, the performance of the algorithm can be further improved by adding the grades of prerequisite courses.},
journal = {Sci. Program.},
month = jan,
numpages = {15}
}

@inproceedings{10.5555/3524938.3525504,
author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U. and Jaggi, Martin},
title = {Extrapolation for large-batch training in deep learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Deep learning networks are typically trained by Stochastic Gradient Descent (SGD) methods that iteratively improve the model parameters by estimating a gradient on a very small fraction of the training data. A major roadblock faced when increasing the batch size to a substantial fraction of the training data for reducing training time is the persistent degradation in performance (generalization gap). To address this issue, recent work propose to add small perturbations to the model parameters when computing the stochastic gradients and report improved generalization performance due to smoothing effects. However, this approach is poorly understood; it requires often model-specific noise and fine-tuning.To alleviate these drawbacks, we propose to use instead computationally efficient extrapolation (extragradient) to stabilize the optimization trajectory while still benefiting from smoothing to avoid sharp minima. This principled approach is well grounded from an optimization perspective and we show that a host of variations can be covered in a unified framework that we propose. We prove the convergence of this novel scheme and rigorously evaluate its empirical performance on ResNet, LSTM, and Transformer. We demonstrate that in a variety of experiments the scheme allows scaling to much larger batch sizes than before whilst reaching or surpassing SOTA accuracy.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {566},
numpages = {11},
series = {ICML'20}
}

@article{10.1016/j.ins.2019.12.015,
author = {Xiao, Yanshan and Yang, Xiaozhou and Liu, Bo},
title = {A new self-paced method for multiple instance boosting learning},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {515},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.12.015},
doi = {10.1016/j.ins.2019.12.015},
journal = {Inf. Sci.},
month = apr,
pages = {80–90},
numpages = {11},
keywords = {Self-Paced learning, Multiple instance boost learning, Multiple instance learning}
}

@article{10.1016/j.knosys.2019.105185,
author = {Liang, Naiyao and Yang, Zuyuan and Li, Zhenni and Xie, Shengli and Su, Chun-Yi},
title = {Semi-supervised multi-view clustering with Graph-regularized Partially Shared Non-negative Matrix Factorization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105185},
doi = {10.1016/j.knosys.2019.105185},
journal = {Know.-Based Syst.},
month = feb,
numpages = {10},
keywords = {Non-negative matrix factorization, Multi-view clustering, Semi-supervised learning, Graph-regularization}
}

@article{10.1007/s00500-020-05005-4,
author = {Malhotra, Ruchika and Lata, Kusum},
title = {A systematic literature review on empirical studies towards prediction of software maintainability},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05005-4},
doi = {10.1007/s00500-020-05005-4},
abstract = {Software maintainability prediction in the earlier stages of software development involves the construction of models for the accurate estimation of maintenance effort. This guides the software practitioners to manage the resources optimally. This study aims at systematically reviewing the prediction models from January 1990 to October 2019 for predicting software maintainability. We analyze the effectiveness of these models according to various aspects. To meet the goal of the research, we have identified 36 research papers. On investigating these papers, we found that various machine learning (ML), statistical (ST), and hybridized (HB) techniques have been applied to develop prediction models to predict software maintainability. The significant finding of this review is that the overall performance of ML-based models is better than that of ST models. The use of HB techniques for prediction of software maintainability is limited. The results of this review revealed that software maintainability prediction (SMP) models developed using ML techniques outperformed models developed using ST techniques. Also, the prediction performance of few models developed using HB techniques is encouraging, yet no conclusive results about the performance of HB techniques could be reported because different HB techniques are applied in a few studies.},
journal = {Soft Comput.},
month = nov,
pages = {16655–16677},
numpages = {23},
keywords = {Hybridized techniques, Statistical techniques, Machine learning techniques, Software maintainability, Software maintenance}
}

@inproceedings{10.1145/3416505.3423563,
author = {Palma, Stefano Dalla and Mohammadi, Majid and Di Nucci, Dario and Tamburri, Damian A.},
title = {Singling the odd ones out: a novelty detection approach to find defects in infrastructure-as-code},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423563},
doi = {10.1145/3416505.3423563},
abstract = {Infrastructure-as-Code (IaC) is increasingly adopted. However, little is known about how to best maintain and evolve it. Previous studies focused on defining Machine-Learning models to predict defect-prone blueprints using supervised binary classification. This class of techniques uses both defective and non-defective instances in the training phase. Furthermore, the high imbalance between defective and non-defective samples makes the training more difficult and leads to unreliable classifiers. In this work, we tackle the defect-prediction problem from a different perspective using novelty detection and evaluate the performance of three techniques, namely OneClassSVM, LocalOutlierFactor, and IsolationForest, and compare their performance with a baseline RandomForest binary classifier. Such models are trained using only non-defective samples: defective data points are treated as novelty because the number of defective samples is too little compared to defective ones. We conduct an empirical study on an extremely-imbalanced dataset consisting of 85 real-world Ansible projects containing only small amounts of defective instances. We found that novelty detection techniques can recognize defects with a high level of precision and recall, an AUC-PR up to 0.86, and an MCC up to 0.31. We deem our results can influence the current trends in defect detection and put forward a new research path toward dealing with this problem.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {31–36},
numpages = {6},
keywords = {Novelty Detection, Infrastructure-as-Code, Defect Prediction},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@article{10.1016/j.neucom.2019.06.072,
author = {Xu, Wei and Liu, Wei and Chi, Haoyuan and Qiu, Song and Jin, Yu},
title = {Self-paced learning with privileged information},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {362},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.072},
doi = {10.1016/j.neucom.2019.06.072},
journal = {Neurocomput.},
month = oct,
pages = {147–155},
numpages = {9},
keywords = {Learning with privileged information, Self-paced learning, Curriculum learning}
}

@article{10.1016/j.neucom.2021.05.039,
author = {Nie, Lun Yiu and Gao, Cuiyun and Zhong, Zhicong and Lam, Wai and Liu, Yang and Xu, Zenglin},
title = {CoreGen: Contextualized Code Representation Learning for Commit Message Generation},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {459},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.05.039},
doi = {10.1016/j.neucom.2021.05.039},
journal = {Neurocomput.},
month = oct,
pages = {97–107},
numpages = {11},
keywords = {Contextualized code representation, Self-supervised learning, Code-to-text generation, Code representation learning, Commit message generation}
}

@article{10.1016/j.datak.2010.01.002,
author = {Reinhartz-Berger, Iris},
title = {Towards automatization of domain modeling},
year = {2010},
issue_date = {May, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {69},
number = {5},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2010.01.002},
doi = {10.1016/j.datak.2010.01.002},
abstract = {A domain model, which captures the common knowledge and the possible variability allowed among applications in a domain, may assist in the creation of other valid applications in that domain. However, to create such domain models is not a trivial task: it requires expertise in the domain, reaching a very high level of abstraction, and providing flexible, yet formal, artifacts. In this paper an approach, called Semi-automated Domain Modeling (SDM), to create draft domain models from applications in those domains, is presented. SDM takes a repository of application models in a domain and matches, merges, and generalizes them into sound draft domain models that include the commonality and variability allowed in these domains. The similarity of the different elements is measured, with consideration of syntactic, semantic, and structural aspects. Unlike ontology and schema integration, these models capture both structural and behavioral aspects of the domain. Running SDM on small repositories of project management applications and scheduling systems, we found that the approach may provide reasonable draft domain models, whose comprehensibility, correctness, completeness, and consistency levels are satisfactory.},
journal = {Data Knowl. Eng.},
month = may,
pages = {491–515},
numpages = {25},
keywords = {UML, Product line engineering, Metamodeling, Domain engineering, Domain analysis, DSL}
}

@inproceedings{10.1145/2975969.2975974,
author = {Fördős, Viktória and Cesarini, Francesco},
title = {CRDTs for the configuration of distributed Erlang systems},
year = {2016},
isbn = {9781450344319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2975969.2975974},
doi = {10.1145/2975969.2975974},
abstract = {CRDT (Conflict-free replicated data type) is a data type that supports conflict free resolution of concurrent, distributed updates. It is often mentioned alongside storage systems that are distributed, fault-tolerant and reliable. These are similar properties and features of Erlang/OTP systems. What distributed Erlang/OTP systems lack, however, is a standardised way to configure multiple nodes. OTP middleware allows you to set configuration parameters called application environment variables on a node basis, they can be updated at runtime, but will not survive a restart unless persisted in the business logic of the system. There is no widely adopted solution to address this omission. In some installations, changes are done manually in the Erlang shell and persisted by editing the configuration files. In others, changes and updates are implemented as part of a new releases and deployed through an upgrade procedure. These tools expect a happy path, and rarely take network failures and consistency into consideration. As a result, issues have been known to cause outages and have left the system in an inconsistent state, with no automated means of detecting the root cause of the problem. In this paper, we introduce a configuration management approach designed for distributed Erlang/OTP systems. They are systems which often trade consistency for availability and scalability, making them a perfect fit for CRDTs. We use a proprietary tool called WombatOAM to update environment variables and check their consistency on both node and cluster-levels. Inconsistencies and failed updates are detected and reported in the form of an alarms, and the history and status of all performed changes are logged, facilitating troubleshooting and recovery efforts. In this paper, we show our approaches to configuration management, and discuss how we approached the issue of consistency in the presence of unreliable networks. We present a qualitative evaluation and a case study to assess the capabilities of WombatOAM’s CRDT based configuration management feature.},
booktitle = {Proceedings of the 15th International Workshop on Erlang},
pages = {42–53},
numpages = {12},
keywords = {WombatOAM, Erlang, Elixir, DevOps, Configuration management, CRDT},
location = {Nara, Japan},
series = {Erlang 2016}
}

@article{10.1016/j.engappai.2019.08.015,
author = {Tavasoli, Hanane and Oommen, B. John and Yazidi, Anis},
title = {On utilizing weak estimators to achieve the online classification of data streams},
year = {2019},
issue_date = {Nov 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {86},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2019.08.015},
doi = {10.1016/j.engappai.2019.08.015},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
pages = {11–31},
numpages = {21},
keywords = {Classification in data streams, Non-stationary environments, Learning automata, Weak estimators}
}

@inproceedings{10.1145/3368826.3377923,
author = {Shaikhha, Amir and Schleich, Maximilian and Ghita, Alexandru and Olteanu, Dan},
title = {Multi-layer optimizations for end-to-end data analytics},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377923},
doi = {10.1145/3368826.3377923},
abstract = {We consider the problem of training machine learning models over multi-relational data. The mainstream approach is to first construct the training dataset using a feature extraction query over input database and then use a statistical software package of choice to train the model. In this paper we introduce Iterative Functional Aggregate Queries (IFAQ), a framework that realizes an alternative approach. IFAQ treats the feature extraction query and the learning task as one program given in the IFAQ's domain-specific language, which captures a subset of Python commonly used in Jupyter notebooks for rapid prototyping of machine learning applications. The program is subject to several layers of IFAQ optimizations, such as algebraic transformations, loop transformations, schema specialization, data layout optimizations, and finally compilation into efficient low-level C++ code specialized for the given workload and data.  We show that a Scala implementation of IFAQ can outperform mlpack, Scikit, and TensorFlow by several orders of magnitude for linear regression and regression tree models over several relational datasets.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {145–157},
numpages = {13},
keywords = {In-Database Machine Learning, Multi-Query Optimization, Query Compilation},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@article{10.1016/j.comcom.2015.01.006,
author = {Marnerides, A.K. and Malinowski, S. and Morla, R. and Kim, H.S.},
title = {Fault diagnosis in DSL networks using support vector machines},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {62},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2015.01.006},
doi = {10.1016/j.comcom.2015.01.006},
abstract = {The adequate operation for a number of service distribution networks relies on the effective maintenance and fault management of their underlay DSL infrastructure. Thus, new tools are required in order to adequately monitor and further diagnose anomalies that other segments of the DSL network cannot identify due to the pragmatic issues raised by hardware or software misconfigurations. In this work we present a fundamentally new approach for classifying known DSL-level anomalies by exploiting the properties of novelty detection via the employment of one-class Support Vector Machines (SVMs). By virtue of the imbalance residing in the training samples that consequently lead to problematic prediction outcomes when used within two-class formulations, we adopt the properties of one-class classification and construct models for independently identifying and classifying a single type of a DSL-level anomaly. Given the fact that the greater number of the installed Digital Subscriber Line Access Multiplexers (DSLAMs) within the DSL network of a large European ISP were misconfigured, thus unable to accurately flag anomalous events, we utilize as inference solutions the models derived by the one-class SVM formulations built by the known labels as flagged by the much smaller number of correctly configured DSLAMs in the same network in order to aid the classification aspect against the monitored unlabeled events. By reaching an average over 95% on a number of classification accuracy metrics such as precision, recall and F-score we show that one-class SVM classifiers overcome the biased classification outcomes achieved by the traditional two-class formulations and that they may constitute as viable and promising components within the design of future network fault management strategies. In addition, we demonstrate their superiority over commonly used two-class machine learning approaches such as Decision Trees and Bayesian Networks that has been used in the same context within past solutions.},
journal = {Comput. Commun.},
month = may,
pages = {72–84},
numpages = {13},
keywords = {Support vector machines, Supervised learning, One-class classifiers, Network management, DSL anomalies}
}

@inproceedings{10.1145/2701319.2701328,
author = {Bécan, Guillaume and Acher, Mathieu and Jézéquel, Jean-Marc and Menguy, Thomas},
title = {On the Variability Secrets of an Online Video Generator},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701328},
doi = {10.1145/2701319.2701328},
abstract = {We relate an original experience concerning a popular online video service that offers to generate variants of an humorous video. To further the understanding of the generator, we have reverse engineered its general behavior, architecture, as well as its variation points and its configuration space. The reverse engineering also allows us to create a new generator and online configurator that proposes 18 variation points -- instead of only 3 as in the original generator. We explain why and how we have collaborated and are collaborating with the original creators of the video generator. We also highlight how our reverse engineering work represents a threat to the original service and call for further investigating variability-aware security mechanisms.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {96–102},
numpages = {7},
keywords = {Video generator, Software product line, Security, Reverse engineering, Configurator},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {variability, developer study, configuration management and life cycle, Dimensions of software configuration},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1007/s10664-020-09856-1,
author = {Ros, Rasmus and Hammar, Mikael},
title = {Data-driven software design with Constraint Oriented Multi-variate Bandit Optimization (COMBO)},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09856-1},
doi = {10.1007/s10664-020-09856-1},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3841–3872},
numpages = {32},
keywords = {Combinatorial optimization, Multi-armed bandits, Machine learning, A/B testing, Continuous experimentation}
}

@article{10.1016/j.jss.2019.02.028,
author = {Jakubovski Filho, Helson Luiz and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Preference based multi-objective algorithms applied to the variability testing of software product lines},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.028},
doi = {10.1016/j.jss.2019.02.028},
journal = {J. Syst. Softw.},
month = may,
pages = {194–209},
numpages = {16},
keywords = {Preference-Based algorithms, Search-Based software engineering, Software product line testing}
}

@inproceedings{10.1145/3194779.3194786,
author = {Torchiano, Marco and Bruno, Giorgio},
title = {Integrating software engineering key practices into an OOP massive in-classroom course: an experience report},
year = {2018},
isbn = {9781450357500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194779.3194786},
doi = {10.1145/3194779.3194786},
abstract = {Programming and software engineering courses in computer science curricula typically focus on both providing theoretical knowledge of programming languages and best-practices, and developing practical development skills. In a massive course --- several hundred students --- the teachers are not able to adequately attend to the practical part, therefore process automation and incentives to students must be used to drive the students in the right direction.Our goals was to design an automated programming assignment infrastructure capable of supporting massive courses. The infrastructure should encourage students to apply the key software engineering (SE) practices --- automated testing, configuration management, and Integrated Development Environment (IDE) --- and acquire the basic skills for using the corresponding tools.We selected a few widely adopted development tools used to support the key software engineering practices and mapped them to the basic activities in our exam assignment management process.This experience report describes the results from the past academic year. The infrastructure we built has been used for a full academic year and supported four exam sessions for a total of over a thousand students. The satisfaction level reported by the students is generally high.},
booktitle = {Proceedings of the 2nd International Workshop on Software Engineering Education for Millennials},
pages = {64–71},
numpages = {8},
keywords = {object-oriented programming, configuration management, automated testing, automated grading, Java},
location = {Gothenburg, Sweden},
series = {SEEM '18}
}

@inproceedings{10.1145/3377929.3390050,
author = {Saidani, Islem and Ouni, Ali and Chouchen, Moataz and Mkaouer, Mohamed Wiem},
title = {On the prediction of continuous integration build failures using search-based software engineering},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3390050},
doi = {10.1145/3377929.3390050},
abstract = {Continuous Integration (CI) aims at supporting developers in integrating code changes quickly through automated building. However, in such context, the build process is typically time and resource-consuming. As a response, the use of machine learning (ML) techniques has been proposed to cut the expenses of CI build time by predicting its outcome. Nevertheless, the existing ML-based solutions are challenged by problems related mainly to the imbalanced distribution of successful and failed builds. To deal with this issue, we introduce a novel approach based on Multi-Objective Genetic Programming (MOGP) to build a prediction model. Our approach aims at finding the best prediction rules based on two conflicting objective functions to deal with both minority and majority classes. We evaluated our approach on a benchmark of 15,383 builds. The results reveal that our technique outperforms state-of-the-art approaches by providing a better balance between both failed and passed builds.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {313–314},
numpages = {2},
keywords = {search-based software engineering, multi-objective optimization, machine learning, continuous integration, build prediction},
location = {Cancún, Mexico},
series = {GECCO '20}
}

@article{10.1007/s10515-019-00253-7,
author = {Angerer, Florian and Grimmer, Andreas and Prähofer, Herbert and Grünbacher, Paul},
title = {Change impact analysis for maintenance and evolution of variable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00253-7},
doi = {10.1007/s10515-019-00253-7},
abstract = {Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This article presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining the possibly impacted source code elements when changing the source code of a product family. The approach also supports engineers, who are adapting the code of specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanisms, it is more precise than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. We report evaluation results on the benefit and performance of the approach using industrial product lines.},
journal = {Automated Software Engg.},
month = jun,
pages = {417–461},
numpages = {45},
keywords = {Variability, Program analysis, Maintenance, Change impact analysis}
}

@inproceedings{10.5555/3524938.3525699,
author = {Rosenfeld, Nir and Oshiba, Kojin and Singer, Yaron},
title = {Predicting choice with set-dependent aggregation},
year = {2020},
publisher = {JMLR.org},
abstract = {Providing users with alternatives to choose from is an essential component of many online platforms, making the accurate prediction of choice vital to their success. A renewed interest in learning choice models has led to improved modeling power, but most current methods are either limited in the type of choice behavior they capture, cannot be applied to large-scale data, or both.Here we propose a learning framework for predicting choice that is accurate, versatile, and theoretically grounded. Our key modeling point is that to account for how humans choose, predictive models must be expressive enough to accommodate complex choice patterns but structured enough to retain statistical efficiency. Building on recent results in economics, we derive a class of models that achieves this balance, and propose a neural implementation that allows for scalable end-to-end training. Experiments on three large choice datasets demonstrate the utility of our approach.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {761},
numpages = {10},
series = {ICML'20}
}

@inproceedings{10.1007/978-3-030-87007-2_27,
author = {Aladics, Tamás and Jász, Judit and Ferenc, Rudolf},
title = {Bug Prediction Using Source Code Embedding Based on Doc2Vec},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_27},
doi = {10.1007/978-3-030-87007-2_27},
abstract = {Bug prediction is a resource demanding task that is hard to automate using static source code analysis. In many fields of computer science, machine learning has proven to be extremely useful in tasks like this, however, for it to work we need a way to use source code as input. We propose a simple, but meaningful representation for source code based on its abstract syntax tree and the Doc2Vec embedding algorithm. This representation maps the source code to a fixed length vector which can be used for various upstream tasks – one of which is bug prediction. We measured this approach’s validity by itself and its effectiveness compared to bug prediction based solely on code metrics. We also experimented on numerous machine learning approaches to check the connection between different embedding parameters with different machine learning models. Our results show that this representation provides meaningful information as it improves the bug prediction accuracy in most cases, and is always at least as good as only using code metrics as features.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {382–397},
numpages = {16},
keywords = {Doc2Vec, Java, Bug prediction, Code metrics, Source code embedding},
location = {Cagliari, Italy}
}

@article{10.1007/s11063-018-9964-8,
author = {Lee, Younghoon and Chung, Minki and Cho, Sungzoon and Choi, Jinhae},
title = {Extraction of Product Evaluation Factors with a Convolutional Neural Network and Transfer Learning},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {1},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-018-9964-8},
doi = {10.1007/s11063-018-9964-8},
abstract = {Earlier studies have indicated that decision-making by a project development team can be improved throughout the design and development process by understanding the key factors that affect customers evaluations of a new product. Aspect extraction could thus be a useful tool for identifying important attributes when evaluating products or services. Aspect extraction based on deep convolutional neural networks has recently been suggested, demonstrating state-of-the-art performance when applied to a customer review of electronic devices. However, this approach is unsuited to the rapidly evolving smartphone industry, which involves a wide range of product lines. Whereas the previous approach required significant amounts of data labeling for each product, we propose a variant of that approach that includes transfer learning. We also propose a novel approach for transferring the architecture sequentially within the product group. The results indicate that the principal key feature of each product is extracted effectively by the proposed method without having to re-train the entire convolutional neural network model. Furthermore, the proposed method performs better than the previous method for each product line.},
journal = {Neural Process. Lett.},
month = aug,
pages = {149–164},
numpages = {16},
keywords = {Domain adaptation, Off-the-shelf features, Transfer learning, Convolutional neural network, Aspect extraction, Product evaluation factor}
}

@inproceedings{10.1145/3302333.3302344,
author = {Ferreira, Fischer and Diniz, João P. and Silva, Cleiton and Figueiredo, Eduardo},
title = {Testing Tools for Configurable Software Systems: A Review-based Empirical Study},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302344},
doi = {10.1145/3302333.3302344},
abstract = {Configurable software systems are software systems that can be adapted or configured according to a set of features with the goal of increasing reuse and productivity. However, testing configurable systems is very challenging due to the number of configurations to run with each test, leading to a combinatorial explosion in the number of configurations and tests. Currently, several testing techniques and tools have been proposed to deal with this challenge, but their potential practical application remains mostly unexplored. The lack of studies to explore the tools that apply those techniques motivated us to investigate the literature to find testing tools for configurable software systems and to understand how they work. In this paper, we conducted a systematic mapping and identified 34 testing tools for configurable software systems. We first summarized and discussed their main characteristics. We then designed and performed a comparative empirical study of the main sound testing tools found: VarexJ and SPLat. They are considered sound testing techniques because they explore all reachable configurations from a given test. Overall, we observed that VarexJ and SPLat presented distinct results for efficiency while testing the target systems and that, although VarexJ found more errors than SPLat for the majority of the target systems, such result deserves a more in-depth investigation because we expected a higher intersection of errors encountered by them.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {10},
keywords = {Testing Configurable Software Systems, Systematic Mapping Study, Software Product Line},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1007/s10664-021-09966-4,
author = {Tuarob, Suppawong and Assavakamhaenghan, Noppadol and Tanaphantaruk, Waralee and Suwanworaboon, Ponlakit and Hassan, Saeed-Ul and Choetkiertikul, Morakot},
title = {Automatic team recommendation for collaborative software development},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09966-4},
doi = {10.1007/s10664-021-09966-4},
abstract = {In large-scale collaborative software development, building a team of software practitioners can be challenging, mainly due to overloading choices of candidate members to fill in each role. Furthermore, having to understand all members’ diverse backgrounds, and anticipate team compatibility could significantly complicate and attenuate such a team formation process. Current solutions that aim to automatically suggest software practitioners for a task merely target particular roles, such as developers, reviewers, and integrators. While these existing approaches could alleviate issues presented by choice overloading, they fail to address team compatibility while members collaborate. In this paper, we propose RECAST, an intelligent recommendation system that suggests team configurations that satisfy not only the role requirements, but also the necessary technical skills and teamwork compatibility, given task description and a task assignee. Specifically, RECAST uses Max-Logit to intelligently enumerate and rank teams based on the team-fitness scores. Machine learning algorithms are adapted to generate a scoring function that learns from heterogenous features characterizing effective software teams in large-scale collaborative software development. RECAST is evaluated against a state-of-the-art team recommendation algorithm using three well-known open-source software project datasets. The evaluation results are promising, illustrating that our proposed method outperforms the baselines in terms of team recommendation with 646% improvement (MRR) using the exact-match evaluation protocol.},
journal = {Empirical Softw. Engg.},
month = jul,
numpages = {53},
keywords = {Machine learning, Collaborative software development, Team recommendation}
}

@inproceedings{10.1145/99277.99283,
author = {Minsky, Naftaly H. and Rozenshtein, David},
title = {Configuration management by consensus: an application of law-governed systems},
year = {1990},
isbn = {089791418X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/99277.99283},
doi = {10.1145/99277.99283},
abstract = {It is self-evident that if one wants to model and control the cooperative process of software development, one must provide for cooperative decision making. In particular, one should be able to base the decision on whether and how to carry out a given operation on the consensus of several, possibly independent, agents. It is important to emphasize that this is not just a matter of computing the conjunction of some set of conditions. One must also provide a mechanism for establishing any desired consensus structure, which would specify who is allowed to state which kinds of concerns regarding this operation, and what the relationship among these concerns should be.In this paper we propose a general framework for such decision making by consensus, which is based on the concept of law-governed software development. As a concrete application domain in which to illustrate this framework, we consider here the issue of configuration binding.},
booktitle = {Proceedings of the Fourth ACM SIGSOFT Symposium on Software Development Environments},
pages = {44–55},
numpages = {12},
location = {Irvine, California, USA},
series = {SDE 4}
}

@article{10.1155/2021/5558561,
author = {Shao, Yanli and Zhao, Jingru and Wang, Xingqi and Wu, Weiwei and Fang, Jinglong and Gao, Honghao},
title = {Research on Cross-Company Defect Prediction Method to Improve Software Security},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/5558561},
doi = {10.1155/2021/5558561},
abstract = {As the scale and complexity of software increase, software security issues have become the focus of society. Software defect prediction (SDP) is an important means to assist developers in discovering and repairing potential defects that may endanger software security in advance and improving software security and reliability. Currently, cross-project defect prediction (CPDP) and cross-company defect prediction (CCDP) are widely studied to improve the defect prediction performance, but there are still problems such as inconsistent metrics and large differences in data distribution between source and target projects. Therefore, a new CCDP method based on metric matching and sample weight setting is proposed in this study. First, a clustering-based metric matching method is proposed. The multigranularity metric feature vector is extracted to unify the metric dimension while maximally retaining the information contained in the metrics. Then use metric clustering to eliminate metric redundancy and extract representative metrics through principal component analysis (PCA) to support one-to-one metric matching. This strategy not only solves the metric inconsistent and redundancy problem but also transforms the cross-company heterogeneous defect prediction problem into a homogeneous problem. Second, a sample weight setting method is proposed to transform the source data distribution. Wherein the statistical source sample frequency information is set as an impact factor to increase the weight of source samples that are more similar to the target samples, which improves the data distribution similarity between the source and target projects, thereby building a more accurate prediction model. Finally, after the above two-step processing, some classical machine learning methods are applied to build the prediction model, and 12 project datasets in NASA and PROMISE are used for performance comparison. Experimental results prove that the proposed method has superior prediction performance over other mainstream CCDP methods.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {19}
}

@article{10.1016/j.artmed.2021.102165,
author = {de Siqueira, Vilson Soares and Borges, Moisés Marcos and Furtado, Rogério Gomes and Dourado, Colandy Nunes and da Costa, Ronaldo Martins},
title = {Artificial intelligence applied to support medical decisions for the automatic analysis of echocardiogram images: A systematic review},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102165},
doi = {10.1016/j.artmed.2021.102165},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {19},
keywords = {Deep Learning, Machine Learning, Echocardiography, Echocardiogram}
}

@article{10.1016/j.compeleceng.2017.11.002,
author = {AbuZeina, Dia and Al-Anzi, Fawaz S.},
title = {Employing fisher discriminant analysis for Arabic text classification},
year = {2018},
issue_date = {February 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.11.002},
doi = {10.1016/j.compeleceng.2017.11.002},
abstract = {Linear discriminant analysis (LDA) is proposed for Arabic text classification.LDA employs less dimensions, which is helpful for sizable textual feature vectors.Despite that LDA is semantic loss feature reduction method, it shows useful results. Fisher's discriminant analysis; also called linear discriminant analysis (LDA), is a popular dimensionality reduction technique that is widely used for features extraction. LDA aims at finding an optimal linear transformation based on maximizing a class separability. Even though LDA shows useful results in various pattern recognition problems, such as face recognition, less attention has been devoted to employing this technique in Arabic information retrieval tasks. In particular, the sizable feature vectors in textual data enforces to implement dimensionality reduction techniques such as LDA. In this paper, we empirically investigated an LDA based method for Arabic text classification. We used a corpus that contains 2,000 documents belonging to five categories. The experimental results showed that the performance of semantic loss LDA based method was almost the same as the semantic rich singular value decomposition (SVD), and that is indication that LDA is a promising method for text mining applications. Display Omitted},
journal = {Comput. Electr. Eng.},
month = feb,
pages = {474–486},
numpages = {13},
keywords = {Text, Linear discriminant analysis, Fisher, Eigenvectors, Classification, Arabic}
}

@inproceedings{10.1145/3377024.3377041,
author = {Beek, Maurice H. ter and Legay, Axel and Lafuente, Alberto Lluch and Vandin, Andrea},
title = {Variability meets security: qantitative security modeling and analysis of highly customizable attack scenarios},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377041},
doi = {10.1145/3377024.3377041},
abstract = {We present a framework for quantitative security modeling and analysis of highly customizable attack scenarios, which resulted as a spin-off from our research in software product line engineering. The graphical security models are based on attributed attack-defense diagrams to capture the structure and properties of vulnerabilities, defenses and countermeasures---with notable similarities to feature diagrams---and on probabilistic models of attack behavior, capable of capturing resource constraints and attack effectiveness. In this paper, we provide an overview of the framework that is described in full technical detail in twin papers, which present the formal syntax and semantics of the domain-specific language and showcase the associated tool with advanced IDE support for performing analyses based on statistical model checking. The properties of interest range from average cost and success probability of attacks to the effectiveness of defenses and countermeasures. Here we illustrate the capabilities of the DSL and the tool by applying them to an example scenario from the security domain. This shows how techniques from variability modeling can be applied to security. We conclude with a vision and roadmap for future research.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {11},
numpages = {9},
keywords = {variability models, statistical model checking, quantitative security, graphical security models, formal analysis tools, attack-defense trees},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3364641.3364656,
author = {Sousa, Amanda and Uchôa, Anderson and Fernandes, Eduardo and Bezerra, Carla I. M. and Monteiro, José Maria and Andrade, Rossana M. C.},
title = {REM4DSPL: A Requirements Engineering Method for Dynamic Software Product Lines},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364656},
doi = {10.1145/3364641.3364656},
abstract = {Context: Dynamic Software Product Line (DSPL) is a set of software products capable of self-adapt and configure in runtime. DSPL products have common features (commonalities) and varying features (managed in runtime according to context changes). Objective: DSPL requirements engineering is challenging. Requirements engineers have to carefully plan self-adaptation while eliciting, modeling, and managing variability requirements. This paper introduces a method for DSPL requirements engineering. Method: We relied on empirically-derived activities of DSPL requirements engineering to build our method. We selected techniques and templates used in other domains such as SPL for refinement and incorporation into the method. We asked DSPL experts via a survey on the method applicability. Result: We introduced the Requirements Engineering Method for DSPL (REM4DSPL). Elicitation is guided by supervised discussions. Modeling relies on feature models. Variability Management is tool-assisted and validated via feature model inspection. DSPL experts agreed on the method applicability and suggested improvements. Conclusion: REM4DSPL relies on empirically-derived activities, techniques that have been successfully used by previous work, and templates adapted to the DSPL context. We expect our method to guide requirements engineers in practice.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {129–138},
numpages = {10},
keywords = {Requirements Engineering, Dynamic Software Product Lines},
location = {Fortaleza, Brazil},
series = {SBQS '19}
}

@inproceedings{10.1145/3383972.3384014,
author = {Lee, Da-Young and Ko, Uram and Aitkazin, Ibrahim and Park, SangUn and Tak, Hae-Sung and Cho, Hwan-Gue},
title = {A Fast Detecting Method for Clone Functions Using Global Alignment of Token Sequences},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384014},
doi = {10.1145/3383972.3384014},
abstract = {In large software projects, proper source code reuse can make development more efficient, but a lot of duplicate code and error code reuse can be a major cause of difficult system maintenance. Efficient clone code detection for large project can help manage the project. However, most of the clone detection methods are difficult to perform on adaptive analysis that adjusts specificity or sensitivity according to the type of clone to be detected. Therefore, when a user wants to find a particular type of clone in a large project, they must analyze it repeatedly using various tools to adjust the options. In this study, we propose a clone detection system based on the global sequence alignment. Lex based token analysis models and global alignment algorithm-based clone detection models were able to detect not only exact matches but also various types of clones by setting lower bound scores. Using features of the global alignment score calculation method to eliminate functions that cannot be clone candidates in advance, alignment analysis was possible even for large projects, and the execution time was predicted. For clone functions, we visualized the matching area, which is the result of alignment analysis, to represent clone information more efficiently.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {17–22},
numpages = {6},
keywords = {global alignment, code analysis, clone function, Clone detection},
location = {Shenzhen, China},
series = {ICMLC '20}
}

@inproceedings{10.1145/3001867.3001876,
author = {Krüger, Jacob and Schröter, Ivonne and Kenner, Andy and Kruczek, Christopher and Leich, Thomas},
title = {FeatureCoPP: compositional annotations},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001876},
doi = {10.1145/3001867.3001876},
abstract = {Software product lines can be implemented with different techniques. Those techniques can be grouped into annotation-based and composition-based approaches, with complementary strengths and weaknesses. A combination seems useful to utilize benefits of both groups but using two techniques in parallel may cause new problems. To our knowledge, there is no approach that integrates composition into an annotation-based approach or vice versa. We propose the use of an extended preprocessor to introduce physical separation of concerns similar to feature-oriented programming. In this paper, we i) present a preliminary implementation that integrates composition into annotation, ii) analyse its benefits and shortcomings, and iii) discuss implementation and tooling. Overall, we enable developers to keep on using familiar preprocessors but also to benefit from composition. Finally, we show the potential of our approach.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {74–84},
numpages = {11},
keywords = {software product line, preprocessor, feature orientation, composition, annotation},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.5555/1030453.1030477,
author = {Rathmell, Judy and Sturrock, David T.},
title = {Arena: the arena product family: enterprise modeling solutions},
year = {2002},
isbn = {0780376153},
publisher = {Winter Simulation Conference},
abstract = {This paper introduces the Arena suite of products for modeling, simulation, and optimization highlighting product architecture and technology features that are targeted toward successful deployment of simulation and Arena throughout an enterprise.},
booktitle = {Proceedings of the 34th Conference on Winter Simulation: Exploring New Frontiers},
pages = {165–172},
numpages = {8},
location = {San Diego, California},
series = {WSC '02}
}

@article{10.1007/s11390-021-0235-1,
author = {Zhang, Jing-Xuan and Tao, Chuan-Qi and Huang, Zhi-Qiu and Chen, Xin},
title = {Discovering API Directives from API Specifications with Text Classification},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {36},
number = {4},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-021-0235-1},
doi = {10.1007/s11390-021-0235-1},
abstract = {Application programming interface (API) libraries are extensively used by developers. To correctly program with APIs and avoid bugs, developers shall pay attention to API directives, which illustrate the constraints of APIs. Unfortunately, API directives usually have diverse morphologies, making it time-consuming and error-prone for developers to discover all the relevant API directives. In this paper, we propose an approach leveraging text classification to discover API directives from API specifications. Specifically, given a set of training sentences in API specifications, our approach first characterizes each sentence by three groups of features. Then, to deal with the unequal distribution between API directives and non-directives, our approach employs an under-sampling strategy to split the imbalanced training set into several subsets and trains several classifiers. Given a new sentence in an API specification, our approach synthesizes the trained classifiers to predict whether it is an API directive. We have evaluated our approach over a publicly available annotated API directive corpus. The experimental results reveal that our approach achieves an F-measure value of up to 82.08%. In addition, our approach statistically outperforms the state-of-the-art approach by up to 29.67% in terms of F-measure.},
journal = {J. Comput. Sci. Technol.},
month = jul,
pages = {922–943},
numpages = {22},
keywords = {text classification, imbalanced learning, API specification, Application programming interface (API) directive}
}

@inproceedings{10.5555/3524938.3525116,
author = {Choo, Davin and Grunau, Christoph and Portmann, Julian and Rozhoň, Václav},
title = {k-means++: few more steps yield constant approximation},
year = {2020},
publisher = {JMLR.org},
abstract = {The k-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is a state-of-the-art algorithm for solving the k-means clustering problem and is known to give an O(log k)-approximation in expectation. Recently, Lattanzi and Sohler (ICML 2019) proposed augmenting k-means++ with O(k log log k) local search steps to yield a constant approximation (in expectation) to the k-means clustering problem. In this paper, we improve their analysis to show that, for any arbitrarily small constant ε &gt; 0, with only εk additional local search steps, one can achieve a constant approximation guarantee (with high probability in k), resolving an open problem in their paper.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {178},
numpages = {9},
series = {ICML'20}
}

@inproceedings{10.1145/3344948.3344964,
author = {Kerdoudi, Mohamed Lamine and Ziadi, Tewfik and Tibermacine, Chouki and Sadou, Salah},
title = {A bottom-up approach for reconstructing software architecture product lines},
year = {2019},
isbn = {9781450371421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344948.3344964},
doi = {10.1145/3344948.3344964},
abstract = {A large component and service-based software system exists in different forms, as different variants targeting different business needs and users. This kind of systems is provided as a set of "independent" products and not as a "single whole". The presence of a single model describing the architecture of the whole system may be of great interest for developers of future variants. Indeed, this enables them to see the invariant part of the whole, on top of which new functionality can be built, in addition to the different options they can use. We investigate in this work the use of software product line reverse engineering approaches, and in particular the framework named BUT4Reuse, for reconstructing an architecture model of a Software Architecture Product Line (SAPL), from a set of variants. We propose a generic process for reconstructing an architecture model of such a product line. We have instantiated this process for the OSGi Java framework and experimented it for building the architecture model of Eclipse IDE SPL.},
booktitle = {Proceedings of the 13th European Conference on Software Architecture - Volume 2},
pages = {46–49},
numpages = {4},
location = {Paris, France},
series = {ECSA '19}
}

@article{10.1007/s00500-021-05934-8,
author = {Huang, Xuan and Hu, Zhenlong and Lin, Lin},
title = {RETRACTED ARTICLE: Deep clustering based on embedded auto-encoder},
year = {2021},
issue_date = {Jan 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05934-8},
doi = {10.1007/s00500-021-05934-8},
abstract = {Deep clustering is a new research direction that combines deep learning and clustering. It performs feature representation and cluster assignments simultaneously, and its clustering performance is significantly superior to traditional clustering algorithms. The auto-encoder is a neural network model, which can learn the hidden features of the input object to achieve nonlinear dimensionality reduction. This paper proposes the embedded auto-encoder network model; specifically, the auto-encoder is embedded into the encoder unit and the decoder unit of the prototype auto-encoder, respectively. To ensure effectively cluster high-dimensional objects, the encoder of model first encodes the raw features of the input objects, and obtains a cluster-friendly feature representation. Then, in the model training stage, by adding smoothness constraints to the objective function of the encoder, the representation capabilities of the hidden layer coding are significantly improved. Finally, the adaptive self-paced learning threshold is determined according to the median distance between the object and its corresponding the centroid, and the fine-tuning sample of the model is automatically selected. Experimental results on multiple image datasets have shown that our model has fewer parameters, higher efficiency and the comprehensive clustering performance is significantly superior to the state-of-the-art clustering methods.},
journal = {Soft Comput.},
month = jun,
pages = {1075–1090},
numpages = {16},
keywords = {Deep clustering, The embedded auto-encoder, Feature representation}
}

@inproceedings{10.1145/3078971.3079003,
author = {Liang, Junwei and Jiang, Lu and Meng, Deyu and Hauptmann, Alexander},
title = {Leveraging Multi-modal Prior Knowledge for Large-scale Concept Learning in Noisy Web Data},
year = {2017},
isbn = {9781450347013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078971.3079003},
doi = {10.1145/3078971.3079003},
abstract = {Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web is associated with rich but noisy contextual information, such as the title and other multi-modal information, which provides weak annotations or labels about the video content. To tackle the problem of large-scale noisy learning, We propose a novel method called Multi-modal WEbly-Labeled Learning (WELL-MM), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human. WELL-MM introduces a novel multi-modal approach to incorporate meaningful prior knowledge called curriculum from the noisy web videos. We empirically study the curriculum constructed from the multi-modal features of the Internet videos and images. The comprehensive experimental results on FCVID and YFCC100M demonstrate that WELL-MM outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data. In addition, the results also verify that WELL-MM is robust to the level of noisiness in the video data. Notably, WELL-MM trained on sufficient noisy web labels is able to achieve a better accuracy to supervised learning methods trained on the clean manually labeled data.},
booktitle = {Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
pages = {32–40},
numpages = {9},
keywords = {webly-supervised learning, web label, video understanding, prior knowledge, noisy data, concept detection, big data},
location = {Bucharest, Romania},
series = {ICMR '17}
}

@inproceedings{10.1145/3001867.3001870,
author = {Ananieva, Sofia and Kowal, Matthias and Thüm, Thomas and Schaefer, Ina},
title = {Implicit constraints in partial feature models},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001870},
doi = {10.1145/3001867.3001870},
abstract = {Developing and maintaining a feature model is a tedious process and gets increasingly difficult with regard to large product lines consisting of thousands of features and constraints. In addition, these large-scale feature models typically involve several stakeholders from different domains during development and maintenance. We aim at supporting such stakeholders by deriving and explaining implicit constraints for partial feature models. A partial feature model can either be a submodel of a feature model representing the full product line or a specific feature model in a set of interrelated models. For every implicit constraint, we generate an explanation exposing which other model parts and constraints interfere with the partial model of interest. Thus, stakeholders are only confronted with a small part of the feature model reducing the complexity while preserving the necessary information about dependencies. Our approach is implemented in the open-source framework FeatureIDE.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {18–27},
numpages = {10},
keywords = {Feature Model, Configurable Software},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Systematic mapping study, Software product line, Search based software engineering, Metaheuristics, Evolutionary algorithm}
}

@inproceedings{10.1109/ICSE43902.2021.00099,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-Box Performance-Influence Models: A Profiling and Learning Approach},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00099},
doi = {10.1109/ICSE43902.2021.00099},
abstract = {Many modern software systems are highly configurable, allowing the user to tune them for performance and more. Current performance modeling approaches aim at finding performance-optimal configurations by building performance models in a black-box manner. While these models provide accurate estimates, they cannot pinpoint causes of observed performance behavior to specific code regions. This does not only hinder system understanding, but it also complicates tracing the influence of configuration options to individual methods.We propose a white-box approach that models configuration-dependent performance behavior at the method level. This allows us to predict the influence of configuration decisions on individual methods, supporting system understanding and performance debugging. The approach consists of two steps: First, we use a coarse-grained profiler and learn performance-influence models for all methods, potentially identifying some methods that are highly configuration- and performance-sensitive, causing inaccurate predictions. Second, we re-measure these methods with a fine-grained profiler and learn more accurate models, at higher cost, though. By means of 9 real-world Java software systems, we demonstrate that our approach can efficiently identify configuration-relevant methods and learn accurate performance-influence models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1059–1071},
numpages = {13},
keywords = {software variability, software product lines, performance, Configuration management},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1109/TSE.2007.70773,
author = {Kim, Sunghun and Whitehead, E. James and Zhang, Yi},
title = {Classifying Software Changes: Clean or Buggy?},
year = {2008},
issue_date = {March 2008},
publisher = {IEEE Press},
volume = {34},
number = {2},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2007.70773},
doi = {10.1109/TSE.2007.70773},
abstract = {This paper introduces a new technique for finding latent software bugs called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes, or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project, as stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean with 78% accuracy and 65% buggy change recall (on average). Change classification has several desirable qualities: (1) the prediction granularity is small (a change to a single file), (2) predictions do not require semantic information about the source code, (3) the technique works for a broad array of project types and programming languages, and (4) predictions can be made immediately upon completion of a change. Contributions of the paper include a description of the change classification approach, techniques for extracting features from source code and change histories, a characterization of the performance of change classification across 12 open source projects, and evaluation of the predictive power of different groups of features.},
journal = {IEEE Trans. Softw. Eng.},
month = mar,
pages = {181–196},
numpages = {16},
keywords = {classification, and association rules, Software maintenance, Metrics/Measurement, Data mining, Configuration Management, Clustering}
}

@article{10.1016/j.jss.2007.06.002,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Industrial validation of COVAMOF},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.06.002},
doi = {10.1016/j.jss.2007.06.002},
abstract = {COVAMOF is a variability management framework for product families that was developed to reduce the number of iterations required during product derivation and to reduce the dependency on experts. In this paper, we present the results of an experiment with COVAMOF in industry. The results show that with COVAMOF, engineers that are not involved in the product family were now capable of deriving the products in 100% of the cases, compared to 29% of the cases without COVAMOF. For experts, the use of COVAMOF reduced the number of iterations by 42%, and the total derivation time by 38%.},
journal = {J. Syst. Softw.},
month = apr,
pages = {584–600},
numpages = {17},
keywords = {Software Variability Management, Product family engineering, Industrial validation}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00026,
author = {Li, Paul Luo and Chai, Xiaoyu and Campbell, Frederick and Liao, Jilong and Abburu, Neeraja and Kang, Minsuk and Niculescu, Irina and Brake, Greg and Patil, Siddharth and Dooley, James and Paddock, Brandon},
title = {Evolving software to be ML-driven utilizing real-world A/B testing: experiences, insights, challenges},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00026},
doi = {10.1109/ICSE-SEIP52600.2021.00026},
abstract = {ML-driven software is heralded as the next major advancement in software engineering; existing software today can benefit from being evolved to be ML-driven. In this paper, we contribute practical knowledge about evolving software to be ML-driven, utilizing real-world A/B testing. We draw on experiences evolving two software features from the Windows operating system to be ML-driven, with more than ten realworld A/B tests on millions of PCs over more than two years. We discuss practical reasons for using A/B testing to engineer ML-driven software, insights for success, as well as on-going realworld challenges. This knowledge may help practitioners, as well as help direct future research and innovations.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {170–179},
numpages = {10},
keywords = {software quality, software engineering, software development management, predictive models, machine learning algorithms, machine learning, learning (artificial intelligence), data analysis, big data applications},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1007/978-3-030-32047-8_26,
author = {Khoshmanesh, Seyedehzahra and Lutz, Robyn R.},
title = {Leveraging Feature Similarity for Earlier Detection of Unwanted Feature Interactions in Evolving Software Product Lines},
year = {2019},
isbn = {978-3-030-32046-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32047-8_26},
doi = {10.1007/978-3-030-32047-8_26},
abstract = {Software product lines enable reuse of shared software across a family of products. As new products are built in the product line, new features are added. The features are units of functionality that provide services to users. Unwanted feature interactions, wherein one feature interferes with another feature’s operation, is a significant problem, especially as large software product lines evolve. Detecting feature interactions is a time-consuming and difficult task for developers. Moreover, feature interactions are often only discovered during testing, at which point costly re-work is needed. This paper proposes a similarity-based method to identify unwanted feature interactions much earlier in the development process. It uses knowledge of prior feature interactions stored with the software product line’s feature model to help find unwanted interactions between a new feature and existing features. The paper describes the framework and algorithms used to detect the feature interactions using three path similarity measures and evaluates the approach on a real-world, evolving software product line. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.},
booktitle = {Similarity Search and Applications: 12th International Conference, SISAP 2019, Newark, NJ, USA, October 2–4, 2019, Proceedings},
pages = {293–307},
numpages = {15},
keywords = {Feature interaction, Similarity measures, Software product lines},
location = {Newark, NJ, USA}
}

@inproceedings{10.5555/564124.564151,
author = {Swets, Roderick J. and Drake, Glenn R.},
title = {Arena: the Arena product family: enterprise modeling solutions},
year = {2001},
isbn = {078037309X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper introduces the Arena suite of products for modeling, simulation, and optimization highlighting product architecture and technology features that are targeted toward successful deployment of simulation and Arena throughout an enterprise.},
booktitle = {Proceedings of the 33nd Conference on Winter Simulation},
pages = {201–208},
numpages = {8},
location = {Arlington, Virginia},
series = {WSC '01}
}

@inproceedings{10.1007/978-3-030-64583-0_47,
author = {Fortez, Giovanna and Robledo, Franco and Romero, Pablo and Viera, Omar},
title = {A Fast Genetic Algorithm for the Max Cut-Clique Problem},
year = {2020},
isbn = {978-3-030-64582-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64583-0_47},
doi = {10.1007/978-3-030-64583-0_47},
abstract = {In Marketing, the goal is to understand the psychology of the customer in order to maximize sales. A common approach is to combine web semantic, sniffing, historical information of the customer, and machine learning techniques.In this paper, we exploit the historical information of sales in order to assist product placement. The rationale is simple: if two items are sold jointly, they should be close. This concept is formalized in a combinatorial optimization problem, called Max Cut-Clique or MCC for short.The hardness of the MCC promotes the development of heuristics. The literature offers a GRASP/VND methodology as well as an Iterated Local Search (ILS) implementation. In this work, a novel Genetic Algorithm is proposed to deal with the MCC. A comparison with respect to previous heuristics reveals that our proposal is competitive with state-of-the-art solutions.},
booktitle = {Machine Learning, Optimization, and Data Science: 6th International Conference, LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part I},
pages = {528–539},
numpages = {12},
keywords = {Metaheuristics, Max cut-clique, Combinatorial optimization problem, Marketing},
location = {Siena, Italy}
}

@article{10.2478/cait-2020-0018,
author = {Angara, Jayasri and Prasad, Srinivas and Sridevi, Gutta},
title = {DevOps Project Management Tools for Sprint Planning, Estimation and Execution Maturity},
year = {2020},
issue_date = {Jun 2020},
publisher = {Walter de Gruyter GmbH},
address = {Berlin, DEU},
volume = {20},
number = {2},
issn = {1314-4081},
url = {https://doi.org/10.2478/cait-2020-0018},
doi = {10.2478/cait-2020-0018},
abstract = {The goal of DevOps is to cut down the project timelines, increase the productivity, and manage rapid development-deployment cycles without impacting business and quality. It requires efficient sprint management. The objective of this paper is to develop different sprint level project management tools for quick project level Go/No-Go decision making (using real-time projects data and machine learning), sprint estimation technique (gamified-consensus based), statistical understanding of overall project management maturity, project sentiment &amp; perception. An attempt is made to device a model to calibrate the perception or the tone of a project culture using sentiment analysis.},
journal = {Cybern. Inf. Technol.},
month = jun,
pages = {79–92},
numpages = {14},
keywords = {sentimental analysis, planning poker, effort estimation, Machine Learning (ML), DevOps}
}

@article{10.1016/j.infsof.2019.106241,
author = {Perkusich, Mirko and Chaves e Silva, Lenardo and Costa, Alexandre and Ramos, Felipe and Saraiva, Renata and Freire, Arthur and Dilorenzo, Ednaldo and Dantas, Emanuel and Santos, Danilo and Gorgônio, Kyller and Almeida, Hyggo and Perkusich, Angelo},
title = {Intelligent software engineering in the context of agile software development: A systematic literature review},
year = {2020},
issue_date = {Mar 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {119},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106241},
doi = {10.1016/j.infsof.2019.106241},
journal = {Inf. Softw. Technol.},
month = mar,
numpages = {19},
keywords = {Artificial intelligence, Bayesian networks, Machine learning, Search-based software engineering, Agile software development, Intelligent software engineering}
}

@article{10.1016/j.eswa.2014.12.040,
author = {Fossaceca, John M. and Mazzuchi, Thomas A. and Sarkani, Shahram},
title = {MARK-ELM},
year = {2015},
issue_date = {May 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.12.040},
doi = {10.1016/j.eswa.2014.12.040},
abstract = {Apply Multiple Kernel Boosting and Multiclass KELM to Network Intrusion Detection.Tested approach on several machine learning datasets and the KDD Cup 99 dataset.Utilized Fractional Polynomial Kernels for the Network ID problem for the first time.Requires no feature selection, minimal pre-processing and works on imbalanced data.Achieves superior detection rates and lower false alarm rates than other approaches. Detection of cyber-based attacks on computer networks continues to be a relevant and challenging area of research. Daily reports of incidents appear in public media including major ex-filtrations of data for the purposes of stealing identities, credit card numbers, and intellectual property as well as to take control of network resources. Methods used by attackers constantly change in order to defeat techniques employed by information technology (IT) teams intended to discover or block intrusions. "Zero Day" attacks whose "signatures" are not yet in IT databases are continually being uncovered. Machine learning approaches have been widely used to increase the effectiveness of intrusion detection platforms. While some machine learning techniques are effective at detecting certain types of attacks, there are no known methods that can be applied universally and achieve consistent results for multiple attack types. The focus of our research is the development of a framework that combines the outputs of multiple learners in order to improve the efficacy of network intrusion on data that contains instances of multiple classes of attacks. We have chosen the Extreme Learning Machine (ELM) as the core learning algorithm due to recent research that suggests that ELMs are straightforward to implement, computationally efficient and have excellent learning performance characteristics on par with the Support Vector Machine (SVM), one of the most widely used and best performing machine learning platforms (Liu, Gao, &amp; Li, 2012). We introduce the novel Multiple Adaptive Reduced Kernel Extreme Learning Machine (MARK-ELM) which combines Multiple Kernel Boosting (Xia &amp; Hoi, 2013) with the Multiple Classification Reduced Kernel ELM (Deng, Zheng, &amp; Zhang, 2013). We tested this approach on several machine learning datasets as well as the KDD Cup 99 (Hettich &amp; Bay, 1999) intrusion detection dataset. Our results indicate that MARK-ELM works well for the majority of University of California, Irvine (UCI) Machine Learning Repository small datasets and is scalable for larger datasets. For UCI datasets we achieved performance similar to the MKBoost Support Vector Machine (SVM) approach. In our experiments we demonstrate that MARK-ELM achieves superior detection rates and much lower false alarm rates than other approaches on intrusion detection data.},
journal = {Expert Syst. Appl.},
month = may,
pages = {4062–4080},
numpages = {19},
keywords = {Network Intrusion Detection, Multiple Kernel Learning, Multiclass Classification, Machine Learning, Kernel Selection, KDD Cup 1999, Fractional Polynomial Kernels, Extreme Learning Machine, Ensemble Learning, Cyber security, Adaptive Boosting}
}

@article{10.1007/s10586-017-1108-9,
author = {Ilavarasi, A. K. and Sathiyabhama, B.},
title = {An evolutionary feature set decomposition based anonymization for classification workloads: Privacy Preserving Data Mining},
year = {2017},
issue_date = {Dec 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-017-1108-9},
doi = {10.1007/s10586-017-1108-9},
abstract = {Privacy has become an important concern while publishing micro data about a population. The emerging area called privacy preserving data mining (PPDM) focus on individual privacy without compromising data mining results. An adversarial exploitation of published data poses a risk of information disclosure about individuals. On the other hand, imposing privacy constraints on the data results in substantial information loss and compromises the legitimate data analysis. Motivated by the increasing growth of PPDM algorithms, we first investigate the privacy implications and the crosscutting issues between privacy versus utility of data. We present a privacy model that embeds the anonymization procedure in to a learning algorithm and this has mitigated the additional overheads imposed on data mining tasks. Our primary concern about PPDM is that the utility of data should not be compromised by the transformation applied. Different data mining classification workloads are analyzed with the proposed anonymization procedure for any side effects incurred. It is shown empirically that classification accuracy obtained for most of the datasets outperforms the results obtained with original dataset.},
journal = {Cluster Computing},
month = dec,
pages = {3515–3525},
numpages = {11},
keywords = {Privacy, Evolutionary partitioning, Decomposition, Data mining, Classification, Anonymization}
}

@inproceedings{10.5555/3524938.3525166,
author = {Degenne, Rémy and Shao, Han and Koolen, Wouter M.},
title = {Structure adaptive algorithms for stochastic bandits},
year = {2020},
publisher = {JMLR.org},
abstract = {We study reward maximisation in a wide class of structured stochastic multi-armed bandit problems, where the mean rewards of arms satisfy some given structural constraints, e.g. linear, unimodal, sparse, etc. Our aim is to develop methods that are flexible (in that they easily adapt to different structures), powerful (in that they perform well empirically and/or provably match instance-dependent lower bounds) and efficient in that the per-round computational burden is small. We develop asymptotically optimal algorithms from instance-dependent lower-bounds using iterative saddle-point solvers. Our approach generalises recent iterative methods for pure exploration to reward maximisation, where a major challenge arises from the estimation of the suboptimality gaps and their reciprocals. Still we manage to achieve all the above desiderata. Notably, our technique avoids the computational cost of the full-blown saddle point oracle employed by previous work, while at the same time enabling finite-time regret bounds. Our experiments reveal that our method successfully leverages the structural assumptions, while its regret is at worst comparable to that of vanilla UCB.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {228},
numpages = {10},
series = {ICML'20}
}

@article{10.1287/inte.2020.1064,
author = {Bogojeska, Jasmina and Giurgiu, Ioana and Stark, George and Wiesmann, Dorothea},
title = {IBM Predictive Analytics Reduces Server Downtime},
year = {2021},
issue_date = {January-February 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {51},
number = {1},
issn = {0092-2102},
url = {https://doi.org/10.1287/inte.2020.1064},
doi = {10.1287/inte.2020.1064},
abstract = {IBM has deployed its Predictive Analytics for Server Incident Reduction (PASIR) solution to more than 360 information technology (IT) environments worldwide since 2013. These environments, covering sectors from banking to travel to e-commerce, are serviced by IBM support groups. Incidents occurring on servers, including problem descriptions and resolutions, are documented in client account-specific ticket management systems. PASIR uses machine learning to classify the incident tickets within an IT environment and identify high-impact incidents that involve server outages by using the respective ticket descriptions and resolutions. It then correlates these high-impact tickets with server properties and utilization measurements to identify problematic server configurations. Finally, for such configurations, PASIR uses statistical multivariate analysis and simulation methods to prescribe improvement and modernization actions. In this paper, we present the results achieved from deploying this solution. We describe the PASIR approach, from ticket classification to the recommendations of remediation actions (e.g., hardware and software upgrades). We demonstrate the model’s effectiveness by comparing predictions on the impact of prescriptive actions with actual system improvements. Since 2013, we have applied PASIR to more than 840,000 client servers, resulting in more precise upgrade spending and environmental stability, thus saving our clients an estimated $7 billion.},
journal = {Interfaces},
month = feb,
pages = {63–75},
numpages = {13},
keywords = {Edelman Award, text classification, random forest, gradient boosting machine, machine learning, predictive analytics}
}

@inproceedings{10.1007/978-3-030-67658-2_34,
author = {Yamaguchi, Akihiro and Maya, Shigeru and Ueno, Ken},
title = {RLTS: Robust Learning Time-Series Shapelets},
year = {2020},
isbn = {978-3-030-67657-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67658-2_34},
doi = {10.1007/978-3-030-67658-2_34},
abstract = {Shapelets are time-series segments effective for classifying time-series instances. Joint learning of both classifiers and shapelets has been studied in recent years because such a method provides both superior classification performance and interpretable results. For robust learning, we introduce Self-Paced Learning (SPL) and adaptive robust losses into this method. The SPL method can assign latent instance weights by considering not only classification losses but also understandable shapelet discovery. Furthermore, the adaptive robustness introduced into feature vectors is jointly learned with shapelets, a classifier, and latent instance weights. We demonstrate the superiority of AUC and the validity of our approach on UCR time-series datasets.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part I},
pages = {595–611},
numpages = {17},
keywords = {Time-series shapelets, Self-paced learning, Robust losses},
location = {Ghent, Belgium}
}

@inproceedings{10.1145/3302333.3302343,
author = {Cruz, Daniel and Figueiredo, Eduardo and Martinez, Jabier},
title = {A Literature Review and Comparison of Three Feature Location Techniques using ArgoUML-SPL},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302343},
doi = {10.1145/3302333.3302343},
abstract = {Over the last decades, the adoption of Software Product Line (SPL) engineering for supporting software reuse has increased. An SPL can be extracted from one single product or from a family of related software products, and feature location strategies are widely used for variability mining. Several feature location strategies have been proposed in the literature and they usually aim to map a feature to its source code implementation. In this paper, we present a systematic literature review that identifies and characterizes existing feature location strategies. We also evaluated three different strategies based on textual information retrieval in the context of the ArgoUML-SPL feature location case study. In this evaluation, we compare the strategies based on their ability to correctly identify the source code of several features from ArgoUML-SPL ground truth. We then discuss the strengths and weaknesses of each feature location strategy.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {10},
keywords = {variability mining, software product lines, reverse engineering, feature location, benchmark},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/1858996.1859010,
author = {Berger, Thorsten and She, Steven and Lotufo, Rafael and Wąsowski, Andrzej and Czarnecki, Krzysztof},
title = {Variability modeling in the real: a perspective from the operating systems domain},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859010},
doi = {10.1145/1858996.1859010},
abstract = {Variability models represent the common and variable features of products in a product line. Several variability modeling languages have been proposed in academia and industry; however, little is known about the practical use of such languages. We study and compare the constructs, semantics, usage and tools of two variability modeling languages, Kconfig and CDL. We provide empirical evidence for the real-world use of the concepts known from variability modeling research. Since variability models provide basis for automated tools (feature dependency checkers and product configurators), we believe that our findings will be of interest to variability modeling language and tool designers.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {73–82},
numpages = {10},
keywords = {variability modeling, product line architectures, feature models, empirical software engineering, configuration},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@article{10.1287/opre.2018.1805,
author = {Keskin, N. Bora and Birge, John R.},
title = {Dynamic Selling Mechanisms for Product Differentiation and Learning},
year = {2019},
issue_date = {July-August 2019},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {67},
number = {4},
issn = {0030-364X},
url = {https://doi.org/10.1287/opre.2018.1805},
doi = {10.1287/opre.2018.1805},
abstract = {Changing regulations, macroeconomic factors, and technological innovations cause substantial uncertainty in the cost of providing services. In their study “Dynamic Selling Mechanisms for Product Differentiation and Learning,” N. Bora Keskin and John R. Birge analyze the design of vertically differentiated product lines in the face of cost uncertainty. Keskin and Birge show that, although cost uncertainty makes a seller shrink its product line, the opportunity to learn about costs encourages expanding the product line. Keskin and Birge also show that imposing minimum quality standards on the product line can facilitate learning and consequently, yield good profit performance.We consider a firm that designs a vertically differentiated product line for a population of customers with heterogeneous quality sensitivities. The firm faces an uncertainty about the cost of quality, and we formulate this uncertainty as a belief distribution on a set of cost models. Over a time horizon of T periods, the firm can dynamically adjust its menu and make noisy observations on the underlying cost model through customers’ purchasing decisions. We characterize how optimal product differentiation depends on the “informativeness” of quality choices, formally measured by a contrast-to-noise ratio defined on the firm’s feasible quality set. Based on this, we design a minimum quality standard (MQS) policy that mimics the salient features of the optimal product differentiation policy and prove that the MQS policy is near-optimal. We also prove that, if there exists a certain continuum of informative quality choices, then even a myopic policy that makes no attempt to learn exhibits near-optimal profit performance. This stands in stark contrast to the poor performance of myopic policies in pricing and learning problems in the absence of product differentiation. Finally, we extend our results to the case where the firm simultaneously learns the customers’ quality-sensitivity distribution as well as the cost model.},
journal = {Oper. Res.},
month = jul,
pages = {1069–1089},
numpages = {21},
keywords = {exploration-exploitation, Bayesian learning, self-selection, dynamic programming}
}

@inproceedings{10.1145/3368089.3409721,
author = {Liu, Liu and Isaacman, Sibren and Kremer, Ulrich},
title = {Global cost/quality management across multiple applications},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409721},
doi = {10.1145/3368089.3409721},
abstract = {Approximation is a technique that optimizes the balance between application outcome quality and its resource usage. Trading quality for performance has been investigated for single application scenarios, but not for environments where multiple approximate applications may run concurrently on the same machine, interfering with each other by sharing machine resources. Applying existing, single application techniques to this multi-programming environment may lead to configuration space size explosion, or result in poor overall application quality outcomes.  Our new RAPID-M system is the first cross-application con-figuration management framework. It reduces the problem size by clustering configurations of individual applications into local"similarity buckets". The global cross-applications configuration selection is based on these local bucket spaces. RAPID-M dynamically assigns buckets to applications such that overall quality is maximized while respecting individual application cost budgets.Once assigned a bucket, reconfigurations within buckets may be performed locally with minimal impact on global selections. Experimental results using six configurable applications show that even large configuration spaces of complex applications can be clustered into a small number of buckets, resulting in search space size reductions of up to 9 orders of magnitude for our six applications. RAPID-M constructs performance cost models with an average prediction error of ≤3%. For our application execution traces, RAPID-M dynamically selects configurations that lower the budget violation rate by 33.9% with an average budget exceeding rate of 6.6% as compared to other possible approaches. RAPID-M successfully finishes 22.75% more executions which translates to a 1.52X global output quality increase under high system loads. Theo verhead ofRAPID-Mis within≤1% of application execution times.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {350–361},
numpages = {12},
keywords = {Performance Prediction, Multi-Programming, Global Configuration Management, Approximate Computing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1145/182063.182067,
author = {Perez, R. A. and Lilkendey, J. T. and Koh, S. W.},
title = {Machine learning for a dynamic manufacturing environment},
year = {1994},
issue_date = {Feb. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1558-1144},
url = {https://doi.org/10.1145/182063.182067},
doi = {10.1145/182063.182067},
abstract = {Within the semiconductor wafer manufacturing process, tight quality control is of utmost importance. This is due to such factors as the highly competitive nature of the business [2] and the complex nature of the process itself. Therefore, it is imperative that processing problems are recognized and corrected as quickly as possible. To accomplish this, a Parametric Test facility exists where a critical quality check is performed on the wafers. This is accomplished by measuring a number of electrical parameters at existing test sites on the wafer. Each measurement must fall within an acceptable range of values for its associated parameter to pass the critical check. If test results indicate that a parameter is outside of its acceptable range, the wafer may fail the quality check and be scrapped. If this happens an expert must examine all the parametric data associated with that wafer and attempt to determine the reason for failure and where in the manufacturing process the problem may have occurred.},
journal = {SIGICE Bull.},
month = feb,
pages = {5–9},
numpages = {5}
}

@article{10.1016/0164-1212(81)90011-X,
author = {Raveling, Jerry},
title = {Status and outlook for DoD configuration management requirements},
year = {1981},
issue_date = {December, 1981},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {2},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/0164-1212(81)90011-X},
doi = {10.1016/0164-1212(81)90011-X},
abstract = {Since the release of Department of Defense (DoD) Directive 5000.29, ''Management of Computer Resources in Major Defense Systems,'' 26 April 1976, a significant increase in interest and emphasis on software configuration management has occurred. A review of the current status of configuration management within the DoD computer resources management environment is provided, and some predictions on future configuration management activities are offered.},
journal = {J. Syst. Softw.},
month = dec,
pages = {363–370},
numpages = {8}
}

@inproceedings{10.1109/ASE.2019.00052,
author = {Horton, Eric and Parnin, Chris},
title = {V2: fast detection of configuration drift in Python},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00052},
doi = {10.1109/ASE.2019.00052},
abstract = {Code snippets are prevalent, but are hard to reuse because they often lack an accompanying environment configuration. Most are not actively maintained, allowing for drift between the most recent possible configuration and the code snippet as the snippet becomes out-of-date over time. Recent work has identified the problem of validating and detecting out-of-date code snippets as the most important consideration for code reuse. However, determining if a snippet is correct, but simply out-of-date, is a non-trivial task. In the best case, breaking changes are well documented, allowing developers to manually determine when a code snippet contains an out-of-date API usage. In the worst case, determining if and when a breaking change was made requires an exhaustive search through previous dependency versions.We present V2, a strategy for determining if a code snippet is out-of-date by detecting discrete instances of configuration drift, where the snippet uses an API which has since undergone a breaking change. Each instance of configuration drift is classified by a failure encountered during validation and a configuration patch, consisting of dependency version changes, which fixes the underlying fault. V2 uses feedback-directed search to explore the possible configuration space for a code snippet, reducing the number of potential environment configurations that need to be validated. When run on a corpus of public Python snippets from prior research, V2 identifies 248 instances of configuration drift.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {477–488},
numpages = {12},
keywords = {environment inference, dependencies, configuration repair, configuration management, configuration drift},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1145/3345314,
author = {Wang, Qingyong and Zhou, Yun and Ding, Weiping and Zhang, Zhiguo and Muhammad, Khan and Cao, Zehong},
title = {Random Forest with Self-Paced Bootstrap Learning in Lung Cancer Prognosis},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3345314},
doi = {10.1145/3345314},
abstract = {Training gene expression data with supervised learning approaches can provide an alarm sign for early treatment of lung cancer to decrease death rates. However, the samples of gene features involve lots of noises in a realistic environment. In this study, we present a random forest with self-paced learning bootstrap for improvement of lung cancer classification and prognosis based on gene expression data. To be specific, we propose an ensemble learning with random forest approach to improving the model classification performance by selecting multi-classifiers. Then, we investigate the sampling strategy by gradually embedding from high- to low-quality samples by self-paced learning. The experimental results based on five public lung cancer datasets show that our proposed method could select significant genes exactly, which improves classification performance compared to that of existing approaches. We believe that our proposed method has the potential to assist doctors in gene selections and lung cancer prognosis.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {34},
numpages = {12},
keywords = {self-paced learning, random forest, classification, bootstrap, Lung cancer}
}

@inproceedings{10.5555/510378.510407,
author = {Bapat, Vivek and Swets, Nancy},
title = {Arena: the Arena product family: enterprise modeling solutions},
year = {2000},
isbn = {0780365828},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {This paper introduces the Arena suite of products for modeling, simulation, and optimization highlighting product architecture and technology features that are targeted toward successful deployment of simulation and Arena throughout an enterprise.},
booktitle = {Proceedings of the 32nd Conference on Winter Simulation},
pages = {163–169},
numpages = {7},
location = {Orlando, Florida},
series = {WSC '00}
}

@inproceedings{10.1109/MODELS.2017.22,
author = {Taentzer, Gabriele and Salay, Rick and Strüber, Daniel and Chechik, Marsha},
title = {Transformations of software product lines: a generalizing framework based on category theory},
year = {2017},
isbn = {9781538634929},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS.2017.22},
doi = {10.1109/MODELS.2017.22},
abstract = {Software product lines are used to manage the development of highly complex software with many variants. In the literature, various forms of rule-based product line modifications have been considered. However, when considered in isolation, their expressiveness for specifying combined modifications of feature models and domain models is limited. In this paper, we present a formal framework for product line transformations that is able to combine several kinds of product line modifications presented in the literature. Moreover, it defines new forms of product line modifications supporting various forms of product lines and transformation rules. Our formalization of product line transformations is based on category theory, and concentrates on properties of product line relations instead of their single elements. Our framework provides improved expressiveness and flexibility of software product line transformations while abstracting from the considered type of model.},
booktitle = {Proceedings of the ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems},
pages = {101–111},
numpages = {11},
location = {Austin, Texas},
series = {MODELS '17}
}

@article{10.1016/j.compind.2018.12.006,
author = {Myrodia, Anna and Randrup, Thomas and Hvam, Lars},
title = {Configuration lifecycle management maturity model},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2018.12.006},
doi = {10.1016/j.compind.2018.12.006},
journal = {Comput. Ind.},
month = apr,
pages = {30–47},
numpages = {18},
keywords = {Case study, Configuration management, Product configuration, Maturity model, Product life cycle, Configuration life cycle management}
}

@article{10.1016/j.datak.2006.06.009,
author = {Kim, Minseong and Park, Sooyong and Sugumaran, Vijayan and Yang, Hwasil},
title = {Managing requirements conflicts in software product lines: A goal and scenario based approach},
year = {2007},
issue_date = {June, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {61},
number = {3},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2006.06.009},
doi = {10.1016/j.datak.2006.06.009},
abstract = {The product line approach is recognized as a successful approach to reuse in software development. However, in many cases, it has resulted in interactions between requirements and/or features. Interaction detection, especially conflict detection between requirements has become more challenging. Thus, detecting conflicts between requirements is essential for successful product line development. Formal methods have been proposed to address this problem, however, they are hard to understand by non-experts and are limited to restricted domains. In addition, there is no overall process that covers all the steps for managing conflicts. We propose an approach for systematically identifying and managing requirements conflicts, which is based on requirements partition in natural language and supported by a tool. To demonstrate its feasibility, the proposed approach has been applied to the home integration system (HIS) domain and the results are discussed.},
journal = {Data Knowl. Eng.},
month = jun,
pages = {417–432},
numpages = {16},
keywords = {Syntactic and semantic requirements conflict detection, Software product line, Requirements partitioning, Requirements conflicts, Goal and scenario authoring}
}

@inproceedings{10.5555/3053577.3053583,
author = {Hubaux, Arnaud and Jannach, Dietmar and Drescher, Conrad and Murta, Leonardo and Männistö, Tomi and Czarnecki, Krzysztof and Heymans, Patrick and Nguyen, Tien and Zanker, Markus},
title = {Unifying software and product configuration: a research roadmap},
year = {2012},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {For more than 30 years, knowledge-based product configuration systems have been successfully applied in many industrial domains. Correspondingly, a large number of advanced techniques and algorithms have been developed in academia and industry to support different aspects of configuration reasoning. While traditional research in the field focused on the configuration of physical artefacts, recognition of the business value of customizable software products led to the emergence of software product line engineering. Despite the significant overlap in research interests, the two fields mainly evolved in isolation. Only limited attempts were made at combining the approaches developed in the different fields. In this paper, we first aim to give an overview of commonalities and differences between software product line engineering and product configuration. We then identify opportunities for cross-fertilization between these fields and finally develop a research agenda to combine their respective techniques. Ultimately, this should lead to a unified configuration approach.},
booktitle = {Proceedings of the 2012 International Conference on Configuration - Volume 958},
pages = {31–35},
numpages = {5},
location = {Montpellier, France},
series = {CONFWS'12}
}

@inproceedings{10.1007/978-3-030-82136-4_43,
author = {Zhu, Ziye and Wang, Yu and Li, Yun},
title = {TroBo: A Novel Deep Transfer Model for&nbsp;Enhancing Cross-Project Bug&nbsp;Localization},
year = {2021},
isbn = {978-3-030-82135-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82136-4_43},
doi = {10.1007/978-3-030-82136-4_43},
abstract = {Bug localization, which aims to locate buggy files in the software project by leveraging bug reports, plays an important role in software quality control. Recently, many automatic bug localization methods based on historical bug-fix data (i.e., bug reports labeled with corresponding buggy code files) have been proposed. However, the lack of bug-fix data for software projects in the early stages of development limits the performance of most existing supervised learning methods. To address this issue, we propose a deep transfer bug localization model called TroBo, which can transfer shared knowledge from label-rich source project to the target project. Specifically, we accomplish the knowledge transfer on both the bug report and code file. For processing bug reports, which belong to informal text data, we design a soft attention-based module to alleviate the noise problem. For processing code files, we apply an adversarial strategy to learn the project-shared features, and additionally extract project-exclusive features for each project. Furthermore, a project-aware classifier is introduced in TroBo to avoid redundancy between shared and exclusive features. Extensive experiments on four large-scale real-world projects demonstrate that our model significantly outperforms the state-of-the-art techniques.},
booktitle = {Knowledge Science, Engineering and Management: 14th International Conference, KSEM 2021, Tokyo, Japan, August 14–16, 2021, Proceedings, Part I},
pages = {529–541},
numpages = {13},
keywords = {Bug-fix data, Attention mechanism, Adversarial training, Transfer learning, Bug localization},
location = {Tokyo, Japan}
}

@article{10.1016/j.infsof.2006.05.003,
author = {Olumofin, Femi G. and Mišić, Vojislav B.},
title = {A holistic architecture assessment method for software product lines},
year = {2007},
issue_date = {April, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.05.003},
doi = {10.1016/j.infsof.2006.05.003},
abstract = {The success of architecture-centric development of software product lines is critically dependent upon the availability of suitable architecture assessment methods. While a number of architecture assessment methods are available and some of them have been widely used in the process of evaluating single product architectures, none of them is equipped to deal with the main challenges of product line development. In this paper we present an adaptation of the Architecture Tradeoff Analysis Method (ATAM) for the task of assessing product line architectures. The new method, labeled Holistic Product Line Architecture Assessment (HoPLAA), uses a holistic approach that focuses on risks and quality attribute tradeoffs - not only for the common product line architecture, but for the individual product architectures as well. In addition, it prescribes a qualitative analytical treatment of variation points using scenarios. The use of the new method is illustrated through a case study.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {309–323},
numpages = {15},
keywords = {Software product line architectures, Software architecture assessment, Architecture Tradeoff Analysis Method (ATAM)}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00010,
author = {Saini, Nishrith and Britto, Ricardo},
title = {Using machine intelligence to prioritise code review requests},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00010},
doi = {10.1109/ICSE-SEIP52600.2021.00010},
abstract = {Modern Code Review (MCR) is the process of reviewing new code changes that need to be merged with an existing codebase. As a developer, one may receive many code review requests every day, i.e., the review requests need to be prioritised. Manually prioritising review requests is a challenging and time-consuming process. To address the above problem, we conducted an industrial case study at Ericsson aiming at developing a tool called Pineapple, which uses a Bayesian Network to prioritise code review requests. To validate our approach/tool, we deployed it in a live software development project at Ericsson, wherein more than 150 developers develop a telecommunication product. We focused on evaluating the predictive performance, feasibility, and usefulness of our approach. The results indicate that Pineapple has competent predictive performance (RMSE = 0.21 and MAE = 0.15). Furthermore, around 82.6% of Pineapple's users believe the tool can support code review request prioritisation by providing reliable results, and around 56.5% of the users believe it helps reducing code review lead time. As future work, we plan to evaluate Pineapple's predictive performance, usefulness, and feasibility through a longitudinal investigation.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {11–20},
numpages = {10},
keywords = {prioritisation, modern code review, machine reasoning, machine learning, machine intelligence, bayesian networks},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1007/978-3-642-25535-9_29,
author = {Mohabbati, Bardia and Gašević, Dragan and Hatala, Marek and Asadi, Mohsen and Bagheri, Ebrahim and Bošković, Marko},
title = {A quality aggregation model for service-oriented software product lines based on variability and composition patterns},
year = {2011},
isbn = {9783642255342},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25535-9_29},
doi = {10.1007/978-3-642-25535-9_29},
abstract = {Quality evaluation is a challenging task in monolithic software systems. It is even more complex when it comes to Service-Oriented Software Product Lines (SOSPL), as it needs to analyze the attributes of a family of SOA systems. In SOSPL, variability can be planned and managed at the architectural level to develop a software product with the same set of functionalities but different degrees of non-functional quality attribute satisfaction. Therefore, architectural quality evaluation becomes crucial due to the fact that it allows for the examination of whether or not the final product satisfies and guarantees all the ranges of quality requirements within the envisioned scope. This paper addresses the open research problem of aggregating QoS attribute ranges with respect to architectural variability. Previous solutions for quality aggregation do not consider architectural variability for composite services. Our approach introduces variability patterns that can possibly occur at the architectural level of an SOSPL. We propose an aggregation model for QoS computation which takes both variability and composition patterns into account.},
booktitle = {Proceedings of the 9th International Conference on Service-Oriented Computing},
pages = {436–451},
numpages = {16},
keywords = {variability management, software product line (SPL), service-oriented architecture (SOA), service variability, process family, non-functional properties, feature modeling, QoS aggregation},
location = {Paphos, Cyprus},
series = {ICSOC'11}
}

@inproceedings{10.1609/aaai.v33i01.33015256,
author = {Wang, Jing and Geng, Xin},
title = {Theoretical analysis of label distribution learning},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015256},
doi = {10.1609/aaai.v33i01.33015256},
abstract = {As a novel learning paradigm, label distribution learning (LDL) explicitly models label ambiguity with the definition of label description degree. Although lots of work has been done to deal with real-world applications, theoretical results on LDL remain unexplored. In this paper, we rethink LDL from theoretical aspects, towards analyzing learnability of LDL. Firstly, risk bounds for three representative LDL algorithms (AA-kNN, AA-BP and SA-ME) are provided. For AA-kNN, Lipschitzness of the label distribution function is assumed to bound the risk, and for AA-BP and SA-ME, rademacher complexity is utilized to give data-dependent risk bounds. Secondly, a generalized plug-in decision theorem is proposed to understand the relation between LDL and classification, uncovering that approximation to the conditional probability distribution function in absolute loss guarantees approaching to the optimal classifier, and also data-dependent error probability bounds are presented for the corresponding LDL algorithms to perform classification. As far as we know, this is perhaps the first research on theory of LDL.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {644},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1007/978-3-030-30490-4_30,
author = {Tarjano, Carlos and Pereira, Valdecy},
title = {Neuro-Spectral Audio Synthesis: Exploiting Characteristics of the Discrete Fourier Transform in the Real-Time Simulation of Musical Instruments Using Parallel Neural Networks},
year = {2019},
isbn = {978-3-030-30489-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30490-4_30},
doi = {10.1007/978-3-030-30490-4_30},
abstract = {Two main approaches are currently prevalent in the digital emulation of musical instruments: manipulation of pre-recorded samples and techniques of real-time synthesis, generally based on physical models with varying degrees of accuracy. Concerning the first, while the processing power of present-day computers enables their use in real-time, many restrictions arising from this sample-based design persist; the huge on disk space requirements and the stiffness of musical articulations being the most prominent. On the other side of the spectrum, pure synthesis approaches, while offering greater flexibility, fail to capture and reproduce certain nuances central to the verisimilitude of the generated sound, offering a dry, synthetic output, at a high computational cost. We propose a method where ensembles of lightweight neural networks working in parallel are learned, from crafted frequency-domain features of an instrument sound spectra, an arbitrary instrument’s voice and articulations realistically and efficiently. We find that our method, while retaining perceptual sound quality on par with sampled approaches, exhibits 1/10 of latency times of industry standard real-time synthesis algorithms, and 1/100 of the disk space requirements of industry standard sample-based digital musical instruments. This method can, therefore, serve as a basis for more efficient implementations in dedicated devices, such as keyboards and electronic drumkits and in general purpose platforms, like desktops and tablets or open-source hardware like Arduino and Raspberry Pi. From a conceptual point of view, this work highlights the advantages of a closer integration of machine learning with other subjects, especially in the endeavor of new product development. Exploiting the synergy between neural networks, digital signal processing techniques and physical modelling, we illustrate the proposed method via the implementation of two virtual instruments: a conventional grand piano and a hibrid stringed instrument.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Text and Time Series: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17–19, 2019, Proceedings, Part IV},
pages = {362–375},
numpages = {14},
keywords = {Real-time audio synthesis, Digital musical instruments, Acoustic modeling, Neural networks},
location = {Munich, Germany}
}

@article{10.1016/j.jisa.2021.102880,
author = {Araujo, Frederico and Ayoade, Gbadebo and Al-Naami, Khaled and Gao, Yang and Hamlen, Kevin W. and Khan, Latifur},
title = {Crook-sourced intrusion detection as a service},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {61},
number = {C},
issn = {2214-2126},
url = {https://doi.org/10.1016/j.jisa.2021.102880},
doi = {10.1016/j.jisa.2021.102880},
journal = {J. Inf. Secur. Appl.},
month = sep,
numpages = {17},
keywords = {Software-as-a-service, Cloud computing, Cyberdeception, Honeypots, Neural networks, Datasets, Intrusion detection}
}

@article{10.1016/j.neucom.2019.10.018,
author = {Ding, Deqiong and Yang, Xiaogao and Xia, Fei and Ma, Tiefeng and Liu, Haiyun and Tang, Chang},
title = {Unsupervised feature selection via adaptive hypergraph regularized latent representation learning},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {378},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.10.018},
doi = {10.1016/j.neucom.2019.10.018},
journal = {Neurocomput.},
month = feb,
pages = {79–97},
numpages = {19},
keywords = {99-00, 00-01, Local structure preservation, Latent representation learning, Hypergraph learning, Unsupervised feature selection}
}

@inproceedings{10.1145/337180.337189,
author = {Kuusela, Juha and Savolainen, Juha},
title = {Requirements engineering for product families},
year = {2000},
isbn = {1581132069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/337180.337189},
doi = {10.1145/337180.337189},
abstract = {In search for improved software quality and high productivity, software reuse has become a key research area. One of the most promising reuse approaches is product families. However, current practices in requirements engineering do not support product families. This paper describes a definition hierarchy method for requirements capturing, structuring, analysis and documentation. This method helps to identify architectural drivers of the product family and shows how different products in the family vary.},
booktitle = {Proceedings of the 22nd International Conference on Software Engineering},
pages = {61–69},
numpages = {9},
keywords = {requirements engineering, quality attributes, product line, product family, design},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@inproceedings{10.1145/3349341.3349504,
author = {Huang, Chunfang and Wang, Xiangrong},
title = {Financial Innovation Based on Artificial Intelligence Technologies},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349504},
doi = {10.1145/3349341.3349504},
abstract = {Nowadays, the degree of the heated topic of artificial intelligence in the world reaches a new height. Due to the breakthrough of deep learning algorithm based on neural network, the level of artificial intelligence technologies has been enhanced significantly. The global financial industry is quietly changing under the catalysis of artificial intelligence. The frontier artificial intelligence technologies, such as the technology of expert system, machine learning and knowledge discovery in database are combed to explore the financial applications of artificial intelligence. Based on these key technologies, this paper proposed three applications of artificial intelligence in the financial field, including intelligent investment adviser, transaction forecast and financial regulation, discusses the key technologies of artificial intelligence and financial innovation products based on these technologies, such as the functions of the transaction prediction system based on artificial intelligence technologies include forecast analysis, index statistics, stock analysis and information retrieval, etc. The structures of the systems are drawn and the design principles are provided. Finally, to guard the safety of the applications of artificial intelligence, the paper gives the suggestions of enhancing identity authentication, introducing monitoring measures and limiting autonomy degree.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {750–754},
numpages = {5},
keywords = {Transaction Forecast, Machine Learning, Intelligent Investment Adviser, Financial Regulation, Deep Learning},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3131151.3131152,
author = {Filho, Helson L. Jakubovski and Lima, Jackson A. Prado and Vergilio, Silvia R.},
title = {Automatic Generation of Search-Based Algorithms Applied to the Feature Testing of Software Product Lines},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131152},
doi = {10.1145/3131151.3131152},
abstract = {The selection of products for the variability testing of Feature Models (FMs) is a complex task impacted by many factors. To solve this problem, Multi-Objective Evolutionary Algorithms (MOEAs) have been successfully used in the field known as Search-Based Software Engineering (SBSE). However, the design of a search-based approach is not an easy task for the software engineer, who can find some difficulties such as: the choice and configuration of the best MOEAs, the choice of the best search operators to be implemented, and so on. In addition to this, existing approaches are dependent on the problem domain and do not allow reuse. In this way the use of Hyper-Heuristic (HH) can help to obtain more generic and reusable search-based approaches, and because of this is considered a trend in the SBSE field. Following this trend and to contribute to reduce the software engineer's efforts, this work explores the use of a hyper-heuristic for automatic generation of MOEAs to select test products from the FM, considering three factors: pairwise coverage, mutation score and cost, given by the number of products. The HH is based on a grammar that represents the elements, parameters and components of existing MOEAs and implements evolutionary operators, such as crossover and mutation, suitable for selection problems. In this way, it can be reused for other similar software engineering problems. Evaluation results show that the proposed approach obtains results that are better or statistically equivalent than similar approaches found in the literature.},
booktitle = {Proceedings of the XXXI Brazilian Symposium on Software Engineering},
pages = {114–123},
numpages = {10},
keywords = {Software Product Line Testing, Search-Based Software Engineering, Hyper-Heuristics},
location = {Fortaleza, CE, Brazil},
series = {SBES '17}
}

@article{10.1007/s10664-018-9656-z,
author = {Blincoe, Kelly and Dehghan, Ali and Salaou, Abdoul-Djawadou and Neal, Adam and Linaker, Johan and Damian, Daniela},
title = {High-level software requirements and iteration changes: a predictive model},
year = {2019},
issue_date = {Jun 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9656-z},
doi = {10.1007/s10664-018-9656-z},
abstract = {Knowing whether a software feature will be completed in its planned iteration can help with release planning decisions. However, existing research has focused on predictions of only low-level software tasks, like bug fixes. In this paper, we describe a mixed-method empirical study on three large IBM projects. We investigated the types of iteration changes that occur. We show that up to 54% of high-level requirements do not make their planned iteration. Requirements are most often pushed out to the next iteration, but high-level requirements are also commonly moved to the next minor or major release or returned to the product or release backlog. We developed and evaluated a model that uses machine learning to predict if a high-level requirement will be completed within its planned iteration. The model includes 29 features that were engineered based on prior work, interviews with IBM developers, and domain knowledge. Predictions were made at four different stages of the requirement lifetime. Our model is able to achieve up to 100% precision. We ranked the importance of our model features and found that some features are highly dependent on project and prediction stage. However, some features (e.g., the time remaining in the iteration and creator of the requirement) emerge as important across all projects and stages. We conclude with a discussion on future research directions.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1610–1648},
numpages = {39},
keywords = {Software requirements, Release planning, Mining software repositories, Machine learning, Completion prediction}
}

@article{10.1016/j.eswa.2015.07.044,
author = {Bautista, Joaquín and Alfaro-Pozo, Rocío and Batalla-García, Cristina},
title = {Consideration of human resources in the Mixed-model Sequencing Problem with Work Overload Minimization},
year = {2015},
issue_date = {December 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {22},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.07.044},
doi = {10.1016/j.eswa.2015.07.044},
abstract = {MMSP with work overload minimization and improvement of working conditions.Compliance with saturation conditions of workers imposed by collective agreement.Increase of work pace factor of workers to reduce production losses.Auxiliary processors to complete the required work and to fulfill all conditions.Case study linked to the Nissan Powertrain Plant in Barcelona. Beginning with a variation of the sequencing problem in a mixed-products line (MMSP-W: Mixed-Model Sequencing Problem with Workload Minimization), we propose two new models that incorporate a set of working conditions in regard with human resources of workstations on the line. These conditions come from collective agreements and therefore must be respected by both company and labor unions. The first model takes into account the saturation limit of the workstations, and the second model also includes the activation of the operators throughout the working day. Two computational experiments were carried out using a case study of the Nissan motor plant in Barcelona with two main objectives: (1) to study the repercussions of the saturation limit on the decrease in productivity on the line and (2) to evaluate the recovery of productivity on the line via both activation of operators, while maintaining the same quality in working conditions achieved by limiting the saturation, and auxiliary processors. By results we state that saturation limitation leads an important increase of work overload, which means average economic losses of 28,731.8 Euros/day. However, the productivity reduction may be counteracted by the work pace factor increase, at certain moments of workday, and/or by the incorporation of auxiliary processors into the line.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {8896–8910},
numpages = {15},
keywords = {Work overload, Sequencing, Saturation, Mixed-product line, Manufacturing operations, Activity factor}
}

@article{10.1016/j.patcog.2019.107173,
author = {Song, Liangchen and Wang, Cheng and Zhang, Lefei and Du, Bo and Zhang, Qian and Huang, Chang and Wang, Xinggang},
title = {Unsupervised domain adaptive re-identification: Theory and practice},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {102},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107173},
doi = {10.1016/j.patcog.2019.107173},
journal = {Pattern Recogn.},
month = jun,
numpages = {11},
keywords = {Unsupervised domain adaptation, Person re-identification}
}

@article{10.1007/s11257-011-9114-8,
author = {Wang, Yang and Kobsa, Alfred},
title = {A PLA-based privacy-enhancing user modeling framework and its evaluation},
year = {2013},
issue_date = {March     2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-011-9114-8},
doi = {10.1007/s11257-011-9114-8},
abstract = {Reconciling personalization with privacy has been a continuing interest in user modeling research. This aim has computational, legal and behavioral/attitudinal ramifications. We present a dynamic privacy-enhancing user modeling framework that supports compliance with users' individual privacy preferences and with the privacy laws and regulations that apply to each user. The framework is based on a software product line architecture. It dynamically selects personalization methods during runtime that meet the current privacy constraints. Since dynamic architectural reconfiguration is typically resource-intensive, we conducted a performance evaluation with four implementations of our system that vary two factors. The results demonstrate that at least one implementation of our approach is technically feasible with comparatively modest additional resources, even for websites with the highest traffic today. To gauge user reactions to privacy controls that our framework enables, we also conducted a controlled experiment that allowed one group of users to specify privacy preferences and view the resulting effects on employed personalization methods. We found that users in this treatment group utilized this feature, deemed it useful, and had fewer privacy concerns as measured by higher disclosure of their personal data.},
journal = {User Modeling and User-Adapted Interaction},
month = mar,
pages = {41–82},
numpages = {42},
keywords = {User modeling, User experiment, Product line architecture, Privacy preferences, Privacy laws, Performance evaluation, Disclosure behavior, Compliance}
}

@article{10.1016/j.future.2019.07.013,
author = {Shen, Rongbo and Yan, Kezhou and Tian, Kuan and Jiang, Cheng and Zhou, Ke},
title = {Breast mass detection from the digitized X-ray mammograms based on the combination of deep active learning and self-paced learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.07.013},
doi = {10.1016/j.future.2019.07.013},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {668–679},
numpages = {12},
keywords = {Self-paced learning, Deep active learning, Mass detection, Mammography, Breast cancer}
}

@article{10.1016/j.imavis.2016.06.005,
author = {Leng, Mengjun and Moutafis, Panagiotis and Kakadiaris, Ioannis A.},
title = {Joint prototype and metric learning for image set classification: Application to video face identification},
year = {2017},
issue_date = {Feb 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {58},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.06.005},
doi = {10.1016/j.imavis.2016.06.005},
journal = {Image Vision Comput.},
month = feb,
pages = {204–213},
numpages = {10},
keywords = {Video face recognition, Prototype learning, Metric learning, Image set classification}
}

@article{10.1016/j.procs.2019.12.135,
author = {Jamil, Muhammad Abid and Nour, Mohamed K and Alhindi, Ahmad and Awang Abhubakar, Normi Sham and Arif, Muhammad and Aljabri, Tareq Fahad},
title = {Towards Software Product Lines Optimization Using Evolutionary Algorithms},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {163},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.135},
doi = {10.1016/j.procs.2019.12.135},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {527–537},
numpages = {11},
keywords = {Multi-objective Algorithms, Feature Models, Software Product Lines, Software Testing, Search Based Software Engineering}
}

@inproceedings{10.1007/978-3-030-79463-7_35,
author = {Kawalerowicz, Marcin and Madeyski, Lech},
title = {Continuous Build Outcome Prediction: A Small-N Experiment in Settings of a Real Software Project},
year = {2021},
isbn = {978-3-030-79462-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79463-7_35},
doi = {10.1007/978-3-030-79463-7_35},
abstract = {We explain the idea of Continuous Build Outcome Prediction (CBOP) practice that uses classification to label the possible build results (success or failure) based on historical data and metrics (features) derived from the software repository. Additionally, we present a preliminary empirical evaluation of CBOP in a real live software project. In a small-n repeated-measure with two conditions and replicates experiment, we study whether CBOP will reduce the Failed Build Ratio (FBR). Surprisingly, the result of the study indicates a slight increase in FBR while using the CBOP, although the effect size is very small. A plausible explanation of the revealed phenomenon may come from the authority principle, which is rarely discussed in the software engineering context in general, and AI-supported software development practices in particular.},
booktitle = {Advances and Trends in Artificial Intelligence. From Theory to Practice: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part II},
pages = {412–425},
numpages = {14},
keywords = {Machine learning, Continuous integration, Agile experimentation, Software defect prediction},
location = {Kuala Lumpur, Malaysia}
}

@article{10.1016/j.neucom.2019.03.062,
author = {Ren, Yazhou and Que, Xiaofan and Yao, Dezhong and Xu, Zenglin},
title = {Self-paced multi-task clustering},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {350},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.03.062},
doi = {10.1016/j.neucom.2019.03.062},
journal = {Neurocomput.},
month = jul,
pages = {212–220},
numpages = {9},
keywords = {Soft weighting, Non-convexity, Self-paced learning, Multi-task clustering}
}

@inproceedings{10.1145/2556624.2556645,
author = {Adelsberger, Stephan and Sobernig, Stefan and Neumann, Gustaf},
title = {Towards assessing the complexity of object migration in dynamic, feature-oriented software product lines},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556645},
doi = {10.1145/2556624.2556645},
abstract = {Dynamic Software Product Lines (DSPLs) implement features of a product family, from which products can be derived and reconfigured at runtime. This way, systems can alternate their configurations without service interruption. The activation and deactivation of features at runtime pose challenges for the implementation of a DSPL, in particular for handling object states such as runtime changes to object-scoped variables, their value assignments, and the variable properties. To quantify the complexity of this object migration, we propose a systematic code-level measurement approach which harvests feature implementations and the corresponding variability models for code introductions responsible for critical changes to object states.We have applied our measurement process tentatively to data sets representing 9 SPLs implemented using Fuji. This way, we arrived at first insights on object-migration complexity in SPLs. For example, we observed that the number of feature-specific object states is distributed very unequally in Fuji SPLs, with a few objects having an overly complex map of potential object states and the majority of objects potentially seeing transitions between 1 and 5 object states. We also evaluated different tactics of applying SAT solvers to analyze variability models in this context.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {8},
keywords = {object migration, feature-oriented programming, feature binding, dynamic software product line, constructor anomaly},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/3450268.3453517,
author = {Kang, Zhuangwei and Barve, Yogesh D. and Bao, Shunxing and Dubey, Abhishek and Gokhale, Aniruddha},
title = {Configuration Tuning for Distributed IoT Message Systems Using Deep Reinforcement Learning: Poster Abstract},
year = {2021},
isbn = {9781450383547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450268.3453517},
doi = {10.1145/3450268.3453517},
abstract = {Distributed messaging systems (DMSs) are often equipped with a large number of configurable parameters that enable users to define application run-time behaviors and information dissemination rules. However, the resulting high-dimensional configuration space makes it difficult for users to determine the best configuration that can maximize application QoS under a variety of operational conditions. This poster introduces a novel, automatic knob tuning framework called DMSConfig. DMSConfig explores the configuration space by interacting with a data-driven environment prediction model(a DMS simulator), which eliminates the prohibitive cost of conducting online interactions with the production environment. DMSConfig employs the deep deterministic policy gradient (DDPG) method and a custom reward mechanism to learn and make configuration decisions based on predicted DMS states and performance. Our initial experimental results, conducted on a single-broker Kafka cluster, show that DMSConfig significantly outperforms the default configuration and has better adaptability to CPU and bandwidth-limited environments. We also confirm that DMSConfig produces fewer violations of latency constraints than three prevalent parameter tuning tools.},
booktitle = {Proceedings of the International Conference on Internet-of-Things Design and Implementation},
pages = {273–274},
numpages = {2},
keywords = {System Configuration, Publish/Subscribe Middleware, Policy-based RL Algorithm},
location = {Charlottesvle, VA, USA},
series = {IoTDI '21}
}

@article{10.3233/HIS-2011-0140,
author = {Ahumada, Hernán and Grinblat, Guillermo L. and Uzal, Lucas C. and Ceccatto, Alejandro and Granitto, Pablo M.},
title = {Evaluation of a new hybrid algorithm for highly imbalanced classification problems},
year = {2011},
issue_date = {October 2011},
publisher = {IOS Press},
address = {NLD},
volume = {8},
number = {4},
issn = {1448-5869},
url = {https://doi.org/10.3233/HIS-2011-0140},
doi = {10.3233/HIS-2011-0140},
abstract = {Many times in classification problems, particularly in critical real world applications, one of the classes has much less samples than the others usually known as the class imbalance problem. In this work we discuss and evaluate the use of the REPMAC algorithm to solve imbalanced problems. Using a clustering method, REPMAC recursively splits the majority class in several subsets, creating a decision tree, until the resulting sub-problems are balanced or easy to solve. We use two diverse clustering methods and three different classifiers coupled with REPMAC to evaluate the new method on several benchmark datasets spanning a wide range of number of features, samples and imbalance degree. We also apply our method to a real world problem, the identification of weed seeds. We find that the good performance of REPMAC is almost independent of the classifier or the clustering method coupled to it, which suggests that its success is mostly related to the use of an appropriate strategy to cope with imbalanced problems.},
journal = {Int. J. Hybrid Intell. Syst.},
month = oct,
pages = {199–211},
numpages = {13},
keywords = {Hybrid Systems, Clustering, Classification, Class Imbalance}
}

@inproceedings{10.1145/3430984.3431022,
author = {Virk, Jitender Singh and Bathula, Deepti R.},
title = {Domain-Specific, Semi-Supervised Transfer Learning for Medical Imaging},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431022},
doi = {10.1145/3430984.3431022},
abstract = {Limited availability of annotated medical imaging data poses a challenge for deep learning algorithms. Although transfer learning minimizes this hurdle in general, knowledge transfer across disparate domains is shown to be less effective. On the other hand, smaller architectures were found to be more compelling in learning better features. Consequently, we propose a lightweight architecture that uses mixed asymmetric kernels (MAKNet) to reduce the number of parameters significantly. Additionally, we train the proposed architecture using semi-supervised learning to provide pseudo-labels for a large medical dataset to assist with transfer learning. The proposed MAKNet provides better classification performance with fewer parameters than popular architectures. Experimental results also highlight the importance of domain-specific knowledge for effective transfer learning. Additionally, we interrogate the proposed network with integrated gradients and perturbation methods to establish the superior quality of the learned features.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {145–153},
numpages = {9},
keywords = {transfer learning, pseudo-labelling, mixed-kernels neural networks, integrated gradients, image perturbations, domain-specific, Semi-supervised learning, Neural networks, CT scans},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@inproceedings{10.1145/3319535.3339815,
author = {Cao, Yulong and Xiao, Chaowei and Cyr, Benjamin and Zhou, Yimeng and Park, Won and Rampazzi, Sara and Chen, Qi Alfred and Fu, Kevin and Mao, Z. Morley},
title = {Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3339815},
doi = {10.1145/3319535.3339815},
abstract = {In Autonomous Vehicles (AVs), one fundamental pillar is perception,which leverages sensors like cameras and LiDARs (Light Detection and Ranging) to understand the driving environment. Due to its direct impact on road safety, multiple prior efforts have been made to study its the security of perception systems. In contrast to prior work that concentrates on camera-based perception, in this work we perform the first security study of LiDAR-based perception in AV settings, which is highly important but unexplored. We consider LiDAR spoofing attacks as the threat model and set the attack goal as spoofing obstacles close to the front of a victim AV. We find that blindly applying LiDAR spoofing is insufficient to achieve this goal due to the machine learning-based object detection process.Thus, we then explore the possibility of strategically controlling the spoofed attack to fool the machine learning model. We formulate this task as an optimization problem and design modeling methods for the input perturbation function and the objective function.We also identify the inherent limitations of directly solving the problem using optimization and design an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75%. As a case study to understand the attack impact at the AV driving decision level, we construct and evaluate two attack scenarios that may damage road safety and mobility.We also discuss defense directions at the AV system, sensor, and machine learning model levels.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2267–2281},
numpages = {15},
keywords = {sensor attack, autonomous driving, adversarial machine learning},
location = {London, United Kingdom},
series = {CCS '19}
}

@inproceedings{10.1007/978-3-030-78609-0_28,
author = {Sun, Ying and Sun, Yanfei and Wu, Fei and Jing, Xiao-Yuan},
title = {Deep Adversarial Learning Based Heterogeneous Defect Prediction},
year = {2021},
isbn = {978-3-030-78608-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78609-0_28},
doi = {10.1007/978-3-030-78609-0_28},
abstract = {Cross-project defect prediction (CPDP) is a hot study that predicts defects in the new project by utilizing the model trained on the data from other projects. However, existing CPDP methods usually assume that source and target projects have the same metrics. Heterogeneous defect prediction (HDP) is proposed and has attracted increasing attention, which refers to the metric sets from source and target projects are different in CPDP. HDP conducts prediction model using the instances with heterogeneous metrics from external projects and then use this model to predict defect-prone software instances in source project. However, building HDP methods is challenging including the distribution difference between source and target projects with heterogeneous metrics. In this paper, we propose a Deep adversarial learning based HDP (DHDP) approach. DHDP leverages deep neural network to learn nonlinear transformation for each project to obtain common feature represent, which the heterogeneous data from different projects can be compared directly. DHDP consists of two parts: a discriminator and a classifier that compete with each other. A classifier tries to minimize the similarity across classes and maximize the inter-class similarity. A discriminator tries to distinguish the source of instances that is source or target project on the common feature space. Expensive experiments are performed on 10 public projects from two datasets in terms of F-measure and G-measure. The experimental results show that DHDP gains superior prediction performance improvement compared to a range of competing methods.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part I},
pages = {326–337},
numpages = {12},
keywords = {Heterogeneous defect prediction, Metric learning, Adversarial learning},
location = {Dublin, Ireland}
}

@article{10.1002/smr.2163,
author = {Chatzimparmpas, Angelos and Bibi, Stamatia},
title = {Maintenance process modeling and dynamic estimations based on Bayesian networks and association rules},
year = {2019},
issue_date = {September 2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {31},
number = {9},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2163},
doi = {10.1002/smr.2163},
abstract = {Managing the maintenance process and estimating accurately the effort and duration required for a new release is considered to be a crucial task as it affects successful software project survival and progress over time. In this study, we propose the combination of two well‐known machine learning (ML) techniques, Bayesian networks (BNs), and association rules (ARs) for modeling the maintenance process by identifying the relationships among the internal and external quality metrics related to a particular project release to both the maintainability of the project and the maintenance process indicators (ie, effort and duration). We also exploit Bayesian inference, to test the effect of certain changes in internal and external project factors to the maintainability of a project. We evaluate our approach through a case study on 957 releases of five open source JavaScript applications. The results show that the maintainability of a release, the changes observed between subsequent releases, and the time required between two releases can be accurately predicted from size, complexity, and activity metrics. The proposed combined approach achieves higher accuracy when evaluated against the BN model accuracy.},
journal = {J. Softw. Evol. Process},
month = oct,
numpages = {25},
keywords = {source code quality, software quality, maintenance, maintainability, JavaScript, developers' activity}
}

@article{10.1016/j.infsof.2019.07.009,
author = {Gomes, Luiz Alberto Ferreira and Torres, Ricardo da Silva and Côrtes, Mario Lúcio},
title = {Bug report severity level prediction in open source software: A survey and research opportunities},
year = {2019},
issue_date = {Nov 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {115},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.07.009},
doi = {10.1016/j.infsof.2019.07.009},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {58–78},
numpages = {21},
keywords = {Machine learning, Systematic mapping, Software repositories, Severity level prediction, Bug reports, Bug tracking systems, Software maintenance}
}

@article{10.1145/3243316,
author = {Fan, Hehe and Zheng, Liang and Yan, Chenggang and Yang, Yi},
title = {Unsupervised Person Re-identification: Clustering and Fine-tuning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3243316},
doi = {10.1145/3243316},
abstract = {The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {83},
numpages = {18},
keywords = {unsupervised learning, convolutional neural network, clustering, Large-scale person re-identification}
}

@inproceedings{10.1007/978-3-642-39253-5_75,
author = {Igler, Bodo},
title = {Feature evaluation for mobile applications: a design science approach based on evolutionary software prototypes},
year = {2013},
isbn = {9783642392528},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39253-5_75},
doi = {10.1007/978-3-642-39253-5_75},
abstract = {The success of mobile applications depends on the incorporation of key features specific to their intended use. This paper proposes a light-weight process model to facilitate the identification of key user interface features and key application logic features.The iterative, incremental process model is aligned with design science research and is based on software product line engineering ideas. In each iteration several prototype variants are built and evaluated by the customer. Both construction and evaluation of prototypes are based on feature models.},
booktitle = {Proceedings of the Second International Conference on Design, User Experience, and Usability: Web, Mobile, and Product Design - Volume Part IV},
pages = {673–681},
numpages = {9},
keywords = {software product lines, mobile computing, feature-oriented development, design science research, design and evaluation methods},
location = {Las Vegas, NV},
series = {DUXU'13}
}

@article{10.5555/3163583.3163679,
author = {Lanna, Andr and Castro, Thiago and Alves, Vander and Rodrigues, Genaina and Schobbens, Pierre-Yves and Apel, Sven},
title = {Feature-family-based reliability analysis of software product lines},
year = {2018},
issue_date = {February 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {94},
number = {C},
issn = {0950-5849},
abstract = {ContextVerification techniques are being applied to ensure that software systems achieve desired quality levels and fulfill functional and non-functional requirements. However, applying these techniques to software product lines is challenging, given the exponential blowup of the number of products. Current product-line verification techniques leverage symbolic model checking and variability information to optimize the analysis, but still face limitations that make them costly or infeasible. In particular, state-of-the-art verification techniques for product-line reliability analysis are enumerative which hinders their applicability, given the latent exponential blowup of the configuration space. ObjectiveThe objectives of this paper are the following: (a) we present a method to efficiently compute the reliability of all configurations of a compositional or annotation-based software product line from its UML behavioral models, (b) we provide a tool that implements the proposed method, and (c) we report on an empirical study comparing the performance of different reliability analysis strategies for software product lines. MethodWe present a novel feature-family-based analysis strategy to compute the reliability of all products of a (compositional or annotation-based) software product line. The feature-based step of our strategy divides the behavioral models into smaller units that can be analyzed more efficiently. The family-based step performs the reliability computation for all configurations at once by evaluating reliability expressions in terms of a suitable variational data structure. ResultsOur empirical results show that our feature-family-based strategy for reliability analysis outperforms, in terms of time and space, four state-of-the-art strategies (product-based, family-based, feature-product-based, and family-product-based) for the same property. It is the only one that could be scaled to a 220-fold increase in the size of the configuration space. ConclusionOur feature-family-based strategy leverages both feature- and family-based strategies by taming the size of the models to be analyzed and by avoiding the products enumeration inherent to some state-of-the-art analysis methods.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {59–81},
numpages = {23},
keywords = {Software reliability analysis, Software product lines, Parametric verification}
}

@inproceedings{10.1145/3447545.3451177,
author = {Canales, Felipe and Hecht, Geoffrey and Bergel, Alexandre},
title = {Optimization of Java Virtual Machine Flags using Feature Model and Genetic Algorithm},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451177},
doi = {10.1145/3447545.3451177},
abstract = {Optimizing the Java Virtual Machine (JVM) options in order to get the best performance out of a program for production is a challenging and time-consuming task. HotSpot, the Oracle's open-source Java VM implementation offers more than 500 options, called flags, that can be used to tune the JVM's compiler, garbage collector (GC), heap size and much more. In addition to being numerous, these flags are sometimes poorly documented and create a need of benchmarking to ensure that the flags and their associated values deliver the best performance and stability for a particular program to execute.Auto-tuning approaches have already been proposed in order to mitigate this burden. However, in spite of increasingly sophisticated search techniques allowing for powerful optimizations, these approaches take little account of the underlying complexities of JVM flags. Indeed, dependencies and incompatibilities between flags are non-trivial to express, which if not taken into account may lead to invalid or spurious flag configurations that should not be considered by the auto-tuner.In this paper, we propose a novel model, inspired by the feature model used in Software Product Line, which takes the complexity of JVM's flags into account. We then demonstrate the usefulness of this model, using it as an input of a Genetic Algorithm (GA) to optimize the execution times of DaCapo Benchmarks.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {183–186},
numpages = {4},
keywords = {optimization, java virtual machine, genetic algorithm, feature model, auto-tuning},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/2377816.2377822,
author = {Buchmann, Thomas and Schwägerl, Felix},
title = {Ensuring well-formedness of configured domain models in model-driven product lines based on negative variability},
year = {2012},
isbn = {9781450313094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377816.2377822},
doi = {10.1145/2377816.2377822},
abstract = {Model-driven development is a well-known practice in modern software engineering. Many tools exist which allow developers to build software in a model-based or even model-driven way, but they do not provide dedicated support for software product line development. Only recently some approaches combined model-driven engineering and software product line engineering. In this paper we present an approach that allows for combining feature models and Ecore-based domain models and provides extensive support to keep the mapping between the involved models consistent. Our key contribution is a declarative textual language which allows to phrase domain-specific consistency constraints which are preserved during the configuration process in order to ensure context-sensitive syntactical correctness of derived domain models.},
booktitle = {Proceedings of the 4th International Workshop on Feature-Oriented Software Development},
pages = {37–44},
numpages = {8},
location = {Dresden, Germany},
series = {FOSD '12}
}

@inproceedings{10.1007/978-3-030-78609-0_47,
author = {Shang, Di and Hu, Zhenda and Wang, Zhaoxia},
title = {Mining Consumer Brand Relationship from Social Media Data: A Natural Language Processing Approach},
year = {2021},
isbn = {978-3-030-78608-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78609-0_47},
doi = {10.1007/978-3-030-78609-0_47},
abstract = {There is a rich collection of studies exploring different aspects of consumer brand relationship. Traditional approaches of questionnaires and analysis are based on measurements collected from a relatively small number of survey participants. With the advancements in natural language processing (NLP) techniques, opportunities exist for applying NLP techniques to discover consumer brand relationship from social media platforms that possess a large amount of data on consumer opinion and sentiment. In this study, we review consumer brand relationship analysis focusing on leveraging NLP and machine learning techniques to address some challenges associated with discovering customer brand relationship from social media data and propose a methodological framework for the approach. This study has implications for both academic research and practitioners as it presents an alternative way to investigate consumer brand relationship.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part I},
pages = {553–565},
numpages = {13},
keywords = {Machine learning, Consumer brand relationship, Text mining, NLP},
location = {Dublin, Ireland}
}

@inproceedings{10.5555/3540261.3541537,
author = {Peng, Jizong and Wang, Ping and Desrosiers, Christian and Pedersoli, Marco},
title = {Self-paced contrastive learning for semi-supervised medical image segmentation with meta-labels},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The contrastive pre-training of a recognition model on a large dataset of unlabeled data often boosts the model's performance on downstream tasks like image classification. However, in domains such as medical imaging, collecting unlabeled data can be challenging and expensive. In this work, we consider the task of medical image segmentation and adapt contrastive learning with meta-label annotations to scenarios where no additional unlabeled data is available. Meta-labels, such as the location of a 2D slice in a 3D MRI scan, often come for free during the acquisition process. We use these meta-labels to pre-train the image encoder, as well as in a semi-supervised learning step that leverages a reduced set of annotated data. A self-paced learning strategy exploiting the weak annotations is proposed to further help the learning process and discriminate useful labels from noise. Results on five medical image segmentation datasets show that our approach: i) highly boosts the performance of a model trained on a few scans, ii) outperforms previous contrastive and semi-supervised approaches, and iii) reaches close to the performance of a model trained on the full data.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1276},
numpages = {14},
series = {NIPS '21}
}

@article{10.1016/j.infsof.2019.05.009,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh and Marston, Chad},
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.009},
doi = {10.1016/j.infsof.2019.05.009},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {131–145},
numpages = {15},
keywords = {Case study, User trust, Business-to-business, Machine learning, Big data}
}

@inproceedings{10.5555/3540261.3542579,
author = {Ostapenko, Oleksiy and Rodríguez, Pau and Caccia, Massimo and Charlin, Laurent},
title = {Continual learning via local module composition},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modularity is a compelling solution to continual learning (CL), the problem of modeling sequences of related tasks. Learning and then composing modules to solve different tasks provides an abstraction to address the principal challenges of CL including catastrophic forgetting, backward and forward transfer across tasks, and sub-linear model growth. We introduce local module composition (LMC), an approach to modular CL where each module is provided a local structural component that estimates a module's relevance to the input. Dynamic module composition is performed layer-wise based on local relevance scores. We demonstrate that agnosticity to task identities (IDs) arises from (local) structural learning that is module-specific as opposed to the task- and/or model-specific as in previous works, making LMC applicable to more CL settings compared to previous works. In addition, LMC also tracks statistics about the input distribution and adds new modules when outlier samples are detected. In the first set of experiments, LMC performs favorably compared to existing methods on the recent Continual Transfer-learning Benchmark without requiring task identities. In another study, we show that the locality of structural learning allows LMC to interpolate to related but unseen tasks (OOD), as well as to compose modular networks trained independently on different task sequences into a third modular network without any fine-tuning. Finally, in search for limitations of LMC we study it on more challenging sequences of 30 and 100 tasks, demonstrating that local module selection becomes much more challenging in presence of a large number of candidate modules. In this setting best performing LMC spawns much fewer modules compared to an oracle based baseline, however it reaches a lower overall accuracy.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2318},
numpages = {15},
series = {NIPS '21}
}

@article{10.1016/j.knosys.2021.107476,
author = {Wang, Jun and Zhang, Xiaofang and Chen, Lin},
title = {How well do pre-trained contextual language representations recommend labels for GitHub issues?},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {232},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107476},
doi = {10.1016/j.knosys.2021.107476},
journal = {Know.-Based Syst.},
month = nov,
numpages = {11},
keywords = {Language model, Data analysis, Issue labeling, Deep learning}
}

@inproceedings{10.14236/ewic/HCI2017.21,
author = {Sboui, Thouraya and Ayed, Mounir Ben and Alimi, Adel M.},
title = {A meta-model for run time adaptation in a UI-DSPL process},
year = {2017},
publisher = {BCS Learning &amp; Development Ltd.},
address = {Swindon, GBR},
url = {https://doi.org/10.14236/ewic/HCI2017.21},
doi = {10.14236/ewic/HCI2017.21},
abstract = {Unlike the conventional Software Product Line (SPL) process, the Dynamic Software Product Line (DSPL) process continues to reconfigure and adapt at runtime. In the context of the development of a family of adaptable user interfaces (UIs) and in order to facilitate the design and the development of the runtime adaptation mechanism, we propose a model which defines fundamental concepts required by the UI adaptation mechanism. The model aims to support user interface designers to develop and conceptualize system that accommodate context-awareness, and dynamic runtime adaptation requirements.},
booktitle = {Proceedings of the 31st British Computer Society Human Computer Interaction Conference},
articleno = {21},
numpages = {7},
keywords = {run-time UI adaptation, adaptation modeling, UI-DSPL approach},
location = {Sunderland, UK},
series = {HCI '17}
}

@article{10.1016/j.infsof.2014.04.002,
author = {Machado, Ivan Do Carmo and Mcgregor, John D. and Cavalcanti, Yguaratã Cerqueira and De Almeida, Eduardo Santana},
title = {On strategies for testing software product lines: A systematic literature review},
year = {2014},
issue_date = {October, 2014},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {56},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2014.04.002},
doi = {10.1016/j.infsof.2014.04.002},
abstract = {Context: Testing plays an important role in the quality assurance process for software product line engineering. There are many opportunities for economies of scope and scale in the testing activities, but techniques that can take advantage of these opportunities are still needed. Objective: The objective of this study is to identify testing strategies that have the potential to achieve these economies, and to provide a synthesis of available research on SPL testing strategies, to be applied towards reaching higher defect detection rates and reduced quality assurance effort. Method: We performed a literature review of two hundred seventy-six studies published from the year 1998 up to the 1st semester of 2013. We used several filters to focus the review on the most relevant studies and we give detailed analyses of the core set of studies. Results: The analysis of the reported strategies comprised two fundamental aspects for software product line testing: the selection of products for testing, and the actual test of products. Our findings indicate that the literature offers a large number of techniques to cope with such aspects. However, there is a lack of reports on realistic industrial experiences, which limits the inferences that can be drawn. Conclusion: This study showed a number of leveraged strategies that can support both the selection of products, and the actual testing of products. Future research should also benefit from the problems and advantages identified in this study.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1183–1199},
numpages = {17},
keywords = {Systematic literature review, Software testing, Software quality, Software product lines}
}

@article{10.1007/s00158-015-1242-z,
author = {Zou, Jun and Yao, Wei-Xing and Xia, Tian-Xiang},
title = {Extension of concurrent subspace optimization to structural optimization of product families},
year = {2015},
issue_date = {August    2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {52},
number = {2},
issn = {1615-147X},
url = {https://doi.org/10.1007/s00158-015-1242-z},
doi = {10.1007/s00158-015-1242-z},
abstract = {This paper discusses the problem of structural optimization of product families with predefined platforms. The main challenge lies in the increased design variables and constraints, and providing an optimal tradeoff for individual products performance in the family which are competitive with each other. The Concurrent Subspace Optimization for multidisciplinary problem is extended to product family design with predefined platforms. The main advantage of the proposed approach is that the system level owns the ability to catch the global tendency of the true design space and the number of evaluations required is reduced by using surrogate models. Each subspace optimization problem has the freedom to specify the unique variables for one family member, and the system level optimizes the product platform using the surrogate models created based on subspace optimizations. The process is solved in an iterative way, and the improving surrogate models guide the optimization to the global optimal design. Results from a truss family example with small design space confirm the ability and efficiency of the extended Concurrent Subspace Optimization to address product family problem by compared with ATC approach. Then the proposed method is successfully applied to a family of unmanned aircraft wing structures, which is more complicated and related to practical implementation issues.},
journal = {Struct. Multidiscip. Optim.},
month = aug,
pages = {281–291},
numpages = {11},
keywords = {Surrogate model, Structural optimal design, Product family, Concurrent subspace optimization}
}

@article{10.1145/1183236.1183267,
author = {Czarnecki, Krzysztof and Antkiewicz, Michal and Kim, Chang Hwan Peter},
title = {Multi-level customization in application engineering},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183267},
doi = {10.1145/1183236.1183267},
abstract = {Developing mechanisms for mapping features to analysis models.},
journal = {Commun. ACM},
month = dec,
pages = {60–65},
numpages = {6}
}

@inproceedings{10.1145/3168365.3168372,
author = {Acher, Mathieu and Temple, Paul and Jézéquel, Jean-Marc and Galindo, José A. and Martinez, Jabier and Ziadi, Tewfik},
title = {VaryLATEX: Learning Paper Variants That Meet Constraints},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168372},
doi = {10.1145/3168365.3168372},
abstract = {How to submit a research paper, a technical report, a grant proposal, or a curriculum vitae that respect imposed constraints such as formatting instructions and page limits? It is a challenging task, especially when coping with time pressure. In this work, we present VaryLATEX, a solution based on variability, constraint programming, and machine learning techniques for documents written in LATEX to meet constraints and deliver on time. Users simply have to annotate LATEX source files with variability information, e.g., (de)activating portions of text, tuning figures' sizes, or tweaking line spacing. Then, a fully automated procedure learns constraints among Boolean and numerical values for avoiding non-acceptable paper variants, and finally, users can further configure their papers (e.g., aesthetic considerations) or pick a (random) paper variant that meets constraints, e.g., page limits. We describe our implementation and report the results of two experiences with VaryLATEX.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {83–88},
numpages = {6},
keywords = {variability modelling, technical writing, machine learning, generators, constraint programming, LATEX},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1016/j.procs.2019.12.024,
author = {Ivan, Ion and Budacu, Eduard and Despa, Mihai Liviu},
title = {Using profiling to assemble an agile collaborative software development team made up of freelancers},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {162},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.024},
doi = {10.1016/j.procs.2019.12.024},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {562–570},
numpages = {9},
keywords = {machine learning, agile, collaborative teams, profiling, data analysis}
}

@inproceedings{10.1145/3340531.3412007,
author = {Chih, Hao-Yi and Fan, Yao-Chung and Peng, Wen-Chih and Kuo, Hai-Yuan},
title = {Product Quality Prediction with Convolutional Encoder-Decoder Architecture and Transfer Learning},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412007},
doi = {10.1145/3340531.3412007},
abstract = {Mining data collected from industrial manufacturing process plays an important role for intelligent manufacturing in Industry 4.0. In this paper, we propose a deep convolutional model for predicting wafer fabrication quality in an intelligent integrated-circuit manufacturing application. The wafer fabrication quality prediction is motivated by the need for improving product line efficiency and reducing manufacturing cost by detecting potential defective work-in-process (WIP) wafers. This work considers the following two crucial data characteristics for wafer fabrication. First, our model is designed to learn spatial correlation between quality measurements on WIP wafers and fabrication results through an encoder-decoder neural network. Second, we leverage the fact that different products share the same raw manufacturing process to enable the knowledge transferring between prediction models of different products. Performance evaluation on real data sets is conducted to validate the strengths of our model on quality prediction, model interpretability, and feasibility of transferring knowledge.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {195–204},
numpages = {10},
keywords = {transfer learning, product quality prediction, multi-task learning, industrial data mining, few-shot learning},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1016/j.knosys.2019.105424,
author = {Liu, Xiaoshuang and Luo, Senlin and Pan, Limin},
title = {Robust boosting via self-sampling},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {193},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105424},
doi = {10.1016/j.knosys.2019.105424},
journal = {Know.-Based Syst.},
month = apr,
numpages = {10},
keywords = {Self-sampling, Robustness, Loss function, Boosting}
}

@inproceedings{10.1109/ASE.2015.44,
author = {Martinez, Jabier and Ziadi, Tewfik and Bissyandé, Tegawendé F. and Klein, Jacques and Traon, Yves le},
title = {Automating the extraction of model-based software product lines from model variants},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.44},
doi = {10.1109/ASE.2015.44},
abstract = {We address the problem of automating 1) the analysis of existing similar model variants and 2) migrating them into a software product line. Our approach, named MoVa2PL, considers the identification of variability and commonality in model variants, as well as the extraction of a CVL-compliant Model-based Software Product Line (MSPL) from the features identified on these variants. MoVa2PL builds on a generic representation of models making it suitable to any MOF-based models. We apply our approach on variants of the open source ArgoUML UML modeling tool as well as on variants of an Inflight Entertainment System. Evaluation with these large and complex case studies contributed to show how our feature identification with structural constraints discovery and the MSPL generation process are implemented to make the approach valid (i.e., the extracted software product line can be used to regenerate all variants considered) and sound (i.e., derived variants which did not exist are at least structurally valid).},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {396–406},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/3379597.3387500,
author = {Fry, Tanner and Dey, Tapajit and Karnauch, Andrey and Mockus, Audris},
title = {A Dataset and an Approach for Identity Resolution of 38 Million Author IDs extracted from 2B Git Commits},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387500},
doi = {10.1145/3379597.3387500},
abstract = {The data collected from open source projects provide means to model large software ecosystems, but often suffer from data quality issues, specifically, multiple author identification strings in code commits might actually be associated with one developer. While many methods have been proposed for addressing this problem, they are either heuristics requiring manual tweaking, or require too much calculation time to do pairwise comparisons for 38M author IDs in, for example, the World of Code collection. In this paper, we propose a method that finds all author IDs belonging to a single developer in this entire dataset, and share the list of all author IDs that were found to have aliases. To do this, we first create blocks of potentially connected author IDs and then use a machine learning model to predict which of these potentially related IDs belong to the same developer. We processed around 38 million author IDs and found around 14.8 million IDs to have an alias, which belong to 5.4 million different developers, with the median number of aliases being 2 per developer. This dataset can be used to create more accurate models of developer behaviour at the entire OSS ecosystem level and can be used to provide a service to rapidly resolve new author IDs.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {518–522},
numpages = {5},
keywords = {Machine Learning, Identity Resolution, Heuristics, Git Commits, Data Sharing},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@article{10.1016/j.scico.2012.06.007,
author = {Cetina, Carlos and Giner, Pau and Fons, Joan and Pelechano, Vicente},
title = {Prototyping Dynamic Software Product Lines to evaluate run-time reconfigurations},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.06.007},
doi = {10.1016/j.scico.2012.06.007},
abstract = {Dynamic Software Product Lines (DSPL) encompass systems that are capable of modifying their own behavior with respect to changes in their operating environment by using run-time reconfigurations. A failure in these reconfigurations can directly impact the user experience since the reconfigurations are performed when the system is already under the users control. In this work, we prototype a Smart Hotel DSPL to evaluate the reliability-based risk of the DSPL reconfigurations, specifically, the probability of malfunctioning (Availability) and the consequences of malfunctioning (Severity). This DSPL prototype was performed with the participation of human subjects by means of a Smart Hotel case study which was deployed with real devices. Moreover, we successfully identified and addressed two challenges associated with the involvement of human subjects in DSPL prototyping: enabling participants to (1) trigger the run-time reconfigurations and to (2) understand the effects of the reconfigurations. The evaluation of the case study reveals positive results regarding both Availability and Severity. However, the participant feedback highlights issues with recovering from a failed reconfiguration or a reconfiguration triggered by mistake. To address these issues, we discuss some guidelines learned in the case study. Finally, although the results achieved by the DSPL may be considered satisfactory for its particular domain, DSPL engineers must provide users with more control over the reconfigurations or the users will not be comfortable with DSPLs.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {2399–2413},
numpages = {15},
keywords = {Variability modeling, Smart Hotel, Dynamic Software Product Line}
}

@article{10.1613/jair.1.11688,
author = {Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
title = {Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11688},
doi = {10.1613/jair.1.11688},
abstract = {Interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as machine learning, computer vision, and natural language processing. Much of the growth in these fields has been made possible with deep learning, a sub-area of machine learning that uses artificial neural networks. This has created significant interest in the integration of vision and language. In this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey stimulates innovative thoughts and ideas to address the existing challenges and build new applications.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1183–1317},
numpages = {135},
keywords = {deep learning, computer vision, machine learning, natural language}
}

@inproceedings{10.1145/3366750.3366762,
author = {Li, Yong and E, Fei},
title = {Intelligent Semi-Submersible Heavy Transport Vessel},
year = {2019},
isbn = {9781450372480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366750.3366762},
doi = {10.1145/3366750.3366762},
abstract = {As the major ocean transport tool of large intact cargo, the SSHTV (semi-submersible heavy transport vessel) has become more and more valued with the rapid advance of ocean engineering, polar exploration and global trade. Simultaneously, higher safety, reliability, efficiency and performance requirements for SSHTV are needed to guarantee owner and operator profit. An intelligent SSHTV concept is presented here, based on development and maturity of sensor technology, information technology, data technology and IoT (Internet of Things) technology, all aimed at satisfying these requirements. The concept combines intelligent navigation, onboard equipment and energy management, intelligent vessel and fleet operation. It embodies the integration of people, things and service, and leads the development trend.},
booktitle = {Proceedings of the 2019 2nd International Conference on Machine Learning and Machine Intelligence},
pages = {63–67},
numpages = {5},
keywords = {surrounding awareness, semi-submersible heavy transport vessel, remote diagnosis, intelligent ship, Marine},
location = {Jakarta, Indonesia},
series = {MLMI '19}
}

@inproceedings{10.1007/978-3-030-76352-7_18,
author = {Liu, Xiaotong and Tong, Yingbei and Xu, Anbang and Akkiraju, Rama},
title = {Using Language Models to Pre-train Features for Optimizing Information Technology Operations Management Tasks},
year = {2020},
isbn = {978-3-030-76351-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-76352-7_18},
doi = {10.1007/978-3-030-76352-7_18},
abstract = {Information Technology (IT) Operations management is a vexing problem for most companies that rely on IT systems for mission-critical business applications. While IT operators are increasingly leveraging analytical tools powered by artificial intelligence (AI), the volume, the variety and the complexity of data generated in the IT Operations domain poses significant challenges in managing the applications. In this work, we present an approach to leveraging language models to pre-train features for optimizing IT Operations management tasks such as anomaly prediction from logs. Specifically, using log-based anomaly prediction as the task, we show that the machine learning models built using language models (embeddings) trained with IT Operations domain data as features outperform those AI models built using language models with general-purpose data as features. Furthermore, we present our empirical results outlining the influence of factors such as the type of language models, the type of input data, and the diversity of input data, on the prediction accuracy of our log anomaly prediction model when language models trained from IT Operations domain data are used as features. We also present the run-time inference performance of log anomaly prediction models built using language models as features in an IT Operations production environment.},
booktitle = {Service-Oriented Computing  – ICSOC 2020 Workshops: AIOps, CFTIC, STRAPS, AI-PA, AI-IOTS, and Satellite Events, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {150–161},
numpages = {12},
keywords = {Anomaly detection, Language modeling, AI for IT operations},
location = {Dubai, United Arab Emirates}
}

@article{10.1016/j.jss.2008.08.026,
author = {Lago, Patricia and Muccini, Henry and van Vliet, Hans},
title = {A scoped approach to traceability management},
year = {2009},
issue_date = {January, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2008.08.026},
doi = {10.1016/j.jss.2008.08.026},
abstract = {Traceability is the ability to describe and follow the life of a software artifact and a means for modeling the relations between software artifacts in an explicit way. Traceability has been successfully applied in many software engineering communities and has recently been adopted to document the transition among requirements, architecture and implementation. We present an approach to customize traceability to the situation at hand. Instead of automating tracing, or representing all possible traces, we scope the traces to be maintained to the activities stakeholders must carry out. We define core traceability paths, consisting of essential traceability links required to support the activities. We illustrate the approach through two examples: product derivation in software product lines, and release planning in software process management. By using a running software product line example, we explain why the core traceability paths identified are needed when navigating from feature to structural models and from family to product level and backward between models used in software product derivation. A feasibility study in release planning carried out in an industrial setting further illustrates the use of core traceability paths during production and measures the increase in performance of the development processes supported by our approach. These examples show that our approach can be successfully used to support both product and process traceability in a pragmatic yet efficient way.},
journal = {J. Syst. Softw.},
month = jan,
pages = {168–182},
numpages = {15},
keywords = {Traceability paths, Traceability issues, Software product line, Software process management}
}

@inproceedings{10.5555/3504035.3504406,
author = {Gong, Tieliang and Wang, Guangtao and Ye, Jieping and Xu, Zongben and Lin, Ming},
title = {Margin based PU learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {The PU learning problem concerns about learning from positive and unlabeled data. A popular heuristic is to iteratively enlarge training set based on some margin-based criterion. However, little theoretical analysis has been conducted to support the success of these heuristic methods. In this work, we show that not all margin-based heuristic rules are able to improve the learned classifiers iteratively. We find that a so-called large positive margin oracle is necessary to guarantee the success of PU learning. Under this oracle, a provable positive-margin based PU learning algorithm is proposed for linear regression and classification under the truncated Gaussian distributions. The proposed algorithm is able to reduce the recovering error geometrically proportional to the positive margin. Extensive experiments on real-world datasets verify our theory and the state-of-the-art performance of the proposed PU learning algorithm.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {371},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.1145/3001867.3001874,
author = {Queiroz, Rodrigo and Berger, Thorsten and Czarnecki, Krzysztof},
title = {Towards predicting feature defects in software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001874},
doi = {10.1145/3001867.3001874},
abstract = {Defect-prediction techniques can enhance the quality assurance activities for software systems. For instance, they can be used to predict bugs in source files or functions. In the context of a software product line, such techniques could ideally be used for predicting defects in features or combinations of features, which would allow developers to focus quality assurance on the error-prone ones. In this preliminary case study, we investigate how defect prediction models can be used to identify defective features using machine-learning techniques. We adapt process metrics and evaluate and compare three classifiers using an open-source product line. Our results show that the technique can be effective. Our best scenario achieves an accuracy of 73 % for accurately predicting features as defective or clean using a Naive Bayes classifier. Based on the results we discuss directions for future work.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {58–62},
numpages = {5},
keywords = {software product lines, features, defect prediction},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/2897845.2897856,
author = {Meng, Guozhu and Xue, Yinxing and Mahinthan, Chandramohan and Narayanan, Annamalai and Liu, Yang and Zhang, Jie and Chen, Tieming},
title = {Mystique: Evolving Android Malware for Auditing Anti-Malware Tools},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897856},
doi = {10.1145/2897845.2897856},
abstract = {In the arms race of attackers and defenders, the defense is usually more challenging than the attack due to the unpredicted vulnerabilities and newly emerging attacks every day. Currently, most of existing malware detection solutions are individually proposed to address certain types of attacks or certain evasion techniques. Thus, it is desired to conduct a systematic investigation and evaluation of anti-malware solutions and tools based on different attacks and evasion techniques. In this paper, we first propose a meta model for Android malware to capture the common attack features and evasion features in the malware. Based on this model, we develop a framework, MYSTIQUE, to automatically generate malware covering four attack features and two evasion features, by adopting the software product line engineering approach. With the help of MYSTIQUE, we conduct experiments to 1) understand Android malware and the associated attack features as well as evasion techniques; 2) evaluate and compare the 57 off-the-shelf anti-malware tools, 9 academic solutions and 4 App market vetting processes in terms of accuracy in detecting attack features and capability in addressing evasion. Last but not least, we provide a benchmark of Android malware with proper labeling of contained attack and evasion features.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {365–376},
numpages = {12},
keywords = {malware generation, evolutionary algorithm, defense capability, android feature model},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@article{10.1016/j.patcog.2011.09.011,
author = {Rasmussen, Peter M. and Hansen, Lars K. and Madsen, Kristoffer H. and Churchill, Nathan W. and Strother, Stephen C.},
title = {Model sparsity and brain pattern interpretation of classification models in neuroimaging},
year = {2012},
issue_date = {June, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2011.09.011},
doi = {10.1016/j.patcog.2011.09.011},
abstract = {Interest is increasing in applying discriminative multivariate analysis techniques to the analysis of functional neuroimaging data. Model interpretation is of great importance in the neuroimaging context, and is conventionally based on a 'brain map' derived from the classification model. In this study we focus on the relative influence of model regularization parameter choices on both the model generalization, the reliability of the spatial patterns extracted from the classification model, and the ability of the resulting model to identify relevant brain networks defining the underlying neural encoding of the experiment. For a support vector machine, logistic regression and Fisher's discriminant analysis we demonstrate that selection of model regularization parameters has a strong but consistent impact on the generalizability and both the reproducibility and interpretable sparsity of the models for both @?"2 and @?"1 regularization. Importantly, we illustrate a trade-off between model spatial reproducibility and prediction accuracy. We show that known parts of brain networks can be overlooked in pursuing maximization of classification accuracy alone with either @?"2 and/or @?"1 regularization. This supports the view that the quality of spatial patterns extracted from models cannot be assessed purely by focusing on prediction accuracy. Our results instead suggest that model regularization parameters must be carefully selected, so that the model and its visualization enhance our ability to interpret the brain.},
journal = {Pattern Recogn.},
month = jun,
pages = {2085–2100},
numpages = {16},
keywords = {Sparsity, Regularization, Pattern analysis, Neuroimaging, NPAIRS resampling, Model interpretation, Machine learning, Kernel methods, Classification}
}

@article{10.1145/1183236.1183254,
author = {Blank, Douglas},
title = {Robots make computer science personal},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183254},
doi = {10.1145/1183236.1183254},
abstract = {They also make it more hands-on, real, practical, and immediate, inspiring a new generation of scientists' deep interest in the field.},
journal = {Commun. ACM},
month = dec,
pages = {25–27},
numpages = {3}
}

@inbook{10.5555/3454287.3455282,
author = {Saxena, Shreyas and Tuzel, Oncel and DeCoste, Dennis},
title = {Data parameters: a new family of parameters for learning a differentiable curriculum},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent works have shown that learning from easier instances first can help deep neural networks (DNNs) generalize better. However, knowing which data to present during different stages of training is a challenging problem. In this work, we address this problem by introducing data parameters. More specifically, we equip each sample and class in a dataset with a learnable parameter (data parameters), which governs their importance in the learning process. During training, at each iteration, as we update the model parameters, we also update the data parameters. These updates are done by gradient descent and do not require hand-crafted rules or design. When applied to image classification task on CIFAR10, CIFAR100, WebVision and ImageNet datasets, and object detection task on KITTI dataset, learning a dynamic curriculum via data parameters leads to consistent gains, without any increase in model complexity or training time. When applied to a noisy dataset, the proposed method learns to learn from clean images and improves over the state-of-the-art methods by 14%. To the best of our knowledge, our work is the first curriculum learning method to show gains on large scale image classification and detection tasks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {995},
numpages = {11}
}

@article{10.1016/j.engappai.2020.103551,
author = {Kara, Ertugrul and Traquair, Mark and Simsek, Murat and Kantarci, Burak and Khan, Shahzad},
title = {Holistic design for deep learning-based discovery of tabular structures in datasheet images},
year = {2020},
issue_date = {Apr 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {90},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2020.103551},
doi = {10.1016/j.engappai.2020.103551},
journal = {Eng. Appl. Artif. Intell.},
month = apr,
numpages = {14},
keywords = {Structure detection, Page object detection, Tabular data extraction, Table detection, Document processing, Image processing, Deep learning}
}

@inproceedings{10.1007/978-3-030-58208-1_3,
author = {Tajiri, Yui and Mimura, Mamoru},
title = {Detection of Malicious PowerShell Using Word-Level Language Models},
year = {2020},
isbn = {978-3-030-58207-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58208-1_3},
doi = {10.1007/978-3-030-58208-1_3},
abstract = {There is a growing tendency for cybercriminals to abuse legitimate tools installed on the target computers for cyberattacks. In particular, the use of PowerShell provided by Microsoft has been increasing every year and has become a threat. In previous studies, a method to detect malicious PowerShell commands using character-level deep learning was proposed. The proposed method combines traditional natural language processing and character-level convolutional neural networks. This method, however, requires time for dynamic analysis. This paper proposes a method to classify unknown PowerShell without dynamic analysis. Our method uses feature vectors extracted from malicious and benign PowerShell scripts using word-level language models for classification. The datasets were generated from benign and malicious PowerShell scripts obtained from Hybrid Analysis, and benign PowerShell scripts obtained from GitHub, which are imbalanced. The experimental result shows that the combination of the LSI and XGBoost produces the highest detection rate. The maximum accuracy achieves approximately 0.95 on the imbalanced dataset. Furthermore, over 50% of unknown malicious PowerShell scripts could be detected in time series analysis without dynamic analysis.},
booktitle = {Advances in Information and Computer Security: 15th International Workshop on Security, IWSEC 2020, Fukui, Japan, September 2–4, 2020, Proceedings},
pages = {39–56},
numpages = {18},
keywords = {XGBoost, Doc2Vec, Latent Semantic Indexing, PowerShell},
location = {Fukui, Japan}
}

@inproceedings{10.5555/3489212.3489216,
author = {Yu, Lingjing and Luo, Bo and Ma, Jun and Zhou, Zhaoyu and Liu, Qingyun},
title = {You are what you broadcast: identification of mobile and IoT devices from (public) WiFi},
year = {2020},
isbn = {978-1-939133-17-5},
publisher = {USENIX Association},
address = {USA},
abstract = {With the rapid growth of mobile devices and WiFi hotspots, security risks arise. In practice, it is critical for administrators of corporate and public wireless networks to identify the type and/or model of devices connected to the network, in order to set access/firewall rules, to check for known vulnerabilities, or to configure IDS accordingly. Mobile devices are not obligated to report their detailed identities when they join a (public) wireless network, while adversaries could easily forge device attributes. In the literature, efforts have been made to utilize features from network traffic for device identification. In this paper, we present OWL, a novel device identification mechanism for both network administrators and normal users. We first extract network traffic features from passively received broadcast and multicast (BC/MC) packets. Embedding representations are learned to model features into six independent and complementary views. We then present a new multi-view wide and deep learning (MvWDL) framework that is optimized on both generalization performance and label-view interaction performance. Meanwhile, a malicious device detection mechanism is designed to assess the inconsistencies across views in the multi-view classifier to identify anomalies. Finally, we demonstrate OWL's performance through experiments, case studies, and qualitative analysis.},
booktitle = {Proceedings of the 29th USENIX Conference on Security Symposium},
articleno = {4},
numpages = {18},
series = {SEC'20}
}

@inproceedings{10.1007/978-3-030-55024-0_5,
author = {Aggarwal, Dippy and Shekhar, Shreyas and Elford, Chris and Jayachandran, Umachandar and Krishnamurthy, Sadashivan and Reding, Jamie and Niebruegge, Brendan},
title = {TPCx-BB (Big Bench) in a Single-Node Environment},
year = {2019},
isbn = {978-3-030-55023-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-55024-0_5},
doi = {10.1007/978-3-030-55024-0_5},
abstract = {Big data tends to concentrate on the data volume and variety which requires large cluster capabilities to process diverse and heterogeneous data. Currently, NoSQL/Hadoop-based cluster frameworks are known to excel at handling this form of data by scaling across nodes and distributed query processing. But for certain data sizes, relational databases can also support these workloads. In this paper, we support this claim over a popular relational database engine, Microsoft* SQL Server* 2019 (pre-release candidate) using a big data benchmark, BigBench. Our work in this paper is the industry first case study that runs BigBench on a single node environment powered by Intel® XeonTM processor 8164 product family and enterprise-class Intel® SSDs. We make the following two contributions: (1) present response times of all 30 BigBench queries when run sequentially to showcase the advanced analytics and machine learning capabilities integrated within SQL Server 2019, and (2) present results from data scalability experiments over two scale factors (1 TB, 3 TB) to understand the impact of increase in data size on query runtimes. We further characterize a subset of queries to understand their resource consumption requirements (CPU/IO/memory) on a single node system. We will conclude by correlating our initial engineering study to similar research studies on cluster-based configurations providing a further hint to the potential of relational databases to run reasonably scaled big-data workloads.},
booktitle = {Performance Evaluation and Benchmarking for the Era of Cloud(s): 11th TPC Technology Conference, TPCTC 2019, Los Angeles, CA, USA, August 26, 2019, Revised Selected Papers},
pages = {64–83},
numpages = {20},
keywords = {Natural language processing, Machine learning, BigBench, Big data, Microsoft* SQL Server* 2019, TPCx-BB},
location = {Los Angeles, CA, USA}
}

@article{10.1007/s10515-021-00287-w,
author = {Gadelha, Guilherme and Ramalho, Franklin and Massoni, Tiago},
title = {Traceability recovery between bug reports and test cases-a Mozilla Firefox case study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-021-00287-w},
doi = {10.1007/s10515-021-00287-w},
abstract = {Automatic recovery of traceability between software artifacts may promote early detection of issues and better calculate change impact. Information Retrieval (IR) techniques have been proposed for the task, but they differ considerably in input parameters and results. It is difficult to assess results when those techniques are applied in isolation, usually in small or medium-sized software projects. Recently, multilayered approaches to machine learning, in special Deep Learning (DL), have achieved success in text classification through their capacity to model complex relationships among data. In this article, we apply several IR and DL techniques for investing automatic traceability between bug reports and manual test cases, using historical data from the Mozilla Firefox’s Quality Assurance (QA) team. In this case study, we assess the following IR techniques: LSI, LDA, and BM25, in addition to a DL architecture called Convolutional Neural Networks (CNNs), through the use of Word Embeddings. In this context of traceability, we observe poor performances from three out of the four studied techniques. Only the LSI technique presented acceptable results, standing out even over the state-of-the-art BM25 technique. The obtained results suggest that the semi-automatic application of the LSI technique – with an appropriate combination of thresholds – may be feasible for real-world software projects.},
journal = {Automated Software Engg.},
month = nov,
numpages = {46},
keywords = {Deep learning, Information retrieval, Traceability, Test cases, System features, Bug reports}
}

@article{10.1007/s10664-019-09686-w,
author = {Minku, Leandro L.},
title = {A novel online supervised hyperparameter tuning procedure applied to cross-company software effort estimation},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09686-w},
doi = {10.1007/s10664-019-09686-w},
abstract = {Software effort estimation is an online supervised learning problem, where new training projects may become available over time. In this scenario, the Cross-Company (CC) approach Dycom can drastically reduce the number of Within-Company (WC) projects needed for training, saving their collection cost. However, Dycom requires CC projects to be split into subsets. Both the number and composition of such subsets can affect Dycom’s predictive performance. Even though clustering methods could be used to automatically create CC subsets, there are no procedures for automatically tuning the number of clusters over time in online supervised scenarios. This paper proposes the first procedure for that. An investigation of Dycom using six clustering methods and three automated tuning procedures is performed, to check whether clustering with automated tuning can create well performing CC splits. A case study with the ISBSG Repository shows that the proposed tuning procedure in combination with a simple threshold-based clustering method is the most successful in enabling Dycom to drastically reduce (by a factor of 10) the number of required WC training projects, while maintaining (or even improving) predictive performance in comparison with a corresponding WC model. A detailed analysis is provided to understand the conditions under which this approach does or does not work well. Overall, the proposed online supervised tuning procedure was generally successful in enabling a very simple threshold-based clustering approach to obtain the most competitive Dycom results. This demonstrates the value of automatically tuning hyperparameters over time in a supervised way.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {3153–3204},
numpages = {52},
keywords = {Hyperparameter tuning, Online learning, Concept drift, Transfer learning, Cross-company learning, Software effort estimation}
}

@article{10.1016/j.infsof.2019.05.007,
author = {Ebrahimi, Neda and Trabelsi, Abdelaziz and Islam, Md. Shariful and Hamou-Lhadj, Abdelwahab and Khanmohammadi, Kobra},
title = {An HMM-based approach for automatic detection and classification of duplicate bug reports},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.007},
doi = {10.1016/j.infsof.2019.05.007},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {98–109},
numpages = {12},
keywords = {Mining software repositories, Machine learning, Hidden Markov models, Stack traces, Duplicate bug reports}
}

@article{10.1016/j.cageo.2014.09.004,
author = {Buccella, Agustina and Cechich, Alejandra and Pol'la, Matias and Arias, Maximiliano and del Socorro Doldan, Maria and Morsan, Enrique},
title = {Marine ecology service reuse through taxonomy-oriented SPL development},
year = {2014},
issue_date = {December 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {73},
number = {C},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2014.09.004},
doi = {10.1016/j.cageo.2014.09.004},
abstract = {Nowadays, reusing software applications encourages researchers and industrials to collaborate in order to increase software quality and to reduce software development costs. However, effective reuse is not easy and only a limited portion of reusable models actually offers effective evidence regarding their appropriateness, usability and/or effectiveness. Focusing reuse on a particular domain, such as marine ecology, allows us to narrow the scope; and along with a systematic approach such as software product line development, helps us to potentially improving reuse. From our experiences developing a subdomain-oriented software product line (SPL for the marine ecology subdomain), in this paper we describe semantic resources created for assisting this development and thus promoting systematic software reuse. The main contributions of our work are focused on the definition of a standard conceptual model for marine ecology applications together with a set of services and guides which assist the process of product derivation. The services are structured in a service taxonomy (as a specialization of the ISO 19119 std) in which we create a new set of categories and services built over a conceptual model for marine ecology applications. We also define and exemplify a set of guides for composing the services of the taxonomy in order to fulfill different functionalities of particular systems in the subdomain. HighlightsSolutions for software reuse for GIS domains by using standard information.Domain-specific taxonomy for supporting the generation of software artifacts.Guides for using geographic services in order to fulfill different GIS functionalities of systems in the domain.Evaluation of the effectiveness of the taxonomy and guides when building an SPL and two derived products.Improvements on time and costs of new GIS products being developed.},
journal = {Comput. Geosci.},
month = dec,
pages = {108–121},
numpages = {14},
keywords = {Software reuse, ISO 19100 standards, Geographic information systems, Domain-specific taxonomies, Domain engineering}
}

@inproceedings{10.1145/3302333.3302347,
author = {Njima, Mercy and Demeyer, Serge},
title = {An Exploratory Study on Migrating Single-Products towards Product Lines in Startup Contexts},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302347},
doi = {10.1145/3302333.3302347},
abstract = {A majority of technology startups fail; inadequate software engineering practices are known to be a contributing factor. The smooth transitioning towards software product lines in particular is a major stumbling block for startups that must broaden their product portfolio to deal with divergent demands imposed by the market. We conducted a preliminary study within two software engineering startups, which revealed the motivating factors and benefits that would lead to the migration from a single product into a product line. Despite the benefits, tackling the challenges foreseen and the identification of features and their relations in the current product is the crucial first step towards implementing an appropriate highly-configurable product portfolio.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {6},
keywords = {Transition, Startups, Software Product Lines, Feature Identification, Exploratory Study},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3379597.3387457,
author = {Pecorelli, Fabiano and Palomba, Fabio and Khomh, Foutse and De Lucia, Andrea},
title = {Developer-Driven Code Smell Prioritization},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387457},
doi = {10.1145/3379597.3387457},
abstract = {Code smells are symptoms of poor implementation choices applied during software evolution. While previous research has devoted effort in the definition of automated solutions to detect them, still little is known on how to support developers when prioritizing them. Some works attempted to deliver solutions that can rank smell instances based on their severity, computed on the basis of software metrics. However, this may not be enough since it has been shown that the recommendations provided by current approaches do not take the developer's perception of design issues into account. In this paper, we perform a first step toward the concept of developer-driven code smell prioritization and propose an approach based on machine learning able to rank code smells according to the perceived criticality that developers assign to them. We evaluate our technique in an empirical study to investigate its accuracy and the features that are more relevant for classifying the developer's perception. Finally, we compare our approach with a state-of-the-art technique. Key findings show that the our solution has an F-Measure up to 85% and outperforms the baseline approach.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {220–231},
numpages = {12},
keywords = {Machine Learning for Software Engineering, Empirical Software Engineering, Code smells},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@article{10.1007/s11263-018-1112-4,
author = {Zhang, Dingwen and Han, Junwei and Zhao, Long and Meng, Deyu},
title = {Leveraging Prior-Knowledge for Weakly Supervised Object Detection Under a Collaborative Self-Paced Curriculum Learning Framework},
year = {2019},
issue_date = {April     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {4},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-018-1112-4},
doi = {10.1007/s11263-018-1112-4},
abstract = {Weakly supervised object detection is an interesting yet challenging research topic in computer vision community, which aims at learning object models to localize and detect the corresponding objects of interest only under the supervision of image-level annotation. For addressing this problem, this paper establishes a novel weakly supervised learning framework to leverage both the instance-level prior-knowledge and the image-level prior-knowledge based on a novel collaborative self-paced curriculum learning (C-SPCL) regime. Under the weak supervision, C-SPCL can leverage helpful prior-knowledge throughout the whole learning process and collaborate the instance-level confidence inference with the image-level confidence inference in a robust way. Comprehensive experiments on benchmark datasets demonstrate the superior capacity of the proposed C-SPCL regime and the proposed whole framework as compared with state-of-the-art methods along this research line.},
journal = {Int. J. Comput. Vision},
month = apr,
pages = {363–380},
numpages = {18},
keywords = {Weakly supervised learning, Self-paced larning, Object detection}
}

@inproceedings{10.1145/1404946.1404947,
author = {Yie, Andres and Casallas, Rubby and Deridder, Dirk and Van Der Straeten, Ragnhild},
title = {Multi-step concern refinement},
year = {2008},
isbn = {9781605581439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1404946.1404947},
doi = {10.1145/1404946.1404947},
abstract = {A Model-Driven Software Product Line (MD-SPL) uses metamodels, models, and transformations to create a family of products using a Model Refinement Line (MRL). However, an MD-SPL must evolve and provide mechanisms to add new crosscutting concerns, such as security or logging, to the applications. Our problem is that we want to preserve and reuse the original MRL. In this paper, we present the challenges associated with this problem. We illustrate them by evaluating different model-driven approaches to add crosscutting concerns into an application using high-level aspects. Furthermore, we propose an approach to add crosscutting concerns as early aspects and to preserve the original MRL. This approach adds a new MRL that refines a high-level model of the concern. This high-level model is related with the high-level application model in the original MRL. The refinement of the application model and the concern model proceeds in parallel. The presented approach is a work in progress and requires us to tackle several challenges in order to implement and validate the proposal.},
booktitle = {Proceedings of the 2008 AOSD Workshop on Early Aspects},
articleno = {1},
numpages = {8},
keywords = {software product line, model-driven engineering, model refinement, early aspects, aspect-oriented software development},
location = {Brussels, Belgium},
series = {EA '08}
}

@inproceedings{10.1145/2430502.2430511,
author = {Kolesnikov, Sergiy S. and Apel, Sven and Siegmund, Norbert and Sobernig, Stefan and Kästner, Christian and Senkaya, Semah},
title = {Predicting quality attributes of software product lines using software and network measures and sampling},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430511},
doi = {10.1145/2430502.2430511},
abstract = {Software product-line engineering aims at developing families of related products that share common assets to provide customers with tailor-made products. Customers are often interested not only in particular functionalities (i.e., features), but also in non-functional quality attributes, such as performance, reliability, and footprint. Measuring quality attributes of all products of a product line usually does not scale. In this research-in-progress report, we propose a systematic approach aiming at efficient and scalable prediction of quality attributes of products. To this end, we establish predictors for certain categories of quality attributes (e.g., a predictor for high memory consumption) based on software and network measures, and receiver operating characteristic analysis. We use these predictors to guide a sampling process that takes the assets of a product line as input and determines the products that fall into the category denoted by the given predictor (e.g., products with high memory consumption). We propose to use predictors to make the process of finding "acceptable" products more efficient. We discuss and compare several strategies to incorporate predictors in the sampling process.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {5},
keywords = {software product lines, sampling, quality attributes, prediction, metrics},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1145/2974927.2974950,
author = {Plummer, Shawn and Warden, David},
title = {Puppet: Introduction, Implementation, &amp; the Inevitable Refactoring},
year = {2016},
isbn = {9781450340953},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2974927.2974950},
doi = {10.1145/2974927.2974950},
abstract = {Puppet is configuration management software that lets you define the desired state for a computer and enforce that state on scheduled periodic executions of the Puppet agent on that computer. About a year and a half after implementing Puppet on our campus, we had a functional but inflexible and poorly organized Puppet configuration code base. We refactored it multiple times before settling on our current layout based on the roles/profiles strategy and Hiera. We will share a brief introduction to Puppet, how it is useful to us and, most importantly, what we learned over 3 years of implementing it on our campus.},
booktitle = {Proceedings of the 2016 ACM SIGUCCS Annual Conference},
pages = {131–134},
numpages = {4},
keywords = {sysadmin, puppet, configuration management, configuration as code., automation},
location = {Denver, Colorado, USA},
series = {SIGUCCS '16}
}

@article{10.1287/ited.2019.0216cs,
author = {Rao, B. Madhu and Xanthopoulos, Petros and Zheng, Qipeng Phil},
title = {Case—Production Scheduling at DeLand Crayon Company },
year = {2020},
issue_date = {January 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {20},
number = {2},
issn = {1532-0545},
url = {https://doi.org/10.1287/ited.2019.0216cs},
doi = {10.1287/ited.2019.0216cs},
abstract = {DeLand Crayon Company (DCC) is a large manufacturer of molded wax crayons. DCC sells three sizes of crayons, namely standard, large and jumbo. Standard-sized crayons, which account for most of the demand, include nine basic colors (red, blue, green, yellow, orange, violet, brown, black, and white), seven popular combination colors (red-orange, yellow-orange, yellow-green, red-violet, blue-green, blue-violet, and pink), 16 fluorescent colors, and four metallic colors. DCC also manufactures several specialty crayons, such as reduced length, hexagonal shaped, golden glitter, glow-in-the-dark, wipe-off, washable, and keno. Demand for non–standard-size crayons, fluorescent, and metallic-colored standard crayons and the specialty crayons is low and uneven, but DCC believes that they must be maintained to create the perception of a broad product line.},
journal = {INFORMS Trans. Edu.},
month = jan,
pages = {99–101},
numpages = {3}
}

@inproceedings{10.1145/3377814.3381715,
author = {Schneider, Jean-Guy and Eklund, Peter W. and Lee, Kevin and Chen, Feifei and Cain, Andrew and Abdelrazek, Mohamed},
title = {Adopting industry agile practices in large-scale capstone education},
year = {2020},
isbn = {9781450371247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377814.3381715},
doi = {10.1145/3377814.3381715},
abstract = {This paper presents the practice and experience in adopting an agile organizational model for a final-year capstone program in Software Engineering. The model developed is motivated by having real (and developing) software artifacts with incrementally changing team members working on a product-line. This in turn results in more sophisticated capstone student-project outcomes. The model proposed supports student mentoring and promotes, through its internal organization, leadership and personal responsibility. The students are supported by professional software engineers, up-skilling workshops, and academic supervisors who act as a personalized reporting and grading point for the team. The academic supervisors are themselves supported by a tribe leader, a faculty member who assumes overall responsibility for a product-line, and who acts as a report to an external industry client/sponsor. This paper describes the motivation for the capstone model, its adoption, and some preliminary observations.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering Education and Training},
pages = {119–129},
numpages = {11},
keywords = {software engineering education, capstone education, agile software development},
location = {Seoul, South Korea},
series = {ICSE-SEET '20}
}

@inproceedings{10.1145/2430502.2430513,
author = {Berger, Thorsten and Rublack, Ralf and Nair, Divya and Atlee, Joanne M. and Becker, Martin and Czarnecki, Krzysztof and Wąsowski, Andrzej},
title = {A survey of variability modeling in industrial practice},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430513},
doi = {10.1145/2430502.2430513},
abstract = {Over more than two decades, numerous variability modeling techniques have been introduced in academia and industry. However, little is known about the actual use of these techniques. While dozens of experience reports on software product line engineering exist, only very few focus on variability modeling. This lack of empirical data threatens the validity of existing techniques, and hinders their improvement. As part of our effort to improve empirical understanding of variability modeling, we present the results of a survey questionnaire distributed to industrial practitioners. These results provide insights into application scenarios and perceived benefits of variability modeling, the notations and tools used, the scale of industrial models, and experienced challenges and mitigation strategies.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {7},
numpages = {8},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1007/978-3-030-62463-7_4,
author = {Sun, Chao and Tang, Mingjing and Liang, Li and Zou, Wei},
title = {Software Entity Recognition Method Based on BERT Embedding},
year = {2020},
isbn = {978-3-030-62462-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-62463-7_4},
doi = {10.1007/978-3-030-62463-7_4},
abstract = {The global open source software ecosystem contains rich information in the field of software engineering. The existing analysis methods for the text content of the knowledge community in this field are mainly focus on the structural relationship and rule-based association and mining. This paper proposes a software entity recognition method based on BERT word embedding. Firstly, the BiLSTM-CRF model is constructed, and the entity recognition model is constructed by combining the word vector embedding in software engineering field. Then, the word vector in the input layer of the model is improved by introducing the BERT pre-training language model. In the process of pre-training of BERT, the pre-training data should be constructed based on the discussion content of Stack Overflow software Q &amp; A community. Then, we use these data to pre-training the BERT model, so as to obtain the word vector representation suitable for software engineering field, improving the effect of entity recognition in software engineering field, and solving the problem that the traditional word vector embedding is mostly based on the general domain data training, which is not fully suitable for software engineering field, and can’t well represent the context semantic information. At the same time, to solve the problem that there are few annotated data in the field of software, this paper tries to extends the data appropriately by the method of model prediction and dictionary matching, and carries out experimental test. Finally, this paper uses the method of deep learning to realize the entity recognition in the field of software engineering, so as to provide support for the extraction of software entities, the construction of software knowledge base, and the intelligent application of software engineering.},
booktitle = {Machine Learning for Cyber Security: Third International Conference, ML4CS 2020, Guangzhou, China, October 8–10, 2020, Proceedings, Part III},
pages = {33–47},
numpages = {15},
keywords = {Stack overflow, BERT model, Entity recognition},
location = {Guangzhou, China}
}

@article{10.1007/s10270-013-0364-2,
author = {Acher, Mathieu and Cleve, Anthony and Collet, Philippe and Merle, Philippe and Duchien, Laurence and Lahire, Philippe},
title = {Extraction and evolution of architectural variability models in plugin-based systems},
year = {2014},
issue_date = {October   2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-013-0364-2},
doi = {10.1007/s10270-013-0364-2},
abstract = {Variability management is a key issue when building and evolving software-intensive systems, making it possible to extend, configure, customize and adapt such systems to customers' needs and specific deployment contexts. A wide form of variability can be found in extensible software systems, typically built on top of plugin-based architectures that offer a (large) number of configuration options through plugins. In an ideal world, a software architect should be able to generate a system variant on-demand, corresponding to a particular assembly of plugins. To this end, the variation points and constraints between architectural elements should be properly modeled and maintained over time (i.e., for each version of an architecture). A crucial, yet error-prone and time-consuming, task for a software architect is to build an accurate representation of the variability of an architecture, in order to prevent unsafe architectural variants and reach the highest possible level of flexibility. In this article, we propose a reverse engineering process for producing a variability model (i.e., a feature model) of a plugin-based architecture. We develop automated techniques to extract and combine different variability descriptions, including a hierarchical software architecture model, a plugin dependency model and the software architect knowledge. By computing and reasoning about differences between versions of architectural feature models, software architect can control both the variability extraction and evolution processes. The proposed approach has been applied to a representative, large-scale plugin-based system (FraSCAti), considering different versions of its architecture. We report on our experience in this context.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {1367–1394},
numpages = {28},
keywords = {Variability, Software evolution, Reverse engineering, Product lines, Configuration management, Architecture recovery}
}

@inproceedings{10.1145/2000259.2000286,
author = {Cavalcanti, Ricardo de Oliveira and de Almeida, Eduardo Santana and Meira, Silvio R.L.},
title = {Extending the RiPLE-DE process with quality attribute variability realization},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000286},
doi = {10.1145/2000259.2000286},
abstract = {Software product lines engineering is a viable way to achieve the productivity gains desired by companies. Product line architecture must benefit from commonalities among products in the family and enable the variability among them. The aspect of variability in quality attributes has been neglected or ignored by most of the researchers as attention has been mainly put in functional variability. This paper describes an architecture and design process for software product lines that can properly deal with quality attribute variability. The proposed approach enhances the RiPLE-DE process for software product line engineering with activities and guidelines for quality attribute variability. An initial experimental study is presented to characterize and evaluate the proposed process enhancements.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {159–164},
numpages = {6},
keywords = {software reuse, software product lines (spl), software architecture, quality attribute variability},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.5555/3291291.3291298,
author = {Islam, Nayreet and Azim, Akramul},
title = {Assuring the runtime behavior of self-adaptive cyber-physical systems using feature modeling},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {A self-adaptive cyber-physical system (SACPS) can adjust its behavior and configurations at runtime in response to varying requirements obtained from the system and the environment. With the increasing use of the SACPS in different application domains, such variations are becoming more common. Users today expect the SACPS to guarantee its functional and timing behavior even in adverse environmental situations. However, uncertainties in the SACPS environment impose challenges on assuring the runtime behavior during system design.Software product line engineering (SPLE) is considered as a useful technique for handling varying requirements. In this paper, we present an approach for assuring the runtime behavior of the SACPS by applying an SPLE technique such as feature modeling. By representing the feature-based model at design time, we characterize the possible adaptation requirements to reusable configurations. The proposed approach aims to model two dynamic variability dimensions: 1) environment variability that describes the conditions under which the SACPS must adapt, and 2) structural variability, that defines the resulting architectural configurations. To validate our approach, the experimental analysis is performed using two case studies: 1) a traffic monitoring SACPS and 2) an automotive SACPS. We demonstrate that the proposed feature-based modeling approach can be used to achieve adaptivity which allows the SACPS to assure functional (defining execution of the correct set of adaptive tasks) and non-functional (defining execution of SACPS in the expected mode) correctness at runtime. The experimental results show that the feature-based SACPS demonstrates significant improvement in terms of self-configuration time, self-adaptation time and scalability with less probability of failure in different environmental situations.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {48–59},
numpages = {12},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@article{10.1287/msom.2019.0805,
author = {Mišić, Velibor V. and Perakis, Georgia},
title = {Data Analytics in Operations Management: A Review},
year = {2020},
issue_date = {January-February 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {22},
number = {1},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2019.0805},
doi = {10.1287/msom.2019.0805},
abstract = {Research in operations management has traditionally focused on models for understanding, mostly at a strategic level, how firms should operate. Spurred by the growing availability of data and recent advances in machine learning and optimization methodologies, there has been an increasing application of data analytics to problems in operations management. In this paper, we review recent applications of data analytics to operations management in three major areas—supply chain management, revenue management, and healthcare operations—and highlight some exciting directions for the future.},
journal = {Manufacturing &amp; Service Operations Management},
month = jan,
pages = {158–169},
numpages = {12},
keywords = {operations management, machine learning, data analytics}
}

@article{10.1007/s10618-008-0097-y,
author = {Forman, George},
title = {Quantifying counts and costs via classification},
year = {2008},
issue_date = {October   2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {2},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-008-0097-y},
doi = {10.1007/s10618-008-0097-y},
abstract = {Many business applications track changes over time, for example, measuring the monthly prevalence of influenza incidents. In situations where a classifier is needed to identify the relevant incidents, imperfect classification accuracy can cause substantial bias in estimating class prevalence. The paper defines two research challenges for machine learning. The `quantification' task is to accurately estimate the number of positive cases (or class distribution) in a test set, using a training set that may have a substantially different distribution. The `cost quantification' variant estimates the total cost associated with the positive class, where each case is tagged with a cost attribute, such as the expense to resolve the case. Quantification has a very different utility model from traditional classification research. For both forms of quantification, the paper describes a variety of methods and evaluates them with a suitable methodology, revealing which methods give reliable estimates when training data is scarce, the testing class distribution differs widely from training, and the positive class is rare, e.g., 1% positives. These strengths can make quantification practical for business use, even where classification accuracy is poor.},
journal = {Data Min. Knowl. Discov.},
month = oct,
pages = {164–206},
numpages = {43},
keywords = {Text mining, Supervised machine learning, Quantification research methodology, Prevalence estimation, Detecting and tracking trends, Concept drift, Classification, Class imbalance, Class distribution estimation}
}

@inproceedings{10.1007/978-3-662-45234-9_20,
author = {Collet, Philippe},
title = {Domain Specific Languages for Managing Feature Models: Advances and Challenges},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_20},
doi = {10.1007/978-3-662-45234-9_20},
abstract = {Managing multiple and complex feature models is a tedious and error-prone activity in software product line engineering. Despite many advances in formal methods and analysis techniques, the supporting tools and APIs are not easily usable together, nor unified. In this paper, we report on the development and evolution of the Familiar Domain-Specific Language DSL. Its toolset is dedicated to the large scale management of feature models through a good support for separating concerns, composing feature models and scripting manipulations. We overview various applications of Familiar and discuss both advantages and identified drawbacks. We then devise salient challenges to improve such DSL support in the near future.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {273–288},
numpages = {16}
}

@article{10.1007/s10270-010-0186-4,
author = {Perrouin, Gilles and Vanwormhoudt, Gilles and Morin, Brice and Lahire, Philippe and Barais, Olivier and Jézéquel, Jean-Marc},
title = {Weaving variability into domain metamodels},
year = {2012},
issue_date = {July      2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-010-0186-4},
doi = {10.1007/s10270-010-0186-4},
abstract = {Domain-specific modeling languages (DSMLs) are the essence of MDE. A DSML describes the concepts of a particular domain in a metamodel, as well as their relationships. Using a DSML, it is possible to describe a wide range of different models that often share a common base and vary on some parts. On the one hand, some current approaches tend to distinguish the variability language from the DSMLs themselves, implying greater learning curve for DSMLs stakeholders and a significant overhead in product line engineering. On the other hand, approaches integrating variability in DSMLs lack generality and tool support. We argue that aspect-oriented modeling techniques enabling flexible metamodel composition and results obtained by the software product line community to manage and resolve variability form the pillars for a solution for integrating variability into DSMLs. In this article, we consider variability as an independent and generic aspect to be woven into the DSML. In particular, we detail how variability is woven and how to perform product line derivation. We validate our approach through the weaving of variability into two different metamodels: Ecore--widely used for DSML definition--and SmartAdapters, our aspect model weaver. These results emphasize how new abilities of the language can be provided by this means.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {361–383},
numpages = {23},
keywords = {Variability and software product lines, Model weaving, Domain specific languages}
}

@article{10.1016/j.neucom.2019.08.002,
author = {Li, Zhenglai and Tang, Chang and Chen, Jiajia and Wan, Cheng and Yan, Weiqing and Liu, Xinwang},
title = {Diversity and consistency learning guided spectral embedding for multi-view clustering},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {370},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.08.002},
doi = {10.1016/j.neucom.2019.08.002},
journal = {Neurocomput.},
month = dec,
pages = {128–139},
numpages = {12},
keywords = {99-00, 00-01, Diversity and consistency learning, Spectral embedding, Multi-view clustering}
}

@article{10.1016/j.neucom.2019.11.104,
author = {Ren, Yazhou and Huang, Shudong and Zhao, Peng and Han, Minghao and Xu, Zenglin},
title = {Self-paced and auto-weighted multi-view clustering},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {383},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.104},
doi = {10.1016/j.neucom.2019.11.104},
journal = {Neurocomput.},
month = mar,
pages = {248–256},
numpages = {9},
keywords = {Soft weighting, Multi-view clustering, Self-paced learning}
}

@inproceedings{10.1145/3377024.3377031,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {Fast static analyses of software product lines: an example with more than 42,000 metrics},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377031},
doi = {10.1145/3377024.3377031},
abstract = {Context: Software metrics, as one form of static analyses, is a commonly used approach in software engineering in order to understand the state of a software system, in particular to identify potential areas prone to defects. Family-based techniques extract variability information from code artifacts in Software Product Lines (SPLs) to perform static analysis for all available variants. Many different types of metrics with numerous variants have been defined in literature. When counting all metrics including such variants, easily thousands of metrics can be defined. Computing all of them for large product lines can be an extremely expensive process in terms of performance and resource consumption.Objective: We address these performance and resource challenges while supporting customizable metric suites, which allow running both, single system and variability-aware code metrics.Method: In this paper, we introduce a partial parsing approach used for the efficient measurement of more than 42,000 code metric variations. The approach covers variability information and restricts parsing to the relevant parts of the Abstract Syntax Tree (AST).Conclusions: This partial parsing approach is designed to cover all relevant information to compute a broad variety of variability-aware code metrics on code artifacts containing annotation-based variability, e.g., realized with C-preprocessor statements. It allows for the flexible combination of single system and variability-aware metrics, which is not supported by existing tools. This is achieved by a novel representation of partially parsed product line code artifacts, which is tailored to the computation of the metrics. Our approach consumes considerably less resources, especially when computing many metric variants in parallel.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {9},
keywords = {variability models, software product lines, metrics, implementation, feature models, abstract syntax trees, SPL, AST},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1016/j.asoc.2016.05.020,
author = {Sachdeva, Jainy and Kumar, Vinod and Gupta, Indra and Khandelwal, Niranjan and Ahuja, Chirag Kamal},
title = {A package-SFERCB-"Segmentation, feature extraction, reduction and classification analysis by both SVM and ANN for brain tumors"},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.020},
doi = {10.1016/j.asoc.2016.05.020},
abstract = {An interactive computer aided dignostic (CAD) system for assisting inexperience young radiologists is developed. The difficulty in brain tumors classification is due to similar size, shape, location, hetrogeniety, presence of oedema, cystic and isointense regions has been the key feature of this research. Genetic Algorithm is employed as it is an easy concept and is well understood by radiologists without going in much depth of engineering.Display Omitted Brain tumors as segmented regions of interests (SROIs) by content based active contour model (CBAC).Feature extraction-intensity and texture based features.Feature reduction by Genetic Algorithm.Classification by Hybrid Models-GA-SVM and GA-ANN. The objective of this experimentation is to develop an interactive CAD system for assisting radiologists in multiclass brain tumor classification. The study is performed on a diversified dataset of 428 post contrast T1-weighted MR images of 55 patients and publically available dataset of 260 post contrast T1-weighted MR images of 10 patients. The first dataset includes primary brain tumors such as Astrocytoma (AS), Glioblastoma Multiforme (GBM), childhood tumor-Medulloblastoma (MED) and Meningioma (MEN), along with secondary tumor-Metastatic (MET). The second dataset consists of Astrocytoma (AS), Low Grade Glioma (LGL) and Meningioma (MEN). The tumor regions are marked by content based active contour (CBAC) model. The regions are than saved as segmented regions of interest (SROIs). 71 intensity and texture feature set is extracted from these SROIs. The features are specifically selected based on the pathological details of brain tumors provided by the radiologist. Genetic Algorithm (GA) selects the set of optimal features from this input set. Two hybrid machine learning models are implemented using GA with support vector machine (SVM) and artificial neural network (ANN) (GA-SVM and GA-ANN) and are tested on two different datasets. GA-SVM is proposed for finding preliminary probability in identifying tumor class and GA-ANN is used for confirmation of accuracy. Test results of the first dataset show that the GA optimization technique has enhanced the overall accuracy of SVM from 79.3% to 91.7% and of ANN from 75.6% to 94.9%. Individual class accuracies delivered by GA-SVM are: AS-89.8%, GBM-83.3%, MED-95.6%, MEN-91.8%, and MET-97.1%. Individual class accuracies delivered by GA-ANN classifier are: AS-96.6%, GBM-86.6%, MED-93.3%, MEN-96%, MET-100%. Similar results are obtained for the second dataset. The overall accuracy of SVM has increased from 80.8% to 89% and that of ANN has increased from 77.5% to 94.1%. Individual class accuracies delivered by GA-SVM are: AS-85.3%, LGL-88.8%, MEN-93%. Individual class accuracies delivered by GA-ANN classifier are: AS-92.6%, LGL-94.4%, MED-95.3%. It is observed from the experiments that GA-ANN classifier has provided better results than GA-SVM. Further, it is observed that along with providing finer results, GA-SVM provides advantage in speed whereas GA-ANN provides advantage in accuracy. The combined results from both the classifiers will benefit the radiologists in forming a better decision for classifying brain tumors.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {151–167},
numpages = {17},
keywords = {Genetic Algorithm (GA), GA-SVM, GA-ANN, Brain tumors}
}

@inproceedings{10.1109/WAIN52551.2021.00027,
author = {Lanubile, Filippo and Calefato, Fabio and Quaranta, Luigi and Amoruso, Maddalena and Fumarola, Fabio and Filannino, Michele},
title = {Towards Productizing AI/ML Models: An Industry Perspective from Data Scientists},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00027},
doi = {10.1109/WAIN52551.2021.00027},
abstract = {The transition from AI/ML models to production-ready AI-based systems is a challenge for both data scientists and software engineers. In this paper, we report the results of a workshop conducted in a consulting company to understand how this transition is perceived by practitioners. Starting from the need for making AI experiments reproducible, the main themes that emerged are related to the use of the Jupyter Notebook as the primary prototyping tool, and the lack of support for software engineering best practices as well as data science specific functionalities.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {129–132},
numpages = {4},
location = {Madrid, Spain}
}

@article{10.1145/3364684,
author = {Bianchini, Ricardo and Fontoura, Marcus and Cortez, Eli and Bonde, Anand and Muzio, Alexandre and Constantin, Ana-Maria and Moscibroda, Thomas and Magalhaes, Gabriel and Bablani, Girish and Russinovich, Mark},
title = {Toward ML-centric cloud platforms},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3364684},
doi = {10.1145/3364684},
abstract = {Exploring the opportunities to use ML, the possible designs, and our experience with Microsoft Azure.},
journal = {Commun. ACM},
month = jan,
pages = {50–59},
numpages = {10}
}

@inproceedings{10.1145/3324884.3416627,
author = {Li, Mingyang and Shi, Lin and Yang, Ye and Wang, Qing},
title = {A deep multitask learning approach for requirements discovery and annotation from open forum},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416627},
doi = {10.1145/3324884.3416627},
abstract = {The ability in rapidly learning and adapting to evolving user needs is key to modern business successes. Existing methods are based on text mining and machine learning techniques to analyze user comments and feedback, and often constrained by heavy reliance on manually codified rules or insufficient training data. Multitask learning (MTL) is an effective approach with many successful applications, with the potential to address these limitations associated with requirements analysis tasks. In this paper, we propose a deep MTL-based approach, DEMAR, to address these limitations when discovering requirements from massive issue reports and annotating the sentences in support of automated requirements analysis. DEMAR consists of three main phases: (1) data augmentation phase, for data preparation and allowing data sharing beyond single task learning; (2) model construction phase, for constructing the MTL-based model for requirements discovery and requirements annotation tasks; and (3) model training phase, enabling eavesdropping by shared loss function between the two related tasks. Evaluation results from eight open-source projects show that, the proposed multitask learning approach outperforms two state-of-the-art approaches (CNC and FRA) and six common machine learning algorithms, with the precision of 91% and the recall of 83% for requirements discovery task, and the overall accuracy of 83% for requirements annotation task. The proposed approach provides a novel and effective way to jointly learn two related requirements analysis tasks. We believe that it also sheds light on further directions of exploring multitask learning in solving other software engineering problems.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {336–348},
numpages = {13},
keywords = {deep learning, multitask learning, requirements annotation, requirements discovery},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/1842752.1842812,
author = {Abbas, Nadeem and Andersson, Jesper and Löwe, Welf},
title = {Autonomic Software Product Lines (ASPL)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842812},
doi = {10.1145/1842752.1842812},
abstract = {We describe ongoing work on a variability mechanism for Autonomic Software Product Lines (ASPL). The autonomic software product lines have self-management characteristics that make product line instances more resilient to context changes and some aspects of product line evolution. Instances sense the context, selects and bind the best component variants to variation-points at run-time. The variability mechanism we describe is composed of a profile guided dispatch based on off-line and on-line training processes. Together they form a simple, yet powerful variability mechanism that continuously learns, which variants to bind given the current context and system goals.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {324–331},
numpages = {8},
keywords = {variation-points, variants, variability, on-line, off-line training, goals, context, autonomic elements, MAPE-K},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@article{10.1145/3472291,
author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
title = {A Survey of Deep Active Learning},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3472291},
doi = {10.1145/3472291},
abstract = {Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due.It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {180},
numpages = {40},
keywords = {deep active learning, active learning, Deep learning}
}

@article{10.1016/j.comcom.2021.07.002,
author = {Ben Slimen, Yosra and Balcerzak, Joanna and Pagès, Albert and Agraz, Fernando and Spadaro, Salvatore and Koutsopoulos, Konstantinos and Al-Bado, Mustafa and Truong, Thuy and Giardina, Pietro G. and Bernini, Giacomo},
title = {Quality of perception prediction in 5G slices for e-Health services using user-perceived QoS},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {178},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2021.07.002},
doi = {10.1016/j.comcom.2021.07.002},
journal = {Comput. Commun.},
month = oct,
pages = {1–13},
numpages = {13},
keywords = {Network cognitive management, Machine learning, Quality of experience, 5G network slicing}
}

@article{10.1016/j.is.2012.11.010,
author = {GröNer, Gerd and BošKović, Marko and Silva Parreiras, Fernando and GašEvić, Dragan},
title = {Modeling and validation of business process families},
year = {2013},
issue_date = {July, 2013},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {38},
number = {5},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2012.11.010},
doi = {10.1016/j.is.2012.11.010},
abstract = {Process modeling is an expensive task that needs to encompass requirements of different stakeholders, assure compliance with different standards, and enable the flexible adaptivity to newly emerging requirements in today's dynamic global market. Identifying reusability of process models is a promising direction towards reducing the costs of process modeling. Recent research has offered several solutions. Such solutions promote effective and formally sound methods for variability modeling and configuration management. However, ensuring behavioral validity of reused process models with respect to the original process models (often referred to as reference process models) is still an open research challenge. To address this challenge, in this paper, we propose the notion of business process families by building upon the well-known software engineering discipline-software product line engineering. Business process families comprise (i) a variability modeling perspective, (ii) a process model template (or reference model), and (iii) mappings between (i) and (ii). For business process families, we propose a correct validation algorithm ensuring that each member of a business process family adheres to the core intended behavior that is specified in the process model template. The proposed validation approach is based on the use of Description Logics, variability is represented by using the well-known Feature Models and behavior of process models is considered in terms of control flow patterns. The paper also reports on the experience gained in two external trial cases and results obtained by measuring the tractability of the implementation of the proposed validation approach.},
journal = {Inf. Syst.},
month = jul,
pages = {709–726},
numpages = {18},
keywords = {Validation, Process model variability, Process model configuration, Control flow relations, Business process families}
}

@article{10.1007/s10799-018-0296-1,
author = {Wang, Yu and Li, Minqiang and Feng, Haiyang and Feng, Nan},
title = {Optimal sequential releasing strategy for software products in the presence of word-of-mouth and requirements uncertainty},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3},
issn = {1385-951X},
url = {https://doi.org/10.1007/s10799-018-0296-1},
doi = {10.1007/s10799-018-0296-1},
abstract = {This study proposes a two-period analytical model to explore the relative optimality of three sequential releasing strategies for a software vendor: (1) Skipping Strategy—skip the limited-functionality version and introduce a full-functionality version in the next period, (2) Replacement Strategy—release a limited-functionality version first and replace it with a full-functionality version later, and (3) Line-extension Strategy—release a limited-functionality version first and extend the product line by releasing a full-functionality version later. Word-of-mouth (WOM) effect and uncertainty in consumers’ requirements are taken into consideration. Our analysis shows that Skipping Strategy is optimal when the net WOM is negative and sufficiently small, while the Replacement Strategy becomes the optimal choice when the net WOM is negative but not sufficiently small. However, the Line-extension Strategy dominates other two strategies when the net WOM is positive. Different pricing patterns may be chosen when adopting Line-extension Strategy, i.e. when the net WOM is positive and not very large, Low–High pricing pattern (a relatively low price for the limited-functionality version and a relatively high price for the full-functionality version) is optimal, while High–Low pricing pattern (a relatively high price for the limited-functionality version and a relatively low price for the full-functionality version) becomes optimal when the net WOM is positive and sufficiently large. Numerical experiments show that the software vendor could improve its profit by choosing optimal quality design&nbsp;but the overall dominance pattern of the three release strategies is similar no matter whether the quality design is exogenously given or optimally chosen.},
journal = {Inf. Technol. and Management},
month = sep,
pages = {153–174},
numpages = {22},
keywords = {Requirements uncertainty, Word-of-mouth, Quality design strategy, Pricing strategy, Sequential releasing strategy, Software products}
}

@article{10.1007/s00607-019-00772-x,
author = {Chimalakonda, Sridhar and Nori, Kesav V.},
title = {A family of software product lines in educational technologies},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {102},
number = {8},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-019-00772-x},
doi = {10.1007/s00607-019-00772-x},
abstract = {Rapid advances in education domain demand the design and customization of educational technologies for a large scale and variety of evolving requirements. Here, scale is the number of systems to be developed and variety stems from a diversified range of instructional designs such as varied goals, processes, content, teaching styles, learning styles and, also for eLearning Systems for 22 Indian Languages and variants. In this paper, we present a family of software product lines as an approach to address this challenge of modeling a family of instructional designs as well as a family of eLearning Systems and demonstrate it for the case of adult literacy in India (287 million learners). We present a multi-level product line that connects product lines at multiple levels of granularity in education domain. We then detail two concrete product lines (), one that generates instructional design editors and two, which generates a family of eLearning Systems based on flexible instructional designs. Finally, we demonstrate our approach by generating eLearning Systems for Hindi and Telugu languages, which led to significant cost savings of 29 person-months for 9 eLearning Systems.},
journal = {Computing},
month = aug,
pages = {1765–1792},
numpages = {28},
keywords = {97B60, 68T30, 68U35, 68N30, 68N19, Adult literacy, eLearning Systems, Instructional design, Educational technologies, Software product lines}
}

@inproceedings{10.1109/ESEM.2017.14,
author = {Falessi, Davide and Russo, Barbara and Mullen, Kathleen},
title = {What if i had no smells?},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.14},
doi = {10.1109/ESEM.2017.14},
abstract = {What would have happened if I did not have any code smell? This is an interesting question that no previous study, to the best of our knowledge, has tried to answer. In this paper, we present a method for implementing a what-if scenario analysis estimating the number of defective files in the absence of smells. Our industrial case study shows that 20% of the total defective files were likely avoidable by avoiding smells. Such estimation needs to be used with the due care though as it is based on a hypothetical history (i.e., zero number of smells and same process and product change characteristics). Specifically, the number of defective files could even increase for some types of smells. In addition, we note that in some circumstances, accepting code with smells might still be a good option for a company.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {78–84},
numpages = {7},
keywords = {technical debt, software estimation, machine learning, code smells},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@article{10.1016/j.specom.2019.09.003,
author = {Shirzhiyan, Zahra and Shamsi, Elham and Jafarpisheh, Amir Salar and Jafari, Amir Homayoun},
title = {Objective classification of auditory brainstem responses to consonant-vowel syllables using local discriminant bases},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {114},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.09.003},
doi = {10.1016/j.specom.2019.09.003},
journal = {Speech Commun.},
month = nov,
pages = {36–48},
numpages = {13},
keywords = {Local discriminant bases, Speech encoding, Speech ABR}
}

@inproceedings{10.1145/3278122.3278126,
author = {Peldszus, Sven and Strüber, Daniel and Jürjens, Jan},
title = {Model-based security analysis of feature-oriented software product lines},
year = {2018},
isbn = {9781450360456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278122.3278126},
doi = {10.1145/3278122.3278126},
abstract = {Today's software systems are too complex to ensure security after the fact – security has to be built into systems by design. To this end, model-based techniques such as UMLsec support the design-time specification and analysis of security requirements by providing custom model annotations and checks. Yet, a particularly challenging type of complexity arises from the variability of software product lines. Analyzing the security of all products separately is generally infeasible. In this work, we propose SecPL, a methodology for ensuring security in a software product line. SecPL allows developers to annotate the system design model with product-line variability and security requirements. To keep the exponentially large configuration space tractable during security checks, SecPL provides a family-based security analysis. In our experiments, this analysis outperforms the naive strategy of checking all products individually. Finally, we present the results of a user study that indicates the usability of our overall methodology.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {93–106},
numpages = {14},
keywords = {UML, Software Product Lines, Security, OCL},
location = {Boston, MA, USA},
series = {GPCE 2018}
}

@inproceedings{10.1007/11751588_46,
author = {Moon, Mikyeong and Yeom, Keunhyuk},
title = {An approach to developing domain architectures based on variability analysis},
year = {2006},
isbn = {3540340726},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11751588_46},
doi = {10.1007/11751588_46},
abstract = {Software product line engineering is a method that prepares for future reuse and supports seamless reuse in the application development process. A domain architecture, sharing a common software architecture across a product line, brings a core set of knowledge and assets to the development process. Domain architectures reduce the complexity and cost of developing and maintaining code. However, technically excellent domain architectures do fail, often because they are not effectively developed. The key concept in the development of domain architecture is variability, the ability to derive various products from the product family. Variability occurs at different levels. It is important to consider variability at the higher architectural levels, and not only at the code level. In this paper, we suggest a method of producing architectures that will be core assets in the product line. We describe a domain architecture where commonality and variability are explicitly considered.},
booktitle = {Proceedings of the 2006 International Conference on Computational Science and Its Applications - Volume Part II},
pages = {441–450},
numpages = {10},
location = {Glasgow, UK},
series = {ICCSA'06}
}

@article{10.1007/s11219-021-09550-5,
author = {Alkharabsheh, Khalid and Crespo, Yania and Fernández-Delgado, Manuel and Viqueira, José R. and Taboada, José A.},
title = {Exploratory study of the impact of project domain and size category on the detection of the God class design smell},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09550-5},
doi = {10.1007/s11219-021-09550-5},
abstract = {Design smell detection has proven to be an efficient strategy to improve software quality and consequently decrease maintainability expenses. This work explores the influence of the&nbsp;information  about&nbsp;project context expressed as project domain and size category information, on the automatic detection of the god class design smell by machine learning techniques. A set of experiments using eight classifiers to detect god classes was conducted on a dataset containing 12, 587 classes from 24 Java projects. The results show that classifiers change their behavior when they are used on datasets that differ in these kinds of project information. The results show that god class design smell detection can be improved by feeding machine learning classifiers with this project context information.},
journal = {Software Quality Journal},
month = jun,
pages = {197–237},
numpages = {41},
keywords = {God class, Project context information, Software metrics, Machine learning, Design smell detection}
}

@inproceedings{10.1007/978-3-030-50334-5_13,
author = {Allen, Donald M. and Goloubew, Dmitry},
title = {Customer Self-remediation of Proactive Network Issue Detection and Notification},
year = {2020},
isbn = {978-3-030-50333-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-50334-5_13},
doi = {10.1007/978-3-030-50334-5_13},
abstract = {Improving computer network availability has been a focus of researchers for the past 30 years and considerable investigation into the use of AI and Machine Learning, primarily in the operate space has been conducted. Previous efforts have been primarily reactive in nature, monitoring networks, developing base models, and trying to predict future failures based on those models. This approach has shown limited success due to the dynamic nature of network equipment and function. Cisco has been developing capabilities over the last decade to proactively analysis network devices and identify issues that could impact a networks availability. In the current approach issues are identified to the customer and it is the customer’s responsibility to identify the issues that they determine need to be fixed. The capability has been trialed over the last 2 years and the research discussed in this paper is focused on the analysis of their actions. Machine Learning is applied to the issue consumption data set and observations made on the features that can be used to predict which issues will be fixed.},
booktitle = {Artificial Intelligence in HCI: First International Conference, AI-HCI 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings},
pages = {197–210},
numpages = {14},
keywords = {Machine Learning, Proactive issue detection, Proactive issue remediation, Network management},
location = {Copenhagen, Denmark}
}

@article{10.1016/j.jss.2019.06.003,
author = {Capilla, Rafael and Fuentes, Lidia and Lochau, Malte},
title = {Software variability in dynamic environments},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.003},
doi = {10.1016/j.jss.2019.06.003},
journal = {J. Syst. Softw.},
month = oct,
pages = {62–64},
numpages = {3}
}

@article{10.1007/s11219-021-09553-2,
author = {Wu, Jie and Wu , Yingbo and Niu, Nan and Zhou, Min},
title = {MHCPDP: multi-source heterogeneous cross-project defect prediction via multi-source transfer learning and autoencoder},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09553-2},
doi = {10.1007/s11219-021-09553-2},
abstract = {Heterogeneous cross-project defect prediction (HCPDP) is aimed at building a defect prediction model for the target project by reusing datasets from source projects, where the source project datasets and target project dataset have different features. Most existing HCPDP methods only remove redundant or unrelated features without exploring the underlying features of cross-project datasets. Additionally, when the&nbsp;transfer learning method is used in HCPDP, these methods ignore the negative effect of transfer learning. In this paper, we propose a novel HCPDP method called multi-source heterogeneous cross-project defect prediction (MHCPDP). To reduce the gap between the target datasets and the source datasets, MHCPDP uses the autoencoder to extract the intermediate features from the original datasets instead of simply removing redundant and unrelated features and adopts a modified autoencoder algorithm to make instance selection for eliminating irrelevant instances from the source domain datasets. Furthermore, by incorporating multiple source projects to increase the number of source datasets, MHCPDP develops a multi-source transfer learning algorithm to reduce the impact of negative transfers and upgrade the performance of the classifier. We comprehensively evaluate MHCPDP on five open source datasets; our experimental results show that MHCPDP not only has significant improvement in two performance metrics but also overcomes the shortcomings of the conventional HCPDP methods.},
journal = {Software Quality Journal},
month = jun,
pages = {405–430},
numpages = {26},
keywords = {Modified autoencoder, Multi-source transfer learning, Heterogeneous cross-project defect prediction, Autoencoder}
}

@inproceedings{10.1609/aaai.v33i01.33015117,
author = {Tang, Ying-Peng and Huang, Sheng-Jun},
title = {Self-paced active learning: query the right thing at the right time},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015117},
doi = {10.1609/aaai.v33i01.33015117},
abstract = {Active learning queries labels from the oracle for the most valuable instances to reduce the labeling cost. In many active learning studies, informative and representative instances are preferred because they are expected to have higher potential value for improving the model. Recently, the results in self-paced learning show that training the model with easy examples first and then gradually with harder examples can improve the performance. While informative and representative instances could be easy or hard, querying valuable but hard examples at early stage may lead to waste of labeling cost. In this paper, we propose a self-paced active learning approach to simultaneously consider the potential value and easiness of an instance, and try to train the model with least cost by querying the right thing at the right time. Experimental results show that the proposed approach is superior to state-of-the-art batch mode active learning methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {628},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/3167132.3167162,
author = {Hielscher, Tommy and Völzke, Henry and Papapetrou, Panagiotis and Spiliopoulou, Myra},
title = {Discovering, selecting and exploiting feature sequence records of study participants for the classification of epidemiological data on hepatic steatosis},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167162},
doi = {10.1145/3167132.3167162},
abstract = {In longitudinal epidemiological studies, participants undergo repeated medical examinations and are thus represented by a potentially large number of short examination outcome sequences. Some of those sequences may contain important information in various forms, such as patterns, with respect to the disease under study, while others may be on features of little relevance to the outcome. In this work, we propose a framework for Discovery, Selection and Exploitation (DiSelEx) of longitudinal epidemiological data, aiming to identify informative patterns among these sequences. DiSelEx combines sequence clustering with supervised learning to identify sequence groups that contribute to class separation. Newly derived and old features are evaluated and selected according to their redundancy and informativeness regarding the target variable. The selected feature set is then used to learn a classification model on the study data. We evaluate DiSelEx on cohort participants for the disorder "hepatic steatosis" and report on the impact on predictive performance when using sequential data in comparison to utilizing only the basic classifier.1},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {6–13},
numpages = {8},
keywords = {time-series clustering, patient similarity, medical data mining, hepatic steatosis, feature selection, epidemiological studies, classification},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1007/978-3-319-35122-3_4,
author = {Tiihonen, Juha and Raatikainen, Mikko and Myllärniemi, Varvana and Männistö, Tomi},
title = {Carrying Ideas from Knowledge-Based Configuration to Software Product Lines},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_4},
doi = {10.1007/978-3-319-35122-3_4},
abstract = {Software variability modelling SVM has become a central concern in software product lines --- especially configurable software product lines CSPL require rigorous SVM. Dynamic SPLs, service oriented SPLs, and autonomous or pervasive systems are examples where CSPLs are applied. Knowledge-based configuration KBC is an established way to address variability modelling aiming for the automatic product configuration of physical products. Our aim was to study what major ideas from KBC can be applied to SVM, particularly in the context of CSPLs. Our main contribution is the identification of major ideas from KBC that could be applied to SVM. First, we call for the separation of types and instances. Second, conceptual clarity of modelling concepts, e.g., having both taxonomical and compositional relations would be useful. Third, we argue for the importance of a conceptual basis that provides a foundation for multiple representations, e.g., graphical and textual. Applying the insights and experiences embedded in these ideas may help in the development of modelling support for software product lines, particularly in terms of conceptual clarity and as a basis for tool support with a high level of automation.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {55–62},
numpages = {8},
keywords = {Variability modelling, Variability management, Knowledge-based configuration, Feature modelling, Conceptualization},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1007/978-3-642-27142-7_48,
author = {Kim, SeHoon and Choi, SeungYoung},
title = {Construction of online behavior monitoring system},
year = {2011},
isbn = {9783642271410},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-27142-7_48},
doi = {10.1007/978-3-642-27142-7_48},
abstract = {Recently, there is a trend of each school being equipped with class behavior analysis rooms for the improvement of class capability. However, many problems exist in the classroom monitoring system such as high cost, limitation of time and space on demonstrator and analyst, ineffective feedback method or difficulty of post self-analysis. In this paper, a method to reduce the cost of installing physical space and overcome the limitation of time for participation of specialists using class behavior analysis system design.},
booktitle = {Proceedings of the Third International Conference on Future Generation Information Technology},
pages = {406–412},
numpages = {7},
keywords = {software product line, online behavior monitoring, evaluation},
location = {Jeju Island, Korea},
series = {FGIT'11}
}

@inproceedings{10.1145/3377024.3377044,
author = {Krüger, Jacob and Berger, Thorsten},
title = {Activities and costs of re-engineering cloned variants into an integrated platform},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377044},
doi = {10.1145/3377024.3377044},
abstract = {Many software systems need to exist in multiple variants. Organizations typically develop variants using clone&amp;own---copying and adapting systems towards new requirements. However, while clone &amp; own is a simple and readily available strategy, it does not scale with the number of variants, and then requires a costly reengineering of the cloned variants into a configurable software platform (a.k.a., software product line). Ideally, organizations could rely on decision models or at least on substantial empirical data to assess the costs and benefits of such a re-engineering. Unfortunately, despite decades of research on product lines and platforms, such data is scarce, not least because obtaining it from industrial reengineering efforts is challenging. We address this gap with a study on re-engineering two cases of cloned variants of open-source Android and Java games. Student developers re-engineered the clones into software product lines, logging their activities and costs. They performed the types of activities typically associated with re-engineering, but the activities were intertwined and done iteratively. The costs were relatively similar among both cases, but the used variability mechanism had a substantial impact. Interestingly, beyond a common diffing tool, no dedicated re-engineering tool was particularly useful. We hope that our results support researchers working on re-engineering techniques and decision models, as well as practitioners trying to assess the costs and activities involved in re-engineering a software platform.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {21},
numpages = {10},
keywords = {software product lines, re-engineering, empirical study, clone &amp; own},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/2635868.2635876,
author = {Schultis, Klaus-Benedikt and Elsner, Christoph and Lohmann, Daniel},
title = {Architecture challenges for internal software ecosystems: a large-scale industry case study},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635876},
doi = {10.1145/2635868.2635876},
abstract = {The idea of software ecosystems encourages organizations to open software projects for external businesses, governing the cross-organizational development by architectural and other measures. Even within a single organization, this paradigm can be of high value for large-scale decentralized software projects that involve various internal, yet self-contained organizational units. However, this intra-organizational decentralization causes architecture challenges that must be understood to reason about suitable architectural measures. We present an in-depth case study on collaboration and architecture challenges in two of these large-scale software projects at Siemens. We performed a total of 46 hours of semi-structured interviews with 17 leading software architects from all involved organizational units. Our major findings are: (1) three collaboration models on a continuum that ranges from high to low coupling, (2) a classification of architecture challenges, together with (3) a qualitative and quantitative exposure of the identified recurring issues along each collaboration model. Our study results provide valuable insights for both industry and academia: Practitioners that find themselves in one of the collaboration models can use empirical evidence on challenges to make informed decisions about counteractive measures. Researchers can focus their attention on challenges faced by practitioners to make software engineering more effective.},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {542–552},
numpages = {11},
keywords = {software product line, software architecture, decentralized software engineering, collaboration, case study, Software ecosystem},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1007/978-3-030-39306-9_12,
author = {Mirhosseini, Samim and Parnin, Chris},
title = {Opunit: Sanity Checks for Computing Environments},
year = {2019},
isbn = {978-3-030-39305-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-39306-9_12},
doi = {10.1007/978-3-030-39306-9_12},
abstract = {Computing environments, including virtual machines and containers, are essential components of modern software engineering infrastructure. Despite emerging tools that support the creation and configuration of computing environments, they are limited in testing and validating the construction of these environments. Furthermore, professionals and students new to these concepts, lack feedback on their construction efforts. In this paper, we argue that the design of environment testing tools should fundamentally support asserting essential properties, such as reachability and availability, in order to maximize usability and utility. We present opunit, an environment testing tool that supports assertion of these properties. We describe properties students failed to check when testing computing environments, which guided the design of opunit. Finally, we share our early experiences with using opunit in the classroom to support education and training in configuration of computing environments.},
booktitle = {Software Engineering Aspects of Continuous Development and New Paradigms of Software Production and Deployment: Second International Workshop, DEVOPS 2019, Château de Villebrumier, France, May 6–8, 2019, Revised Selected Papers},
pages = {167–180},
numpages = {14},
keywords = {DevOps training, Testing, Environment verification, Configuration management},
location = {Villebrumier, France}
}

@inproceedings{10.1145/3241403.3241454,
author = {Sinkala, Zipani Tom and Blom, Martin and Herold, Sebastian},
title = {A mapping study of software architecture recovery for software product lines},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241454},
doi = {10.1145/3241403.3241454},
abstract = {Migrating a family of software systems from ad-hoc development approaches such as `clone-and-own' towards software product lines (SPL) is a challenging task. Software architecture recovery techniques can play a crucial role in such a migration. However, it is to date still unclear how these techniques, which have been mostly developed for single system architecture recovery in mind, can be utilized in an SPL context most effectively. In this paper, we present a mapping study examining 35 research articles with the purpose of discussing the current state of the art in applying software architecture recovery techniques for SPL and identifying potential research gaps in this area. The results provide evidence that currently used approaches do not seem to consider the potential architectural degradation that might exist in the family of systems to be migrated. Moreover, it is hard to generalize across empirical studies as currently it seems difficult to compare and benchmark the approaches applied for software product line architecture (SPLA) extraction/reconstruction.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {49},
numpages = {7},
keywords = {software product lines, software architecture recovery, mapping study},
location = {Madrid, Spain},
series = {ECSA '18}
}

@inproceedings{10.1007/978-3-030-79382-1_24,
author = {Munoz, Daniel-Jesus and Gurov, Dilian and Pinto, Monica and Fuentes, Lidia},
title = {Category Theory Framework for Variability Models with Non-functional Requirements},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_24},
doi = {10.1007/978-3-030-79382-1_24},
abstract = {In Software Product Line (SPL) engineering one uses Variability Models (VMs) as input to automated reasoners to generate optimal products according to certain Quality Attributes (QAs). Variability models, however, and more specifically those including numerical features (i.e., NVMs), do not natively support QAs, and consequently, neither do automated reasoners commonly used for variability resolution. However, those satisfiability and optimisation problems have been covered and refined in other relational models such as databases.Category Theory (CT) is an abstract mathematical theory typically used to capture the common aspects of seemingly dissimilar algebraic structures. We propose a unified relational modelling framework subsuming the structured objects of VMs and QAs and their relationships into algebraic categories. This abstraction allows a combination of automated reasoners over different domains to analyse SPLs. The solutions’ optimisation can now be natively performed by a combination of automated theorem proving, hashing, balanced-trees and chasing algorithms. We validate this approach by means of the edge computing SPL tool HADAS.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {397–413},
numpages = {17},
keywords = {Category theory, Quality attribute, Non-functional requirement, Feature, Numerical variability model},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.5555/3463952.3463958,
author = {Mey, Alexander and Oliehoek, Frans A.},
title = {Environment Shift Games: Are Multiple Agents the Solution, and not the Problem, to Non-Stationarity?},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Machine learning and artificial intelligence models that interact with and in an environment will unavoidably have impact on this environment and change it. This is often a problem as many methods do not anticipate such a change in the environment and thus may start acting sub-optimally. Although efforts are made to deal with this problem, we believe that a lot of potential is unused. Driven by the recent success of predictive machine learning, we believe that in many scenarios one can predict when and how a change in the environment will occur. In this paper we introduce a blueprint that intimately connects this idea to the multiagent setting, showing that the multiagent community has a pivotal role to play in addressing the challenging problem of changing environments.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {23–27},
numpages = {5},
keywords = {non-stationarity, sequential decision making},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/2517208.2517218,
author = {Basso, Fábio Paulo and Pillat, Raquel Mainardi and Oliveira, Toacy Cavalcante and Becker, Leandro Buss},
title = {Supporting large scale model transformation reuse},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517218},
doi = {10.1145/2517208.2517218},
abstract = {The growth of applications developed with the support of model transformations makes reuse a required practice, specially when applied to transformation assets (e.g. transformation chains, algorithms, and configuration files). In order to promote reuse one must consider the different implementations, communalities, and variants among these assets. In this domain, a couple techniques have been used as solutions to adapt reusable assets for specific needs. However, so far, no work has discussed their combined use in real software projects. In this paper, we present a new tool named WCT, which can be used to adapt transformation assets. Moreover, through lessons learned in industry, we address some reuse techniques devoted to adapt these assets.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {169–178},
numpages = {10},
keywords = {transformation reuse, product line technique, model transformation chain, mde, feature model},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@article{10.1145/3449356,
author = {Balakrishnan, Aravind and Lee, Jaeyoung and Gaurav, Ashish and Czarnecki, Krzysztof and Sedwards, Sean},
title = {Transfer Reinforcement Learning for Autonomous Driving: From WiseMove to WiseSim},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/3449356},
doi = {10.1145/3449356},
abstract = {Reinforcement learning (RL) is an attractive way to implement high-level decision-making policies for autonomous driving, but learning directly from a real vehicle or a high-fidelity simulator is variously infeasible. We therefore consider the problem of transfer reinforcement learning and study how a policy learned in a simple environment using WiseMove can be transferred to our high-fidelity simulator, WiseMove. WiseMove is a framework to study safety and other aspects of RL for autonomous driving. WiseMove accurately reproduces the dynamics and software stack of our real vehicle. We find that the accurately modelled perception errors in WiseMove contribute the most to the transfer problem. These errors, when even naively modelled in WiseMove, provide an RL policy that performs better in WiseMove than a hand-crafted rule-based policy. Applying domain randomization to the environment in WiseMove yields an even better policy. The final RL policy reduces the failures due to perception errors from 10% to 2.75%. We also observe that the RL policy has significantly less reliance on velocity compared to the rule-based policy, having learned that its measurement is unreliable.},
journal = {ACM Trans. Model. Comput. Simul.},
month = jul,
articleno = {15},
numpages = {26},
keywords = {policy distillation, deep reinforcement learning, autonomous driving, Transfer reinforcement learning}
}

@article{10.1145/2000791.2000794,
author = {Anvik, John and Murphy, Gail C.},
title = {Reducing the effort of bug report triage: Recommenders for development-oriented decisions},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000791.2000794},
doi = {10.1145/2000791.2000794},
abstract = {A key collaborative hub for many software development projects is the bug report repository. Although its use can improve the software development process in a number of ways, reports added to the repository need to be triaged. A triager determines if a report is meaningful. Meaningful reports are then organized for integration into the project's development process.To assist triagers with their work, this article presents a machine learning approach to create recommenders that assist with a variety of decisions aimed at streamlining the development process. The recommenders created with this approach are accurate; for instance, recommenders for which developer to assign a report that we have created using this approach have a precision between 70% and 98% over five open source projects. As the configuration of a recommender for a particular project can require substantial effort and be time consuming, we also present an approach to assist the configuration of such recommenders that significantly lowers the cost of putting a recommender in place for a project. We show that recommenders for which developer should fix a bug can be quickly configured with this approach and that the configured recommenders are within 15% precision of hand-tuned developer recommenders.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {10},
numpages = {35},
keywords = {task assignment, recommendation, machine learning, configuration assistance, Bug report triage}
}

@inproceedings{10.1109/SEmotion52567.2021.00011,
author = {Cummaudo, Alex and Graetsch, Ulrike Maria and Curumsing, Maheswaree K and Vasa, Rajesh and Barnett, Scott and Grundy, John},
title = {Emotions in Computer Vision Service Q&amp;#x0026;A},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEmotion52567.2021.00011},
doi = {10.1109/SEmotion52567.2021.00011},
abstract = {Software developers are increasingly using cloud-based services that provide machine learning capabilities to implement &amp;#x2018;intelligent&amp;#x2019; features. Studies show that incorporating machine learning into an application increases technical debt, creates data dependencies, and introduces uncertainty due to their non-deterministic behaviour. We know very little about the emotional state of software developers who have to deal with such issues; and the impacts on productivity. This paper presents a preliminary effort to better understand the emotions of developers when experiencing issues with these services with the wider goal of discovering potential service improvements. We conducted a landscape analysis of emotions found in 1,425 Stack Overflow questions about a specific and mature subset of these cloud-based services, namely those that provide computer vision techniques. To speed up the emotion identification process, we trialled an automatic approach using a pre-trained emotion classifier that was specifically trained on Stack Overflow content, EmoTxt, and manually verified its classification results. We found that the identified emotions vary for different types of questions, and a discrepancy exists between automatic and manual emotion analysis due to subjectivity.},
booktitle = {2021 IEEE/ACM Sixth International Workshop on Emotion Awareness in Software Engineering (SEmotion)},
pages = {13–18},
numpages = {6},
location = {Madrid, Spain}
}

@article{10.1007/s10664-014-9336-6,
author = {Sobernig, Stefan and Apel, Sven and Kolesnikov, Sergiy and Siegmund, Norbert},
title = {Quantifying structural attributes of system decompositions in 28 feature-oriented software product lines},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9336-6},
doi = {10.1007/s10664-014-9336-6},
abstract = {A key idea of feature orientation is to decompose a software product line along the features it provides. Feature decomposition is orthogonal to object-oriented decomposition--it crosscuts the underlying package and class structure. It has been argued often that feature decomposition improves system structure by reducing coupling and by increasing cohesion. However, recent empirical findings suggest that this is not necessarily the case. In this exploratory, observational study, we investigate the decompositions of 28 feature-oriented software product lines into classes, features, and feature-specific class fragments. The product lines under investigation are implemented using the feature-oriented programming language Fuji. In particular, we quantify and compare the internal attributes import coupling and cohesion of the different product-line decompositions in a systematic, reproducible manner. For this purpose, we adopt three established software measures (e.g., coupling between units, CBU; internal-ratio unit dependency, IUD) as well as standard concentration statistics (e.g., Gini coefficient). In our study, we found that feature decomposition can be associated with higher levels of structural coupling in a product line than a decomposition into classes. Although coupling can be concentrated in very few features in most feature decompositions, there are not necessarily hot-spot features  in all product lines. Interestingly, feature cohesion is not necessarily higher than class cohesion, whereas features are more equal in serving dependencies internally than classes of a product line. Our empirical study raises critical questions about alleged advantages of feature decomposition. At the same time, we demonstrate how our measurement approach of coupling and cohesion has potential to support static and dynamic analyses of software product lines (i.e., type checking and feature-interaction detection) by facilitating product sampling.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1670–1705},
numpages = {36},
keywords = {Structural coupling, Structural cohesion, Software product lines, Software measurement, Fuji, Feature-oriented programming}
}

@article{10.1016/j.cie.2021.107782,
author = {Chien, Chen-Fu and Lan, Yu-Bin},
title = {Agent-based approach integrating deep reinforcement learning and hybrid genetic algorithm for dynamic scheduling for Industry 3.5 smart production},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {162},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107782},
doi = {10.1016/j.cie.2021.107782},
journal = {Comput. Ind. Eng.},
month = dec,
numpages = {11},
keywords = {Industry 3.5, Semiconductor manufacturing, Hybrid genetic algorithm, Dynamic scheduling, Deep reinforcement learning}
}

@article{10.1007/s10844-018-0500-0,
author = {Krátky, Peter and Chudá, Daniela},
title = {Recognition of web users with the aid of biometric user model},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {3},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-018-0500-0},
doi = {10.1007/s10844-018-0500-0},
abstract = {Methods aimed at recognition of users are able to identify browsers or machines, but cannot distinguish physical persons. Multiple identities of single users are common on the Internet and this phenomenon decreases trustfulness of presented content as well as quality of provided services. This paper proposes a novel method for recognition of persons on the Web using input device usage patterns (keyboard, computer mouse, touchscreen), behavioral biometrics. The essential part of this method is a biometric component attached to the user model of an information system serving as a biometric identifier. The recognition of users relies in matching these components, specifically comparing values distribution shapes, which are characterizing users. The paper presents results of the method performance, which were obtained in a series of experiments focused on different aspects of evaluation (recognition rate, scalability, etc.). More specifically, a conducted case study shows application of the method to solve an issue in website visits analysis caused by erasing cookies.},
journal = {J. Intell. Inf. Syst.},
month = dec,
pages = {621–646},
numpages = {26},
keywords = {Users recognition, User identifier, Session identification, Mouse dynamics, Mouse biometrics, Identity deduplication, Identification on the Web, Behavioral biometrics}
}

@inproceedings{10.1145/3377812.3382153,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {FeatureNET: diversity-driven generation of deep learning models},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382153},
doi = {10.1145/3377812.3382153},
abstract = {We present FeatureNET, an open-source Neural Architecture Search (NAS) tool1 that generates diverse sets of Deep Learning (DL) models. FeatureNET relies on a meta-model of deep neural networks, consisting of generic configurable entities. Then, it uses tools developed in the context of software product lines to generate diverse (maximize the differences between the generated) DL models. The models are translated to Keras and can be integrated into typical machine learning pipelines. FeatureNET allows researchers to generate seamlessly a large variety of models. Thereby, it helps choosing appropriate DL models and performing experiments with diverse models (mitigating potential threats to validity). As a NAS method, FeatureNET successfully generates models performing equally well with handcrafted models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {41–44},
numpages = {4},
keywords = {neural architecture search, configuration search, NAS, AutoML},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1016/j.jss.2013.06.034,
author = {Alférez, G. H. and Pelechano, V. and Mazo, R. and Salinesi, C. and Diaz, D.},
title = {Dynamic adaptation of service compositions with variability models},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.034},
doi = {10.1016/j.jss.2013.06.034},
abstract = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
journal = {J. Syst. Softw.},
month = may,
pages = {24–47},
numpages = {24},
keywords = {Web service composition, Verification, Variability, Models at runtime, Dynamic software product line, Dynamic adaptation, Constraint programming, Autonomic computing}
}

@article{10.1016/j.infsof.2013.05.002,
author = {Rodriguez, Daniel and Ruiz, Roberto and Riquelme, Jose C. and Harrison, Rachel},
title = {A study of subgroup discovery approaches for defect prediction},
year = {2013},
issue_date = {October, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.002},
doi = {10.1016/j.infsof.2013.05.002},
abstract = {Context: Although many papers have been published on software defect prediction techniques, machine learning approaches have yet to be fully explored. Objective: In this paper we suggest using a descriptive approach for defect prediction rather than the precise classification techniques that are usually adopted. This allows us to characterise defective modules with simple rules that can easily be applied by practitioners and deliver a practical (or engineering) approach rather than a highly accurate result. Method: We describe two well-known subgroup discovery algorithms, the SD algorithm and the CN2-SD algorithm to obtain rules that identify defect prone modules. The empirical work is performed with publicly available datasets from the Promise repository and object-oriented metrics from an Eclipse repository related to defect prediction. Subgroup discovery algorithms mitigate against characteristics of datasets that hinder the applicability of classification algorithms and so remove the need for preprocessing techniques. Results: The results show that the generated rules can be used to guide testing effort in order to improve the quality of software development projects. Such rules can indicate metrics, their threshold values and relationships between metrics of defective modules. Conclusions: The induced rules are simple to use and easy to understand as they provide a description rather than a complete classification of the whole dataset. Thus this paper represents an engineering approach to defect prediction, i.e., an approach which is useful in practice, easily understandable and can be applied by practitioners.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1810–1822},
numpages = {13},
keywords = {Subgroup discovery, Rules, Imbalanced datasets, Defect prediction}
}

@article{10.1016/j.asoc.2016.05.015,
author = {Dou, Dongyang and Zhou, Shishuai},
title = {Comparison of four direct classification methods for intelligent fault diagnosis of rotating machinery},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.015},
doi = {10.1016/j.asoc.2016.05.015},
abstract = {Display Omitted A rule-based method was proposed based on MLEM2 and enhanced by a new rule reasoning mechanism.Eight time-domain and five dimensionless frequency-domain parameters were adopted.The proposed method had the ability of feature reduction.The proposed method was an all-rounder compared with KNN, PNN and PSO-SVM as it was very friendly. Condition monitoring of rotating machinery is important to promptly detect early faults, identify potential problems, and prevent complete failure. Four direct classification methods were introduced to diagnose the regular condition, inner race defect, outer race defect, and rolling element defect of rolling bearings. These include the K-Nearest Neighbor algorithm (KNN), Probabilistic Neural Network (PNN), Particle Swarm Optimization optimized Support Vector Machine (PSO-SVM) and a Rule-Based Method (RBM) based on the MLEM2 algorithm and a new Rule Reasoning Mechanism (RRM). All of them can be run on the Fault Decision Table (FDT) containing numerical variables and output fault categories directly. The diagnosis results were discussed in terms of accuracy, time consumption, intelligibility, and maintainability. Especially, the interactions of the systems and human experts were compared in detail. It was concluded that all the four methods can work satisfactorily on accuracy, in an order of the PSO-SVM ranking the first, followed by the RBM that functioned the friendliest. Moreover, the RBM had the ability of feature reduction by itself, and would be most suitable for real-time applications.},
journal = {Appl. Soft Comput.},
month = sep,
pages = {459–468},
numpages = {10},
keywords = {SVM, Rule, Rotating machinery, PNN, Fault diagnosis}
}

@article{10.1016/j.neucom.2019.11.001,
author = {Li, Huafeng and Zhou, Weiyan and Yu, Zhengtao and Yang, Biao and Jin, Huaiping},
title = {Person re-identification with dictionary learning regularized by stretching regularization and label consistency constraint},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {379},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.001},
doi = {10.1016/j.neucom.2019.11.001},
journal = {Neurocomput.},
month = feb,
pages = {356–369},
numpages = {14},
keywords = {Stretch regularization, Label consistency constraint, Dictionary learning, Person re-identification}
}

@inproceedings{10.1145/3324884.3416573,
author = {Mühlbauer, Stefan and Apel, Sven and Siegmund, Norbert},
title = {Identifying software performance changes across variants and versions},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416573},
doi = {10.1145/3324884.3416573},
abstract = {We address the problem of identifying performance changes in the evolution of configurable software systems. Finding optimal configurations and configuration options that influence performance is already difficult, but in the light of software evolution, configuration-dependent performance changes may lurk in a potentially large number of different versions of the system.In this work, we combine two perspectives---variability and time---into a novel perspective. We propose an approach to identify configuration-dependent performance changes retrospectively across the software variants and versions of a software system. In a nutshell, we iteratively sample pairs of configurations and versions and measure the respective performance, which we use to update a model of likelihoods for performance changes. Pursuing a search strategy with the goal of measuring selectively and incrementally further pairs, we increase the accuracy of identified change points related to configuration options and interactions.We have conducted a number of experiments both on controlled synthetic data sets as well as in real-world scenarios with different software systems. Our evaluation demonstrates that we can pinpoint performance shifts to individual configuration options and interactions as well as commits introducing change points with high accuracy and at scale. Experiments on three real-world systems explore the effectiveness and practicality of our approach.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {611–622},
numpages = {12},
keywords = {active learning, configurable software systems, machine learning, software evolution, software performance},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenmüller, Marko and Kuhlemann, Martin and Kästner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Software product lines, SPL Conqueror, Non-functional properties, Measurement and optimization, Feature-oriented software development}
}

@inproceedings{10.1145/2491411.2491440,
author = {Blincoe, Kelly and Valetto, Giuseppe and Damian, Daniela},
title = {Do all task dependencies require coordination? the role of task properties in identifying critical coordination needs in software projects},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491440},
doi = {10.1145/2491411.2491440},
abstract = {Several methods exist to detect the coordination needs within software teams. Evidence exists that developers’ awareness about coordination needs improves work performance. Distinguishing with certainty between critical and trivial coordination needs and identifying and prioritizing which specific tasks a pair of developers should coordinate about remains an open problem. We investigate what work dependencies should be considered when establishing coordination needs within a development team. We use our conceptualization of work dependencies named Proximity and leverage machine learning techniques to analyze what additional task properties are indicative of coordination needs. In a case study of the Mylyn project, we were able to identify from all potential coordination requirements a subset of 17% that are most critical. We define critical coordination requirements as those that can cause the most disruption to task duration when left unmanaged. These results imply that coordination awareness tools could be enhanced to make developers aware of only the coordination needs that can bring about the highest performance benefit.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {213–223},
numpages = {11},
keywords = {Task Dependencies, Proximity, Machine Learning, Coordination Requirements, Collaborative Software Development, Awareness},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1016/j.cie.2020.106336,
author = {Dou, Runliang and Huang, Rui and Nan, Guofang and Liu, Jing},
title = {Less diversity but higher satisfaction: An intelligent product configuration method for type-decreased mass customization},
year = {2020},
issue_date = {Apr 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {142},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2020.106336},
doi = {10.1016/j.cie.2020.106336},
journal = {Comput. Ind. Eng.},
month = apr,
numpages = {13},
keywords = {Social inertia, Kano’s model, Fuzzy clustering, Computational intelligence, Mass customization}
}

@article{10.5555/1577069.1577086,
author = {Li, Junning and Wang, Z. Jane},
title = {Controlling the False Discovery Rate of the Association/Causality Structure Learned with the PC Algorithm},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {In real world applications, graphical statistical models are not only a tool for operations such as classification or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level, and a heuristic modification of the method is able to control the FDR more accurately around the user-specified level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models.},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {475–514},
numpages = {40}
}

@article{10.1016/j.ins.2011.01.039,
author = {Rodríguez, D. and Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S.},
title = {Searching for rules to detect defective modules: A subgroup discovery approach},
year = {2012},
issue_date = {May, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {191},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2011.01.039},
doi = {10.1016/j.ins.2011.01.039},
abstract = {Data mining methods in software engineering are becoming increasingly important as they can support several aspects of the software development life-cycle such as quality. In this work, we present a data mining approach to induce rules extracted from static software metrics characterising fault-prone modules. Due to the special characteristics of the defect prediction data (imbalanced, inconsistency, redundancy) not all classification algorithms are capable of dealing with this task conveniently. To deal with these problems, Subgroup Discovery (SD) algorithms can be used to find groups of statistically different data given a property of interest. We propose EDER-SD (Evolutionary Decision Rules for Subgroup Discovery), a SD algorithm based on evolutionary computation that induces rules describing only fault-prone modules. The rules are a well-known model representation that can be easily understood and applied by project managers and quality engineers. Thus, rules can help them to develop software systems that can be justifiably trusted. Contrary to other approaches in SD, our algorithm has the advantage of working with continuous variables as the conditions of the rules are defined using intervals. We describe the rules obtained by applying our algorithm to seven publicly available datasets from the PROMISE repository showing that they are capable of characterising subgroups of fault-prone modules. We also compare our results with three other well known SD algorithms and the EDER-SD algorithm performs well in most cases.},
journal = {Inf. Sci.},
month = may,
pages = {14–30},
numpages = {17},
keywords = {Subgroup discovery, Rules, Imbalanced datasets, Defect prediction}
}

@inproceedings{10.1109/ICSE-NIER.2019.00028,
author = {Trubiani, Catia and Apel, Sven},
title = {PLUS: performance learning for uncertainty of software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00028},
doi = {10.1109/ICSE-NIER.2019.00028},
abstract = {Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software performance engineering domain: How to model the variable configuration options (i.e., software and hardware resources) and their intrinsic uncertainties (e.g., resource demand, processor speed) to represent the performance characteristics of software systems? This way, software engineers are exposed to a quantitative evaluation of their systems that supports them in the task of identifying performance critical configurations along with their uncertainties.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {77–80},
numpages = {4},
keywords = {uncertainty, machine learning},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@article{10.1007/s10845-011-0521-9,
author = {Huang, Jing and Süer, Gürsel A. and Urs, Shravan B.},
title = {Genetic algorithm for rotary machine scheduling with dependent processing times},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {5},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-011-0521-9},
doi = {10.1007/s10845-011-0521-9},
abstract = {This paper focuses on scheduling a rotary injection molding machine with dependent processing times. The injection machine has n pairs of positions to process n pairs of shoes. It is rotated after every cycle time. Cycle time is the maximum injection time of the jobs currently loaded in the machine. Thus, for all practical purposes, the processing time of a job depends on the combination of the jobs currently assigned to the machine. The uncertainty of processing time makes this problem more complicated than traditional parallel machine scheduling problems. Additionally, since switching jobs leads to mold changes, set-up time is also included in the analysis. We develop a Sequential Genetic Algorithm (SGA) to identify the best schedule with regard to makespan. In this approach, multiple GA evolvers are connected by using a feeding strategy, where each GA evolver identifies the best schedule with minimum makespan for the corresponding product family. A multi-segment (product lines) chromosome representation is applied to represent the product line sequence as well as the job sequence within a product family. Furthermore, an adaptive feeding strategy is also proposed to improve results and reduce computation times. Besides SGA, we also improve the performance of a traditional heuristic procedure by proposing a minimum ΔIT heuristic approach. The experimentation is performed by using four experimental data sets with different demand patterns and nine data sets from a shoe manufacturing plant. The results indicate that our SGA provides better schedule with respect to makespan value, while heuristic procedures take insignificant time to obtain results. Another observation is that adaptive feeding strategy helps to find good results in a shorter time.},
journal = {J. Intell. Manuf.},
month = oct,
pages = {1931–1948},
numpages = {18},
keywords = {Rotary machine scheduling, Heuristic procedure, Genetic algorithm, Adaptive feeding}
}

@article{10.1145/3322122,
author = {Gong, Chen and Yang, Jian and Tao, Dacheng},
title = {Multi-Modal Curriculum Learning over Graphs},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3322122},
doi = {10.1145/3322122},
abstract = {Curriculum Learning (CL) is a recently proposed learning paradigm that aims to achieve satisfactory performance by properly organizing the learning sequence from simple curriculum examples to more difficult ones. Up to now, few works have been done to explore CL for the data with graph structure. Therefore, this article proposes a novel CL algorithm that can be utilized to guide the Label Propagation (LP) over graphs, of which the target is to “learn” the labels of unlabeled examples on the graphs. Specifically, we assume that different unlabeled examples have different levels of difficulty for propagation, and their label learning should follow a simple-to-difficult sequence with the updated curricula. Furthermore, considering that the practical data are often characterized by multiple modalities, every modality in our method is associated with a “teacher” that not only evaluates the difficulties of examples from its own viewpoint, but also cooperates with other teachers to generate the overall simplest curriculum examples for propagation. By taking the curriculums suggested by the teachers as a whole, the common preference (i.e., commonality) of teachers on selecting the simplest examples can be discovered by a row-sparse matrix, and their distinct opinions (i.e., individuality) are captured by a sparse noise matrix. As a result, an accurate curriculum sequence can be established and the propagation quality can thus be improved. Theoretically, we prove that the propagation risk bound is closely related to the examples’ difficulty information, and empirically, we show that our method can generate higher accuracy than the state-of-the-art CL approach and LP algorithms on various multi-modal tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {35},
numpages = {25},
keywords = {semi-supervised learning, multi-modal learning, label propagation, Curriculum learning}
}

@inproceedings{10.1145/375212.375277,
author = {Savolainen, Juha and Kuusela, Juha},
title = {Violatility analysis framework for product lines},
year = {2001},
isbn = {1581133588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375212.375277},
doi = {10.1145/375212.375277},
abstract = {Evolution of a software intensive system is unavoidable. In fact, evolution can be seen as a part of reuse process. During the evolution of the software asset, the major part of the system functionality is normally reused. So the key issue is to identify the volatile parts of the domain requirements. Additionally, there is promise that tailored tool support may help supporting evolution in software intensive systems. In this paper, we describe the volatility analysis method for product lines. This highly practical method has been used in multiple domains and is able to express and estimate common types of evolutional characteristics. The method is able to represent volatility in multiple levels and has capacity to tie the volatility estimation to one product line member specification. We  also briefly describe current tool support for the method. The main contribution of this paper is a volatility analysis framework that can be used to describe how requirements are estimated to evolve in the future. The method is based on the definition hierarchy framework.},
booktitle = {Proceedings of the 2001 Symposium on Software Reusability: Putting Software Reuse in Context},
pages = {133–141},
numpages = {9},
keywords = {volatility analysis, variability, requirements engineering, product line, evolution, domain analysis, commonality},
location = {Toronto, Ontario, Canada},
series = {SSR '01}
}

@article{10.1016/j.jpdc.2019.04.016,
author = {Li, Yinhao and Alqahtani, Awatif and Solaiman, Ellis and Perera, Charith and Jayaraman, Prem Prakash and Buyya, Rajkumar and Morgan, Graham and Ranjan, Rajiv},
title = {IoT-CANE: A unified knowledge management system for data-centric Internet of Things application systems},
year = {2019},
issue_date = {Sep 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.04.016},
doi = {10.1016/j.jpdc.2019.04.016},
journal = {J. Parallel Distrib. Comput.},
month = sep,
pages = {161–172},
numpages = {12},
keywords = {Configuration management, Ripple Down Rules, Recommender system, Knowledge representation, Internet of Things}
}

@article{10.1007/s00766-014-0216-9,
author = {Karataş, Ahmet Serkan and Oğuztüzün, Halit},
title = {Attribute-based variability in feature models},
year = {2016},
issue_date = {June      2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {2},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0216-9},
doi = {10.1007/s00766-014-0216-9},
abstract = {Extended feature models enable the expression of complex cross-tree constraints involving feature attributes. The inclusion of attributes in cross-tree relations not only enriches the constraints, but also engenders an extended type of variability that involves attributes. In this article, we elaborate on the effects of this new variability type on feature models. We start by analyzing the nature of the variability involving attributes and extend the definitions of the configuration and the product to suit the emerging requirements. Next, we propose classifications for the features, configurations, and products to identify and formalize the ramifications that arise due to the new type of variability. Then, we provide a semantic foundation grounded on constraint satisfaction for our proposal. We introduce an ordering relation between configurations and show that the set of all the configurations represented by a feature model forms a semilattice. This is followed by a demonstration of how the feature model analyses will be affected using illustrative examples selected from existing and novel analysis operations. Finally, we summarize our experiences, gained from a commercial research and development project that employs an extended feature model.},
journal = {Requir. Eng.},
month = jun,
pages = {185–208},
numpages = {24},
keywords = {Variability management, Variability involving attributes, Software product lines, Extended feature models}
}

@article{10.1007/s10515-010-0066-8,
author = {Apel, Sven and Kästner, Christian and Gröβlinger, Armin and Lengauer, Christian},
title = {Type safety for feature-oriented product lines},
year = {2010},
issue_date = {September 2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0066-8},
doi = {10.1007/s10515-010-0066-8},
abstract = {A feature-oriented product line is a family of programs that share a common set of features. A feature implements a stakeholder's requirement and represents a design decision or configuration option. When added to a program, a feature involves the introduction of new structures, such as classes and methods, and the refinement of existing ones, such as extending methods. A feature-oriented decomposition enables a generator to create an executable program by composing feature code solely on the basis of the feature selection of a user--no other information needed. A key challenge of product line engineering is to guarantee that only well-typed programs are generated. As the number of valid feature combinations grows combinatorially with the number of features, it is not feasible to type check all programs individually. The only feasible approach is to have a type system check the entire code base of the feature-oriented product line. We have developed such a type system on the basis of a formal model of a feature-oriented Java-like language. The type system guaranties type safety for feature-oriented product lines. That is, it ensures that every valid program of a well-typed product line is well-typed. Our formal model including type system is sound and complete.},
journal = {Automated Software Engg.},
month = sep,
pages = {251–300},
numpages = {50},
keywords = {Type systems, Software product lines, Safe composition, Feature-oriented programming, Feature featherweight Java}
}

@article{10.1177/1046878111434474,
author = {Cannon, James N. and Cannon, Hugh M. and Schwaiger, Manfred},
title = {Modeling the “Profitable-Product Death Spiral”: Accounting for Strategic Product-Mix Interactions in Marketing Simulation Games},
year = {2012},
issue_date = {December  2012},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {43},
number = {6},
issn = {1046-8781},
url = {https://doi.org/10.1177/1046878111434474},
doi = {10.1177/1046878111434474},
abstract = {Business simulation game designers typically ignore product line interactions in the design of marketing simulation games. This article addresses the failing by modeling Rust, Zeithaml, and Lemon's concept of the profitable-product death spiral, a product-mix interaction theory based on the concept of customer lifetime value (CLV). According to their theory, marketers often enter a cycle of decreasing demand by deleting less profitable products. When customers seek multiple products from the same company, the deletion of less profitable ones will often reduce demand for more profitable products as well, rendering them less profitable. Unchecked, the cycle continues until the company fails. This article discusses how to model the â death-spiralâ effect by adapting Teach's gravity-flow model to evaluate the product mix as a kind of â meta-product,â where desired products function as product attributes.},
journal = {Simul. Gaming},
month = dec,
pages = {761–777},
numpages = {17},
keywords = {strategic product-mix decisions, profitable-product death spiral, product-mix strategy, product line interactions, meta-product, marketing simulations, gravity-flow model, death spiral, customer lifetime value, customer equity, business simulation game}
}

@article{10.1007/s10772-017-9429-x,
author = {Phu, Vo Ngoc and Tran, Vo Thi and Chau, Vo Thi and Dat, Nguyen Duy and Duy, Khanh Ly},
title = {A decision tree using ID3 algorithm for English semantic analysis},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-017-9429-x},
doi = {10.1007/s10772-017-9429-x},
abstract = {Natural language processing has been studied for many years, and it has been applied to many researches and commercial applications. A new model is proposed in this paper, and is used in the English document-level emotional classification. In this survey, we proposed a new model by using an ID3 algorithm of a decision tree to classify semantics (positive, negative, and neutral) for the English documents. The semantic classification of our model is based on many rules which are generated by applying the ID3 algorithm to 115,000 English sentences of our English training data set. We test our new model on the English testing data set including 25,000 English documents, and achieve 63.6% accuracy of sentiment classification results.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {593–613},
numpages = {21},
keywords = {id3, Sentiment classification, ID3 algorithm, English sentiment classification, English document opinion mining, Decision tree}
}

@article{10.3233/JIFS-210246,
author = {Alagarsamy, Ramachandran and Arunpraksh, R. and Ganapathy, Sannasi and Rajagopal, Aghila and Kavitha, R.J.},
title = {A fuzzy content recommendation system using similarity analysis, content ranking and clustering},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-210246},
doi = {10.3233/JIFS-210246},
abstract = {Recently, the e-learners are drastically increased from the last two decades. Everything is learnt through internet without help of the tutor as well. For this purpose, the e-learners are required more e-learning applications that are able to supply optimal and satisfied data based on their capability. No content recommendation system is available for recommending suitable contents to the learners. For this purpose, this paper proposes a new semantic and fuzzy aware content recommendation system for retrieving the suitable content for the users. In this content recommendation system, we propose two content pre-processing algorithms namely Target Keyword based Data Pre-processing Algorithm (TKDPA) and Intelligent Anova-T Residual Algorithm (IAATRA) for selecting the more relevant features from the document. Moreover, a new Fuzzy rule based Similarity Matching algorithm (FRSMA) is proposed and used in this system for finding the similarity between the two terms and also rank them by using the newly proposed Similarity and Temporal aware Weighted Document Ranking Algorithm (STWDRA). In addition, a content clustering process is also incorporated for gathering relevant content. Finally, a new Fuzzy, Target Keyword and Similarity Score based Content Recommendation Algorithm (FTKSCRA) is also proposed for recommending the more relevant content to the learners accurately. The experiments have been conducted for evaluating the proposed content recommendation system and proved as better than the existing recommendation systems in terms of precision, recall, f-measure and prediction accuracy.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6429–6441},
numpages = {13},
keywords = {fuzzy rules and annova-T, semantic analysis, content recommendation, clustering, content ranking, Fuzzy logic}
}

@article{10.1016/j.eswa.2010.10.032,
author = {Tabassian, Mahdi and Ghaderi, Reza and Ebrahimpour, Reza},
title = {Knitted fabric defect classification for uncertain labels based on Dempster-Shafer theory of evidence},
year = {2011},
issue_date = {May, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {5},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.10.032},
doi = {10.1016/j.eswa.2010.10.032},
abstract = {Research highlights Uncertainty in labels of learning data is considered. Different types of features provide complementary information from data. The uncertainties in the initial labels of the learning data in each feature space are detected using local information of the data. Dempster-Shafer theory is used in a neural network ensemble structure to reduce the accepted uncertainty. A new approach for classification of circular knitted fabric defect is proposed which is based on accepting uncertainty in labels of the learning data. In the basic classification methodologies it is assumed that correct labels are assigned to samples and these approaches concentrate on the strength of categorization. However, there are some classification problems in which a considerable amount of uncertainty exists in the labels of samples. The core of innovation in this research has been usage of the uncertain information of labeling and their combination with the Dempster-Shafer theory of evidence. The experimental results show the robustness of the proposed method in comparison with usual classification techniques of supervised learning where the certain labels are assigned to training data.},
journal = {Expert Syst. Appl.},
month = may,
pages = {5259–5267},
numpages = {9},
keywords = {Wavelet transform, Uncertainty in labels, Theory of evidence, MLP neural network, K-nearest neighbors, Circular knitted fabric defect}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature model extraction from large collections of informal product descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Product Lines, Feature Models, Domain Analysis},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1016/j.engappai.2019.103394,
author = {Izakian, Zahedeh and Mesgari, M. Saadi and Weibel, Robert},
title = {A feature extraction based trajectory segmentation approach based on multiple movement parameters},
year = {2020},
issue_date = {Feb 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2019.103394},
doi = {10.1016/j.engappai.2019.103394},
journal = {Eng. Appl. Artif. Intell.},
month = feb,
numpages = {16},
keywords = {Sliding window, Movement parameter, Trajectory clustering, Trajectory segmentation}
}

@article{10.1007/s00158-013-0891-z,
author = {Woodruff, Matthew J. and Reed, Patrick M. and Simpson, Timothy W.},
title = {Many objective visual analytics: rethinking the design of complex engineered systems},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {48},
number = {1},
issn = {1615-147X},
url = {https://doi.org/10.1007/s00158-013-0891-z},
doi = {10.1007/s00158-013-0891-z},
abstract = {Many cognitive and computational challenges accompany the design of complex engineered systems. This study proposes the many-objective visual analytics (MOVA) framework as a new approach to the design of complex engineered systems. MOVA emphasizes learning through problem reformulation, enabled by visual analytics and many-objective search. This study demonstrates insights gained by evolving the formulation of a General Aviation Aircraft (GAA) product family design problem. This problem's considerable complexity and difficulty, along with a history encompassing several formulations, make it well-suited to demonstrate the MOVA framework. The MOVA framework results compare a single objective, a two objective, and a ten objective formulation for optimizing the GAA product family. Highly interactive visual analytics are exploited to demonstrate how decision biases can arise for lower dimensional, highly aggregated problem formulations.},
journal = {Struct. Multidiscip. Optim.},
month = jul,
pages = {201–219},
numpages = {19},
keywords = {Product family design, Multidimensional data visualization, Multi-objective optimization}
}

@inproceedings{10.1145/3023956.3023963,
author = {Halin, Axel and Nuttinck, Alexandre and Acher, Mathieu and Devroey, Xavier and Perrouin, Gilles and Heymans, Patrick},
title = {Yo variability! JHipster: a playground for web-apps analyses},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023963},
doi = {10.1145/3023956.3023963},
abstract = {Though variability is everywhere, there has always been a shortage of publicly available cases for assessing variability-aware tools and techniques as well as supports for teaching variability-related concepts. Historical software product lines contains industrial secrets their owners do not want to disclose to a wide audience. The open source community contributed to large-scale cases such as Eclipse, Linux kernels, or web-based plugin systems (Drupal, WordPress). To assess accuracy of sampling and prediction approaches (bugs, performance), a case where all products can be enumerated is desirable. As configuration issues do not lie within only one place but are scattered across technologies and assets, a case exposing such diversity is an additional asset. To this end, we present in this paper our efforts in building an explicit product line on top of JHipster, an industrial open-source Web-app configurator that is both manageable in terms of configurations (≈ 163,000) and diverse in terms of technologies used. We present our efforts in building a variability-aware chain on top of JHipster's configurator and lessons learned using it as a teaching case at the University of Rennes. We also sketch the diversity of analyses that can be performed with our infrastructure as well as early issues found using it. Our long term goal is both to support students and researchers studying variability analysis and JHipster developers in the maintenance and evolution of their tools.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {44–51},
numpages = {8},
keywords = {web-apps, variability-related analyses, case study},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/3387940.3392206,
author = {Capiluppi, Andrea and Ajienka, Nemitari},
title = {Towards A Dependency-Driven Taxonomy of Software Types},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392206},
doi = {10.1145/3387940.3392206},
abstract = {Context: The evidence on software health and ecosystems could be improved if there was a systematic way to identify the types of software for which empirical evidence applies. Results and guidelines on software health are unlikely to be globally applicable: the context and the domain where the evidence has been tested are more likely to influence the results on software maintenance and health.Objective: The objectives of this paper are (i) to discuss the implications of adopting a specific taxonomy of software types, and (ii) to define, where possible, dependencies or similarities between parts of the taxonomy.Method: We discuss bottom-up and top-down taxonomies, and we show how different taxonomies fare against each other. We also propose two case studies, based on software projects divided in categories and sub-categories.Results: We show that one taxonomy does not consistently represent another taxonomy's categories. We also show that it is possible to establish directional dependencies (e.g., 'larger than') between attributes of different categories, and sub-categories.Conclusion: This paper establishes the need of directional-driven dependencies between categories of software types, that have an immediate effect on their maintenance and their relative software health.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {687–694},
numpages = {8},
keywords = {OO (object-oriented), Machine Learning, Latent Dirichlet Allocation, FOSS, Expert Opinions, Application Domains},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.4018/ijksr.2013070109,
author = {Penciuc, Diana and Abel, Marie-Hélène and Van Den Abeele, Didier},
title = {Support for Collaborative Building of a Railway Technical Solution during Tendering},
year = {2013},
issue_date = {July 2013},
publisher = {IGI Global},
address = {USA},
volume = {4},
number = {3},
issn = {1947-8429},
url = {https://doi.org/10.4018/ijksr.2013070109},
doi = {10.4018/ijksr.2013070109},
abstract = {As systems become more and more complex, more complex processes, organization and division of work are needed to achieve their conception and realization. The growing difficulty consists in the number and distribution of collaborators in disparate regions on the globe, the multifaceted communities that need to be coordinated in order to assure integration and coherence of their work. It is also the case of building railway technical solutions. The heterogeneity of customer market adds a supplementary challenge: adapt the solution to the customer background, context and real needs. In this context the authors propose a workspace to support collaboration when building customer technical solutions. The authors think that adequate collaboration support needs to be provided for each community and that a common backbone is needed between these communities to assure integration and coherence of their work. This paper gives a model and implementation of a dedicated workspace that can handle collaboration during complex processes like the construction of a railway technical solution.},
journal = {Int. J. Knowl. Soc. Res.},
month = jul,
pages = {103–113},
numpages = {11},
keywords = {Railway System Engineering, Product-Line Development, Knowledge Management, Communities of Knowledge, Collaborative Engineering}
}

@inproceedings{10.1145/3377024.3377047,
author = {Mahmood, Wardah and Chagama, Moses and Berger, Thorsten and Hebig, Regina},
title = {Causes of merge conflicts: a case study of ElasticSearch},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377047},
doi = {10.1145/3377024.3377047},
abstract = {Software branching and merging allows collaborative development and creating software variants, commonly referred to as clone &amp; own. While simple and cheap, a trade-off is the need to merge code and to resolve merge conflicts, which frequently occur in practice. When resolving conflicts, a key challenge for developer is to understand the changes that led to the conflict. While merge conflicts and their characteristics are reasonably well understood, that is not the case for the actual changes that cause them.We present a case study of the changes---on the code and on the project-level (e.g., feature addition, refactoring, feature improvement)---that lead to conflicts. We analyzed the development history of ElasticSearch, a large open-source project that heavily relies on branching (forking) and merging. We inspected 40 merge conflicts in detail, sampled from 534 conflicts not resolvable by a semi-structured merge tool. On a code (structural) level, we classified the semantics of changes made. On a project-level, we categorized the decisions that motivated these changes. We contribute a categorization of code- and project-level changes and a detailed dataset of 40 conflict resolutions with a description of both levels of changes. Similar to prior studies, most of our conflicts are also small; while our categorization of code-level changes surprisingly differs from that of prior work. Refactoring, feature additions and feature enhancements are the most common causes of merge conflicts, most of which could potentially be avoided with better development tooling.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {9},
numpages = {9},
keywords = {software merging, conflict resolution, case study},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1007/978-3-642-34321-6_35,
author = {Eichelberger, Holger and Kröher, Christian and Schmid, Klaus},
title = {Variability in service-oriented systems: an analysis of existing approaches},
year = {2012},
isbn = {9783642343209},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34321-6_35},
doi = {10.1007/978-3-642-34321-6_35},
abstract = {In service-oriented systems services can be easily reused and shared without modification. However, there are business situations where a variation of services is needed to meet the requirements of a specific customer or context. Variation of software systems has been well researched in product line engineering in terms of Variability Implementation Techniques (VITs). While most VITs focus on the customization of traditional software systems, several VITs have been developed for service-oriented systems. In this paper, we discuss the problem of service customization and provide an overview of different VITs for service variability. For this purpose, we will define four dimensions to describe, characterize and analyze existing VITs: the technical core idea, the object of variation, the forms of variation, and the binding time.},
booktitle = {Proceedings of the 10th International Conference on Service-Oriented Computing},
pages = {516–524},
numpages = {9},
location = {Shanghai, China},
series = {ICSOC'12}
}

@article{10.1016/j.eswa.2020.113808,
author = {Mohsin, Hufsa and Shi, Chongyang},
title = {SPBC: A self-paced learning model for bug classification from historical repositories of open-source software},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113808},
doi = {10.1016/j.eswa.2020.113808},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {15},
keywords = {Bug classification, Bug report analysis, Self-paced learning, Defect localization, Bug triaging}
}

@inproceedings{10.1145/3459637.3482458,
author = {Keramati, Mahsa and Zohrevand, Zahra and Glässer, Uwe},
title = {Norma: A Hybrid Feature Alignment for Class-Aware Unsupervised Domain Adaptation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482458},
doi = {10.1145/3459637.3482458},
abstract = {Unsupervised domain adaptation is the problem of transferring extracted knowledge from a labeled source domain to an unlabeled target domain. To achieve discriminative domain adaptation recent studies take advantage of target sample pseudo-labels to impose class-aware distribution alignment across the source and target domains. Still, they have some shortcomings such as making decisions based on inaccurate pseudo-labeled samples that mislead the adaptation process. In this paper, we propose a progressive deep feature alignment, called Norma, to tackle class-aware unsupervised domain adaptation for image classification by enforcing inter-class compactness and intra-class discrepancy through a hybrid learning process. To this end, Norma's optimization process is defined based on a novel triplet loss which not only addresses soft prototype alignment but also pushes away multiple negative centroids. Also, to extract maximum discriminative domain knowledge per iteration, we propose a joint positive and negative learning procedure along with an uncertainty-guided progressive pseudo-labeling on the basis of prototype-based clustering and conditional probability. Our experimental results on several benchmarks demonstrate that Norma outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {833–843},
numpages = {11},
keywords = {transfer learning, pseudo-labeling, negative learning, image classification, deep-metric learning, class-aware alignment, adversarial unsupervised domain adaptation},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1016/j.compind.2008.03.003,
author = {Luo, Xinggang and Tu, Yiliu and Tang, Jiafu and Kwong, C. K.},
title = {Optimizing customer's selection for configurable product in B2C e-commerce application},
year = {2008},
issue_date = {October, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {8},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2008.03.003},
doi = {10.1016/j.compind.2008.03.003},
abstract = {Many companies provide configurable products on Internet to satisfy customers' diversified requirements. Most of business-to-consumer (B2C) e-commerce software systems use tree- or wizard-like approaches to guide customers in configuring a customized product on Internet web pages. However, customers may feel confused while they are selecting components of a product from option lists, since they are usually not familiar with the technical details of these components. A few e-commerce sites use recommendation systems to provide suggested products for customers, but they have to maintain user profiles and have limitations such as new user problem and complexity. Therefore, they may not be suitable for small and medium-sized enterprises. This research proposes a new approach to help customers configure their expected products. By using this approach, once a customer inputs the levels of importance of requirements, total budget of the expected product, the software system can figure out a customized product which maximally meets the customer's expectations, and can also provide the suboptimal solutions for further selections. A mathematical model to formulate this optimization problem is established. A case study is used to demonstrate the feasibility and effectiveness of this approach.},
journal = {Comput. Ind.},
month = oct,
pages = {767–776},
numpages = {10},
keywords = {e-Commerce, Recommendation system, Product family, Product configuration}
}

@inproceedings{10.1145/2660190.2660191,
author = {Kolesnikov, Sergiy and Roth, Judith and Apel, Sven},
title = {On the relation between internal and external feature interactions in feature-oriented product lines: a case study},
year = {2014},
isbn = {9781450329804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660190.2660191},
doi = {10.1145/2660190.2660191},
abstract = {The feature-interaction problem has been explored for many years. Still, we lack sufficient knowledge about the interplay of different kinds of interactions in software product lines. Exploring the relations between different kinds of feature interactions will allow us to learn more about the nature of interactions and their causes. This knowledge can then be applied for improving existing approaches for detecting, managing, and resolving feature interactions. We present a framework for studying relations between different kinds of interactions. Furthermore, we report and discuss the results of a preliminary study in which we examined correlations between internal feature interactions (quantified by a set of software measures) and external feature interactions (represented by product-line-specific type errors). We performed the evaluation on a set of 15 feature-oriented, Java-based product lines. We observed moderate correlations between the interactions under discussion. This gives us confidence that we can apply our approach to studying other types of external feature interactions (e.g., performance interactions).},
booktitle = {Proceedings of the 6th International Workshop on Feature-Oriented Software Development},
pages = {1–8},
numpages = {8},
keywords = {software measures, feature-oriented software development, feature interactions},
location = {Västerås, Sweden},
series = {FOSD '14}
}

@article{10.1016/j.procs.2021.01.128,
author = {Rabiser, Rick and Zoitl, Alois},
title = {Towards Mastering Variability in Software-Intensive Cyber-Physical Production Systems},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {180},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.01.128},
doi = {10.1016/j.procs.2021.01.128},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {50–59},
numpages = {10},
keywords = {evolution, variability mining, variability modeling, cyber-physical production systems, variability}
}

@article{10.1007/s11042-017-5172-1,
author = {Ma, Xueqi and Tao, Dapeng and Liu, Weifeng},
title = {Effective human action recognition by combining manifold regularization and pairwise constraints},
year = {2019},
issue_date = {May       2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {10},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-017-5172-1},
doi = {10.1007/s11042-017-5172-1},
abstract = {The ever-growing popularity of mobile networks and electronics has prompted intensive research on multimedia data (e.g. text, image, video, audio, etc.) management. This leads to the researches of semi-supervised learning that can incorporate a small number of labeled and a large number of unlabeled data by exploiting the local structure of data distribution. Manifold regularization and pairwise constraints are representative semi-supervised learning methods. In this paper, we introduce a novel local structure preserving approach by considering both manifold regularization and pairwise constraints. Specifically, we construct a new graph Laplacian that takes advantage of pairwise constraints compared with the traditional Laplacian. The proposed graph Laplacian can better preserve the local geometry of data distribution and achieve the effective recognition. Upon this, we build the graph regularized classifiers including support vector machines and kernel least squares as special cases for action recognition. Experimental results on a multimodal human action database (CAS-YNU-MHAD) show that our proposed algorithms outperform the general algorithms.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {13313–13329},
numpages = {17},
keywords = {Pairwise constraints, Manifold regularization, Local structure preserving, Action recognition}
}

@inproceedings{10.1145/3368089.3409700,
author = {Cambronero, José P. and Cito, Jürgen and Rinard, Martin C.},
title = {AMS: generating AutoML search spaces from weak specifications},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409700},
doi = {10.1145/3368089.3409700},
abstract = {We consider a usage model for automated machine learning (AutoML) in which users can influence the generated pipeline by providing a weak pipeline specification: an unordered set of API components from which the AutoML system draws the components it places into the generated pipeline. Such specifications allow users to express preferences over the components that appear in the pipeline, for example a desire for interpretable components to appear in the pipeline. We present AMS, an approach to automatically strengthen weak specifications to include unspecified complementary and functionally related API components, populate the space of hyperparameters and their values, and pair this configuration with a search procedure to produce a strong pipeline specification: a full description of the search space for candidate pipelines. ams uses normalized pointwise mutual information on a code corpus to identify complementary components, BM25 as a lexical similarity score over the target API's documentation to identify functionally related components, and frequency distributions in the code corpus to extract key hyperparameters and values. We show that strengthened specifications can produce pipelines that outperform the pipelines generated from the initial weak specification and an expert-annotated variant, while producing pipelines that still reflect the user preferences captured in the original weak specification.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {763–774},
numpages = {12},
keywords = {search-based software engineering, program mining, automated machine learning},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.3233/JID210027,
author = {Salinesi, Camille and Achtaich, Asmaa and Souissi, Nissrine and Mazo, Raúl and Roudies, Ounsa and Villota, Angela},
title = {State-Constraint Transition: A Language for the Formal Specification of Dynamic Cyber-System Requirements},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {2},
issn = {1092-0617},
url = {https://doi.org/10.3233/JID210027},
doi = {10.3233/JID210027},
abstract = {Existing formal languages for the specification of self-adaptive cyber-physical systems focus on re-configuring the system-to-be depending on its current context, to satisfy the user’s requirements, that is by dynamically composing the software’s structure and behavior. While these approaches specify context-sensitive requirements, they rarely consider their run-time dynamic and scalable nature. The State-Constraint Transition (SCT) modeling language, introduced in this paper, provides an answer to the problems linked to the specification of dynamic requirements by introducing the concept of configuration states, in which requirements are translated into constraints. The expressiveness of existing approaches is thus extended, combining the ease of use of well-established notations, notably those based on characteristics, and those based on Finite-state Machines (FSM), with the computational power and expressiveness of the constraint programming approach. The paper briefly presents the results of the preliminary evaluation, which assesses the expressiveness, scalability, and domain independence of the SCT language.},
journal = {J. Integr. Des. Process Sci.},
month = jan,
pages = {80–99},
numpages = {20},
keywords = {languages, constraint programming, state-machines, requirements engineering, software product lines, self-adaptive systems, Cyber-physical systems}
}

@article{10.1155/2020/8509821,
author = {Li, Hui and Qu, Yang and Guo, Shikai and Gao, Guofeng and Chen, Rong and Chen, Guo and Mobayen, Saleh},
title = {Surprise Bug Report Prediction Utilizing Optimized Integration with Imbalanced Learning Strategy},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1076-2787},
url = {https://doi.org/10.1155/2020/8509821},
doi = {10.1155/2020/8509821},
abstract = {In software projects, a large number of bugs are usually reported to bug repositories. Due to the limited budge and work force, the developers often may not have enough time and ability to inspect all the reported bugs, and thus they often focus on inspecting and repairing the highly impacting bugs. Among the high-impact bugs, surprise bugs are reported to be a fatal threat to the software systems, though they only account for a small proportion. Therefore, the identification of surprise bugs becomes an important work in practices. In recent years, some methods have been proposed by the researchers to identify surprise bugs. Unfortunately, the performance of these methods in identifying surprise bugs is still not satisfied for the software projects. The main reason is that surprise bugs only occupy a small percentage of all the bugs, and it is difficult to identify these surprise bugs from the imbalanced distribution. In order to overcome the imbalanced category distribution of the bugs, a method based on machine learning to predict surprise bugs is presented in this paper. This method takes into account the textual features of the bug reports and employs an imbalanced learning strategy to balance the datasets of the bug reports. Then these datasets after balancing are used to train three selected classifiers which are built by three different classification algorithms and predict the datasets with unknown type. In particular, an ensemble method named optimization integration is proposed to generate a unique and best result, according to the results produced by the three classifiers. This ensemble method is able to adjust the ability of the classifier to detect different categories based on the characteristics of different projects and integrate the advantages of three classifiers. The experiments performed on the datasets from 4 software projects show that this method performs better than the previous methods in terms of detecting surprise bugs.},
journal = {Complex.},
month = jan,
numpages = {14}
}

@article{10.1017/S0890060404040077,
author = {Zha, Xuan F. and Sriram, Ram D. and Lu, Wen F.},
title = {Evaluation and selection in product design for mass customization: A knowledge decision support approach},
year = {2004},
issue_date = {January 2004},
publisher = {Cambridge University Press},
address = {USA},
volume = {18},
number = {1},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060404040077},
doi = {10.1017/S0890060404040077},
abstract = {Mass customization has been identified as a competitive strategy by an increasing number of companies. Family-based product design is an efficient and effective means to realize sufficient product variety, while satisfying a range of customer demands in support for mass customization. This paper presents a knowledge decision support approach to product family design evaluation and selection for mass customization process. Here, product family design is viewed as a selection problem with the following stages: product family (design alternatives) generation, product family design evaluation, and selection for customization. The fundamental issues underlying product family design for mass customization are discussed. Then, a knowledge support framework and its relevant technologies are developed for module-based product family design for mass customization. A systematic fuzzy clustering and ranking model is proposed and discussed in detail. This model supports the imprecision inherent in decision making with fuzzy customers' preference relations and uses fuzzy analysis techniques for evaluation and selection. A neural network technique is also adopted to adjust the membership function to enhance the model. The focus of this paper is on the development of a knowledge-intensive support scheme and a comprehensive systematic fuzzy clustering and ranking methodology for product family design evaluation and selection. A case study and the scenario of knowledge support for power supply family evaluation, selection, and customization are provided for illustration.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = jan,
pages = {87–109},
numpages = {23},
keywords = {Product Platform, Product Family Design, Multicriteria Decision Making, Mass Customization, Knowledge Support, Fuzzy Ranking, Fuzzy Clustering, Design Evaluation, Customer-Driven Design}
}

@article{10.1016/j.ins.2019.02.051,
author = {Ros, Frédéric and Guillaume, Serge},
title = {         Munec: a mutual neighbor-based clustering algorithm},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {486},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.02.051},
doi = {10.1016/j.ins.2019.02.051},
journal = {Inf. Sci.},
month = jun,
pages = {148–170},
numpages = {23},
keywords = {Clustering, Distance, Density, Single link, Mutual neighbors}
}

@article{10.1007/s10618-016-0475-9,
author = {Garcia, Luís P. and Lorena, Ana C. and Matwin, Stan and Carvalho, André C.},
title = {Ensembles of label noise filters: a ranking approach},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {5},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-016-0475-9},
doi = {10.1007/s10618-016-0475-9},
abstract = {Label noise can be a major problem in classification tasks, since most machine learning algorithms rely on data labels in their inductive process. Thereupon, various techniques for label noise identification have been investigated in the literature. The bias of each technique defines how suitable it is for each dataset. Besides, while some techniques identify a large number of examples as noisy and have a high false positive rate, others are very restrictive and therefore not able to identify all noisy examples. This paper investigates how label noise detection can be improved by using an ensemble of noise filtering techniques. These filters, individual and ensembles, are experimentally compared. Another concern in this paper is the computational cost of ensembles, once, for a particular dataset, an individual technique can have the same predictive performance as an ensemble. In this case the individual technique should be preferred. To deal with this situation, this study also proposes the use of meta-learning to recommend, for a new dataset, the best filter. An extensive experimental evaluation of the use of individual filters, ensemble filters and meta-learning was performed using public datasets with imputed label noise. The results show that ensembles of noise filters can improve noise filtering performance and that a recommendation system based on meta-learning can successfully recommend the best filtering technique for new datasets. A case study using a real dataset from the ecological niche modeling domain is also presented and evaluated, with the results validated by an expert.},
journal = {Data Min. Knowl. Discov.},
month = sep,
pages = {1192–1216},
numpages = {25},
keywords = {Recommendation system, Noise ranking, Noise filters, Label noise, Ensemble filters}
}

@article{10.4018/ijdwm.2014040104,
author = {Liao, Shu-Hsien and Wen, Chih-Hao and Hsian, Pei-Yuan and Li, Chien-Wen and Hsu, Che-Wei},
title = {Mining Customer Knowledge for a Recommendation System in Convenience Stores},
year = {2014},
issue_date = {April 2014},
publisher = {IGI Global},
address = {USA},
volume = {10},
number = {2},
issn = {1548-3924},
url = {https://doi.org/10.4018/ijdwm.2014040104},
doi = {10.4018/ijdwm.2014040104},
abstract = {Taiwan's rapid economic growth with increasing personal income leads increasing numbers of young unmarried people to eat out, and shopping at convenience stores for food is indispensable to the lives of these people. Thus, it is an essential issue for convenience store owners to know how to accurately market appropriate products and to choose effective endorsers for brands or products in order to attract target consumers. Data mining is a business intelligence analysis approach with great potential to help businesses focus on the most important business information contained in a database. Therefore, this study uses the Apriori algorithm as an association rules approach, and clustering analysis for data mining. The authors divide consumers into three groups by their consumer profiles and then find each group's product preference mixes, product endorsers, and product/brand line extensions for new product development. These are developed as a recommendation system for 7-11 convenience stores in Taiwan.},
journal = {Int. J. Data Warehous. Min.},
month = apr,
pages = {55–86},
numpages = {32},
keywords = {Recommendation System, Endorsers, Data Mining, Convenience Stores, Business Intelligence, Brand and Product Line Extensions}
}

@inproceedings{10.5555/3495724.3496750,
author = {Zhang, Dingwen and Tian, Haibin and Han, Jungong},
title = {Few-cost salient object detection with adversarial-paced learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Detecting and segmenting salient objects from given image scenes has received great attention in recent years. A fundamental challenge in training the existing deep saliency detection models is the requirement of large amounts of annotated data. While gathering large quantities of training data becomes cheap and easy, annotating the data is an expensive process in terms of time, labor and human expertise. To address this problem, this paper proposes to learn the effective salient object detection model based on the manual annotation on a few training images only, thus dramatically alleviating human labor in training models. To this end, we name this task as the few-cost salient object detection and propose an adversarial-paced learning (APL)-based framework to facilitate the few-cost learning scenario. Essentially, APL is derived from the self-paced learning (SPL) regime but it infers the robust learning pace through the data-driven adversarial learning mechanism rather than the heuristic design of the learning regularizer. Comprehensive experiments on four widely-used benchmark datasets demonstrate that the proposed method can effectively approach to the existing supervised deep salient object detection models with only 1k human-annotated training images.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1026},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.5555/3504035.3504690,
author = {Neill, James O' and Buitelaar, Paul},
title = {Few shot transfer learning between word relatedness and similarity tasks using a gated recurrent siamese network},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Word similarity and word relatedness are fundamental to natural language processing and more generally, understanding how humans relate concepts in semantic memory. A growing number of datasets are being proposed as evaluation benchmarks, however, the heterogeneity and focus of each respective dataset makes it difficult to draw plausible conclusions as to how a unified semantic model would perform. Additionally, we want to identify the transferability of knowledge obtained from one task to another, within the same domain and across domains. Hence, this paper first presents an evaluation and comparison of eight chosen datasets tested using the best performing regression models. As a baseline, we present regression models that incorporate both lexical features and word embeddings to produce consistent and competitive results compared to the state of the art. We present our main contribution, the best performing model across seven of the eight datasets - a Gated Recurrent Siamese Network that learns relationships between lexical word definitions. A parameter transfer learning strategy is employed for the Siamese Network. Subsequently, we present a secondary contribution which is the best performing non-sequential model: an Inductive and Transductive Transfer Learning strategy for transferring decision trees within a Random Forest to a target task that is learned from only few instances. The method involves measuring semantic distance between hidden factored matrix representations of decision tree traversal matrices.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {655},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1007/s10009-019-00544-0,
author = {Giantamidis, Georgios and Tripakis, Stavros and Basagiannis, Stylianos},
title = {Learning Moore machines from input–output traces},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {1},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-019-00544-0},
doi = {10.1007/s10009-019-00544-0},
abstract = {The problem of learning automata from example traces (but no equivalence or membership queries) is fundamental in automata learning theory and practice. In this paper, we study this problem for finite-state machines with inputs and outputs, and in particular for Moore machines. We develop three algorithms for solving this problem: (1) the PTAP algorithm, which transforms a set of input–output traces into an incomplete Moore machine and then completes the machine with self-loops; (2) the PRPNI algorithm, which uses the well-known RPNI algorithm for automata learning to learn a product of automata encoding a Moore machine; and (3) the MooreMI algorithm, which directly learns a Moore machine using PTAP extended with state merging. We prove that MooreMI has the fundamental identification in the limit property. We compare the algorithms experimentally in terms of the size of the learned machine and several notions of accuracy, introduced in this paper. We also carry out a performance comparison against two existing tools (LearnLib and flexfringe). Finally, we compare with OSTIA, an algorithm that learns a more general class of transducers and find that OSTIA generally does not learn a Moore machine, even when fed with a characteristic sample.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = feb,
pages = {1–29},
numpages = {29},
keywords = {Characteristic sample, Passive learning, Automata learning, Mealy machine, Moore machine, Finite state machine}
}

@article{10.1016/j.ins.2021.05.008,
author = {Zhang, Nana and Ying, Shi and Ding, Weiping and Zhu, Kun and Zhu, Dandan},
title = {WGNCS: A robust hybrid cross-version defect model via multi-objective optimization and deep enhanced feature representation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {570},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.05.008},
doi = {10.1016/j.ins.2021.05.008},
journal = {Inf. Sci.},
month = sep,
pages = {545–576},
numpages = {32},
keywords = {Convolutional neural network, Wasserstein GAN with Gradient Penalty, Deep learning techniques, Multi-objective feature selection, Cross-version defect prediction}
}

@inproceedings{10.1109/WAIN52551.2021.00019,
author = {Granlund, Tuomas and Kopponen, Aleksi and Stirbu, Vlad and Myllyaho, Lalli and Mikkonen, Tommi},
title = {MLOps Challenges in Multi-Organization Setup: Experiences from Two Real-World Cases},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00019},
doi = {10.1109/WAIN52551.2021.00019},
abstract = {The emerging age of connected, digital world means that there are tons of data, distributed to various organizations and their databases. Since this data can be confidential in nature, it cannot always be openly shared in seek of artificial intelligence (AI) and machine learning (ML) solutions. Instead, we need integration mechanisms, analogous to integration patterns in information systems, to create multi-organization AI/ML systems. In this paper, we present two real-world cases. First, we study integration between two organizations in detail. Second, we address scaling of AI/ML to multi-organization context. The setup we assume is that of continuous deployment, often referred to DevOps in software development. When also ML components are deployed in a similar fashion, term MLOps is used. Towards the end of the paper, we list the main observations and draw some final conclusions. Finally, we propose some directions for future work.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {82–88},
numpages = {7},
location = {Madrid, Spain}
}

@inproceedings{10.5555/3504035.3504869,
author = {Fan, Xin and Liu, Risheng and Huyan, Kang and Feng, Yuyao and Luo, Zhongxuan},
title = {Self-reinforced cascaded regression for face alignment},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Cascaded regression is prevailing in face alignment thanks to its accuracy and robustness, but typically demands manually annotated examples having low discrepancy between shape-indexed features and shape updates. In this paper, we propose a self-reinforced strategy that iteratively expands the quantity and improves the quality of training examples, thus upgrading the performance of cascaded regression itself. The reinforced term evaluates the example quality upon the consistence on both local appearance and global geometry of human faces, and constitutes the example evolution by the philosophy of "survival of the fittest". We train a set of discriminative classifiers, each associated with one landmark label, to prune those examples with inconsistent local appearance, and further validate the geometric relationship among groups of labeled landmarks against the common global geometry derived from a projective invariant. We embed this generic strategy into typical cascaded regressions, and the alignment results on several benchmark data sets demonstrate its effectiveness to predict good examples starting from a small subset.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {834},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.1007/978-3-030-82017-6_11,
author = {Höhn, Sviatlana and Faradouris, Niko},
title = {What Does It Cost to Deploy an XAI System: A Case Study in Legacy Systems},
year = {2021},
isbn = {978-3-030-82016-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82017-6_11},
doi = {10.1007/978-3-030-82017-6_11},
abstract = {Enterprise Resource Planning (ERP) software is used by businesses and extended via customisation. Automated custom code analysis and migration is a critical issue at ERP release upgrade times. Despite research advances, automated code analysis and transformation require a huge amount of manual work related to parser adaptation, rule extension and post-processing. These operations become unmanageable if the frequency of updates increases from yearly to monthly intervals. This article describes how the process of custom code analysis to custom code transformation can be automated in an explainable way. We develop an aggregate taxonomy for explainability and analyse the requirements based on roles. We explain in which steps on the new code migration process machine learning is used. Further, we analyse additional effort needed to make the new way of code migration explainable to different stakeholders.},
booktitle = {Explainable and Transparent AI and Multi-Agent Systems: Third International Workshop, EXTRAAMAS 2021, Virtual Event, May 3–7, 2021, Revised Selected Papers},
pages = {173–186},
numpages = {14},
keywords = {Explainability taxonomy, Multi-modal conversational interfaces, Explainable automated source-code transformation}
}

@article{10.1016/j.knosys.2006.04.004,
author = {Zha, Xuan F. and Sriram, Ram D.},
title = {Platform-based product design and development: A knowledge-intensive support approach},
year = {2006},
issue_date = {November, 2006},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {19},
number = {7},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2006.04.004},
doi = {10.1016/j.knosys.2006.04.004},
abstract = {This paper presents a knowledge-intensive support paradigm for platform-based product family design and development. The fundamental issues underlying the product family design and development, including product platform and product family modeling, product family generation and evolution, and product family evaluation for customization, are discussed. A module-based integrated design scheme is proposed with knowledge support for product family architecture modeling, product platform establishment, product family generation, and product variant assessment. A systematic methodology and the relevant technologies are investigated and developed for knowledge supported product family design process. The developed information and knowledge-modeling framework and prototype system can be used for platform product design knowledge capture, representation and management and offer on-line support for designers in the design process. The issues and requirements related to developing a knowledge-intensive support system for modular platform-based product family design are also addressed.},
journal = {Know.-Based Syst.},
month = nov,
pages = {524–543},
numpages = {20},
keywords = {Product platform, Product family, Product architecture, Modular design, Knowledge support}
}

@inproceedings{10.1145/2070821.2070824,
author = {Menzies, Tim and Bird, Christian and Zimmermann, Thomas and Schulte, Wolfram and Kocaganeli, Ekrem},
title = {The inductive software engineering manifesto: principles for industrial data mining},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070824},
doi = {10.1145/2070821.2070824},
abstract = {The practices of industrial and academic data mining are very different. These differences have significant implications for (a) how we manage industrial data mining projects; (b) the direction of academic studies in data mining; and (c) training programs for engineers who seek to use data miners in an industrial setting.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {19–26},
numpages = {8},
keywords = {industry, inductive engineering},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@inproceedings{10.1007/978-3-030-13709-0_30,
author = {Bourel, Mathias and Canale, Eduardo and Robledo, Franco and Romero, Pablo and Stábile, Luis},
title = {A GRASP/VND Heuristic for the Max Cut-Clique Problem},
year = {2018},
isbn = {978-3-030-13708-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-13709-0_30},
doi = {10.1007/978-3-030-13709-0_30},
abstract = {In Market Basket Analysis, the goal is to understand the human behavior in order to maximize sales. An evident behavior is to buy correlated items. As a consequence, the determination of a set of items with a large correlation with others is a valuable tool for Market Basket Analysis.In this paper we address a combinatorial optimization problem that formalizes the previous application. Given a simple graph  (where the nodes are items and links represent correlation), we want to find the clique  such that the number of links shared between  and  is maximized. This problem is known in the literature as Max Cut-Clique (MCC).The contributions of this paper are three-fold. First, the computational complexity of the MCC is established. Second, a full GRASP/VND methodology enriched with a Tabu Search is here developed, where the main ingredients are novel local searches and a Restricted Candidate List that trades greediness for randomization in a multi-start fashion. A Tabu Search is also included in order to avoid locally optimum solutions. Finally, a fair comparison with respect to recent heuristics reveals that our proposal is competitive with state-of-the-art solutions.},
booktitle = {Machine Learning, Optimization, and Data Science: 4th International Conference, LOD 2018, Volterra, Italy, September 13-16, 2018, Revised Selected Papers},
pages = {357–367},
numpages = {11},
keywords = {Market Basket Analysis, Combinatorial optimization, Max Cut-Clique, Metaheuristics},
location = {Volterra, Italy}
}

@article{10.5555/2598944.2599210,
author = {Maldonado, Sebastián and López, Julio},
title = {Alternative second-order cone programming formulations for support vector classification},
year = {2014},
issue_date = {June, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {268},
issn = {0020-0255},
abstract = {This paper presents two novel second-order cone programming (SOCP) formulations that determine a linear predictor using Support Vector Machines (SVMs). Inspired by the soft-margin SVM formulation, our first approach (@x-SOCP-SVM) proposes a relaxation of the conic constraints via a slack variable, penalizing it in the objective function. The second formulation (r-SOCP-SVM) is based on the LP-SVM formulation principle: the bound of the VC dimension is loosened properly using the l"~-norm, and the margin is directly maximized. The proposed methods have several advantages: The first approach constructs a flexible classifier, extending the benefits of the soft-margin SVM formulation to second-order cones. The second method obtains comparable results to the SOCP-SVM formulation with less computational effort, since one conic restriction is eliminated. Experiments on well-known benchmark datasets from the UCI Repository demonstrate that our approach accomplishes the best classification performance compared to the traditional SOCP-SVM formulation, LP-SVM, and to standard linear SVM.},
journal = {Inf. Sci.},
month = jun,
pages = {328–341},
numpages = {14},
keywords = {Support Vector Machine, Second-order cone programming, Linear programming SVM}
}

@article{10.1145/3359981,
author = {Leite, Leonardo and Rocha, Carla and Kon, Fabio and Milojicic, Dejan and Meirelles, Paulo},
title = {A Survey of DevOps Concepts and Challenges},
year = {2019},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3359981},
doi = {10.1145/3359981},
abstract = {DevOpsis a collaborative and multidisciplinary organizational effort to automate continuous delivery of new software updates while guaranteeing their correctness and reliability. The present survey investigates and discusses DevOps challenges from the perspective of engineers, managers, and researchers. We review the literature and develop a DevOps conceptual map, correlating the DevOps automation tools with these concepts. We then discuss their practical implications for engineers, managers, and researchers. Finally, we critically explore some of the most relevant DevOps challenges reported by the literature.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {127},
numpages = {35},
keywords = {versioning, release process, continuous (delivery, deployment, integration), configuration management, and build process, DevOps}
}

@article{10.1007/s10916-015-0219-1,
author = {Aydın, Serap and Tunga, M. Alper and Yetkin, Sinan},
title = {Mutual Information Analysis of Sleep EEG in Detecting Psycho-Physiological Insomnia},
year = {2015},
issue_date = {May       2015},
publisher = {Plenum Press},
address = {USA},
volume = {39},
number = {5},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-015-0219-1},
doi = {10.1007/s10916-015-0219-1},
abstract = {The primary goal of this study is to state the clear changes in functional brain connectivity during all night sleep in psycho-physiological insomnia (PPI). The secondary goal is to investigate the usefulness of Mutual Information (MI) analysis in estimating cortical sleep EEG arousals for detection of PPI. For these purposes, healthy controls and patients were compared to each other with respect to both linear (Pearson correlation coefficient and coherence) and nonlinear quantifiers (MI) in addition to phase locking quantification for six sleep stages (stage.1---4, rem, wake) by means of interhemispheric dependency between two central sleep EEG derivations. In test, each connectivity estimation calculated for each couple of epoches (C3-A2 and C4-A1) was identified by the vector norm of estimation. Then, patients and controls were classified by using 10 different types of data mining classifiers for five error criteria such as accuracy, root mean squared error, sensitivity, specificity and precision. High performance in a classification through a measure will validate high contribution of that measure to detecting PPI. The MI was found to be the best method in detecting PPI. In particular, the patients had lower MI, higher PCC for all sleep stages. In other words, the lower sleep EEG synchronization suffering from PPI was observed. These results probably stand for the loss of neurons that then contribute to less complex dynamical processing within the neural networks in sleep disorders an the functional central brain connectivity is nonlinear during night sleep. In conclusion, the level of cortical hemispheric connectivity is strongly associated with sleep disorder. Thus, cortical communication quantified in all existence sleep stages might be a potential marker for sleep disorder induced by PPI.},
journal = {J. Med. Syst.},
month = may,
pages = {1–10},
numpages = {10},
keywords = {Sleep EEG, Mutual information, Data mining, Classification, Brain connectivity}
}

@inproceedings{10.1109/ICSE43902.2021.00040,
author = {Lin, Jinfeng and Liu, Yalin and Zeng, Qingkai and Jiang, Meng and Cleland-Huang, Jane},
title = {Traceability Transformed: Generating more Accurate Links with Pre-Trained BERT Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00040},
doi = {10.1109/ICSE43902.2021.00040},
abstract = {Software traceability establishes and leverages associations between diverse development artifacts. Researchers have proposed the use of deep learning trace models to link natural language artifacts, such as requirements and issue descriptions, to source code; however, their effectiveness has been restricted by availability of labeled data and efficiency at runtime. In this study, we propose a novel framework called Trace BERT (T-BERT) to generate trace links between source code and natural language artifacts. To address data sparsity, we leverage a three-step training strategy to enable trace models to transfer knowledge from a closely related Software Engineering challenge, which has a rich dataset, to produce trace links with much higher accuracy than has previously been achieved. We then apply the T-BERT framework to recover links between issues and commits in Open Source Projects. We comparatively evaluated accuracy and efficiency of three BERT architectures. Results show that a Single-BERT architecture generated the most accurate links, while a Siamese-BERT architecture produced comparable results with significantly less execution time. Furthermore, by learning and transferring knowledge, all three models in the framework outperform classical IR trace models. On the three evaluated real-word OSS projects, the best T-BERT stably outperformed the VSM model with average improvements of 60.31% measured using Mean Average Precision (MAP). RNN severely underper-formed on these projects due to insufficient training data, while T-BERT overcame this problem by using pretrained language models and transfer learning.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {324–335},
numpages = {12},
keywords = {language models, deep learning, Software traceability},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1007/978-3-030-77385-4_42,
author = {Halilaj, Lavdim and Dindorkar, Ishan and Lüttin, Jürgen and Rothermel, Susanne},
title = {A Knowledge Graph-Based Approach for Situation Comprehension in Driving Scenarios},
year = {2021},
isbn = {978-3-030-77384-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77385-4_42},
doi = {10.1007/978-3-030-77385-4_42},
abstract = {Making an informed and right decision poses huge challenges for drivers in day-to-day traffic situations. This task vastly depends on many subjective and objective factors, including the current driver state, her destination, personal preferences and abilities as well as surrounding environment. In this paper, we present CoSI (Context and Situation Intelligence), a Knowledge Graph (KG)-based approach for fusing and organizing heterogeneous types and sources of information. The KG serves as a coherence layer representing information in the form of entities and their inter-relationships augmented with additional semantic axioms. Harnessing the power of axiomatic rules and reasoning capabilities enables inferring additional knowledge from what is already encoded. Thus, dedicated components exploit and consume the semantically enriched information to perform tasks such as situation classification, difficulty assessment, and trajectory prediction. Further, we generated a synthetic dataset to simulate real driving scenarios with a large range of driving styles and vehicle configurations. We use KG embedding techniques based on a Graph Neural Network (GNN) architecture for a classification task of driving situations and achieve over 95% accuracy whereas vector-based approaches achieve only 75% accuracy for the same task. The results suggest that the KG-based information representation combined with GNN are well suited for situation understanding tasks as required in driver assistance and automated driving systems.},
booktitle = {The Semantic Web: 18th International Conference, ESWC 2021, Virtual Event, June 6–10, 2021, Proceedings},
pages = {699–716},
numpages = {18},
keywords = {Graph neural network, Knowledge graph embedding, Knowledge graph, Situation comprehension}
}

@article{10.1007/s00521-020-05490-6,
author = {Balaji, N. and Lakshmi, S. and Anand, M. and Anbarasan, M. and Mathiyalagan, P.},
title = {An efficient and secure feature location approach in source code using Jacobian matrix-based clustering},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {12},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05490-6},
doi = {10.1007/s00521-020-05490-6},
abstract = {In software, a feature is a functionality, which has the chief role in identifying the primary location of the source code (SC), and it is stated by means of requirements as well as accessibility to developers and also users. Prevailing feature location (FL) methods have issues in security effectiveness. This paper proposed an effective and safe FL approach in SC utilizing the Jacobian matrix-based clustering (JMC) to trounce these issues. Originally, the requirement-based approach removes the repeated test cases (TC). Subsequently, the weight-based salp swarms algorithm (Ψ-SSA) selects the significant TC attributes. Followed by which, the associations rule mining (ARM) process is implemented. The ARM encompasses the itemset, support, frequent itemset, closed frequent itemset, confidence, in addition to divergence. Subsequently, the affinity is computed by means of the blend of confidence with divergence. In this affinity computation, the MALO optimizes the weight value. Then, the exponent-based elliptic curves cryptographic encrypts the affinity value. Subsequently, the score value is computed for the encrypted affinity value centered on the entropy computation. Finally, the JMC locates the feature centered on the score value. Experimental assessment exhibits that the proposed system’s performance is better than that of the prevailing research methodologies.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {7235–7247},
numpages = {13},
keywords = {Jacobian matrix-based clustering (JMC), Exponent-based elliptic curve cryptography (E-ECC), Modified ant lion optimization (MALO), Weight-based salp swarm algorithm (Ψ-SSA), Requirement-based approach (RBA)}
}

@article{10.1016/j.jss.2009.06.048,
author = {Khurum, Mahvish and Gorschek, Tony},
title = {A systematic review of domain analysis solutions for product lines},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.048},
doi = {10.1016/j.jss.2009.06.048},
abstract = {Domain analysis is crucial and central to software product line engineering (SPLE) as it is one of the main instruments to decide what to include in a product and how it should fit in to the overall software product line. For this reason many domain analysis solutions have been proposed both by researchers and industry practitioners. Domain analysis comprises various modeling and scoping activities. This paper presents a systematic review of all the domain analysis solutions presented until 2007. The goal of the review is to analyze the level of industrial application and/or empirical validation of the proposed solutions with the purpose of mapping maturity in terms of industrial application, as well as to what extent proposed solutions might have been evaluated in terms of usability and usefulness. The finding of this review indicates that, although many new domain analysis solutions for software product lines have been proposed over the years, the absence of qualitative and quantitative results from empirical application and/or validation makes it hard to evaluate the potential of proposed solutions with respect to their usability and/or usefulness for industry adoption. The detailed results of the systematic review can be used by individual researchers to see large gaps in research that give opportunities for future work, and from a general research perspective lessons can be learned from the absence of validation as well as from good examples presented. From an industry practitioner view, the results can be used to gauge as to what extent solutions have been applied and/or validated and in what manner, both valuable as input prior to industry adoption of a domain analysis solution.},
journal = {J. Syst. Softw.},
month = dec,
pages = {1982–2003},
numpages = {22},
keywords = {Usefulness, Usability, Systematic review, Empirical evidence, Domain scoping, Domain modeling, Domain analysis}
}

@inproceedings{10.1007/978-3-642-34026-0_10,
author = {Schaefer, Ina and Lochau, Malte and Leucker, Martin},
title = {Approaches for mastering change},
year = {2012},
isbn = {9783642340253},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34026-0_10},
doi = {10.1007/978-3-642-34026-0_10},
abstract = {Modern software systems are highly configurable and exist in many different variants in order to operate different application contexts. This is called static variability and predominantly considered in software product line engineering [6,14]. Furthermore, software systems have to evolve over time in order to dealwith changing requirements which is referred to by the term temporal evolvability [10,13]. Additionally, modern software systems are designed to dynamically adapt their internal structure and behavior at runtime dependent on their environment in order to efficiently use the available resources, such as energy or computing power [5]. These three dimensions of change, static variability, temporal evolvability and dynamic adaptation, increase the complexity of system development in all phase, from requirements engineering and system design to implementation and quality assurance. In [15], the challenges of static variability and temporal evolution in all phases of the software development process are discussed. In [15], the engineering challenges of self-adaptive systems are described and future research directions are pointed out.},
booktitle = {Proceedings of the 5th International Conference on Leveraging Applications of Formal Methods, Verification and Validation: Technologies for Mastering Change - Volume Part I},
pages = {127–130},
numpages = {4},
location = {Heraklion, Crete, Greece},
series = {ISoLA'12}
}

@inproceedings{10.5555/3466184.3466364,
author = {Heger, Jens and Voss, Thomas},
title = {Dynamically changing sequencing rules with reinforcement learning in a job shop system with stochastic influences},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Sequencing operations can be difficult, especially under uncertain conditions. Applying decentral sequencing rules has been a viable option; however, no rule exists that can outperform all other rules under varying system performance. For this reason, reinforcement learning (RL) is used as a hyper heuristic to select a sequencing rule based on the system status. Based on multiple training scenarios considering stochastic influences, such as varying inter arrival time or customers changing the product mix, the advantages of RL are presented. For evaluation, the trained agents are exploited in a generic manufacturing system. The best agent trained is able to dynamically adjust sequencing rules based on system performance, thereby matching and outperforming the presumed best static sequencing rules by ≈ 3%. Using the trained policy in an unknown scenario, the RL heuristic is still able to change the sequencing rule according to the system status, thereby providing robust performance.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1608–1618},
numpages = {11},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1016/j.knosys.2017.03.026,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Garca, Salvador and Tang, Jia-Fu and Herrera, Francisco},
title = {Exploring the effectiveness of dynamic ensemble selection in the one-versus-one scheme},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.03.026},
doi = {10.1016/j.knosys.2017.03.026},
abstract = {The One-versus-One (OVO) strategy is one of the most common and effective techniques to deal with multi-class classification problems. The basic idea of an OVO scheme is to divide a multi-class classification problem into several easier-to-solve binary classification problems with considering each possible pair of classes from the original problem, which is then built into a binary classifier by an independent base learner. In this study, we propose a novel methodology which attempts to select a group of base classifiers in each pairwise dataset for each unknown pattern. To implement this, the Dynamic Ensemble Selection (DES) method based on a competence measure is employed to select the most appropriate ensemble in each binary classification problem derived from the OVO decomposition. In order to verify the validity and effectiveness of our proposed method, we carry out a thorough experimental study. We first compare our proposal with several state-of-the-art approaches. Then, we perform the comparison of several well-known aggregation strategies to combine the binary ensemble obtained by Dynamic Ensemble Selection. Finally, we explore whether further improvement can be achieved by considering the competence-based method in OVO scheme. The extracted findings drawn from the empirical analysis are supported by the proper statistical analysis and indicate that there is a positive synergy between the DES method and the Distance-based Relative Competence Weighting (DRCW) approach for the OVO scheme.},
journal = {Know.-Based Syst.},
month = jun,
pages = {53–63},
numpages = {11},
keywords = {Pairwise learning, One-versus-One, Multi-classification, Dynamic ensemble selection, Decomposition strategies}
}

@article{10.1287/orsc.2017.1164,
author = {Ganz, Scott C.},
title = {Ignorant Decision Making and Educated Inertia: Some Political Pathologies of Organizational Learning},
year = {2018},
issue_date = {February 2018},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {29},
number = {1},
issn = {1526-5455},
url = {https://doi.org/10.1287/orsc.2017.1164},
doi = {10.1287/orsc.2017.1164},
abstract = {Studies of failures in organizational information gathering, learning, and decision making highlight psychological and institutional causes. However, organizations are also political coalitions that face internal contestation over strategies, policies, and goals. The decision of whether to collect information impacts both the goals that organization members try to meet and the organization's capacity to meet them. This paper develops a formal model that introduces political conflict into a theory of organizational learning. The model has a key insight: organizations characterized by political conflict will forego learning when leaders do not need to learn to build consensus in support of change, and will learn when leaders are unable to build such a consensus without learning. As a result, political conflict leads organizations to implement changes without first learning and to frequently learn when no changes are forthcoming. These tendencies toward ignorant decision making and educated inertia will be more pronounced when environmental variability is high, when learning about existing policies does not also teach about potential policy alternatives, and when organization members are risk averse. The model offers predictions about pathological learning behaviors that are consistent with considerable prior qualitative research. These patterns cannot be produced by experiential learning models in which political conflict is not present.The e-companion is available at &lt;ext-link ext-link-type="uri" href="https://doi.org/10.1287/orsc.2017.1164"&gt;https://doi.org/10.1287/orsc.2017.1164&lt;/ext-link&gt;.},
journal = {Organization Science},
month = feb,
pages = {39–57},
numpages = {19},
keywords = {power and politics, organizational learning, interorganizational relationships, conflict}
}

@article{10.1016/j.jss.2019.110420,
author = {de Oliveira, Marcos César and Freitas, Davi and Bonifácio, Rodrigo and Pinto, Gustavo and Lo, David},
title = {Finding needles in a haystack: Leveraging co-change dependencies to recommend refactorings},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110420},
doi = {10.1016/j.jss.2019.110420},
journal = {J. Syst. Softw.},
month = dec,
numpages = {19},
keywords = {Design quality, Software clustering, Remodularization, Co-change dependencies, Refactoring}
}

@article{10.1287/ited.2019.0225,
author = {Cole, Rosanna and Snider, Brent},
title = {Game—Rolling the Dice on Global Supply Chain Sustainability: A Total Cost of Ownership Simulation},
year = {2020},
issue_date = {May 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {20},
number = {3},
issn = {1532-0545},
url = {https://doi.org/10.1287/ited.2019.0225},
doi = {10.1287/ited.2019.0225},
abstract = {Sustainability in management education is a potential solution to societal challenges, influencing students’ worldviews and attitudes to contribute to a more profound social change. Through this innovative dice-based classroom simulation, students are exposed to supply chain sustainability, total cost of ownership (TCO), and risk management while also understanding their linkages through effective instructor debrief. Student teams compete by selecting sourcing options such as supplier location, transportation methods, and sustainability reputation from a menu, then see how their decisions fare as the product-line life cycle is simulated with a dice. The debrief facilitated by the instructor compares and contrasts results across the teams, generating insights into the interrelationships between supply chain sustainability choices, TCO, and risk management. Successfully conducted by multiple instructors, in multiple countries, and across all levels of management education (undergraduate, master of science, and executive master of business administration), survey results (n = 350) plus a pilot study (n = 31) confirm that this dice-based simulation accomplishes multiple learning objectives while also providing a highly engaging experiential learning classroom environment for this sample.},
journal = {INFORMS Trans. Edu.},
month = may,
pages = {165–176},
numpages = {12},
keywords = {sustainability, active learning, evaluating students, teaching supply chain management, classroom games}
}

@article{10.1155/2015/196098,
author = {Yang, Jinfeng and Xiao, Yong and Wang, Jiabing and Ma, Qianli and Shen, Yanhua},
title = {A fast clustering algorithm for data with a few labeled instances},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-5265},
url = {https://doi.org/10.1155/2015/196098},
doi = {10.1155/2015/196098},
abstract = {The diameter of a cluster is the maximum intracluster distance between pairs of instances within the same cluster, and the split of a cluster is the minimum distance between instances within the cluster and instances outside the cluster. Given a few labeled instances, this paper includes two aspects. First, we present a simple and fast clustering algorithm with the following property: if the ratio of the minimum split to the maximum diameter (RSD) of the optimal solution is greater than one, the algorithm returns optimal solutions for three clustering criteria. Second, we study the metric learning problem: learn a distance metric to make the RSD as large as possible. Compared with existing metric learning algorithms, one of our metric learning algorithms is computationally efficient: it is a linear programming model rather than a semidefinite programming model used by most of existing algorithms. We demonstrate empirically that the supervision and the learned metric can improve the clustering quality.},
journal = {Intell. Neuroscience},
month = jan,
articleno = {21},
numpages = {1}
}

@article{10.1016/j.compind.2009.07.005,
author = {Brière-Côté, Antoine and Rivest, Louis and Desrochers, Alain},
title = {Adaptive generic product structure modelling for design reuse in engineer-to-order products},
year = {2010},
issue_date = {January, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {61},
number = {1},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2009.07.005},
doi = {10.1016/j.compind.2009.07.005},
abstract = {In product lifecycle management, the efficiency of information reuse relies on the definition and management of equivalence information between various product data and structure representations. Equivalence information ensures the consistency and traceability of product information throughout the product lifecycle. The sales-delivery process of engineer-to-order (ETO) products presents a great potential for design reuse, i.e. the reuse of previously validated design solutions in the design of new product variants according to customer-specific requirements. A product family data model that focuses on the interdependencies of viewpoints on information will therefore improve the setup of design reuse mechanisms such as modularity. This paper describes the Adaptive Generic Product Structure (AGPS), a dynamic structure-based product family modelling approach that enables the systematic aggregation of product variants and their distinctive components. The purpose of the approach is to capitalize on the expanding component variety developed within previous product variants as early as the sales lead phase of the sales-delivery process, in order to reduce customer-driven design costs and shorten lead-times. An illustrative example based on the aerospace industry is presented.},
journal = {Comput. Ind.},
month = jan,
pages = {53–65},
numpages = {13},
keywords = {Product structure, Product family, Modularity, Engineer-to-order, Design reuse}
}

@article{10.1145/3230709,
author = {Liu, Wenhe and Chang, Xiaojun and Yan, Yan and Yang, Yi and Hauptmann, Alexander G.},
title = {Few-Shot Text and Image Classification via Analogical Transfer Learning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230709},
doi = {10.1145/3230709},
abstract = {Learning from very few samples is a challenge for machine learning tasks, such as text and image classification. Performance of such task can be enhanced via transfer of helpful knowledge from related domains, which is referred to as transfer learning. In previous transfer learning works, instance transfer learning algorithms mostly focus on selecting the source domain instances similar to the target domain instances for transfer. However, the selected instances usually do not directly contribute to the learning performance in the target domain. Hypothesis transfer learning algorithms focus on the model/parameter level transfer. They treat the source hypotheses as well-trained and transfer their knowledge in terms of parameters to learn the target hypothesis. Such algorithms directly optimize the target hypothesis by the observable performance improvements. However, they fail to consider the problem that instances that contribute to the source hypotheses may be harmful for the target hypothesis, as instance transfer learning analyzed. To relieve the aforementioned problems, we propose a novel transfer learning algorithm, which follows an analogical strategy. Particularly, the proposed algorithm first learns a revised source hypothesis with only instances contributing to the target hypothesis. Then, the proposed algorithm transfers both the revised source hypothesis and the target hypothesis (only trained with a few samples) to learn an analogical hypothesis. We denote our algorithm as Analogical Transfer Learning. Extensive experiments on one synthetic dataset and three real-world benchmark datasets demonstrate the superior performance of the proposed algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {71},
numpages = {20},
keywords = {classification, Transfer learning}
}

@article{10.1016/j.sigpro.2016.03.008,
author = {Derrode, Stéphane and Pieczynski, Wojciech},
title = {Unsupervised classification using hidden Markov chain with unknown noise copulas and margins},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2016.03.008},
doi = {10.1016/j.sigpro.2016.03.008},
abstract = {We consider the problem of unsupervised classification of hidden Markov models (HMC) with dependent noise. Time is discrete, the hidden process takes its values in a finite set of classes, while the observed process is continuous. We adopt an extended HMC model in which the rich possibilities of different kinds of dependence in the noise are modelled via copulas. A general model identification algorithm, in which different noise margins and copulas corresponding to different classes are selected in given families and estimated in an automated way, from the sole observed process, is proposed. The interest of the whole procedure is shown via experiments on simulated data and on a real SAR image. HighlightsHidden Markov chain with non-Gaussian correlated noise modelled via a copula representation.Design of a generalized ICE algorithm for model identification and parameters estimation.Automatic selection of best-fitting copulas and margins within sets of admissible shapes.Illustration with SAR image segmentation.},
journal = {Signal Process.},
month = nov,
pages = {8–17},
numpages = {10},
keywords = {Unsupervised classification, Pearson's system of distributions, Model selection, Iterative conditional estimation, Hidden Markov models, Dependent noise, Copulas}
}

@inproceedings{10.1109/ICSE43902.2021.00094,
author = {Dey, Tapajit and Karnauch, Andrey and Mockus, Audris},
title = {Representation of Developer Expertise in Open Source Software},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00094},
doi = {10.1109/ICSE43902.2021.00094},
abstract = {Background: Accurate representation of developer expertise has always been an important research problem. While a number of studies proposed novel methods of representing expertise within individual projects, these methods are difficult to apply at an ecosystem level. However, with the focus of software development shifting from monolithic to modular, a method of representing developers' expertise in the context of the entire OSS development becomes necessary when, for example, a project tries to find new maintainers and look for developers with relevant skills. Aim: We aim to address this knowledge gap by proposing and constructing the Skill Space where each API, developer, and project is represented and postulate how the topology of this space should reflect what developers know (and projects need). Method: we use the World of Code infrastructure to extract the complete set of APIs in the files changed by open source developers and, based on that data, employ Doc2Vec embeddings for vector representations of APIs, developers, and projects. We then evaluate if these embeddings reflect the postulated topology of the Skill Space by predicting what new APIs/projects developers use/join, and whether or not their pull requests get accepted. We also check how the developers' representations in the Skill Space align with their self-reported API expertise. Result: Our results suggest that the proposed embeddings in the Skill Space appear to satisfy the postulated topology and we hope that such representations may aid in the construction of signals that increase trust (and efficiency) of open source ecosystems at large and may aid investigations of other phenomena related to developer proficiency and learning.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {995–1007},
numpages = {13},
keywords = {World of Code, Vector Embedding, Skill Space, Project embedding, Open Source, Machine Learning, Expertise, Doc2Vec, Developer embedding, Developer Expertise, API embedding, API},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/1868688.1868699,
author = {Kulkarni, Vinay},
title = {Raising family is a good practice},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868699},
doi = {10.1145/1868688.1868699},
abstract = {The need for adaptiveness of business applications is on the rise with continued increase in business dynamics. Code-centric techniques show unacceptable responsiveness in this dynamic context as business applications are subjected to changes along multiple dimensions that continue to evolve simultaneously. Recent literature suggests the use of product line architectures to increase adaptiveness by capturing commonality and variability to suitably configure the application. Use of model driven techniques for developing business applications is argued as a preferable option because platform independent specification can be retargeted to technology platform of choice through a code generation process. Business applications can be visualized to vary along five dimensions, namely, Functionality (F), Business process (P), Design decisions (D), Architecture (A) and Technology platform (T). Use of models is largely limited to F and P dimensions in commonly used model-driven development techniques thus limiting the benefits of product line concept to these two dimensions. We argue this is not sufficient to achieve the desired adaptiveness, and it is critical to extend the product line concept to D, A and T dimensions also. To address adaptation needs of business applications, this paper presents a model-driven generative approach that further builds on the ideas of separation of concerns, variability management and feature modeling. Early experience and lessons learnt are discussed, and future work outlined.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {72–79},
numpages = {8},
keywords = {variability, product lines, product families, model-driven development, commonality, business applications, adaptiveness},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@article{10.1016/j.csi.2019.04.011,
author = {Barros-Justo, José L. and Benitti, Fabiane B.V. and Matalonga, Santiago},
title = {Trends in software reuse research: A tertiary study},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2019.04.011},
doi = {10.1016/j.csi.2019.04.011},
journal = {Comput. Stand. Interfaces},
month = oct,
numpages = {18},
keywords = {Tertiary study, Systematic literature review, Trends in software reuse, Software reuse}
}

@inproceedings{10.1007/978-3-030-48077-6_3,
author = {Clarisó, Robert and Cabot, Jordi},
title = {Diverse Scenario Exploration in Model Finders Using Graph Kernels and Clustering},
year = {2020},
isbn = {978-3-030-48076-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-48077-6_3},
doi = {10.1007/978-3-030-48077-6_3},
abstract = {Complex software systems can be described using modeling notations such as UML/OCL or Alloy. Then, some correctness properties of these systems can be checked using model finders, which compute sample scenarios either fulfilling the desired properties or illustrating potential faults. Such scenarios allow designers to validate, verify and test the system under development.Nevertheless, when asked to produce several scenarios, model finders tend to produce similar solutions. This lack of diversity impairs their effectiveness as testing or validation assets. To solve this problem, we propose the use of graph kernels, a family of methods for computing the (dis)similarity among pairs of graphs. With this metric, it is possible to cluster scenarios effectively, improving the usability of model finders and making testing and validation more efficient.},
booktitle = {Rigorous State-Based Methods: 7th International Conference, ABZ 2020, Ulm, Germany, May 27–29, 2020, Proceedings},
pages = {27–43},
numpages = {17},
keywords = {Diversity, Clustering, Graph kernels, Testing, Verification and validation, Model-driven engineering},
location = {Ulm, Germany}
}

@inproceedings{10.1145/3196494.3196511,
author = {Hendler, Danny and Kels, Shay and Rubin, Amir},
title = {Detecting Malicious PowerShell Commands using Deep Neural Networks},
year = {2018},
isbn = {9781450355766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196494.3196511},
doi = {10.1145/3196494.3196511},
abstract = {Microsoft's PowerShell is a command-line shell and scripting language that is installed by default on Windows machines. Based on Microsoft's .NET framework, it includes an interface that allows programmers to access operating system services. While PowerShell can be configured by administrators for restricting access and reducing vulnerabilities, these restrictions can be bypassed. Moreover, PowerShell commands can be easily generated dynamically, executed from memory, encoded and obfuscated, thus making the logging and forensic analysis of code executed by PowerShell challenging. For all these reasons, PowerShell is increasingly used by cybercriminals as part of their attacks' tool chain, mainly for downloading malicious contents and for lateral movement. Indeed, a recent comprehensive technical report by Symantec dedicated to PowerShell's abuse by cybercrimials [52] reported on a sharp increase in the number of malicious PowerShell samples they received and in the number of penetration tools and frameworks that use PowerShell. This highlights the urgent need of developing effective methods for detecting malicious PowerShell commands. In this work, we address this challenge by implementing several novel detectors of malicious PowerShell commands and evaluating their performance. We implemented both "traditional" natural language processing (NLP) based detectors and detectors based on character-level convolutional neural networks (CNNs). Detectors' performance was evaluated using a large real-world dataset. Our evaluation results show that, although our detectors (and especially the traditional NLP-based ones) individually yield high performance, an ensemble detector that combines an NLP-based classifier with a CNN-based classifier provides the best performance, since the latter classifier is able to detect malicious commands that succeed in evading the former. Our analysis of these evasive commands reveals that some obfuscation patterns automatically detected by the CNN classifier are intrinsically difficult to detect using the NLP techniques we applied. Our detectors provide high recall values while maintaining a very low false positive rate, making us cautiously optimistic that they can be of practical value.},
booktitle = {Proceedings of the 2018 on Asia Conference on Computer and Communications Security},
pages = {187–197},
numpages = {11},
keywords = {powershell, neural networks, natural language processing, malware detection, deep learning},
location = {Incheon, Republic of Korea},
series = {ASIACCS '18}
}

@inproceedings{10.1145/3425898.3426959,
author = {Bordis, Tabea and Runge, Tobias and Schaefer, Ina},
title = {Correctness-by-construction for feature-oriented software product lines},
year = {2020},
isbn = {9781450381741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425898.3426959},
doi = {10.1145/3425898.3426959},
abstract = {Software product lines are increasingly used to handle the growing demand of custom-tailored software variants. They provide systematic reuse of software paired with variability mechanisms in the code to implement whole product families rather than single software products. A common domain of application for product lines are safety-critical systems, which require behavioral correctness to avoid dangerous situations in-field. While most approaches concentrate on post-hoc verification for product lines, we argue that a stepwise approach to create correct programs may be beneficial for developers to manage the growing variability. Correctness-by-construction is such a stepwise approach to create programs using a set of small, tractable refinement rules that guarantee the correctness of the program with regard to its specification. In this paper, we propose the first approach to develop correct-by-construction software product lines using feature-oriented programming. First, we extend correctness-by-construction by two refinement rules for variation points in the code. Second, we give a proof for the soundness of the proposed rules. Third, we implement our technique in a tool called VarCorC and show the applicability of the tool by conducting two case studies.},
booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {22–34},
numpages = {13},
keywords = {software product lines, formal verification, feature-oriented programming, correctness-by-construction},
location = {Virtual, USA},
series = {GPCE 2020}
}

@inproceedings{10.5555/1964571.1964603,
author = {Hubaux, Arnaud and Boucher, Quentin and Hartmann, Herman and Michel, Raphaël and Heymans, Patrick},
title = {Evaluating a textual feature modelling language: four industrial case studies},
year = {2010},
isbn = {9783642194399},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature models are commonly used in software product line engineering as a means to document variability. Since their introduction, feature models have been extended and formalised in various ways. The majority of these extensions are variants of the original tree-based graphical notation. But over time, textual dialects have also been proposed. The textual variability language (TVL) was proposed to combine the advantages of both graphical and textual notations. However, its benefits and limitations have not been empirically evaluated up to now. In this paper, we evaluate TVL with four cases from companies of different sizes and application domains. The study shows that practitioners can benefit from TVL. The participants appreciated the notation, the advantages of a textual language and considered the learning curve to be gentle. The study also reveals some limitations of the current version of TVL.},
booktitle = {Proceedings of the Third International Conference on Software Language Engineering},
pages = {337–356},
numpages = {20},
location = {Eindhoven, The Netherlands},
series = {SLE'10}
}

@inproceedings{10.5555/1885639.1885679,
author = {Yoshimura, Kentaro and Atarashi, Yoshitaka and Fukuda, Takeshi},
title = {A method to identify feature constraints based on feature selections mining},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we describe a novel method to identify constraints among features in a software product line, based on feature selections made in the past. Our approach takes feature selections of derived products as the input and extracts association rules between features such as "Products that selected feature i also selected feature j." We evaluated our method by applying it to a product line at Hitachi and identified 21 new constraints among 123 optional features.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {425–429},
numpages = {5},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1007/s10664-021-10026-0,
author = {Silva, Camila Costa and Galster, Matthias and Gilson, Fabian},
title = {Topic modeling in software engineering research},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10026-0},
doi = {10.1007/s10664-021-10026-0},
abstract = {Topic modeling using models such as Latent Dirichlet Allocation (LDA) is a text mining technique to extract human-readable semantic “topics” (i.e., word clusters) from a corpus of textual documents. In software engineering, topic modeling has been used to analyze textual data in empirical studies (e.g., to find out what developers talk about online), but also to build new techniques to support software engineering tasks (e.g., to support source code comprehension). Topic modeling needs to be applied carefully (e.g., depending on the type of textual data analyzed and modeling parameters). Our study aims at describing how topic modeling has been applied in software engineering research with a focus on four aspects: (1) which topic models and modeling techniques have been applied, (2) which textual inputs have been used for topic modeling, (3) how textual data was “prepared” (i.e., pre-processed) for topic modeling, and (4) how generated topics (i.e., word clusters) were named to give them a human-understandable meaning. We analyzed topic modeling as applied in 111 papers from ten highly-ranked software engineering venues (five journals and five conferences) published between 2009 and 2020. We found that (1) LDA and LDA-based techniques are the most frequent topic modeling techniques, (2) developer communication and bug reports have been modelled most, (3) data pre-processing and modeling parameters vary quite a bit and are often vaguely reported, and (4) manual topic naming (such as deducting names based on frequent words in a topic) is common.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {62},
keywords = {Literature analysis, Natural language processing, Text mining, Topic modeling}
}

@inproceedings{10.5555/3305890.3305916,
author = {Ma, Fan and Meng, Deyu and Xie, Qi and Li, Zina and Dong, Xuanyi},
title = {Self-paced co-training},
year = {2017},
publisher = {JMLR.org},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two different views and exchanges labels of unlabeled instances in an iterative way. During co-training process, labels of unlabeled instances in the training pool are very likely to be false especially in the initial training rounds, while the standard co-training algorithm utilizes a "draw without replacement" manner and does not remove these false labeled instances from training. This issue not only tends to degenerate its performance but also hampers its fundamental theory. Besides, there is no optimization model to explain what objective a co-training process optimizes. To these issues, in this study we design a new co-training algorithm named self-paced co-training (SPaCo) with a "draw with replacement" learning mode. The rationality of SPaCo can be proved under theoretical assumptions utilized in traditional co-training research, and furthermore, the algorithm exactly complies with the alternative optimization process for an optimization model of self-paced curriculum learning, which can be finely explained in robust learning manner. Experimental results substantiate the superiority of the proposed method as compared with current state-of-the-art co-training methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2275–2284},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{10.1016/j.ins.2019.12.046,
author = {Chen, Dongzi and Yang, Qinli and Liu, Jiaming and Zeng, Zhu},
title = {Selective prototype-based learning on concept-drifting data streams},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {516},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.12.046},
doi = {10.1016/j.ins.2019.12.046},
journal = {Inf. Sci.},
month = apr,
pages = {20–32},
numpages = {13},
keywords = {99-00, 00-01, Prototype, Classification, Concept drift, Data stream}
}

@inproceedings{10.1007/978-3-642-34026-0_34,
author = {Kitamura, Takashi and Do, Ngoc Thi Bich and Ohsaki, Hitoshi and Fang, Ling and Yatabe, Shunsuke},
title = {Test-Case design by feature trees},
year = {2012},
isbn = {9783642340253},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34026-0_34},
doi = {10.1007/978-3-642-34026-0_34},
abstract = {This paper proposes a test-case design method for black-box testing, called "Feature Oriented Testing (FOT)". The method is realized by applying Feature Models (FMs) developed in software product line engineering to test-case designs. We develop a graphical language for test-case design called "Feature Trees for Testing (FTT)" based on FMs. To firmly underpin the method, we provide a formal semantics of FTT, by means of test-cases derived from test-case designs modelled with FTT. Based on the semantics we develop an automated test-suite generation and correctness checking of test-case designs using SAT, as computer-aided analysis techniques of the method. Feasibility of the method is demonstrated from several viewpoints including its implementation, complexity analysis, experiments, a case study, and an assistant tool.},
booktitle = {Proceedings of the 5th International Conference on Leveraging Applications of Formal Methods, Verification and Validation: Technologies for Mastering Change - Volume Part I},
pages = {458–473},
numpages = {16},
keywords = {combination testing, black-box testing, SAT-based analysis},
location = {Heraklion, Crete, Greece},
series = {ISoLA'12}
}

@inproceedings{10.5555/1887899.1887907,
author = {Lung, Chung-Horng and Balasubramaniam, Balasangar and Selvarajah, Kamalachelva and Elankeswaran, Poopalasinkam and Gopalasundaram, Umatharan},
title = {Towards architecture-centric software generation},
year = {2010},
isbn = {3642151132},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Architecture-centric software generation has the potential to support flexible design and large-scale reuse. This paper describes the development of an architecture-centric framework that consists of multiple architecture alternatives, from which the architect can select and generate a working prototype in a top-down manner through a user interface rather than building it from scratch. The framework is primarily built with well-understood design patterns in distributed and concurrent computing. The development process involves extensive domain analysis, variability management, and bottom-up component engineering effort. The framework enables the architect or designer to effectively conduct upfront software architecture analysis and/or rapid architectural prototyping.},
booktitle = {Proceedings of the 4th European Conference on Software Architecture},
pages = {38–52},
numpages = {15},
keywords = {variability management, patterns, generative technique, domain analysis, concurrency, architecture-centric development},
location = {Copenhagen, Denmark},
series = {ECSA'10}
}

@inproceedings{10.1145/3469096.3469872,
author = {Yang, Eugene and Lewis, David D. and Frieder, Ophir},
title = {On minimizing cost in legal document review workflows},
year = {2021},
isbn = {9781450385961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469096.3469872},
doi = {10.1145/3469096.3469872},
abstract = {Technology-assisted review (TAR) refers to human-in-the-loop machine learning workflows for document review in legal discovery and other high recall review tasks. Attorneys and legal technologists have debated whether review should be a single iterative process (one-phase TAR workflows) or whether model training and review should be separate (two-phase TAR workflows), with implications for the choice of active learning algorithm. The relative cost of manual labeling for different purposes (training vs. review) and of different documents (positive vs. negative examples) is a key and neglected factor in this debate. Using a novel cost dynamics analysis, we show analytically and empirically that these relative costs strongly impact whether a one-phase or two-phase workflow minimizes cost. We also show how category prevalence, classification task difficulty, and collection size impact the optimal choice not only of workflow type, but of active learning method and stopping point.},
booktitle = {Proceedings of the 21st ACM Symposium on Document Engineering},
articleno = {30},
numpages = {10},
keywords = {total recall, high-recall retrieval, cost modeling, active learning},
location = {Limerick, Ireland},
series = {DocEng '21}
}

@inproceedings{10.1145/236337.236386,
author = {Bonar, Jeffrey and Lehrer, Nancy and Looney, Kelly and Schnier, Randy and Russel, James and Selker, Ted and Nickolas, Stewart},
title = {Components on the Internet (panel)},
year = {1996},
isbn = {089791788X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/236337.236386},
doi = {10.1145/236337.236386},
abstract = {The explosive emergence of the Internet forces us to rethink the traditional roles for client, server, and objects. Component technology has emerged as the way objects and frameworks are packaged for flexible object integration. In particular, components allow users who are not programmers to do their own integration. Up until now, components have mainly focused on object integration on end-users desktops and on a client: compound documents, application assembly, etc.Existing component models like OpenDoc and Microsoft's OLE/OCX/COM provide function for compound documents, persistence, scripting, inking, and code management (registration). A series of Internet scenarios are emerging that demand new kinds of components. These scenarios are thin clients (Internet terminals), virtual reality/rich multimedia clients, collaboration, and business data access. Each panelist will explore one of these scenarios for it's requirements on an Internet component model.},
booktitle = {Proceedings of the 11th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {444–448},
numpages = {5},
location = {San Jose, California, USA},
series = {OOPSLA '96}
}

@inproceedings{10.1145/3180465.3180469,
author = {Nadig, Deepak and Ramamurthy, Byrav and Bockelman, Brian and Swanson, David},
title = {Identifying Anomalies in GridFTP transfers for Data-Intensive Science through Application-Awareness},
year = {2018},
isbn = {9781450356350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180465.3180469},
doi = {10.1145/3180465.3180469},
abstract = {Network anomaly detection systems can be used to identify anomalous transfers or threats, which, when undetected, can trigger large-scale malicious events. Data-intensive science projects rely on high-throughput computing and high-speed networking resources for data analysis and processing. In this paper, we propose an anomaly detection framework and architecture for identifying anomalies in GridFTP transfers. Application-awareness plays an important role in our proposed architecture and is used to communicate GridFTP application metadata to the machine learning and anomaly detection system. We demonstrate the effectiveness of our architecture by evaluating the framework with a real-world, large-scale dataset of GridFTP transfers. Preliminary results show that our framework can be used to develop novel anomaly detection services with diverse feature sets for distributed and data-intensive projects.},
booktitle = {Proceedings of the 2018 ACM International Workshop on Security in Software Defined Networks &amp; Network Function Virtualization},
pages = {7–12},
numpages = {6},
keywords = {software defined networks., gridftp, application-awareness, anomaly detection},
location = {Tempe, AZ, USA},
series = {SDN-NFV Sec'18}
}

@article{10.1016/j.ijinfomgt.2018.10.006,
author = {Fernandes, Marta and Canito, Alda and Bolón-Canedo, Verónica and Conceição, Luís and Praça, Isabel and Marreiros, Goreti},
title = {Data analysis and feature selection for predictive maintenance: A case-study in the metallurgic industry},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2018.10.006},
doi = {10.1016/j.ijinfomgt.2018.10.006},
journal = {Int. J. Inf. Manag.},
month = jun,
pages = {252–262},
numpages = {11},
keywords = {Rule-based model, Feature selection, Data analysis, Predictive maintenance}
}

@article{10.1007/s00180-012-0381-6,
author = {Schmidt, Miriam and Palm, Günther and Schwenker, Friedhelm},
title = {Spectral graph features for the classification of graphs and graph sequences},
year = {2014},
issue_date = {February  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1–2},
issn = {0943-4062},
url = {https://doi.org/10.1007/s00180-012-0381-6},
doi = {10.1007/s00180-012-0381-6},
abstract = {In this paper, the classification power of the eigenvalues of six graph-associated matrices is investigated. Each matrix contains a certain type of geometric/ spatial information, which may be important for the classification process. The performances of the different feature types is evaluated on two data sets: first a benchmark data set for optical character recognition, where the extracted eigenvalues were utilized as feature vectors for multi-class classification using support vector machines. Classification results are presented for all six feature types, as well as for classifier combinations at decision level. For the decision level combination, probabilistic output support vector machines have been applied, with a performance up to 92.4 %. To investigate the power of the spectra for time dependent tasks, too, a second data set was investigated, consisting of human activities in video streams. To model the time dependency, hidden Markov models were utilized and the classification rate reached 98.3 %.},
journal = {Comput. Stat.},
month = feb,
pages = {65–80},
numpages = {16},
keywords = {Spectrum, Optical character recognition, Human activity recognition, Graph-associated matrices, Graph classification}
}

@inproceedings{10.1145/2970276.2970336,
author = {Li, Yi and Zhu, Chenguang and Rubin, Julia and Chechik, Marsha},
title = {Precise semantic history slicing through dynamic delta refinement},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970336},
doi = {10.1145/2970276.2970336},
abstract = {Semantic history slicing solves the problem of extracting changes related to a particular high-level functionality from the software version histories. State-of-the-art techniques combine static program analysis and dynamic execution tracing to infer an over-approximated set of changes that can preserve the functional behaviors captured by a test suite. However, due to the conservative nature of such techniques, the sliced histories may contain irrelevant changes. In this paper, we propose a divide-and-conquer-style partitioning approach enhanced by dynamic delta refinement to produce minimal semantic history slices. We utilize deltas in dynamic invariants generated from successive test executions to learn significance of changes with respect to the target functionality. Empirical results indicate that these measurements accurately rank changes according to their relevance to the desired test behaviors and thus partition history slices in an efficient and effective manner.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {495–506},
numpages = {12},
keywords = {software configuration management, program analysis, dynamic invariants, Semantic history slicing},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/1842752.1842815,
author = {Galster, Matthias},
title = {Describing variability in service-oriented software product lines},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842815},
doi = {10.1145/1842752.1842815},
abstract = {Service-oriented architectures are a standard-based and technology-independent distributed computing paradigm for discovering, binding and assembling loosely-coupled software services. Software product lines on the other hand allow a generic architecture to be configured and deployed in different instances. Product lines facilitate systematic reuse through managing variability. In this paper, we combine ideas from the service domain and the product line domain and investigate what types of variability exist in service-oriented software architectures. Moreover, we suggest a way for representing variability in service-oriented architectures by formalizing the notion of variability. To allow different viewpoints on variability, we define stakeholder roles that occur in the context of service-oriented software architectures. By applying the proposed concepts, we hope to improve variability management at the software architecture level of service-oriented systems.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {344–350},
numpages = {7},
keywords = {variability, service-oriented architectures, modeling},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@article{10.5555/2882638.2882640,
author = {Guo, Liang},
title = {Consumption Flexibility, Product Configuration, and Market Competition},
year = {2006},
issue_date = {March 2006},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {25},
number = {2},
issn = {1526-548X},
abstract = {When purchase and consumption decisions are separated in time and when future utility is state dependent, consumers may desire to pursue consumption flexibility by purchasing different products together multiple buying. This paper analyzes the effects of consumption flexibility on competing firms' marketing mix decisions, in a model in which future preference uncertainty exists and consumers differ in their preferred product location on a horizontal attribute. The analysis shows that the nature of price competition in such markets is dependent upon whether consumer multiple buying and thus primary demand is endogenously induced. When preference uncertainty is important, the firms are involved in a “flexibility trap” in which primary demand is expanded but profits decrease with the spread of consumer heterogeneity. This counter-intuitive result is caused by the firms being induced to over-cut prices to increase primary demand when consumption flexibility is important. In response to this, the firms may configure their products to alleviate the adverse effect of consumer heterogeneity. For example, if preference uncertainty is important, the firms may choose to minimize differentiation on the horizontal attribute, or extend the current product line, to deal with the “flexibility trap.” The implications of allowing for positive salvage value, uncertainty heterogeneity, preference correlation, and state-dependent preference configuration are also investigated.},
journal = {Marketing Science},
month = mar,
pages = {116–130},
numpages = {15},
keywords = {product line extension, price competition, preference uncertainty, positioning, market expansion, horizontal differentiation, consumption flexibility}
}

@article{10.1016/j.infsof.2010.03.014,
author = {Alves, Vander and Niu, Nan and Alves, Carina and Valença, George},
title = {Requirements engineering for software product lines: A systematic literature review},
year = {2010},
issue_date = {August, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.03.014},
doi = {10.1016/j.infsof.2010.03.014},
abstract = {Context: Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed. Objective: This paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement. Method: A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009. Results: The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy. Conclusions: Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {806–820},
numpages = {15},
keywords = {Systematic literature review, Software product lines, Requirements engineering}
}

@article{10.1016/j.dss.2007.01.001,
author = {Jiao, Jianxin (Roger) and Zhang, Lianfeng (Linda) and Pokharel, Shaligram and He, Zhen},
title = {Identifying generic routings for product families based on text mining and tree matching},
year = {2007},
issue_date = {April, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {43},
number = {3},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2007.01.001},
doi = {10.1016/j.dss.2007.01.001},
abstract = {Product customization leads to an exponentially increased number of product and process variants, which exaggerates the difficulties in building up customization capabilities for make-to-order production systems. It is imperative for companies to configure existing operations routings by exploiting similarities among product and process families so as to take advantage of repetitions. Corresponding to a product family, a process family comprises a set of similar production processes that share certain common operations routings (namely, generic routings). In addition to leveraging the costs of delivering variety, exploiting process families around generic routings can reduce development risks by reusing existing facilities and proven process elements. This paper applies data mining techniques to identify generic routings from large amount of production information and process data available in a firm's legacy systems. Generic routing identification encompasses three consecutive stages, including routing similarity measure, routing clustering and routing unification. Text mining and tree matching techniques are applied to cope with the textual and structural types of data underlying generic routings. A case study of mass customization production of vibration motors for mobile phones is reported to illustrate the feasibility and potential of generic routing identification.},
journal = {Decis. Support Syst.},
month = apr,
pages = {866–883},
numpages = {18},
keywords = {Tree matching, Text mining, Product variety, Product family, Process configuration, Operations routing, Mass customization, Generic representation, Data mining}
}

@article{10.1007/s11192-019-03307-5,
author = {Tattershall, E. and Nenadic, G. and Stevens, R. D.},
title = {Detecting bursty terms in computer science research},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {122},
number = {1},
issn = {0138-9130},
url = {https://doi.org/10.1007/s11192-019-03307-5},
doi = {10.1007/s11192-019-03307-5},
abstract = {Research topics rise and fall in popularity over time, some more swiftly than others. The fastest rising topics are typically called bursts; for example “deep learning”, “internet of things” and “big data”. Being able to automatically detect and track bursty terms in the literature could give insight into how scientific thought evolves over time. In this paper, we take a trend detection algorithm from stock market analysis and apply it to over 30&nbsp;years of computer science research abstracts, treating the prevalence of each term in the dataset like the price of a stock. Unlike previous work in this domain, we use the free text of abstracts and titles, resulting in a finer-grained analysis. We report a list of bursty terms, and then use historical data to build a classifier to predict whether they will rise or fall in popularity in the future, obtaining accuracy in the region of 80%. The proposed methodology can be applied to any time-ordered collection of text to yield past and present bursty terms and predict their probable fate.},
journal = {Scientometrics},
month = jan,
pages = {681–699},
numpages = {19},
keywords = {MACD, DBLP, Machine learning, Term life cycles, Bibliometrics, Computer science}
}

@article{10.1016/j.eswa.2019.03.031,
author = {Ros, Frédéric and Guillaume, Serge},
title = {A hierarchical clustering algorithm and an improvement of the single linkage criterion to deal with noise},
year = {2019},
issue_date = {Aug 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.03.031},
doi = {10.1016/j.eswa.2019.03.031},
journal = {Expert Syst. Appl.},
month = aug,
pages = {96–108},
numpages = {13},
keywords = {Density, Dissimilarity, Agglomerative}
}

@inproceedings{10.1145/1017753.1017787,
author = {Hardung, Bernd and Kölzow, Thorsten and Krüger, Andreas},
title = {Reuse of software in distributed embedded automotive systems},
year = {2004},
isbn = {1581138601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1017753.1017787},
doi = {10.1145/1017753.1017787},
abstract = {Until recently, in the automotive industry, reuse of software has entirely been a typical activity of suppliers. They try to reduce the increasing software development costs that stem from rising complexity and size of software in the modern automobile. Lately, also the automotive manufacturers began to develop specific software with competitive relevance. Now they have to deal with the problem of reuse, too. Nevertheless, there is a difference between the manufacturers' and the suppliers' point of view because the manufacturers have to integrate the networked hardware components to one automotive system. Therefore, the manufacturers have to deal with additional problems compared to the supplier. At the beginning of this paper, the specific problems of reuse of software in the automotive domain are shown from the perspective of automotive manufacturers. After that, a framework is proposed to deal with these problems. Moreover, the application of this framework is shown in a realistic application example.},
booktitle = {Proceedings of the 4th ACM International Conference on Embedded Software},
pages = {203–210},
numpages = {8},
keywords = {software, reuse, product line, automotive, architecture},
location = {Pisa, Italy},
series = {EMSOFT '04}
}

@article{10.1016/j.ins.2021.06.013,
author = {Yang, Guoli and Kang, Yuanji and Zhu, Xianqiang and Zhu, Cheng and Xiao, Gaoxi},
title = {Info2vec: An aggregative representation method in multi-layer and heterogeneous networks},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {574},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.06.013},
doi = {10.1016/j.ins.2021.06.013},
journal = {Inf. Sci.},
month = oct,
pages = {444–460},
numpages = {17},
keywords = {Cyberspace, Representation learning, Multi-layer networks}
}

@inproceedings{10.5555/3495724.3496497,
author = {Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {Self-paced deep reinforcement learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Curriculum reinforcement learning (CRL) improves the learning speed and stability of an agent by exposing it to a tailored series of tasks throughout learning. Despite empirical successes, an open question in CRL is how to automatically generate a curriculum for a given reinforcement learning (RL) agent, avoiding manual design. In this paper, we propose an answer by interpreting the curriculum generation as an inference problem, where distributions over tasks are progressively learned to approach the target task. This approach leads to an automatic curriculum generation, whose pace is controlled by the agent, with solid theoretical motivation and easily integrated with deep RL algorithms. In the conducted experiments, the curricula generated with the proposed algorithm significantly improve learning performance across several environments and deep RL algorithms, matching or outperforming state-of-the-art existing CRL algorithms.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {773},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1016/j.infsof.2011.09.003,
author = {Conejero, José M. and Figueiredo, Eduardo and Garcia, Alessandro and Hernández, Juan and Jurado, Elena},
title = {On the relationship of concern metrics and requirements maintainability},
year = {2012},
issue_date = {February, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.09.003},
doi = {10.1016/j.infsof.2011.09.003},
abstract = {Context: Maintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability. Objective: In this setting, this paper introduces an empirical analysis where the correlation between crosscutting properties and two ISO/IEC 9126 maintainability attributes, namely changeability and stability, is presented. Method: This correlation is based on the utilization of a set of concern metrics that allows the quantification of crosscutting, scattering and tangling. Results: Our study confirms that a change in a crosscutting concern is more difficult to be accomplished and that artefacts addressing crosscutting concerns are found to be less stable later as the system evolves. Moreover, our empirical analysis reveals that crosscutting properties introduce non-syntactic dependencies between software artefacts, thereby decreasing the quality of software in terms of changeability and stability as well. These subtle dependencies cannot be easily detected without the use of concern metrics. Conclusion: The correlation provides evidence that the presence of certain crosscutting properties negatively affects to changeability and stability. The whole analysis is performed using as target cases three software product lines, where maintainability properties are of upmost importance not only for individual products but also for the core architecture of the product line.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {212–238},
numpages = {27},
keywords = {Stability, Requirements engineering, Product lines, Maintainability, Crosscutting, Concern metrics}
}

@article{10.1016/j.infsof.2011.11.009,
author = {Angelov, Samuil and Grefen, Paul and Greefhorst, Danny},
title = {A framework for analysis and design of software reference architectures},
year = {2012},
issue_date = {April, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.11.009},
doi = {10.1016/j.infsof.2011.11.009},
abstract = {Context: A software reference architecture is a generic architecture for a class of systems that is used as a foundation for the design of concrete architectures from this class. The generic nature of reference architectures leads to a less defined architecture design and application contexts, which makes the architecture goal definition and architecture design non-trivial steps, rooted in uncertainty. Objective: The paper presents a structured and comprehensive study on the congruence between context, goals, and design of software reference architectures. It proposes a tool for the design of congruent reference architectures and for the analysis of the level of congruence of existing reference architectures. Method: We define a framework for congruent reference architectures. The framework is based on state of the art results from literature and practice. We validate our framework and its quality as analytical tool by applying it for the analysis of 24 reference architectures. The conclusions from our analysis are compared to the opinions of experts on these reference architectures documented in literature and dedicated communication. Results: Our framework consists of a multi-dimensional classification space and of five types of reference architectures that are formed by combining specific values from the multi-dimensional classification space. Reference architectures that can be classified in one of these types have better chances to become a success. The validation of our framework confirms its quality as a tool for the analysis of the congruence of software reference architectures. Conclusion: This paper facilitates software architects and scientists in the inception, design, and application of congruent software reference architectures. The application of the tool improves the chance for success of a reference architecture.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {417–431},
numpages = {15},
keywords = {Software reference architecture, Software product line architecture, Software domain architecture, Software architecture design}
}

@article{10.1504/IJKESDP.2013.052716,
author = {Elfaki, Abdelrahman Osman and Fong, Sim Liew and Aik, Kevin Loo Teow and Johar, Md Gapar Md},
title = {Towards detecting redundancy in domain engineering process using first order logic rules},
year = {2013},
issue_date = {March 2013},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {4},
number = {1},
issn = {1755-3210},
url = {https://doi.org/10.1504/IJKESDP.2013.052716},
doi = {10.1504/IJKESDP.2013.052716},
abstract = {Software product line SPL is an emerging methodology for developing software products. SPL consists of two processes: domain-engineering and application-engineering. Successful software product is highly dependent on the validity of a domain engineering process. Therefore, validation is a significant process within the domain-engineering. Anomalies such as dead feature, redundancy, and wrong-cardinality are well-known problems in SPL. In the literature, redundancy did not take the signs of attentions as a dead feature and wrong-cardinality. The maturity of the SPL can be enhanced by detecting and removing the redundancy from the domain engineering. This paper proposes first order logic FOL rules for detecting the redundancy in domain-engineering process. Detecting redundancy in the domain engineering direct is our contribution. Our methodology comprised of three steps: 1 variability is modelled in the form of predicates as a prerequisite; 2 for each type of the redundancy, a general form is formulated to swathe all possible cases; 3 FOL rules are illustrated to implement each possibility based on deducing the results from predefined cases. As a result, all forms of redundancies in the domain-engineering process are amorphous. Finally, experiments are conducted to attest the scalability of our method.},
journal = {Int. J. Knowl. Eng. Soft Data Paradigm.},
month = mar,
pages = {1–20},
numpages = {20}
}

@article{10.1007/s10664-012-9208-x,
author = {Feigenspan, Janet and Kästner, Christian and Apel, Sven and Liebig, Jörg and Schulze, Michael and Dachselt, Raimund and Papendieck, Maria and Leich, Thomas and Saake, Gunter},
title = {Do background colors improve program comprehension in the #ifdef hell?},
year = {2013},
issue_date = {August    2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-012-9208-x},
doi = {10.1007/s10664-012-9208-x},
abstract = {Software-product-line engineering aims at the development of variable and reusable software systems. In practice, software product lines are often implemented with preprocessors. Preprocessor directives are easy to use, and many mature tools are available for practitioners. However, preprocessor directives have been heavily criticized in academia and even referred to as "#ifdef hell", because they introduce threats to program comprehension and correctness. There are many voices that suggest to use other implementation techniques instead, but these voices ignore the fact that a transition from preprocessors to other languages and tools is tedious, erroneous, and expensive in practice. Instead, we and others propose to increase the readability of preprocessor directives by using background colors to highlight source code annotated with ifdef directives. In three controlled experiments with over 70 subjects in total, we evaluate whether and how background colors improve program comprehension in preprocessor-based implementations. Our results demonstrate that background colors have the potential to improve program comprehension, independently of size and programming language of the underlying product. Additionally, we found that subjects generally favor background colors. We integrate these and other findings in a tool called FeatureCommander, which facilitates program comprehension in practice and which can serve as a basis for further research.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {699–745},
numpages = {47},
keywords = {Software visualization, Software product lines, Program comprehension, Preprocessors, FeatureCommander, Empirical software engineering}
}

@inproceedings{10.1109/ICSE43902.2021.00060,
author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {A Context-based Automated Approach for Method Name Consistency Checking and Suggestion},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00060},
doi = {10.1109/ICSE43902.2021.00060},
abstract = {Misleading method names in software projects can confuse developers, which may lead to software defects and affect code understandability. In this paper, we present DeepName, a context-based, deep learning approach to detect method name inconsistencies and suggest a proper name for a method. The key departure point is the philosophy of "Show Me Your Friends, I'll Tell You Who You Are". Unlike the state-of-the-art approaches, in addition to the method's body, we also consider the interactions of the current method under study with the other ones including the caller and callee methods, and the sibling methods in the same enclosing class. The sequences of sub-tokens in the program entities' names in the contexts are extracted and used as the input for an RNN-based encoder-decoder to produce the representations for the current method. We modify that RNN model to integrate the copy mechanism and our newly developed component, called the non-copy mechanism, to emphasize on the possibility of a certain sub-token not to be copied to follow the current sub-token in the currently generated method name.We conducted several experiments to evaluate DEEPNAME on large datasets with +14M methods. For consistency checking, DeepName improves the state-of-the-art approach by 2.1%, 19.6%, and 11.9% relatively in recall, precision, and F-score, respectively. For name suggestion, DeepName improves relatively over the state-of-the-art approaches in precision (1.8%-30.5%), recall (8.8%-46.1%), and F-score (5.2%-38.2%). To assess DEEPNAME's usefulness, we detected inconsistent methods and suggested new method names in active projects. Among 50 pull requests, 12 were merged into the main branch. In total, in 30/50 cases, the team members agree that our suggested method names are more meaningful than the current names.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {574–586},
numpages = {13},
keywords = {Naturalness of Software, Inconsistent Method Name Checking, Entity Name Suggestion, Deep Learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1155/2019/2384706,
author = {Yang, Xingguang and Yu, Huiqun and Fan, Guisheng and Shi, Kai and Chen, Liqiong and Tramontana, Emiliano},
title = {Local versus Global Models for Just-In-Time Software Defect Prediction},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1058-9244},
url = {https://doi.org/10.1155/2019/2384706},
doi = {10.1155/2019/2384706},
abstract = {Just-in-time software defect prediction (JIT-SDP) is an active topic in software defect prediction, which aims to identify defect-inducing changes. Recently, some studies have found that the variability of defect data sets can affect the performance of defect predictors. By using local models, it can help improve the performance of prediction models. However, previous studies have focused on module-level defect prediction. Whether local models are still valid in the context of JIT-SDP is an important issue. To this end, we compare the performance of local and global models through a large-scale empirical study based on six open-source projects with 227417 changes. The experiment considers three evaluation scenarios of cross-validation, cross-project-validation, and timewise-cross-validation. To build local models, the experiment uses the k-medoids to divide the training set into several homogeneous regions. In addition, logistic regression and effort-aware linear regression (EALR) are used to build classification models and effort-aware prediction models, respectively. The empirical results show that local models perform worse than global models in the classification performance. However, local models have significantly better effort-aware prediction performance than global models in the cross-validation and cross-project-validation scenarios. Particularly, when the number of clusters k is set to 2, local models can obtain optimal effort-aware prediction performance. Therefore, local models are promising for effort-aware JIT-SDP.},
journal = {Sci. Program.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/3368089.3409727,
author = {Chen, Qingrong and Wang, Teng and Legunsen, Owolabi and Li, Shanshan and Xu, Tianyin},
title = {Understanding and discovering software configuration dependencies in cloud and datacenter systems},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409727},
doi = {10.1145/3368089.3409727},
abstract = {A large percentage of real-world software configuration issues, such as misconfigurations, involve multiple interdependent configuration parameters. However, existing techniques and tools either do not consider dependencies among configuration parameters— termed configuration dependencies—or rely on one or two dependency types and code patterns as input. Without rigorous understanding of configuration dependencies, it is hard to deal with many resulting configuration issues.  This paper presents our study of software configuration dependencies in 16 widely-used cloud and datacenter systems, including dependencies within and across software components. To understand types of configuration dependencies, we conduct an exhaustive search of descriptions in structured configuration metadata and unstructured user manuals. We find and manually analyze 521 configuration dependencies. We define five types of configuration dependencies and identify their common code patterns. We report on consequences of not satisfying these dependencies and current software engineering practices for handling the consequences.  We mechanize the knowledge gained from our study in a tool, cDep, which detects configuration dependencies. cDep automatically discovers five types of configuration dependencies from bytecode using static program analysis. We apply cDep to the eight Java and Scala software systems in our study. cDep finds 87.9% (275/313) of the related subset of dependencies from our study. cDep also finds 448 previously undocumented dependencies, with a 6.0% average false positive rate. Overall, our results show that configuration dependencies are more prevalent and diverse than previously reported and should henceforth be considered a first-class issue in software configuration engineering.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {362–374},
numpages = {13},
keywords = {dependency, datacenter systems, cloud systems, Configuration},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1016/j.knosys.2013.01.018,
author = {FernáNdez, Alberto and LóPez, Victoria and Galar, Mikel and Del Jesus, MaríA José and Herrera, Francisco},
title = {Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {42},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2013.01.018},
doi = {10.1016/j.knosys.2013.01.018},
abstract = {The imbalanced class problem is related to the real-world application of classification in engineering. It is characterised by a very different distribution of examples among the classes. The condition of multiple imbalanced classes is more restrictive when the aim of the final system is to obtain the most accurate precision for each of the concepts of the problem. The goal of this work is to provide a thorough experimental analysis that will allow us to determine the behaviour of the different approaches proposed in the specialised literature. First, we will make use of binarization schemes, i.e., one versus one and one versus all, in order to apply the standard approaches to solving binary class imbalanced problems. Second, we will apply several ad hoc procedures which have been designed for the scenario of imbalanced data-sets with multiple classes. This experimental study will include several well-known algorithms from the literature such as decision trees, support vector machines and instance-based learning, with the intention of obtaining global conclusions from different classification paradigms. The extracted findings will be supported by a statistical comparative analysis using more than 20 data-sets from the KEEL repository.},
journal = {Know.-Based Syst.},
month = apr,
pages = {97–110},
numpages = {14},
keywords = {Preprocessing, Pairwise learning, Multi-classification, Imbalanced data-sets, Cost-sensitive learning}
}

@inproceedings{10.1007/978-3-030-64694-3_9,
author = {Hammad, Muhammad and Babur, Önder and Abdul Basit, Hamid and Brand, Mark van den},
title = {DeepClone: Modeling Clones to Generate Code Predictions},
year = {2020},
isbn = {978-3-030-64693-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64694-3_9},
doi = {10.1007/978-3-030-64694-3_9},
abstract = {Programmers often reuse code from source code repositories to reduce the development effort. Code clones are candidates for reuse in exploratory or rapid development, as they represent often repeated functionality in software systems. To facilitate code clone reuse, we propose DeepClone, a novel approach utilizing a deep learning algorithm for modeling code clones to predict the next set of tokens (possibly a complete clone method body) based on the code written so far. The predicted tokens require minimal customization to fit the context. DeepClone applies natural language processing techniques to learn from a large code corpus, and generates code tokens using the model learned. We have quantitatively evaluated our solution to assess (1) our model’s quality and its accuracy in&nbsp;token prediction, and (2) its performance and effectiveness in clone method prediction. We also discuss various application scenarios for our approach.},
booktitle = {Reuse in Emerging Software Engineering Practices: 19th International Conference on Software and Systems Reuse, ICSR 2020, Hammamet, Tunisia, December 2–4, 2020, Proceedings},
pages = {135–151},
numpages = {17},
keywords = {Code prediction, Code clone, Deep learning, Language modeling},
location = {Hammamet, Tunisia}
}

@inproceedings{10.1145/1385486.1385495,
author = {Sunkle, Sagar and Kuhlemann, Martin and Siegmund, Norbert and Rosenmüller, Marko and Saake, Gunter},
title = {Generating highly customizable SQL parsers},
year = {2008},
isbn = {9781595939647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1385486.1385495},
doi = {10.1145/1385486.1385495},
abstract = {Database technology and the Structured Query Language (SQL) have grown enormously in recent years. Applications from different domains have different requirements for using database technology and SQL. The major problem of current standards of SQL is complexity and unmanageability. In this paper we present an approach based on software product line engineering which can be used to create customizable SQL parsers and consequently different SQL dialects. We present an overview of how SQL can be decomposed in terms of features and compose different features to create different parsers for SQL.},
booktitle = {Proceedings of the 2008 EDBT Workshop on Software Engineering for Tailor-Made Data Management},
pages = {29–33},
numpages = {5},
keywords = {tailor-made data management, feature-oriented programming, embedded systems},
location = {Nantes, France},
series = {SETMDM '08}
}

@article{10.1007/s00766-013-0187-2,
author = {Zdravkovic, Jelena and Svee, Eric-Oluf and Giannoulis, Constantinos},
title = {Capturing consumer preferences as requirements for software product lines},
year = {2015},
issue_date = {March     2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0187-2},
doi = {10.1007/s00766-013-0187-2},
abstract = {Delivering great consumer experiences in competitive market conditions requires software vendors to move away from traditional modes of thinking to an outside-in perspective, one that shifts their business to becoming consumer-centric. Requirements engineers operating in these conditions thus need new means to both capture real preferences of consumers and then relate them to requirements for software customized in different ways to fit anyone. Additionally, because system development models require inputs that are more concrete than abstract, the indistinct values of consumers need to be classified and formalized. To address this challenge, this study aims to establish a conceptual link between preferences of consumers and system requirements, using software product line (SPL) as a means for systematically accommodating the variations within the preferences. The novelty of this study is a conceptual model of consumer preference, which integrates generic value frameworks from both psychology and marketing, and a method for its transformation to requirements for SPL using a goal-oriented RE framework as the mediator. The presented artifacts are grounded in an empirical study related to the development of a system for online education.},
journal = {Requir. Eng.},
month = mar,
pages = {71–90},
numpages = {20},
keywords = {Value modeling, Value, SPL, Requirements, Goal modeling, Features, Consumer value}
}

@inproceedings{10.1145/3109859.3109907,
author = {Qazi, Maleeha and Fung, Glenn M. and Meissner, Katie J. and Fontes, Eduardo R.},
title = {An Insurance Recommendation System Using Bayesian Networks},
year = {2017},
isbn = {9781450346528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109859.3109907},
doi = {10.1145/3109859.3109907},
abstract = {In this paper we describe a deployed recommender system to predict insurance products for new and existing customers. Our goal is to give our customers personalized recommendations based on what other similar people with similar portfolios have, in order to make sure they were adequately covered for their needs. Our system uses customer characteristics in addition to customer portfolio data. Since the number of possible recommendable products is relatively small, compared to other recommender domains, and missing data is relatively frequent, we chose to use Bayesian Networks for modeling our system. Experimental results show advantages of using probabilistic graphical models over the widely used low-rank matrix factorization model for the insurance domain.},
booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
pages = {274–278},
numpages = {5},
keywords = {structure learning, recommender systems, insurance domain, deployed system, bayesian networks},
location = {Como, Italy},
series = {RecSys '17}
}

@article{10.1155/2021/9956244,
author = {Li, Lei and Zhu, Yuquan and Cai, Tao and Niu, Dejiao and Shi, Huaji and Zou, Tingting and Huang, Chenxi},
title = {A Temporal Pool Learning Algorithm Based on Location Awareness},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/9956244},
doi = {10.1155/2021/9956244},
abstract = {Hierarchical Temporal Memory is a new type of artificial neural network model, which imitates the structure and information processing flow of the human brain. Hierarchical Temporal Memory has strong adaptability and fast learning ability and becomes a hot spot in current research. Hierarchical Temporal Memory obtains and saves the temporal characteristics of input sequences by the temporal pool learning algorithm. However, the current algorithm has some problems such as low learning efficiency and poor learning effect when learning time series data. In this paper, a temporal pool learning algorithm based on location awareness is proposed. The cell selection rules based on location awareness and the dendritic updating rules based on adjacent inputs are designed to improve the learning efficiency and effect of the algorithm. Through the algorithm prototype, three different datasets are used to test and analyze the algorithm performance. The experimental results verify that the algorithm can quickly obtain the complete characteristics of the input sequence. No matter whether there are similar segments in the sequence, the proposed algorithm has higher prediction recall and precision than the existing algorithms.},
journal = {Sci. Program.},
month = jan,
numpages = {12}
}

@inproceedings{10.1145/3180155.3180257,
author = {Xue, Yinxing and Li, Yan-Fu},
title = {Multi-objective integer programming approaches for solving optimal feature selection problem: a new perspective on multi-objective optimization problems in SBSE},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180257},
doi = {10.1145/3180155.3180257},
abstract = {The optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study we first expose the mathematical nature of this problem --- multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution).},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1231–1242},
numpages = {12},
keywords = {multi-objective integer programming (MOIP), multi-objective optimization (MOO), optimal feature selection problem},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2866614.2866616,
author = {Schulze, Sandro and Schulze, Michael and Ryssel, Uwe and Seidl, Christoph},
title = {Aligning Coevolving Artifacts Between Software Product Lines and Products},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866616},
doi = {10.1145/2866614.2866616},
abstract = {Software product lines (SPLs) play a pivotal role for developing a vast amount of related programs efficiently and with high quality. To this end, the SPL engineering process is separated into two levels: domain engineering (DE), which captures variability and development artifacts of the entire SPL, and application engineering (AE), which encompasses a variant-specific subset of the aforementioned artifacts. In the industrial practice of evolving an SPL, it is common that evolution is performed on both levels, which may affect the same artifacts (e.g., code, models) in different ways due to changes on the product line (DE) and the variant level (AE). As a result, conflicts may arise that have to be solved properly to guarantee correctness and validity of the affected artifacts. In this paper, we propose a methodology for resolving such conflicts to ensure correctness and consistency among artifacts while minimizing manual effort. Our method is comprehensive in two ways: First, we consider all kinds of artifacts (code and non-code) that may be subject to evolutionary changes in both DE and AE. Second, we also take into account that changing one particular artifact (e.g., a requirement) may require further changes to other artifacts of the same level. This way, our method reflects common industrial practices in SPL development and, thus, provides benefits for efficiently evolving real-world SPLs.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {9–16},
numpages = {8},
keywords = {software evolution, SPL engineering},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@article{10.1287/isre.2020.0921,
author = {Lee, Gene Moo and He, Shu and Lee, Joowon and Whinston, Andrew B.},
title = {Matching Mobile Applications for Cross-Promotion},
year = {2020},
issue_date = {September 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {31},
number = {3},
issn = {1526-5536},
url = {https://doi.org/10.1287/isre.2020.0921},
doi = {10.1287/isre.2020.0921},
abstract = {As the mobile app market grows rapidly, with millions of apps and billions of users, search costs are increasing tremendously. Similar to the case of recommender systems, the challenge is how apps can be recommended to the right users and how consumers can find the right apps. This paper studies a new mobile app ad framework, cross-promotion (CP), which is to promote new “target” apps within other “source” apps. With unique random matching experiment data, we empirically test the important determinants of ad effectiveness. We then propose a machine-learning-based framework to optimally match source apps to target apps to improve ad effectiveness in terms of app downloads and postdownload usages. The simulation results show that app analytics capability is essential in building accurate prediction models and in increasing ad effectiveness of CP campaigns and that, at the expense of privacy, individual user data can further improve the matching performance. The paper has important managerial implications because it provides direct guidance to better utilize CP for app developers and to leverage data analytics and machine-learning models for platform managers. It also provides policy implications on the trade-off between utility and privacy in the growing data economy.The mobile applications (apps) market is one of the most successful software markets. As the platform grows rapidly, with millions of apps and billions of users, search costs are increasing tremendously. The challenge is how app developers can target the right users with their apps and how consumers can find the apps that fit their needs. Cross-promotion, advertising a mobile app (target app) in another app (source app), is introduced as a new app-promotion framework to alleviate the issue of search costs. In this paper, we model source app user behaviors (downloads and postdownload usages) with respect to different target apps in cross-promotion campaigns. We construct a novel app similarity measure using latent Dirichlet allocation topic modeling on apps’ production descriptions and then analyze how the similarity between the source and target apps influences users’ app download and usage decisions. To estimate the model, we use a unique data set from a large-scale random matching experiment conducted by a major mobile advertising company in Korea. The empirical results show that consumers prefer more diversified apps when they are making download decisions compared with their usage decisions, which is supported by the psychology literature on people’s variety-seeking behavior. Lastly, we propose an app-matching system based on machine-learning models (on app download and usage prediction) and generalized deferred acceptance algorithms. The simulation results show that app analytics capability is essential in building accurate prediction models and in increasing ad effectiveness of cross-promotion campaigns and that, at the expense of privacy, individual user data can further improve the matching performance. This paper has implications on the trade-off between utility and privacy in the growing mobile economy.},
journal = {Info. Sys. Research},
month = sep,
pages = {865–891},
numpages = {27},
keywords = {mobile analytics, algorithm, deferred acceptance, machine learning, topic modeling, two-sided platform, search cost, matching, cross-promotion, mobile applications}
}

@inproceedings{10.1145/581339.581373,
author = {van Ommering, Rob},
title = {Building product populations with software components},
year = {2002},
isbn = {158113472X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/581339.581373},
doi = {10.1145/581339.581373},
abstract = {Two trends have made reuse of embedded software for consumer electronics an urgent issue: the software of individual products becomes more and more complex, and the market demands a larger variety of products at an increasing rate. For that reason, various business groups within Philips organize their products as product families. A third trend is the integration of functions that until now were only found in separate products (e.g. a TV with Dolby Digital sound and a built-in DVD player). This requires software reuse between product families, which - when organized systematically - leads to a product population approach.We have set up such a product population approach, and applied it in various business groups within our organization. We use a component technology that stimulates context independence, and allows the composition of new products out of existing parts. We use an architectural description language to explicitly describe the architecture, and also to generate efficient bindings. We have aligned our development process and organization with the new 'compositional' way of working. This paper outlines our approach and reports on our experiences with it.},
booktitle = {Proceedings of the 24th International Conference on Software Engineering},
pages = {255–265},
numpages = {11},
keywords = {product population, product family, diversity, configuration management, component based development, architectural description language},
location = {Orlando, Florida},
series = {ICSE '02}
}

@inproceedings{10.1145/3366424.3386196,
author = {West, Rebecca and Jadda, Khalifeh Al and Ahsan, Unaiza and Qu, Huiming and Cui, Xiquan},
title = {Interpretable Methods for Identifying Product Variants},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3386196},
doi = {10.1145/3366424.3386196},
abstract = {For e-commerce companies with large product selections, the organization and grouping of products in meaningful ways is important for creating great customer shopping experiences and cultivating an authoritative brand image. One important way of grouping products is to identify a family of product variants, where the variants are mostly the same with slight and yet distinct differences (e.g. color or pack size). In this paper, we introduce a novel approach to identifying product variants. It combines both constrained clustering and tailored NLP techniques (e.g. extraction of product family name from unstructured product title and identification of products with similar model numbers) to achieve superior performance compared with an existing baseline using a vanilla classification approach. In addition, we design the algorithm to meet certain business criteria, including meeting high accuracy requirements on a wide range of categories (e.g. appliances, decor, tools, and building materials, etc.) as well as prioritizing the interpretability of the model to make it accessible and understandable to all business partners.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {448–453},
numpages = {6},
keywords = {product variants, natural language processing, constrained clustering},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1109/ICSE.2017.58,
author = {Behringer, Benjamin and Palz, Jochen and Berger, Thorsten},
title = {PEoPL: projectional editing of product lines},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.58},
doi = {10.1109/ICSE.2017.58},
abstract = {The features of a software product line---a portfolio of system variants---can be realized using various implementation techniques (a.k.a., variability mechanisms). Each technique represents the software artifacts of features differently, typically classified into annotative (e.g., C preprocessor) and modular representations (e.g., feature modules), each with distinct advantages and disadvantages. Annotative representations are easy to realize, but annotations clutter source code and hinder program comprehension. Modular representations support comprehension, but are difficult to realize. Most importantly, to engineer feature artifacts, developers need to choose one representation and adhere to it for evolving and maintaining the same artifacts.We present PEoPL, an approach to combine the advantages of annotative and modular representations. When engineering a feature artifact, developers can choose the most-suited representation and even use different representations in parallel. PEoPL relies on separating a product line into an internal and external representation, the latter by providing editable projections used by the developers. We contribute a programming-language-independent internal representation of variability, five editable projections reflecting different variability representations, a supporting IDE, and a tailoring of PEoPL to Java. We evaluate PEoPL's expressiveness, scalability, and flexibility in eight Java-based product lines, finding that all can be realized, that projections are feasible, and that variant computation is fast (&lt;45ms on average for our largest subject Berkeley DB).},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {563–574},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@article{10.1016/j.jbi.2011.11.002,
author = {Forestier, Germain and Lalys, Florent and Riffaud, Laurent and Trelhu, Brivael and Jannin, Pierre},
title = {Classification of surgical processes using dynamic time warping},
year = {2012},
issue_date = {April, 2012},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {45},
number = {2},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2011.11.002},
doi = {10.1016/j.jbi.2011.11.002},
abstract = {In the creation of new computer-assisted intervention systems, Surgical Process Models (SPMs) are an emerging concept used for analyzing and assessing surgical interventions. SPMs represent Surgical Processes (SPs) which are formalized as symbolic structured descriptions of surgical interventions using a pre-defined level of granularity and a dedicated terminology. In this context, one major challenge is the creation of new metrics for the comparison and the evaluation of SPs. Thus, correlations between these metrics and pre-operative data are used to classify surgeries and highlight specific information on the surgery itself and on the surgeon, such as his/her level of expertise. In this paper, we explore the automatic classification of a set of SPs based on the Dynamic Time Warping (DTW) algorithm. DTW is used to compute a similarity measure between two SPs that focuses on the different types of activities performed during surgery and their sequencing, by minimizing time differences. Indeed, it turns out to be a complementary approach to the classical methods that only focus on differences in the time and the number of activities. Experiments were carried out on 24 lumbar disk herniation surgeries to discriminate the surgeons level of expertise according to a prior classification of SPs. Supervised and unsupervised classification experiments have shown that this approach was able to automatically identify groups of surgeons according to their level of expertise (senior and junior), and opens many perspectives for the creation of new metrics for comparing and evaluating surgeries.},
journal = {J. of Biomedical Informatics},
month = apr,
pages = {255–264},
numpages = {10},
keywords = {Surgical process models, Surgery evaluation, Dynamic time warping, Clustering, Classification}
}

@article{10.3233/IDT-130182,
author = {Valverde, Raul and Saade, Raafat George and Talla, Malleswara},
title = {ITIL-based IT service support process reengineering},
year = {2014},
issue_date = {April 2014},
publisher = {IOS Press},
address = {NLD},
volume = {8},
number = {2},
issn = {1872-4981},
url = {https://doi.org/10.3233/IDT-130182},
doi = {10.3233/IDT-130182},
abstract = {The Information Technology Infrastructure Library ITIL supports best practices, reengineering activities and IT service support processes. ITIL framework only provides recommendations, and companies need to utilize this framework to improve their IT service support processes and establish best practices. This study provides a methodology on how to apply the ITIL framework for evaluating the IT service support processes, its reengineering and alignment to best practices, and subsequent integration into a decision support system framework. A case study approach was used to identify a set of Key Performance Indicators KPI which were monitored by a decision support system DSS for triggering on-going reengineering of IT service support processes. This paper focuses on the implementation of the ITIL guidelines at the operational level, improvement of the service desk, and incident, problem, change, release, and configuration management. It also presents the implementation of the ITIL guidelines at the tactical level for the improvement of the service level, capacity, IT service continuity, service availability, and security management. We conclude by providing recommendations for future research.},
journal = {Int. Dec. Tech.},
month = apr,
pages = {111–130},
numpages = {20},
keywords = {Key Performance Indicators, Information Technology Infrastructure Library, Decision Support System}
}

@article{10.1007/s11390-019-1960-6,
author = {Alqmase, Mohammed and Alshayeb, Mohammad and Ghouti, Lahouari},
title = {Threshold Extraction Framework for Software Metrics},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1960-6},
doi = {10.1007/s11390-019-1960-6},
abstract = {Software metrics are used to measure different attributes of software. To practically measure software attributes using these metrics, metric thresholds are needed. Many researchers attempted to identify these thresholds based on personal experiences. However, the resulted experience-based thresholds cannot be generalized due to the variability in personal experiences and the subjectivity of opinions. The goal of this paper is to propose an automated clustering framework based on the expectation maximization (EM) algorithm where clusters are generated using a simplified 3-metric set (LOC, LCOM, and CBO). Given these clusters, different threshold levels for software metrics are systematically determined such that each threshold reflects a specific level of software quality. The proposed framework comprises two major steps: the clustering step where the software quality historical dataset is decomposed into a fixed set of clusters using the EM algorithm, and the threshold extraction step where thresholds, specific to each software metric in the resulting clusters, are estimated using statistical measures such as the mean (μ) and the standard deviation (σ) of each software metric in each cluster. The paper’s findings highlight the capability of EM-based clustering, using a minimum metric set, to group software quality datasets according to different quality levels.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1063–1078},
numpages = {16},
keywords = {empirical study, expectation maximization, metric threshold}
}

@article{10.1016/j.comnet.2021.108199,
author = {Arce, Pau and Salvo, David and Piñero, Gema and Gonzalez, Alberto},
title = {FIWARE based low-cost wireless acoustic sensor network for monitoring and classification of urban soundscape},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {196},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108199},
doi = {10.1016/j.comnet.2021.108199},
journal = {Comput. Netw.},
month = sep,
numpages = {10},
keywords = {Edge computing, FIWARE, Urban sound classification, Acoustic sensor networks}
}

@article{10.1016/j.jss.2015.04.024,
author = {Neves, L. and Borba, P. and Alves, V. and Turnes, L. and Teixeira, L. and Sena, D. and Kulesza, U.},
title = {Safe evolution templates for software product lines},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.04.024},
doi = {10.1016/j.jss.2015.04.024},
abstract = {We extend our investigation of compositional product lines with more subjectsWe also investigate annotative product lines, and propose templates for this contextWe contribute to the body of evidence on safe evolution of product linesWe bring additional evidence of the expressiveness of the proposed templates Software product lines enable generating related software products from reusable assets. Adopting a product line strategy can bring significant quality and productivity improvements. However, evolving a product line can be risky, since it might impact many products. When introducing new features or improving its design, it is important to make sure that the behavior of existing products is not affected. To ensure that, one usually has to analyze different types of artifacts, an activity that can lead to errors. To address this issue, in this work we discover and analyze concrete evolution scenarios from five different product lines. We discover a total of 13 safe evolution templates, which are generic transformations that developers can apply when evolving compositional and annotative product lines, with the goal of preserving the behavior of existing products. We also evaluate the templates by analyzing the evolution history of these product lines. In this evaluation, we observe that the templates can address the modifications that developers performed in the analyzed scenarios, which corroborates the expressiveness of our template set. We also observe that the templates could also have helped to avoid the errors that we identified during our analysis.},
journal = {J. Syst. Softw.},
month = aug,
pages = {42–58},
numpages = {17},
keywords = {Software product lines, Refinement, Evolution}
}

@article{10.1016/j.knosys.2020.106660,
author = {Liu, Zhen and Feng, Xiaodong and Wang, Yecheng and Zuo, Wenbo},
title = {Self-paced learning enhanced neural matrix factorization for noise-aware recommendation},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {213},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2020.106660},
doi = {10.1016/j.knosys.2020.106660},
journal = {Know.-Based Syst.},
month = feb,
numpages = {12},
keywords = {Self-paced learning, Instance weighting, Noisy and outlier corruption, Deep learning, Recommendation}
}

@article{10.1007/s10766-016-0417-6,
author = {Allombert, V. and Gava, F. and Tesson, J.},
title = {Multi-ML: Programming Multi-BSP Algorithms in ML},
year = {2017},
issue_date = {April     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {2},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-016-0417-6},
doi = {10.1007/s10766-016-0417-6},
abstract = {bsp is a bridging model between abstract execution and concrete parallel systems. Structure and abstraction brought by bsp allow to have portable parallel programs with scalable performance predictions, without dealing with low-level details of architectures. In the past, we designed bsml for programming bsp algorithms in ml. However, the simplicity of the bsp model does not fit the complexity of today's hierarchical architectures such as clusters of machines with multiple multi-core processors. The multi-bsp model is an extension of the bsp model which brings a tree-based view of nested components of hierarchical architectures. To program multi-bsp algorithms in ml, we propose the multi-ml language as an extension of bsml where a specific kind of recursion is used to go through a hierarchy of computing nodes. We define a formal semantics of the language and present preliminary experiments which show performance improvements with respect to bsml.},
journal = {Int. J. Parallel Program.},
month = apr,
pages = {340–361},
numpages = {22},
keywords = {multi-bsp, ml, bsp, Parallel programming}
}

@article{10.1007/s10618-010-0175-9,
author = {Silla, Carlos N. and Freitas, Alex A.},
title = {A survey of hierarchical classification across different application domains},
year = {2011},
issue_date = {January   2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1–2},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-010-0175-9},
doi = {10.1007/s10618-010-0175-9},
abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages.},
journal = {Data Min. Knowl. Discov.},
month = jan,
pages = {31–72},
numpages = {42},
keywords = {Tree-structured class hierarchies, Hierarchical classification, DAG-structured class hierarchies}
}

@article{10.1007/s10799-014-0188-y,
author = {Bhargava, Hemant K.},
title = {Platform technologies and network goods: insights on product launch and management},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1385-951X},
url = {https://doi.org/10.1007/s10799-014-0188-y},
doi = {10.1007/s10799-014-0188-y},
abstract = {Entrepreneurs of technology platforms and network goods face distinctive challenges in managing customer adoption, and in trading off growth and profitability. The firm has several levers of control including managing product design and the intensity of network effects, managing the timing of product announcement versus actual product release, selecting the target market for initial product launch, and whether to sell a single version or an expanded product line. Product line expansion is especially useful under network effects. A freemium approach can help the firm manage both growth (via the free product) and profitability (via the premium higher-priced version). However, expanding the product line carries substantial fixed costs (e.g., marketing cost, cost of additional plant, managing multiple sets of inventory, increased distribution cost). Firms are deterred from incurring these fixed costs when there is uncertainty about product success. Such uncertainty is particularly relevant for multi-sided networks--where the value from joining one network (e.g., users) depends on the size of the other side (e.g., developers)--because potential participants on each side may be uncertain about participation on the other side. Despite uncertainty, product line expansion can be attractive for both startups and established firms. Established firms face lower uncertainty about developer participation, and should expand when fixed costs of expansion are low (and do so early in the product's life cycle). In contrast, startup firms face greater uncertainty in securing participation from third-party developers, and are more likely to benefit from a "wait and see" or deferred expansion strategy.},
journal = {Inf. Technol. and Management},
month = sep,
pages = {199–209},
numpages = {11},
keywords = {Versioning, Two-sided markets, Revenue model, Product launch, Product design, Network effects}
}

@article{10.5555/2946645.3007037,
author = {Wei, Ermo and Luke, Sean},
title = {Lenient learning in independent-learner stochastic cooperative games},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We introduce the Lenient Multiagent Reinforcement Learning 2 (LMRL2) algorithm for independent-learner stochastic cooperative games. LMRL2 is designed to overcome a pathology called relative overgeneralization, and to do so while still performing well in games with stochastic transitions, stochastic rewards, and miscoordination. We discuss the existing literature, then compare LMRL2 against other algorithms drawn from the literature which can be used for games of this kind: traditional ("Distributed") Q-learning, Hysteretic Q-learning, WoLF-PHC, SOoN, and (for repeated games only) FMQ. The results show that LMRL2 is very effective in both of our measures (complete and correct policies), and is found in the top rank more often than any other technique. LMRL2 is also easy to tune: though it has many available parameters, almost all of them stay at default settings. Generally the algorithm is optimally tuned with a single parameter, if any. We then examine and discuss a number of side-issues and options for LMRL2.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2914–2955},
numpages = {42},
keywords = {reinforcement learning, multiagent learning, lenient learning, independent learner, game theory}
}

@article{10.1017/S0890060404040028,
author = {Simpson, Timothy W.},
title = {Product platform design and customization: Status and promise},
year = {2004},
issue_date = {January 2004},
publisher = {Cambridge University Press},
address = {USA},
volume = {18},
number = {1},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060404040028},
doi = {10.1017/S0890060404040028},
abstract = {In an effort to improve customization for today's highly competitive global marketplace, many companies are utilizing product families and platform-based product development to increase variety, shorten lead times, and reduce costs. The key to a successful product family is the product platform from which it is derived either by adding, removing, or substituting one or more modules to the platform or by scaling the platform in one or more dimensions to target specific market niches. This nascent field of engineering design has matured rapidly in the past decade, and this paper provides a comprehensive review of the flurry of research activity that has occurred during that time to facilitate product family design and platform-based product development for mass customization. Techniques for identifying platform leveraging strategies within a product family are reviewed along with metrics for assessing the effectiveness of product platforms and product families. Special emphasis is placed on optimization approaches and artificial intelligence techniques to assist in the process of product family design and platform-based product development. Web-based systems for product platform customization are also discussed. Examples from both industry and academia are presented throughout the paper to highlight the benefits of product families and product platforms. The paper concludes with a discussion of potential areas of research to help bridge the gap between planning and managing families of products and designing and manufacturing them.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = jan,
pages = {3–20},
numpages = {18},
keywords = {Product Variety, Product Platform, Product Family, Mass Customization}
}

@inproceedings{10.5555/3507788.3507804,
author = {Ria and Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Process-metrics trends analysis for evaluating file-level error proneness},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Assessing the likelihood of a source code file being buggy or healthy in upcoming commits given its past behavior and its interaction with other files, has been an area where the software engineering community has paid significant attention over the years. Early efforts aimed on associating software metrics with maintainability indexes, while more recent efforts focused on the use of machine learning for classifying a software module as error prone or not. In most approaches to date, this analysis is primarily based on source code metrics or on information extracted from the system's source code, and to a lesser extend on information that relates to process metrics. In this paper, we propose a process-metrics based method for predicting the behavior of a file, based both on its GitHub commits and its interdependences with other co-committed files. More specifically, for each file and for each commit a file participates in, we compute a dependency score this file has with its other co-committed files. This score is appropriately amplified if the file is participating in a bug-fixing commit, or decayed over time if it does not. By examining, over several open source systems, the trend of that dependency score for every file as a product of time, for files whose outcome is known and that are used as gold standard, we report statistics which shed light on estimating the likelihood of whether these trends can predict the future behavior of a file or not.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {113–122},
numpages = {10},
location = {Toronto, Canada},
series = {CASCON '21}
}

@article{10.1287/msom.2020.0869,
author = {Hu, Ming and Liu, Jingchen and Zhai, Xin},
title = {Intertemporal Segmentation via Flexible-Duration Group Buying},
year = {2021},
issue_date = {September–October 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {23},
number = {5},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2020.0869},
doi = {10.1287/msom.2020.0869},
abstract = {Problem definition
: We study a special form of group buying: the group buying succeeds only if the number of sign-ups reaches a preset threshold, with no duration constraint. Customers with heterogeneous valuations arrive sequentially and decide between signing up for the group buying or purchasing a regular product. To decide whether to join the group buying, customers need to estimate their expected waiting time, which varies depending on the cumulative sign-ups by the time of their arrival. The firm decides on the prices for the group-buying product and regular product, with the product quality levels and group-buying size exogenously determined. 
Academic/practical relevance
: This type of group buying is often adopted for a special edition of the product and offered alongside a constantly available regular product. 
Methodology
: We study the product line design with the group-buying sign-up behavior of customers characterized by the rational expectations equilibrium in a random pledging process. 
Results
: We show that group buying with flexible duration can result in intertemporal customer segmentation, as different segments might be admitted at different times in the dynamic sign-up process. Such intertemporal segmentation is a natural discrimination scheme and has nontrivial implications. First, the efficiency loss due to waiting for enough sign-ups may decrease when a larger batch size is required for economic production. Second, as valuation heterogeneity in the market increases, the firm may not always benefit from offering group buying along with the regular product. Third, group buying can achieve a win-win-win situation for both high-end and low-end customers as well as the firm. 
Managerial implications
: In addition to demonstrating the profitability of flexible-duration group buying, we show that the firm can strengthen its profitability by contingently setting prices or concealing sign-up information in group buying. We also confirm the robustness of our main insights by considering customers’ heterogeneous patience levels and horizontally differentiated products, among other factors.},
journal = {Manufacturing &amp; Service Operations Management},
month = sep,
pages = {1157–1174},
numpages = {18},
keywords = {price discrimination, intertemporal segmentation, flexible duration, group buying}
}

@inproceedings{10.1007/978-3-319-42061-5_1,
author = {Babur, Önder and Cleophas, Loek and Brand, Mark},
title = {Hierarchical Clustering of Metamodels for Comparative Analysis and Visualization},
year = {2016},
isbn = {9783319420608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-42061-5_1},
doi = {10.1007/978-3-319-42061-5_1},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models or metamodels. A good example is the comparison and merging of metamodel variants into a common metamodel in domain model recovery. Although there are many sophisticated techniques to process the input dataset, little attention has been given to the initial data analysis, visualization and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. In this paper we present a generic approach for metamodel comparison, analysis and visualization as an exploratory first step for domain model recovery. We propose representing metamodels in a vector space model, and applying hierarchical clustering techniques to compare and visualize them as a tree structure. We demonstrate our approach on two Ecore datasets: a collection of 50 state machine metamodels extracted from GitHub as top search results; and $$sim $$~100 metamodels from 16 different domains, obtained from AtlanMod Metamodel Zoo.},
booktitle = {Proceedings of the 12th European Conference on Modelling Foundations and Applications - Volume 9764},
pages = {3–18},
numpages = {16},
keywords = {Vector space model, R, Model-Driven Engineering, Model comparison, Hierarchical clustering}
}

@article{10.1016/j.jss.2017.11.004,
author = {Carvalho, Michelle Larissa Luciano and da Silva, Matheus Lessa Gonalves and Gomes, Gecynalda Soares da Silva and Santos, Alcemir Rodrigues and Machado, Ivan do Carmo and Souza, Magno Lu de Jesus and de Almeida, Eduardo Santana},
title = {On the implementation of dynamic software product lines},
year = {2018},
issue_date = {February 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.11.004},
doi = {10.1016/j.jss.2017.11.004},
abstract = {A set of criteria to characterize mechanisms suitable to implement dynamic variability.A characterization of thirteen DSPL-ready variability mechanisms.Empirical evaluation of OOP and AOP from the perspective of DSPL evolution.Evidence showing that AOP is a feasible strategy to implement DSPL projects. Dynamic Software Product Line (DSPL) engineering is a paradigm aimed at handling adaptations at runtime. An inherent challenge in DSPL engineering is to reduce the design complexity of adaptable software, particularly in terms of evolution. Existing research only recently started to investigate evolution in this field, but does not assess the impact of different implementations under software quality in evolutionary scenarios. This work presents a characterization of thirteen dynamic variability mechanisms. Based on such characterization, we implemented a DSPL using Object-oriented Programming (OOP) mechanisms. From this implementation, we evidenced that DSPL requires changes and extensions to design, in terms of functionality and adaptation capabilities. Since Aspect-oriented Programming (AOP) was well ranked according to characterization and some studies have demonstrated the likely synergies between AOP and DSPL, we decided to compare it with OOP. We empirically evaluated how OOP and AOP could affect source code quality from the viewpoint of an evolving DSPL. As a result, AOP yields better results in terms of size, SoC, cohesion, and coupling measures. Conversely, AOP provides lower change propagation impact. Although the packages in AOP were more susceptible to changes than in OOP, we could indicate that AOP may be a feasible strategy for DSPL implementation.},
journal = {J. Syst. Softw.},
month = feb,
pages = {74–100},
numpages = {27},
keywords = {Variability mechanisms, Software evolution, Evidence-based software engineering, Dynamic software product lines}
}

@article{10.1287/opre.2019.1928,
author = {Mišić, Velibor V.},
title = {Optimization of Tree Ensembles},
year = {2020},
issue_date = {September-October 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {68},
number = {5},
issn = {0030-364X},
url = {https://doi.org/10.1287/opre.2019.1928},
doi = {10.1287/opre.2019.1928},
abstract = {From Tree Ensemble Models to DecisionsPredictive models based on ensembles of trees, such as random forests and gradient boosted trees, are widely used in machine learning and data science. In many applications, the features that these models use are controllable and can be regarded as decision variables. This leads to a natural prescriptive analytics problem: how should these features be set, so as to maximize the value predicted by the tree ensemble model? In “Optimization of Tree Ensembles” Velibor V. Mišić proposes a MIO model of this problem, proposes a hierarchy of approximations to this formulation based on truncating the trees at a particular depth, and develops two specialized constraint generation methods for solving the problem at scale. Using real data sets, including two detailed case studies in drug design and customized pricing, the author shows how this approach can efficiently solve large-scale problem instances to full or near optimality and outperforms solutions obtained by heuristic approaches.Tree ensemble models such as random forests and boosted trees are among the most widely used and practically successful predictive models in applied machine learning and business analytics. Although such models have been used to make predictions based on exogenous, uncontrollable independent variables, they are increasingly being used to make predictions where the independent variables are controllable and are also decision variables. In this paper, we study the problem of tree ensemble optimization: given a tree ensemble that predicts some dependent variable using controllable independent variables, how should we set these variables so as to maximize the predicted value? We formulate the problem as a mixed-integer optimization problem. We theoretically examine the strength of our formulation, provide a hierarchy of approximate formulations with bounds on approximation quality and exploit the structure of the problem to develop two large-scale solution methods, one based on Benders decomposition and one based on iteratively generating tree split constraints. We test our methodology on real data sets, including two case studies in drug design and customized pricing, and show that our methodology can efficiently solve large-scale instances to near or full optimality, and outperforms solutions obtained by heuristic approaches.},
journal = {Oper. Res.},
month = sep,
pages = {1605–1624},
numpages = {20},
keywords = {customized pricing, drug design, mixed-integer optimization, random forests, tree ensembles, Optimization, statistics, applications: integer: programming}
}

@inproceedings{10.5555/645882.672394,
author = {Simon, Daniel and Eisenbarth, Thomas},
title = {Evolutionary Introduction of Software Product Lines},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product lines have proved to be a successful and efficient means for managing the development of software in industry. The significant benefits over traditional software architectures have the potential to convince software companies to adopt the product line approach for their existing products. In that case, the question arises how to best convert the existing products into a software product line. For several reasons, an evolutionary approach is desirable. But so far, there is little guidance on the evolutionary introduction of software product lines.In this paper, we propose a lightweight iterative process supporting the incremental introduction of product line concepts for existing software products. Starting with the analysis of the legacy code, we assess what parts of the software can be restructured for product line needs at reasonable costs. For the analysis of the products, we use feature analysis, a reengineering technique tailored to the specific needs of the initiation of software product lines.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {272–282},
numpages = {11},
series = {SPLC 2}
}

@inproceedings{10.1145/3297280.3297511,
author = {Allian, Ana Paula and Sena, Bruno and Nakagawa, Elisa Yumi},
title = {Evaluating variability at the software architecture level: an overview},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297511},
doi = {10.1145/3297280.3297511},
abstract = {Software architecture are designed for developing software systems needed for a diverse of business goals. Consequently, architecture has to deal with a significant amount of variability in functionality and quality attributes to create different products. Due to this variability, the evaluation in software architectures is much more complex, as different alternatives of systems might be developed leading to an expensive and time consuming task. Several methods and techniques have been proposed to evaluate product line architectures (PLAs) aiming to asses whether or not the architecture will lead to the desired quality attributes. However, there is little consensus on the existing evaluations methods is most suitable for evaluating variability in software architectures, instead of only considering PLAs. Understanding and explicitly evaluating variations in architectures is a cost-effective way of mitigating substantial risk to organizations and their software systems. Therefore, the main contribution of this research work is to present the state of the art about means for evaluating software architectures (including, PLAs, software architectures, reference and enterprise architectures) that contain variability information. We conducted a Systematic Mapping Study (SMS) to provide an overview and insight to practitioners about the most relevant techniques and methods developed for this evaluation. Results indicate that most evaluation techniques assess variability as a quality attribute in PLAs through scenario-based; however, little is known about their real effectiveness as most studies present gaps and lack of evaluation, which difficult the usage of such techniques in an industrial environment.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2354–2361},
numpages = {8},
keywords = {evaluation, software architecture, software variability, systematic mapping study},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1287/mnsc.2015.2352,
author = {Egelman, Carolyn D. and Epple, Dennis and Argote, Linda and Fuchs, Erica R. H.},
title = {Learning by Doing in Multiproduct Manufacturing: Variety, Customizations, and Overlapping Product Generations},
year = {2017},
issue_date = {February 2017},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {63},
number = {2},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2015.2352},
doi = {10.1287/mnsc.2015.2352},
abstract = {Extending research on organizational learning to multiproduct environments is of particular importance given that the vast majority of products are manufactured in such environments. We investigate learning in a multiproduct facility drawing on exceptionally rich data for a manufacturing firm that is a leading producer of high-technology hardware components. Weekly data for 10 years from the firm's production and human resource tracking systems are augmented by surveys of managers and engineers and by extensive firsthand observation. We find that productivity improves when multiple generations of the firm's primary product family are produced concurrently, reflecting the firm's ability to augment and transfer knowledge from older to newer product generations. No significant transfer of knowledge is evident between the primary product family and other products. Productivity is, however, adversely affected when the production facility is faced with extensive within-product buyer-specific customizations. We develop the implications of these findings for theory and practice.This paper was accepted by Serguei Netessine, operations management.},
journal = {Manage. Sci.},
month = feb,
pages = {405–423},
numpages = {19},
keywords = {strategy, productivity, production scheduling, organizational studies, manufacturing, learning}
}

@inproceedings{10.1007/11763864_10,
author = {Lee, Yuqin and Yang, Chuanyao and Zhu, Chongxiang and Zhao, Wenyun},
title = {An approach to managing feature dependencies for product releasing in software product lines},
year = {2006},
isbn = {3540346066},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11763864_10},
doi = {10.1007/11763864_10},
abstract = {Product line software engineering is a systematic approach to realize large scale software reuse. Software product lines deal with reusable assets across a domain by exploring requirements commonality and variability. Requirements dependencies have very strong influence on all development phases of member products in a product line. There are many feature oriented approaches on requirement dependencies. However, most of them are limited to the problem domain. Among those few focusing on the solution domain, they are limited to modeling requirement dependencies. This paper presents a feature oriented approach to managing domain requirements dependencies. Not only is a requirement dependencies model presented, but a directed graph-based approach is also developed to analyze domain requirement dependencies for effective release of member products in a product line. This approach returns a simple directed graph, and uses an effective algorithm to get a set of requirements to be released in a member product. A case study for spot and futures transaction domain is described to illustrate the approach.},
booktitle = {Proceedings of the 9th International Conference on Reuse of Off-the-Shelf Components},
pages = {127–141},
numpages = {15},
location = {Turin, Italy},
series = {ICSR'06}
}

@article{10.1287/mnsc.2017.2919,
author = {Ramachandran, Karthik and Tereyağoğlu, Necati and Xia, Yusen},
title = {Multidimensional Decision Making in Operations: An Experimental Investigation of Joint Pricing and Quantity Decisions},
year = {2018},
issue_date = {December 2018},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {64},
number = {12},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2017.2919},
doi = {10.1287/mnsc.2017.2919},
abstract = {Firms in several industries, such as medicine, apparel, and publishing, must jointly determine the price and production quantity of their products well in advance of the selling season. Normative prescriptions to solve this problem have generally ignored behavioral aspects of decision making while behavioral research has paid limited attention to interdependent, multidimensional decisions. We experimentally examine subjects' performance when they jointly determine price and quantities. We find that subjects systematically deviate from the theoretically optimal price and quantity levels. Contrary to expectation, decomposing the price and quantity decisions does not improve subjects' decisions. In a series of follow-up experiments, we isolate the effects of a interdependence between decisions and b demand uncertainty. We show that decisions improve by making subjects more aware of interdependence and by reducing the uncertainty. However, reducing complexity through partial automation of the interdependent dimensions does not improve the decisions made by subjects. We also find that subjects anchor on cost for price decisions and on mean demand potential for quantity decisions, thereby explaining the consistent underpricing and overordering behavior across experiments. This paper was accepted by Vishal Gaur, operations management.},
journal = {Manage. Sci.},
month = dec,
pages = {5544–5558},
numpages = {15},
keywords = {task interdependence, pricing, multidimensional decision making, demand uncertainty, behavioral operations, anchoring}
}

