@article{10.1016/j.infsof.2020.106389,
author = {Chac\'{o}n-Luna, Ana Eva and Guti\'{e}rrez, Antonio Manuel and Galindo, Jos\'{e} A. and Benavides, David},
title = {Empirical software product line engineering: A systematic literature review},
year = {2020},
issue_date = {Dec 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {128},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106389},
doi = {10.1016/j.infsof.2020.106389},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {22},
keywords = {Software product lines, Empirical strategies, Case study, Experiment, Systematic literature review}
}

@inproceedings{10.1145/3307630.3342421,
author = {Chac\'{o}n-Luna, Ana E. and Ruiz, Elvira G. and Galindo, Jos\'{e} A. and Benavides, David},
title = {Variability Management in a Software Product Line Unaware Company: Towards a Real Evaluation},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342421},
doi = {10.1145/3307630.3342421},
abstract = {Software Product Lines (SPL) enable systematic reuse within an organization thus, enabling the reduction of costs, efforts, development time and the average number of defects per product. However, there is little empirical evidence of SPL adoption in the literature, which makes it difficult to strengthen or elaborate adjustments or improvements to SPL frameworks. In this article, we present the first steps towards an empirical evaluation by showing how companies that do not know about of SPL manage variability in their products, pointing out the strengths and weaknesses of their approaches. To this end, we present the design of a case study that we plan to carry out in the future in two companies to evaluate how companies perform variability management when they are not aware of software product lines. Our assumption is that most of the companies manage variability but no many of them are aware of software product lines. In addition, the first preliminary results of the case study applied in a company are presented.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {82–89},
numpages = {8},
keywords = {a case study, software product lines, variability management},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s11227-021-03627-5,
author = {Kiani, Azaz Ahmed and Hafeez, Yaser and Imran, Muhammad and Ali, Sadia},
title = {A dynamic variability management approach working with agile product line engineering practices for reusing features},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {8},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-021-03627-5},
doi = {10.1007/s11227-021-03627-5},
abstract = {Agile software development (ASD) and software product line (SPL) have shown significant benefits for software engineering processes and practices. Although both methodologies promise similar benefits, they are based on different foundations. SPL encourages systematic reuse that exploits the commonalities of various products belonging to a common domain and manages their variations systematically. In contrast, ASD stresses a flexible and rapid development of products using iterative and incremental approaches. ASD encourages active involvement of customers and their frequent feedback. Both ASD and SPL require alternatives to extend agile methods for several reasons such as (1) to manage reusability and variability across the products of any domain, (2) to avoid the risk of developing core assets that will become obsolete and not used in future projects, and (3) to meet the requirements of changing markets. This motivates the researchers for the integration of ASD and SPL approaches. As a result, an innovative approach called agile product line engineering (APLE) by integrating SPL and ASD has been introduced. The principal aim of APLE is to maximize the benefits of ASD and SPL and address the shortcomings of both. However, combining both is a major challenge. Researchers have proposed a few approaches that try to put APLE into practice, but none of the existing approaches cover all APLE features needed. This paper proposes a new dynamic variability approach for APLE that uses APLE practices for reusing features. The proposed approach (PA) is based on the agile method Scrum and the reactive approach of SPL. In this approach, reusable core assets respond reactively to customer requirements. The PA constructs and develops the SPL architecture iteratively and incrementally. It provides the benefits of reusability and maintainability of SPLs while keeping the delivery-focused approach from agile methods. We conducted a quantitative survey of software companies applying the APLE to assess the performance of the PA and hypotheses of empirical study. Findings of empirical evaluation provide evidence on integrating ASD and SPL and the application of APLE into practices.},
journal = {J. Supercomput.},
month = aug,
pages = {8391–8432},
numpages = {42},
keywords = {Software product line, Agile software development, Agile software product line, Agile product line engineering}
}

@inproceedings{10.1145/3382026.3425771,
author = {Morais Ferreira, David and Becker, Martin and Tenev, Vasil L.},
title = {Experience Report on Variability Improvement in a Product Line Engineering Unaware Company},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425771},
doi = {10.1145/3382026.3425771},
abstract = {Product Line Engineering (PLE) enables strategic reuse within an organisation, thus reducing development costs, decreasing the time to market, and increasing product quality. As a core activity in PLE, variability management supports modelling of commonality and variability throughout the engineering life cycle. Given the increased complexity of modern software-intensive systems, variability management is becoming increasingly important. Transitioning to PLE approaches is a challenging task, as potential benefits must be carefully weighed against costs introduced by PLE approaches. This paper presents a collaborative approach for reverse-engineering variability and configuration knowledge with minimal domain expert involvement and provides insights into the experience we gained from our industrial collaboration.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {21–28},
numpages = {8},
keywords = {Experience report, industrial collaboration, product line engineering, reverse engineering, variability improvement},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {deep neural networks, machine learning, software product lines, transfer learning, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342418,
author = {Rinc\'{o}n, Luisa and Mazo, Ra\'{u}l and Salinesi, Camille},
title = {Analyzing the Convenience of Adopting a Product Line Engineering Approach: An Industrial Qualitative Evaluation},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342418},
doi = {10.1145/3307630.3342418},
abstract = {Engineering Software Product Lines may be a strategy to reduce costs and efforts for developing software and increasing business productivity. However, it cannot be considered as a "silver bullet" that applies to all types of organizations. Companies must consider pros and cons to determine sound reasons and justify its adoption. In previous work, we proposed the APPLIES evaluation framework to help decision-makers find arguments that may justify (or not) adopting a product line engineering approach. This paper presents our experience using this framework in a mid-sized software development company with more than 25 years of experience but without previous experience in product line engineering. This industrial experience, conducted as a qualitative empirical evaluation, helped us to evaluate to what extent APPLIES is practical to be used in a real environment and to gather ideas from real potential users to improve the framework.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {90–97},
numpages = {8},
keywords = {empirical evaluation, product line adoption, product line engineering, qualitative evaluation},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3489849.3489948,
author = {Lebiedz, Jacek and Wiszniewski, Bogdan},
title = {CAVE applications: from craft manufacturing to product line engineering},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489948},
doi = {10.1145/3489849.3489948},
abstract = {Product line engineering model is suitable for engineering related software products in an efficient manner, taking advantage of their similarities while managing their differences. Our feature driven software product line (SPL) solution based on that model allows for instantiation of different CAVE products based on the set of core assets and driven by a set of common VR features with the minimal budget and time to market.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {57},
numpages = {2},
keywords = {VR application features, core assets, production stations},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/2970276.2970288,
author = {Schw\"{a}gerl, Felix and Westfechtel, Bernhard},
title = {SuperMod: tool support for collaborative filtered model-driven software product line engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970288},
doi = {10.1145/2970276.2970288},
abstract = {The increase in productivity implied by model-driven software product line engineering is weakened by the complexity exposed to the user having to manage a multi-variant model. Recently, a new paradigm has emerged: filtered software product line engineering transfers the established check-out/modify/commit workflow from version control to variability management, allowing to iteratively develop the multi-variant model in a single-variant view. This paper demonstrates SuperMod, a tool that supports collaborative filtered model-driven product line engineering, implemented for and with the Eclipse Modeling Framework. Concerning variability management, the tool offers capabilities for editing feature models and specifying feature configurations, both being well-known formalisms in product line engineering. Furthermore, collaborative editing of product lines is provided through distributed version control. The accompanying video shows that SuperMod seamlessly integrates into existing tool landscapes, reduces the complexity of multi-variant editing, automates a large part of variability management, and ensures consistency. A tool demonstration video is available here: http://youtu.be/5XOk3x5kjFc},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {822–827},
numpages = {6},
keywords = {Model-driven software engineering, filtered editing, software product line engineering, version control},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/3233027.3233038,
author = {Martinez, Jabier and T\"{e}rnava, Xhevahire and Ziadi, Tewfik},
title = {Software product line extraction from variability-rich systems: the robocode case study},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233038},
doi = {10.1145/3233027.3233038},
abstract = {The engineering of a Software Product Line (SPL), either by creating it from scratch or through the re-engineering of existing variants, it uses to be a project that spans several years with a high investment. It is often hard to analyse and quantify this investment, especially in the context of extractive SPL adoption when the related software variants are independently created by different developers following different system architectures and implementation conventions. This paper reports an experience on the creation of an SPL by re-engineering system variants implemented around an educational game called Robocode. The objective of this game is to program a bot (a battle tank) that battles against the bots of other developers. The world-wide Robocode community creates and maintains a large base of knowledge and implementations that are mainly organized in terms of features, although not presented as an SPL. Therefore, a group of master students analysed this variability-rich domain and extracted a Robocode SPL. We present the results of such extraction augmented with an analysis and a quantification regarding the spent time and effort. We believe that the results and the a-posteriori analysis can provide insights on global challenges on SPL adoption. We also provide all the elements to SPL educators to reproduce the teaching activity, and we make available this SPL to be used for any research purpose.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {132–142},
numpages = {11},
keywords = {education, extractive software product line adoption, reverse-engineering, robocode, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3336294.3336310,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {Industrial and Academic Software Product Line Research at SPLC: Perceptions of the Community},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336310},
doi = {10.1145/3336294.3336310},
abstract = {We present preliminary insights into the perception of researchers and practitioners of the software product line (SPL) community on previous, current, and future research efforts. We were particularly interested in up-and-coming and outdated topics and whether the views of academics and industry researchers differ. Also, we compared the views of the community with the results of an earlier literature survey published at SPLC 2018. We conducted a questionnaire-based survey with attendees of SPLC 2018. We received 33 responses (about a third of the attendees) from both, very experienced attendees and younger researchers, and from academics as well as industry researchers. We report preliminary findings regarding popular and unpopular SPL topics, topics requiring further work, and industry versus academic researchers' views. Differences between academic and industry researchers become visible only when analyzing comments on open questions. Most importantly, while topics popular among respondents are also popular in the literature, topics respondents think require further work have often already been well researched. We conclude that the SPL community needs to do a better job preserving and communicating existing knowledge and particularly also needs to widen its scope.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {189–194},
numpages = {6},
keywords = {SPLC, academia, industry, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {NLTK, feature model extraction, natural language processing, requirements engineering, software product line},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {SBSE, SPL, genetic programming, program synthesis},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3106195.3106223,
author = {Iglesias, Aitziber and Lu, Hong and Arellano, Crist\'{o}bal and Yue, Tao and Ali, Shaukat and Sagardui, Goiuria},
title = {Product Line Engineering of Monitoring Functionality in Industrial Cyber-Physical Systems: A Domain Analysis},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106223},
doi = {10.1145/3106195.3106223},
abstract = {In recent years, manufacturing technology is evolving and progressively becoming more dynamic and complex. This means that manufacturing technology (e.g., based on Industry 4.0) should be able to control the production process at runtime by monitoring physical elements and adapting itself. Such functionality is aimed at increasing production effectiveness and reducing the production cost. We argue that monitoring process can be viewed as a software product line having commonalities and variability. To support our argument, we analyzed and conducted domain analysis of two monitoring systems of Industrial Cyber-Physical Systems (ICPSs) from two industrial domains including automated warehouses and press machines. Based on the domain analysis, we present a common solution for monitoring including a software product line. With such product line, a user can configure, monitor, and visualize data of an ICPS at runtime. However, such solution could not handle the dynamic functionality related to monitoring of ICPS. Thus, we propose the use of dynamic product line and present a set of research questions that must be addressed for such solution.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {195–204},
numpages = {10},
keywords = {Cyber Physical System, Dynamic Software Product Line, Industrial domains, Software Product Line},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3382025.3414953,
author = {Abbas, Muhammad and Jongeling, Robbert and Lindskog, Claes and Enoiu, Eduard Paul and Saadatmand, Mehrdad and Sundmark, Daniel},
title = {Product line adoption in industry: an experience report from the railway domain},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414953},
doi = {10.1145/3382025.3414953},
abstract = {The software system controlling a train is typically deployed on various hardware architectures and must process various signals across those deployments. The increase of such customization scenarios and the needed adherence of the software to various safety standards in different application domains has led to the adoption of product line engineering within the railway domain. This paper explores the current state-of-practice of software product line development within a team developing industrial embedded software for a train propulsion control system. Evidence is collected using a focus group session with several engineers and through inspection of archival data. We report several benefits and challenges experienced during product line adoption and deployment. Furthermore, we identify and discuss improvement opportunities, focusing mainly on product line evolution and test automation.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {3},
numpages = {11},
keywords = {challenges and opportunities, overloaded assets, software product-line engineering},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {Software product lines, product-line analysis}
}

@inproceedings{10.1145/3483899.3483909,
author = {Furtado, Viviane and OliveiraJr, Edson and Kalinowski, Marcos},
title = {Guidelines for Promoting Software Product Line Experiments},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483909},
doi = {10.1145/3483899.3483909},
abstract = {The importance of experimentation for Software Engineering research has been notably established in the last years. The software engineering community has discussed how to proper report and evaluate experiments using different approaches, such as quality criteria, scales, and checklists. Nevertheless, there are no guidelines to support researchers and practitioners active in straightforward software engineering research areas, as in Software Product Lines (SPL), at conducting experiments. We hypothesize that experimentation guidelines may aid such a specific area by providing advice and actual excerpts reflecting good practices of SPL experimentation, thus experimentally evolving this area. Therefore, the goal of this paper is to provide guidelines for properly reporting and promoting SPL experiments. We defined such guidelines based on well-known software engineering experiment reports, quality evaluation checklists, and data extracted from 211 SPL experiments identified in a systematic mapping study. We evaluated the guidelines with a qualitative study with SPL and experimentation experts applying open and axial coding procedures. The evaluation enabled us to improve the guidelines. The resulting guidelines contain specific advice to researchers active in SPL and provide examples taken from published SPL experiments. The experts’ positive points indicate that the proposed guidelines can aid SPL researchers and practitioners. Sharing the resulting guidelines could support conducting SPL experiments and allow further area evolution based on prospective experiment replications and reproductions from well-designed and reported experiments.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {31–40},
numpages = {10},
keywords = {Experiment Reporting and Sharing, Guidelines, Qualitative Study, SPL Experiments},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1145/3385032.3385043,
author = {Bilic, Damir and Sundmark, Daniel and Afzal, Wasif and Wallin, Peter and Causevic, Adnan and Amlinger, Christoffer and Barkah, Dani},
title = {Towards a Model-Driven Product Line Engineering Process: An Industrial Case Study},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385043},
doi = {10.1145/3385032.3385043},
abstract = {Many organizations developing software-intensive systems face challenges with high product complexity and large numbers of variants. In order to effectively maintain and develop these product variants, Product-Line Engineering methods are often considered, while Model-based Systems Engineering practices are commonly utilized to tackle product complexity. In this paper, we report on an industrial case study concerning the ongoing adoption of Product Line Engineering in the Model-based Systems Engineering environment at Volvo Construction Equipment (Volvo CE) in Sweden. In the study, we identify and define a Product Line Engineering process that is aligned with Model-based Systems Engineering activities at the engines control department of Volvo CE. Furthermore, we discuss the implications of the migration from the current development process to a Model-based Product Line Engineering-oriented process. This process, and its implications, are derived by conducting and analyzing interviews with Volvo CE employees, inspecting artifacts and documents, and by means of participant observation. Based on the results of a first system model iteration, we were able to document how Model-based Systems Engineering and variability modeling will affect development activities, work products and stakeholders of the work products.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {9},
numpages = {11},
keywords = {Engine System Development, Model-Based Systems Engineering, Product Line Engineering},
location = {Jabalpur, India},
series = {ISEC '20}
}

@inproceedings{10.1145/3461001.3475157,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Ayala, Inmaculada and Kr\"{u}ger, Jacob and Mosser, S\'{e}bastien},
title = {International Workshop on Variability Management for Modern Technologies (VM4ModernTech 2021)},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3475157},
doi = {10.1145/3461001.3475157},
abstract = {Variability is an inherent property of software systems that allows developers to deal with the needs of different customers and environments, creating a family of related systems. Variability can be managed in an opportunistic fashion, for example, using clone-and-own, or by employing a systematic approach, for instance, using a software product line (SPL). In the SPL community, variability management has been discussed for systems in various domains, such as defense, avionics, or finance, and for different platforms, such as desktops, web applications, or embedded systems. Unfortunately, other research communities---particularly those working on modern technologies, such as microservice architectures, cyber-physical systems, robotics, cloud computing, autonomous driving, or ML/AI-based systems---are less aware of the state-of-the-art in variability management, which is why they face similar problems and start to redeveloped the same solutions as the SPL community already did. With the International Workshop on Variability Management for Modern Technologies, we aim to foster and strengthen synergies between the communities researching variability management and modern technologies. More precisely, we aim to attract researchers and practitioners to contribute processes, techniques, tools, empirical studies, and problem descriptions or solutions that are related to reuse and variability management for modern technologies. By inviting different communities and establishing collaborations between them, we hope that the workshop can raise the interest of researchers outside the SPL community for variability management, and thus reduce the extent of costly redevelopments in research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {202},
numpages = {1},
keywords = {software architecture, variability management},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1007/978-3-319-35122-3_2,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Automated Composition of Service Mashups Through Software Product Line Engineering},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_2},
doi = {10.1007/978-3-319-35122-3_2},
abstract = {The growing number of online resources, including data and services, has motivated both researchers and practitioners to provide methods and tools for non-expert end-users to create desirable applications by putting these resources together leading to the so called mashups. In this paper, we focus on a class of mashups referred to as service mashups. A service mashup is built from existing services such that the developed service mashup offers added-value through new functionalities. We propose an approach which adopts concepts from software product line engineering and automated AI planning to support the automated composition of service mashups. One of the advantages of our work is that it allows non-experts to build and optimize desired mashups with little knowledge of service composition. We report on the results of the experimentation that we have performed which support the practicality and scalability of our proposed work.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {20–38},
numpages = {19},
keywords = {Automated composition, Feature model, Planning, Service mashups, Software product lines, Workflow optimization},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/2934466.2934483,
author = {Richenhagen, Johannes and Rumpe, Bernhard and Schlo\ss{}er, Axel and Schulze, Christoph and Thissen, Kevin and von Wenckstern, Michael},
title = {Test-driven semantical similarity analysis for software product line extraction},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934483},
doi = {10.1145/2934466.2934483},
abstract = {Software product line engineering rests upon the assumption that a set of products share a common base of similar functionality. The correct identification of similarities between different products can be a time-intensive task. Hence, this paper proposes an automated semantical similarity analysis supporting software product line extraction and maintenance. Under the assumption of an already identified compatible interface, the degree of semantical similarity is identified based on provided test cases. Therefore, the analysis can also be applied in a test-driven development. This is done by translating available test sequences for both components into two I/O extended finite automata and performing an abstraction of the defined behavior until a simulation relation is established. The test-based approach avoids complexity issues regarding the state space explosion problem, a common issue in model checking. The proposed approach is applied on different variants and versions of industrially used software components provided by an automotive supplier to demonstrate the method's applicability.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {174–183},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Artificial intelligence, Automated feature selection, Inconsistencies, Industrial SPL tools, Literature review, Predictive analytics, Software product line}
}

@inproceedings{10.1145/3422392.3422402,
author = {da Silva, Leandro Flores and Oliveira, Edson},
title = {Evaluating usefulness, ease of use and usability of an UML-based Software Product Line Tool},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422402},
doi = {10.1145/3422392.3422402},
abstract = {Software Product Line (SPL) is a software development approach that systematically applies reuse of artifacts in a specific domain. In the last years, the industry has increasingly required the support of tools for most SPL life cycle activities, targeting feature models and related diagrams, variability management and SPL specific products configuration. However, existing literature does not present any tools with native support to UML-based SPLs. In addition, relying on manipulating XMI files for general-purpose UML tools for such SPLs takes significant effort, and it is time-consuming and error-prone. In this scenario, we developed SMartyModeling, with support to UML stereotype-based variability management. To evolve our tool, we evaluated it throughout a survey answered by 37 participants. We adopted questions from the Technology Acceptance Model (TAM) and the System Usability Scale (SUS). We organized it into three sections of Likert-scaled questions for usefulness, ease of use, and usability. A last section consisted of open questions focused on positive and negative aspects and an overview of the evalaution. SMartyModeling was well evaluated in relation to usefulness, ease of use, and usability. We analyzed and interpreted the respondents quotes using correlation techniques and open and axial coding. The analysis of open questions allowed us a direct identification of points to improve the tool, its limitations and positive aspects.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {798–807},
numpages = {10},
keywords = {SPL tool support, Software Product Line, UML},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/2897695.2897701,
author = {Abilio, Ramon and Vale, Gustavo and Figueiredo, Eduardo and Costa, Heitor},
title = {Metrics for feature-oriented programming},
year = {2016},
isbn = {9781450341776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897695.2897701},
doi = {10.1145/2897695.2897701},
abstract = {Feature-oriented programming (FOP) is a programming technique to implement software product lines based on composition mechanisms called refinements. A software product line is a set of software systems that share a common, managed set of features satisfying the specific needs of a particular market segment. The literature reports various software metrics for software product lines developed using object-oriented and aspect-oriented programming. However, after a literature review, we observed that we lack the definition of FOP-specific metrics. Based on this observation, this paper proposes a set of eight novel metrics for feature-oriented programming. These metrics were derived both from our experience in FOP and from existing software metrics. We demonstrate the applicability of the proposed metrics by applying them to a software product line.},
booktitle = {Proceedings of the 7th International Workshop on Emerging Trends in Software Metrics},
pages = {36–42},
numpages = {7},
keywords = {feature-oriented programming, software metrics, software product lines, software quality},
location = {Austin, Texas},
series = {WETSoM '16}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1016/j.infsof.2013.05.006,
author = {Mohabbati, Bardia and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and M\"{u}ller, Hausi A.},
title = {Combining service-orientation and software product line engineering: A systematic mapping study},
year = {2013},
issue_date = {November, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.006},
doi = {10.1016/j.infsof.2013.05.006},
abstract = {Context: Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective: This study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE. Method: We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000-2011 and classified them with respect to research focus, types of research and contribution. Result: The mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches. In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion: Our study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1845–1859},
numpages = {15},
keywords = {Service-oriented architecture, Software product lines, Systematic mapping}
}

@inproceedings{10.1109/CEC48606.2020.9185675,
author = {Ibias, Alfredo and Llana, Luis},
title = {Feature Selection using Evolutionary Computation Techniques for Software Product Line Testing},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC48606.2020.9185675},
doi = {10.1109/CEC48606.2020.9185675},
abstract = {Software product lines are an excellent mechanism in the development of software. Testing software product lines is an intensive process where selecting the right features where to focus it can be a critical task. Selecting the best combination of features from a software product line is a complex problem addressed in the literature. In this paper, we address the problem of finding the combination of features with the highest probability of being requested from a software product line with probabilities. We use Evolutive Computation techniques to address this problem. Specifically, we use the Ant Colony Optimization algorithm to find the best combination of features. Our results report that our framework overcomes the limitations of the brute force algorithm.},
booktitle = {2020 IEEE Congress on Evolutionary Computation (CEC)},
pages = {1–8},
numpages = {8},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3289402.3289504,
author = {Sebbaq, Hanane and Retbi, Asmaa and Idrissi, Mohammed Khalidi and Bennani, Samir},
title = {Software Product Line to overcome the variability issue in E-Learning: Systematic literature review},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289504},
doi = {10.1145/3289402.3289504},
abstract = {The disparity of educational technologies, pedagogies and learning styles implies a problem of variability when modeling E-learning systems. Furthermore, the current learning context, which has become very open and heterogeneous, raises the problem of automating the modeling, development and maintenance of personalized E-learning systems based on various pedagogies. For its part, the "Software Product Line" is a paradigm that aims to produce product families based on the principles of reuse, configuration and derivation. The main purpose of this literature review is to explore the different potential applications of "SPL" in the E-learning domain to figure out the problem of variability. We will adopt a protocol for a systematic review of literature, after which we will draw up an analysis report.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {4},
numpages = {8},
keywords = {E-learning, Software Product line, Variability, heterogeneity, scale, systematic literature review, variety},
location = {Rabat, Morocco},
series = {SITA'18}
}

@inproceedings{10.1145/2648511.2648537,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina and Gimenes, Itana M. S. and Oizumi, Willian Nalepa},
title = {A search-based approach for software product line design},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648537},
doi = {10.1145/2648511.2648537},
abstract = {The Product Line Architecture (PLA) can be improved by taking into account key factors such as feature modularization, and by continuously evaluating its design according to metrics. Search-Based Software Engineering (SBSE) principles can be used to support an informed-design of PLAs. However, existing search-based design works address only traditional software design not considering intrinsic Software Product Line aspects. This paper presents MOA4PLA, a search-based approach to support the PLA design. It gives a multi-objective treatment to the design problem based on specific PLA metrics. A metamodel to represent the PLA and a novel search operator to improve feature modularization are proposed. Results point out that the application of MOA4PLA leads to PLA designs with well modularized features, contributing to improve features reusability and extensibility. It raises a set of solutions with different design trade-offs that can be used to improve the PLA design.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {237–241},
numpages = {5},
keywords = {multi-objective algorithms, searchbased PLA design, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {clustering, configuration workflow, process discovery, process mining, variability},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s10270-017-0614-9,
author = {Guizzo, Giovani and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {Applying design patterns in the search-based optimization of software product line architectures},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0614-9},
doi = {10.1007/s10270-017-0614-9},
abstract = {The design of the product line architecture (PLA) is a difficult activity that can benefit from the application of design patterns and from the use of a search-based optimization approach, which is generally guided by different objectives related, for instance, to cohesion, coupling and PLA extensibility. The use of design patterns for PLAs is a recent research field, not completely explored yet. Some works apply the patterns manually and for a specific domain. Approaches to search-based PLA design do not consider the usage of these patterns. To allow such use, this paper introduces a mutation operator named “Pattern-Driven Mutation Operator” that includes methods to automatically identify suitable scopes and apply the patterns Strategy, Bridge and Mediator with the search-based approach multi-objective optimization approach for PLA. A metamodel is proposed to represent and identify suitable scopes to receive each one of the patterns, avoiding the introduction of architectural anomalies. Empirical results are also presented, showing evidences that the use of the proposed operator produces a greater diversity of solutions and improves the quality of the PLAs obtained in the search-based optimization process, regarding the values of software metrics.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1487–1512},
numpages = {26},
keywords = {Design pattern, Search-based software engineering, Software product line architecture}
}

@article{10.1016/j.chb.2017.04.026,
author = {Gharsellaoui, Hamza and Maazoun, Jihen and Bouassida, Nadia and Ahmed, Samir Ben and Ben-Abdallah, Hanene},
title = {A Software Product Line Design Based Approach for Real-time Scheduling of Reconfigurable Embedded Systems},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0747-5632},
url = {https://doi.org/10.1016/j.chb.2017.04.026},
doi = {10.1016/j.chb.2017.04.026},
journal = {Comput. Hum. Behav.},
month = feb,
numpages = {11},
keywords = {Real-time scheduling, Reconfigurable embedded systems, SPL design, UML marte}
}

@article{10.1016/j.knosys.2019.104883,
author = {Ayala, Inmaculada and Amor, Mercedes and Horcas, Jose-Miguel and Fuentes, Lidia},
title = {A goal-driven software product line approach for evolving multi-agent systems in the Internet of Things},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {184},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.104883},
doi = {10.1016/j.knosys.2019.104883},
journal = {Know.-Based Syst.},
month = nov,
numpages = {18},
keywords = {Software product line, Evolution, Internet of Things, MAS-PL, Goal models, GORE}
}

@inproceedings{10.1145/1321631.1321730,
author = {Dhungana, Deepak and Rabiser, Rick and Gr\"{u}nbacher, Paul and Neumayer, Thomas},
title = {Integrated tool support for software product line engineering},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321730},
doi = {10.1145/1321631.1321730},
abstract = {Product line engineering comprises many heterogeneous activities such as capturing the variability of reusable assets, supporting the derivation of products from the product line, evolving the product line, or tailoring the approach to the specifics of a domain. The inherent complexity of product lines implicates that tool support is inevitable to facilitate smooth performance and to avoid costly errors. Product line engineering tools have to support heterogeneous stakeholders involved in diverse activities. Tool integration therefore is of particular importance to foster their seamless cooperation. However, the integration is difficult to achieve due to the diversity of models and work products. This paper describes the DOPLER tool suite which has been developed to provide such integrated support. The tool suite is flexible and extensible to support domain-specific needs},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {533–534},
numpages = {2},
keywords = {model evolution, multi-team modeling, product derivation, product line engineering, product line tools, variability modeling},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@inproceedings{10.1145/2362536.2362580,
author = {Hamza, Haitham S. and Martinez, Jabier and Thurimella, Anil Kumar and Deogun, Jitender S.},
title = {Third International Workshop on Knowledge-Oriented Product Line Engineering (KOPLE 2012)},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362580},
doi = {10.1145/2362536.2362580},
abstract = {Software Product Line Engineering (PLE) exploits systematic reuse by identifying and methodically reusing software artifacts to develop different but related software systems. Developing Product Lines requires analysis skills to identify, model, and encode domain and product knowledge into artifacts that can be systematically reused across the development life-cycle. As such, Knowledge plays a paramount role in the success of the various activities of PLE. The objective of the KOPLE workshop series is to bring together SPL researchers and practitioners from academia and industry to investigate the role of Knowledge in PLE. Knowledge is usually encapsulated in PL architectures in a tacit or implicit way, and this may appear to be sufficient for industry to implement successful product lines. Nevertheless, KOPLE also aims to become a discussion forum about techniques and methods to convert from tacit to explicit Knowledge in PLE and to process and use this Knowledge for optimizing and innovating PLE processes.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {292–293},
numpages = {2},
keywords = {conceptual graphs, knowledge engineering, ontology, product lines, software reuse, tacit knowledge},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {preference-based multi-objective algorithms, search-based software engineering, software product line testing},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {configurable systems, machine learning, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Human-computer interaction, Machine Learning, Product Line Architecture},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1007/s10664-014-9358-0,
author = {Koziolek, Heiko and Goldschmidt, Thomas and Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan and Gamer, Thomas and Aleksy, Markus},
title = {Assessing software product line potential: an exploratory industrial case study},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9358-0},
doi = {10.1007/s10664-014-9358-0},
abstract = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {411–448},
numpages = {38},
keywords = {Business case, Domain analysis, Software product lines}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3461002.3473946,
author = {Meixner, Kristof and Feichtinger, Kevin and Rabiser, Rick and Biffl, Stefan},
title = {A reusable set of real-world product line case studies for comparing variability models in research and practice},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473946},
doi = {10.1145/3461002.3473946},
abstract = {Real-world cases describing (product) variability in production systems are rare and often not accessible. Thus, researchers often use toy examples or develop fictitious case studies. These are designed to demonstrate their approach but rarely to compare multiple approaches. In this paper, we aim at making variability modeling evaluations comparable. We present and provide a reusable set of four real-world case studies that are easy to access, with artifacts represented in a universal, variability-model-agnostic way, the industrial Product-Process-Resource Domain-Specific Language (PPR DSL). We report how researchers can use the case studies, automatically transforming the Domain-Specific Language (DSL) artifacts to well-known variability models, e.g., product feature models, using the Variability Evolution Roundtrip Transformation (VERT) process. We compare the expressiveness and complexity of the transformed feature models. We argue that the case studies with the DSL and the flexible transformation capabilities build a valuable contribution to making future research results more comparable and facilitating evaluations with real-world product lines.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {105–112},
numpages = {8},
keywords = {case studies, cyber-physical production system, feature extraction, variability modeling},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/1964138.1964139,
author = {Silva, Alan Pedro da and Costa, Evandro and Bittencourt, Ig Ibert and Brito, Patrick H. S. and Holanda, Olavo and Melo, Jean},
title = {Ontology-based software product line for building semantic web applications},
year = {2010},
isbn = {9781450305426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964138.1964139},
doi = {10.1145/1964138.1964139},
abstract = {The Software Product Lines (SPL) has proved very effective in building large-scale software. However, few works seek to adjust the approach of software product line to applications in the context of semantic web. This is because applications in this context assume the use of semantic services and intelligent agents. As a result, it is necessary that there are assets that provide adequate interoperability both semantic services and intelligent agents. In this sense, it is proposed in this paper the use of ontologies for the specification of entire a project of a SPL. With this, it can be a sufficiently formal specification that can be interpreted by both software engineers and computational algorithms.},
booktitle = {Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering},
articleno = {1},
numpages = {6},
keywords = {ontology, semantic web, software product line},
location = {Reno, Nevada},
series = {KOPLE '10}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Coverage, Debugging/fault localization, EXAM score, Genetic algorithm, Software product line, Test generation}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {configurable systems, machine learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3132498.3133835,
author = {Cardoso, Mateus Passos Soares and Lima, Crescencio and de Almeida, Eduardo Santana and do Carmo Machado, Ivan and von Flach G. Chavez, Christina},
title = {Investigating the variability impact on the recovery of software product line architectures: an exploratory study},
year = {2017},
isbn = {9781450353250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132498.3133835},
doi = {10.1145/3132498.3133835},
abstract = {The Product Line Architecture (PLA) of a Software Product Line (SPL) is the core architecture that represents a high-level design for all the products of an SPL, including variation points and variants. If PLA documentation is missing, it can be recovered by reverse engineering the products. The recovered PLA is a relevant asset for developers and architects, that can be used to drive specific activities of SPL development and evolution, such as, understanding its structure and its variation points, and assessing reuse. This paper presents an exploratory study that investigated the effectiveness of recovered PLAs to address variability identification and support reuse assessment. We recovered the PLA of 15 open source SPL projects using the PLAR, a tool that supports PLA recovery and assessment based on information extracted from SPL products' source code. For each project, reuse assessment was supported by existing reuse metrics. The yielded results revealed that the number of products used in PLA recovery affected the variability identification, and the number of optional features affected the components reuse rate. These findings suggest that a minimum set of representative products should be identified and selected for PLA recovery, and the component reuse rate is a candidate metric for SPL reuse assessment.},
booktitle = {Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse},
articleno = {12},
numpages = {10},
keywords = {product line architecture, product line architecture recovery, software product lines, variability},
location = {Fortaleza, Cear\'{a}, Brazil},
series = {SBCARS '17}
}

@inproceedings{10.1145/1629716.1629720,
author = {Chae, Wonseok and Blume, Matthias},
title = {Language support for feature-oriented product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629720},
doi = {10.1145/1629716.1629720},
abstract = {Product line engineering is an emerging paradigm of developing a family of products. While product line analysis and design mainly focus on reasoning about commonality and variability of family members, product line implementation gives its attention to mechanisms of managing variability. In many cases, however, product line methods do not impose any specific synthesis mechanisms on product line implementation, so implementation details are left to developers. In our previous work, we adopted feature-oriented product line engineering to build a family of compilers and managed variations using the Standard ML module system. We demonstrated the applicability of this module system to product line implementation. Although we have benefited from the product line engineering paradigm, it mostly served us as a design paradigm to change the way we think about a set of closely related compilers, not to change the way we build them. The problem was that Standard ML did not fully realize this paradigm at the code level, which caused some difficulties when we were developing a set of compilers.In this paper, we address such issues with a language-based solution. MLPolyR is our choice of an implementation language. It supports three different programming styles. First, its first-class cases facilitate composable extensions at the expression levels. Second, its module language provides extensible and parameterized modules, which make large-scale extensible programming possible. Third, its macro system simplifies specification and composition of feature related code. We will show how the combination of these language features work together to facilitate the product line engineering paradigm.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {3–10},
numpages = {8},
keywords = {feature-oriented programming, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/2739482.2764650,
author = {Karimpour, Reza and Ruhe, Guenther},
title = {A Search Based Approach Towards Robust Optimization in Software Product Line Scoping},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764650},
doi = {10.1145/2739482.2764650},
abstract = {Software product line (SPL) scoping is important for planning upfront investment. One challenge with scoping comes from inaccuracies in estimated parameters and uncertainty in environment. In this paper, a method to incorporate uncertainty in SPL scoping optimization and its application to generate robust solutions is proposed. We model scoping optimization as a multi-objective problem with profit and stability as heuristics. To evaluate our proposal, a number of experiments are conducted. Analysis of results show that both performance stability and feasibility stability were improved providing the product line manager enhanced decision-making support.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1415–1416},
numpages = {2},
keywords = {multi-objective, robust optimization, software product line portfolio scoping, uncertainty},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {configuration management, microservice platforms, product families, technologies and concepts, variability management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Delta Modeling, Software Evolution, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/1808937.1808942,
author = {Estublier, Jacky and Dieng, Idrissa A. and Leveque, Thomas},
title = {Software product line evolution: the Selecta system},
year = {2010},
isbn = {9781605589688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808937.1808942},
doi = {10.1145/1808937.1808942},
abstract = {The current technology gives little room for the different kinds of evolution needed for any software product line (SPL): evolution of the associated engineering environment, evolution of the market and SPL scope, evolution of the products and variability. The paper describes how these different evolution needs are addressed in the CADSE and Selecta systems. The solution we propose uses metamodeling and generation for the engineering environment evolution, composition for scope and market evolution, a component database and a selection language for the product and variability evolution. The paper presents the Selecta system and shortly discusses the experience.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Product Line Approaches in Software Engineering},
pages = {32–39},
numpages = {8},
keywords = {IDE, evolution, product families, product lines, software environments},
location = {Cape Town, South Africa},
series = {PLEASE '10}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {asset health, extensibility, industrial analytics, interoperability, knowledge, performance, reusability, software product line},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.scico.2013.10.010,
author = {Sousa Ferreira, Gabriel Coutinho and Gaia, Felipe Nunes and Figueiredo, Eduardo and De Almeida Maia, Marcelo},
title = {On the use of feature-oriented programming for evolving software product lines - A comparative study},
year = {2014},
issue_date = {November, 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {93},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2013.10.010},
doi = {10.1016/j.scico.2013.10.010},
abstract = {Feature-oriented programming (FOP) is a programming technique based on composition mechanisms, called refinements. It is often assumed that feature-oriented programming is more suitable than other variability mechanisms for implementing Software Product Lines (SPLs). However, there is no empirical evidence to support this claim. In fact, recent research work found out that some composition mechanisms might degenerate the SPL modularity and stability. However, there is no study investigating these properties focusing on the FOP composition mechanisms. This paper presents quantitative and qualitative analysis of how feature modularity and change propagation behave in the context of two evolving SPLs, namely WebStore and MobileMedia. Quantitative data have been collected from the SPLs developed in three different variability mechanisms: FOP refinements, conditional compilation, and object-oriented design patterns. Our results suggest that FOP requires few changes in source code and a balanced number of added modules, providing better support than other techniques for non-intrusive insertions. Therefore, it adheres closer to the Open-Closed principle. Additionally, FOP seems to be more effective tackling modularity degeneration, by avoiding feature tangling and scattering in source code, than conditional compilation and design patterns. These results are based not only on the variability mechanism itself, but also on careful SPL design. However, the aforementioned results are weaker when the design needs to cope with crosscutting and fine-grained features.},
journal = {Sci. Comput. Program.},
month = nov,
pages = {65–85},
numpages = {21},
keywords = {Conditional compilation, Design patterns, Feature-oriented programming, Software product lines, Variability management}
}

@article{10.1016/j.infsof.2012.09.007,
author = {Guana, Victor and Correal, Dario},
title = {Improving software product line configuration: A quality attribute-driven approach},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.09.007},
doi = {10.1016/j.infsof.2012.09.007},
abstract = {Context: During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product's intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task. Objective: Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets. Method: We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product's functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years. Results: Through our SPL experiment, we established that our approach improved in nearly 98% the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools. Conclusion: We can conclude that our domain-specific modeling approach significantly improves the software architect's decision making when selecting the most suitable combinations of reusable components in the context of a SPL.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {541–562},
numpages = {22},
keywords = {Domain specific modeling, Model driven - software product lines, Quality evaluation, Sensitivity points, Software architecture, Variability management}
}

@article{10.1007/s10270-015-0471-3,
author = {Bonif\'{a}cio, Rodrigo and Borba, Paulo and Ferraz, Cristiano and Accioly, Paola},
title = {Empirical assessment of two approaches for specifying software product line use case scenarios},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0471-3},
doi = {10.1007/s10270-015-0471-3},
abstract = {Modularity benefits, including the independent maintenance and comprehension of individual modules, have been widely advocated. However, empirical assessments to investigate those benefits have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). In this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the benefits of modularity in the context of two approaches for specifying product line use case scenarios: PLUSS and MSVCM. The first uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario specifications. After evaluating these approaches through the specifications of several systems, we find out that MSVCM reduces feature scattering and improves scenario cohesion. These results suggest that evolving a product line specification using MSVCM requires only localized changes. On the other hand, the results of six experiments reveal that MSVCM requires more time to derive the product line specifications and, contrasting with the modularity results, reduces the time to evolve a product line specification only when the subjects have been well trained and are used to the task of evolving product line specifications.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {97–123},
numpages = {27},
keywords = {Experimentation in software engineering, Requirements engineering, Software modularity, Software product lines, Usage scenarios}
}

@article{10.1007/s10515-014-0160-4,
author = {Devine, Thomas and Goseva-Popstojanova, Katerina and Krishnan, Sandeep and Lutz, Robyn R.},
title = {Assessment and cross-product prediction of software product line quality: accounting for reuse across products, over multiple releases},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0160-4},
doi = {10.1007/s10515-014-0160-4},
abstract = {The goals of cross-product reuse in a software product line (SPL) are to mitigate production costs and improve the quality. In addition to reuse across products, due to the evolutionary development process, a SPL also exhibits reuse across releases. In this paper, we empirically explore how the two types of reuse--reuse across products and reuse across releases--affect the quality of a SPL and our ability to accurately predict fault proneness. We measure the quality in terms of post-release faults and consider different levels of reuse across products (i.e., common, high-reuse variation, low-reuse variation, and single-use packages), over multiple releases. Assessment results showed that quality improved for common, low-reuse variation, and single-use packages as they evolved across releases. Surprisingly, within each release, among preexisting (`old') packages, the cross-product reuse did not affect the change and fault proneness. Cross-product predictions based on pre-release data accurately ranked the packages according to their post-release faults and predicted the 20 % most faulty packages. The predictions benefited from data available for other products in the product line, with models producing better results (1) when making predictions on smaller products (consisting mostly of common packages) rather than on larger products and (2) when trained on larger products rather than on smaller products.},
journal = {Automated Software Engg.},
month = jun,
pages = {253–302},
numpages = {50},
keywords = {Assessment, Cross-product prediction, Cross-product reuse, Cross-release reuse, Fault proneness prediction, Longitudinal study, Software product lines}
}

@article{10.5555/2747015.2747184,
author = {da Silva, Ivonei Freitas and da Mota Silveira Neto, Paulo Anselmo and O'Leary, P\'{a}draig and de Almeida, Eduardo Santana and Meira, Silvio Romero de Lemos},
title = {Software product line scoping and requirements engineering in a small and medium-sized enterprise},
year = {2014},
issue_date = {February 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0164-1212},
abstract = {HighlightsWe described a detailed qualitative study on software product line scoping and requirements engineering.We examine weaknesses regarding the iterativeness, adaptability, and communication.Agile methods can mitigate the iterativeness, adaptability, and communication weaknesses. Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods.},
journal = {J. Syst. Softw.},
month = feb,
pages = {189–206},
numpages = {18},
keywords = {Agile methods, Requirements engineering, Software product line scoping}
}

@inproceedings{10.1145/3377930.3390215,
author = {Silva, Diego Fernandes da and Okada, Luiz Fernando and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Enhancing search-based product line design with crossover operators},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390215},
doi = {10.1145/3377930.3390215},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA designing has been formulated as a multi-objective optimization problem and successfully solved by a state-of-the-art search-based approach. However, the majority of empirical studies optimize PLA designs without applying one of the fundamental genetic operators: the crossover. An operator for PLA design, named Feature-driven Crossover, was proposed in a previous study. In spite of the promising results, this operator occasionally generated incomplete solutions. To overcome these limitations, this paper aims to enhance the search-based PLA design optimization by improving the Feature-driven Crossover and introducing a novel crossover operator specific for PLA design. The proposed operators were evaluated in two well-studied PLA designs, using three experimental configurations of NSGA-II in comparison with a baseline that uses only mutation operators. Empirical results show the usefulness and efficiency of the presented operators on reaching consistent solutions. We also observed that the two operators complement each other, leading to PLA design solutions with better feature modularization than the baseline experiment.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1250–1258},
numpages = {9},
keywords = {multi-objective evolutionary algorithm, recombination operators, software architecture, software product line},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1016/j.procs.2019.12.173,
author = {Chemingui, Houssem and Gam, Ines and Mazo, Ra\'{u}l and Salinesi, Camille and Ghezala, Henda Ben},
title = {Product Line Configuration Meets Process Mining},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {164},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.173},
doi = {10.1016/j.procs.2019.12.173},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {199–210},
numpages = {12},
keywords = {Product line engineering, configuration process, process mining, enhancing, configuration difficulties}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Controlled experimentation, Feature model, Maintainability, Quality attributes, Software prediction model, Software product line, Structural complexity}
}

@inproceedings{10.1145/2648511.2648527,
author = {Villela, Karina and Silva, Adeline and Vale, Tassio and de Almeida, Eduardo Santana},
title = {A survey on software variability management approaches},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648527},
doi = {10.1145/2648511.2648527},
abstract = {Variability Management (VM) is a key practice in the development of variant-rich systems. Over the years, attention has been paid to VM approaches adopted by traditional software product lines. The increasing demand for dynamic and highly configurable systems, however, calls for a closer look at the approaches used to develop these systems. We therefore conducted a survey with practitioners from organizations developing variant-rich systems in order to characterize the state of the practice. We also wanted to identify factors that might influence the adoption of specific VM approaches as well as the perception of problems/difficulties posed by those. We analyzed the answers of 31 respondents from thirteen countries and found that there is a correlation between the business domain and the adopted VM approaches. With regard to the problems/difficulties, the difficulty of assuring the quality of maintenance due to the explosion of dependencies was a major issue. This paper reports on relevant findings that could help companies to better understand their problems and researchers to design new/improved solutions.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {147–156},
numpages = {10},
keywords = {product line, state-of-the-practice, survey, variability, variability management},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {configuration, machine learning, multi-objective search, product line, rule mining},
location = {Berlin, Germany},
series = {GECCO '17}
}

@article{10.1016/j.infsof.2007.10.013,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {The software product line architecture: An empirical investigation of key process activities},
year = {2008},
issue_date = {October, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {50},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.10.013},
doi = {10.1016/j.infsof.2007.10.013},
abstract = {Software architecture has been a key area of concern in software industry due to its profound impact on the productivity and quality of software products. This is even more crucial in case of software product line, because it deals with the development of a line of products sharing common architecture and having controlled variability. The main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting a comprehensive empirical investigation covering a broad range of organizations currently involved in the business of software product lines. This is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge. The results of this investigation provide empirical evidence that software product line architecture process activities play a significant role in successfully developing and managing a software product line.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1098–1113},
numpages = {16},
keywords = {Domain engineering, Empirical study, Software architecture, Software engineering, Software product line}
}

@inproceedings{10.1007/978-3-642-33176-3_7,
author = {ter Beek, Maurice H. and Muccini, Henry and Pelliccione, Patrizio},
title = {Assume-guarantee testing of evolving software product line architectures},
year = {2012},
isbn = {9783642331756},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33176-3_7},
doi = {10.1007/978-3-642-33176-3_7},
abstract = {Despite some work on testing software product lines, maintaining the quality of products when a software product line evolves is still an open problem. In this paper, we propose a novel assume-guarantee testing approach as a solution to the following research question: how can we verify the correct functioning of products of an software product line when core components evolve? The underlying idea is to retest only some of the products that conform to the software product line architecture and to infer, using assume-guarantee reasoning, the correctness of the other products. Assume-guarantee reasoning moreover permits the retesting of only those components that are affected by the changes.},
booktitle = {Proceedings of the 4th International Conference on Software Engineering for Resilient Systems},
pages = {91–105},
numpages = {15},
keywords = {assume-guarantee testing, compositional verification, evolving software product lines, software testing},
location = {Pisa, Italy},
series = {SERENE'12}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {constraints and variability mining, machine learning, software product lines, software testing, variability modeling},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Feature Model Validation, Machine Learning, Natural Language Processing, Requirements Engineering, Software Product Line},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1007/s11219-020-09522-1,
author = {Bhushan, Megha and Negi, Arun and Samant, Piyush and Goel, Shivani and Kumar, Ajay},
title = {A classification and systematic review of product line feature model defects},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09522-1},
doi = {10.1007/s11219-020-09522-1},
abstract = {Product line (PL)-based development is a thriving research area to develop software-intensive systems. Feature models (FMs) facilitate derivation of valid products from a PL by managing commonalities and variabilities among software products. However, the researchers in academia as well as in the industries experience difficulties in quality assessment of FMs. The increasing complexity and size of FMs may lead to defects, which outweigh the benefits of PL. This paper provides a systematic literature review and key research issues related to the FM defects in PL. We derive a typology of FM defects according to their level of importance. The information on defects’ identification and explanations are provided with formalization. Further, corrective explanations are presented which incorporates various techniques used to fix defects with their implementation. This information would help software engineering community by enabling developers or modelers to find the types of defects and their causes and to choose an appropriate technique to fix defects in order to produce defect-free products from FMs, thereby enhancing the overall quality of PL-based development.},
journal = {Software Quality Journal},
month = dec,
pages = {1507–1550},
numpages = {44},
keywords = {Feature model, Software product line, Defect, Product line model, Quality}
}

@article{10.1016/j.infsof.2012.11.008,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Goseva-Popstojanova, Katerina and Dorman, Karin S.},
title = {Predicting failure-proneness in an evolving software product line},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.008},
doi = {10.1016/j.infsof.2012.11.008},
abstract = {ContextPrevious work by researchers on 3years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. ObjectiveThe work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. MethodThis investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. ResultsOur experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. ConclusionAs the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1479–1495},
numpages = {17},
keywords = {Change metrics, Failure-prone files, Post-release defects, Prediction, Reuse, Software product lines}
}

@inproceedings{10.1145/2791060.2791110,
author = {McVoy, Larry},
title = {Preliminary product line support in BitKeeper},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791110},
doi = {10.1145/2791060.2791110},
abstract = {One of the challenges of implementing a product line process is finding the appropriate tools for automation. One of our larger customers was implementing a product line process by-hand in a labor intensive and fragile way. We collaborated with them to evolve our distributed version control system, BitKeeper, into a tool that could handle their performance and product line requirements. The resulting product line generated several complex CPUs (around a billion transistors each).In this paper, we describe their by-hand process for producing different variations of a computer processor; we'll provide some background on the distributed version control system they were using; we'll describe the architectural changes implemented in BitKeeper for supporting product line work flows; we'll describe some of the changes we did to increase performance and provide some benchmark results comparing BitKeeper to Git, and we'll describe the work flow resulting from using the new architecture to replace their by-hand process.In the final section we'll discuss the current limitations of the existing tool, and describe how we plan on evolving it to overcome those limitations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {245–252},
numpages = {8},
keywords = {code reuse, configuration management, software product lines, version control},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1007/s10515-011-0099-7,
author = {Bagheri, Ebrahim and Ensan, Faezeh and Gasevic, Dragan},
title = {Decision support for the software product line domain engineering lifecycle},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0099-7},
doi = {10.1007/s10515-011-0099-7},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of the target domain and through the development of comprehensive and variability-covering feature models. The feature models developed within the software product line development process need to cover the relevant features and aspects of the target domain. In other words, the feature models should be elaborate representations of the feature space of that domain. Given that feature models, i.e., software product line feature models, are developed mostly by domain analysts by sifting through domain documentation, corporate records and transcribed interviews, the process is a cumbersome and error-prone one. In this paper, we propose a decision support platform that assists domain analysts throughout the domain engineering lifecycle by: (1) automatically performing natural language processing tasks over domain documents and identifying important information for the domain analysts such as the features and integrity constraints that exist in the domain documents; (2) providing a collaboration platform around the domain documents such that multiple domain analysts can collaborate with each other during the process using a Wiki; (3) formulating semantic links between domain terminology with external widely used ontologies such as WordNet in order to disambiguate the terms used in domain documents; and (4) developing traceability links between the unstructured information available in the domain documents and their formal counterparts within the formal feature model representations. Results obtained from our controlled experimentations show that the decision support platform is effective in increasing the performance of the domain analysts during the domain engineering lifecycle in terms of both the coverage and accuracy measures.},
journal = {Automated Software Engg.},
month = sep,
pages = {335–377},
numpages = {43},
keywords = {Domain engineering, Feature models, NLP model inference, Software product lines}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Variability management, Software product lines, Multiple-case study, Challenges}
}

@article{10.1016/j.jss.2019.05.001,
author = {Kicsi, Andr\'{a}s and Csuvik, Viktor and Vid\'{a}cs, L\'{a}szl\'{o} and Horv\'{a}th, Ferenc and Besz\'{e}des, \'{A}rp\'{a}d and Gyim\'{o}thy, Tibor and Kocsis, Ferenc},
title = {Feature analysis using information retrieval, community detection and structural analysis methods in product line adoption},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.05.001},
doi = {10.1016/j.jss.2019.05.001},
journal = {J. Syst. Softw.},
month = sep,
pages = {70–90},
numpages = {21},
keywords = {Software product line, Feature extraction, Information retrieval, Community detection}
}

@inproceedings{10.1145/3307630.3342384,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {MetricHaven: More than 23,000 Metrics for Measuring Quality Attributes of Software Product Lines},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342384},
doi = {10.1145/3307630.3342384},
abstract = {Variability-aware metrics are designed to measure qualitative aspects of software product lines. As we identified in a prior SLR [6], there exist already many metrics that address code or variability separately, while the combination of both has been less researched. MetricHaven fills this gap, as it extensively supports combining information from code files and variability models. Further, we also enable the combination of well established single system metrics with novel variability-aware metrics, going beyond existing variability-aware metrics. Our tool supports most prominent single system and variability-aware code metrics. We provide configuration support for already implemented metrics, resulting in 23,342 metric variations. Further, we present an abstract syntax tree developed for MetricHaven, that allows the realization of additional code metrics.Tool: https://github.com/KernelHaven/MetricHavenVideo: https://youtu.be/vPEmD5Sr6gM},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {25–28},
numpages = {4},
keywords = {SPL, feature models, implementation, metrics, software product lines, variability models},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461002.3473948,
author = {Xu, Hao and Baarir, Souheib and Ziadi, Tewfik and Hillah, Lom Messan and Essodaigui, Siham and Bossu, Yves},
title = {Optimisation for the product configuration system of Renault: towards an integration of symmetries},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473948},
doi = {10.1145/3461002.3473948},
abstract = {The problem of configuring model variability is widespread in many different domains. Renault, a leading french automobile manufacturer, has developed its technology internally to model vehicle diversity. This technology relies on the approach known as knowledge compilation. Since its inception, continuous progress has been made in the tool while monitoring the latest developments from the software field and academia. However, the growing number of vehicle models brings potential risks and higher requirements for the tool. This paper presents a short reminder of Renault's technology principles and the improvements we intend to achieve by analyzing and leveraging notable data features of Renault problem instances. In particular, the aim is to exploit symmetry properties.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {86–90},
numpages = {5},
keywords = {SAT, knowledge compilation, product line, symmetries},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1007/s11219-011-9156-5,
author = {Roos-Frantz, Fabricia and Benavides, David and Ruiz-Cort\'{e}s, Antonio and Heuer, Andr\'{e} and Lauenroth, Kim},
title = {Quality-aware analysis in product line engineering with the orthogonal variability model},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9156-5},
doi = {10.1007/s11219-011-9156-5},
abstract = {Software product line engineering is about producing a set of similar products in a certain domain. A variability model documents the variability amongst products in a product line. The specification of variability can be extended with quality information, such as measurable quality attributes (e.g., CPU and memory consumption) and constraints on these attributes (e.g., memory consumption should be in a range of values). However, the wrong use of constraints may cause anomalies in the specification which must be detected (e.g., the model could represent no products). Furthermore, based on such quality information, it is possible to carry out quality-aware analyses, i.e., the product line engineer may want to verify whether it is possible to build a product that satisfies a desired quality. The challenge for quality-aware specification and analysis is threefold. First, there should be a way to specify quality information in variability models. Second, it should be possible to detect anomalies in the variability specification associated with quality information. Third, there should be mechanisms to verify the variability model to extract useful information, such as the possibility to build a product that fulfils certain quality conditions (e.g., is there any product that requires less than 512 MB of memory?). In this article, we present an approach for quality-aware analysis in software product lines using the orthogonal variability model (OVM) to represent variability. We propose to map variability represented in the OVM associated with quality information to a constraint satisfaction problem and to use an off-the-shelf constraint programming solver to automatically perform the verification task. To illustrate our approach, we use a product line in the automotive domain which is an example that was created in a national project by a leading car company. We have developed a prototype tool named FaMa-OVM, which works as a proof of concepts. We were able to identify void models, dead and false optional elements, and check whether the product line example satisfies quality conditions.},
journal = {Software Quality Journal},
month = sep,
pages = {519–565},
numpages = {47},
keywords = {Automated analysis, Orthogonal variability model, Quality modelling, Quality-aware analysis, Software product lines}
}

@article{10.1016/j.jss.2007.12.797,
author = {Ajila, Samuel A. and Kaba, Ali B.},
title = {Evolution support mechanisms for software product line process},
year = {2008},
issue_date = {October, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {10},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.12.797},
doi = {10.1016/j.jss.2007.12.797},
abstract = {Software product family process evolution needs specific support for incremental change. Product line process evolution involves in addition to identifying new requirements the building of a meta-process describing the migration from the old process to the new one. This paper presents basic mechanisms to support software product line process evolution. These mechanisms share four strategies - change identification, change impact, change propagation, and change validation. It also examines three kinds of evolution processes - architecture, product line, and product. In addition, change management mechanisms are identified. Specifically we propose support mechanisms for static local entity evolution and complex entity evolution including transient evolution process. An evolution model prototype based on dependency relationships structure of the various product line artifacts is developed.},
journal = {J. Syst. Softw.},
month = oct,
pages = {1784–1801},
numpages = {18},
keywords = {Feature-based object oriented model, Meta-process, Product line architecture, Software development process, Software product line process evolution, Transient process, Use case modeling}
}

@article{10.1016/j.artmed.2021.102165,
author = {de Siqueira, Vilson Soares and Borges, Mois\'{e}s Marcos and Furtado, Rog\'{e}rio Gomes and Dourado, Colandy Nunes and da Costa, Ronaldo Martins},
title = {Artificial intelligence applied to support medical decisions for the automatic analysis of echocardiogram images: A systematic review},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102165},
doi = {10.1016/j.artmed.2021.102165},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {19},
keywords = {Echocardiogram, Echocardiography, Machine Learning, Deep Learning}
}

@inproceedings{10.1145/3382025.3414943,
author = {Th\"{u}m, Thomas},
title = {A BDD for Linux? the knowledge compilation challenge for variability},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414943},
doi = {10.1145/3382025.3414943},
abstract = {What is the number of valid configurations for Linux? How to generate uniform random samples for Linux? Can we create a binary decision diagram for Linux? It seems that the product-line community tries hard to answer such questions for Linux and other configurable systems. However, attempts are often not published due to the publication bias (i.e., unsuccessful attempts are not published). As a consequence, researchers keep trying by potentially spending redundant effort. The goal of this challenge is to guide research on these computationally complex problems and to foster the exchange between researchers and practitioners.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {16},
numpages = {6},
keywords = {artificial intelligence, binary decision diagrams, configurable system, decision models, feature models, knownledge compilation, product configuration, satisfiability solving, software configuration, software product line},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3377024.3380451,
author = {Bencomo, Nelly},
title = {Next steps in variability management due to autonomous behaviour and runtime learning},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3380451},
doi = {10.1145/3377024.3380451},
abstract = {One of the basic principles in product lines is to delay design decisions related to offered functionality and quality to later phases of the life cycle [25]. Instead of deciding on what system to develop in advance, a set of assets and a common reference architecture are specified and implemented during the Domain Engineering process. Later on, during Application Engineering, specific systems are developed to satisfy the requirements reusing the assets and architecture [16]. Traditionally, this is during the Application Engineering when delayed design decisions are solved. The realization of this delay relies heavily on the use of variability in the development of product lines and systems. However, as systems become more interconnected and diverse, software architects cannot easily foresee the software variants and the interconnections between components. Consequently, a generic a priori model is conceived to specify the system's dynamic behaviour and architecture. The corresponding design decisions are left to be solved at runtime [13].Surprisingly, few research initiatives have investigated variability models at runtime [9]. Further, they have been applied only at the level of goals and architecture, which contrasts to the needs claimed by the variability community, i.e., Software Product Lines (SPLC) and Dynamic Software Product Lines (DSPL) [2, 10, 14, 22]. Especially, the vision of DSPL with their ability to support runtime updates with virtually zero downtime for products of a software product line, denotes the obvious need of variability models being used at runtime to adapt the corresponding programs. A main challenge for dealing with runtime variability is that it should support a wide range of product customizations under various scenarios that might be unknown until the execution time, as new product variants can be identified only at runtime [10, 11]. Contemporary variability models face the challenge of representing runtime variability to therefore allow the modification of variation points during the system's execution, and underpin the automation of the system's reconfiguration [15]. The runtime representation of feature models (i.e. the runtime model of features) is required to automate the decision making [9].Software automation and adaptation techniques have traditionally required a priori models for the dynamic behaviour of systems [17]. With the uncertainty present in the scenarios involved, the a priori model is difficult to define [20, 23, 26]. Even if foreseen, its maintenance is labour-intensive and, due to architecture decay, it is also prone to get out-of-date. However, the use of models@runtime does not necessarily require defining the system's behaviour model beforehand. Instead, different techniques such as machine learning, or mining software component interactions from system execution traces can be used to build a model which is in turn used to analyze, plan, and execute adaptations [18], and synthesize emergent software on the fly [7].Another well-known problem posed by the uncertainty that characterize autonomous systems is that different stakeholders (e.g. end users, operators and even developers) may not understand them due to the emergent behaviour. In other words, the running system may surprise its customers and/or developers [4]. The lack of support for explanation in these cases may compromise the trust to stakeholders, who may eventually stop using a system [12, 24]. I speculate that variability models can offer great support for (i) explanation to understand the diversity of the causes and triggers of decisions during execution and their corresponding effects using traceability [5], and (ii) better understand the behaviour of the system and its environment.Further, an extension and potentially reframing of the techniques associated with variability management may be needed to help taming uncertainty and support explanation and understanding of the systems. The use of new techniques such as machine learning exacerbates the current situation. However, at the same time machine learning techniques can also help and be used, for example, to explore the variability space [1]. What can the community do to face the challenges associated?We need to meaningfully incorporate techniques from areas such as artificial intelligence, machine learning, optimization, planning, decision theory, and bio-inspired computing into our variability management techniques to provide explanation and management of the diversity of decisions, their causes and the effects associated. My own previous work has progressed [3, 5, 6, 8, 11, 12, 19, 21] to reflect what was discussed above.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {2},
numpages = {2},
keywords = {autonomous systems, dynamic software product lines, dynamic variability, machine learning, uncertainty, variability management},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3297280.3297479,
author = {Ne\v{s}i\'{c}, Damir and Nyberg, Mattias and Gallina, Barbara},
title = {Constructing product-line safety cases from contract-based specifications},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297479},
doi = {10.1145/3297280.3297479},
abstract = {Safety cases are used to argue that safety-critical systems satisfy the requirements that are determined to mitigate the potential hazards in the systems operating environment. Although typically a manual task, safety cases have been successfully created for systems without many configuration options. However, in highly configurable systems, typically developed as a Product Line (PL), arguing about each possible configuration, and ensuring the completeness of the safety case are still open research problems. This paper presents a novel and general approach, based on Contract-Based Specification (CBS), for the construction of a safety case for an arbitrary PL. Starting from a general CBS framework, we present a PL extensions that allows expressing configurable systems and preserves the properties of the original CBS framework. Then, we define the transformation from arbitrary PL models, created using extended CBS framework, to a safety case argumentation-structure, expressed using the Goal Structuring Notation. Finally, the approach is exemplified on a simplified, but real, and currently produced system by Scania CV AB.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2022–2031},
numpages = {10},
keywords = {contract-based specification, product line engineering, safety case},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3461001.3474452,
author = {He\ss{}, Tobias and Sundermann, Chico and Th\"{u}m, Thomas},
title = {On the scalability of building binary decision diagrams for current feature models},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3474452},
doi = {10.1145/3461001.3474452},
abstract = {Binary decision diagrams (BDD) have been proposed for numerous product-line analyses. These analyses typically exploit properties unique to decision diagrams, such as negation in constant time and space. Furthermore, the existence of a BDD representing the configuration space of a product line removes the need to employ SAT or #SAT solvers for their analysis. Recent work has shown that the performance of state-of-the-art BDD libraries is significantly lower than previously reported and hypothesized. In this work, we provide an assessment of the state-of-the-art of BDD scalability in this domain and explain why previous results on the scalability of BDDs do not apply to more recent product-line instances.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {131–135},
numpages = {5},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1016/j.infsof.2006.05.004,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {Managing the business of software product line: An empirical investigation of key business factors},
year = {2007},
issue_date = {February, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.05.004},
doi = {10.1016/j.infsof.2006.05.004},
abstract = {Business has been highlighted as a one of the critical dimensions of software product line engineering. This paper's main contribution is to increase the understanding of the influence of key business factors by showing empirically that they play an imperative role in managing a successful software product line. A quantitative survey of software organizations currently involved in the business of developing software product lines over a wide range of operations, including consumer electronics, telecommunications, avionics, and information technology, was designed to test the conceptual model and hypotheses of the study. This is the first study to demonstrate the relationships between the key business factors and software product lines. The results provide evidence that organizations in the business of software product line development have to cope with multiple key business factors to improve the overall performance of the business, in addition to their efforts in software development. The conclusions of this investigation reinforce current perceptions of the significance of key business factors in successful software product line business.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {194–208},
numpages = {15},
keywords = {Key business factor, Management, Marketing strategy, Software engineering economics, Software product line, Strategic planning}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {artificial intelligence, configuration, feature model, planning techniques, software product line engineering},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1287/opre.2018.1825,
author = {Bertsimas, Dimitris and Mi\v{s}i\'{c}, Velibor V.},
title = {Exact First-Choice Product Line Optimization},
year = {2019},
issue_date = {May-June 2019},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {67},
number = {3},
issn = {0030-364X},
url = {https://doi.org/10.1287/opre.2018.1825},
doi = {10.1287/opre.2018.1825},
abstract = {Which products should a firm offer based on its customers’ preferences? This is the question posed in the problem of product line design, a well-studied and notoriously difficult problem that is central in marketing science. In “Exact First-Choice Product Line Optimization” by Dimitris Bertsimas and Velibor V. Mi\v{s}i\'{c}, the authors propose a new approach for solving this problem when segments of customers choose products according to a ranking. They propose a new mixed-integer optimization model of the problem, which they show to be tighter than prior formulations, and a solution approach based on Benders decomposition, which exploits the surprising fact that the subproblem can be solved efficiently for both integer and fractional master solutions. A well-known product line instance based on a conjoint data set of over 3,000 products and 300 respondents, which required a week of computation time to solve in prior work, is solved by the authors’ approach in just over 10 minutes.A fundamental problem faced by firms is that of product line design: given a set of candidate products that may be offered to a collection of customers, what subset of those products should be offered to maximize the profit that is realized when customers make purchases according to their preferences? In this paper, we consider the product line design problem when customers choose according to a first-choice rule and present a new mixed-integer optimization formulation of the problem. We theoretically analyze the strength of our formulation and show that it is stronger than alternative formulations that have been proposed in the literature, thus contributing to a unified understanding of the different formulations for this problem. We also present a novel solution approach for solving our formulation at scale, based on Benders decomposition, which exploits the surprising fact that Benders cuts for both the relaxation and the integer problem can be generated in a computationally efficient manner. We demonstrate the value of our formulation and Benders decomposition approach through two sets of experiments. In the first, we use synthetic instances to show that our formulation is computationally tractable and can be solved an order of magnitude faster for small- to medium-scale instances than the alternate, previously proposed formulations. In the second, we consider a previously studied product line design instance based on a real conjoint data set, involving over 3,000 candidate products and over 300 respondents. We show that this problem, which required a week of computation time to solve in prior work, is solved by our approach to full optimality in approximately 10 minutes.The e-companion is available at .},
journal = {Oper. Res.},
month = may,
pages = {651–670},
numpages = {20},
keywords = {product line design, first-choice models, mixed-integer optimization, Benders decomposition}
}

@inproceedings{10.5555/1753235.1753247,
author = {Chen, Lianping and Ali Babar, Muhammad and Ali, Nour},
title = {Variability management in software product lines: a systematic review},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Variability Management (VM) in Software Product Line (SPL) is a key activity that usually affects the degree to which a SPL is successful. SPL community has spent huge amount of resources on developing various approaches to dealing with variability related challenges over the last decade. To provide an overview of different aspects of the proposed VM approaches, we carried out a systematic literature review of the papers reporting VM in SPL. This paper presents and discusses the findings from this systematic literature review. The results reveal the chronological backgrounds of various approaches over the history of VM research, and summarize the key issues that drove the evolution of different approaches. This study has also identified several gaps that need to be filled by future efforts in this line of research.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {81–90},
numpages = {10},
keywords = {software product lines, systematic reviews, variability management},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2857546.2857608,
author = {Rahmat, Azizah and Kassim, Suzana and Selamat, Mohd Hasan and Hassan, Sa'adah},
title = {Actor in Multi Product Line},
year = {2016},
isbn = {9781450341424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857546.2857608},
doi = {10.1145/2857546.2857608},
abstract = {Software product line (SPL) involved variability modeling in domain engineering that will be matched to the respected application engineering. Several researches existed within the scope of mapping from reference architecture (RA) in domain engineering to system architecture in application engineering within the same domain. However, the mapping of cross domain RA or Multi Product Line (MPL) required more systematic mapping due to the several participating product line architecture (PLA) that will further instantiated to specific system architecture. The objective of this paper was to propose an actor-oriented approach in the mapping process of reference architecture, product line architecture and system architecture of MPL. Since the reference architecture consisted of several components, the scope of this research was within the functional decomposition or source code level. The experiment was involving the runtime behavior of the java code. The code with actor-oriented approach had shown the least amount of time taken to complete the main method compared to the non-actor-oriented approach. In conclusion, actor-oriented approach performs better performance in the mapping of reference architecture to product line architecture and system architecture. For future work, the consistency of the mapping will be evaluated.},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
articleno = {61},
numpages = {8},
keywords = {reference architecture, multi product line, cross-domain reference architecture, actor, Software product line},
location = {Danang, Viet Nam},
series = {IMCOM '16}
}

@inproceedings{10.1145/1595808.1595819,
author = {Anastasopoulos, Michail},
title = {Increasing efficiency and effectiveness of software product line evolution: an infrastructure on top of configuration management},
year = {2009},
isbn = {9781605586786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595808.1595819},
doi = {10.1145/1595808.1595819},
abstract = {Software Product Line Engineering entails the strategic development of software assets that are to be reused many times across the members of a product line. Assuring that the investment in reuse holds over time is an important requirement in this case. To that end it is necessary that evolution is carefully managed: Changes in reusable assets and their customized instances need to be tracked and propagated efficiently. Configuration Management is a mature discipline for that purpose. However traditional configuration management does not address product line evolution scenarios explicitly. Over time this can lead to great evolution management effort. This paper presents an infrastructure - in particular its validation - that sits on top of traditional configuration management and is tailored to evolution scenarios in Product Line Engineering. The result is a reduction of effort and an increase of correctness},
booktitle = {Proceedings of the Joint International and Annual ERCIM Workshops on Principles of Software Evolution (IWPSE) and Software Evolution (Evol) Workshops},
pages = {47–56},
numpages = {10},
keywords = {software product lines, evolution},
location = {Amsterdam, The Netherlands},
series = {IWPSE-Evol '09}
}

@article{10.1016/j.jss.2007.10.025,
author = {Hanssen, Geir K. and F\'{\i}gri, Tor E.},
title = {Process fusion: An industrial case study on agile software product line engineering},
year = {2008},
issue_date = {June, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {6},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.10.025},
doi = {10.1016/j.jss.2007.10.025},
abstract = {This paper presents a case study of a software product company that has successfully integrated practices from software product line engineering and agile software development. We show how practices from the two fields support the company's strategic and tactical ambitions, respectively. We also discuss how the company integrates strategic, tactical and operational processes to optimize collaboration and consequently improve its ability to meet market needs, opportunities and challenges. The findings from this study are relevant to software product companies seeking ways to balance agility and product management. The findings also contribute to research on industrializing software engineering.},
journal = {J. Syst. Softw.},
month = jun,
pages = {843–854},
numpages = {12},
keywords = {Agile software development, Software product development, Software product line engineering, Software product management}
}

@inproceedings{10.1145/2020390.2020397,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Are change metrics good predictors for an evolving software product line?},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020397},
doi = {10.1145/2020390.2020397},
abstract = {Background: Previous research on three years of early data for an Eclipse product identified some predictors of failure-prone files that work well for that data set. Additionally, Eclipse has been used to explore characteristics of product line software in previous research.Aims: To assess whether change metrics are good predictors of failure-prone files over time for the family of products in the evolving Eclipse product line.Method: We repeat, to the extent possible, the decision tree portion of the prior study to assess our ability to replicate the method, and then extend it by including four more recent years of data. We compare the most prominent predictors with the previous study's results. We then look at the data for three additional Eclipse products as they evolved over time. We explore whether the set of good predictors change over time for one product and whether the set differs among products.Results: We find that change metrics are consistently good and incrementally better predictors across the evolving products in Eclipse. There is also some consistency regarding which change metrics are the best predictors.Conclusion: Change metrics are good predictors for failure-prone files for the Eclipse product line. A small subset of these change metrics is fairly stable and consistent across products and releases.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {7},
numpages = {10},
keywords = {change metrics, failure-prone files, post-release defects, prediction, reuse, software product lines},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1016/j.jss.2018.05.069,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-adaptation of service compositions through product line reconfiguration},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.05.069},
doi = {10.1016/j.jss.2018.05.069},
journal = {J. Syst. Softw.},
month = oct,
pages = {84–105},
numpages = {22},
keywords = {Service composition, Feature model, Software product lines, Self adaptation}
}

@inproceedings{10.1145/3382025.3414952,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Gasca, Rafael M. and Carmona-Fombella, Jose Antonio and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa},
title = {AMADEUS: towards the AutoMAteD secUrity teSting},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414952},
doi = {10.1145/3382025.3414952},
abstract = {The proper configuration of systems has become a fundamental factor to avoid cybersecurity risks. Thereby, the analysis of cybersecurity vulnerabilities is a mandatory task, but the number of vulnerabilities and system configurations that can be threatened is extremely high. In this paper, we propose a method that uses software product line techniques to analyse the vulnerable configuration of the systems. We propose a solution, entitled AMADEUS, to enable and support the automatic analysis and testing of cybersecurity vulnerabilities of configuration systems based on feature models. AMADEUS is a holistic solution that is able to automate the analysis of the specific infrastructures in the organisations, the existing vulnerabilities, and the possible configurations extracted from the vulnerability repositories. By using this information, AMADEUS generates automatically the feature models, that are used for reasoning capabilities to extract knowledge, such as to determine attack vectors with certain features. AMADEUS has been validated by demonstrating the capacities of feature models to support the threat scenario, in which a wide variety of vulnerabilities extracted from a real repository are involved. Furthermore, we open the door to new applications where software product line engineering and cybersecurity can be empowered.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {11},
numpages = {12},
keywords = {vulnerable configuration, vulnerabilities, testing, reasoning, pentesting, feature model, cybersecurity},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {software product line, optimisation, mobile edge computing, mobile cloud computing, latency, energy efficiency},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414968,
author = {Li, Yang and Schulze, Sandro and Scherrebeck, Helene Hvidegaard and Fogdal, Thomas Sorensen},
title = {Automated extraction of domain knowledge in practice: the case of feature extraction from requirements at danfoss},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414968},
doi = {10.1145/3382025.3414968},
abstract = {Software product line supports structured reuse of software artifacts in order to realize the maintenance and evolution of the typically large number of variants, which promotes the industrialization of software development, especially for software-intensive products. However, for a legacy system, it is non-trivial to gain information about commonalities and differences of the variants. Meanwhile, software requirements specifications as the initial artifacts can be used to achieve this information to generate a domain model. Unfortunately, manually analyzing these requirements is time-consuming and inefficient. To address this problem, we explored the usage of feature extraction techniques to automatically extract domain knowledge from requirements to assist domain engineers. In detail, we applied Doc2Vec and a clustering algorithm to process the requirements for achieving the initial feature tree. Moreover, we utilized key words/phrases extraction techniques to provide key information to domain engineers for further analyzing the extraction results. In particular, we developed a GUI to support the extraction process. The empirical evaluation indicates that most of the extracted features and terms are beneficial to improve the process of feature extraction.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {4},
numpages = {11},
keywords = {software product lines, reverse engineering, requirement documents, feature extraction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3236405.3236426,
author = {Belarbi, Maouaheb},
title = {A methodological framework to enable the generation of code from DSML in SPL},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236426},
doi = {10.1145/3236405.3236426},
abstract = {Software Product Line has acquired a significant momentum at the end of the 1990ies since it allows the production of variable software systems corresponding to the same domain portfolio. The effectiveness of the derivation process depends on how well variability is defined and implemented which is a crucial topic area that was addressed among two essential trends: On the one hand, starting from Domain Specific Modelling Language to express domain requirements and automate the code generation with Model-Driven Engineering techniques and on the second hand, exploiting the soar of variability mechanisms.In this context, the current research presents a method that unifies the two aforementioned approaches to cover the overall strategies by defining a framework that allows a better code generation in terms of documentation, maintainability, rapidity,etc. The starting point is the usage of the Domain Specific Modelling Language to represent the stakeholders requirements. Then, the resulting meta-model will be converted into one our several Feature Diagrams on which variability mechanisms can be applied to generate all the family products.A preliminary experiment has been undertaken to design the methodology of the proposed software factory in a meta-model. The validation task was evaluated with an academic use case called HandiWeb developed to facilitate handicap persons access to the internet. The first results allow us to put the hand on the key challenges that must be resolved by the proposed methodology.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {64–71},
numpages = {8},
keywords = {variability, software factory, methodology, SPL, DSML},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/1062455.1062551,
author = {Verlage, Martin and Kiesgen, Thomas},
title = {Five years of product line engineering in a small company},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062551},
doi = {10.1145/1062455.1062551},
abstract = {In 1999, a new team at MARKET MAKER Software AG began to develop a software product line for managing and displaying stock market data and financial market news. The basic idea was to use web technology in all applications for delivering services to customers. It soon turned out that the company had to change both the processes and the organization. This report summarizes the changes made and the lessons learned over the past five years, when the product line idea was introduced into a small company which faced the pressure to quickly market the first product line instances.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {534–543},
numpages = {10},
keywords = {project management, product line engineering, experience report, SME},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@inproceedings{10.1145/3461001.3471142,
author = {Gu\'{e}gain, \'{E}douard and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {On reducing the energy consumption of software product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471142},
doi = {10.1145/3461001.3471142},
abstract = {Along the last decade, several studies considered green software design as a key development concern to improve the energy efficiency of software. Yet, few techniques address this concern for Software Product Lines (SPL). In this paper, we therefore introduce two approaches to measure and reduce the energy consumption of a SPL by analyzing a limited set of products sampled from this SPL. While the first approach relies on the analysis of individual feature consumptions, the second one takes feature interactions into account to better mitigate energy consumption of resulting products.Our experimental results on a real-world SPL indicate that both approaches succeed to produce significant energy improvements on a large number of products, while consumption data was modeled from a small set of sampled products. Furthermore, we show that taking feature interactions into account leads to more products improved with higher energy savings per product.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {89–99},
numpages = {11},
keywords = {consumption, energy, measurement, mitigation, software product lines},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414973,
author = {Schlie, Alexander and Kn\"{u}ppel, Alexander and Seidl, Christoph and Schaefer, Ina},
title = {Incremental feature model synthesis for clone-and-own software systems in MATLAB/Simulink},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414973},
doi = {10.1145/3382025.3414973},
abstract = {Families of related MATLAB/Simulink systems commonly emerge ad hoc using clone-and-own practices. Extractively migrating systems towards a software product line (SPL) can be a remedy. A feature model (FM) represents all potential configurations of an SPL, ideally, in non-technical domain terms. However, yielding a sensible FM from automated synthesis remains a major challenge due to domain knowledge being a prerequisite for features to be adequate abstractions. In incremental reverse engineering, subsequent generation of FMs may further overwrite changes and design decisions made during previous manual FM refinement.In this paper, we propose an approach to largely automate the synthesis of a suitable FM from a set of cloned MATLAB/Simulink models as part of reverse engineering an SPL. We fully automate the extraction of an initial, i.e., a technical, FM that closely aligns with realization artifacts and their variability, and further provide operations to manually refine it to incorporate domain knowledge. Most importantly, we provide concepts to capture such operations and to replay them on a structurally different technical FM stemming from a subsequent reverse engineering increment that included further systems of the portfolio. We further provide an implementation and demonstrate the feasibility of our approach using two MATLAB/Simulink data sets from the automotive domain.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {7},
numpages = {12},
keywords = {variability, synthesis, refinement, mapping, individual, incremental, feature model, clone-and-own, MATLAB/Simulink, 150% model},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2815782.2815799,
author = {Schaefer, Ina and Seidl, Christoph and Cleophas, Loek and Watson, Bruce W.},
title = {SPLicing TABASCO: Custom-Tailored Software Product Line Variants from Taxonomy-Based Toolkits},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815799},
doi = {10.1145/2815782.2815799},
abstract = {Taxonomy-Based Software Construction (TABASCO) applies extensive domain analyses to create conceptual hierarchies of algorithmic domains. Those are used as basis for the implementation of software toolkits. The monolithic structure of TABASCO-based toolkits restricts their adoption on resource-constrained or special-purpose devices. In this paper, we address this problem by applying Software Product Line (SPL) techniques to TABASCO-based toolkits: We use software taxonomies as input to creating a conceptual representation of variability as feature models of an SPL. We apply the variability realization mechanism delta modeling to transform realization artifacts, such as source code, to only contain elements for a particular selection of features. Our method is suitable for proactive, reactive and extractive SPL development so that it supports a seamless adoption and evolution of an SPL approach for TABASCO-based toolkits. We demonstrate the feasibility of the method with three case studies by proactively, reactively and extractively transforming TABASCO-based toolkits to SPLs, which allow derivation of variants with custom-tailored functionality.},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {34},
numpages = {10},
keywords = {Taxonomy-Based Software Construction (TABASCO) toolkit, Software Product Line (SPL) adoption},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {variability model, software product lines, sampling, configurable systems, benchmark, SAT},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1016/j.infsof.2012.02.005,
author = {Thurimella, Anil Kumar and Bruegge, Bernd},
title = {Issue-based variability management},
year = {2012},
issue_date = {September, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.02.005},
doi = {10.1016/j.infsof.2012.02.005},
abstract = {Context: Variability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions. Objective: Decision-making should be supported to identify variability in domain engineering and to resolve variation points in application engineering. We capture the rationale behind variability management decisions. The captured rationale information is useful to evaluate future changes of variability models as well as to handle future instantiations of variation points. We claim that maintaining rationale will enhance the longevity of variability models. Furthermore, decisions should be performed using a formal communication between domain engineering and application engineering. Method: We initiate the novel area of issue-based variability management (IVM) by extending variability management with rationale management. The key contributions of this paper are: (i) an issue-based variability management methodology (IVMM), which combines questions, options and criteria (QOC) and a specific variability approach; (ii) a meta-model for IVMM and a process for variability management and (iii) a tool for the methodology, which was developed by extending an open source rationale management tool. Results: Rationale approaches (e.g. questions, options and criteria) guide distributed stakeholders when selecting choices for instantiating variation points. Similarly, rationale approaches also aid the elicitation of variability and the evaluation of changes. The rationale captured within the decision-making process can be reused to perform future decisions on variability. Conclusion: IVMM was evaluated comparatively based on an experimental survey, which provided evidence that IVMM is more effective than a variability modeling approach that does not use issues.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {933–950},
numpages = {18},
keywords = {Empirical software engineering, Product line engineering, Rationale management, Requirements engineering}
}

@inproceedings{10.1145/3461001.3473065,
author = {Michelon, Gabriela K. and Sotto-Mayor, Bruno and Martinez, Jabier and Arrieta, Aitor and Abreu, Rui and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Spectrum-based feature localization: a case study using ArgoUML},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473065},
doi = {10.1145/3461001.3473065},
abstract = {Feature localization (FL) is a basic activity in re-engineering legacy systems into software product lines. In this work, we explore the use of the Spectrum-based localization technique for this task. This technique is traditionally used for fault localization but with practical applications in other tasks like the dynamic FL approach that we propose. The ArgoUML SPL benchmark is used as a case study and we compare it with a previous hybrid (static and dynamic) approach from which we reuse the manual and testing execution traces of the features. We conclude that it is feasible and sound to use the Spectrum-based approach providing promising results in the benchmark metrics.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {126–130},
numpages = {5},
keywords = {spectrum-based localization, dynamic feature localization, ArgoUML SPL benchmark},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2491627.2491631,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: a case study in the telecommunication domain},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491631},
doi = {10.1145/2491627.2491631},
abstract = {In the research on software product lines, product variants typically differ by their functionality, and quality attributes are more or less similar across products. To accumulate empirical evidence, this paper presents a descriptive case study of performance variability in a software product line of mobile network base stations. The goal is to study the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The results highlight that the evolution of customer needs motivates performance variability; performance variability can be realized either with software or hardware variability strategy, with the latter often being prevailing; and the software strategy can be kept focused by downgrading performance.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {32–41},
numpages = {10},
keywords = {variability, software product line, case study, architecture},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3383219.3383229,
author = {Li, Yang and Schulze, Sandro and Xu, Jiahua},
title = {Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383229},
doi = {10.1145/3383219.3383229},
abstract = {In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Software Product Lines, Requirement Documents, Feature Terms Identification, Feature Extraction},
location = {Trondheim, Norway},
series = {EASE '20}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Configuration Process, Decision Process, Economic Value, Software Product Line, Utility Elicitation}
}

@inproceedings{10.1145/3461002.3473074,
author = {Fantechi, Alessandro and Gnesi, Stefania and Livi, Samuele and Semini, Laura},
title = {A spaCy-based tool for extracting variability from NL requirements},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473074},
doi = {10.1145/3461002.3473074},
abstract = {In previous work, we have shown that ambiguity detection in requirements can also be used as a way to capture latent aspects of variability. Natural Language Processing (NLP) tools have been used for a lexical analysis aimed at ambiguity indicators detection, and we have studied the necessary adaptations to those tools for pointing at potential variability, essentially by adding specific dictionaries for variability. We have identified also some syntactic rules able to detect potential variability, such as disjunction between nouns or pairs of indicators in a subordinate proposition. This paper describes a new prototype NLP tool, based on the spaCy library, specifically designed to detect variability. The prototype is shown to preserve the same recall exhibited by previously used lexical tools, with a higher precision.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {32–35},
numpages = {4},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1287/opre.2016.1546,
author = {Bertsimas, Dimitris and Mi\v{s}i\'{c}, Velibor V.},
title = {Robust Product Line Design},
year = {2017},
issue_date = {January-February 2017},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {65},
number = {1},
issn = {0030-364X},
url = {https://doi.org/10.1287/opre.2016.1546},
doi = {10.1287/opre.2016.1546},
abstract = {The majority of approaches to product line design that have been proposed by marketing scientists assume that the underlying choice model that describes how the customer population will respond to a new product line is known precisely. In reality, however, marketers do not precisely know how the customer population will respond and can only obtain an estimate of the choice model from limited conjoint data. In this paper, we propose a new type of optimization approach for product line design under uncertainty. Our approach is based on the paradigm of robust optimization where, rather than optimizing the expected revenue with respect to a single model, one optimizes the worst-case expected revenue with respect to an uncertainty set of models. This framework allows us to account for parameter uncertainty, when we may be confident about the type of model structure but not about the values of the parameters, and structural uncertainty, when we may not even be confident about the right model structure to use to describe the customer population. Through computational experiments with a real conjoint data set, we demonstrate the benefits of our approach in addressing parameter and structural uncertainty. With regard to parameter uncertainty, we show that product lines designed without accounting for parameter uncertainty are fragile and can experience worst-case revenue losses as high as 23%, and that the robust product line can significantly outperform the nominal product line in the worst case, with relative improvements of up to 14%. With regard to structural uncertainty, we similarly show that product lines that are designed for a single model structure can be highly suboptimal under other structures (worst-case losses of up to 37%), while a product line that optimizes against the worst of a set of structurally distinct models can outperform single model product lines by as much as 55% in the worst case and can guarantee good aggregate performance over structurally distinct models.},
journal = {Oper. Res.},
month = feb,
pages = {19–37},
numpages = {19},
keywords = {product line design, robust optimization, parameter uncertainty, structural uncertainty, model uncertainty}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {variability model, system evolution, fault, automatic repair},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {variability extraction, software product lines, reverse engineering, requirement documents, feature identification},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.4018/IJWSR.2019010103,
author = {Sun, Chang-ai and Wang, Zhen and Wang, Ke and Xue, Tieheng and Aiello, Marco},
title = {Adaptive BPEL Service Compositions via Variability Management: A Methodology and Supporting Platform},
year = {2019},
issue_date = {January 2019},
publisher = {IGI Global},
address = {USA},
volume = {16},
number = {1},
issn = {1545-7362},
url = {https://doi.org/10.4018/IJWSR.2019010103},
doi = {10.4018/IJWSR.2019010103},
abstract = {Service-Oriented Architectures are a popular development paradigm to enable distributed applications constructed from independent web services. When coordinated, web services are an infrastructure to fulfill dynamic and vertical integration of business. They may face frequent changes of both requirements and execution environments. Static and predefined service compositions using business process execution language BPEL are not able to cater for such rapid and unpredictable context shifts. The authors propose a variability management-based adaptive and configurable service composition approach that treats changes as first-class citizens and consists of identifying, expressing, realizing, and managing changes of service compositions. The proposed approach is realized with a language called VxBPEL to support variability in service compositions and a platform for design, execution, analysis, and maintenance of VxBPEL-based service compositions. Four case studies validate the feasibility of the proposed approach while exhibiting good performance of the supporting platform.},
journal = {Int. J. Web Serv. Res.},
month = jan,
pages = {37–69},
numpages = {33},
keywords = {Variability Management, Service Oriented Architectures, Service Composition, Business Process Execution Language, Adaptive Systems}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {sampling evalutaion, sampling, product lines},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1016/j.cl.2018.01.003,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {Personalized recommender systems for product-line configuration processes},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.003},
doi = {10.1016/j.cl.2018.01.003},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {451–471},
numpages = {21},
keywords = {Product lines, Feature model, Product-line configuration, Recommender systems, Personalized recommendations}
}

@inproceedings{10.1145/3422392.3422498,
author = {Freire, Willian Marques and Massago, Mamoru and Zavadski, Arthur Cattaneo and Malachini, Aline Maria and Amaral, Miotto and Colanzi, Thelma Elita},
title = {OPLA-Tool v2.0: a Tool for Product Line Architecture Design Optimization},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422498},
doi = {10.1145/3422392.3422498},
abstract = {The Multi-objective Optimization Approach for Product Line Architecture Design (MOA4PLA) is the seminal approach that successfully optimizes Product Line Architecture (PLA) design using search algorithms. The tool named OPLA-Tool was developed in order to automate the use of MOA4PLA. Over time, the customization of the tool to suit the needs of new research and application scenarios led to several problems. The main problems identified in the original version of OPLA-Tool are environment configuration, maintainability and usability problems, and PLA design modeling and visualization. Such problems motivated the development of a new version of this tool: OPLA-Tool v2.0, presented in this work. In this version, those problems were solved by the source code refactoring, migration to a web-based graphical user interface (GUI) and inclusion of a new support tool for PLA modeling and visualization. Furthermore, OPLA-Tool v2.0 has new functionalities, such as new objective functions, new search operators, intelligent interaction with users during the optimization process, multi-user authentication and simultaneous execution of several experiments to PLA optimization. Such a new version of OPLA-Tool is an important achievement to PLA design optimization as it provides an easier and more complete way to automate this task.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {818–823},
numpages = {6},
keywords = {Software product line, multi-objective evolutionary algorithms, product line architecture},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/3461001.3471144,
author = {Uta, Mathias and Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Tran, Thi Ngoc Trang and Helic, Denis},
title = {Evaluating recommender systems in feature model configuration},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471144},
doi = {10.1145/3461001.3471144},
abstract = {Configurators can be evaluated in various ways such as efficiency and completeness of solution search, optimality of the proposed solutions, usability of configurator user interfaces, and configuration consistency. Due to the increasing size and complexity of feature models, the integration of recommendation algorithms with feature model configurators becomes relevant. In this paper, we show how the output of a recommender system can be evaluated within the scope of feature model configuration scenarios. Overall, we argue that the discussed ways of measuring recommendation quality help developers to gain a broader view on evaluation techniques in constraint-based recommendation domains.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {58–63},
numpages = {6},
keywords = {recommender systems, feature models, evaluation, configuration},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {machine learning, inductive generalization from examples, generating feature models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791096,
author = {F\'{e}derle, \'{E}dipo Luis and do Nascimento Ferreira, Thiago and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {OPLA-tool: a support tool for search-based product line architecture design},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791096},
doi = {10.1145/2791060.2791096},
abstract = {The Product Line Architecture (PLA) design is a complex task, influenced by many factors such as feature modularization and PLA extensibility, which are usually evaluated according to different metrics. Hence, the PLA design is an optimization problem and problems like that have been successfully solved in the Search-Based Software Engineering (SBSE) area, by using metaheuristics such as Genetic Algorithm. Considering this fact, this paper introduces a tool named OPLA-Tool, conceived to provide computer support to a search-based approach for PLA design. OPLA-Tool implements all the steps necessary to use multi-objective optimization algorithms, including PLA transformations and visualization through a graphical interface. OPLA-Tool receives as input a PLA at the class diagram level, and produces a set of good alternative diagrams in terms of cohesion, feature modularization and reduction of crosscutting concerns.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {370–373},
numpages = {4},
keywords = {search-based software engineering, product line architecture design, multi-objective evolutionary algorithms},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.engappai.2016.03.009,
author = {Rodr\'{\i}guez, Guillermo and Soria, \'{A}lvaro and Campo, Marcelo},
title = {Artificial intelligence in service-oriented software design},
year = {2016},
issue_date = {August 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {53},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2016.03.009},
doi = {10.1016/j.engappai.2016.03.009},
abstract = {Service-Oriented Architecture (SOA) has gained considerable popularity for the development of distributed enterprise-wide applications within the software industry. The SOA paradigm promotes the reusability and integrability of software in heterogeneous environments by means of open standards. Most software companies capitalize on SOA by discovering and composing services already accessible over the Internet, whereas other organizations need internal control of applications and develop new services with quality-attribute properties tailored to their particular environment. Therefore, based on architectural and business requirements, developers can elaborate different alternatives within a SOA framework to design their software applications. Each of these alternatives will imply trade-offs among quality attributes, such as performance, dependability and availability, among others. In this context, Artificial Intelligence (AI) can assist developers in dealing with service-oriented design with the positive impact on scalability and management of generic quality attributes. In this paper, we offer a detailed, conceptualized and synthesized analysis of AI research works that have aimed at discovering, composing, or developing services. We also identify open research issues and challenges in the aforementioned research areas. The results of the characterization of 69 contemporary approaches and potential research directions for the areas are also shown. It is concluded that AI has aimed at exploiting the semantic resources and achieving quality-attribute properties so as to produce flexible and adaptive-to-change service discovery, composition, and development.},
journal = {Eng. Appl. Artif. Intell.},
month = aug,
pages = {86–104},
numpages = {19},
keywords = {Artificial Intelligence, Service-oriented design, Web service composition, Web service development, Web services discovery}
}

@article{10.1016/j.cosrev.2020.100341,
author = {Kotsiopoulos, Thanasis and Sarigiannidis, Panagiotis and Ioannidis, Dimosthenis and Tzovaras, Dimitrios},
title = {Machine Learning and Deep Learning in smart manufacturing: The Smart Grid paradigm},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2020.100341},
doi = {10.1016/j.cosrev.2020.100341},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {25},
keywords = {Industry 4.0, Machine Learning, Deep Learning, Industrial AI, Smart Grid}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: learning featured transition systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour of a whole family of software-based systems. Reasoning at the family level yields important economies of scale and quality improvements for a broad range of systems such as software product lines, adaptive and configurable systems. Yet, to fully benefit from the above advantages, a model of the system family's behaviour is necessary. Such a model is often prohibitively expensive to create manually due to the number of variants. For large long-lived systems with outdated specifications or for systems that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes to automate the learning of such models from existing artefacts. To advance research at a fundamental level, our learning target are Featured Transition Systems (FTS), an abstract formalism that can be used to provide a pivot semantics to a range of variability-aware state-based modelling languages. The main research questions addressed by this PhD project are: (1) Can we learn variability-aware models efficiently? (2) Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e., with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {variability mining, software product lines, model learning, featured transition systems, active automata learning},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471154,
author = {Bergel, Alexandre and Ghzouli, Razan and Berger, Thorsten and Chaudron, Michel R. V.},
title = {FeatureVista: interactive feature visualization},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471154},
doi = {10.1145/3461001.3471154},
abstract = {Comprehending and characterizing the spread and interaction of features in a software system is know to be difficult and error-prone. This paper presents FeatureVista, a lightweight tool providing interactive, glyph-based, and iconic visualization concepts designed to visually characterize the feature locations in software assets (source code). FeatureVista supports navigating between software components and features in an equal fashion. Our pilot study indicates that FeatureVista is intuitive and supports comprehending features. It helps to precisely characterize relations among features in large software systems and to contrast explicit software component definitions (e.g., package, class, method) with annotated feature portions---which so far was a largely manual and error-prone activity, albeit essential to get an adequate understanding of a software system. We suggest research directions for true, feature-oriented interfaces that can be used to manage software assets.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {196–201},
numpages = {6},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1007/s10515-019-00266-2,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong},
title = {Using multi-objective search and machine learning to infer rules constraining product configurations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00266-2},
doi = {10.1007/s10515-019-00266-2},
abstract = {Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.},
journal = {Automated Software Engg.},
month = jun,
pages = {1–62},
numpages = {62},
keywords = {Product line, Configuration, Rule mining, Multi-objective search, Machine learning, Interacting products}
}

@article{10.1287/mksc.2019.1160,
author = {Xu, Zibin and Dukes, Anthony},
title = {Product Line Design Under Preference Uncertainty Using Aggregate Consumer Data},
year = {2019},
issue_date = {July-August 2019},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {38},
number = {4},
issn = {1526-548X},
url = {https://doi.org/10.1287/mksc.2019.1160},
doi = {10.1287/mksc.2019.1160},
abstract = {This research studies the product line design problem when consumers are subject to perceptual errors in assessing their intrinsic preferences.This research studies the product line design problem when consumers are subject to perceptual errors in assessing their intrinsic preferences. If perceptual errors are driven by common variables, then a firm may use aggregate consumer data (e.g., conjoint studies or anonymous usage data) to deduce the errors and infer the consumer preferences. In this way, we develop microfoundations necessary to show when and how the firm can understand consumer preferences better than consumers themselves, a situation we call superior knowledge. But is superior knowledge ever unprofitable? How should the firm with superior knowledge design its product line? Do consumers receive more-relevant products or simply have more surplus extracted? Can data collection help consumers make better choices? Our results suggest that consumers’ rational suspicions may prevent the firm from exploiting its superior knowledge. In addition, the burden of signaling may force the firm to offer efficient quality for its products. Therefore, allowing the firm to collect aggregate consumer data may be strictly Pareto improving.},
journal = {Marketing Science},
month = jul,
pages = {669–689},
numpages = {21},
keywords = {consumer data collection, product line design, superior knowledge, uninformed preference, perceptual error, signaling model}
}

@inproceedings{10.1145/3336294.3336315,
author = {Wolschke, Christian and Becker, Martin and Schneickert, S\"{o}ren and Adler, Rasmus and MacGregor, John},
title = {Industrial Perspective on Reuse of Safety Artifacts in Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336315},
doi = {10.1145/3336294.3336315},
abstract = {In the future, safety-critical industrial products will have to be maintained and variants will have to be produced. In order to do this economically, the safety artifacts of the components should also be reused. At present, however, it is still unclear how this reuse could take place. Moreover this reuse is complicated, by the different situations in the various industries involved and by the corresponding standards applied.Current industrial practice for certification processes relies on a component-based view of reuse. We investigate the possibilities of product lines with managed processes for reuse also across multiple domains.In order to identify the challenges and possible solutions, we conducted interviews with industry partners from the domains of ICT, Rail, Automotive, and Industrial Automation, and from small- and medium-sized enterprises to large organizations. The semi-structured interviews identified the characteristics of current safety engineering processes, the handling of general variety and reuse, the approach followed for safety artifacts, and the need for improvement.In addition, a detailed literature survey summarizes existing approaches. We investigate which modularity concepts exist for dealing with safety, how variability concepts integrate safety, by which means process models can consider safety, and how safety cases are evolved while maintenance takes place. An overview of similar research projects complements the analysis.The identified challenges and potential solution proposals show how safety is related to Software Product Lines.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {143–154},
numpages = {12},
keywords = {safety standards, safety reuse, product line certification, open source certification, modular safety},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {variability modeling, software product lines, monte carlo tree search, feature models, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1007/978-3-030-38629-0_23,
author = {Zervoudakis, Konstantinos and Tsafarakis, Stelios and Paraskevi-Panagiota, Sovatzidi},
title = {A New Hybrid Firefly – Genetic Algorithm for the Optimal Product Line Design Problem},
year = {2019},
isbn = {978-3-030-38628-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38629-0_23},
doi = {10.1007/978-3-030-38629-0_23},
abstract = {The optimal product line design is one of the most critical decisions for a firm to stay competitive, since it is related to the sustainability and profitability of a company. It is classified as an NP-hard problem since no algorithm can certify in polynomial time that the optimum it identifies is the overall optimum of the problem. The focus of this research is to propose a new hybrid optimization method (FAGA) combining Firefly algorithm (FA) and Genetic algorithm (GA). The proposed hybrid method is applied to the product line design problem and its performance is compared to those of previous approaches, like genetic algorithm (GA) and simulated annealing (SA), by using both actual and artificial consumer-related data preferences for specific products. The comparison results demonstrate that the proposed hybrid method is superior to both genetic algorithm and simulated annealing in terms of accuracy, efficiency and convergence speed.},
booktitle = {Learning and Intelligent Optimization: 13th International Conference, LION 13, Chania, Crete, Greece, May 27–31, 2019, Revised Selected Papers},
pages = {284–297},
numpages = {14},
keywords = {Product line design, Hybridization, Firefly algorithm, Genetic algorithm},
location = {Chania, Crete, Greece}
}

@inproceedings{10.1145/3106195.3106215,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-healing in Service Mashups Through Feature Adaptation},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106215},
doi = {10.1145/3106195.3106215},
abstract = {The composition of the functionality of multiple services into a single unique service mashup has received wide interest in the recent years. Given the distributed nature of these mashups where the constituent services can be located on different servers, it is possible that a change in the functionality or availability of a constituent service result in the failure of the service mashup. In this paper, we propose a novel method based on the Software Product Line Engineering (SPLE) paradigm which is able to find an alternate valid service mashup which has maximum possible number of original service mashup features in order to mitigate a service failure when complete recovery is not possible. This method also has an advantage that it can recover or mitigate the failure automatically without requiring the user to specify any adaptation rule or strategy. We show the practicality of our proposed approach through extensive experiments.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {94–103},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3307630.3342419,
author = {Ghofrani, Javad and Kozegar, Ehsan and Bozorgmehr, Arezoo and Soorati, Mohammad Divband},
title = {Reusability in Artificial Neural Networks: An Empirical Study},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342419},
doi = {10.1145/3307630.3342419},
abstract = {Machine learning, especially deep learning has aroused interests of researchers and practitioners for the last few years in development of intelligent systems such as speech, natural language, and image processing. Software solutions based on machine learning techniques attract more attention as alternatives to conventional software systems. In this paper, we investigate how reusability techniques are applied in implementation of artificial neural networks (ANNs). We conducted an empirical study with an online survey among experts with experience in developing solutions with ANNs. We analyze the feedback of more than 100 experts to our survey. The results show existing challenges and some of the applied solutions in an intersection between reusability and ANNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {122–129},
numpages = {8},
keywords = {systematic reuse, survey, reusability, empirical study, artificial neural networks},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.5555/1885639.1885642,
author = {Bagheri, Ebrahim and Di Noia, Tommaso and Ragone, Azzurra and Gasevic, Dragan},
title = {Configuring software product line feature models based on Stakeholders' soft and hard requirements},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature modeling is a technique for capturing commonality and variability. Feature models symbolize a representation of the possible application configuration space, and can be customized based on specific domain requirements and stakeholder goals. Most feature model configuration processes neglect the need to have a holistic approach towards the integration and satisfaction of the stakeholder's soft and hard constraints, and the application-domain integrity constraints. In this paper, we will show how the structure and constraints of a feature model can be modeled uniformly through Propositional Logic extended with concrete domains, called P(N). Furthermore, we formalize the representation of soft constraints in fuzzy P(N) and explain how semi-automated feature model configuration is performed. The model configuration derivation process that we propose respects the soundness and completeness properties.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {16–31},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/3233027.3233033,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse engineering variability from requirement documents based on probabilistic relevance and word embedding},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233033},
doi = {10.1145/3233027.3233033},
abstract = {Feature and variability extraction from different artifacts is an indispensable activity to support systematic integration of single software systems and Software Product Line (SPL). Beyond manually extracting variability, a variety of approaches, such as feature location in source code and feature extraction in requirements, has been proposed to provide an automatic identification of features and their variation points. Compared with source code, requirements contain more complete variability information and provide traceability links to other artifacts from early development phases. In this paper, we propose a method to automatically extract features and relationships based on a probabilistic relevance and word embedding. In particular, our technique consists of three steps: First, we apply word2vec to obtain a prediction model, which we use to determine the word level similarity of requirements. Second, based on word level similarity and the significance of a word in a domain, we compute the requirements level similarity using probabilistic relevance. Third, we adopt hierarchical clustering to group features and we define four criteria to detect variation points between identified features. We perform a case study to evaluate the usability and robustness of our method and to compare it with the results of other related approaches. Initial results reveal that our approach identifies the majority of features correctly and also extracts variability information with reasonable accuracy.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {121–131},
numpages = {11},
keywords = {variability extraction, software product lines, reverse engineering, requirement documents, feature identification},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {neural architecture search, feature model, configuration search, NAS, AutoML},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2934466.2934484,
author = {Vasilevskiy, Anatoly and Chauvel, Franck and Haugen, \O{}ystein},
title = {Toward robust product realisation in software product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934484},
doi = {10.1145/2934466.2934484},
abstract = {Product derivation is a building process of products from selected features in software product lines (SPLs). Realisation paves the way for automatic product derivation. A realisation defines a mapping between abstract features in a feature tree and their implementation artefacts in a model, and therefore governs the derivation of a new product. We experience that a realisation is not always straightforward and robust against modifications in the model. In the paper, we introduce an approach to build robust realisations. It consists of automated planning techniques and a layered architecture to yield a product. We demonstrate how our approach can leverage modern means of software design, development and validation. We evaluate the approach on a use-case provided by an industry partner and compare our technique to the existing realisation layer in the Base Variability Resolution (BVR) language.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {184–193},
numpages = {10},
keywords = {variation point, realisation, product line, product derivation, model, fragment substitution, bvr, automated planning},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1007/s10270-020-00839-w,
author = {Pol’la, Matias and Buccella, Agustina and Cechich, Alejandra},
title = {Analysis of variability models: a systematic literature review},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00839-w},
doi = {10.1007/s10270-020-00839-w},
abstract = {Dealing with variability, during Software Product Line Engineering (SPLE), means trying to allow software engineers to develop a set of similar applications based on a manageable range of variable functionalities according to expert users’ needs. Particularly, variability management (VM) is an activity that allows flexibility and a high level of reuse during software development. In the last years, we have witnessed a proliferation of methods, techniques and supporting tools for VM in general, and for its analysis in particular. More precisely, a specific field has emerged, named (automated) variability analysis, focusing on verifying variability models across the SPLE’s phases. In this paper, we introduce a systematic literature review of existing proposals (as primary studies) focused on analyzing variability models. We define a classification framework, which is composed of 20 sub-characteristics addressing general aspects, such as scope and validation, as well as model-specific aspects, such as variability primitives, reasoner type. The framework allows to look at the analysis of variability models during its whole life cycle—from design to derivation—according to the activities involved during an SPL development. Also, the framework helps us answer three research questions defined for showing the state of the art and drawing challenges for the near future. Among the more interesting challenges, we can highlight the needs of more applications in industry, the existence of more mature tools, and the needs of providing more semantics in the way of variability primitives for identifying inconsistencies in the models.},
journal = {Softw. Syst. Model.},
month = aug,
pages = {1043–1077},
numpages = {35},
keywords = {Variability analysis, Software Product Line, Variability management, Supporting tools}
}

@inproceedings{10.1145/2797433.2797456,
author = {Galster, Matthias},
title = {Architecting for Variability in Quality Attributes of Software Systems},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797456},
doi = {10.1145/2797433.2797456},
abstract = {Variability in software systems is usually concerned with variability in features and functionality. However, variability also occurs in quality attributes (e.g., performance, security) and quality attribute requirements (for example, a performance requirement may state that a system must respond to a user request within 0.1 seconds). We discuss what variability in quality attributes is, including several scenarios in which variability in quality attributes can occur. We then discuss the state of research and what we know about variability in quality attributes, including some existing research to address the challenge of identifying, implementing and managing variability in quality attributes. Finally, we discuss potential directions for future research.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {23},
numpages = {4},
keywords = {software architecture, quality attributes, Variability},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {prediction, feature, defect, classification},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.jss.2017.01.026,
author = {Arvanitou, Elvira Maria and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander and Galster, Matthias and Avgeriou, Paris},
title = {A mapping study on design-time quality attributes and metrics},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {127},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.01.026},
doi = {10.1016/j.jss.2017.01.026},
abstract = {Support to the quality attribute (QA) &amp; metric selection process.Maintainability is the most studied QA for most domains and development phases.Quality attributes are usually assessed through a correlation to a single metric.Metrics are validated in empirical settings and may lack theoretical validity. Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently examined high-level quality attribute, regardless of the application domain or the development phase, (c) assessment of quality attributes is usually performed by a single metric, rather than a combination of multiple metrics, and (d) metrics are mostly validated in an empirical setting. These outcomes are interpreted and discussed based on related work, offering useful implications to both researchers and practitioners.},
journal = {J. Syst. Softw.},
month = may,
pages = {52–77},
numpages = {26},
keywords = {Design-time quality attributes, Mapping study, Measurement, Software quality}
}

@inproceedings{10.1145/2791060.2791103,
author = {Mazo, Ra\'{u}l and Mu\~{n}oz-Fern\'{a}ndez, Juan C. and Rinc\'{o}n, Luisa and Salinesi, Camille and Tamura, Gabriel},
title = {VariaMos: an extensible tool for engineering (dynamic) product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791103},
doi = {10.1145/2791060.2791103},
abstract = {This paper presents the new release of VariaMos, a Java-based tool for defining variability modeling languages, modeling (dynamic) product lines and cyber-physical self-adaptive systems, and supporting automated verification, analysis, configuration and simulation of these models. In particular, we describe the characteristics of this new version regarding its first release: (1) the capability to create languages for modeling systems with variability, even with different views; (2) the capability to use the created language to model (dynamic) product lines; (3) the capability to analyze and configure these models according to the changing context and requirements; and (4) the capability to execute them over several simulation scenarios. Finally, we show how to use VariaMos with an example, and we compare it with other tools found in the literature.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {374–379},
numpages = {6},
keywords = {variability, tool, simulation, product line engineering, dynamic product line models, constraints},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Vulnerability Analysis and Management, Vulnerability, Variability Model, Feature Model, Exploit},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2647908.2655979,
author = {Eichelberger, Holger and El-Sharkawy, Sascha and Kr\"{o}her, Christian and Schmid, Klaus},
title = {EASy-producer: product line development for variant-rich ecosystems},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655979},
doi = {10.1145/2647908.2655979},
abstract = {Development of software product lines requires tool support, e.g., to define variability models, to check variability models for consistency and to derive the artifacts for a specific product. Further capabilities are required when product lines are combined to software ecosystems, i.e., management and development of distributed product lines across multiple different organizations.In this paper, we describe EASy-Producer, a prototypical tool set for the development of software product lines in general and variant-rich ecosystems in particular. To support the product line engineer, EASy-Producer differentiates between simplified views limiting the capabilities and expert views unleashing its full power. We will discuss how these two views support the definition of variability models, the derivation of product configurations and the instantiation of artifacts.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {133–137},
numpages = {5},
keywords = {software product lines, ecosystems, EASy-producer},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2934466.2934474,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Purposeful performance variability in software product lines: a comparison of two case studies},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934474},
doi = {10.1145/2934466.2934474},
abstract = {Within software product lines, customers may have different quality needs. To produce products with purposefully different quality attributes, several challenges must be addressed. First, one must be able to distinguish product quality attributes to the customers in a meaningful way. Second, one must create the desired quality attribute differences during product-line architecture design and derivation. To study how performance is varied purposefully in software product lines, we conducted a comparison and re-analysis of two industrial case studies in the telecommunication and mobile game domains. The results show that performance variants must be communicated to the customer in a way that links to customer value and her role. When performance or its adaptation are crucial for the customer, performance differences must be explicitly "designed in" with software or hardware means. Due to the emergent nature of performance, it is important to test performance and manage how other variability affects performance.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {144–153},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {software variability, computer-aided software engineering, Software engineering}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@inproceedings{10.1145/3106195.3106201,
author = {Kim, Jongwook and Batory, Don and Dig, Danny},
title = {Refactoring Java Software Product Lines},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106201},
doi = {10.1145/3106195.3106201},
abstract = {Refactoring is a staple of Object-Oriented (OO) program development. It should be a staple of OO Software Product Line (SPL) development too. X15 is the first tool to support the refactoring of Java SPL codebases. X15 (1) uses Java custom annotations to encode variability in feature-based Java SPLs, (2) projects a view of an SPL product (a program that corresponds to a legal SPL configuration), and (3) allows programmers to edit and refactor the product, propagating changes back to the SPL codebase. Case studies apply 2316 refactorings in 8 public Java SPLs and show that X15 is as efficient, expressive, and scalable as a state-of-the-art feature-unaware Java refactoring engine.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {59–68},
numpages = {10},
keywords = {software product lines, refactoring},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233042,
author = {Mesa, Oslien and Vieira, Reginaldo and Viana, Marx and Durelli, Vinicius H. S. and Cirilo, Elder and Kalinowski, Marcos and Lucena, Carlos},
title = {Understanding vulnerabilities in plugin-based web systems: an exploratory study of wordpress},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233042},
doi = {10.1145/3233027.3233042},
abstract = {A common software product line strategy involves plugin-based web systems that support simple and quick incorporation of custom behaviors. As a result, they have been widely adopted to create web-based applications. Indeed, the popularity of ecosystems that support plugin-based development (e.g., WordPress) is largely due to the number of customization options available as community-contributed plugins. However, plugin-related vulnerabilities tend to be recurrent, exploitable and hard to be detected and may lead to severe consequences for the customized product. Hence, there is a need to further understand such vulnerabilities to enable preventing relevant security threats. Therefore, we conducted an exploratory study to characterize vulnerabilities caused by plugins in web-based systems. To this end, we went over WordPress vulnerability bulletins cataloged by the National Vulnerability Database as well as associated patches maintained by the WordPress plugins repository. We identified the main types of vulnerabilities caused by plugins as well as their impact and the size of the patch to fix the vulnerability. Moreover, we identified the most common security-related topics discussed among WordPress developers. We observed that, while plugin-related vulnerabilities may have severe consequences and might remain unnoticed for years before being fixed, they can commonly be mitigated with small and localized changes to the source code. The characterization helps to provide an understanding on how typical plugin-based vulnerabilities manifest themselves in practice. Such information can be helpful to steer future research on plugin-based vulnerability detection and prevention.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {149–159},
numpages = {11},
keywords = {software product lines, security, plugin-based web systems, exploratory study},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3350768.3351993,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Towards the support of user preferences in search-based product line architecture design: an exploratory study},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351993},
doi = {10.1145/3350768.3351993},
abstract = {Software Product Lines (SPLs) is a reuse approach in which a family of products is generalized in a common architecture that can be adapted to different clients. The Product Line Architecture (PLA) is one of the most important artifacts of a SPL. PLA design requires great human effort as it involves several factors that are usually in conflict. To ease this task, PLA design can be formulated as an optimization problem with many factors, i.e, as a multi-objective optimization problem. In this context, the MOA4PLA approach was proposed to optimize PLA design using search algorithms and metrics specific to the context. This approach supported by OPLA-Tool has already been used in several works demonstrating its applicability. However, MOA4PLA does not take into account aspects that are subjective, such as the preferences of a particular Decision Maker (DM). To do so, this paper presents a proposal to incorporate the user preferences in the optimization process performed by MOA4PLA, through an interactive process in which the DM subjectively evaluates the solutions in processing time. Thus, the solutions generated can be better suited to the DM's needs or preferences. In order to allow the user interaction, modifications were made in MOA4PLA and implemented in the OPLA-Tool. Aiming at an initial validation of the proposal, an exploratory study was carried out, composed of two experiments: a qualitative and a quantitative. These experiments were realized with the participation of a software architect. Empirical results pointed out that the proposed interactive process enables the generation of PLAs that are in accordance with the architect's preferences. Another significant contribution are the lessons learned on how to improve the interactive process.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {387–396},
numpages = {10},
keywords = {Product Line Architecture, Multi-Objective Optimization, Human-computer interaction},
location = {Salvador, Brazil},
series = {SBES '19}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Software product lines, SPL Conqueror, Non-functional properties, Measurement and optimization, Feature-oriented software development}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {smart production, smart product, smart factory, product line of factories, product and production configuration},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2993412.3004847,
author = {Anvaari, Mohsen and S\o{}rensen, Carl-Fredrik and Zimmermann, Olaf},
title = {Associating architectural issues with quality attributes: a survey on expert agreement},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3004847},
doi = {10.1145/2993412.3004847},
abstract = {The architectural decision-making process is a complex and crucial endeavor in companies that develop large and distributed software systems. In this process, choosing and evaluating a solution for each architectural issue depends on decision drivers. The drivers are mainly the business factors (e.g., cost, time-to-market, etc.) and software quality attributes (e.g., security, adaptability, etc.). This paper examines whether there is agreement among experts in associating (i.e., relating) architectural issues with relevant quality attributes. We conducted a survey with 37 experts from several industrial domains who, at least once a month, make one or more architectural decisions. The results show there is poor agreement among these experts in identifying and scoring relevant quality attributes for each architectural issue. Poor agreement implies that the associating task is subjective, and that experts inconsistently define and interpret the relevance of various quality attributes for a given architectural issue that may hurt the sustainability of their architectural decisions. This paper suggests that practitioners in their decision-making should employ approaches that are more systematic. The approaches should be supported by methods and tools designed to diminish the biases of intuitive, experience-based approaches of associating architectural issues with quality attributes.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {11},
numpages = {7},
keywords = {survey, software quality attribute, inter-rater agreement, architectural synthesis, architectural decision},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@article{10.1016/j.procs.2018.05.082,
author = {Hitesh and Kumari, A. Charan},
title = {Feature Selection Optimization in SPL using Genetic Algorithm},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.082},
doi = {10.1016/j.procs.2018.05.082},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {1477–1486},
numpages = {10},
keywords = {Software product line, Genetic Algorithm, Feature Model, Software Product Line Engineering}
}

@article{10.1155/2021/1057371,
author = {Yang, Yinghui and cheikhrouhou, omar},
title = {The Potential Energy of Artificial Intelligence Technology in University Education Reform from the Perspective of Communication Science},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/1057371},
doi = {10.1155/2021/1057371},
abstract = {In today’s rapid development of science and technology, science is everywhere in people’s lives, and science communication is everywhere. Science and communication are not only not far away but also very close. Since machine learning algorithms with deep learning as a theme have achieved great success in the fields of vision and speech recognition, as well as the large amount of data resources that cloud computing, big data, and other technologies can provide, the development speed of artificial intelligence has been greatly improved, and it has had a significant impact in various industries in the society, and the country has put forward the concept of intelligent education for this purpose. However, there have been few systematic discussions on the combination of artificial intelligence with education and teaching. Therefore, this article uses artificial intelligence technology to study the potential energy space of artificial intelligence technology in college education reform from the perspective of science communication, designs and implements an online education platform for colleges and universities, and conducts a trial of platform use in a domestic college and universities. Some teachers and students conduct a satisfaction survey after the platform is used, and the conclusions show that whether in the teacher group or the student group, most teachers and students are relatively satisfied with the online education platform designed in this article. The reform of college education includes many aspects. This article is a research study on the form of college education, changing from traditional offline education to online platform education. This research can provide a certain reference for the reform of college education.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {7}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannem\"{u}ller, Martin and Matth\'{e}, Michael and Sch\"{u}rr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {performance-influence models, machine learning, dynamic software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2973839.2973842,
author = {Lima, Crescencio and Chavez, Christina},
title = {A Systematic Review on Metamodels to Support Product Line Architecture Design},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973842},
doi = {10.1145/2973839.2973842},
abstract = {Product Line Architecture (PLA) design is a key activity for developing successful Software Product Line (SPL) projects. PLA design is a difficult task, mostly due to the complexity of the software systems that SPLs deal with, and their variabilities. Metamodels have been used to support the representation of assets that compose a PLA, SPL variability and the relationships among them. The goal of this study is to characterize the use of metamodeling on PLA design, aiming to identify the main characteristics of metamodels, the elements used for PLA and variability representation and trace the evolution of metamodels. We conducted a systematic literature review to identify the primary studies on the use of metamodels in PLA Design. Thirty-five studies that proposed metamodels to support PLA design were selected. The review main findings are: (i) it is difficult to identify the existence of research trends because the number of publication varies and metamodels lack standardization; (ii) several metamodels support feature representation; (iii) the majority of studies addressed variability representation with variation points in UML diagrams; and, (iv) five evolution lines that describe how metamodels evolved over the years were identified.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {13–22},
numpages = {10},
keywords = {Variability, Systematic Literature Review, Software Product Lines, Product Line Architecture, Metamodels},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@article{10.1016/j.infsof.2015.09.004,
author = {Souza Neto, Pl\'{a}cido A. and Vargas-Solar, Genoveva and da Costa, Umberto Souza and Musicante, Martin A.},
title = {Designing service-based applications in the presence of non-functional properties},
year = {2016},
issue_date = {January 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {69},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.09.004},
doi = {10.1016/j.infsof.2015.09.004},
abstract = {ContextThe development of distributed software systems has become an important problem for the software engineering community. Service-based applications are a common solution for this kind of systems. Services provide a uniform mechanism for discovering, integrating and using these resources. In the development of service based applications not only the functionality of services and compositions should be considered, but also conditions in which the system operates. These conditions are called non-functional requirements (NFR). The conformance of applications to NFR is crucial to deliver software that meets the expectations of its users. ObjectiveThis paper presents the results of a systematic mapping carried out to analyze how NFR have been addressed in the development of service-based applications in the last years, according to different points of view. MethodOur analysis applies the systematic mapping approach. It focuses on the analysis of publications organized by categories called facets, which are combined to answer specific research questions. The facets compose a classification schema which is part of the contribution and results. ResultsThis paper presents our findings on how NFR have been supported in the development of service-based applications by proposing a classification scheme consisting in five facets: (i) programming paradigm (object/service oriented); (ii) contribution (methodology, system, middleware); (iii) software process phase; (iv) technique or mathematical model used for expressing NFR; and (v) the types of NFR addressed by the papers, based on the classification proposed by the ISO/IEC 9126 specification. The results of our systematic mapping are presented as bubble charts that provide a quantitative analysis to show the frequencies of publications for each facet. The paper also proposes a qualitative analysis based on these plots. This analysis discusses how NFR (quality properties) have been addressed in the design and development of service-based applications, including methodologies, languages and tools devised to support different phases of the software process. ConclusionThis systematic mapping showed that NFR are not fully considered in all software engineering phases for building service based applications. The study also let us conclude that work has been done for providing models and languages for expressing NFR and associated middleware for enforcing them at run time. An important finding is that NFR are not fully considered along all software engineering phases and this opens room for proposing methodologies that fully model NFR. The data collected by our work and used for this systematic mapping are available in https://github.com/placidoneto/systematic-mapping_service-based-app_nfr.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {84–105},
numpages = {22},
keywords = {Non-functional requirements, Service-based software process, Systematic mapping}
}

@article{10.1016/j.eswa.2020.114161,
author = {Houssein, Essam H. and Emam, Marwa M. and Ali, Abdelmgeid A. and Suganthan, Ponnuthurai Nagaratnam},
title = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114161},
doi = {10.1016/j.eswa.2020.114161},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {20},
keywords = {Thermography images, Ultrasound images, Mammogram images, Medical imaging modalities, Magnetic resonance imaging (MRI), Machine learning, Histological images, Deep learning, Computer-aided diagnosis system (CAD), Convolutional neural network, Breast cancer classification}
}

@inproceedings{10.1145/2791060.2791075,
author = {Fang, Miao and Leyh, Georg and Doerr, Joerg and Elsner, Christoph and Zhao, Jingjing},
title = {Towards model-based derivation of systems in the industrial automation domain},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791075},
doi = {10.1145/2791060.2791075},
abstract = {Many systems in the industrial automation domain include information systems. They manage manufacturing processes and control numerous distributed hardware and software components. In current practice, the development and reuse of such systems is costly and time-consuming, due to the variability of systems' topology and processes. Up to now, product line approaches for systematic modeling and management of variability have not been well established for such complex domains.In this paper, we present a model-based approach to support the derivation of systems in the target domain. The proposed architecture of the derivation infrastructure enables feature-, topology- and process configuration to be integrated into the multi-staged derivation process. We have developed a prototype to prove feasibility and improvement of derivation efficiency. We report the evaluation results that we collected through semi-structured interviews from domain stakeholders. The results show high potential to improve derivation efficiency by adopting the approach in practice. Finally, we report the lessons learned that raise the opportunities and challenges for future research.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {283–292},
numpages = {10},
keywords = {variability modeling, product line, model-based engineering, derivation},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1155/2021/9976306,
author = {Wang, Wei and Wu, Wenqing},
title = {Using Machine Learning Algorithms to Recognize Shuttlecock Movements},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9976306},
doi = {10.1155/2021/9976306},
abstract = {Shuttlecock is an excellent traditional national sport in China. Because of its simplicity, convenience, and fun, it is loved by the broad masses of people, especially teenagers and children. The development of shuttlecock sports into a confrontational event is not long, and it takes a period of research to master the tactics and strategies of shuttlecock sports. Based on this, this article proposes the use of machine learning algorithms to recognize the movement of shuttlecock movements, aiming to provide more theoretical and technical support for shuttlecock competitions by identifying features through actions with the assistance of technical algorithms. This paper uses literature research methods, model methods, comparative analysis methods, and other methods to deeply study the motion characteristics of shuttlecock motion, the key algorithms of machine learning algorithms, and other theories and construct the shuttlecock motion recognition based on multiview clustering algorithm. The model analyzes the robustness and accuracy of the machine learning algorithm and other algorithms, such as a variety of performance comparisons, and the results of the shuttlecock motion recognition image. For the key movements of shuttlecock movement, disk, stretch, hook, wipe, knock, and abduction, the algorithm proposed in this paper has a good movement recognition rate, which can reach 91.2%. Although several similar actions can be recognized well, the average recognition accuracy rate can exceed 75%, and even through continuous image capture, the number of occurrences of the action can be automatically analyzed, which is beneficial to athletes. And the coach can better analyze tactics and research strategies.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/3132498.3133834,
author = {Santos, Marcelo C. B. and Colanzi, Thelma E. and Amaral, Aline M. M. M. and OliveiraJr, Edson},
title = {Preliminary study on the correlation of objective functions to optimize product-line architectures},
year = {2017},
isbn = {9781450353250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132498.3133834},
doi = {10.1145/3132498.3133834},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line (SPL). The Multi-Objective Approach for PLA Design (MOA4PLA) aims at optimizing the PLA design by using search algorithms easing the design activity. From an original PLA, MOA4PLA automatically obtains alternative designs to improve the original one in terms of the objectives selected for optimization. The use of search algorithms is an incipient research topic, which includes several open research questions. The evaluation model of MOA4PLA is composed of various objective functions, which use software metrics to evaluate different factors that influence on the PLA design. However, the simultaneous optimization of all objective functions is a computationally complex task. In this sense, it is worthwhile to investigate the possible correlation between objective functions because the discovery of correlated functions allows to reduce the number of objectives to be optimized by the search algorithm. Hence, in this paper we perform a preliminary study to investigate the correlation among five objective functions related to metrics that provide indicators on conventional architectural properties, such as coupling, cohesion and size. To accomplish the objective of this paper, four controlled experiments were carried out with four different PLA designs. Empirical results provide preliminary evidence that two pairs of functions are positively correlated and two other pairs of functions are negatively correlated. From such findings, several guidelines were derived to help architects to both reduce and select the objectives related to conventional architectural properties to be tackled during the PLA design optimization.},
booktitle = {Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse},
articleno = {11},
numpages = {10},
keywords = {search-based software engineering, product-line architecture, correlation study},
location = {Fortaleza, Cear\'{a}, Brazil},
series = {SBCARS '17}
}

@article{10.1145/3437479.3437485,
author = {Yoo, Shin and Aleti, Aldeida and Turhan, Burak and Minku, Leandro L. and Miranskyy, Andriy and Meri\c{c}li, \c{C}etin},
title = {The 8th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3437479.3437485},
doi = {10.1145/3437479.3437485},
abstract = {The International Workshop on Realizing Arti cial Intelligence Synergies in Software Engineering (RAISE) aims to present the state of the art in the crossover between Software Engineering and Arti cial Intelligence. This workshop explored not only the appli- cation of AI techniques to SE problems but also the application of SE techniques to AI problems. Software has become critical for realizing functions central to our society. For example, software is essential for nancial and transport systems, energy generation and distribution systems, and safety-critical medical applications. Software development costs trillions of dollars each year yet, still, many of our software engineering methods remain mostly man- ual. If we can improve software production by smarter AI-based methods, even by small margins, then this would improve a crit- ical component of the international infrastructure, while freeing up tens of billions of dollars for other tasks.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {23–24},
numpages = {2}
}

@inproceedings{10.1145/3336294.3336307,
author = {Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Simao, Adenilso},
title = {Learning from Difference: An Automated Approach for Learning Family Models from Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336307},
doi = {10.1145/3336294.3336307},
abstract = {Substantial effort has been spent on extending specification notations and their associated reasoning techniques to software product lines (SPLs). Family-based analysis techniques operate on a single artifact, referred to as a family model, that is annotated with variability constraints. This modeling approach paves the way for efficient model-based testing and model checking for SPLs. Albeit reasonably efficient, the creation and maintenance of family models tend to be time consuming and error-prone, especially if there are crosscutting features. To tackle this issue, we introduce FFSMDiff, a fully automated technique to learn featured finite state machines (FFSM), a family-based formalism that unifies Mealy Machines from SPLs into a single representation. Our technique incorporates variability to compare and merge Mealy machines and annotate states and transitions with feature constraints. We evaluate our technique using 34 products derived from three different SPLs. Our results support the hypothesis that families of Mealy machines can be effectively merged into succinct FFSMs with fewer states, especially if there is high feature sharing among products. These indicate that FFSMDiff is an efficient family-based model learning technique.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {52–63},
numpages = {12},
keywords = {software product lines, model learning, family model, 150% model},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2701319.2701330,
author = {Rabiser, Rick and Vierhauser, Michael and Gr\"{u}nbacher, Paul},
title = {Variability Management for a Runtime Monitoring Infrastructure},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701330},
doi = {10.1145/2701319.2701330},
abstract = {Many software systems today are systems of systems (SoS), which are difficult to analyze due to their size, complexity, heterogeneity, and variability. For instance, unexpected behavior of SoS is often caused by the complex interactions between the involved systems and their environment at runtime. Monitoring infrastructures (MIs) provide support for engineers and support staff analyzing the behavior of SoS during development and operation. Variability plays an important role in MIs, however, while some approaches exist, managing variability of MIs remains challenging. In this paper, we describe how we applied a variability management approach to support the reconfiguration of a SoS monitoring infrastructure (MI) at runtime. Our approach provides configuration support for setting up the MI to reflect system variability. It also supports runtime reconfiguration of the MI to reflect the different monitoring tasks of users and to support evolution. We motivate our work using the case of monitoring a real-world SoS from the domain of industrial automation and discuss variability-related challenges in four monitoring scenarios. We evaluate the feasibility of our approach by applying it to these scenarios. We also demonstrate that our approach reduces manual reconfiguration effort and helps to reduce the overhead of the MI.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {35–42},
numpages = {8},
keywords = {variability management, reconfiguration, large-scale systems, Software monitoring},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/3109729.3109745,
author = {Markiegi, Urtzi},
title = {Test optimisation for Highly-Configurable Cyber-Physical Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109745},
doi = {10.1145/3109729.3109745},
abstract = {Cyber-Physical Systems (CPS) have become one of the core-enabling technologies for multiple domains, such as manufacturing, healthcare, energy and transportation. Furthermore, these domains are demanding CPS to be highly-configurable in order to respond to multiple and changing market requirements. Testing these Highly-Configurable Cyber-Physical Systems (HCCPS) is challenging. First, when working with CPSs, considerable time is required in order to tackle physical processes during testing. And secondly, in highly-configurable systems, a large number of system variants need to be tested. Consequently, reducing HCCPS testing time is essential.In this context, a research work is presented to reduce the overall testing time of HCCPS, focusing on a merged strategy of product and test cases optimisation. In particular, two approaches are proposed in order to achieve the testing time reduction. The first approach aims to reduce the HCCPS testing time by an iterative allocation of products and test cases. The second approach aims to reduce the HCCPS testing time by a feedback driven dynamic and iterative allocation of products and test cases.A preliminary experiment has been undertaken to test the iterative allocation approach. In this experiment, products to be tested are selected and prioritised. Next, multiple testing iterations are perform until the time-budget is consumed. In each iteration a small number of test cases are allocated for each of the products to be tested. The experiment was evaluated with an academic HCCPS and preliminary results suggest that the proposed approach reduces the fault detection time when compared with traditional approaches.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {139–144},
numpages = {6},
keywords = {Software Engineering, Search-Based Software Engineering, Product Line Testing, Highly-Configurable Systems, Fault Detection, Cyber-Physical Systems},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2491627.2491646,
author = {Marijan, Dusica and Gotlieb, Arnaud and Sen, Sagar and Hervieu, Aymeric},
title = {Practical pairwise testing for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491646},
doi = {10.1145/2491627.2491646},
abstract = {One key challenge for software product lines is efficiently managing variability throughout their lifecycle. In this paper, we address the problem of variability in software product lines testing. We (1) identify a set of issues that must be addressed to make software product line testing work in practice and (2) provide a framework that combines a set of techniques to solve these issues. The framework integrates feature modelling, combinatorial interaction testing and constraint programming techniques. First, we extract variability in a software product line as a feature model with specified feature interdependencies. We then employ an algorithm that generates a minimal set of valid test cases covering all 2-way feature interactions for a given time interval. Furthermore, we evaluate the framework on an industrial SPL and show that using the framework saves time and provides better test coverage. In particular, our experiments show that the framework improves industrial testing practice in terms of (i) 17% smaller set of test cases that are (a) valid and (b) guarantee all 2-way feature coverage (as opposite to 19.2% 2-way feature coverage in the hand made test set), and (ii) full flexibility and adjustment of test generation to available testing time.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {227–235},
numpages = {9},
keywords = {variability management, software product lines, feature modelling},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@article{10.1016/j.infsof.2010.12.006,
author = {Chen, Lianping and Ali Babar, Muhammad},
title = {A systematic review of evaluation of variability management approaches in software product lines},
year = {2011},
issue_date = {April, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.12.006},
doi = {10.1016/j.infsof.2010.12.006},
abstract = {ContextVariability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated. ObjectiveThe objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches. MethodWe carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007. ResultsWe selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects. ConclusionThe findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {344–362},
numpages = {19},
keywords = {Empirical studies, Software product line, Systematic literature reviews, Variability management}
}

@inproceedings{10.1145/2791060.2791069,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Empirical comparison of regression methods for variability-aware performance prediction},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791069},
doi = {10.1145/2791060.2791069},
abstract = {Product line engineering derives product variants by selecting features. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable product variant. We infer such a correlation using four regression methods based on small samples of measured configurations, without additional effort to detect feature interactions. We conduct experiments on six real-world case studies to evaluate the prediction accuracy of the regression methods. A key finding in our empirical study is that one regression method, called Bagging, is identified as the best to make accurate and robust predictions for the studied systems.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {186–190},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2491627.2491641,
author = {Koziolek, Heiko and Goldschmidt, Thomas and de Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan},
title = {Experiences from identifying software reuse opportunities by domain analysis},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491641},
doi = {10.1145/2491627.2491641},
abstract = {In a large corporate organization there are sometimes similar software products in certain subdomains with a perceived functional overlap. This promises to be an opportunity for systematic reuse to reduce software development and maintenance costs. In such situations companies have used different domain analysis approaches (e.g., SEI Technical Probe) that helped to assess technical and organizational potential for a software product line approach. We applied existing domain analysis approaches for software product line engineering and tailored them to include a feature analysis as well as architecture evaluation. In this paper, we report our experiences from applying the approach in two subdomains of industrial automation.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {208–217},
numpages = {10},
keywords = {software product lines, domain analysis, business case},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3106195.3106207,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106207},
doi = {10.1145/3106195.3106207},
abstract = {Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering &amp; machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {133–142},
numpages = {10},
keywords = {Variability Extraction, Systematic Literature Review, Software Product Lines, Reverse Engineering, Natural Language Documents, Feature Identification},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1109/ICSE43902.2021.00100,
author = {Velez, Miguel and Jamshidi, Pooyan and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {White-Box Analysis over Machine Learning: Modeling Performance of Configurable Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00100},
doi = {10.1109/ICSE43902.2021.00100},
abstract = {Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly accurate performance-influence models to the most accurate and expensive black-box approach, but at a reduced cost and with additional benefits from interpretable and local models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1072–1084},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1109/ICTAI.2014.107,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {A Comparative Analysis of Two Multi-objective Evolutionary Algorithms in Product Line Architecture Design Optimization},
year = {2014},
isbn = {9781479965724},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2014.107},
doi = {10.1109/ICTAI.2014.107},
abstract = {The Product Line Architecture (PLA) design is a multi-objective optimization problem that can be properly solved with search-based algorithms. However, search-based PLA design is an incipient research field. Due to this, works in this field have addressed main points to solve the problem: adequate representation, specific search operators and suitable evaluation fitness functions. Similarly what happens in the search-based design of traditional software, existing works on search-based PLA design use NSGA-II, without evaluating the characteristics of this algorithm, such as the use of crossover operator. Considering this fact, this paper reports results from a comparative analysis of two algorithms, NSGA-II and PAES, to the PLA design problem. PAES was chosen because it implements a different evolution strategy that does not employ crossover. An experimental study was carried out with nine PLAs and results of the conducted study attest that NSGA-II performs better than PAES in the PLA design context.},
booktitle = {Proceedings of the 2014 IEEE 26th International Conference on Tools with Artificial Intelligence},
pages = {681–688},
numpages = {8},
keywords = {multi-objective algorithms, product line architecture design, software product line},
series = {ICTAI '14}
}

@inproceedings{10.1145/3109729.3109758,
author = {Ben Snaiba, Ziad and de Vink, Erik P. and Willemse, Tim A.C.},
title = {Family-Based Model Checking of SPL based on mCRL2},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109758},
doi = {10.1145/3109729.3109758},
abstract = {We discuss how the general-purpose model checker mCRL2 can be used for family-based verification of behavioral properties of software product lines. This is achieved by exploiting a feature-oriented extension of the modal μ-calculus for the specification of SPL properties, and for its model checking by encoding it back into the logic of mCRL2. Using the example of the well-known minepump SPL an illustration of the possibilities of the approach is given.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {mCRL2, Software Product Lines, Family-based model checking},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3236404,
author = {Gazzillo, Paul and Koc, Ugur and Nguyen, ThanhVu and Wei, Shiyi},
title = {Localizing configurations in highly-configurable systems},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236404},
doi = {10.1145/3233027.3236404},
abstract = {The complexity of configurable systems has grown immensely, and it is only getting more complex. Such systems are a challenge for software testing and maintenance, because bugs and other defects can and do appear in any configuration. One common requirement for many development tasks is to identify the configurations that lead to a given defect or some other program behavior. We distill this requirement down to a challenge question: given a program location in a source file, what are valid configurations that include the location? The key obstacle is scalability. When there are thousands of configuration options, enumerating all combinations is exponential and infeasible. We provide a set of target programs of increasing difficulty and variations on the challenge question so that submitters of all experience levels can try out solutions. Our hope is to engage the community and stimulate new and interesting approaches to the problem of analyzing configurations.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {269–273},
numpages = {5},
keywords = {variability, testing, program analysis, configurations},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1007/978-3-030-30244-3_10,
author = {Khoza, Sibusiso C. and Grobler, Jacomine},
title = {Comparing Machine Learning and Statistical Process Control for Predicting Manufacturing Performance},
year = {2019},
isbn = {978-3-030-30243-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30244-3_10},
doi = {10.1007/978-3-030-30244-3_10},
abstract = {Quality has become one of the most important factors in the success of manufacturing companies. In this paper, the use of machine learning algorithms in quality control is compared to the use of statistical process monitoring, a classical quality management technique. The test dataset has a large number of features, which requires the use of principal component analysis and clustering to isolate the data into potential process groups. A Random Forest, Support Vector Machine and Naive Bayes algorithms were used to predict when the manufacturing process is out of control. The Random Forest algorithm performed significantly better than both the Naive Bayes and SVM algorithms in all 3 clusters of the dataset. The results were benchmarked against Hotelling’s  control charts which were trained using 80% of each cluster dataset and tested on the remaining 20%. In comparison with Hotelling’s  multivariate statistical process monitoring charts, the Random Forest algorithm still emerges as the better quality control method.},
booktitle = {Progress in Artificial Intelligence: 19th EPIA Conference on Artificial Intelligence, EPIA 2019, Vila Real, Portugal, September 3–6, 2019, Proceedings, Part II},
pages = {108–119},
numpages = {12},
location = {Vila Real, Portugal}
}

@inproceedings{10.1007/978-3-030-26250-1_32,
author = {Robin, Jacques and Mazo, Raul and Madeira, Henrique and Barbosa, Raul and Diaz, Daniel and Abreu, Salvador},
title = {A Self-certifiable Architecture for Critical Systems Powered by Probabilistic Logic Artificial Intelligence},
year = {2019},
isbn = {978-3-030-26249-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26250-1_32},
doi = {10.1007/978-3-030-26250-1_32},
abstract = {We present a versatile architecture for AI-powered self-adaptive self-certifiable critical systems. It aims at supporting semi-automated low-cost re-certification for self-adaptive systems after each adaptation of their behavior to a persistent change in their operational environment throughout their lifecycle.},
booktitle = {Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings},
pages = {391–397},
numpages = {7},
keywords = {AI certification, Autonomic architecture, Argumentation, Rule-based constraint solving, Probabilistic logic machine learning},
location = {Turku, Finland}
}

@inproceedings{10.1145/3233027.3233041,
author = {Montalvillo, Leticia and D\'{\i}az, Oscar and Fogdal, Thomas},
title = {Reducing coordination overhead in SPLs: peering in on peers},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233041},
doi = {10.1145/3233027.3233041},
abstract = {SPL product customers might not always wait for the next core asset release. When an organization aims to react to market events, quick bug fixes or urgent customer requests, strategies are needed to support fast adaptation, e.g. with product-specific extensions, which are later propagated into the SPL. This leads to the grow-and-prune model where quick reaction to changes often requires copying and specialization (grow) to be later cleaned up by merging and refactoring (prune). This paper focuses on the grow stage. Here, application engineers branch off the core-asset Master branch to account for their products' specifics within the times and priorities of their customers without having to wait for the next release of the core assets. However, this practice might end up in the so-called "integration hell". When long-living branches are merged back into the Master, the amount of code to be integrated might cause build failures or requires complex troubleshooting. On these premises, we advocate for making application engineers aware of potential coordination problems right during coding rather than deferring it until merge time. To this end, we introduce the notion of "peering bar" for Version Control Systems, i.e. visual bars that reflect whether your product's features are being upgraded in other product branches. In this way, engineers are aware of what their peers are doing on the other SPL's products. Being products from the same SPL, they are based on the very same core assets, and hence, bug ixes or functional enhancements undertaken for a product might well serve other products. This work introduces design principles for peering bars. These principles are fleshed out for GitHub as the Version Control System, and pure::variants as the SPL framework.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {110–120},
numpages = {11},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {software product lines, self-configuration, runtime decision-making, recommender systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.infsof.2012.08.010,
author = {Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
title = {Variability in quality attributes of service-based software systems: A systematic literature review},
year = {2013},
issue_date = {February, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.08.010},
doi = {10.1016/j.infsof.2012.08.010},
abstract = {Context: Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective: We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method: A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results: Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions: The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {320–343},
numpages = {24},
keywords = {Quality attributes, Service-based systems, Systematic literature review, Variability}
}

@inproceedings{10.1145/2491627.2491642,
author = {Martini, Antonio and Pareto, Lars and Bosch, Jan},
title = {Communication factors for speed and reuse in large-scale agile software development},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491642},
doi = {10.1145/2491627.2491642},
abstract = {An open issue in industry is the combination of software reuse in the context of large scale Agile Software Development. The speed offered by Agile Software Development is needed for short time to market, while reuse strategies such as Software Product Line Engineering are needed for long-term productivity, efficiency, and profit. The paper investigates, through a survey, communication factors affecting both speed and reuse in 3 large companies developing embedded systems and employing Agile Software Development and Software Product Line Engineering. Our results include a prioritized list of communication related factors obtained by statistical analysis and the recognition and spread of the factors in the companies. We have recognized 5 interfaces with the Agile development team that need to be improved: system engineers (architects), product management, distributed teams, inter-project teams and sales unit. Few factors (involving inter-project communication) depend on the business drivers for the company. We also reveal that Agile teams need strategic and architectural inputs in order to be implanted in a large company employing Software Product Line Engineering. Academic and industrial training as well as different tactics for co-location would improve the communication skills of engineers. There is also a need for solutions, in the reference architecture, for fostering Agile Software Development: the goal is the combination of the focus on customer value of the teams, reusability, system requirements and avoidance of organizational dependencies.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {42–51},
numpages = {10},
keywords = {speed, software reuse, software process improvement (SPI), factors, embedded systems, development speed, communication, agile software development},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2647648.2647649,
author = {Raschke, Wolfgang and Zilli, Massimiliano and Loinig, Johannes and Weiss, Reinhold and Steger, Christian and Kreiner, Christian},
title = {Embedding research in the industrial field: a case of a transition to a software product line},
year = {2014},
isbn = {9781450330459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647648.2647649},
doi = {10.1145/2647648.2647649},
abstract = {Java Cards [4, 5] are small resource-constrained embedded systems that have to fulfill rigorous security requirements. Multiple application scenarios demand diverse product performance profiles which are targeted towards markets such as banking applications and mobile applications. In order to tailor the products to the customer's needs we implemented a Software Product Line (SPL). This paper reports on the industrial case of an adoption to a SPL during the development of a highly-secure software system. In order to provide a scientific method which allows the description of research in the field, we apply Action Research (AR). The rationale of AR is to foster the transition of knowledge from a mature research field to practical problems encountered in the daily routine. Thus, AR is capable of providing insights which might be overlooked in a traditional research approach. In this paper we follow the iterative AR process, and report on the successful transfer of knowledge from a research project to a real industrial application.},
booktitle = {Proceedings of the 2014 International Workshop on Long-Term Industrial Collaboration on Software Engineering},
pages = {3–8},
numpages = {6},
keywords = {software reuse, knowledge transfer, action research},
location = {Vasteras, Sweden},
series = {WISE '14}
}

@inproceedings{10.1145/2993412.3003392,
author = {Boss, Birgit and Tischer, Christian and Krishnan, Sreejith and Nutakki, Arun and Gopinath, Vinod},
title = {Setting up architectural SW health builds in a new product line generation},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3003392},
doi = {10.1145/2993412.3003392},
abstract = {Setting up a new product line generation in a mature domain, typically does not start from scratch but takes into consideration the architecture and assets of the former product line generation. Being able to accommodate legacy and 3rd party code is one of the major product line qualities to be met. On the other side, product line qualities like reusability, maintainability and alterability, i.e. being able to cope up with a large amount of variability, with configurability and fast integratability are major drivers.While setting up a new product line generation and thus a new corresponding architecture, we this time focused on architectural software (SW) health and tracking of architectural metrics from the very beginning. Taking the definition of "architecture being a set of design decisions" [18] literally, we attempt to implement an architectural check for every design decision taken. Architectural design decisions in our understanding do not only - and even not mainly - deal with the definition of components and their interaction but with patterns and rules or anti-patterns. The rules and anti-patterns, "what not to do" or more often also "what not to do &lt;u&gt;any more&lt;/u&gt;", is even more important in setting up a new product line generation because developers are not only used to the old style of developing and the old architecture, but also still have to develop assets for both generations.In this article we describe selected architectural checks that we have implemented, the layered architecture check and the check for usage of obsolete services. Additionally we discuss selected architectural metrics: the coupling coefficient metrics and the instability metrics. In the summary and outlook we describe our experiences and still open topics in setting up architectural SW health checks for a large-scale product line.The real-world examples are taken from the domain of Engine Control Unit development at Robert Bosch GmbH.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {16},
numpages = {7},
keywords = {technical debt, software erosion, software architecture, product line development, embedded software, architectural technical debt, architectural checks},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@article{10.1016/j.ijinfomgt.2018.10.006,
author = {Fernandes, Marta and Canito, Alda and Bol\'{o}n-Canedo, Ver\'{o}nica and Concei\c{c}\~{a}o, Lu\'{\i}s and Pra\c{c}a, Isabel and Marreiros, Goreti},
title = {Data analysis and feature selection for predictive maintenance: A case-study in the metallurgic industry},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2018.10.006},
doi = {10.1016/j.ijinfomgt.2018.10.006},
journal = {Int. J. Inf. Manag.},
month = jun,
pages = {252–262},
numpages = {11},
keywords = {Predictive maintenance, Data analysis, Feature selection, Rule-based model}
}

@inproceedings{10.1145/2019136.2019150,
author = {Serajzadeh, Hadi and Shams, Fereidoon},
title = {The application of swarm intelligence in service-oriented product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019150},
doi = {10.1145/2019136.2019150},
abstract = {Changing markets and environments has made the ability to rapidly adapt to these changes a necessity in software systems. However the costs of changing and adapting systems to new requirements still remains an unsolved issue. In this context service-oriented software product lines were introduced with the aim to combine the reusability of software product line with the flexibility of service-oriented architecture. Although this approach helps build flexible software systems with high levels of reuse, certain issues are raised. The main issue is the complexity that a service-oriented product line will face. Developing systems from internal and external assets, taking into consideration the variety and number of these assets, can cause problems in deciding which asset is best suited for the system. To help solve these issues we propose the use of approaches based on artificial intelligence. In this paper we show how swarm intelligence can be used in service-oriented product lines to reduce complexity and find optimal solutions for the development of software systems. We also present an example of the application of swarm intelligence in finding the optimal product for a service-oriented product line.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {12},
numpages = {7},
keywords = {swarm intelligence, service-oriented product line, optimization},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3106195.3106225,
author = {Schlie, Alexander and Wille, David and Schulze, Sandro and Cleophas, Loek and Schaefer, Ina},
title = {Detecting Variability in MATLAB/Simulink Models: An Industry-Inspired Technique and its Evaluation},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106225},
doi = {10.1145/3106195.3106225},
abstract = {Model-based languages such as MATLAB/Simulink play an essential role in the model-driven development of software systems. To comply with new requirements, it is common practice to create new variants by copying existing systems and modifying them. Commonly referred to as clone-and-own, severe problems arise in the long-run when no dedicated variability management is installed. To allow for a documented and structured reuse of systems, their variability information needs to be reverse-engineered. In this paper, we propose an advanced comparison procedure, the Matching Window Technique, and a customizable metric. Both allow us to overcome structural alterations commonly performed during clone-and-own. We analyze related MATLAB/Simulink models and determine, classify and represent their variability information in an understandable way. With our technique, we assist model engineers in maintaining and evolving existing variants. We provide three feasibility studies with real-world models from the automotive domain and show our technique to be fast and precise. Furthermore, we perform semi-structured interviews with domain experts to assess the potential applicability of our technique in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {215–224},
numpages = {10},
keywords = {variability mining, software maintainability, MATLAB/Simulink},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.cie.2014.01.011,
author = {Deng, S. and Aydin, R. and Kwong, C. K. and Huang, Yun},
title = {Integrated product line design and supplier selection: A multi-objective optimization paradigm},
year = {2014},
issue_date = {April, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {70},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2014.01.011},
doi = {10.1016/j.cie.2014.01.011},
abstract = {Product line design is commonly used to provide higher product variety for satisfying diversified customer needs. To reduce the cost and development time and improve quality of products, companies quite often consider sourcing. Conventionally, product line design and supplier selection are dealt with separately. Some previous studies have been attempted to consider product line design and supplier selection simultaneously but two shortcomings were noted. First, the previous studies considered several objectives as a single objective function in the formulation of optimization models for the integrated problem. Second, positions of product variants to be offered in a product line in competitive markets are not clearly defined that would affect the formulation of marketing strategies for the product line. In this paper, a methodology for integrated product line design and supplier selection is proposed to address the shortcomings in which a multi-objective optimization model is formulated to determine their specifications and select suppliers for maximizing the profit, quality and performance as well as minimizing the cost of the product line. In addition, joint-spacing mapping is introduced to help estimate market share of products and indicate positions of product variants. The proposed methodology can provide decision makers with a better tradeoff among various objectives of product line design, and define market positions of product variants explicitly. The results generated based on the methodology could help companies develop product lines with higher profits, better product quality and larger market share to be obtained. A case study of a product line design of notebook computers was performed to illustrate the effectiveness of the proposed methodology. The results have shown that Pareto optimal product line designs and the specifications of product variants can be determined. Suppliers of components and modules can be selected with considerations of minimum sourcing cost, and maximum performance and quality of product variants. Prices and positions of the product variants can also be determined.},
journal = {Comput. Ind. Eng.},
month = apr,
pages = {150–158},
numpages = {9},
keywords = {Multi-objective optimization, NSGA II, Product line design, Supplier selection}
}

@inproceedings{10.1145/2934466.2934469,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof and Yu, Huiqun},
title = {A mathematical model of performance-relevant feature interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934469},
doi = {10.1145/2934466.2934469},
abstract = {Modern software systems have grown significantly in their size and complexity, therefore understanding how software systems behave when there are many configuration options, also called features, is no longer a trivial task. This is primarily due to the potentially complex interactions among the features. In this paper, we propose a novel mathematical model for performance-relevant, or quantitative in general, feature interactions, based on the theory of Boolean functions. Moreover, we provide two algorithms for detecting all such interactions with little measurement effort and potentially guaranteed accuracy and confidence level. Empirical results on real-world configurable systems demonstrated the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {25–34},
numpages = {10},
keywords = {performance, fourier transform, feature interactions, boolean functions},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1007/978-3-030-78361-7_26,
author = {Fujinuma, Ryota and Asahi, Yumi},
title = {Proposal of Credit Risk Model Using Machine Learning in Motorcycle Sales},
year = {2021},
isbn = {978-3-030-78360-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78361-7_26},
doi = {10.1007/978-3-030-78361-7_26},
abstract = {While the new BIS regulations are reviewing the way of thinking about loans all over the world, many people in Central and South America still have a vague way of thinking about loans. It is due to the global recession. As a result, companies have not been able to recover their manufacturing costs. Therefore, in this study, we create a classification model of customers who default and customers who do not default. Also, explore the characteristics of the default customers. This is because it is thought that it will be easier for companies to improve the loan problem and secure profits.In this study, we compare the accuracy of Random Forest and XG boost. Since the data handled in this study were unbalanced data, data expansion by Synthetic Minority Over-sampling Technique (SMOTE) was effective. Mainly the accuracy of Recall has increased by 30%. Feature selection is performed by correlation, which is one of the filter methods. This can be expected to have the effect of improving accuracy and the effect of improving the interpretability of the model. We were able to reduce it from 46 variables to 22 variables. Furthermore, the accuracy increased by 1% for Binary Accuracy and 1% for Recall. The accuracy decreased when the number of variables was reduced by 23 variables or more. This is probably because important features have been deleted. Shows the accuracy of the model. The accuracy of Random Forest is Binary Accuracy = 61.3%, Recall = 58.2%. The accuracy of XGboost is Binary Accuracy = 60.3%, Recall = 61.6%. Therefore, XG boost became the model that can identify the default of the customer than the random forest.Finally, SHApley Additive exPlanations (SHAP) analyzes what variables contribute to the model. From this analysis result, we will explore the characteristics of what kind of person is the default customer. The variables with the highest contribution were the type of vehicle purchased, the area where the customer lives, and credit information. It turns out that customers who have gone loan bankruptcy in the past tend to be loan bankruptcy again.},
booktitle = {Human Interface and the Management of Information. Information-Rich and Intelligent Environments: Thematic Area, HIMI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II},
pages = {353–363},
numpages = {11},
keywords = {Loan, Loan bankruptcy, Credit risk model, Machine learning}
}

@inproceedings{10.1145/2934466.2934485,
author = {Lape\~{n}a, Ra\'{u}l and Ballarin, Manuel and Cetina, Carlos},
title = {Towards clone-and-own support: locating relevant methods in legacy products},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934485},
doi = {10.1145/2934466.2934485},
abstract = {Clone-and-Own (CAO) is a common practice in families of software products consisting of reusing code from methods in legacy products in new developments. In industrial scenarios, CAO consumes high amounts of time and effort without guaranteeing good results. We propose a novel approach, Computer Assisted CAO (CACAO), that given the natural language requirements of a new product, and the legacy products from that family, ranks the legacy methods in the family for each of the new product requirements according to their relevancy to the new development. We evaluated our approach in the industrial domain of train control software. Without CACAO, software engineers tasked with the development of a new product had to manually review a total of 2200 methods in the family. Results show that CACAO can reduce the number of methods to be reviewed, and guide software engineers towards the identification of relevant legacy methods to be reused in the new product.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {194–203},
numpages = {10},
keywords = {software reuse, families of software products, clone and own},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3106195.3106203,
author = {Seidl, Christoph and Berger, Thorsten and Elsner, Christoph and Schultis, Klaus-Benedikt},
title = {Challenges and Solutions for Opening Small and Medium-Scale Industrial Software Platforms},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106203},
doi = {10.1145/3106195.3106203},
abstract = {Establishing open software platforms is becoming increasingly important. Many vendors of large and well-known open platforms, such as Android or iOS, have successfully established huge ecosystems of platform extensions (apps). While such platforms are important role models, the practices and technologies employed by their vendors are often not applicable for smaller platform vendors, who have different goals and carry substantial legacy, such as an existing closed platform. Yet, many vendors start to open their platforms---for instance, when they alone cannot realize all incoming requirements anymore. Unfortunately, very few best practices exist to guide this opening process, especially for small and medium-scale industrial platforms with their specific solutions. We present a study of industrial organizations that successfully opened closed platforms. Using a survey, we identified 18 opened platforms, providing a broad picture, which is complemented with in-depth, qualitative insights from a case study of three organizations. We elicited the platforms' core characteristics, the organizations' opening strategies, as well as challenges and solutions. We believe that our results support practitioners seeking to open platforms, and researchers striving to build better methods and tools.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {153–162},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2934466.2934476,
author = {Eichelberger, Holger and Qin, Cui and Sizonenko, Roman and Schmid, Klaus},
title = {Using IVML to model the topology of big data processing pipelines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934476},
doi = {10.1145/2934466.2934476},
abstract = {Creating product lines of Big Data stream processing applications introduces a number of novel challenges to variability modeling. In this paper, we discuss these challenges and demonstrate how advanced variability modeling capabilities can be used to directly model the topology of processing pipelines as well as their variability. We also show how such processing pipelines can be modeled, configured and validated using the Integrated Variability Modeling Language (IVML).},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {204–208},
numpages = {5},
keywords = {variability modeling, topologies, software product lines},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1007/978-3-642-33666-9_46,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel and Walawege, Suneth},
title = {A product line modeling and configuration methodology to support model-based testing: an industrial case study},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_46},
doi = {10.1007/978-3-642-33666-9_46},
abstract = {Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behavioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {726–742},
numpages = {17},
keywords = {product line engineering, model-based testing, behavioral variability, aspect-oriented modeling, UML state machine},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@book{10.5555/2559472,
author = {Rocha, Claudio and Akune, Fernando and El-Shafie, Ahmed},
title = {Artificial Intelligence and Hybrid Systems},
year = {2013},
isbn = {1477554734},
publisher = {CreateSpace Independent Publishing Platform},
address = {North Charleston, SC, USA},
abstract = {The use of artificial intelligence and hybrid systems has increased dramatically due to their ability in handling real world problems involving uncertainty, vagueness and high complexity. The development of these systems has attracted the interest of the Artificial Intelligence community and established as a promising field of research. In order to present the ideas and practices of the hybridization process of intelligent systems, in this book, we included some recent and interesting studies on this topic. Chapter 1 thoroughly analyzes the behaviors of perceptron, including solution space for every possible patterns and convergence trajectories toward solutions, as well as briefly discusses the behaviors of multiple perceptrons. In Chapter 2, accurate as well as efficient non-invasive computational intelligence approaches (different artificial neural network models and a Sugeno-type fuzzy logic inference system) are implemented for the classification of two-phase flow in boiling water reactors. Chapter 3 contains a nice compilation of indifference-zone selection procedures as they apply to reliability analysis; many of those results come from the authors' own research. Chapter 4 discusses the software project design with the implementation of product line concepts to meet the customers demands for the production of high quality software applications in shortest possible time, within low budget and using less number of resources. Chapter 5 proposes integration of deterministic and probabilistic for improvement of human reliability analysis. The operator action success criteria time windows needed for human reliability analysis were determined through deterministic safety analysis. Chapter 6 presents a scheme to produce Network Anomaly Detection models based on Evolutionary Computation. The models are Hidden Markov Models, produced automatically, with no human intervention. Chapter 7 proposes an integrated online system which ranks the relational data. A compressed data structure is introduced to encode the dominant relationship of the data. Efficient querying strategies and updating scheme are devised to facilitate the ranking process. Chapter 8 proposes a method to combine the output of dependency parsers trained with the same parser generator but with specifically selected corpora. Chapter 9 proposes a new optimization model for the unit commitment problem using a stochastic hybrid algorithm combining simulated annealing and evolutionary algorithm. Chapter 10 develops neural network (NN) models to predict 85th percentile speed for two-lane rural highways in Oklahoma. Several input parameters, namely physical characteristics of road, traffic parameters, pavement condition indices, and accident data were considered in developing the models. In Chapter 11, a comparative study between micro-controller and computer for various network types is given, and an implementation of the a Radial Basis Function (RBF) network on the low end and inexpensive micro-controller is proposed. Chapter 12 provides an improved and updated survey of the recent researches and patents which concern about statistical background modeling and subtraction. Chapter 13 offers an outline of the hybrid approach, the design methodology of the INVENTS system, as well as its structure and user interface. In Chapter 14, it presents and discusses applications of the artificial neural network approach for improving equivalent circuit based transistor noise models in terms of the accuracy and the range of validity. Chapter 15 presents a simple yet efficient approach to improve IR on the web by reusing the information contained in the past user queries as the expansion for the initial query.}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {software product-lines, software design, self-adaptation, product-line management, online learning, knowledge sharing},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2245276.2231956,
author = {Horikoshi, Hisayuki and Nakagawa, Hiroyuki and Tahara, Yasuyuki and Ohsuga, Akihiko},
title = {Dynamic reconfiguration in self-adaptive systems considering non-functional properties},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231956},
doi = {10.1145/2245276.2231956},
abstract = {Self-adaptive systems have recently been receiving much attention because of their ability to cope with the changes of environment, failures, and unanticipated events. These systems need an adaptation mechanism, which automatically computes the possible configurations, and decides the most appropriate configuration to fit the environment. In particular, the satisfaction of non-functional requirements must be considered when selecting the best reconfiguration. However, there are trade-off problems among non-functional requirements. Moreover, the adaptation mechanisms are typically developed separately from the components to be implemented, and it complicates the construction of such systems. We propose (1) a feature-oriented analysis technique, which can identify adaptation points, and calculate the contribution to non-functional goals of the configuration; (2) a component specification model, which extends an architectural description language for self-adaptation; (3) a reconfiguration framework aimed to reduce the complexity of the reconfiguration and generate the best configuration at run-time. We evaluate the feasibility of our framework by four different scenarios, and show that our framework reduces the complexity of the reconfiguration, and solves the trade-off problem among non-functional requirements.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1144–1150},
numpages = {7},
keywords = {software architecture, self-adaptive systems, feature-oriented analysis, dynamic reconfiguration, architecture description language},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/2019136.2019162,
author = {Quinton, Cl\'{e}ment and Mosser, S\'{e}bastien and Parra, Carlos and Duchien, Laurence},
title = {Using multiple feature models to design applications for mobile phones},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019162},
doi = {10.1145/2019136.2019162},
abstract = {The design of a mobile phone application is a tedious task according to its intrinsic variability. Software designers must take into account in their development process the versatility of available platforms (e.g., Android, iPhone). In addition to this, the variety of existing devices and their divergences (e.g., frontal camera, GPS) introduce another layer of complexity in the development process. These two dimensions can be formalized as Software Product Lines (SPL), independently defined. In this paper, we use a dedicated metamodel to bridge the gap between an application SPL and a mobile device one. This meta-model is also the support for the product derivation process. The approach is implemented in a framework named Applide, and is used to successfully derive customer relationship management software on different devices.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {23},
numpages = {8},
keywords = {software product line, smartphones, meta-model, feature model, application for mobile phones},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3109729.3109734,
author = {Marc\'{e}n, Ana C. and Font, Jaime and Pastor, \'{O}scar and Cetina, Carlos},
title = {Towards Feature Location in Models through a Learning to Rank Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109734},
doi = {10.1145/3109729.3109734},
abstract = {In this work, we propose a feature location approach to discover software artifacts that implement the feature functionality in a model. Given a model and a feature description, model fragments extracted from the model and the feature description are encoded based on a domain ontology. Then, a Learning to Rank algorithm is used to train a classifier that is based on the model fragments and feature description encoded. Finally, the classifier assesses the similarity between a population of model fragments and the target feature being located to find the set of most suitable feature realizations. We have evaluated the approach with an industrial case study, locating features with mean precision and recall values of around 73.75% and 73.31%, respectively (the sanity check obtains less than 35%).},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3177540.3177555,
author = {Wang, Li-C.},
title = {Machine Learning for Feature-Based Analytics},
year = {2018},
isbn = {9781450356268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177540.3177555},
doi = {10.1145/3177540.3177555},
abstract = {Applying machine learning in Electronic Design Automation (EDA) has received growing interests in recent years. One approach to analyze data in EDA applications can be called feature-based analytics. In this context, the paper explains the inadequacy of adopting a traditional machine learning problem formulation view. Then, an alternative machine learning view is suggested where learning from data is treated as an iterative search process. The theoretical and practical considerations for implementing such a search process are discussed in the context of various applications.},
booktitle = {Proceedings of the 2018 International Symposium on Physical Design},
pages = {74–81},
numpages = {8},
keywords = {version space, machine learning, learnable, feature-based analytics, design automation, Occam's razor},
location = {Monterey, California, USA},
series = {ISPD '18}
}

@article{10.1007/s10664-015-9414-4,
author = {Mkaouer, Mohamed Wiem and Kessentini, Marouane and Bechikh, Slim and O\'{z} Cinne\'{z}Ide, Mel and Deb, Kalyanmoy},
title = {On the use of many quality attributes for software refactoring: a many-objective search-based software engineering approach},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9414-4},
doi = {10.1007/s10664-015-9414-4},
abstract = {Search-based software engineering (SBSE) solutions are still not scalable enough to handle high-dimensional objectives space. The majority of existing work treats software engineering problems from a single or bi-objective point of view, where the main goal is to maximize or minimize one or two objectives. However, most software engineering problems are naturally complex in which many conflicting objectives need to be optimized. Software refactoring is one of these problems involving finding a compromise between several quality attributes to improve the quality of the system while preserving the behavior. To this end, we propose a novel representation of the refactoring problem as a many-objective one where every quality attribute to improve is considered as an independent objective to be optimized. In our approach based on the recent NSGA-III algorithm, the refactoring solutions are evaluated using a set of 8 distinct objectives. We evaluated this approach on one industrial project and seven open source systems. We compared our findings to: several other many-objective techniques (IBEA, MOEA/D, GrEA, and DBEA-Eps), an existing multi-objective approach a mono-objective technique and an existing refactoring technique not based on heuristic search. Statistical analysis of our experiments over 31 runs shows the efficiency of our approach.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {2503–2545},
numpages = {43},
keywords = {Many-objective optimization, Refactoring, Search-based software engineering, Software quality}
}

@inproceedings{10.1145/3336294.3336297,
author = {Munoz, Daniel-Jesus and Oh, Jeho and Pinto, M\'{o}nica and Fuentes, Lidia and Batory, Don},
title = {Uniform Random Sampling Product Configurations of Feature Models That Have Numerical Features},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336297},
doi = {10.1145/3336294.3336297},
abstract = {Analyses of Software Product Lines (SPLs) rely on automated solvers to navigate complex dependencies among features and find legal configurations. Often these analyses do not support numerical features with constraints because propositional formulas use only Boolean variables. Some automated solvers can represent numerical features natively, but are limited in their ability to count and Uniform Random Sample (URS) configurations, which are key operations to derive unbiased statistics on configuration spaces.Bit-blasting is a technique to encode numerical constraints as propositional formulas. We use bit-blasting to encode Boolean and numerical constraints so that we can exploit existing #SAT solvers to count and URS configurations. Compared to state-of-art Satisfiability Modulo Theory and Constraint Programming solvers, our approach has two advantages: 1) faster and more scalable configuration counting and 2) reliable URS of SPL configurations. We also show that our work can be used to extend prior SAT-based SPL analyses to support numerical features and constraints.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {289–301},
numpages = {13},
keywords = {software product lines, propositional formula, numerical features, model counting, feature model, bit-blasting},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2791060.2791093,
author = {Souto, Sabrina and Gopinath, Divya and d'Amorim, Marcelo and Marinov, Darko and Khurshid, Sarfraz and Batory, Don},
title = {Faster bug detection for software product lines with incomplete feature models},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791093},
doi = {10.1145/2791060.2791093},
abstract = {A software product line (SPL) is a family of programs that are differentiated by features --- increments in functionality. Systematically testing an SPL is challenging because it requires running each test of a test suite against a combinatorial number of programs. Feature models capture dependencies among features and can (1) reduce the space of programs to test and (2) enable accurate categorization of failing tests as failures of programs or the tests themselves, not as failures due to illegal combinations of features. In practice, sadly, feature models are not always available.We introduce SPLif, the first approach for testing SPLs that does not require the a priori availability of feature models. Our insight is to use a profile of passing and failing test runs to quickly identify failures that are indicative of real problems in test or code rather than specious failures due to illegal feature combinations.Experimental results on five SPLs and one large configurable system (GCC) demonstrate the effectiveness of our approach. SPLif enabled the discovery of five news bugs in GCC, three of which have already been fixed.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {151–160},
numpages = {10},
keywords = {software testing, feature models, GCC},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1155/2019/4368036,
author = {Deli\'{c}, Vlado and Peri\'{c}, Zoran and Se\v{c}ujski, Milan and Jakovljevi\'{c}, Nik\v{s}a and Nikoli\'{c}, Jelena and Mi\v{s}kovi\'{c}, Dragi\v{s}a and Simi\'{c}, Nikola and Suzi\'{c}, Sini\v{s}a and Deli\'{c}, Tijana and Gastaldo, Paolo},
title = {Speech Technology Progress Based on New Machine Learning Paradigm},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1687-5265},
url = {https://doi.org/10.1155/2019/4368036},
doi = {10.1155/2019/4368036},
abstract = {Speech technologies have been developed for decades as a typical signal processing area, while the last decade has brought a huge progress based on new machine learning paradigms. Owing not only to their intrinsic complexity but also to their relation with cognitive sciences, speech technologies are now viewed as a prime example of interdisciplinary knowledge area. This review article on speech signal analysis and processing, corresponding machine learning algorithms, and applied computational intelligence aims to give an insight into several fields, covering speech production and auditory perception, cognitive aspects of speech communication and language understanding, both speech recognition and text-to-speech synthesis in more details, and consequently the main directions in development of spoken dialogue systems. Additionally, the article discusses the concepts and recent advances in speech signal compression, coding, and transmission, including cognitive speech coding. To conclude, the main intention of this article is to highlight recent achievements and challenges based on new machine learning paradigms that, over the last decade, had an immense impact in the field of speech signal processing.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@inproceedings{10.1109/ASE.2013.6693104,
author = {Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
title = {Scalable product line configuration: a straw to break the camel's back},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693104},
doi = {10.1109/ASE.2013.6693104},
abstract = {Software product lines are hard to configure. Techniques that work for medium sized product lines fail for much larger product lines such as the Linux kernel with 6000+ features. This paper presents simple heuristics that help the Indicator-Based Evolutionary Algorithm (IBEA) in finding sound and optimum configurations of very large variability models in the presence of competing objectives. We employ a combination of static and evolutionary learning of model structure, in addition to utilizing a pre-computed solution used as a "seed" in the midst of a randomly-generated initial population. The seed solution works like a single straw that is enough to break the camel's back -given that it is a feature-rich seed. We show promising results where we can find 30 sound solutions for configuring upward of 6000 features within 30 minutes.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {465–474},
numpages = {10},
keywords = {variability models, multiobjective optimization, evolutionary algorithms, automated configuration, SMT solvers},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1145/2364412.2364439,
author = {Vale, Tassio and Figueiredo, Gustavo Bittencourt and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {A study on service identification methods for software product lines},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364439},
doi = {10.1145/2364412.2364439},
abstract = {The combination of service-orientation and software product line engineering, called Service-Oriented Product Line Engineering (SOPLE) have received attention by researchers and practitioners in the last years, and these areas can address issues of each other. One service-orientation issue is service identification. It consists of determining candidate services to a service-oriented environment based on pre-existing software artifacts, e.g., business process, source code, and so on. In order to provide a systematic identification of services, there are many available service identification methods in the literature, regarding different understanding of services, goals, and techniques. Due to this heterogeneity, this paper presents an in-depth comparison of service identification methods as well as a recommendation of the most suitable ones in the SOPLE context. This work can help the decision making of the most suitable method according to stakeholders' needs.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {156–163},
numpages = {8},
keywords = {software product lines, service-oriented product lines, service-oriented computing, service identification},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/1858996.1859064,
author = {Boucher, Quentin and Classen, Andreas and Heymans, Patrick and Bourdoux, Arnaud and Demonceau, Laurent},
title = {Tag and prune: a pragmatic approach to software product line implementation},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859064},
doi = {10.1145/1858996.1859064},
abstract = {To realise variability at the code level, product line methods classically advocate usage of inheritance, components, frameworks, aspects or generative techniques. However, these might require unaffordable paradigm shifts for the developers if the software was not thought at the outset as a product line. Furthermore, these techniques can be conflicting with a company's coding practices or external regulations.These concerns were the motivation for the industry-university collaboration described in this paper where we develop a minimally intrusive coding technique based on tags. It is supported by a toolchain and is now in use in the partner company for the development of flight grade satellite communication software libraries.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {333–336},
numpages = {4},
keywords = {feature diagram, code tagging},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@article{10.1007/s10664-019-09769-8,
author = {Ochodek, Miroslaw and Hebig, Regina and Meding, Wilhelm and Frost, Gert and Staron, Miroslaw},
title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09769-8},
doi = {10.1007/s10664-019-09769-8},
abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {220–265},
numpages = {46},
keywords = {Measurement, Machine learning, Action research, Code reviews}
}

@article{10.1287/msom.2019.0788,
author = {Li, Hongmin and Webster, Scott and Yu, Gwangjae},
title = {Product Design Under Multinomial Logit Choices: Optimization of Quality and Prices in an Evolving Product Line},
year = {2020},
issue_date = {September–October 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {22},
number = {5},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2019.0788},
doi = {10.1287/msom.2019.0788},
abstract = {Problem definition: We study a product-line design problem in which customer choice among multiple products is given by a multinomial logit (MNL) model. A firm determines product quality and prices in an evolving product line to maximize profit. In particular, given the prices and quality of products that already exist in a product line, the firm optimizes prices and/or quality of the new products. Academic/practical relevance: We extend the literature on discrete choice models to include the interaction between product quality and product price and consider two variations of the problem, each mirroring a relevant decision setting found in practice: Variation i is a price-optimization problem in which the firm determines prices of the new products given the quality. Variation ii is a joint price and quality optimization of the new products. Methodology: We apply convex optimization techniques and analyze properties of optimal solutions. Results: We establish concavity of the profit function under price optimization and present tractable solution approaches for the joint quality–price optimization. For each problem variation, we characterize the optimal solution and develop efficient algorithms. We show that the interaction of price and quality is central not only to reconciling the divergence of the existing literature’s equal-markup price prediction from differentiated markups observed in practice, but also for explaining differentiated quality measures across products; this empirically observed strategy can now be quantified and optimized with the model developed in this paper. In addition, we show that the presence of existing products tends to drive the firm to offer new products with both higher quality and prices because of the price–quality interaction. Managerial implications: Findings of this paper offer not only managerial guidelines, but also tools for decision support because of the wide empirical applicability of the MNL model. An important managerial implication is that the lack of realism in the linear utility of the MNL model can be addressed by including price–quality interaction, which is central to understanding the quality and price decision in product-line design. The interaction rationalizes the matching of high markup with high quality and justifies differentiated offering of new products in the presence of existing products.},
journal = {Manufacturing &amp; Service Operations Management},
month = sep,
pages = {1011–1025},
numpages = {15},
keywords = {pricing, revenue management, multinomial logit, product line design}
}

@inproceedings{10.1145/3493244.3493274,
author = {Silva, Leandro F. and OliveiraJr, Edson},
title = {SMartyModeling: an instance of VMTools-RA for Engineering UML-based Software Product Lines},
year = {2021},
isbn = {9781450395533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493244.3493274},
doi = {10.1145/3493244.3493274},
abstract = {Software Product Line (SPL) life cycle comprises a set of essential activities. Variability Management (VM) is one of its most important activities to the success of an SPL, especially those based on UML, as the solution space encompasses different diagrams and perspectives on variability. However, the lack of tools to support UML-based SPLs reflects difficulties in adopting this approach. This scenario motivated the development of SMartyModeling, an environment for engineering UML-based SPL. The SMartyModeling architecture was instantiated based on VMTools-RA, an existing reference architecture for software variability tools. VMTools-RA describes architectural requirements, elements and views on software variability, which aid one to instantiate variability tool architectures. The instantiation process started from the identification of requirements, selection of elements, modules, and visions of VMTools-RA, planning and design of the architectural solutions, implementation of modules and organization of features. We then analyzed the feasibility of adopting VMTools-RA for instantiating an specific tool architecture. In this sense, such instantiation is part of the development process of SMartyModeling, which includes the main activities related to VM. We also empirically evaluated SMartyModeling in three ways: (i) a field study to analyze the instantiation process and the decisions taken; (ii) a comparative experiment analyzing efficiency and effectiveness of SMartyModeling in relation to a general purpose UML tool; and (iii) an evaluation of aspects related to perceived ease of use and perceived usability. The results of such evaluations provide initial evidence VMTools-RA is feasible to instantiate specific architectures and SMartyModeling is feasible to support to VM for UML-based SPLs.},
booktitle = {Proceedings of the XX Brazilian Symposium on Software Quality},
articleno = {33},
numpages = {10},
keywords = {Software Product Line. SMartyModeling. Environment for Modeling Software Product Line. VMTools-RA. UML. Empirical studies.},
location = {Virtual Event, Brazil},
series = {SBQS '21}
}

@inproceedings{10.5555/1885639.1885655,
author = {Chen, Lianping and Babar, Muhammad Ali},
title = {Variability management in software product lines: an investigation of contemporary industrial challenges},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Variability management is critical for achieving the large scale reuse promised by the software product line paradigm. It has been studied for almost 20 years. We assert that it is important to explore how well the body of knowledge of variability management solves the challenges faced by industrial practitioners, and what are the remaining and (or) emerging challenges. To gain such understanding of the challenges of variability management faced by practitioners, we have conducted an empirical study using focus group as data collection method. The results of the study highlight several technical challenges that are often faced by practitioners in their daily practices. Different from previous studies, the results also reveal and shed light on several non-technical challenges that were almost neglected by existing research.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {166–180},
numpages = {15},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/2791060.2791068,
author = {B\'{e}can, Guillaume and Behjati, Razieh and Gotlieb, Arnaud and Acher, Mathieu},
title = {Synthesis of attributed feature models from product descriptions},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791068},
doi = {10.1145/2791060.2791068},
abstract = {Many real-world product lines are only represented as nonhierarchical collections of distinct products, described by their configuration values. As the manual preparation of feature models is a tedious and labour-intensive activity, some techniques have been proposed to automatically generate boolean feature models from product descriptions. However, none of these techniques is capable of synthesizing feature attributes and relations among attributes, despite the huge relevance of attributes for documenting software product lines. In this paper, we introduce for the first time an algorithmic and parametrizable approach for computing a legal and appropriate hierarchy of features, including feature groups, typed feature attributes, domain values and relations among these attributes. We have performed an empirical evaluation by using both randomized configuration matrices and real-world examples. The initial results of our evaluation show that our approach can scale up to matrices containing 2,000 attributed features, and 200,000 distinct configurations in a couple of minutes.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {1–10},
numpages = {10},
keywords = {product descriptions, attributed feature models},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.infsof.2012.07.010,
author = {Buchmann, Thomas and Dotor, Alexander and Westfechtel, Bernhard},
title = {MOD2-SCM: A model-driven product line for software configuration management systems},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.010},
doi = {10.1016/j.infsof.2012.07.010},
abstract = {Context: Software Configuration Management (SCM) is the discipline of controlling the evolution of large and complex software systems. Over the years many different SCM systems sharing similar concepts have been implemented from scratch. Since these concepts usually are hard-wired into the respective program code, reuse is hardly possible. Objective: Our objective is to create a model-driven product line for SCM systems. By explicitly describing the different concepts using models, reuse can be performed on the modeling level. Since models are executable, the need for manual programming is eliminated. Furthermore, by providing a library of loosely coupled modules, we intend to support flexible composition of SCM systems. Method: We developed a method and a tool set for model-driven software product line engineering which we applied to the SCM domain. For domain analysis, we applied the FORM method, resulting in a layered feature model for SCM systems. Furthermore, we developed an executable object-oriented domain model which was annotated with features from the feature model. A specific SCM system is configured by selecting features from the feature model and elements of the domain model realizing these features. Results: Due to the orthogonality of both feature model and domain model, a very large number of SCM systems may be configured. We tested our approach by creating instances of the product line which mimic wide-spread systems such as CVS, GIT, Mercurial, and Subversion. Conclusion: The experiences gained from this project demonstrate the feasibility of our approach to model-driven software product line engineering. Furthermore, our work advances the state of the art in the domain of SCM systems since it support the modular composition of SCM systems at the model rather than the code level.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {630–650},
numpages = {21},
keywords = {Software product line engineering, Software configuration management, Model-driven software engineering, Model transformation, Feature models, Executable models, Code generation}
}

@article{10.1155/2021/4767388,
author = {Soleymani, Ali and Arabgol, Fatemeh and Shojae Chaeikar, Saman},
title = {A Novel Approach for Detecting DGA-Based Botnets in DNS Queries Using Machine Learning Techniques},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {2090-7141},
url = {https://doi.org/10.1155/2021/4767388},
doi = {10.1155/2021/4767388},
abstract = {In today’s security landscape, advanced threats are becoming increasingly difficult to detect as the pattern of attacks expands. Classical approaches that rely heavily on static matching, such as blacklisting or regular expression patterns, may be limited in flexibility or uncertainty in detecting malicious data in system data. This is where machine learning techniques can show their value and provide new insights and higher detection rates. The behavior of botnets that use domain-flux techniques to hide command and control channels was investigated in this research. The machine learning algorithm and text mining used to analyze the network DNS protocol and identify botnets were also described. For this purpose, extracted and labeled domain name datasets containing healthy and infected DGA botnet data were used. Data preprocessing techniques based on a text-mining approach were applied to explore domain name strings with n-gram analysis and PCA. Its performance is improved by extracting statistical features by principal component analysis. The performance of the proposed model has been evaluated using different classifiers of machine learning algorithms such as decision tree, support vector machine, random forest, and logistic regression. Experimental results show that the random forest algorithm can be used effectively in botnet detection and has the best botnet detection accuracy.},
journal = {J. Comput. Netw. Commun.},
month = jan,
numpages = {13}
}

@article{10.14778/2078324.2078325,
author = {Pavlo, Andrew and Jones, Evan P. C. and Zdonik, Stanley},
title = {On predictive modeling for optimizing transaction execution in parallel OLTP systems},
year = {2011},
issue_date = {October 2011},
publisher = {VLDB Endowment},
volume = {5},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/2078324.2078325},
doi = {10.14778/2078324.2078325},
abstract = {A new emerging class of parallel database management systems (DBMS) is designed to take advantage of the partitionable workloads of on-line transaction processing (OLTP) applications [23, 20]. Transactions in these systems are optimized to execute to completion on a single node in a shared-nothing cluster without needing to coordinate with other nodes or use expensive concurrency control measures [18]. But some OLTP applications cannot be partitioned such that all of their transactions execute within a single-partition in this manner. These distributed transactions access data not stored within their local partitions and subsequently require more heavy-weight concurrency control protocols. Further difficulties arise when the transaction's execution properties, such as the number of partitions it may need to access or whether it will abort, are not known beforehand. The DBMS could mitigate these performance issues if it is provided with additional information about transactions. Thus, in this paper we present a Markov model-based approach for automatically selecting which optimizations a DBMS could use, namely (1) more efficient concurrency control schemes, (2) intelligent scheduling, (3) reduced undo logging, and (4) speculative execution. To evaluate our techniques, we implemented our models and integrated them into a parallel, main-memory OLTP DBMS to show that we can improve the performance of applications with diverse workloads.},
journal = {Proc. VLDB Endow.},
month = oct,
pages = {85–96},
numpages = {12}
}

@article{10.1287/mnsc.1080.0864,
author = {Belloni, Alexandre and Freund, Robert and Selove, Matthew and Simester, Duncan},
title = {Optimizing Product Line Designs: Efficient Methods and Comparisons},
year = {2008},
issue_date = {September 2008},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {54},
number = {9},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.1080.0864},
doi = {10.1287/mnsc.1080.0864},
abstract = {We take advantage of recent advances in optimization methods and computer hardware to identify globally optimal solutions of product line design problems that are too large for complete enumeration. We then use this guarantee of global optimality to benchmark the performance of more practical heuristic methods. We use two sources of data: (1) a conjoint study previously conducted for a real product line design problem, and (2) simulated problems of various sizes. For both data sources, several of the heuristic methods consistently find optimal or near-optimal solutions, including simulated annealing, divide-and-conquer, product-swapping, and genetic algorithms.},
journal = {Manage. Sci.},
month = sep,
pages = {1544–1552},
numpages = {9},
keywords = {product line design, optimization, conjoint}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and J\'{e}z\'{e}quel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Quality assurance, Machine learning, Software testing, Software variability, Configurable system, Software product line}
}

@article{10.1016/j.asoc.2018.12.012,
author = {Arevalillo, Jorge M.},
title = {A machine learning approach to assess price sensitivity with application to automobile loan segmentation},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {76},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2018.12.012},
doi = {10.1016/j.asoc.2018.12.012},
journal = {Appl. Soft Comput.},
month = mar,
pages = {390–399},
numpages = {10},
keywords = {Price sensitivity, Model based recursive partitioning, Random forests, Conditional inference trees, Machine learning}
}

@inproceedings{10.1145/2791060.2791078,
author = {Vale, Gustavo and Albuquerque, Danyllo and Figueiredo, Eduardo and Garcia, Alessandro},
title = {Defining metric thresholds for software product lines: a comparative study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791078},
doi = {10.1145/2791060.2791078},
abstract = {A software product line (SPL) is a set of software systems that share a common and variable set of features. Software metrics provide basic means to quantify several modularity aspects of SPLs. However, the effectiveness of the SPL measurement process is directly dependent on the definition of reliable thresholds. If thresholds are not properly defined, it is difficult to actually know whether a given metric value indicates a potential problem in the feature implementation. There are several methods to derive thresholds for software metrics. However, there is little understanding about their appropriateness for the SPL context. This paper aims at comparing three methods to derive thresholds based on a benchmark of 33 SPLs. We assess to what extent these methods derive appropriate values for four metrics used in product-line engineering. These thresholds were used for guiding the identification of a typical anomaly found in features' implementation, named God Class. We also discuss the lessons learned on using such methods to derive thresholds for SPLs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {176–185},
numpages = {10},
keywords = {thresholds, software product lines, metrics},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.5555/1753235.1753241,
author = {John, Isabel and Eisenbarth, Michael},
title = {A decade of scoping: a survey},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Scoping can be defined as the process of deciding in which parts of an organization's products, features and domains systematic reuse is economically useful. It generally is the first phase in product line engineering. For a decade now scoping has been recognized as a discipline of it's own in product line engineering. So it's time to look at what has been done in scoping in the last years and what is still to be done. In this survey, we identify and characterize existing scoping approaches with the main goal to derive open areas and research questions for further research in scoping. We analyze and compare existing approaches and derive open and partially addressed research questions that can be tackled by researchers in product line engineering in the next years.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {31–40},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1155/2021/2658090,
author = {Shi, Xiaorui and Cui, Wei and Zhu, Ping and Yang, Yanhua and Souri, Alireza},
title = {Research on Automobile Assembly Line Optimization Based on Industrial Engineering Technology and Machine Learning Algorithm},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/2658090},
doi = {10.1155/2021/2658090},
abstract = {Aiming at the lack of search depth of traditional genetic algorithm in automobile assembly line balance optimization, an improved genetic algorithm based on bagging integrated clustering is proposed for balance optimization. Through the integrated learning of several K-means algorithm based learners through bagging, a population clustering analysis method based on bagging integrated clustering algorithm is established, and then, a dual objective automobile assembly line balance optimization model is established. The population clustering analysis method is used to improve the intersection link of genetic algorithm to improve the search depth. The effectiveness and search performance of the improved genetic algorithm in solving the double objective assembly line balance problem are verified in an example.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {9}
}

@inproceedings{10.1145/2791060.2791116,
author = {Eichelberger, Holger and Schmid, Klaus},
title = {IVML: a DSL for configuration in variability-rich software ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791116},
doi = {10.1145/2791060.2791116},
abstract = {Variability-rich Software Ecosystems need configuration capabilities just as in any product line. However, various additional capabilities are required, taking into account the software ecosystem characteristics. In order to address these specific needs, we developed the Integrated Variability Modeling Language (IVML) for describing configurations of variability-rich software ecosystems. IVML is a variability modeling and configuration language along with accompanying reasoning facilities.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {365–369},
numpages = {5},
keywords = {software product lines, ecosystems, EASy-producer},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.5555/2882704.2882711,
author = {Netessine, Serguei and Taylor, Terry A.},
title = {Product Line Design and Production Technology},
year = {2007},
issue_date = {January 2007},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {26},
number = {1},
issn = {1526-548X},
abstract = {In this paper we characterize the impact of production technology on the optimal product line design. We analyze a problem in which a manufacturer segments the market on quality attributes and offers products that are partial substitutes. Because consumers self-select from the product line, product cannibalization is an issue. In addition, the manufacturer sets a production schedule in order to balance production setups with accumulation of inventories in the presence of economies of scale. We show that simultaneous optimization of the product line design and production schedule leads to insights that differ significantly from the common intuition and assertions in the literature, which omits either the demand side or the supply side of the equation. In particular, we demonstrate that more expensive production technology always leads to lower product prices and may at the same time lead to higher quality products. Further, a less efficient production technology does not necessarily increase total production costs or reduce consumer welfare. We also demonstrate that in the presence of production technology, the demand cannibalization problem may distort product quality upward or the number of products upward, which is contrary to the standard result.},
journal = {Marketing Science},
month = jan,
pages = {101–117},
numpages = {17},
keywords = {segmentation, scale economies, product line, marketing-manufacturing interface, cannibalization, EOQ}
}

@inproceedings{10.1145/3375627.3375858,
author = {Zucker, Julian and d'Leeuwen, Myraeka},
title = {Arbiter: A Domain-Specific Language for Ethical Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375858},
doi = {10.1145/3375627.3375858},
abstract = {The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {421–425},
numpages = {5},
keywords = {domain-specific languages, ethical machine learning},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1007/978-3-030-38085-4_19,
author = {Christodoulopoulos, Konstantinos and Sartzetakis, Ippokratis and Soumplis, Polizois and Varvarigos, Emmanouel (Manos)},
title = {Machine Learning Assisted Quality of Transmission Estimation and Planning with Reduced Margins},
year = {2019},
isbn = {978-3-030-38084-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38085-4_19},
doi = {10.1007/978-3-030-38085-4_19},
abstract = {In optical transport networks, the Quality of Transmission (QoT) using a physical layer model (PLM) is estimated before establishing new or reconfiguring established optical connections. Traditionally, high margins are added to account for the model’s inaccuracy and the uncertainty in the current and evolving physical layer conditions, targeting uninterrupted operation for several years, until the end-of-life (EOL). Reducing the margins increases network efficiency but requires accurate QoT estimation. We present two machine learning (ML) assisted QoT estimators that leverage monitoring data of existing connections to understand the actual physical layer conditions and achieve high estimation accuracy. We then quantify the benefits of planning/upgrading a network over multiple periods with accurate QoT estimation as opposed to planning with EOL margins.},
booktitle = {Optical Network Design and Modeling: 23rd IFIP WG 6.10 International Conference, ONDM 2019, Athens, Greece, May 13–16, 2019, Proceedings},
pages = {211–222},
numpages = {12},
keywords = {Overprovisioning, Static network planning, End-of-life margins, Physical layer impairments, Monitoring, Cross-layer optimization, Incremental multi-period planning, Marginless},
location = {Athens, Greece}
}

@inproceedings{10.1145/2635868.2635919,
author = {Cordy, Maxime and Heymans, Patrick and Legay, Axel and Schobbens, Pierre-Yves and Dawagne, Bruno and Leucker, Martin},
title = {Counterexample guided abstraction refinement of product-line behavioural models},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635919},
doi = {10.1145/2635868.2635919},
abstract = {The model-checking problem for Software Products Lines (SPLs) is harder than for single systems: variability constitutes a new source of complexity that exacerbates the state-explosion problem. Abstraction techniques have successfully alleviated state explosion in single-system models. However, they need to be adapted to SPLs, to take into account the set of variants that produce a counterexample. In this paper, we apply CEGAR (Counterexample-Guided Abstraction Refinement) and we design new forms of abstraction specifically for SPLs. We carry out experiments to evaluate the efficiency of our new abstractions. The results show that our abstractions, combined with an appropriate refinement strategy, hold the potential to achieve large reductions in verification time, although they sometimes perform worse. We discuss in which cases a given abstraction should be used.},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {190–201},
numpages = {12},
keywords = {Software Product Lines, Model Checking, CEGAR, Abstraction},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1145/2647908.2655957,
author = {Murguzur, Aitor and Capilla, Rafael and Trujillo, Salvador and Ortiz, \'{O}scar and Lopez-Herrejon, Roberto E.},
title = {Context variability modeling for runtime configuration of service-based dynamic software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655957},
doi = {10.1145/2647908.2655957},
abstract = {In emerging domains such as Cloud-based Industrial Control Systems (ICSs) and SCADA systems where data-intensive and high performance computing are needed, a higher degree of flexibility is being demanded to meet new stakeholder requirements, context changes and intrinsic complexity. In this light, Dynamic Software Product Lines (DSPLs) provide a way to build self-managing systems exploiting traditional product line engineering concepts at runtime. Although context-awareness is widely perceived to be a first-class concern in such runtime variability mechanisms, existing approaches do not provide the necessary level of formalization to model and enact context variability for DSPLs. This is crucial for operational analytics processes since variant configuration could differ from context to context depending on diverse data values linked to context features and cross-tree constraints in a feature model. In this paper, we propose a context variability modeling approach, demonstrate its applicability and usability via a wind farm use case, and present the fundamental building blocks of a framework for enabling context variability in service-based DSPLs which provide Workflow as a Service (WFaaS).},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {2–9},
numpages = {8},
keywords = {process variability, data-aware systems, context variability, context awareness},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1007/s10957-006-9135-3,
author = {Fruchter, G. E. and Fligler, A. and Winer, R. S.},
title = {Optimal Product Line Design: Genetic Algorithm Approach to Mitigate Cannibalization},
year = {2006},
issue_date = {November  2006},
publisher = {Plenum Press},
address = {USA},
volume = {131},
number = {2},
issn = {0022-3239},
url = {https://doi.org/10.1007/s10957-006-9135-3},
doi = {10.1007/s10957-006-9135-3},
abstract = {In this marketing-oriented era where manufacturers maximize profits through customer satisfaction, there is an increasing need to design a product line rather than a single product. By offering a product line, the manufacturer can customize his or her products to the needs of a variety of segments in order to maximize profits by satisfying more customers than a single product would. When the amount of data on customer preferences or possible product configurations is large and no analytical relations can be established, the problem of an optimal product line design becomes very difficult and there are no traditional methods to solve it. In this paper, we show that the usage of genetic algorithms, a mathematical heuristics mimicking the process of biological evolution, can solve efficiently the problem. Special domain operators were developed to help the genetic algorithm mitigate cannibalization and enhance the algorithm's local search abilities. Using manufacturer's profits as the criteria for fitness in evaluating chromosomes, the usage of domain specific operators was found to be highly beneficial with better final results. Also, we have hybridized the genetic algorithm with a linear programming postprocessing step to fine tune the prices of products in the product line. Attacking the core difficulty of cannibalization in the algorithm, the operators introduced in this work are unique.},
journal = {J. Optim. Theory Appl.},
month = nov,
pages = {227–244},
numpages = {18},
keywords = {product line design, pricing, marketing, heuristics, cannibalization, Genetic algorithms}
}

@inproceedings{10.1145/2019136.2019178,
author = {Brataas, Gunnar and Jiang, Shanshan and Reichle, Roland and Geihs, Kurt},
title = {Performance property prediction supporting variability for adaptive mobile systems},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019178},
doi = {10.1145/2019136.2019178},
abstract = {A performance property prediction (PPP) method for component-based self-adaptive applications is presented. Such performance properties are required by an adaptation middleware for reasoning about adaptation activities. Our PPP method is based on the Structure and Performance (SP) framework, a conceptually simple, yet powerful performance modelling framework based on matrices. The main contribution of this paper are the integration of SP-based PPP into a comprehensive model- and variability-based adaptation framework for context-aware mobile applications. A meta model for the SP method is described. The framework is demonstrated using a practical example.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {37},
numpages = {8},
keywords = {mobile systems, autonomic computing},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.mcm.2005.02.006,
author = {Nichols, K. B. and Venkataramanan, M. A. and Ernstberger, K. W.},
title = {Product line selection and pricing analysis: Impact of genetic relaxations},
year = {2005},
issue_date = {December, 2005},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {42},
number = {13},
issn = {0895-7177},
url = {https://doi.org/10.1016/j.mcm.2005.02.006},
doi = {10.1016/j.mcm.2005.02.006},
abstract = {A model for the product line selection and pricing problem (PLSP) is presented andthree solution procedures based on a genetic algorithm are developed to analyze the results based on consumer preference patterns. Since the PLSP model is nonlinear and integer, two of the solution procedures use genetic encoding to ''relax'' the NP hard model. The relaxations result in linear integer and shortest path models for the fitness evaluation which are solved using branch and bound and labeling algorithms, respectively. Performance of the quality of solutions generated by the procedures is evaluated for various problem sizes and customer preference structures. The results show that the genetic relaxations provide efficient and effective solution methodologies for the problem, when compared to the pure artificial intelligence technique of genetic search. The impact of the preference structure on the product line and the managerial implications of the solution characteristics generated by the genetic relaxations are also discussed. The models can be used to explicitly consider tradeoffs between marketing and operations concerns in designing a product line.},
journal = {Math. Comput. Model.},
month = dec,
pages = {1397–1410},
numpages = {14},
keywords = {Product line, Pricing, Heuristics, Genetic algorithms}
}

@inproceedings{10.1145/2362536.2362554,
author = {Martini, Antonio and Pareto, Lars and Bosch, Jan},
title = {Enablers and inhibitors for speed with reuse},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362554},
doi = {10.1145/2362536.2362554},
abstract = {An open issue in industry is software reuse in the context of large scale Agile product development. The speed offered by agile practices is needed to hit the market, while reuse is needed for long-term productivity, efficiency, and profit. The paper presents an empirical investigation of factors influencing speed and reuse in three large product developing organizations seeking to implement Agile practices. The paper identifies, through a multiple case study with 3 organizations, 114 business-, process-, organizational-, architecture-, knowledge- and communication factors with positive or negative influences on reuse, speed or both. Contributions are a categorized inventory of influencing factors, a display for organizing factors for the purpose of process improvement work, and a list of key improvement areas to address when implementing reuse in organizations striving to become more Agile. Categories identified include good factors with positive influences on reuse or speed, harmful factors with negative influences, and complex factors involving inverse or ambiguous relationships. Key improvement areas in the studied organizations are intra-organizational communication practices, reuse awareness and practices, architectural integration and variability management. Results are intended to support process improvement work in the direction of Agile product development. Feedback on results from the studied organizations has been that the inventory captures current situations, and is useful for software process improvement work.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {116–125},
numpages = {10},
keywords = {speed, software reuse, software process improvement (SPI), inhibitors, enablers, embedded systems, agile software development},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1109/ICECCS.2007.38,
author = {Zhang, Weishan and Kunz, Thomas and Marius, Klaus},
title = {Product Line Enabled Intelligent Mobile Middleware},
year = {2007},
isbn = {0769528953},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICECCS.2007.38},
doi = {10.1109/ICECCS.2007.38},
abstract = {Current mobile middleware is designed according to a "one-size-fits-all' paradigm, which lacks the flexibility for customization and adaptation to different situations, and does not support user-centered application scenarios well. In this paper we describe an ongoing intelligent mobile middleware research project called PLIMM that focuses on user-centered application scenarios. PLIMM is designed based on software product line ideas which make it possible for specialized customization and optimization for different purposes and hardware/software platforms. To enable intelligence, the middleware needs access to a range of context models. We model these contexts with OWL, focusing on user-centered concepts. The basic building block of PLIMM is the enhanced BDI agent where OWL context ontology logic reasoning will add indirect beliefs to the belief sets. Our approach also addresses the handling of ontology evolutions resulting from the timely adaptation of ontology to changes and the consistent propagation of these changes to all related artifacts, using Frame based product line configuration techniques.},
booktitle = {Proceedings of the 12th IEEE International Conference on Engineering Complex Computer Systems},
pages = {148–160},
numpages = {13},
series = {ICECCS '07}
}

@inproceedings{10.1145/3483899.3483905,
author = {Freire, Willian and Tonh\~{a}o, Simone and Bonetti, Tiago and Shigenaga, Marcelo and Cadette, William and Felizardo, Fernando and Amaral, Aline and OliveiraJr, Edson and Colanzi, Thelma},
title = {On the configuration of multi-objective evolutionary algorithms for PLA design optimization},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483905},
doi = {10.1145/3483899.3483905},
abstract = {Search-based algorithms have been successfully applied in the Product Line Architecture (PLA) optimization using the seminal approach called Multi-Objective Approach for Product-Line Architecture Design (MOA4PLA). This approach produces a set of alternative PLA designs intending to improve the different factors being optimized. Currently, the MOA4PLA uses the NSGA-II algorithm, a multi-objective evolutionary algorithm (MOEA) that can optimize several architectural properties simultaneously. Despite the promising results, studying the best values for the algorithm parameters is essential to obtain even better results. This is also crucial to ease the adoption of MOA4PLA by newcomers or non-expert companies willing to start using search-based software engineering to PLA design. Three crossover operators for the PLA design optimization were proposed recently. However, reference values for parameters have not been defined for PLA design optimization using crossover operators. In this context, the objective of this work is conducting an experimental study to discover which are the most effective crossover operators and the best values to configure the MOEA parameters, such as population size, number of generations, and mutation and crossover rates. A quantitative analysis based on quality indicators and statistical tests was performed using four PLA designs to determine the most suitable parameter values to the search-based algorithm. Empirical results pointed out the best combination of crossover operators and the most suitable values to configure MOA4PLA.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {software product line, software architecture, recombination operators, Multi-objective evolutionary algorithm},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1109/ASE.2003.1240325,
author = {Wile, David S.},
title = {Architecture style-based calculi for non-functional properties},
year = {2003},
isbn = {0769520359},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2003.1240325},
doi = {10.1109/ASE.2003.1240325},
abstract = {Engineers wield various "calculi" to help determine solutions to their problems, calculation tools varying in power from tensile strength tables to the differential calculus. A calculus is normally based on induction over an algebraic structure. Here I explore how architecture styles can be used to describe such structures. An example calculus based on an "integration" style is presented, which is intended for use as a sub-style of other architecture styles. Calculation rules in terms of the architectural elements can be used to compute nonfunctional attributes of artifacts described in such styles. Naturally, computerized support for calculi will help to automate the tasks of software engineers.},
booktitle = {Proceedings of the 18th IEEE International Conference on Automated Software Engineering},
pages = {299–303},
numpages = {5},
location = {Montreal, Quebec, Canada},
series = {ASE'03}
}

@inproceedings{10.1145/1858996.1859009,
author = {Vierhauser, Michael and Gr\"{u}nbacher, Paul and Egyed, Alexander and Rabiser, Rick and Heider, Wolfgang},
title = {Flexible and scalable consistency checking on product line variability models},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859009},
doi = {10.1145/1858996.1859009},
abstract = {The complexity of product line variability models makes it hard to maintain their consistency over time regardless of the modeling approach used. Engineers thus need support for detecting and resolving inconsistencies. We describe experiences of applying a tool-supported approach for incremental consistency checking on variability models. Our approach significantly improves the overall performance and scalability compared to batch-oriented techniques and allows providing immediate feedback to modelers. It is extensible as new consistency constraints can easily be added. Furthermore, the approach is flexible as it is not limited to variability models and it also checks the consistency of the models with the underlying code base of the product line. We report the results of a thorough evaluation based on real-world product line models and discuss lessons learned.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {variability models, software product lines, performance, model consistency, memory consumption, lessons learned, incremental consistency checking},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.5555/1753235.1753267,
author = {Mendonca, Marcilio and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {SAT-based analysis of feature models is easy},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Feature models are a popular variability modeling notation used in product line engineering. Automated analyses of feature models, such as consistency checking and interactive or offline product selection, often rely on translating models to propositional logic and using satisfiability (SAT) solvers.Efficiency of individual satisfiability-based analyses has been reported previously. We generalize and quantify these studies with a series of independent experiments. We show that previously reported efficiency is not incidental. Unlike with the general SAT instances, which fall into easy and hard classes, the instances induced by feature modeling are easy throughout the spectrum of realistic models. In particular, the phenomenon of phase transition is not observed for realistic feature models.Our main practical conclusion is a general encouragement for researchers to continued development of SAT-based methods to further exploit this efficiency in future.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {231–240},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1007/s10664-014-9353-5,
author = {Asadi, Mohsen and Soltani, Samaneh and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
title = {The effects of visualization and interaction techniques on feature model configuration},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9353-5},
doi = {10.1007/s10664-014-9353-5},
abstract = {A Software Product Line is a set of software systems of a domain, which share some common features but also have significant variability. A feature model is a variability modeling artifact which represents differences among software products with respect to variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by selecting features in the feature model (referred to as the configuration process) and by instantiating the reference model. However, feature model configuration can be a cumbersome task because: 1) feature models may consist of a large number of features, which are hard to comprehend and maintain; and 2) many factors including technical limitations, implementation costs, stakeholders' requirements and expectations must be considered in the configuration process. Recognizing these issues, a significant amount of research efforts has been dedicated to different aspects of feature model configuration such as automating the configuration process. Several approaches have been proposed to alleviate the feature model configuration challenges through applying visualization and interaction techniques. However, there have been limited empirical insights available into the impact of visualization and interaction techniques on the feature model configuration process. In this paper, we present a set of visualization and interaction interventions for representing and configuring feature models, which are then empirically validated to measure the impact of the proposed interventions. An empirical study was conducted by following the principles of control experiments in software engineering and by applying the well-known software quality standard ISO 9126 to operationalize the variables investigated in the experiment. The results of the empirical study revealed that the employed visualization and interaction interventions significantly improved completion time of comprehension and changing of the feature model configuration. Additionally, according to results, the proposed interventions are easy-to-use and easy-to-learn for the participants.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1706–1743},
numpages = {38},
keywords = {Tools, Software product line engineering, Controlled experiment}
}

@inproceedings{10.1145/1173706.1173736,
author = {Trujillo, Salvador and Batory, Don and Diaz, Oscar},
title = {Feature refactoring a multi-representation program into a product line},
year = {2006},
isbn = {1595932372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1173706.1173736},
doi = {10.1145/1173706.1173736},
abstract = {Feature refactoring is the process of decomposing a program into aset of modules, called features, that encapsulate increments in program functionality. Different compositions of features yield different programs. As programs are defined using multiple representations, such as code, makefiles, and documentation, feature refactoring requires all representations to be factored. Thus, composing features produces consistent representations of code, make files, documentation, etc. for a target program. We present acase study of feature refactoring a substantial tool suite that usesmultiple representations. We describe the key technical problems encountered, and sketch the tool support needed for simplifying such refactorings in the future.},
booktitle = {Proceedings of the 5th International Conference on Generative Programming and Component Engineering},
pages = {191–200},
numpages = {10},
keywords = {software product lines, refinements, refactoring, program synthesis, multiple representations, feature-oriented programming, AHEAD},
location = {Portland, Oregon, USA},
series = {GPCE '06}
}

@article{10.1016/j.eswa.2007.01.036,
author = {Liao, Shu-Hsien and Chen, Chyuan-Meei and Wu, Chung-Hsin},
title = {Mining customer knowledge for product line and brand extension in retailing},
year = {2008},
issue_date = {April, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {34},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.01.036},
doi = {10.1016/j.eswa.2007.01.036},
abstract = {Retailing consists of the final activities and steps needed to place a product in the hands of the consumer or to provide services to the consumer. In fact, retailing is actually the last step in a supply chain that may stretch from Europe or Asia to the customer's hometown. Therefore, any firm that sells a product or provides a service to the final consumer is performing the retailing function. On the other hand, product line extension, which adds depth to an existing product line by introducing new products in the same product category, can give customers greater choice and help to protect the firm from flanking attack by a competitor. In addition, a product line extension is marketed under the same general brand as a previous item or items. Thus, to distinguish the brand extension from the other item(s) under the primary brand, the retailer can either add secondary brand identification or add a generic brand. This paper investigates product line and brand extension issues in the Taiwan branch of a leading international retailing company, Carrefour, which is a hypermarket retailer. This paper develops a relational database and proposes Apriori algorithm and K-means as methodologies for association rule and cluster analysis for data mining, which is then implemented to mine customer knowledge from household customers. Knowledge extraction by data mining results is illustrated as knowledge patterns/rules and clusters in order to propose suggestions and solutions to the case firm for product line and brand extensions and knowledge management.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {1763–1776},
numpages = {14},
keywords = {Retailing, Product line extension, Knowledge extraction, Data mining, Cluster analysis, Brand extension, Association rules}
}

@article{10.1016/j.future.2018.09.053,
author = {Cecchinel, Cyril and Fouquet, Fran\c{c}ois and Mosser, S\'{e}bastien and Collet, Philippe},
title = {Leveraging live machine learning and deep sleep to support a self-adaptive efficient configuration of battery powered sensors},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.053},
doi = {10.1016/j.future.2018.09.053},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {225–240},
numpages = {16}
}

@book{10.5555/2901596,
author = {Talia, Domenico and Trunfio, Paolo and Marozzo, Fabrizio},
title = {Data Analysis in the Cloud: Models, Techniques and Applications},
year = {2015},
isbn = {0128028815},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
edition = {1st},
abstract = {Data Analysis in the Cloud introduces and discusses models, methods, techniques, and systems to analyze the large number of digital data sources available on the Internet using the computing and storage facilities of the cloud. Coverage includes scalable data mining and knowledge discovery techniques together with cloud computing concepts, models, and systems. Specific sections focus on map-reduce and NoSQL models. The book also includes techniques for conducting high-performance distributed analysis of large data on clouds. Finally, the book examines research trends such as Big Data pervasive computing, data-intensive exascale computing, and massive social network analysis.Introduces data analysis techniques and cloud computing conceptsDescribes cloud-based models and systems for Big Data analyticsProvides examples of the state-of-the-art in cloud data analysisExplains how to develop large-scale data mining applications on cloudsOutlines the main research trends in the area of scalable Big Data analysis}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {weighted approach, product derivation, critical path analysis, configuration management, Software product line}
}

@article{10.1016/j.scico.2006.10.007,
author = {Pe\~{n}a, Joaquin and Hinchey, Michael G. and Resinas, Manuel and Sterritt, Roy and Rash, James L.},
title = {Designing and managing evolving systems using a MAS product line approach},
year = {2007},
issue_date = {April, 2007},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {66},
number = {1},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2006.10.007},
doi = {10.1016/j.scico.2006.10.007},
abstract = {We view an evolutionary system as being a software product line. The core architecture is the unchanging part of the system, and each version of the system may be viewed as a product from the product line. Each ''product'' may be described as the core architecture with some agent-based additions. The result is a multiagent system software product line. We describe an approach to such a software product line-based approach using the MaCMAS agent-oriented methodology. The approach scales to enterprise architectures as a multiagent system is an appropriate means of representing a changing enterprise architecture and the interaction between components in it. In addition, we reduce the gap between the enterprise architecture and the software architecture.},
journal = {Sci. Comput. Program.},
month = apr,
pages = {71–86},
numpages = {16},
keywords = {Swarm-based systems, Multiagent systems product lines, Enterprise architecture evolution}
}

@article{10.1016/j.jss.2019.01.044,
author = {Th\"{u}m, Thomas and Kn\"{u}ppel, Alexander and Kr\"{u}ger, Stefan and Bolle, Stefanie and Schaefer, Ina},
title = {Feature-oriented contract composition},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.044},
doi = {10.1016/j.jss.2019.01.044},
journal = {J. Syst. Softw.},
month = jun,
pages = {83–107},
numpages = {25},
keywords = {Formal methods, Deductive verification, Design by contract, Software product lines, Feature-oriented programming}
}

@article{10.1007/s10664-014-9359-z,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: proposing theories from a case study},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9359-z},
doi = {10.1007/s10664-014-9359-z},
abstract = {In the software product line research, product variants typically differ by their functionality and quality attributes are not purposefully varied. The goal is to study purposeful performance variability in software product lines, in particular, the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The research method was a theory-building case study that was augmented with a systematic literature review. The case was a mobile network base station product line with capacity variability. The data collection, analysis and theorizing were conducted in several stages: the initial case study results were augmented with accounts from the literature. We constructed three theoretical models to explain and characterize performance variability in software product lines: the models aim to be generalizable beyond the single case. The results describe capacity variability in a base station product line. Thereafter, theoretical models of performance variability in software product lines in general are proposed. Performance variability is motivated by customer needs and characteristics, by trade-offs and by varying operating environment constraints. Performance variability can be realized by hardware or software means; moreover, the software can either realize performance differences in an emergent way through impacts from other variability or by utilizing purposeful varying design tactics. The results point out two differences compared with the prevailing literature. Firstly, when the customer needs and characteristics enable price differentiation, performance may be varied even with no trade-offs or production cost differences involved. Secondly, due to the dominance of feature modeling, the literature focuses on the impact management realization. However, performance variability can be realized through purposeful design tactics to downgrade the available software resources and by having more efficient hardware.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1623–1669},
numpages = {47},
keywords = {Variability, Software product line, Software architecture, Case study}
}

@inproceedings{10.5555/1416502.1416534,
author = {Ram, N. Sankar and Rodrigues, Paul},
title = {Intelligent risk prophecy using more quality attributes injected ATAM and design patterns},
year = {2008},
isbn = {9789606766428},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Architectural Tradeoff Analysis Method is a method for evaluation of architecture-level designs and identifies trade-off points between attributes, facilitates communication between stakeholders. ATAM has got the limitations like not a predictor of quality achievement., not deals more quality attributes, Efficiency always depends on the expertise and potential of stakeholders. In this paper we have proposed a system which uses ATAM to predict the risk analysis, with more possible quality attributes. We have used artificial intelligence to predict the risk of the SA based on the Knowledge base of the Stakeholder Experts.},
booktitle = {Proceedings of the 7th WSEAS International Conference on Software Engineering, Parallel and Distributed Systems},
pages = {169–173},
numpages = {5},
keywords = {stakeholders (SH), stakeholder experts (SHE), software architecture(SA), risk factor (RF), design pattern (DP), architectural tradeoff analysis method (ATAM)},
location = {Cambridge, UK},
series = {SEPADS'08}
}

@inproceedings{10.5555/2820656.2820667,
author = {Buchmann, Thomas and Baumgartl, Johannes and Henrich, Dominik and Westfechtel, Bernhard},
title = {Robots and their variability: a societal challenge and a potential solution},
year = {2015},
publisher = {IEEE Press},
abstract = {A robot is essentially a real-time, distributed embedded system operating in a physical environment. Often, control and communication paths within the system are tightly coupled to the actual hardware configuration of the robot. Furthermore, the domain contains a high amount of variability on different levels, ranging from hardware, over software to the environment in which the robot is operated. Today, special robots are used in households to perform monotonous and recurring tasks like vacuuming or mowing the lawn. In the future there may be robots that can be configured and programmed for more complicated tasks, like washing dishes or cleaning up or to assist elderly people. Nowadays, programming a robot is a highly complex and challenging task, which can be carried out only by programmers with dedicated background in robotics. Societal acceptance of robots can only be achieved, if they are easy to program. In this paper we present our approach to provide customized programming environments enabling programmers without background knowledge in robotics to specify robot programs. Our solution was realized using product line techniques.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {27–30},
numpages = {4},
keywords = {software product line, robot, model-driven development, code generation, DSL},
location = {Florence, Italy},
series = {PLEASE '15}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {many-objective problems, dimensionality reduction, Software product line testing},
location = {Natal, Brazil},
series = {SAST '20}
}

@inproceedings{10.1007/978-3-030-89370-5_18,
author = {Tian, Yuze and Zhong, Xian and Liu, Wenxuan and Jia, Xuemei and Zhao, Shilei and Ye, Mang},
title = {Random Walk Erasing with Attention Calibration for Action Recognition},
year = {2021},
isbn = {978-3-030-89369-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89370-5_18},
doi = {10.1007/978-3-030-89370-5_18},
abstract = {Action recognition in videos has attracted growing research interests because of the explosive surveillance data in social security applications. In this process, due to the distraction and deviation of the network caused by occlusions, human action features usually suffer different degrees of performance degradation. Considering the occlusion scene in the wild, we find that the occluded objects usually move unpredictably but continuously. Thus, we propose a random walk erasing with attention calibration (RWEAC) for action recognition. Specifically, we introduce the random walk erasing (RWE) module to simulate the unknown occluded real conditions in frame sequence, expanding the diversity of data samples. In the case of erasing (or occlusion), the attention area is sparse. We leverage the attention calibration (AC) module to force the attention to stay stable in other regions of interest. In short, our novel RWEAC network enhances the ability to learn comprehensive features in a complex environment and make the feature representation robust. Experiments are conducted on the challenging video action recognition UCF101 and HMDB51 datasets. The extensive comparison results and ablation studies demonstrate the effectiveness and strength of the proposed method.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III},
pages = {236–251},
numpages = {16},
keywords = {Siamese network, Attention calibration, Data augmentation, Random walk erasing, Action recognition},
location = {Hanoi, Vietnam}
}

@inproceedings{10.1145/1882291.1882309,
author = {Ramasubbu, Narayan and Balan, Rajesh Krishna},
title = {Evolution of a bluetooth test application product line: a case study},
year = {2010},
isbn = {9781605587912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882291.1882309},
doi = {10.1145/1882291.1882309},
abstract = {In this paper, we study the decision making process involved in the five year lifecycle of a Bluetooth software product produced by a large, multi-national test and measurement firm. In this environment, customer change requests either have to be added as a standard feature in the product, or developed as a special customized version of the product. We first discuss the influential factors, such as evolving standards, market share, installed-base, and complexity, which collectively determined how the firm responded to product change requests. We then develop a predictive decision model to test the collective impact of these factors on determining whether to standardize or customize a customer's change request. Finally, we develop and test a customization cost estimation model, for use by software product teams, which specifically accounts for factors unique to the customization stage of a product lifecycle.},
booktitle = {Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {107–116},
numpages = {10},
keywords = {software process, software evolution, software engineering economics, product life cycle, product development, complexity},
location = {Santa Fe, New Mexico, USA},
series = {FSE '10}
}

@article{10.1007/s42979-021-00541-8,
author = {Saber, Takfarinas and Brevet, David and Botterweck, Goetz and Ventresque, Anthony},
title = {Reparation in Evolutionary Algorithms for Multi-objective Feature Selection in Large Software Product Lines},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {3},
url = {https://doi.org/10.1007/s42979-021-00541-8},
doi = {10.1007/s42979-021-00541-8},
abstract = {Software Product Lines Engineering is the area of software engineering that aims to systematise the modelling, creation and improvement of groups of interconnected software systems by formally expressing possible alternative products in the form of Feature Models. Deriving a software product/system from a feature model is called Feature Configuration. Engineers select the subset of features (software components) from a feature model that suits their needs, while respecting the underlying relationships/constraints of the system–which is challenging on its own. Since there exist several (and often antagonistic) perspectives on which the quality of software could be assessed, the problem is even more challenging as it becomes a multi-objective optimisation problem. Current multi-objective feature selection in software product line approaches (e.g., SATIBEA) combine the scalability of a genetic algorithm (IBEA) with a solution reparation approach based on a SAT solver or one of its derivatives. In this paper, we propose MILPIBEA, a novel hybrid algorithm which combines IBEA with the accuracy of a mixed-integer linear programming (MILP) reparation. We show that the MILP reparation modifies fewer features from the original infeasible solutions than the SAT reparation and in a shorter time. We also demonstrate that MILPIBEA outperforms SATIBEA on average on various multi-objective performance metrics, especially on the largest feature models. The other major challenge in software engineering in general and in software product lines, in particular, is evolution. While the change in software components is common in the software engineering industry, the particular case of multi-objective optimisation of evolving software product lines is not well-tackled yet. We show that MILPIBEA is not only able to better take advantage of the evolution than SATIBEA, but it is also the one that continues to improve the quality of the solutions when SATIBEA stagnates. Overall, IBEA performs better when combined with MILP instead of SAT reparation when optimising the multi-objective feature selection in large and evolving software product lines.},
journal = {SN Comput. Sci.},
month = mar,
numpages = {14},
keywords = {Mixed-integer linear programming, Reparation, Evolutionary algorithm, Multi-objective optimisation, Feature selection, Software product line}
}

@article{10.1287/mnsc.1090.1058,
author = {Wang, Xinfang (Jocelyn) and Camm, Jeffrey D. and Curry, David J.},
title = {A Branch-and-Price Approach to the Share-of-Choice Product Line Design Problem},
year = {2009},
issue_date = {October 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {55},
number = {10},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.1090.1058},
doi = {10.1287/mnsc.1090.1058},
abstract = {We develop a branch-and-price algorithm for constructing an optimal product line using partworth estimates from choice-based conjoint analysis. The algorithm determines the specific attribute levels for each multiattribute product in a set of products to maximize the resulting product line's share of choice, i.e., the number of respondents for whom at least one new product's utility exceeds the respondent's reservation utility. Computational results using large commercial and simulated data sets demonstrate that the algorithm can identify provably optimal, robust solutions to realistically sized problems.},
journal = {Manage. Sci.},
month = oct,
pages = {1718–1728},
numpages = {11},
keywords = {share of choice, product line design, optimization, marketing, integer programming, conjoint analysis, combinatorial optimization, column generation, branch and price}
}

@inproceedings{10.5555/1768029.1768057,
author = {Djebbi, Olfa and Salinesi, Camille},
title = {RED-PL, a method for deriving product requirements from a product line requirements model},
year = {2007},
isbn = {9783540729877},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product lines (SPL) modeling has proven to be an effective approach to reuse in software development. Several variability approaches were developed to plan requirements reuse, but only little of them actually address the issue of deriving product requirements. Indeed, while the modeling approaches sell on requirements reuse, the associated derivation techniques actually focus on deriving and reusing technical product data.This paper presents a method that intends to support requirements derivation. Its underlying principle is to take advantage of approaches made for reuse PL requirements and to complete them by a requirements development process by reuse for single products. The proposed approach matches users' product requirements with PL requirements models and derives a collection of requirements that is (i) consistent, and (ii) optimal with respect to users' priorities and company's constraints. The proposed methodological process was validated in an industrial setting by considering the requirement engineering phase of a product line of blood analyzers.},
booktitle = {Proceedings of the 19th International Conference on Advanced Information Systems Engineering},
pages = {279–293},
numpages = {15},
keywords = {requirements, product line, derivation},
location = {Trondheim, Norway},
series = {CAiSE'07}
}

@inproceedings{10.5555/1753235.1753245,
author = {Cetina, Carlos and Haugen, \O{}ystein and Zhang, Xiaorui and Fleurey, Franck and Pelechano, Vicente},
title = {Strategies for variability transformation at run-time},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {More and more approaches propose to use Software Product Lines (SPLs) modelling techniques to implement dynamic adaptive systems. The resulting Dynamic Software Product Lines (DSPLs) present new challenges since the variability transformations used to derive alternative configurations have to be intensively used at runtime. This paper proposes to use the Common Variability Language (CVL) for modelling runtime variability and evaluates a set of alternative strategies for implementing the associated variability transformations. All the proposed strategies have been implemented and evaluated on the case-study of a smart-home system. Results show that the proposed strategies provide the same reconfiguration service with significant differences in quality-of-service.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {61–70},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1007/978-3-030-64694-3_17,
author = {Benmerzoug, Amine and Yessad, Lamia and Ziadi, Tewfik},
title = {Analyzing the Impact of Refactoring Variants on Feature Location},
year = {2020},
isbn = {978-3-030-64693-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64694-3_17},
doi = {10.1007/978-3-030-64694-3_17},
abstract = {Due to the increasing importance of feature location process, several studies evaluate the performance of different techniques based on IR strategies and a set of software variants as input artifacts. The proposed techniques attempt to improve the results obtained but it is often a difficult task. None of the existing feature location techniques considers the changing nature of the input artifacts, which may undergo series of refactoring changes. In this paper, we investigate the impact of refactoring variants on the feature location techniques. We first evaluate the performance of two techniques through the ArgoUML SPL benchmark when the variants are refactored. We then discuss the degraded results and the possibility of restoring them. Finally, we outline a process of variant alignment that aims to preserve the performance of the feature location.},
booktitle = {Reuse in Emerging Software Engineering Practices: 19th International Conference on Software and Systems Reuse, ICSR 2020, Hammamet, Tunisia, December 2–4, 2020, Proceedings},
pages = {279–291},
numpages = {13},
keywords = {Refactoring, Feature location, Software Product Line},
location = {Hammamet, Tunisia}
}

@inproceedings{10.1007/978-3-030-55789-8_59,
author = {Abeyrathna, Kuruge Darshana and Granmo, Ole-Christoffer and Goodwin, Morten},
title = {Integer Weighted Regression Tsetlin Machines},
year = {2020},
isbn = {978-3-030-55788-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-55789-8_59},
doi = {10.1007/978-3-030-55789-8_59},
abstract = {The Regression Tsetlin Machine (RTM) addresses the lack of interpretability impeding state-of-the-art nonlinear regression models. It does this by using conjunctive clauses in propositional logic to capture the underlying non-linear frequent patterns in the data. These, in turn, are combined into a continuous output through summation, akin to a linear regression function, however, with non-linear components and binary weights. However, the resolution of the RTM output is proportional to the number of clauses employed. This means that computation cost increases with resolution. To address this problem, we here introduce integer weighted RTM clauses. Our integer weighted clause is a compact representation of multiple clauses that capture the same sub-pattern—w repeating clauses are turned into one, with an integer weight w. This reduces computation cost w times, and increases interpretability through a sparser representation. We introduce a novel learning scheme, based on so-called stochastic searching on the line. We evaluate the potential of the integer weighted RTM empirically using two artificial datasets. The results show that the integer weighted RTM is able to acquire on par or better accuracy using significantly less computational resources compared to regular RTM and an RTM with real-valued weights.},
booktitle = {Trends in Artificial Intelligence Theory and Applications. Artificial Intelligence Practices: 33rd International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2020, Kitakyushu, Japan, September 22-25, 2020, Proceedings},
pages = {686–694},
numpages = {9},
keywords = {Stochastic searching on the line, Interpretable machine learning, Weighted tsetlin machines, Regression tsetlin machines, Tsetlin machines},
location = {Kitakyushu, Japan}
}

@inproceedings{10.5555/648033.744208,
author = {Muthig, Dirk and Patzke, Thomas},
title = {Generic Implementation of Product Line Components},
year = {2002},
isbn = {3540007377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {An argument pro component-based software development is the idea of constructing software systems by assembling preexisting components instead of redeveloping similar or identical functionality always from scratch. Unfortunately, integrating existing components practically means adaptation and use rather than use only, which makes an ideal component-based development hard to realize in practice. Product line engineering, however, tackles this problem by making components as generic as needed for a particular product family and thus allows component reuse. Such a component covers variabilities and thus its implementation must consider variabilities as well.In this paper, we describe a process for implementing generic product line components and give an overview of variability mechanisms at the implementation level, illustrated by a running example, a generic test component.},
booktitle = {Revised Papers from the International Conference NetObjectDays on Objects, Components, Architectures, Services, and Applications for a Networked World},
pages = {313–329},
numpages = {17},
series = {NODe '02}
}

@inproceedings{10.1145/2647908.2655969,
author = {ter Beek, Maurice H. and Mazzanti, Franco},
title = {VMC: recent advances and challenges ahead},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655969},
doi = {10.1145/2647908.2655969},
abstract = {The variability model checker VMC accepts a product family specified as a Modal Transition System (MTS) with additional variability constraints. Consequently, it offers behavioral variability analyses over both the family and its valid product behavior. This ranges from product derivation and simulation to efficient on-the-fly model checking of logical properties expressed in a variability-aware version of action-based CTL. In this paper, we first explain the reasons and assumptions underlying the choice for a modeling and analysis framework based on MTSs. Subsequently, we present recent advances on proving inheritance of behavioral analysis properties from a product family to its valid products. Finally, we illustrate challenges remaining for the future.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {70–77},
numpages = {8},
keywords = {product families, model checking, behavioral variability},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.5555/1753235.1753249,
author = {Montagud, Sonia and Abrah\~{a}o, Silvia},
title = {Gathering current knowledge about quality evaluation in software product lines},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Recently, a number of methods and techniques for assessing the quality of software product lines have been proposed. However, to the best of our knowledge, there is no study which summarizes all the existing evidence about them. This paper presents a systematic review that investigates what methods and techniques have been employed (in the last 10 years) to evaluate the quality of software product lines and how they were employed. A total of 39 research papers have been reviewed from an initial set of 1388 papers. The results show that 25% of the papers reported evaluations at the Design phase of the Domain Engineering phase. The most widely used mechanism for modeling quality attributes was extended feature models and the most evaluated artifact was the base architecture. In addition, the results of the review have identified several research gaps. Specifically, 77% of the papers employed case studies as a "proof of concept" whereas 23% of the papers did not perform any type of validation. Our results are particularly relevant in positioning new research activities and in the selection of quality evaluation methods or techniques that best fit a given purpose.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {91–100},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Configurable systems, Machine learning, Software product lines, Systematic literature review}
}

@inproceedings{10.1007/978-3-030-21290-2_42,
author = {Reinhartz-Berger, Iris and Shimshoni, Ilan and Abdal, Aviva},
title = {Behavior-Derived Variability Analysis: Mining Views for Comparison and Evaluation},
year = {2019},
isbn = {978-3-030-21289-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-21290-2_42},
doi = {10.1007/978-3-030-21290-2_42},
abstract = {The large variety of computerized solutions (software and information systems) calls for a systematic approach to their comparison and evaluation. Different methods have been proposed over the years for analyzing the similarity and variability of systems. These methods get artifacts, such as requirements, design models, or code, of different systems (commonly in the same domain), identify and calculate their similarities, and represent the variability in models, such as feature diagrams. Most methods rely on implementation considerations of the input systems and generate outcomes based on predefined, fixed strategies of comparison (referred to as variability views). In this paper, we introduce an approach for mining relevant views for comparison and evaluation, based on the input artifacts. Particularly, we equip SOVA – a Semantic and Ontological Variability Analysis method – with data mining techniques in order to identify relevant views that highlight variability or similarity of the input artifacts (natural language requirement documents). The comparison is done using entropy and Rand index measures. The method and its outcomes are evaluated on a case of three photo sharing applications.},
booktitle = {Advanced Information Systems Engineering: 31st International Conference, CAiSE 2019, Rome, Italy, June 3–7, 2019, Proceedings},
pages = {675–690},
numpages = {16},
keywords = {Software Product Line Engineering, Variability analysis, Requirements specifications, Feature diagrams},
location = {Rome, Italy}
}

@article{10.1007/s10664-020-09853-4,
author = {Hajri, Ines and Goknil, Arda and Pastore, Fabrizio and Briand, Lionel C.},
title = {Automating system test case classification and prioritization for use case-driven testing in product lines},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09853-4},
doi = {10.1007/s10664-020-09853-4},
abstract = {Product Line Engineering (PLE) is a crucial practice in many software development environments where software systems are complex and developed for multiple customers with varying needs. At the same time, many development processes are use case-driven and this strongly influences their requirements engineering and system testing practices. In this paper, we propose, apply, and assess an automated system test case classification and prioritization approach specifically targeting system testing in the context of use case-driven development of product families. Our approach provides: (i) automated support to classify, for a new product in a product family, relevant and valid system test cases associated with previous products, and (ii) automated prioritization of system test cases using multiple risk factors such as fault-proneness of requirements and requirements volatility in a product family. Our evaluation was performed in the context of an industrial product family in the automotive domain. Results provide empirical evidence that we propose a practical and beneficial way to classify and prioritize system test cases for industrial product lines.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3711–3769},
numpages = {59},
keywords = {Requirements engineering, Automotive, Test case selection and prioritization, Regression testing, Use case driven development, Product Line Engineering}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green software development and research with the HADAS toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {variability, software product line, repository, optimisation, metrics, energy efficiency, clafer, CVL},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@article{10.1016/j.jss.2018.07.054,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.054},
doi = {10.1016/j.jss.2018.07.054},
journal = {J. Syst. Softw.},
month = oct,
pages = {511–532},
numpages = {22},
keywords = {Systematic literature review, Product configuration, Extended product line}
}

@inproceedings{10.5555/646863.707954,
author = {M\'{a}rkus, Andr\'{a}s and V\'{a}ncza, J\'{o}zsef},
title = {Product Line Design with Customer Preferences},
year = {2001},
isbn = {3540422196},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {When customizing their product lines, manufacturers attempt to fulfill the requirements of the customers within the technical and economical constraints of the manufacturing environment. Product line design is a recurrent process that aims at finding the proper balance between the exploration of new product alternatives and the exploitation of the known selling potential of the available variants. This paper offers a framework where, driven by the interaction of customer preferences and the reallocation of manufacturing resources, product families emerge from technically feasible product alternatives.},
booktitle = {Proceedings of the 14th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems: Engineering of Intelligent Systems},
pages = {846–855},
numpages = {10},
series = {IEA/AIE '01}
}

@inproceedings{10.1145/3474624.3476016,
author = {Bezerra, Carla and Lima, Rafael and Silva, Publio},
title = {DyMMer 2.0: A Tool for Dynamic Modeling and Evaluation of Feature Model},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476016},
doi = {10.1145/3474624.3476016},
abstract = {Managing dynamic variability has motivated several researchers to combine Dynamic Software Product Lines (DSPLs) practices with runtime variability mechanisms. By combining these approaches, a DSPL acquires important features, ranging from the ability to reconfigure by changing the context, adding or removing features, crash recovery, and re-adaptation based on changes in the model’s features. Feature model (FM) is an important artifact of a DPSL and there is a lack of tools that support the modeling of this artifact. We have extended the DyMMer tool for modeling FM of DSPLs from an adaptation mechanism based on MAPE-K to solve this problem. We migrated the DyMMer tool to a web version and incorporated new features: (i) modeling of FMs from SPLs and DSPLs, (ii) development of an adaptation mechanism for FM of DSPLs, (iii) repository of FMs, (iv) inclusion of thresholds for measures, and (v) user authentication. We believe that this tool is useful for research in the area of DSPLs, and also for dynamic domain modeling and evaluation. Video: https://youtu.be/WVHW6bI8ois},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {121–126},
numpages = {6},
keywords = {Modeling, Feature Model, Dynamic Software Product Line},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1016/j.jss.2011.04.020,
author = {Hanssen, Geir K.},
title = {A longitudinal case study of an emerging software ecosystem: Implications for practice and theory},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.04.020},
doi = {10.1016/j.jss.2011.04.020},
abstract = {Software ecosystems is an emerging trend within the software industry, implying a shift from closed organizations and processes towards open structures, where actors external to the software development organization are becoming increasingly involved in development. This forms an ecosystem of organizations that are related through the shared interest in a software product, leading to new opportunities and new challenges to the industry and its organizational environment. To understand why and how this change occurs, we have followed the development of a software product line organization for a period of approximately five years. We have studied their change from a waterfall-like approach, via agile software product line engineering, towards an emerging software ecosystem. We discuss implications for practice, and propose a nascent theory on software ecosystems. We conclude that the observed change has led to an increase in collaboration across (previously closed) organizational borders, and to the development of a shared value consisting of two components: the technology (the product line, as an extensible platform), and the business domain it supports. Opening up both the technical interface of the product and the organizational interfaces are key enablers of such a change.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1455–1466},
numpages = {12},
keywords = {Software product line engineering, Software ecosystems, Longitudinal case study, Agile software development}
}

@article{10.1016/S0164-1212(03)00013-X,
author = {Zelkowitz, Marvin V. and Rus, Ioana},
title = {Defect evolution in a product line environment},
year = {2004},
issue_date = {February, 2004},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {70},
number = {1–2},
issn = {0164-1212},
url = {https://doi.org/10.1016/S0164-1212(03)00013-X},
doi = {10.1016/S0164-1212(03)00013-X},
abstract = {One mechanism used for monitoring the development of the Space Shuttle flight control software, in order to minimize any risks to the missions, is the independent verification and validation (IV&amp;V) process. Using data provided by both the Shuttle software developer and the IV&amp;V contractor, in this paper we describe the overall IV&amp;V process as used on the Space Shuttle program and provide an analysis of the use of metrics to document and control this process over multiple releases of this software. Our findings reaffirm the value of IV&amp;V, show the impact of IV&amp;V on multiple releases of a large complex software system, and indicate that some of the traditional measures of defect detection and repair are not applicable in a multiple-release environment such as this one.},
journal = {J. Syst. Softw.},
month = feb,
pages = {143–154},
numpages = {12},
keywords = {Space Shuttle program, Software safety and reliability, Software independent verification and validation, Product line development, Process characterization, Metrics, Life and mission critical software, Evolutionary software}
}

@article{10.1007/s10664-004-6190-y,
author = {Svahnberg, Mikael and Wohlin, Claes},
title = {An Investigation of a Method for Identifying a Software Architecture Candidate with Respect to Quality Attributes},
year = {2005},
issue_date = {April     2005},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {10},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-004-6190-y},
doi = {10.1007/s10664-004-6190-y},
abstract = {To sustain the qualities of a software system during evolution, and to adapt the quality attributes as the requirements evolve, it is necessary to have a clear software architecture that is understood by all developers and to which all changes to the system adheres. This software architecture can be created beforehand, but must also be updated to reflect changes in the domain, and hence the requirements of the software. The choice of which software architecture to use is typically based on informal decisions. There exist, to the best of our knowledge, little factual knowledge of which quality attributes are supported or obstructed by different architecture approaches. In this paper we present an empirical study of a method that enables quantification of the perceived support different software architectures give for different quality attributes. This in turn enables an informed decision of which architecture candidate best fit the mixture of quality attributes required by a system being designed.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {149–181},
numpages = {33},
keywords = {quality attributes, analytic hierarchy process, Software architectures}
}

@article{10.1007/s10664-021-09940-0,
author = {Cashman, Mikaela and Firestone, Justin and Cohen, Myra B. and Thianniwet, Thammasak and Niu, Wei},
title = {An empirical investigation of organic software product lines},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09940-0},
doi = {10.1007/s10664-021-09940-0},
abstract = {Software product line engineering is a best practice for managing reuse in families of software systems that is increasingly being applied to novel and emerging domains. In this work we investigate the use of software product line engineering in one of these new domains, synthetic biology. In synthetic biology living organisms are programmed to perform new functions or improve existing functions. These programs are designed and constructed using small building blocks made out of DNA. We conjecture that there are families of products that consist of common and variable DNA parts, and we can leverage product line engineering to help synthetic biologists build, evolve, and reuse DNA parts. In this paper we perform an investigation of domain engineering that leverages an open-source repository of more than 45,000 reusable DNA parts. We show the feasibility of these new types of product line models by identifying features and related artifacts in up to 93.5% of products, and that there is indeed both commonality and variability. We then construct feature models for four commonly engineered functions leading to product lines ranging from 10 to 7.5 \texttimes{} 1020 products. In a case study we demonstrate how we can use the feature models to help guide new experimentation in aspects of application engineering. Finally, in an empirical study we demonstrate the effectiveness and efficiency of automated reverse engineering on both complete and incomplete sets of products. In the process of these studies, we highlight key challenges and uncovered limitations of existing SPL techniques and tools which provide a roadmap for making SPL engineering applicable to new and emerging domains.},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {43},
keywords = {BioBricks, Reverse engineering, Synthetic biology, Software product lines}
}

@article{10.1007/s10270-020-00803-8,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat and Nie, Kunming},
title = {A framework for automated multi-stage and multi-step product configuration of cyber-physical systems},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00803-8},
doi = {10.1007/s10270-020-00803-8},
abstract = {Product line engineering (PLE) has been employed to large-scale cyber-physical systems (CPSs) to provide customization based on users’ needs. A PLE methodology can be characterized by its support for capturing and managing the abstractions as commonalities and variabilities and the automation of the configuration process for effective selection and customization of reusable artifacts. The automation of a configuration process heavily relies on the captured abstractions and formally specified constraints using a well-defined modeling methodology. Based on the results of our previous work and a thorough literature review, in this paper, we propose a conceptual framework to support multi-stage and multi-step automated product configuration of CPSs, including a comprehensive classification of constraints and a list of automated functionalities of a CPS configuration solution. Such a framework can serve as a guide for researchers and practitioners to evaluate an existing CPS PLE solution or devise a novel CPS PLE solution. To validate the framework, we conducted three real-world case studies. Results show that the framework fulfills all the requirements of the case studies in terms of capturing and managing variabilities and constraints. Results of the literature review indicate that the framework covers all the functionalities concerned by the literature, suggesting that the framework is complete for enabling the maximum automation of configuration in CPS PLE.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {211–265},
numpages = {55},
keywords = {Real-world case studies, Variability modeling, Constraint classification, Multi-stage and multi-step configuration process, Automated configuration, Product line engineering, Cyber-physical systems}
}

@article{10.1109/TSMCB.2003.817051,
author = {Balakrishnan, P. V. and Gupta, R. and Jacob, V. S.},
title = {Development of hybrid genetic algorithms for product line designs},
year = {2004},
issue_date = {February 2004},
publisher = {IEEE Press},
volume = {34},
number = {1},
issn = {1083-4419},
url = {https://doi.org/10.1109/TSMCB.2003.817051},
doi = {10.1109/TSMCB.2003.817051},
abstract = {In this paper, we investigate the efficacy of artificial intelligence (AI) based meta-heuristic techniques namely genetic algorithms (GAs), for the product line design problem. This work extends previously developed methods for the single product design problem. We conduct a large scale simulation study to determine the effectiveness of such an AI based technique for providing good solutions and bench mark the performance of this against the current dominant approach of beam search (BS). We investigate the potential advantages of pursuing the avenue of developing hybrid models and then implement and study such hybrid models using two very distinct approaches: namely, seeding the initial GA population with the BS solution, and employing the BS solution as part of the GA operator's process. We go on to examine the impact of two alternate string representation formats on the quality of the solutions obtained by the above proposed techniques. We also explicitly investigate a critical managerial factor of attribute importance in terms of its impact on the solutions obtained by the alternate modeling procedures. The alternate techniques are then evaluated, using statistical analysis of variance, on a fairly large number of data sets, as to the quality of the solutions obtained with respect to the state-of-the-art benchmark and in terms of their ability to provide multiple, unique product line options.},
journal = {Trans. Sys. Man Cyber. Part B},
month = feb,
pages = {468–483},
numpages = {16}
}

@inproceedings{10.1145/2647908.2655958,
author = {Eichelberger, Holger and Schmid, Klaus},
title = {Resource-optimizing adaptation for big data applications},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655958},
doi = {10.1145/2647908.2655958},
abstract = {The resource requirements of Big Data applications may vary dramatically over time, depending on changes in the context. If resources should not be defined for the maximum case, but available resources are mostly static, there is a need to adapt resource usage by modifying the processing behavior. The QualiMaster project researches such an approach for the analysis of systemic risks in the financial markets.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {10–11},
numpages = {2},
keywords = {systematic-risks, stream-processing, resource adaptation, financial markets, adaptive systems, QualiMaster},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3053600.3053604,
author = {Pita Costa, Joao and Galinac Grbac, Tihana},
title = {The Topological Data Analysis of Time Series Failure Data in Software Evolution},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053604},
doi = {10.1145/3053600.3053604},
abstract = {As evolving complex systems have become the central part of almost every human activity, their reliability is the key property for their successful application, especially with the emerging Internet of Services concept. There are many quantitative mathematical models, the so called reliability growth models, aiming to predict and estimate reliability of software systems based on the failure count time series. This paper suggests a novel and still unexplored qualitative approach to understand failure time series studying its topological features and their influence on failure distributions, thus affecting mission critical system properties, among which is reliability. To illustrate the new ideas, we analyse here the time series failure data of evolving software systems across the system versions for two open source software systems and one mission critical industrial software system, and discuss their topological relations and behaviour. We conclude that topological analysis might be useful for characterising software system behaviour early enough and for early characterization of system reliability that may contribute to software reliability modeling.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {25–30},
numpages = {6},
keywords = {topological data analysis, system defectiveness, software structure evolution},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@article{10.1007/s10270-011-0220-1,
author = {Hubaux, Arnaud and Heymans, Patrick and Schobbens, Pierre-Yves and Deridder, Dirk and Abbasi, Ebrahim Khalil},
title = {Supporting multiple perspectives in feature-based configuration},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-011-0220-1},
doi = {10.1007/s10270-011-0220-1},
abstract = {Feature diagrams have become commonplace in software product line engineering as a means to document variability early in the life cycle. Over the years, their application has also been extended to assist stakeholders in the configuration of software products. However, existing feature-based configuration techniques offer little support for tailoring configuration views to the profiles of the various stakeholders. In this paper, we propose a lightweight, yet formal and flexible, mechanism to leverage multidimensional separation of concerns in feature-based configuration. We propose a technique to specify concerns in feature diagrams and to generate automatically concern-specific configuration views. Three alternative visualisations are proposed. Our contributions are motivated and illustrated through excerpts from a real web-based meeting management application which was also used for a preliminary evaluation. We also report on the progress made in the development of a tool supporting multi-view feature-based configuration.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {641–663},
numpages = {23},
keywords = {Software product line engineering, Separation of concerns, Multi-view, Feature-based configuration, Feature diagram}
}

@article{10.1016/j.infsof.2009.11.001,
author = {Rabiser, Rick and Gr\"{u}nbacher, Paul and Dhungana, Deepak},
title = {Requirements for product derivation support: Results from a systematic literature review and an expert survey},
year = {2010},
issue_date = {March, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.001},
doi = {10.1016/j.infsof.2009.11.001},
abstract = {Context: An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support. Objective: Our aim is to identify and validate requirements for tool-supported product derivation. Method: We identify the requirements through a systematic literature review and validate them with an expert survey. Results: We discuss the resulting requirements and provide implementation examples from existing product derivation approaches. Conclusions: We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {324–346},
numpages = {23},
keywords = {Systematic literature review, Software product line, Product line engineering, Product derivation}
}

@article{10.1016/j.jss.2014.08.034,
author = {Alsawalqah, Hamad I. and Kang, Sungwon and Lee, Jihyun},
title = {A method to optimize the scope of a software product platform based on end-user features},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.08.034},
doi = {10.1016/j.jss.2014.08.034},
abstract = {A novel method to optimize the scope of a software product platform is proposed.The method is supported with a mathematical formulation and an optimization solver.Depending on the input parameters and the objectives, competing scopes can exist.The method shows how trade-off analysis can be performed among competing scopes.The results of the method were validated as "satisfiable" to "very satisfiable". ContextDue to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development. AimThis paper proposes a novel method to find the optimized scope of a software product platform based on end-user features. MethodThe proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives. ResultsIn a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope.},
journal = {J. Syst. Softw.},
month = dec,
pages = {79–106},
numpages = {28},
keywords = {Software product line engineering, Product platform scope, Commonality decision}
}

@inproceedings{10.5555/645882.672259,
author = {Deursen, Arie van and Jonge, Merijn de and Kuipers, Tobias},
title = {Feature-Based Product Line Instantiation Using Source-Level Packages},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we discuss the construction of software products from customer-specific feature selections. We address variability management with the Feature Description Language (FDL) to capture variation points of product line architectures. We describe feature packaging, which covers selecting and packaging implementation components according to feature selections using the autobundle tool. Finally, we discuss a generic approach, based on the abstract factory design pattern, to make instantiated (customer-specific) variability accessible in applications.The solutions and techniques presented in this paper are based on our experience with the product line architecture of the commercial documentation generator DocGen.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {217–234},
numpages = {18},
series = {SPLC 2}
}

@article{10.1613/jair.1.11688,
author = {Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
title = {Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11688},
doi = {10.1613/jair.1.11688},
abstract = {Interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as machine learning, computer vision, and natural language processing. Much of the growth in these fields has been made possible with deep learning, a sub-area of machine learning that uses artificial neural networks. This has created significant interest in the integration of vision and language. In this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey stimulates innovative thoughts and ideas to address the existing challenges and build new applications.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1183–1317},
numpages = {135},
keywords = {deep learning, computer vision, machine learning, natural language}
}

@inproceedings{10.5555/648114.748907,
author = {Ferber, Stefan and Heidl, Peter and Lutz, Peter},
title = {Reviewing Product Line Architectures: Experience Report of ATAM in an Automotive Context},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product lines are an important system development paradigm in the automotive industry to amortize costs beyond a single product. The paradigm is well established in the mechanical and electrical engineering practise in automotive companies like Bosch. As software is covering more and more functionality in cars, software product lines are getting more attention. The architecture of a software-intensive system is a key asser in developing a software product line.The Architecture Trade-off Analysis Method (ATAM) developed by the SEI assesses the quality of software architecture early in the development process. ATAM is therefore a useful review technique to guarantee important quality attributes of every single product created with the product line architecture later on. This article reports about the experience Bosch made in using ATAM in two cases.Benefits in using ATAM are not only the review results itself but a better documented and better understood architecture. We experienced the most important benefit of ATAM is the rising stakeholder's awareness of architectural decisions, tradeoffs, and risks. It illuminates the software architecture better than any written documentation.Bosch employees are trained in the evaluation roles in order to trasition ATAM to Bosch.The reports conclude with some suggestions fo improving the ATAM itself and the training of ATAM roles.},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {364–382},
numpages = {19},
series = {PFE '01}
}

@article{10.1145/1842713.1842717,
author = {Robinson, William N. and Ding, Yi},
title = {A survey of customization support in agent-based business process simulation tools},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/1842713.1842717},
doi = {10.1145/1842713.1842717},
abstract = {Agent-based business process simulation has grown in popularity, in part because of its analysis capabilities. The analyses depend on the kinds of simulations that can be built, adapted, and extended, which in turn depend on the underlying simulation framework. We report the results of our analysis of 19 agent-based process simulation tools and their simulation frameworks. We conclude that a growing number of simulation tools are using component-based software techniques. Nevertheless, most simulation tools do not directly support requirements models, their transformation into executable simulations, or the management of model variants over time. Such practices are becoming more widely applied in software engineering under the term software product line engineering (SPLE). Based on our analysis, agent-based process simulation tools may improve their customization capacity by: (1) supporting object modeling more completely and (2) supporting software product line engineering issues.},
journal = {ACM Trans. Model. Comput. Simul.},
month = oct,
articleno = {14},
numpages = {29},
keywords = {software product line engineering, modularity, event-driven simulation, encapsulation, application frameworks, Agent-based modeling}
}

@inproceedings{10.1145/3097983.3098186,
author = {Sharma, Ashlesh and Srinivasan, Vidyuth and Kanchan, Vishal and Subramanian, Lakshminarayanan},
title = {The Fake vs Real Goods Problem: Microscopy and Machine Learning to the Rescue},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098186},
doi = {10.1145/3097983.3098186},
abstract = {Counterfeiting of physical goods is a global problem amounting to nearly 7% of world trade. While there have been a variety of overt technologies like holograms and specialized barcodes and covert technologies like taggants and PUFs, these solutions have had a limited impact on the counterfeit market due to a variety of factors - clonability, cost or adoption barriers. In this paper, we introduce a new mechanism that uses machine learning algorithms on microscopic images of physical objects to distinguish between genuine and counterfeit versions of the same product. The underlying principle of our system stems from the idea that microscopic characteristics in a genuine product or a class of products (corresponding to the same larger product line), exhibit inherent similarities that can be used to distinguish these products from their corresponding counterfeit versions. A key building block for our system is a wide-angle microscopy device compatible with a mobile device that enables a user to easily capture the microscopic image of a large area of a physical object. Based on the captured microscopic images, we show that using machine learning algorithms (ConvNets and bag of words), one can generate a highly accurate classification engine for separating the genuine versions of a product from the counterfeit ones; this property also holds for "super-fake" counterfeits observed in the marketplace that are not easily discernible from the human eye. We describe the design of an end-to-end physical authentication system leveraging mobile devices, portable hardware and a cloud-based object verification ecosystem. We evaluate our system using a large dataset of 3 million images across various objects and materials such as fabrics, leather, pills, electronics, toys and shoes. The classification accuracy is more than 98% and we show how our system works with a cellphone to verify the authenticity of everyday objects.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {2011–2019},
numpages = {9},
keywords = {physical authentication, microscopy, conventional neural networks, computer vision},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1007/11497455_8,
author = {Lee, Seo Jeong and Kim, Soo Dong},
title = {A rendezvous of content adaptable service and product line modeling},
year = {2005},
isbn = {3540262008},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11497455_8},
doi = {10.1007/11497455_8},
abstract = {Content adaptable applications are often used in ubiquitous computing environment, and it aims to service the adaptable contents to users. In this environment, the services are dynamically selected and provided, the contexts are changed frequently. Then, the application services are to be modeled to derive the adaptable service effectively and to reuse the model. Modeling with software features and product line concepts may support for making service decision strategy. In this paper, we propose a service decision modeling technique for content adaptable applications in ubiquitous environment. It consists of defining variation points and their variants, finding out the dependencies between them, and then building the variant selection strategies. These can accomplish to define the decision model based on content adaptable service, and the definition templates help the reuse more effective.},
booktitle = {Proceedings of the 6th International Conference on Product Focused Software Process Improvement},
pages = {69–83},
numpages = {15},
location = {Oulu, Finland},
series = {PROFES'05}
}

@article{10.1016/j.eswa.2010.07.119,
author = {Kankar, P. K. and Sharma, Satish C. and Harsha, S. P.},
title = {Fault diagnosis of ball bearings using machine learning methods},
year = {2011},
issue_date = {March, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.07.119},
doi = {10.1016/j.eswa.2010.07.119},
abstract = {Ball bearings faults are one of the main causes of breakdown of rotating machines. Thus, detection and diagnosis of mechanical faults in ball bearings is very crucial for the reliable operation. This study is focused on fault diagnosis of ball bearings using artificial neural network (ANN) and support vector machine (SVM). A test rig of high speed rotor supported on rolling bearings is used. The vibration response are obtained and analyzed for the various defects of ball bearings. The specific defects are considered as crack in outer race, inner race with rough surface and corrosion pitting in balls. Statistical methods are used to extract features and to reduce the dimensionality of original vibration features. A comparative experimental study of the effectiveness of ANN and SVM is carried out. The results show that the machine learning algorithms mentioned above can be used for automated diagnosis of bearing faults. It is also observed that the severe (chaotic) vibrations occur under bearings with rough inner race surface and ball with corrosion pitting.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1876–1886},
numpages = {11},
keywords = {Support vector machine, Fault diagnosis, Artificial neural network}
}

@article{10.1016/j.neucom.2019.06.075,
author = {Xue, Yani and Li, Miqing and Shepperd, Martin and Lauria, Stasha and Liu, Xiaohui},
title = {A novel aggregation-based dominance for Pareto-based evolutionary algorithms to configure software product lines},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {364},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.075},
doi = {10.1016/j.neucom.2019.06.075},
journal = {Neurocomput.},
month = oct,
pages = {32–48},
numpages = {17},
keywords = {Multi-objective optimization, Evolutionary algorithm, Software product line, Optimal feature selection}
}

@inproceedings{10.1007/978-3-030-33246-4_45,
author = {Gonz\'{a}lez-Rojas, Oscar and Tafurth, Juan},
title = {Multi-cloud Services Configuration Based on Risk Optimization},
year = {2019},
isbn = {978-3-030-33245-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33246-4_45},
doi = {10.1007/978-3-030-33246-4_45},
abstract = {Nowadays risk analysis becomes critical in the Cloud Computing domain due to the increasing number of threats affecting applications running on cloud infrastructures. Multi-cloud environments allow connecting and migrating services from multiple cloud providers to manage risks. This paper addresses the question of how to model and configure multi-cloud services that can adapt to changes in user preferences and threats on individual and composite services. We propose an approach that combines Product Line (PL) and Machine Learning (ML) techniques to model and timely find optimal configurations of large adaptive systems such as multi-cloud services. A three-layer variability modeling on domain, user preferences, and adaptation constraints is proposed to configure multi-cloud solutions. ML regression algorithms are used to quantify the risk of resulting configurations by analyzing how a service was affected by incremental threats over time. An experimental evaluation on a real life electronic identification and trust multi-cloud service shows the applicability of the proposed approach to predict the risk for alternative re-configurations on autonomous and decentralized services that continuously change their availability and provision attributes.},
booktitle = {On the Move to Meaningful Internet Systems: OTM 2019 Conferences: Confederated International Conferences: CoopIS, ODBASE, C&amp;TC 2019, Rhodes, Greece, October 21–25, 2019, Proceedings},
pages = {733–749},
numpages = {17},
keywords = {Machine learning, Risk optimization, Product line configuration, Variability modeling, Multi-cloud services},
location = {Rhodes, Greece}
}

@inproceedings{10.1109/ICCSIT.2008.90,
author = {Zhigang, Zeng and Guohua, Chen},
title = {Application of Genetic Algorithm in Soft-sensing for Drying System of Al-product Auto-product-line},
year = {2008},
isbn = {9780769533087},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCSIT.2008.90},
doi = {10.1109/ICCSIT.2008.90},
abstract = {This paper constructs a soft-sensing model by genetic algorithm for Al-product auto-product-line. The paper deduces the optimum work point and input, and realizes static control and optimum of energy using inner variable and feed-back control. At last, the author illuminates that genetic algorithm has a great advantage comparing with the other traditional ways by application of genetic algorithm in the example.},
booktitle = {Proceedings of the 2008 International Conference on Computer Science and Information Technology},
pages = {687–690},
numpages = {4},
keywords = {genetic algorithm, soft-sensing, drying system, optimum of energy},
series = {ICCSIT '08}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Video testing, Variability modeling, Software product line engineering, Feature modeling, Domain-specific languages, Configuration, Automated reasoning}
}

@inproceedings{10.1007/978-3-030-78609-0_47,
author = {Shang, Di and Hu, Zhenda and Wang, Zhaoxia},
title = {Mining Consumer Brand Relationship from Social Media Data: A Natural Language Processing Approach},
year = {2021},
isbn = {978-3-030-78608-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78609-0_47},
doi = {10.1007/978-3-030-78609-0_47},
abstract = {There is a rich collection of studies exploring different aspects of consumer brand relationship. Traditional approaches of questionnaires and analysis are based on measurements collected from a relatively small number of survey participants. With the advancements in natural language processing (NLP) techniques, opportunities exist for applying NLP techniques to discover consumer brand relationship from social media platforms that possess a large amount of data on consumer opinion and sentiment. In this study, we review consumer brand relationship analysis focusing on leveraging NLP and machine learning techniques to address some challenges associated with discovering customer brand relationship from social media data and propose a methodological framework for the approach. This study has implications for both academic research and practitioners as it presents an alternative way to investigate consumer brand relationship.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part I},
pages = {553–565},
numpages = {13},
keywords = {Machine learning, Consumer brand relationship, Text mining, NLP},
location = {Dublin, Ireland}
}

@inproceedings{10.1145/2600428.2609601,
author = {Cormack, Gordon V. and Grossman, Maura R.},
title = {Evaluation of machine-learning protocols for technology-assisted review in electronic discovery},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609601},
doi = {10.1145/2600428.2609601},
abstract = {Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P&lt;0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P&lt;0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of "stabilization" -- determining when training is adequate, and therefore may stop.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {153–162},
numpages = {10},
keywords = {technology-assisted review, predictive coding, electronic discovery, e-discovery},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Web System, Software Product Line, Machine Learning, Energy Aware},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@article{10.1016/j.dss.2003.09.002,
author = {Alexouda, Georgia},
title = {A user-friendly marketing decision support system for the product line design using evolutionary algorithms},
year = {2005},
issue_date = {January 2005},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {38},
number = {4},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2003.09.002},
doi = {10.1016/j.dss.2003.09.002},
abstract = {A marketing decision support system (MDSS) is presented. It has a user-friendly and easy to learn menu driven interface. Its purpose is to assist a marketing manager in designing a line of substitute products. Optimal product line design is a very important marketing decision. The MDSS uses three different optimization criteria. It examines different scenarios using the "What if analysis". Also, it finds optimal solutions only for small sized problems using the complete enumeration method and near optimal solutions for real sized problems using evolutionary algorithms. The user is not forced to be familiar with the underlying models.},
journal = {Decis. Support Syst.},
month = jan,
pages = {495–509},
numpages = {15},
keywords = {product line design, marketing decision support systems, heuristic methods, evolutionary algorithms, NP-hard problems}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacit\'{u}a, Ricardo and Sep\'{u}lveda, Samuel and Mazo, Ra\'{u}l},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Models, Classification, Framework, Software product lines, Feature model}
}

@article{10.1016/j.cor.2004.07.011,
author = {(Sundar) Balakrishnan, P. V. and Gupta, Rakesh and Jacob, Varghese S.},
title = {An investigation of mating and population maintenance strategies in hybrid genetic heuristics for product line designs},
year = {2006},
issue_date = {March 2006},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {33},
number = {3},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2004.07.011},
doi = {10.1016/j.cor.2004.07.011},
abstract = {This research builds on prior work on developing near optimal solutions to the product line design problems within the conjoint analysis framework. In this research, we investigate and compare different genetic algorithm operators; in particular, we examine systematically the impact of employing alternative population maintenance strategies and mutation techniques within our problem context. Two alternative population maintenance methods, that we term ''Emigration'' and ''Malthusian'' strategies, are deployed to govern how individual product lines in one generation are carried over to the next generation. We also allow for two different types of reproduction methods termed ''Equal Opportunity'' in which the parents to be paired for mating are selected with equal opportunity and a second based on always choosing the best string in the current generation as one of the parents which is referred to as the ''Queen bee'', while the other parent is randomly selected from the set of parent strings. We also look at the impact of integrating the artificial intelligence approach with a traditional optimization approach by seeding the GA with solutions obtained from a Dynamic Programming heuristic proposed by others. A detailed statistical analysis is also carried out to determine the impact of various problem and technique aspects on multiple measures of performance through means of a Monte Carlo simulation study. Our results indicate that such proposed procedures are able to provide multiple ''good'' solutions. This provides more flexibility for the decision makers as they now have the opportunity to select from a number of very good product lines. The results obtained using our approaches are encouraging, with statistically significant improvements averaging 5% or more, when compared to the traditional benchmark of the heuristic dynamic programming technique.},
journal = {Comput. Oper. Res.},
month = mar,
pages = {639–659},
numpages = {21},
keywords = {Marketing, Malthusian, Hybrid heuristics, Genetic algorithms, Dynamic programming, Decision support, Conjoint analysis, Attribute importance}
}

@inproceedings{10.1007/978-3-030-58545-7_45,
author = {Li, Junbing and Zhang, Changqing and Zhu, Pengfei and Wu, Baoyuan and Chen, Lei and Hu, Qinghua},
title = {SPL-MLL: Selecting Predictable Landmarks for Multi-label Learning},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_45},
doi = {10.1007/978-3-030-58545-7_45},
abstract = {Although significant progress achieved, multi-label classification is still challenging due to the complexity of correlations among different labels. Furthermore, modeling the relationships between input and some (dull) classes further increases the difficulty of accurately predicting all possible labels. In this work, we propose to select a small subset of labels as landmarks which are easy to predict according to input (predictable) and can well recover the other possible labels (representative). Different from existing methods which separate the landmark selection and landmark prediction in the 2-step manner, the proposed algorithm, termed Selecting Predictable Landmarks for Multi-Label Learning (SPL-MLL), jointly conducts landmark selection, landmark prediction, and label recovery in a unified framework, to ensure both the representativeness and predictableness for selected landmarks. We employ the Alternating Direction Method (ADM) to solve our problem. Empirical studies on real-world datasets show that our method achieves superior classification performance over other state-of-the-art methods.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {783–799},
numpages = {17},
keywords = {Multi-label learning, Predictable landmarks, A unified framework},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s10270-017-0610-0,
author = {Guo, Jianmei and Liang, Jia Hui and Shi, Kai and Yang, Dingyu and Zhang, Jingsong and Czarnecki, Krzysztof and Ganesh, Vijay and Yu, Huiqun},
title = {SMTIBEA: a hybrid multi-objective optimization algorithm for configuring large constrained software product lines},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0610-0},
doi = {10.1007/s10270-017-0610-0},
abstract = {A key challenge to software product line engineering is to explore a huge space of various products and to find optimal or near-optimal solutions that satisfy all predefined constraints and balance multiple often competing objectives. To address this challenge, we propose a hybrid multi-objective optimization algorithm called SMTIBEA that combines the indicator-based evolutionary algorithm (IBEA) with the satisfiability modulo theories (SMT) solving. We evaluated the proposed algorithm on five large, constrained, real-world SPLs. Compared to the state-of-the-art, our approach significantly extends the expressiveness of constraints and simultaneously achieves a comparable performance. Furthermore, we investigate the performance influence of the SMT solving on two evolutionary operators of the IBEA.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1447–1466},
numpages = {20},
keywords = {Software product lines, Search-based software engineering, Multi-objective evolutionary algorithms, Feature models, Constraint solving}
}

@article{10.1007/s10009-019-00528-0,
author = {Dimovski, Aleksandar S.},
title = {CTL⋆ family-based model checking using variability abstractions and modal transition systems},
year = {2020},
issue_date = {Feb 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {1},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-019-00528-0},
doi = {10.1007/s10009-019-00528-0},
abstract = {Variational systems can produce a (potentially huge) number of related systems, known as products or variants, by using features (configuration options) to mark the variable functionality. In many of the application domains, their rigorous verification and analysis are very important, yet the appropriate tools rarely are able to analyse variational systems. Recently, this problem was addressed by designing specialized so-called family-based model checking algorithms, which allow simultaneous verification of all variants in a single run by exploiting the commonalities between the variants. Yet, their computational cost still greatly depends on the number of variants (the size of configuration space), which is often huge. Moreover, their implementation and maintenance represent a costly research and development task. One of the most promising approaches to fighting the configuration space explosion problem is variability abstractions, which simplify variability away from variational systems. In this work, we show how to achieve efficient family-based model checking of CTL⋆ temporal properties using variability abstractions and off-the-shelf (single-system) tools. We use variability abstractions for deriving abstract family-based model checking, where the variability model of a variational system is replaced with an abstract (smaller) version of it, called modal transition system, which preserves the satisfaction of both universal and existential temporal properties, as expressible in CTL⋆. Modal transition systems contain two kinds of transitions, termed may- and must-transitions, which are defined by the conservative (over-approximating) abstractions and their dual (under-approximating) abstractions, respectively. The variability abstractions can be combined with different partitionings of the configuration space to infer suitable divide-and-conquer verification plans for the given variational system. We illustrate the practicality of this approach for several variational systems using the standard version of (single-system) NuSMV model checker.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = feb,
pages = {35–55},
numpages = {21},
keywords = {CTL* temporal logic, Featured transition systems, Modal transition systems, Abstract interpretation, Family-based model checking, Software product line engineering}
}

@inproceedings{10.1007/978-3-030-89370-5_24,
author = {Xu, Lu and Zhong, Xian and Liu, Wenxuan and Zhao, Shilei and Yang, Zhengwei and Zhong, Luo},
title = {Subspace Enhancement and Colorization Network for Infrared Video Action Recognition},
year = {2021},
isbn = {978-3-030-89369-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89370-5_24},
doi = {10.1007/978-3-030-89370-5_24},
abstract = {Human action recognition is an essential area of research in the field of computer vision. However, existing methods ignore the essence of infrared image spectral imaging. Compared with the visible modality with all three channels, the infrared modality with approximate single-channel pays more attention to the lightness contrast and loses the channel information. Therefore, we explore channel duplication and tend to investigate more appropriate feature presentations. We propose a subspace enhancement and colorization network (S2ECNet) to recognize infrared video action recognition. Specifically, we apply the subspace enhancement (S2E) module to promote edge contour extraction with subspace. Meanwhile, a subspace colorization (S2C) module is utilized for better completing missing semantic information. What is more, the optical flow provides effective supplements for temporal information. Experiments conducted on the infrared action recognition dataset InfAR demonstrates the competitiveness of the proposed method compared with the state-of-the-arts.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III},
pages = {321–336},
numpages = {16},
keywords = {Feature fusion, Optical flow, Subspace colorization, Subspace enhancement, Infrared video action recognition},
location = {Hanoi, Vietnam}
}

@article{10.5555/3455716.3455773,
author = {Ma, Fan and Meng, Deyu and Dong, Xuanyi and Yang, Yi},
title = {Self-paced multi-view co-training},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a "draw without replacement" strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {57},
numpages = {38},
keywords = {probably approximately correct learnable, ε-expansion theory, semi-supervised learning, multi-view learning, self-paced learning, co-training}
}

@article{10.1016/j.infsof.2013.02.007,
author = {Santos Rocha, Roberto dos and Fantinato, Marcelo},
title = {The use of software product lines for business process management},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.02.007},
doi = {10.1016/j.infsof.2013.02.007},
abstract = {ContextBusiness Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT. ObjectivePresenting the results of a study undertaken to seek and assess PL approaches for BPM through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic PL approaches for BPM. MethodA SLR was conducted with four research questions formulated to evaluate PL approaches for BPM. Results63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that PLs only partially address the BPM lifecycle since the last business process phase is not a current concern on the found approaches. ConclusionsThe found PL approaches for BPM only cover partially the BPM lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic PL proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that PL approaches for BPM are still at an early stage and gaining maturity.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1355–1373},
numpages = {19},
keywords = {Software product line, PL, Business process management, BPM}
}

@article{10.1016/j.eswa.2015.02.020,
author = {Dermeval, Diego and Ten\'{o}rio, Thyago and Bittencourt, Ig Ibert and Silva, Alan and Isotani, Seiji and Ribeiro, M\'{a}rcio},
title = {Ontology-based feature modeling},
year = {2015},
issue_date = {July 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {11},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.02.020},
doi = {10.1016/j.eswa.2015.02.020},
abstract = {We compare two ontology-based feature modeling styles by conducting an experiment.The results show that ontology factor has statistical significance in all metrics.The results show that the ontology based on instances is more flexible.The results show that the ontology based on instances demands less time to change. A software product line (SPL) is a set of software systems that have a particular set of common features and that satisfy the needs of a particular market segment or mission. Feature modeling is one of the key activities involved in the design of SPLs. The feature diagram produced in this activity captures the commonalities and variabilities of SPLs. In some complex domains (e.g., ubiquitous computing, autonomic systems and context-aware computing), it is difficult to foresee all functionalities and variabilities a specific SPL may require. Thus, Dynamic Software Product Lines (DSPLs) bind variation points at runtime to adapt to fluctuations in user needs as well as to adapt to changes in the environment. In this context, relying on formal representations of feature models is important to allow them to be automatically analyzed during system execution. Among the mechanisms used for representing and analyzing feature models, description logic (DL) based approaches demand to be better investigated in DSPLs since it provides capabilities, such as automated inconsistency detection, reasoning efficiency, scalability and expressivity. Ontology is the most common way to represent feature models knowledge based on DL reasoners. Previous works conceived ontologies for feature modeling either based on OWL classes and properties or based on OWL individuals. However, considering change or evolution scenarios of feature models, we need to compare whether a class-based or an individual-based feature modeling style is recommended to describe feature models to support SPLs, and especially its capabilities to deal with changes in feature models, as required by DSPLs. In this paper, we conduct a controlled experiment to empirically compare two approaches based on each one of these modeling styles in several changing scenarios (e.g., add/remove mandatory feature, add/remove optional feature and so on). We measure time to perform changes, structural impact of changes (flexibility) and correctness for performing changes in our experiment. Our results indicate that using OWL individuals requires less time to change and is more flexible than using OWL classes and properties. These results provide insightful assumptions towards the definition of an approach relying on reasoning capabilities of ontologies that can effectively support products reconfiguration in the context of DSPL.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {4950–4964},
numpages = {15},
keywords = {Software product line, Ontology, Feature modeling, Empirical software engineering}
}

@article{10.1007/s00766-015-0237-z,
author = {Oliinyk, Olesia and Petersen, Kai and Schoelzke, Manfred and Becker, Martin and Schneickert, Soeren},
title = {Structuring automotive product lines and feature models: an exploratory study at Opel},
year = {2017},
issue_date = {March     2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-015-0237-z},
doi = {10.1007/s00766-015-0237-z},
abstract = {Automotive systems are highly complex and customized systems containing a vast amount of variability. 
Feature modeling plays a key role in customization. Empirical evidence through industry application, and in particular methodological guidance of how to structure automotive product lines and their feature models is needed. The overall aim of this work is to provide guidance to practitioners how to structure automotive product lines and their feature models, understanding strengths and weaknesses of alternative structures. The research was conducted in three phases. In the first phase, the context situation was understood using interviews and workshops. In the second phase, possible structures of product lines and feature models were evaluated based on industry feedback collected in workshops. In the third phase, the structures were implemented in the tool GEARS and practitioner feedback was collected. One key challenge was the unavailability of structuring guidelines, which was the focus of this research. The structures considered most suitable for the automotive product line were multiple product lines with modular decomposition. The structures most suitable for the feature model were functional decomposition, using context variability, models corresponding to assets, and feature categories. Other structures have been discarded, and the rationales have been presented. It was possible to support the most suitable structures with the commercial tool GEARS. The implementation in GEARS and the feedback from the practitioners provide early indications for the potential usefulness of the structures and the tool implementation.},
journal = {Requir. Eng.},
month = mar,
pages = {105–135},
numpages = {31},
keywords = {Variability modeling, Product line engineering, Feature modeling, Empirical, Case study, Automotive}
}

@article{10.5555/3546258.3546440,
author = {Klink, Pascal and Abdulsamad, Hany and Belousov, Boris and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {A probabilistic interpretation of self-paced learning with applications to reinforcement learning},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {182},
numpages = {52},
keywords = {rl-as-inference, tempered inference, self-paced learning, reinforcement learning, curriculum learning}
}

@article{10.5555/3322706.3361993,
author = {Glimsdal, Sondre and Granmo, Ole-Christoffer},
title = {Thompson sampling guided stochastic searching on the line for deceptive environments with applications to root-finding problems},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {The multi-armed bandit problem forms the foundation for solving a wide range of online stochastic optimization problems through a simple, yet effective mechanism. One simply casts the problem as a gambler who repeatedly pulls one out of N slot machine arms, eliciting random rewards. Learning of reward probabilities is then combined with reward maximization, by carefully balancing reward exploration against reward exploitation. In this paper, we address a particularly intriguing variant of the multi-armed bandit problem, referred to as the Stochastic Point Location (SPL) problem. The gambler is here only told whether the optimal arm (point) lies to the "left" or to the "right" of the arm pulled, with the feedback being erroneous with probability 1 - π. This formulation thus targets optimization in continuous action spaces with both informative and deceptive feedback. To tackle this class of problems, we formulate a compact and scalable Bayesian representation of the solution space that simultaneously captures both the location of the optimal arm as well as the probability of receiving correct feedback. We further introduce the accompanying Thompson Sampling guided Stochastic Point Location (TS-SPL) scheme for balancing exploration against exploitation. By learning π, TS-SPL also supports deceptive environments that are lying about the direction of the optimal arm. This, in turn, allows us to address the fundamental Stochastic Root Finding (SRF) problem. Empirical results demonstrate that our scheme deals with both deceptive and informative environments, significantly outperforming competing algorithms both for SRF and SPL.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1910–1933},
numpages = {24},
keywords = {thompson sampling, stochastic point location, searching on the line, probabilistic bisection search, deceptive environment}
}

@article{10.1016/S0305-0548(03)00173-4,
author = {Alexouda, G.},
title = {An evolutionary algorithm approach to the share of choices problem in the product line design},
year = {2004},
issue_date = {November 2004},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {31},
number = {13},
issn = {0305-0548},
url = {https://doi.org/10.1016/S0305-0548(03)00173-4},
doi = {10.1016/S0305-0548(03)00173-4},
abstract = {In this paper an evolutionary algorithm (EA) approach for solving the share of choices problem in the design of a line of substitute products is presented. Because this problem is NP-hard, we are forced to use heuristic methods, which do not guarantee obtaining the optimal solution. In order to see whether there exists any computational advantage of the EA approach, it is compared to the beam search (BS) heuristic method. An extensive comparative computational study is performed. The solutions found by the EA are close to optimal. Moreover, in most cases the EA obtains a better solution than that found by the BS method. In most problem sizes the BS method is faster than the EA. However, the CPU time needed by the EA is very reasonable. Moreover, the EA can be used as an effective second step to the BS method.},
journal = {Comput. Oper. Res.},
month = nov,
pages = {2215–2229},
numpages = {15},
keywords = {share of choices problem, product line design, marketing, heuristic methods, evolutionary algorithms}
}

@article{10.1016/j.scico.2017.10.013,
author = {Castro, Thiago and Lanna, Andr and Alves, Vander and Teixeira, Leopoldo and Apel, Sven and Schobbens, Pierre-Yves},
title = {All roads lead to Rome},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2017.10.013},
doi = {10.1016/j.scico.2017.10.013},
abstract = {The formalization of seven strategies for product-line reliability analysis.The first feature-family-product-based strategy for product-line model checking.A general principle for lifting analyses to product lines using ADDs.Proofs that the formalized strategies commute.All strategies proven sound with respect to single-product reliability analysis. Software product line engineering is a means to systematically manage variability and commonality in software systems, enabling the automated synthesis of related programs (products) from a set of reusable assets. However, the number of products in a software product line may grow exponentially with the number of features, so it is practically infeasible to quality-check each of these products in isolation. There is a number of variability-aware approaches to product-line analysis that adapt single-product analysis techniques to cope with variability in an efficient way. Such approaches can be classified along three analysis dimensions (product-based, family-based, and feature-based), but, particularly in the context of reliability analysis, there is no theory comprising both (a) a formal specification of the three dimensions and resulting analysis strategies and (b) proof that such analyses are equivalent to one another. The lack of such a theory hinders formal reasoning on the relationship between the analysis dimensions and derived analysis techniques. We formalize seven approaches to reliability analysis of product lines, including the first instance of a feature-family-product-based analysis in the literature. We prove the formalized analysis strategies to be sound with respect to the probabilistic approach to reliability analysis of a single product. Furthermore, we present a commuting diagram of intermediate analysis steps, which relates different strategies and enables the reuse of soundness proofs between them.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {116–160},
numpages = {45},
keywords = {Verification, Software product lines, Reliability analysis, Product-line analysis, Model checking}
}

@inproceedings{10.1007/978-3-030-43680-3_11,
author = {Saber, Takfarinas and Brevet, David and Botterweck, Goetz and Ventresque, Anthony},
title = {MILPIBEA: Algorithm for&nbsp;Multi-objective Features Selection in&nbsp;(Evolving) Software Product Lines},
year = {2020},
isbn = {978-3-030-43679-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-43680-3_11},
doi = {10.1007/978-3-030-43680-3_11},
abstract = {Software Product Lines Engineering (SPLE) proposes techniques to model, create and improve groups of related software systems in a systematic way, with different alternatives formally expressed, e.g., as Feature Models. Selecting the ‘best’ software system(s) turns into a problem of improving the quality of selected subsets of software features (components) from feature models, or as it is widely known, Feature Configuration. When there are different independent dimensions to assess how good a software product is, the problem becomes even more challenging – it is then a multi-objective optimisation problem. Another big issue for software systems is evolution where software components change. This is common in the industry but, as far as we know, there is no algorithm designed to the particular case of multi-objective optimisation of evolving software product lines. In this paper we present MILPIBEA, a novel hybrid algorithm which combines the scalability of a genetic algorithm (IBEA) with the accuracy of a mixed-integer linear programming solver (IBM ILOG CPLEX). We also study the behaviour of our solution (MILPIBEA) in contrast with SATIBEA, a state-of-the-art algorithm in static software product lines. We demonstrate that MILPIBEA outperforms SATIBEA on average, especially for the most challenging problem instances, and that MILPIBEA is the one that continues to improve the quality of the solutions when SATIBEA stagnates (in the evolving context).},
booktitle = {Evolutionary Computation in Combinatorial Optimization: 20th European Conference, EvoCOP 2020, Held as Part of EvoStar 2020, Seville, Spain, April 15–17, 2020, Proceedings},
pages = {164–179},
numpages = {16},
keywords = {Mixed-integer linear programming, Evolutionary algorithm, Multi-objective optimisation, Feature selection, Software product line},
location = {Seville, Spain}
}

@inproceedings{10.1145/1621607.1621633,
author = {Sanen, Frans and Truyen, Eddy and Joosen, Wouter},
title = {Mapping problem-space to solution-space features: a feature interaction approach},
year = {2009},
isbn = {9781605584942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1621607.1621633},
doi = {10.1145/1621607.1621633},
abstract = {Mapping problem-space features into solution-space features is a fundamental configuration problem in software product line engineering. A configuration problem is defined as generating the most optimal combination of software features given a requirements specification and given a set of configuration rules. Current approaches however provide little support for expressing complex configuration rules between problem and solution space that support incomplete requirements specifications. In this paper, we propose an approach to model complex configuration rules based on a generalization of the concept of problem-solution feature interactions. These are interactions between solution-space features that only arise in specific problem contexts. The use of an existing tool to support our approach is also discussed: we use the DLV answer set solver to express a particular configuration problem as a logic program whose answer set corresponds to the optimal combinations of solution-space features. We motivate and illustrate our approach with a case study in the field of managing dynamic adaptations in distributed software, where the goal is to generate an optimal protocol for accommodating a given adaptation.},
booktitle = {Proceedings of the Eighth International Conference on Generative Programming and Component Engineering},
pages = {167–176},
numpages = {10},
keywords = {software product line engineering, problem-solution feature interactions, distributed runtime adaptation, default logic, configuration knowledge, DLV},
location = {Denver, Colorado, USA},
series = {GPCE '09}
}

@article{10.4018/jismd.2012100101,
author = {Asadi, Mohsen and Mohabbati, Bardia and Ga\v{s}evic, Dragan and Bagheri, Ebrahim and Hatala, Marek},
title = {Developing Semantically-Enabled Families of Method-Oriented Architectures},
year = {2012},
issue_date = {October 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012100101},
doi = {10.4018/jismd.2012100101},
abstract = {Method Engineering ME aims to improve software development methods by creating and proposing adaptation frameworks whereby methods are created to provide suitable matches with the requirements of the organization and address project concerns and fit specific situations. Therefore, methods are defined and modularized into components stored in method repositories. The assembly of appropriate methods depends on the particularities of each project, and rapid method construction is inevitable in the reuse and management of existing methods. The ME discipline aims at providing engineering capability for optimizing, reusing, and ensuring flexibility and adaptability of methods; there are three key research challenges which can be observed in the literature: 1 the lack of standards and tooling support for defining, publishing, discovering, and retrieving methods which are only locally used by their providers without been largely adapted by other organizations; 2 dynamic adaptation and assembly of methods with respect to imposed continuous changes or evolutions of the project lifecycle; and 3 variability management in software methods in order to enable rapid and effective construction, assembly and adaptation of existing methods with respect to particular situations. The authors propose semantically-enabled families of method-oriented architecture by applying service-oriented product line engineering principles and employing Semantic Web technologies.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {1–26},
numpages = {26},
keywords = {Software Product Line, Software Development, Semantic Web, Method Oriented Architecture MOA, Method Engineering}
}

@inproceedings{10.1145/3321408.3326676,
author = {Yan, Liu and Hu, Wenxin and Han, Longzhe},
title = {Optimize SPL test cases with adaptive simulated annealing genetic algorithm},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3326676},
doi = {10.1145/3321408.3326676},
abstract = {In Software Product Line (SPL) testing, reduced test suite with high coverage is useful for early features interaction detection. sGA (simplified genetic algorithm) and SAGA(simulated annealing genetic algorithm) can generate high coverage test suite. However, small probability mutations in updating test suite may reduce search efficiency and thus miss better solutions. An improved test cases generation method based on ASAGA (Adaptive simulated annealing genetic algorithm) is proposed. Experiments on SPLOT (Software Product Lines Online Tools) feature models show that the proposed hybrid ASAGA method can ensure local optimization accuracy and achieve smaller-size test suite with higher coverage.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {148},
numpages = {7},
keywords = {test case, software test, similarity measurement, feature model, ASAGA},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@article{10.1016/j.jss.2013.10.010,
author = {White, Jules and Galindo, Jos\'{e} A. and Saxena, Tripti and Dougherty, Brian and Benavides, David and Schmidt, Douglas C.},
title = {Evolving feature model configurations in software product lines},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {87},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.10.010},
doi = {10.1016/j.jss.2013.10.010},
abstract = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps.},
journal = {J. Syst. Softw.},
month = jan,
pages = {119–136},
numpages = {18},
keywords = {Software product line, Multi-step configuration, Feature model}
}

@inproceedings{10.5555/3495724.3496169,
author = {Parvaneh, Amin and Abbasnejad, Ehsan and Teney, Damien and Shi, Javen Qinfeng and van den Hengel, Anton},
title = {Counterfactual vision-and-language navigation: unravelling the unseen},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The task of vision-and-language navigation (VLN) requires an agent to follow text instructions to find its way through simulated household environments. A prominent challenge is to train an agent capable of generalising to new environments at test time, rather than one that simply memorises trajectories and visual details observed during training. We propose a new learning strategy that learns both from observations and generated counterfactual environments. We describe an effective algorithm to generate counterfactual observations on the fly for VLN, as linear combinations of existing environments. Simultaneously, we encourage the agent's actions to remain stable between original and counterfactual environments through our novel training objective – effectively removing spurious features that would otherwise bias the agent. Our experiments show that this technique provides significant improvements in generalisation on benchmarks for Room-to-Room navigation and Embodied Question Answering.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {445},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.5220/0005679102800287,
author = {Diedrich, Alexander and B\"{o}ttcher, Bj\"{o}rn and Niggemann, Oliver},
title = {Exposing Design Mistakes During Requirements Engineering by Solving Constraint Satisfaction Problems to Obtain Minimum Correction Subsets},
year = {2016},
isbn = {9789897581724},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005679102800287},
doi = {10.5220/0005679102800287},
abstract = {In recent years, the complexity of production plants and therefore of the underlying automation systems has grown significantly. This makes the manual design of automation systems increasingly difficult. As a result, errors are found only during production, plant modifications are hindered by not maintainable automation solutions and criteria such as energy efficiency or cost are often not optimized. This work shows how utilizing Minimum Correction Subsets (MCS) of a Constraint Satisfaction Problem improves the collaboration of automation system designers and prevents inconsistent requirements and thus subsequent errors in the design. This opens up a new field of application for constraint satisfaction techniques. As a use case, an example from the field of automation system design is presented. To meet the automation industry\^{a} s requirement for standardised solutions that assure reliability, the calculation of MCS is formulated in such a way that most constraint solvers can be used without any extensions. Experimental results with typical problems demonstrate the practicalness concerning runtime and hardware resources.},
booktitle = {Proceedings of the 8th International Conference on Agents and Artificial Intelligence},
pages = {280–287},
numpages = {8},
keywords = {Product Line Engineering, Minimum Correction Subsets., Feature Models, Constraint Satisfaction},
location = {Rome, Italy},
series = {ICAART 2016}
}

@article{10.1007/s10462-020-09907-5,
author = {Uma Maheswari, S. and Shahina, A. and Nayeemulla Khan, A.},
title = {Understanding Lombard speech: a review of compensation techniques towards improving speech based recognition systems},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {4},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09907-5},
doi = {10.1007/s10462-020-09907-5},
abstract = {Building voice-based Artificial Intelligence (AI) systems that can efficiently interact with humans through speech has become plausible today due to rapid strides in efficient data-driven AI techniques. Such a human–machine voice interaction in real world would often involve a noisy ambience, where humans tend to speak with additional vocal effort than in a quiet ambience, to mitigate the noise-induced suppression of vocal self-feedback. This noise induced change in the vocal effort is called Lombard speech. In order to build intelligent conversational devices that can operate in a noisy ambience, it is imperative to study the characteristics and processing of Lombard speech. Though the progress of research on Lombard speech started several decades ago, it needs to be explored further in the current scenario which is seeing an explosion of voice-driven applications. The system designed to work with normal speech spoken in a quiet ambience fails to provide the same performance in changing environmental contexts. Different contexts lead to different styles of Lombard speech and hence there arises a need for efficient ways of handling variations in speaking styles in noise. The Lombard speech is also more intelligible than normal speech of a speaker. Applications like public announcement systems with speech output interface should talk with varying degrees of vocal effort to enhance naturalness in a way that humans adapt to speak in noise, in real time. This review article is an attempt to summarize the progress of work on the possible ways of processing Lombard speech to build smart and robust human–machine interactive systems with speech input–output interface, irrespective of operating environmental contexts, for different application needs. This article is a comprehensive review of the studies on Lombard speech, highlighting the key differences observed in acoustic and perceptual analysis of Lombard speech and detailing the Lombard effect compensation methods towards improving the robustness of speech based recognition systems.},
journal = {Artif. Intell. Rev.},
month = apr,
pages = {2495–2523},
numpages = {29},
keywords = {Lombard speech synthesis, Lombard effect compensation, Automatic recognition systems, Perceptual analysis, Acoustic analysis, Lombard speech}
}

@inproceedings{10.1145/3168365.3168375,
author = {Bezerra, Carla I. M. and Andrade, Rossana M. C. and Monteiro, Jos\'{e} M. S. and Cedraz, Davi},
title = {Aggregating Measures using Fuzzy Logic for Evaluating Feature Models},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168375},
doi = {10.1145/3168365.3168375},
abstract = {In the context of Software Product Lines (SPLs), evaluating the quality of a feature model is essential to ensure that errors in the early stages do not spread throughout the SPL. One way to evaluate a feature model is to use measures. However, measures alone are not enough to characterize the feature model quality, because most of them cover specific aspects, such as the number of features. So, there is a need for methods to aggregate measures at the level of quality sub-characteristic or characteristic. In this paper, we aim to investigate how to aggregate measures that have been proposed to evaluate the quality of feature models in SPL. We have used the fuzzy logic theory in order to aggregate these measures. The new aggregated measures can be applied to evaluate different and complex aspects of a feature model, such as: size, stability, flexibility and dynamicity. Moreover, to evaluate the use of the new aggregate measures, we applied them in different feature models. Our findings suggest that aggregate measures can assist the domain engineer in evaluating the maintainability of feature models.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {35–42},
numpages = {8},
keywords = {Software Product Line, Measures, Fuzzy Logic, Feature Models},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1016/j.jss.2019.02.028,
author = {Jakubovski Filho, Helson Luiz and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Preference based multi-objective algorithms applied to the variability testing of software product lines},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.028},
doi = {10.1016/j.jss.2019.02.028},
journal = {J. Syst. Softw.},
month = may,
pages = {194–209},
numpages = {16},
keywords = {Preference-Based algorithms, Search-Based software engineering, Software product line testing}
}

@article{10.1287/mnsc.45.11.1524,
author = {Swamidass, Paul M. and Nair, Satish S. and Mistry, Sanjay I.},
title = {The Use of a Neural Factory to Investigate the Effect of Product Line Width on Manufacturing Performance},
year = {1999},
issue_date = {November 1999},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {45},
number = {11},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.45.11.1524},
doi = {10.1287/mnsc.45.11.1524},
abstract = {The dual goals of this study are: (1) to develop an empirically valid neural model of U.S. factories in a range of industries producing discrete products, and (2) to use the model to test the effect of changes in product line width on plant performance variables. Accordingly, a neural factory was developed using 59 input and 5 output/performance variables, and was trained using field data collected from 385 U.S. manufacturing plants. The model was validated using a holdout sample before conducting sensitivity tests. The study demonstrates that, through the use of parametric sensitivity analysis, the neural factory could be used to investigate the relationship between inputs and performance of a factory.While the focused factory principle would favor a smaller product line, economies of scope theory would favor a larger product line for the good of the factory; this implies a rather complex relationship between product line width (PLW) and plant performance. The neural factory was used to study the sensitivity of output/performance variables when product line width was varied over a range extending from 10% to 200% of the average values. The sensitivity analysis of the neural factory shows that, as the product line increases, it (1) does not affect cost-of-goods-sold (COGS), (2) decreases return on investment, (3) has a negative effect on the top management's perception of manufacturing performance, (4) increases inventory turns, and (5) increases sales per employee.The explanations for these findings show how complex and intertwined the relationships between PLW and performance variables are. They enhance our understanding of PLW and provide some new directions for future empirical research.},
journal = {Manage. Sci.},
month = nov,
pages = {1524–1538},
numpages = {15},
keywords = {product line width, factory focus, neural model of manufacturing, neural network, manufacturing plant model, factory model, neural factory}
}

@article{10.1007/s10270-017-0641-6,
author = {Li, Yan and Yue, Tao and Ali, Shaukat and Zhang, Li},
title = {Enabling automated requirements reuse and configuration},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0641-6},
doi = {10.1007/s10270-017-0641-6},
abstract = {A system product line (PL) often has a large number of reusable and configurable requirements, which in practice are organized hierarchically based on the architecture of the PL. However, the current literature lacks approaches that can help practitioners to systematically and automatically develop structured and configuration-ready PL requirements repositories. In the context of product line engineering and model-based engineering, automatic requirements structuring can benefit from models. Such a structured PL requirements repository can greatly facilitate the development of product-specific requirements repository, the product configuration at the requirements level, and the smooth transition to downstream product configuration phases (e.g., at the architecture design phase). In this paper, we propose a methodology with tool support, named as Zen-ReqConfig, to tackle the above challenge. Zen-ReqConfig is built on existing model-based technologies, natural language processing, and similarity measure techniques. It automatically devises a hierarchical structure for a PL requirements repository, automatically identifies variabilities in textual requirements, and facilitates the configuration of products at the requirements level, based on two types of variability modeling techniques [i.e., cardinality-based feature modeling (CBFM) and a UML-based variability modeling methodology (named as SimPL)]. We evaluated Zen-ReqConfig with five case studies. Results show that Zen-ReqConfig can achieve a better performance based on the character-based similarity measure Jaro than the term-based similarity measure Jaccard. With Jaro, Zen-ReqConfig can allocate textual requirements with high precision and recall, both over 95% on average and identify variabilities in textual requirements with high precision (over 97% on average) and recall (over 94% on average). Zen-ReqConfig achieved very good time performance: with less than a second for generating a hierarchical structure and less than 2 s on average for allocating a requirement. When comparing SimPL and CBFM, no practically significant difference was observed, and they both performed well when integrated with Zen-ReqConfig.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2177–2211},
numpages = {35},
keywords = {Reuse, Requirements, Product line, Feature model, Configuration}
}

@article{10.1504/ijbm.2021.112219,
author = {Gao, Feng and Luo, Daizhong and Ma, Xinqiang},
title = {Research on facial expression recognition of video stream based on OpenCV},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {1},
issn = {1755-8301},
url = {https://doi.org/10.1504/ijbm.2021.112219},
doi = {10.1504/ijbm.2021.112219},
abstract = {In order to overcome the poor performance of expression similarity measurement in traditional video stream facial expression recognition methods, an OpenCV based facial expression recognition method is proposed. In this method, the video stream face detection image is obtained by the window detection of various features in each position for the video stream image through the cascade classifier, and the image preprocessing is implemented. Based on OpenCV, the most important eyes and mouth in the facial expression are modeled, the eye feature model and mouth feature model are constructed, and the facial expression recognition of the video stream is realised through the constructed model. The experimental results show that the performance of expression similarity measurement is better, and the recognition rate of different expressions is more than 90%.},
journal = {Int. J. Biometrics},
month = jan,
pages = {114–129},
numpages = {15},
keywords = {facial expression recognition, face, video stream, OpenCV}
}

@inproceedings{10.1007/978-3-030-79382-1_24,
author = {Munoz, Daniel-Jesus and Gurov, Dilian and Pinto, Monica and Fuentes, Lidia},
title = {Category Theory Framework for Variability Models with Non-functional Requirements},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_24},
doi = {10.1007/978-3-030-79382-1_24},
abstract = {In Software Product Line (SPL) engineering one uses Variability Models (VMs) as input to automated reasoners to generate optimal products according to certain Quality Attributes (QAs). Variability models, however, and more specifically those including numerical features (i.e., NVMs), do not natively support QAs, and consequently, neither do automated reasoners commonly used for variability resolution. However, those satisfiability and optimisation problems have been covered and refined in other relational models such as databases.Category Theory (CT) is an abstract mathematical theory typically used to capture the common aspects of seemingly dissimilar algebraic structures. We propose a unified relational modelling framework subsuming the structured objects of VMs and QAs and their relationships into algebraic categories. This abstraction allows a combination of automated reasoners over different domains to analyse SPLs. The solutions’ optimisation can now be natively performed by a combination of automated theorem proving, hashing, balanced-trees and chasing algorithms. We validate this approach by means of the edge computing SPL tool HADAS.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {397–413},
numpages = {17},
keywords = {Category theory, Quality attribute, Non-functional requirement, Feature, Numerical variability model},
location = {Melbourne, VIC, Australia}
}

@article{10.1016/j.micpro.2021.103964,
author = {Gokilavani, N. and Bharathi, B.},
title = {Multi-Objective based test case selection and prioritization for distributed cloud environment},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {82},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2021.103964},
doi = {10.1016/j.micpro.2021.103964},
journal = {Microprocess. Microsyst.},
month = apr,
numpages = {6},
keywords = {Cloud environment, Software testing, Similarity-based clustering, Test case prioritization, Test case selection, Particle swarm optimization, Software product line}
}

@inproceedings{10.1609/aaai.v33i01.33014023,
author = {Jin, Binbin and Zhao, Hongke and Chen, Enhong and Liu, Qi and Ge, Yong},
title = {Estimating the days to success of campaigns in crowdfunding: a deep survival perspective},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014023},
doi = {10.1609/aaai.v33i01.33014023},
abstract = {Crowdfunding is an emerging mechanism for entrepreneurs or individuals to solicit funding from the public for their creative ideas. However, in these platforms, quite a large proportion of campaigns (projects) fail to raise enough money of backers' supports by the declared expiration date. Actually, it is very urgent to predict the exact success time of campaigns. But this problem has not been well explored due to a series of domain and technical challenges. In this paper, we notice the implicit factor of distribution of backing behaviors has a positive impact on estimating the success time of the campaign. Therefore, we present a focused study on predicting two specific tasks, i.e., backing distribution prediction and success time prediction of campaigns. Specifically, we propose a 5eq2seq based model with Multi-facet Priors (SMP), which can integrate heterogeneous features to jointly model the backing distribution and success time. Additionally, to keep the change of backing distributions more smooth as the backing behaviors increases, we develop a linear evolutionary prior for backing distribution prediction. Furthermore, due to high failure rate, the success time of most campaigns is unob-servable. We model this censoring phenomenon from the survival analysis perspective and also develop a non-increasing prior and a partial prior for success time prediction. Finally, we conduct extensive experiments on a real-world dataset from Indiegogo. Experimental results clearly validate the effectiveness of SMP.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {494},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/1655925.1656013,
author = {Alsawalqah, Hamad I. and Abotsi, Komi S. and Lee, Dan Hyung},
title = {An automated mechanism for organizing and retrieving core asset artifacts for product derivation in SPL},
year = {2009},
isbn = {9781605587103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1655925.1656013},
doi = {10.1145/1655925.1656013},
abstract = {Software Product Line, SPL, is a software development strategy in which products are developed from a common set of core assets in a prescribed way with product specific features to satisfy specific market segment [1]. The SPL development process is carried out in two phases: the first phase is about building core assets called domain engineering, which has gained a lot of researchers' attention. The second step is about instantiating the specifics of the products by adding to the common part the specific features that identify the product from the other application engineering. For large and complex domains, it is argued that organizing and retrieving the development of artifacts from the core asset required by the application under development is a way of shortening the application development time, thus reduces the time to market. In this paper, we propose an automation mechanism for organizing the core assets using feature based organization to divide the customized domain feature model based on the application features and their dependencies. When that retrieval step where the artifacts are represented by relations that inherit the dependencies between the features in each division of the feature model, takes place, the final result is a set of development artifacts with their traceability links to be customized based on the application variability model and integrated with the application specific artifacts. To demonstrate our work, we applied this mechanism on a watch, a case study in the digital watch domain.},
booktitle = {Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human},
pages = {480–485},
numpages = {6},
keywords = {software product line, product derivation, ontology, feature model, digital watch},
location = {Seoul, Korea},
series = {ICIS '09}
}

@article{10.1177/1046878118821402,
author = {Schmeller, Rebecca},
title = {In Strategy Simulations, Data Analysis Matters Most (More Than Number of Log Ins and More Than Time Spent Logged In)},
year = {2019},
issue_date = {Feb 2019},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {50},
number = {1},
issn = {1046-8781},
url = {https://doi.org/10.1177/1046878118821402},
doi = {10.1177/1046878118821402},
abstract = {Background. This data study examined the predictive power of student login activities (frequency, duration, opening reports) on simulation performance (seven Assurance of Learning scores), using Glo-Bus strategy simulation.Methods. The study used a sample of 351 students at a small, private Ohio university over four academic years (fall 2013 through spring 2017) to conduct regression analysis of three predictor variables (login data about reports, frequency, duration) and seven outcome variables (Assurance of Learning scores for finance, operations, marketing, HR, CSR, leadership, collaboration).Results. Opening reports was found to be more predictive of simulation performance than the other two login variables (frequency of logins, duration of logins).Discussion. In the literature, there were studies about strategy simulation login activity, but they did not address how Glo-Bus login activity types were related to Assurance of Learning scores. Some studies about strategy simulations called for more narrowly defined objectives and student accountability. This study supported that call – revealing that opening reports was more predictive of Assurance of Learning scores – suggesting students be held accountable for opening reports.Conclusions and Recommendations. Because this study found opening reports to be more predictive than other login activity, the software designers should promote report opening by making it a more direct and immediate aspect of the software and by including report analysis skills in the embedded quizzes. Additionally, instructor materials should include guidance on how to teach report use to Glo-Bus simulation students.},
journal = {Simul. Gaming},
month = feb,
pages = {62–75},
numpages = {14},
keywords = {strategy simulations, login activity, Glo-Bus, business capstone courses, Assurance of Learning}
}

@article{10.1016/j.procs.2019.12.135,
author = {Jamil, Muhammad Abid and Nour, Mohamed K and Alhindi, Ahmad and Awang Abhubakar, Normi Sham and Arif, Muhammad and Aljabri, Tareq Fahad},
title = {Towards Software Product Lines Optimization Using Evolutionary Algorithms},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {163},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.135},
doi = {10.1016/j.procs.2019.12.135},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {527–537},
numpages = {11},
keywords = {Multi-objective Algorithms, Feature Models, Software Product Lines, Software Testing, Search Based Software Engineering}
}

@article{10.1287/mksc.7.2.107,
author = {Dobson, Gregory and Kalish, Shlomo},
title = {Positioning and Pricing a Product Line},
year = {1988},
issue_date = {May 1988},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {7},
number = {2},
issn = {1526-548X},
url = {https://doi.org/10.1287/mksc.7.2.107},
doi = {10.1287/mksc.7.2.107},
abstract = {A central problem in marketing is: how should the firm position reposition and price a line of related substitute products in order to maximize profits or welfare. We formulate this problem faced by a monopolist as a mathematical program, outline how to obtain the market data from a sample of customers, discuss what cost data are relevant, and suggest a heuristic algorithm to solve the problem. The output of the process is a list of products to offer, their prices, and the customer segments which purchase each product.While additional real world complexities, e.g., uncertainty about customer wants, product performance, and competitive response, are not modeled, we believe the system developed can serve as an important input into the decision process when new products are designed and priced. The methodology can be used as a part of a decision support system, where management specifies the number of products desired. The system suggests a few good solutions, together with the prices and customer segments served by each product.We use the standard assumption that the market is composed of different customer segments of various sizes, each containing homogeneous customers. Customers choose one brand only, the one that provides them with maximum value for the money. The firm faces both fixed and variable production and marketing costs for each product. Competition is either nonexistent, or assumed not to respond to the firm's moves.The information available to the firm is the sizes and preferences of the segments, based on a sample of customers, and the cost data. As an alternative to the traditional approach of estimating a parametric utility function, and aggregating customers into segments, we can also use the raw data as input, where each customer in the sample represents a segment. This, we believe, allows us to reduce the errors introduced in the process.Heuristics for solving the problem are suggested. The heuristics are evaluated on a set of simulated problems, and compared to the optimal solutions. The heuristics perform well when compared to all feasible solutions on a set of small simulated problems. We also discuss the application of the procedure to a 'real life' sized problem.},
journal = {Marketing Science},
month = may,
pages = {107–125},
numpages = {19},
keywords = {product positioning pricing, product line, heuristics, decision support}
}

@inproceedings{10.1145/3302333.3302344,
author = {Ferreira, Fischer and Diniz, Jo\~{a}o P. and Silva, Cleiton and Figueiredo, Eduardo},
title = {Testing Tools for Configurable Software Systems: A Review-based Empirical Study},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302344},
doi = {10.1145/3302333.3302344},
abstract = {Configurable software systems are software systems that can be adapted or configured according to a set of features with the goal of increasing reuse and productivity. However, testing configurable systems is very challenging due to the number of configurations to run with each test, leading to a combinatorial explosion in the number of configurations and tests. Currently, several testing techniques and tools have been proposed to deal with this challenge, but their potential practical application remains mostly unexplored. The lack of studies to explore the tools that apply those techniques motivated us to investigate the literature to find testing tools for configurable software systems and to understand how they work. In this paper, we conducted a systematic mapping and identified 34 testing tools for configurable software systems. We first summarized and discussed their main characteristics. We then designed and performed a comparative empirical study of the main sound testing tools found: VarexJ and SPLat. They are considered sound testing techniques because they explore all reachable configurations from a given test. Overall, we observed that VarexJ and SPLat presented distinct results for efficiency while testing the target systems and that, although VarexJ found more errors than SPLat for the majority of the target systems, such result deserves a more in-depth investigation because we expected a higher intersection of errors encountered by them.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {10},
keywords = {Testing Configurable Software Systems, Systematic Mapping Study, Software Product Line},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1007/978-3-030-87589-3_19,
author = {Yan, Yutong and Conze, Pierre-Henri and Lamard, Mathieu and Zhang, Heng and Quellec, Gwenol\'{e} and Cochener, B\'{e}atrice and Coatrieux, Gouenou},
title = {Deep Active Learning for Dual-View Mammogram Analysis},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_19},
doi = {10.1007/978-3-030-87589-3_19},
abstract = {Supervised deep learning on medical imaging requires massive manual annotations, which are expertise-needed and time-consuming to perform. Active learning aims at reducing annotation efforts by adaptively selecting the most informative samples for labeling. We propose in this paper a novel deep active learning approach for dual-view mammogram analysis, especially for breast mass segmentation and detection, where the necessity of labeling is estimated by exploiting the consistency of predictions arising from craniocaudal (CC) and mediolateral-oblique (MLO) views. Intuitively, if mass segmentation or detection is robustly performed, prediction results achieved on CC and MLO views should be consistent. Exploiting the inter-view consistency is hence a good way to guide the sampling mechanism which iteratively selects the next image pairs to be labeled by an oracle. Experiments on public DDSM-CBIS and INbreast datasets demonstrate that comparable performance with respect to fully-supervised models can be reached using only 6.83% (9.56%) of labeled data for segmentation (detection). This suggests that combining dual-view mammogram analysis and active learning can strongly contribute to the development of computer-aided diagnosis systems.},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {180–189},
numpages = {10},
keywords = {Computer-aided diagnosis, Active learning, Dual-view mammogram analysis, Mass detection, Mass segmentation, Breast cancer},
location = {Strasbourg, France}
}

@article{10.1016/j.engappai.2021.104473,
author = {Liu, Ze-yu and Liu, Jian-wei and Zuo, Xin and Hu, Ming-fei},
title = {Multi-scale iterative refinement network for RGB-D salient object detection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2021.104473},
doi = {10.1016/j.engappai.2021.104473},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
numpages = {16},
keywords = {Multi-scale refinement, RGB-D image, Salient object detection}
}

@inproceedings{10.1145/3447545.3451177,
author = {Canales, Felipe and Hecht, Geoffrey and Bergel, Alexandre},
title = {Optimization of Java Virtual Machine Flags using Feature Model and Genetic Algorithm},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451177},
doi = {10.1145/3447545.3451177},
abstract = {Optimizing the Java Virtual Machine (JVM) options in order to get the best performance out of a program for production is a challenging and time-consuming task. HotSpot, the Oracle's open-source Java VM implementation offers more than 500 options, called flags, that can be used to tune the JVM's compiler, garbage collector (GC), heap size and much more. In addition to being numerous, these flags are sometimes poorly documented and create a need of benchmarking to ensure that the flags and their associated values deliver the best performance and stability for a particular program to execute.Auto-tuning approaches have already been proposed in order to mitigate this burden. However, in spite of increasingly sophisticated search techniques allowing for powerful optimizations, these approaches take little account of the underlying complexities of JVM flags. Indeed, dependencies and incompatibilities between flags are non-trivial to express, which if not taken into account may lead to invalid or spurious flag configurations that should not be considered by the auto-tuner.In this paper, we propose a novel model, inspired by the feature model used in Software Product Line, which takes the complexity of JVM's flags into account. We then demonstrate the usefulness of this model, using it as an input of a Genetic Algorithm (GA) to optimize the execution times of DaCapo Benchmarks.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {183–186},
numpages = {4},
keywords = {optimization, java virtual machine, genetic algorithm, feature model, auto-tuning},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1007/978-3-030-93046-2_7,
author = {Guo, Jingwen and Lu, Zhisheng and Wang, Ti and Huang, Weibo and Liu, Hong},
title = {Object Goal Visual Navigation Using Semantic Spatial Relationships},
year = {2021},
isbn = {978-3-030-93045-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-93046-2_7},
doi = {10.1007/978-3-030-93046-2_7},
abstract = {The target-driven visual navigation is a popular learning-based method and has been successfully applied to a wide range of applications. However, it has some disadvantages, including being ineffective at adapting to unseen environments. In this paper, a navigation method based on Semantic Spatial Relationships (SSR) is proposed and is shown to have more reliable performance when dealing with novel conditions. The construction of joint semantic hierarchical feature vector allows for learning implicit relationship between current observation and target objects, which benefits from construction of prior knowledge graph and semantic space. This differs from the traditional target driven methods, which integrate the visual input vector directly into the reinforcement learning path planning module. Moreover, the proposed method takes both local and global features of observed image into consideration and is thus less conservative and more robust in regards to random scenes. An additional analysis indicates that the proposed SSR performs well on classical metrics. The effectiveness of the proposed SSR model is demonstrated comparing with state-of-the-art methods in unknown scenes.},
booktitle = {Artificial Intelligence: First CAAI International Conference, CICAI 2021, Hangzhou, China, June 5–6, 2021, Proceedings, Part I},
pages = {77–88},
numpages = {12},
keywords = {Hierarchical relationship, Semantic graph, Visual navigation},
location = {Hangzhou, China}
}

@article{10.5555/3455716.3455897,
author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
title = {Curriculum learning for reinforcement learning domains: a framework and survey},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {181},
numpages = {50},
keywords = {transfer learning, reinforcement learning, curriculum learning}
}

@article{10.1016/j.infsof.2017.01.012,
author = {Reinhartz-Berger, Iris and Figl, Kathrin and Haugen, ystein},
title = {Investigating styles in variability modeling},
year = {2017},
issue_date = {July 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {87},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.01.012},
doi = {10.1016/j.infsof.2017.01.012},
abstract = {ContextA common way to represent product lines is with variability modeling. Yet, there are different ways to extract and organize relevant characteristics of variability. Comprehensibility of these models and the ease of creating models are important for the efficiency of any variability management approach. ObjectiveThe goal of this paper is to investigate the comprehensibility of two common styles to organize variability into models hierarchical and constrained where the dependencies between choices are specified either through the hierarchy of the model or as cross-cutting constraints, respectively. MethodWe conducted a controlled experiment with a sample of 90 participants who were students with prior training in modeling. Each participant was provided with two variability models specified in Common Variability Language (CVL) and was asked to answer questions requiring interpretation of provided models. The models included 920 nodes and 819 edges and used the main variability elements. After answering the questions, the participants were asked to create a model based on a textual description. ResultsThe results indicate that the hierarchical modeling style was easier to comprehend from a subjective point of view, but there was also a significant interaction effect with the degree of dependency in the models, that influenced objective comprehension. With respect to model creation, we found that the use of a constrained modeling style resulted in higher correctness of variability models. ConclusionsPrior exposure to modeling style and the degree of dependency among elements in the model determine what modeling style a participant chose when creating the model from natural language descriptions. Participants tended to choose a hierarchical style for modeling situations with high dependency and a constrained style for situations with low dependency. Furthermore, the degree of dependency also influences the comprehension of the variability model.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {81–102},
numpages = {22},
keywords = {Variability modeling, Textual constraints, Product line engineering, Hierarchical modeling, Feature modeling, Empirical research, Comprehensibility, Cognitive aspects}
}

@inproceedings{10.3115/981923.981932,
author = {Crout, J. Norwood},
title = {Artificial Intelligence Corporation},
year = {1981},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/981923.981932},
doi = {10.3115/981923.981932},
booktitle = {Proceedings of the 19th Annual Meeting on Association for Computational Linguistics},
pages = {31},
numpages = {1},
location = {Stanford, California},
series = {ACL '81}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Systematic mapping study, Software product line, Search based software engineering, Metaheuristics, Evolutionary algorithm}
}

@inproceedings{10.1145/3397481.3450678,
author = {Reyes, Guillermo and Alles, Alexandra},
title = {Multi-modal Multi-scale Attention Guidance in Cyber-Physical Environments},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397481.3450678},
doi = {10.1145/3397481.3450678},
abstract = {This work proposes a new method for guiding a user’s attention towards objects of interest in a cyber-physical environment (CPE). CPEs are environments that contain several computing systems that interact with each other and with the physical world. These environments contain several sensors (cameras, eye trackers, etc.) and output devices (lamps, screens, speakers, etc.). These devices can be used to first track the user’s position, orientation, and focus of attention to then find the most suitable output device to guide the user’s attention towards a target object. We argue that the most suitable device in this context is the one that attracts attention closest to the target and is salient enough to capture the user’s attention. The method is implemented as a function which estimates the ”closeness” and ”salience” of each visual and auditive output device in the environment. Some parameters of this method are then evaluated through a user study in the context of a virtual reality supermarket. The results show that multi-modal guidance can lead to better guiding performance. However, this depends on the set parameters.},
booktitle = {Proceedings of the 26th International Conference on Intelligent User Interfaces},
pages = {356–365},
numpages = {10},
keywords = {Attention, Attention Guidance, Cyber-Physical Environments, Intelligent Environments, Multi-modal, Multi-scale},
location = {College Station, TX, USA},
series = {IUI '21}
}

@article{10.1016/j.infsof.2015.11.004,
author = {Heradio, Ruben and Perez-Morago, Hector and Fernandez-Amoros, David and Javier Cabrerizo, Francisco and Herrera-Viedma, Enrique},
title = {A bibliometric analysis of 20 years of research on software product lines},
year = {2016},
issue_date = {April 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {72},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.11.004},
doi = {10.1016/j.infsof.2015.11.004},
abstract = {Context: Software product line engineering has proven to be an efficient paradigm to developing families of similar software systems at lower costs, in shorter time, and with higher quality.Objective: This paper analyzes the literature on product lines from 1995 to 2014, identifying the most influential publications, the most researched topics, and how the interest in those topics has evolved along the way.Method: Bibliographic data have been gathered from ISI Web of Science and Scopus. The data have been examined using two prominent bibliometric approaches: science mapping and performance analysis.Results: According to the study carried out, (i) software architecture was the initial motor of research in SPL; (ii) work on systematic software reuse has been essential for the development of the area; and (iii) feature modeling has been the most important topic for the last fifteen years, having the best evolution behavior in terms of number of published papers and received citations.Conclusion: Science mapping has been used to identify the main researched topics, the evolution of the interest in those topics and the relationships among topics. Performance analysis has been used to recognize the most influential papers, the journals and conferences that have published most papers, how numerous is the literature on product lines and what is its distribution over time.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {1–15},
numpages = {15},
keywords = {Software product lines, Science mapping, Performance analysis, Bibliometrics}
}

@article{10.1016/j.neucom.2019.11.001,
author = {Li, Huafeng and Zhou, Weiyan and Yu, Zhengtao and Yang, Biao and Jin, Huaiping},
title = {Person re-identification with dictionary learning regularized by stretching regularization and label consistency constraint},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {379},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.001},
doi = {10.1016/j.neucom.2019.11.001},
journal = {Neurocomput.},
month = feb,
pages = {356–369},
numpages = {14},
keywords = {Stretch regularization, Label consistency constraint, Dictionary learning, Person re-identification}
}

@article{10.5555/2845848.2845852,
author = {Kohli, Rajeev and Sukumar, R.},
title = {Heuristics for Product-Line Design Using Conjoint Analysis},
year = {1990},
issue_date = {December 1990},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {36},
number = {12},
issn = {0025-1909},
abstract = {Recently proposed methods for product-line selection use the total utilities of candidate items to construct product lines maximizing seller's return or buyers' welfare. For conjoint hybrid conjoint data, enumerating the utilities of candidate items can be computationally infeasible if the number of attributes and attribute levels is large and most multi-attribute alternatives are feasible. For such problems, constructing product lines directly from part-worths data is preferable. We propose such methods, extending Kohli and Krishnamurti's 1987 dynamic-programming heuristic for selecting a single item maximizing share to structure product lines maximizing share, seller's return, or buyers' utilitarian welfare. The computational performance of the heuristics and their approximation of product-line solutions is evaluated using simulated data. Across problem instances, the dynamic-programming heuristics identify solutions that are no worse, in terms of approximating optimal solutions, to the solutions of heuristics for the current two-step approaches to product-line design. An application using hybrid-conjoint data for a consumer-durable product is described.},
journal = {Manage. Sci.},
month = dec,
pages = {1464–1478},
numpages = {15},
keywords = {programming: heuristics, marketing: product policy, conjoint analysis}
}

@article{10.1007/s10515-010-0066-8,
author = {Apel, Sven and K\"{a}stner, Christian and Gr\"{o}βlinger, Armin and Lengauer, Christian},
title = {Type safety for feature-oriented product lines},
year = {2010},
issue_date = {September 2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0066-8},
doi = {10.1007/s10515-010-0066-8},
abstract = {A feature-oriented product line is a family of programs that share a common set of features. A feature implements a stakeholder's requirement and represents a design decision or configuration option. When added to a program, a feature involves the introduction of new structures, such as classes and methods, and the refinement of existing ones, such as extending methods. A feature-oriented decomposition enables a generator to create an executable program by composing feature code solely on the basis of the feature selection of a user--no other information needed. A key challenge of product line engineering is to guarantee that only well-typed programs are generated. As the number of valid feature combinations grows combinatorially with the number of features, it is not feasible to type check all programs individually. The only feasible approach is to have a type system check the entire code base of the feature-oriented product line. We have developed such a type system on the basis of a formal model of a feature-oriented Java-like language. The type system guaranties type safety for feature-oriented product lines. That is, it ensures that every valid program of a well-typed product line is well-typed. Our formal model including type system is sound and complete.},
journal = {Automated Software Engg.},
month = sep,
pages = {251–300},
numpages = {50},
keywords = {Type systems, Software product lines, Safe composition, Feature-oriented programming, Feature featherweight Java}
}

@inproceedings{10.1609/aaai.v33i01.33019005,
author = {Wu, Xiang and Huang, Huaibo and Patel, Vishal M. and He, Ran and Sun, Zhenan},
title = {Disentangled variational representation for heterogeneous face recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33019005},
doi = {10.1609/aaai.v33i01.33019005},
abstract = {Visible (VIS) to near infrared (NIR) face matching is a challenging problem due to the significant domain discrepancy between the domains and a lack of sufficient data for training cross-modal matching algorithms. Existing approaches attempt to tackle this problem by either synthesizing visible faces from NIR faces, extracting domain-invariant features from these modalities, or projecting heterogeneous data onto a common latent space for cross-modal matching. In this paper, we take a different approach in which we make use of the Disentangled Variational Representation (DVR) for cross-modal matching. First, we model a face representation with an intrinsic identity information and its within-person variations. By exploring the disentangled latent variable space, a variational lower bound is employed to optimize the approximate posterior for NIR and VIS representations. Second, aiming at obtaining more compact and discriminative disentangled latent space, we impose a minimization of the identity information for the same subject and a relaxed correlation alignment constraint between the NIR and VIS modality variations. An alternative optimization scheme is proposed for the disentangled variational representation part and the heterogeneous face recognition network part. The mutual promotion between these two parts effectively reduces the NIR and VIS domain discrepancy and alleviates over-fitting. Extensive experiments on three challenging NIR-VIS heterogeneous face recognition databases demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1105},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.5555/2871477.2871480,
author = {Dobson, Gregory and Kalish, Shlomo},
title = {Heuristics for Pricing and Positioning a Product-Line Using Conjoint and Cost Data},
year = {1993},
issue_date = {February 1993},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {39},
number = {2},
issn = {0025-1909},
abstract = {Designing and pricing a product-line is the very essence of every business. In recent years quantitative methods to assist managers in this task have been gaining in popularity. Conjoint analysis is already widely used to measure preferences for different product profiles, and build market simulation models. In the last few years several papers have been published that suggest how to optimally choose a product-line based on such data.We formalize this problem as a mathematical program where the objective of the firm is either profit or total welfare. Unlike alternative published approaches, we introduce fixed and variable costs for each product profile. The number of products to be introduced is endogenously determined on the basis of their desirability, fixed and variable costs, and in the case of profits, their cannibalization effect on other products. While the problem is difficult NP-complete, we show that the maximum welfare problem is equivalent to the uncapacitated plant location problem, which can be solved very efficiently using the greedy interchange heuristic. Based on past published experience with this problem, and on simulations we perform, we show that optimal or near optimal solutions are obtained in seconds for large problems. We develop a new greedy heuristic for the profit problem, and its application to simulated problems shows that it too runs quickly, and with better performance than various alternatives and previously published heuristics. We also show how the methodology can be applied, taking existing products of both the firm and the competition into account.},
journal = {Manage. Sci.},
month = feb,
pages = {160–175},
numpages = {16},
keywords = {product-line design, product positioning, pricing, heuristics, conjoint analysis}
}

@inproceedings{10.5555/3524938.3525304,
author = {Gupta, Pankaj and Chaudhary, Yatin and Runkler, Thomas and Sch\"{u}tze, Hinrich},
title = {Neural topic modeling with continual lifelong learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel selective data augmentation, co-training and topic regularization approaches. Given a stream of document collections, we apply the proposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three sparse document collections as future tasks and demonstrate improved performance quantified by perplexity, topic coherence and information retrieval task.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {366},
numpages = {11},
series = {ICML'20}
}

@article{10.1007/s10515-010-0076-6,
author = {Dhungana, Deepak and Gr\"{u}nbacher, Paul and Rabiser, Rick},
title = {The DOPLER meta-tool for decision-oriented variability modeling: a multiple case study},
year = {2011},
issue_date = {March     2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0076-6},
doi = {10.1007/s10515-010-0076-6},
abstract = {The variability of a product line is typically defined in models. However, many existing variability modeling approaches are rigid and don't allow sufficient domain-specific adaptations. We have thus been developing a flexible and extensible approach for defining product line variability models. Its main purposes are to guide stakeholders through product derivation and to automatically generate product configurations. Our approach is supported by the DOPLER (  D ecision-  O riented  P roduct  L ine  E ngineering for effective  R euse) meta-tool that allows modelers to specify the types of reusable assets, their attributes, and dependencies for their specific system and context. The aim of this paper is to investigate the suitability of our approach for different domains. More specifically, we explored two research questions regarding the implementation of variability and the utility of DOPLER for variability modeling in different domains. We conducted a multiple case study consisting of four cases in the domains of industrial automation systems and business software. In each of these case studies we analyzed variability implementation techniques. Experts from our industry partners then developed domain-specific meta-models, tool extensions, and variability models for their product lines using DOPLER. The four cases demonstrate the flexibility of the DOPLER approach and the extensibility and adaptability of the supporting meta tool.},
journal = {Automated Software Engg.},
month = mar,
pages = {77–114},
numpages = {38},
keywords = {Product line engineering, Meta-tools, Decision models}
}

@article{10.1007/s11042-020-09956-6,
author = {Yadav, Hitesh and Chhikara, Rita and Kumari, A. Charan},
title = {A novel hybrid approach for feature selection in software product lines},
year = {2021},
issue_date = {Feb 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {4},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09956-6},
doi = {10.1007/s11042-020-09956-6},
abstract = {Software Product Line (SPL) customizes software by combining various existing features of the software with multiple variants. The main challenge is selecting valid features considering the constraints of the feature model. To solve this challenge, a hybrid approach is proposed to optimize the feature selection problem in software product lines. The Hybrid approach ‘Hyper-PSOBBO’ is a combination of Particle Swarm Optimization (PSO), Biogeography-Based Optimization (BBO) and hyper-heuristic algorithms. The proposed algorithm has been compared with Bird Swarm Algorithm (BSA), PSO, BBO, Firefly, Genetic Algorithm (GA) and Hyper-heuristic. All these algorithms are performed in a set of 10 feature models that vary from a small set of 100 to a high-quality data set of 5000. The detailed empirical analysis in terms of performance has been carried out on these feature models. The results of the study indicate that the performance of the proposed method is higher to other state-of-the-art algorithms.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {4919–4942},
numpages = {24},
keywords = {Feature model (FM), Software product lines (SPL), Bird swarm optimization (BSA), Genetic algorithm (GA), Firefly, Biogeography-based optimization, Hyper-heuristic, Particle swarm optimization}
}

@inproceedings{10.5555/3367243.3367442,
author = {Li, Longyuan and Yan, Junchi and Yang, Xiaokang and Jin, Yaohui},
title = {Learning interpretable deep state space model for probabilistic time series forecasting},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Probabilistic time series forecasting involves estimating the distribution of future based on its history, which is essential for risk management in downstream decision-making. We propose a deep state space model for probabilistic time series forecasting whereby the non-linear emission model and transition model are parameterized by networks and the dependency is modeled by recurrent neural nets. We take the automatic relevance determination (ARD) view and devise a network to exploit the exogenous variables in addition to time series. In particular, our ARD network can incorporate the uncertainty of the exogenous variables and eventually helps identify useful exogenous variables and suppress those irrelevant for forecasting. The distribution of multi-step ahead forecasts are approximated by Monte Carlo simulation. We show in experiments that our model produces accurate and sharp probabilistic forecasts. The estimated uncertainty of our forecasting also realistically increases over time, in a spontaneous manner.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {2901–2908},
numpages = {8},
location = {Macao, China},
series = {IJCAI'19}
}

@article{10.1145/3464939,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat},
title = {Recommending Faulty Configurations for Interacting Systems Under Test Using Multi-objective Search},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3464939},
doi = {10.1145/3464939},
abstract = {Modern systems, such as cyber-physical systems, often consist of multiple products within/across product lines communicating with each other through information networks. Consequently, their runtime behaviors are influenced by product configurations and networks. Such systems play a vital role in our daily life; thus, ensuring their correctness by thorough testing becomes essential. However, testing these systems is particularly challenging due to a large number of possible configurations and limited available resources. Therefore, it is important and practically useful to test these systems with specific configurations under which products will most likely fail to communicate with each other. Motivated by this, we present a search-based configuration recommendation (SBCR) approach to recommend faulty configurations for the system under test (SUT) based on cross-product line (CPL) rules. CPL rules are soft constraints, constraining product configurations while indicating the most probable system states with a certain degree of confidence. In SBCR, we defined four search objectives based on CPL rules and combined them with six commonly applied search algorithms. To evaluate SBCR (i.e., SBCRNSGA-II, SBCRIBEA, SBCRMoCell, SBCRSPEA2, SBCRPAES, and SBCRSMPSO), we performed two case studies (Cisco and Jitsi) and conducted difference analyses. Results show that for both of the case studies, SBCR significantly outperformed random search-based configuration recommendation (RBCR) for 86% of the total comparisons based on six quality indicators, and 100% of the total comparisons based on the percentage of faulty configurations (PFC). Among the six variants of SBCR, SBCRSPEA2 outperformed the others in 85% of the total comparisons based on six quality indicators and 100% of the total comparisons based on PFC.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {53},
numpages = {36},
keywords = {testing, multi-objective search, mined rules, interacting products, configuration recommendation, Product line}
}

@inproceedings{10.5555/3463952.3463958,
author = {Mey, Alexander and Oliehoek, Frans A.},
title = {Environment Shift Games: Are Multiple Agents the Solution, and not the Problem, to Non-Stationarity?},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Machine learning and artificial intelligence models that interact with and in an environment will unavoidably have impact on this environment and change it. This is often a problem as many methods do not anticipate such a change in the environment and thus may start acting sub-optimally. Although efforts are made to deal with this problem, we believe that a lot of potential is unused. Driven by the recent success of predictive machine learning, we believe that in many scenarios one can predict when and how a change in the environment will occur. In this paper we introduce a blueprint that intimately connects this idea to the multiagent setting, showing that the multiagent community has a pivotal role to play in addressing the challenging problem of changing environments.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {23–27},
numpages = {5},
keywords = {non-stationarity, sequential decision making},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@article{10.1287/opre.2019.1928,
author = {Mi\v{s}i\'{c}, Velibor V.},
title = {Optimization of Tree Ensembles},
year = {2020},
issue_date = {September-October 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {68},
number = {5},
issn = {0030-364X},
url = {https://doi.org/10.1287/opre.2019.1928},
doi = {10.1287/opre.2019.1928},
abstract = {From Tree Ensemble Models to DecisionsPredictive models based on ensembles of trees, such as random forests and gradient boosted trees, are widely used in machine learning and data science. In many applications, the features that these models use are controllable and can be regarded as decision variables. This leads to a natural prescriptive analytics problem: how should these features be set, so as to maximize the value predicted by the tree ensemble model? In “Optimization of Tree Ensembles” Velibor V. Mi\v{s}i\'{c} proposes a MIO model of this problem, proposes a hierarchy of approximations to this formulation based on truncating the trees at a particular depth, and develops two specialized constraint generation methods for solving the problem at scale. Using real data sets, including two detailed case studies in drug design and customized pricing, the author shows how this approach can efficiently solve large-scale problem instances to full or near optimality and outperforms solutions obtained by heuristic approaches.Tree ensemble models such as random forests and boosted trees are among the most widely used and practically successful predictive models in applied machine learning and business analytics. Although such models have been used to make predictions based on exogenous, uncontrollable independent variables, they are increasingly being used to make predictions where the independent variables are controllable and are also decision variables. In this paper, we study the problem of tree ensemble optimization: given a tree ensemble that predicts some dependent variable using controllable independent variables, how should we set these variables so as to maximize the predicted value? We formulate the problem as a mixed-integer optimization problem. We theoretically examine the strength of our formulation, provide a hierarchy of approximate formulations with bounds on approximation quality and exploit the structure of the problem to develop two large-scale solution methods, one based on Benders decomposition and one based on iteratively generating tree split constraints. We test our methodology on real data sets, including two case studies in drug design and customized pricing, and show that our methodology can efficiently solve large-scale instances to near or full optimality, and outperforms solutions obtained by heuristic approaches.},
journal = {Oper. Res.},
month = sep,
pages = {1605–1624},
numpages = {20},
keywords = {customized pricing, drug design, mixed-integer optimization, random forests, tree ensembles, Optimization, statistics, applications: integer: programming}
}

@inproceedings{10.1007/978-3-030-58577-8_17,
author = {Pan, Lili and Ai, Shijie and Ren, Yazhou and Xu, Zenglin},
title = {Self-Paced Deep Regression Forests with Consideration on Underrepresented Examples},
year = {2020},
isbn = {978-3-030-58576-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58577-8_17},
doi = {10.1007/978-3-030-58577-8_17},
abstract = {Deep discriminative models (e.g.&nbsp;deep regression forests, deep neural decision forests) have achieved remarkable success recently to solve problems such as facial age estimation and head pose estimation. Most existing methods pursue robust and unbiased solutions either through learning discriminative features, or reweighting samples. We argue what is more desirable is learning gradually to discriminate like our human beings, and hence we resort to self-paced learning (SPL). Then, a natural question arises: can self-paced regime lead deep discriminative models to achieve more robust and less biased solutions? To this end, this paper proposes a new deep discriminative model—self-paced deep regression forests with consideration on underrepresented examples (SPUDRFs). It tackles the fundamental ranking and selecting problem in SPL from a new perspective: fairness. This paradigm is fundamental and could be easily combined with a variety of deep discriminative models (DDMs). Extensive experiments on two computer vision tasks, i.e., facial age estimation and head pose estimation, demonstrate the efficacy of SPUDRFs, where state-of-the-art performances are achieved.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX},
pages = {271–287},
numpages = {17},
keywords = {Underrepresented examples, Self-paced learning, Entropy, Deep regression forests},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.infsof.2012.02.002,
author = {Holl, Gerald and Gr\"{u}nbacher, Paul and Rabiser, Rick},
title = {A systematic review and an expert survey on capabilities supporting multi product lines},
year = {2012},
issue_date = {August, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.02.002},
doi = {10.1016/j.infsof.2012.02.002},
abstract = {Context: Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented. Objective: The aims of this paper are thus to identify, describe, and classify existing approaches supporting MPLs and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs. Method: Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting MPLs from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry. Results: The paper discusses key research issues in MPLs and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized. Conclusions: We conclude that approaches supporting MPLs need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate MPL approaches.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {828–852},
numpages = {25},
keywords = {Systematic literature review, Product line engineering, Multi product lines, Large-scale systems}
}

@inproceedings{10.5555/3367471.3367606,
author = {Yang, Liang and Chen, Zhiyang and Gu, Junhua and Guo, Yuanfang},
title = {Dual self-paced graph convolutional network: towards reducing attribute distortions induced by topology},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {The success of graph convolutional neural networks (GCNNs) based semi-supervised node classification is credited to the attribute smoothing (propagating) over the topology. However, the attributes may be interfered by the utilization of the topology information. This distortion will induce a certain amount of misclassifications of the nodes, which can be correctly predicted with only the attributes. By analyzing the impact of the edges in attribute propagations, the simple edges, which connect two nodes with similar attributes, should be given priority during the training process compared to the complex ones according to curriculum learning. To reduce the distortions induced by the topology while exploit more potentials of the attribute information, Dual Self-Paced Graph Convolutional Network (DSP-GCN) is proposed in this paper. Specifically, the unlabelled nodes with confidently predicted labels are gradually added into the training set in the node-level self-paced learning, while edges are gradually, from the simple edges to the complex ones, added into the graph during the training process in the edge-level self-paced learning. These two learning strategies are designed to mutually reinforce each other by coupling the selections of the edges and unlabelled nodes. Experimental results of transductive semi-supervised node classification on many real networks indicate that the proposed DSP-GCN has successfully reduced the attribute distortions induced by the topology while it gives superior performances with only one graph convolutional layer.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {4062–4069},
numpages = {8},
location = {Macao, China},
series = {IJCAI'19}
}

@article{10.1007/s10994-021-05965-0,
author = {Giffon, Luc and Emiya, Valentin and Kadri, Hachem and Ralaivola, Liva},
title = {QuicK-means: accelerating inference for K-means by learning fast transforms},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {5},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-021-05965-0},
doi = {10.1007/s10994-021-05965-0},
abstract = {K-means—and the celebrated Lloyd’s algorithm—is more than the clustering method it was originally designed to be. It has indeed proven pivotal to help increase the speed of many machine learning, data analysis techniques such as indexing, nearest-neighbor search and prediction, data compression and, lately, inference with kernel machines. Here, we introduce an efficient extension of K-means, dubbed QuicK-means, that rests on the idea of expressing the matrix of the K cluster centroids as a product of sparse matrices, a feat made possible by recent results devoted to find approximations of matrices as a product of sparse factors. Using such a decomposition squashes the complexity of the matrix-vector product between the factorized K\texttimes{}D centroid matrix U and any vector from OKD to OAlogB+B, with A=minK,D and B=maxK,D, where D is the dimension of the data. This drastic computational saving has a direct impact in the assignment process of a point to a cluster. We propose to learn such a factorization during the Lloyd’s training procedure. We show that resorting to a factorization step at each iteration does not impair the convergence of the optimization scheme, and demonstrate the benefits of our approach experimentally.},
journal = {Mach. Learn.},
month = may,
pages = {881–905},
numpages = {25},
keywords = {Machine learning, Fast transforms, Clustering, k-means}
}

@inproceedings{10.1109/ASE.2013.6693103,
author = {Pohl, Richard and Stricker, Vanessa and Pohl, Klaus},
title = {Measuring the structural complexity of feature models},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693103},
doi = {10.1109/ASE.2013.6693103},
abstract = {The automated analysis of feature models (FM) is based on SAT, BDD, and CSP - known NP-complete problems. Therefore, the analysis could have an exponential worst-case execution time. However, for many practical relevant analysis cases, state-of-the-art (SOTA) analysis tools quite successfully master the problem of exponential worst-case execution time based on heuristics. So far, however, very little is known about the structure of FMs that cause the cases in which the execution time (hardness) for analyzing a given FM increases unpredictably for SOTA analysis tools. In this paper, we propose to use width measures from graph theory to characterize the structural complexity of FMs as a basis for an estimation of the hardness of analysis operations on FMs with SOTA analysis tools. We present an experiment that we use to analyze the reasonability of graph width measures as metric for the structural complexity of FMs and the hardness of FM analysis. Such a complexity metric can be used as a basis for a unified method to systematically improve SOTA analysis tools.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {454–464},
numpages = {11},
keywords = {software product line, performance measurement, feature model, automated analysis},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@article{10.1016/j.procs.2017.08.206,
author = {Mani, Neel and Helfert, Markus and Pahl, Claus},
title = {A Domain-specific Rule Generation Using Model-Driven Architecture in Controlled Variability Model},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.08.206},
doi = {10.1016/j.procs.2017.08.206},
abstract = {The business environment changes rapidly and needs to adapt to the enterprise business systems must be considered for new types of requirements to accept changes in the business strategies and processes. This raises new challenges that the traditional development approaches cannot always provide a complete solution in an efficient way. However, most of the current proposals for automatic generation are not devised to cope with rapid integration of the changes in the business requirement of end user (stakeholders and customers) resource. Domain-specific Rules constitute a key element for domain specific enterprise application, allowing configuration of changes, and management of the domain constraint within a domain. In this paper, we propose an approach to the development of an automatic generation of the domain-specific rules by using variability feature model and ontology definition of domain model concepts coming from Software product line engineering and Model Driven Architecture. We provide a process approach to generate a domain-specific rule based on the end user requirement.},
journal = {Procedia Comput. Sci.},
month = sep,
pages = {2354–2362},
numpages = {9},
keywords = {Variability Model, Rule Generation, Model Driven Architecture, Domain-specific rules, Business Process Model}
}

@inproceedings{10.1007/978-3-030-86380-7_23,
author = {Krysi\'{n}ska, Izabela and Morzy, Miko\l{}aj and Kajdanowicz, Tomasz},
title = {Curriculum Learning Revisited: Incremental Batch Learning with Instance Typicality Ranking},
year = {2021},
isbn = {978-3-030-86379-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86380-7_23},
doi = {10.1007/978-3-030-86380-7_23},
abstract = {The technique of curriculum learning mimics cognitive mechanisms observed in human learning, where simpler concepts are presented prior to gradual introduction of more difficult concepts. Until now, the major obstacle for curriculum methods was the lack of a reliable method for estimating the difficulty of training instances. In this paper we show that, instead of trying to assess the difficulty of learning instances, a simple graph-based method of computing the typicality of instances can be used in conjunction with curriculum methods. We design new batch schedulers which organize ordered instances into batches of varying size and learning difficulty. Our method does not require any changes to the architecture of trained models, we improve the training merely by manipulating the order and frequency of instance presentation to the model.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part IV},
pages = {279–291},
numpages = {13},
keywords = {Batch training, Typicality, Curriculum learning},
location = {Bratislava, Slovakia}
}

@article{10.1287/mksc.4.1.1,
author = {Green, Paul E. and Krieger, Abba M.},
title = {Models and Heuristics for Product Line Selection},
year = {1985},
issue_date = {February 1985},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {4},
number = {1},
issn = {1526-548X},
url = {https://doi.org/10.1287/mksc.4.1.1},
doi = {10.1287/mksc.4.1.1},
abstract = {Recently, a number of articles have appeared in the marketing literature dealing with single product design optimization. The present paper represents a start toward extending this research to product line decisions. We first formulate two versions of the problem and describe their various characteristics. Since finding the optimum solution is computationally prohibitive, even for modest size problems, various heuristic approaches are presented and evaluated. Applications involving synthetic and real data are discussed. The paper concludes with comments on related problems and future application areas.},
journal = {Marketing Science},
month = feb,
pages = {1–19},
numpages = {19},
keywords = {product, optimization, heuristics}
}

@article{10.1016/j.neunet.2021.03.022,
author = {Zhong, Yongjian and Du, Bo and Xu, Chang},
title = {Learning to reweight examples in multi-label classification},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.03.022},
doi = {10.1016/j.neunet.2021.03.022},
journal = {Neural Netw.},
month = oct,
pages = {428–436},
numpages = {9},
keywords = {Reweight instance, Self-paced learning, Multi-label classification}
}

@article{10.1007/s00766-003-0166-0,
author = {Thompson, Jeffrey M. and Heimdahl, Mats P.},
title = {Structuring product family requirements for n-dimensional and hierarchical product lines},
year = {2003},
issue_date = {February  2003},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-003-0166-0},
doi = {10.1007/s00766-003-0166-0},
abstract = {The software product-line approach (for software product families) is one of the success stories of software reuse. When applied, it can result in cost savings and increases in productivity. In addition, in safety-critical systems the approach has the potential for reuse of analysis and testing results, which can lead to a safer system. Nevertheless, there are times when it seems like a product family approach should work when, in fact, there are difficulties in properly defining the boundaries of the product family. In this paper, we draw on our experiences in applying the software product-line approach to a family of mobile robots, a family of flight guidance systems, and a family of cardiac pacemakers, as well as case studies done by others to (1) illustrate how domain structure can currently limit applicability of product-line approaches to certain domains and (2) demonstrate our progress towards a solution using a set-theoretic approach to reason about domains of what we call n-dimensional and hierarchical product families.},
journal = {Requir. Eng.},
month = feb,
pages = {42–54},
numpages = {13},
keywords = {Requirements structuring, Requirements reuse, Product line modelling, Product line engineering, Domain Engineering}
}

@inproceedings{10.1007/978-3-030-65310-1_20,
author = {Metzger, Andreas and Quinton, Cl\'{e}ment and Mann, Zolt\'{a}n \'{A}d\'{a}m and Baresi, Luciano and Pohl, Klaus},
title = {Feature Model-Guided Online Reinforcement Learning for Self-Adaptive Services},
year = {2020},
isbn = {978-3-030-65309-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65310-1_20},
doi = {10.1007/978-3-030-65310-1_20},
abstract = {A self-adaptive service can maintain its QoS requirements in the presence of dynamic environment changes. To develop a self-adaptive service, service engineers have to create self-adaptation logic encoding when the service should execute which adaptation actions. However, developing self-adaptation logic may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. Online reinforcement learning addresses design time uncertainty by learning suitable adaptation actions through interactions with the environment at runtime. To learn more about its environment, reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens has an impact on the performance of the learning process. We focus on two problems related to how a service’s adaptation actions are explored: (1) Existing solutions randomly explore adaptation actions and thus may exhibit slow learning if there are many possible adaptation actions to choose from. (2) Existing solutions are unaware of service evolution, and thus may explore new adaptation actions introduced during such evolution rather late. We propose novel exploration strategies that use feature models (from software product line engineering) to guide exploration in the presence of many adaptation actions and in the presence of service evolution. Experimental results for a self-adaptive cloud management service indicate an average speed-up of the learning process of 58.8% in the presence of many adaptation actions, and of 61.3% in the presence of service evolution. The improved learning performance in turn led to an average QoS improvement of 7.8% and 23.7% respectively
.},
booktitle = {Service-Oriented Computing: 18th International Conference, ICSOC 2020, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {269–286},
numpages = {18},
keywords = {Cloud service, Feature model, Reinforcement learning, Adaptation},
location = {Dubai, United Arab Emirates}
}

@article{10.5555/2946645.3053434,
author = {Szab\'{o}, Zolt\'{a}n and Sriperumbudur, Bharath K. and P\'{o}czos, Barnab\'{a}s and Gretton, Arthur},
title = {Learning theory for distribution regression},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax optimal rate (Caponnetto and De Vito, 2007; Steinwart et al., 2009). This result answers a 17-year-old open question, establishing the consistency of the classical set kernel (Haussler, 1999; G\"{a}rtner et al., 2002) in regression. We also cover consistency for more recent kernels on distributions, including those due to Christmann and Steinwart (2010).},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5272–5311},
numpages = {40},
keywords = {two-Stage sampled distribution regression, multi-instance learning, minimax optimality, mean embedding, Kernel ridge regression}
}

@inproceedings{10.1145/3364641.3364656,
author = {Sousa, Amanda and Uch\^{o}a, Anderson and Fernandes, Eduardo and Bezerra, Carla I. M. and Monteiro, Jos\'{e} Maria and Andrade, Rossana M. C.},
title = {REM4DSPL: A Requirements Engineering Method for Dynamic Software Product Lines},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364656},
doi = {10.1145/3364641.3364656},
abstract = {Context: Dynamic Software Product Line (DSPL) is a set of software products capable of self-adapt and configure in runtime. DSPL products have common features (commonalities) and varying features (managed in runtime according to context changes). Objective: DSPL requirements engineering is challenging. Requirements engineers have to carefully plan self-adaptation while eliciting, modeling, and managing variability requirements. This paper introduces a method for DSPL requirements engineering. Method: We relied on empirically-derived activities of DSPL requirements engineering to build our method. We selected techniques and templates used in other domains such as SPL for refinement and incorporation into the method. We asked DSPL experts via a survey on the method applicability. Result: We introduced the Requirements Engineering Method for DSPL (REM4DSPL). Elicitation is guided by supervised discussions. Modeling relies on feature models. Variability Management is tool-assisted and validated via feature model inspection. DSPL experts agreed on the method applicability and suggested improvements. Conclusion: REM4DSPL relies on empirically-derived activities, techniques that have been successfully used by previous work, and templates adapted to the DSPL context. We expect our method to guide requirements engineers in practice.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {129–138},
numpages = {10},
keywords = {Requirements Engineering, Dynamic Software Product Lines},
location = {Fortaleza, Brazil},
series = {SBQS '19}
}

@inproceedings{10.1007/978-3-030-89363-7_28,
author = {Dai, Huan and Zhang, Yupei and Yun, Yue and Shang, Xuequn},
title = {An Improved Deep Model for Knowledge Tracing and Question-Difficulty Discovery},
year = {2021},
isbn = {978-3-030-89362-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89363-7_28},
doi = {10.1007/978-3-030-89363-7_28},
abstract = {Knowledge Tracing (KT) aims to analyze a student’s acquisition of skills over time by examining the student’s performance on questions of those skills. In recent years, a recurrent neural network model called deep knowledge tracing (DKT) has been proposed to handle the knowledge tracing task and literature has shown that DKT generally outperforms traditional methods. However, DKT and its variants often lead to oscillation results on a skill’s state may due to it ignoring the skill’s difficulty or the question’s difficulty. As a result, even when a student performs well on a skill, the prediction of that skill’s mastery level decreases instead, and vice versa. This is undesirable and unreasonable because student’s performance is expected to transit gradually over time. In this paper, we propose to learn the knowledge tracing model in a “simple-to-difficult” process, leading to a method of Self-paced Deep Knowledge Tracing (SPDKT). SPDKT learns the difficulty of per question from the student’s responses to optimize the question’s order and smooth the learning process. With mitigating the cause of oscillations, SPDKT has the capability of robustness to the puzzling questions. The experiments on real-world datasets show SPDKT achieves state-of-the-art performance on question response prediction and reaches interesting interpretations in education.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part II},
pages = {362–375},
numpages = {14},
keywords = {Personalized education, Deep learning, Self-paced learning, Knowledge tracing},
location = {Hanoi, Vietnam}
}

@article{10.1016/j.ijar.2007.03.006,
author = {Peterson, Leif E. and Coleman, Matthew A.},
title = {Machine learning-based receiver operating characteristic (ROC) curves for crisp and fuzzy classification of DNA microarrays in cancer research},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {47},
number = {1},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2007.03.006},
doi = {10.1016/j.ijar.2007.03.006},
abstract = {Receiver operating characteristic (ROC) curves were generated to obtain classification area under the curve (AUC) as a function of feature standardization, fuzzification, and sample size from nine large sets of cancer-related DNA microarrays. Classifiers used included k-nearest neighbor (kNN), naive Bayes classifier (NBC), linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), learning vector quantization (LVQ1), logistic regression (LOG), polytomous logistic regression (PLOG), artificial neural networks (ANN), particle swarm optimization (PSO), constricted particle swarm optimization (CPSO), kernel regression (RBF), radial basis function networks (RBFN), gradient descent support vector machines (SVMGD), and least squares support vector machines (SVMLS). For each data set, AUC was determined for a number of combinations of sample size, total sum[-log(p)] of feature t-tests, with and without feature standardization and with (fuzzy) and without (crisp) fuzzification of features. Altogether, a total of 2,123,530 classification runs were made. At the greatest level of sample size, ANN resulted in a fitted AUC of 90%, while PSO resulted in the lowest fitted AUC of 72.1%. AUC values derived from 4NN were the most dependent on sample size, while PSO was the least. ANN depended the most on total statistical significance of features used based on sum[-log(p)], whereas PSO was the least dependent. Standardization of features increased AUC by 8.1% for PSO and -0.2% for QDA, while fuzzification increased AUC by 9.4% for PSO and reduced AUC by 3.8% for QDA. AUC determination in planned microarray experiments without standardization and fuzzification of features will benefit the most if CPSO is used for lower levels of feature significance (i.e., sum[-log(p)]~50) and ANN is used for greater levels of significance (i.e., sum[-log(p)]~500). When only standardization of features is performed, studies are likely to benefit most by using CPSO for low levels of feature statistical significance and LVQ1 for greater levels of significance. Studies involving only fuzzification of features should employ LVQ1 because of the substantial gain in AUC observed and low expense of LVQ1. Lastly, PSO resulted in significantly greater levels of AUC (89.5% average) when feature standardization and fuzzification were performed. In consideration of the data sets used and factors influencing AUC which were investigated, if low-expense computation is desired then LVQ1 is recommended. However, if computational expense is of less concern, then PSO or CPSO is recommended.},
journal = {Int. J. Approx. Reasoning},
month = jan,
pages = {17–36},
numpages = {20},
keywords = {Soft computing, Receiver operator characteristic (ROC) curve, Gene expression, Fuzzy classification, DNA microarrays, Area under the curve (AUC)}
}

@inproceedings{10.1007/978-3-642-41533-3_24,
author = {Gonz\'{a}lez-Huerta, Javier and Insfr\'{a}n, Emilio and Abrah\~{a}o, Silvia},
title = {Defining and Validating a Multimodel Approach for Product Architecture Derivation and Improvement},
year = {2013},
isbn = {9783642415326},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41533-3_24},
doi = {10.1007/978-3-642-41533-3_24},
abstract = {Software architectures are the key to achieving the non-functional requirements NFRs in any software project. In software product line SPL development, it is crucial to identify whether the NFRs for a specific product can be attained with the built-in architectural variation mechanisms of the product line architecture, or whether additional architectural transformations are required. This paper presents a multimodel approach for quality-driven product architecture derivation and improvement QuaDAI. A controlled experiment is also presented with the objective of comparing the effectiveness, efficiency, perceived ease of use, intention to use and perceived usefulness with regard to participants using QuaDAI as opposed to the Architecture Tradeoff Analysis Method ATAM. The results show that QuaDAI is more efficient and perceived as easier to use than ATAM, from the perspective of novice software architecture evaluators. However, the other variables were not found to be statistically significant. Further replications are needed to obtain more conclusive results.},
booktitle = {Proceedings of the 16th International Conference on Model-Driven Engineering Languages and Systems - Volume 8107},
pages = {388–404},
numpages = {17},
keywords = {Software Product Lines, Quality Attributes, Model Transformations, Controlled Experiment, Architectural Patterns}
}

@article{10.1145/1183236.1183239,
author = {Bichler, Martin and Kalagnanam, Jayant R.},
title = {Software frameworks for advanced procurement auction markets},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183239},
doi = {10.1145/1183236.1183239},
abstract = {A range of versatile auction formats are coming that allow more flexibility in specifying demand and supply.},
journal = {Commun. ACM},
month = dec,
pages = {104–108},
numpages = {5}
}

@inproceedings{10.1145/3451421.3451427,
author = {Liu, Xiaoli and Li, Jiali and Cao, Peng},
title = {SP-MTFL: A self paced multi-task feature learning method for cognitive performance predicting of Alzheimer's disease},
year = {2021},
isbn = {9781450389686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3451421.3451427},
doi = {10.1145/3451421.3451427},
abstract = {Machine learning algorithms and multivariate data analysis methods have been widely utilized in the field of Alzheimer's disease (AD) research in recent years. Predicting cognitive performance of subjects from neuroimage measures and identifying relevant imaging biomarkers are important research topics in the study of Alzheimer's disease. Multi-task based feature learning (MTFL) have been widely studied to select a discriminative feature subset from MRI features, and improve the performance by incorporating inherent correlations among multiple clinical cognitive measures. Inspired by the fact that humans often learn from easy concepts to hard ones in the cognitive process, we propose a self-paced multi-task feature learning framework that attempts to learn the tasks by simultaneously taking into consideration the complexities of both tasks and instances per task in this study. Experimental results on ADNI are provided, and the comparison results demonstrate the effectiveness of our approach and show that our approach outperforms the state-of-the-art methods.},
booktitle = {The Fourth International Symposium on Image Computing and Digital Medicine},
pages = {23–27},
numpages = {5},
keywords = {regression, multi-task learning, Self-paced learning, Machine learning, Alzheimer's disease},
location = {Shenyang, China},
series = {ISICDM 2020}
}

@article{10.5555/3455716.3455938,
author = {Weinshall, Daphna and Amir, Dan},
title = {Theory of curriculum learning, with convex loss functions},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Curriculum Learning is motivated by human cognition, where teaching often involves gradually exposing the learner to examples in a meaningful order, from easy to hard. Although methods based on this concept have been empirically shown to improve performance of several machine learning algorithms, no theoretical analysis has been provided even for simple cases. To address this shortfall, we start by formulating an ideal definition of difficulty score - the loss of the optimal hypothesis at a given datapoint. We analyze the possible contribution of curriculum learning based on this score in two convex problems - linear regression, and binary classification by hinge loss minimization. We show that in both cases, the convergence rate of SGD optimization decreases monotonically with the difficulty score, in accordance with earlier empirical results. We also prove that when the difficulty score is fixed, the convergence rate of SGD optimization is monotonically increasing with respect to the loss of the current hypothesis at each point. We discuss how these results settle some confusion in the literature where two apparently opposing heuristics are reported to improve performance: curriculum learning in which easier points are given priority, vs hard data mining where the more difficult points are sought out.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {222},
numpages = {19},
keywords = {hinge loss minimization, linear regression, curriculum learning}
}

@inproceedings{10.1007/978-3-030-32047-8_26,
author = {Khoshmanesh, Seyedehzahra and Lutz, Robyn R.},
title = {Leveraging Feature Similarity for Earlier Detection of Unwanted Feature Interactions in Evolving Software Product Lines},
year = {2019},
isbn = {978-3-030-32046-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32047-8_26},
doi = {10.1007/978-3-030-32047-8_26},
abstract = {Software product lines enable reuse of shared software across a family of products. As new products are built in the product line, new features are added. The features are units of functionality that provide services to users. Unwanted feature interactions, wherein one feature interferes with another feature’s operation, is a significant problem, especially as large software product lines evolve. Detecting feature interactions is a time-consuming and difficult task for developers. Moreover, feature interactions are often only discovered during testing, at which point costly re-work is needed. This paper proposes a similarity-based method to identify unwanted feature interactions much earlier in the development process. It uses knowledge of prior feature interactions stored with the software product line’s feature model to help find unwanted interactions between a new feature and existing features. The paper describes the framework and algorithms used to detect the feature interactions using three path similarity measures and evaluates the approach on a real-world, evolving software product line. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.},
booktitle = {Similarity Search and Applications: 12th International Conference, SISAP 2019, Newark, NJ, USA, October 2–4, 2019, Proceedings},
pages = {293–307},
numpages = {15},
keywords = {Feature interaction, Similarity measures, Software product lines},
location = {Newark, NJ, USA}
}

@article{10.1007/s10664-020-09911-x,
author = {Ramos-Guti\'{e}rrez, Bel\'{e}n and Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Discovering configuration workflows from existing logs using process mining},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09911-x},
doi = {10.1007/s10664-020-09911-x},
abstract = {Variability models are used to build configurators, for guiding users through the configuration process to reach the desired setting that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the design options that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suits stakeholders according to previous configurations. For example, when configuring a Linux distribution the configuration process starts by choosing the network or the graphic card and then, other packages concerning a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), a framework that can automatically assist determining the configuration workflow that better fits the configuration logs generated by user activities given a set of logs of previous configurations and a variability model. COLOSSI is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Derived from the possible complexity of both logs and the discovered processes, often, it is necessary to divide the traces into small ones. This provides an easier configuration workflow to be understood and followed by the user during the configuration process. In this paper, we apply and compare four different techniques for the traces clustering: greedy, backtracking, genetic and hierarchical algorithms. Our proposal is validated in three different scenarios, to show its feasibility, an ERP configuration, a Smart Farming, and a Computer Configuration. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering along with the necessity to apply clustering techniques for the trace preparation in the context of configuration workflows.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {41},
keywords = {Clustering, Process discovery, Process mining, Configuration workflow, Variability}
}

@inproceedings{10.5555/3466184.3466446,
author = {Rodriguez, Brodderick and Yilmaz, Levent},
title = {Learning rule-based explanatory models from exploratory multi-simulation for decision-support under uncertainty},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Exploratory modeling and simulation is an effective strategy when there are substantial contextual uncertainty and representational ambiguity in problem formulation. However, two significant challenges impede the use of an ensemble of models in exploratory simulation. The first challenge involves streamlining the maintenance and synthesis of multiple models from plausible features that are identified from and subject to the constraints of the research hypothesis. The second challenge is making sense of the data generated by multi-simulation over a model ensemble. To address both challenges, we introduce a computational framework that integrates feature-driven variability management with an anticipatory learning classifier system to generate explanatory rules from multi-simulation data.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2293–2304},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.5555/3524938.3525259,
author = {Geng, Sinong and Nassif, Houssam and Manzanares, Carlos A. and Reppen, A. Max and Sircar, Ronnie},
title = {Deep PQR: solving inverse reinforcement learning using anchor actions},
year = {2020},
publisher = {JMLR.org},
abstract = {We propose a reward function estimation framework for inverse reinforcement learning with deep energy-based policies. We name our method PQR, as it sequentially estimates the Policy, the Q- function, and the Reward function by deep learning. PQR does not assume that the reward solely depends on the state, instead it allows for a dependency on the choice of action. Moreover, PQR allows for stochastic state transitions. To accomplish this, we assume the existence of one anchor action whose reward is known, typically the action of doing nothing, yielding no reward. We present both estimators and algorithms for the PQR method. When the environment transition is known, we prove that the PQR reward estimator uniquely recovers the true reward. With unknown transitions, we bound the estimation error of PQR. Finally, the performance of PQR is demonstrated by synthetic and real-world datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {321},
numpages = {11},
series = {ICML'20}
}

@inproceedings{10.1145/3302333.3302343,
author = {Cruz, Daniel and Figueiredo, Eduardo and Martinez, Jabier},
title = {A Literature Review and Comparison of Three Feature Location Techniques using ArgoUML-SPL},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302343},
doi = {10.1145/3302333.3302343},
abstract = {Over the last decades, the adoption of Software Product Line (SPL) engineering for supporting software reuse has increased. An SPL can be extracted from one single product or from a family of related software products, and feature location strategies are widely used for variability mining. Several feature location strategies have been proposed in the literature and they usually aim to map a feature to its source code implementation. In this paper, we present a systematic literature review that identifies and characterizes existing feature location strategies. We also evaluated three different strategies based on textual information retrieval in the context of the ArgoUML-SPL feature location case study. In this evaluation, we compare the strategies based on their ability to correctly identify the source code of several features from ArgoUML-SPL ground truth. We then discuss the strengths and weaknesses of each feature location strategy.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {10},
keywords = {variability mining, software product lines, reverse engineering, feature location, benchmark},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1609/aaai.v33i01.33014951,
author = {Shu, Yang and Cao, Zhangjie and Long, Mingsheng and Wang, Jianmin},
title = {Transferable curriculum for weakly-supervised domain adaptation},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014951},
doi = {10.1609/aaai.v33i01.33014951},
abstract = {Domain adaptation improves a target task by knowledge transfer from a source domain with rich annotations. It is not uncommon that "source-domain engineering" becomes a cumbersome process in domain adaptation: the high-quality source domains highly related to the target domain are hardly available. Thus, weakly-supervised domain adaptation has been introduced to address this difficulty, where we can tolerate the source domain with noises in labels, features, or both. As such, for a particular target task, we simply collect the source domain with coarse labeling or corrupted data. In this paper, we try to address two entangled challenges of weakly-supervised domain adaptation: sample noises of the source domain and distribution shift across domains. To disentangle these challenges, a Transferable Curriculum Learning (TCL) approach is proposed to train the deep networks, guided by a transferable curriculum informing which of the source examples are noiseless and transferable. The approach enhances positive transfer from clean source examples to the target and mitigates negative transfer of noisy source examples. A thorough evaluation shows that our approach significantly outperforms the state-of-the-art on weakly-supervised domain adaptation tasks.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {608},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.5555/2880272.2880275,
author = {Desai, Preyas S.},
title = {Quality Segmentation in Spatial Markets: When Does Cannibalization Affect Product Line Design?},
year = {2001},
issue_date = {August 2001},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {20},
number = {3},
issn = {1526-548X},
abstract = {Durable goods manufacturers often design product lines by segmenting their markets on quality attributes-attributes that exhibit a "more is better" property for all consumers. Since products within a product line are partial substitutes, and consumers can self-select the products they want to purchase, multiproduct firms have to carefully consider the cannibalization problem in designing their product lines. Existing research has analyzed the cannibalization problem for a monopolist who faces consumers who differ in their quality valuations. If lower-quality products are sufficiently attractive, higher-valuation consumers may find it beneficial to buy lower-quality products rather than the higher-quality products targeted to them. That is, lower-quality products can potentially cannibalize higher-quality products. The cannibalization problem forces the firm to provide only the highest-valuation segment with its preferred efficient quality. All other segments get qualities lower than their preferred efficient qualities. When the cannibalization problem is very severe, the firm may not serve some of the lowest-valuation segments.However, not much is known about how and when the cannibalization problem affects product line design in an oligopoly. Also, consumers may differ not only in their quality valuations but also in their taste preferences. The objective of this paper is to fill these gaps by examining whether the cannibalization problem affects a firm's price and quality decisions in a model with consumer differences in quality valuations, as well as in their taste preferences, in both monopoly and duopoly settings. The paper addresses questions such as the following. With both types of consumer differences, should a firm, even a monopolist, provide efficient quality only to the top segment? Are there conditions under which other segments can also get their preferred quality levels? If so, how do consumer and firm characteristics affect the likelihood of different segments getting their preferred qualities? How does competition affect the firm's choice of qualities?I develop a model in which the market is made up of two segments, with one segment valuing quality more than the other. Consumers within each segment are distributed over Hotelling's 1929 linear city. Consumers in the two segments can have different taste preferences transportation costs. Firm locations in the two segments may also be different.The paper begins with an analysis of the monopoly case. I find that when both segments are fully covered, the standard self-selection results of the high-valuation segment getting its preferred quality and the low-valuation segment getting less than its preferred quality do hold. Interestingly, when both segments are incompletely covered, under some conditions, the monopolist's price and quality choices are not determined by the cannibalization problem. In these cases, the monopolist finds it optimal to provide each segment with its preferred quality. Thus, the equilibrium quality levels in a second-degree price discrimination situation resemble the third-degree price discrimination solution. I characterize the relevant conditions in terms of consumer characteristics.I then consider the case of two firms competing in the market, each offering two products-one for the high-valuation segment and the other for the low-valuation segment. Here also both types of outcomes are possible, depending on consumers and firm characteristics. Under some conditions, the cannibalization problem does not affect the firms' price and quality choices, and each firm provides each segment with that segment's preferred quality. Each firm finds it optimal to serve both segments. When these conditions do not hold, only the high-valuation segment gets its preferred quality. I interpret the conditions necessary for these results to exist in terms of characteristics of the consumers and the firms.An interesting insight from the analysis is that as the taste preferences of the low-valuation segment become weaker their "transportation cost" becomes lower, the more intense competition in the low-valuation segment makes it more attractive for the high-valuation consumers to buy the products meant for the low-valuation segment. This worsens the cannibalization problem, and the low-valuation segment may not get its preferred quality. On the other hand, when the taste preferences of the high-valuation segments are sufficiently weak, more intense competition in the high-valuation segment reduces that segment's incentives to buy the product meant for the low-valuation segment. This mitigates the cannibalization problem and makes it more likely for the low-valuation segment to get its preferred quality.Similarly, when firms are less differentiated in the low-valuation segment, stronger competition between the firms makes the cannibalization problem worse, and the low-valuation segment may not get its preferred quality. When the differentiation between the firms is sufficiently weak in the high-valuation segment, the high-valuation segment is more likely to be better off buying the product meant for it. As the high-valuation segment's incentives to buy the lower-quality product are reduced, the low-valuation segment is more likely to get its preferred quality.},
journal = {Marketing Science},
month = aug,
pages = {265–283},
numpages = {19},
keywords = {Vertical Differentiation, Product Line Design, Price Discrimination, Horizontal Differentiation, Cannibalization}
}

@inproceedings{10.5555/3524938.3524952,
author = {Ahn, Sungsoo and Seo, Younggyo and Shin, Jinwoo},
title = {Learning what to defer for maximum independent sets},
year = {2020},
publisher = {JMLR.org},
abstract = {Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the number of elements in the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme, coined learning what to defer (LwD), where the agent adaptively shrinks or stretch the number of stages by learning to distribute the element-wise decisions of the solution at each stage. We apply the proposed framework to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {14},
numpages = {11},
series = {ICML'20}
}

@inproceedings{10.1109/ASE.2009.11,
author = {Gr\"{u}nbacher, Paul and Rabiser, Rick and Dhungana, Deepak and Lehofer, Martin},
title = {Model-Based Customization and Deployment of Eclipse-Based Tools: Industrial Experiences},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.11},
doi = {10.1109/ASE.2009.11},
abstract = {Developers of software engineering tools are facing high expectations regarding capabilities and usability. Users expect tools tailored to their specific needs and integrated in their working environment. This increases tools' complexity and complicates their customization and deployment despite available mechanisms for adaptability and extensibility. A main challenge lies in understanding and managing the dependencies between different technical mechanisms for realizing tool variability. We report on industrial experiences of applying a model-based and tool-supported product line approach for the customization and deployment of two Eclipse-based tools. We illustrate challenges of customizing these tools to different development contexts: In the first case study we developed variability models of a product line tool suite used by an industry partner and utilized these models for tool customization and deployment. In the second case study we applied the same approach to a maintenance and setup tool of our industry partner. Our experiences suggest to design software tools as product lines; to formally describe the tools' variability in models; and to provide end-user capabilities for customizing and deploying the tools.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {247–256},
numpages = {10},
keywords = {product line engineering, industrial experience, end-user customization, deployment, Elicpse-based tools},
series = {ASE '09}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {multi-objective optimisation, multi-criteria decision analysis, design-space exploration, decision modelling},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/3194078.3194082,
author = {Pukhkaiev, Dmytro and G\"{o}tz, Sebastian},
title = {BRISE: energy-efficient benchmark reduction},
year = {2018},
isbn = {9781450357326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194078.3194082},
doi = {10.1145/3194078.3194082},
abstract = {A considerable portion of research activities in computer science heavily relies on the process of benchmarking, e.g., to evaluate a hypothesis in an empirical study. The goal is to reveal how a set of independent variables (factors) influences one or more dependent variables. With a vast number of factors or a high amount of factors' values (levels), this process becomes time- and energy-consuming. Current approaches to lower the benchmarking effort suffer from two deficiencies: (1) they focus on reducing the number of factors and, hence, are inapplicable to experiments with only two factors, but a vast number of levels and (2) being adopted from, e.g., combinatorial optimization they are designed for a different search space structure and, thus, can be very wasteful. This paper provides an approach for benchmark reduction, based on adaptive instance selection and multiple linear regression. We evaluate our approach using four empirical studies, which investigate the effect made by dynamic voltage and frequency scaling in combination with dynamic concurrency throttling on the energy consumption of a computing system (parallel compression, sorting, and encryption algorithms as well as database query processing). Our findings show the effectiveness of the approach. We can save 78% of benchmarking effort, while the result's quality decreases only by 3 pp, due to using only a near-optimal configuration.},
booktitle = {Proceedings of the 6th International Workshop on Green and Sustainable Software},
pages = {23–30},
numpages = {8},
keywords = {non-functional properties, fractional factorial design, benchmarking, adaptive instance selection, active learning},
location = {Gothenburg, Sweden},
series = {GREENS '18}
}

@article{10.1287/msom.2019.0796,
author = {Olsen, Tava Lennon and Tomlin, Brian},
title = {Industry 4.0: Opportunities and Challenges for Operations Management},
year = {2020},
issue_date = {January-February 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {22},
number = {1},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2019.0796},
doi = {10.1287/msom.2019.0796},
abstract = {Industry 4.0 connotes a new industrial revolution centered around cyber-physical systems. It posits that the real-time connection of physical and digital systems, along with new enabling technologies, will change the way that work is done and therefore, how work should be managed. It has the potential to break, or at least change, the traditional operations trade-offs among the competitive priorities of cost, flexibility, speed, and quality. This article describes the technologies inherent in Industry 4.0 and the opportunities and challenges for research in this area. The focus is on goods-producing industries, which includes both the manufacturing and agricultural sectors. Specific technologies discussed include additive manufacturing, the internet of things, blockchain, advanced robotics, and artificial intelligence.},
journal = {Manufacturing &amp; Service Operations Management},
month = jan,
pages = {113–122},
numpages = {10},
keywords = {artificial intelligence, robotics, blockchain, internet of things, additive manufacturing}
}

@article{10.1016/j.patcog.2017.10.005,
author = {Zhou, Sanping and Wang, Jinjun and Meng, Deyu and Xin, Xiaomeng and Li, Yubing and Gong, Yihong and Zheng, Nanning},
title = {Deep self-paced learning for person re-identification},
year = {2018},
issue_date = {April 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.10.005},
doi = {10.1016/j.patcog.2017.10.005},
abstract = {We propose a novel deep self-paced learning algorithm to supervise the learning of deep neural network, in which a soft polynomial regularizer term is proposed to gradually involve the faithful samples into training process in a self-paced manner.We optimize the gradient back-propagation of relative distance metric by introducing a symmetric regularizer term, which can convert the back-propagation from the asymmetric mode to a symmetric one.We build an effective part-based deep neural network, in which features of different body parts are first discriminately learned in the convolutional layers and then fused in the fully connected layers. Person re-identification(Re-ID) usually suffers from noisy samples with background clutter and mutual occlusion, which makes it extremely difficult to distinguish different individuals across the disjoint camera views. In this paper, we propose a novel deep self-paced learning(DSPL) algorithm to alleviate this problem, in which we apply a self-paced constraint and symmetric regularization to help the relative distance metric training the deep neural network, so as to learn the stable and discriminative features for person Re-ID. Firstly, we propose a soft polynomial regularizer term which can derive the adaptive weights to samples based on both the training loss and model age. As a result, the high-confidence fidelity samples will be emphasized and the low-confidence noisy samples will be suppressed at early stage of the whole training process. Such a learning regime is naturally implemented under a self-paced learning(SPL) framework, in which samples weights are adaptively updated based on both model age and sample loss using an alternative optimization method. Secondly, we introduce a symmetric regularizer term to revise the asymmetric gradient back-propagation derived by the relative distance metric, so as to simultaneously minimize the intra-class distance and maximize the inter-class distance in each triplet unit. Finally, we build a part-based deep neural network, in which the features of different body parts are first discriminately learned in the lower convolutional layers and then fused in the higher fully connected layers. Experiments on several benchmark datasets have demonstrated the superior performance of our method as compared with the state-of-the-art approaches.},
journal = {Pattern Recogn.},
month = apr,
pages = {739–751},
numpages = {13},
keywords = {Self-paced learning, Person re-identification, Metric learning, Convolutional neural network}
}

@article{10.1016/j.jss.2008.08.026,
author = {Lago, Patricia and Muccini, Henry and van Vliet, Hans},
title = {A scoped approach to traceability management},
year = {2009},
issue_date = {January, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2008.08.026},
doi = {10.1016/j.jss.2008.08.026},
abstract = {Traceability is the ability to describe and follow the life of a software artifact and a means for modeling the relations between software artifacts in an explicit way. Traceability has been successfully applied in many software engineering communities and has recently been adopted to document the transition among requirements, architecture and implementation. We present an approach to customize traceability to the situation at hand. Instead of automating tracing, or representing all possible traces, we scope the traces to be maintained to the activities stakeholders must carry out. We define core traceability paths, consisting of essential traceability links required to support the activities. We illustrate the approach through two examples: product derivation in software product lines, and release planning in software process management. By using a running software product line example, we explain why the core traceability paths identified are needed when navigating from feature to structural models and from family to product level and backward between models used in software product derivation. A feasibility study in release planning carried out in an industrial setting further illustrates the use of core traceability paths during production and measures the increase in performance of the development processes supported by our approach. These examples show that our approach can be successfully used to support both product and process traceability in a pragmatic yet efficient way.},
journal = {J. Syst. Softw.},
month = jan,
pages = {168–182},
numpages = {15},
keywords = {Traceability paths, Traceability issues, Software product line, Software process management}
}

@article{10.1007/s10515-005-2643-9,
author = {Li, Harry C. and Krishnamurthi, Shriram and Fisler, Kathi},
title = {Modular Verification of Open Features Using Three-Valued Model Checking},
year = {2005},
issue_date = {July      2005},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {12},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-005-2643-9},
doi = {10.1007/s10515-005-2643-9},
abstract = {Feature-oriented programming organizes programs around features rather than objects, thus better supporting extensible, product-line architectures. Programming languages increasingly support this style of programming, but programmers get little support from verification tools. Ideally, programmers should be able to verify features independently of each other and use automated compositional reasoning techniques to infer properties of a system from properties of its features. Achieving this requires carefully designed interfaces: they must hold sufficient information to enable compositional verification, yet tools should be able to generate this information automatically because experience indicates programmers cannot or will not provide it manually. We present a model of interfaces that supports automated, compositional, feature-oriented model checking. To demonstrate their utility, we automatically detect the feature-interaction problems originally found manually by Robert Hall in an email suite case study.},
journal = {Automated Software Engg.},
month = jul,
pages = {349–382},
numpages = {34},
keywords = {modular verification, model checking, feature-oriented programming, cross-cutting concerns in software}
}

@inproceedings{10.1007/978-3-030-22999-3_4,
author = {Havelock, Jessica and Oommen, B. John and Granmo, Ole-Christoffer},
title = {On Using “Stochastic Learning on the Line” to Design Novel Distance Estimation Methods for Three-Dimensional Environments},
year = {2019},
isbn = {978-3-030-22998-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-22999-3_4},
doi = {10.1007/978-3-030-22999-3_4},
abstract = {We consider the unsolved problem of Distance Estimation (DE) when the inputs are the x and y coordinates (i.e., the latitudinal and longitudinal positions) of the points under consideration, and the elevation/altitudes of the points specified, for example, in terms of their z coordinates (3DDE). The aim of the problem is to yield an accurate value for the real (road) distance between the points specified by all the three coordinates of the cities in question (This is a typical problem encountered in a GISs and GPSs.). In our setting, the distance between any pair of cities is assumed to be computed by merely having access to the coordinates and known inter-city distances of a small subset of the cities, where these are also specified in terms of their 3D coordinates. The 2D variant of the problem has, typically, been tackled by utilizing parametric functions called “Distance Estimation Functions” (DEFs). To solve the 3D problem, we resort to the Adaptive Tertiary Search (ATS) strategy, proposed by Oommen et al., to affect the learning. By utilizing the information provided in the 3D coordinates of the nodes and the true road distances from this subset, we propose a scheme to estimate the inter-nodal distances. In this regard, we use the ATS strategy to calculate the best parameters for the DEF. While “Goodness-of-Fit” (GoF) functions can be used to show that the results are competitive, we show that they are rather not necessary to compute the parameters. Our results demonstrate the power of the scheme, even though we completely move away from the traditional GoF-based paradigm that has been used for four decades. Our results conclude that the 3DDE yields results that are far superior to those obtained by the corresponding 2DDE.},
booktitle = {Advances and Trends in Artificial Intelligence. From Theory to Practice: 32nd International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2019, Graz, Austria, July 9–11, 2019, Proceedings},
pages = {39–49},
numpages = {11},
keywords = {Stochastic Point Location, Adaptive Tertiary Search, Learning Automata, Estimating real-life distances, Road distance estimation},
location = {Graz, Austria}
}

@inproceedings{10.5555/3524938.3525166,
author = {Degenne, R\'{e}my and Shao, Han and Koolen, Wouter M.},
title = {Structure adaptive algorithms for stochastic bandits},
year = {2020},
publisher = {JMLR.org},
abstract = {We study reward maximisation in a wide class of structured stochastic multi-armed bandit problems, where the mean rewards of arms satisfy some given structural constraints, e.g. linear, unimodal, sparse, etc. Our aim is to develop methods that are flexible (in that they easily adapt to different structures), powerful (in that they perform well empirically and/or provably match instance-dependent lower bounds) and efficient in that the per-round computational burden is small. We develop asymptotically optimal algorithms from instance-dependent lower-bounds using iterative saddle-point solvers. Our approach generalises recent iterative methods for pure exploration to reward maximisation, where a major challenge arises from the estimation of the suboptimality gaps and their reciprocals. Still we manage to achieve all the above desiderata. Notably, our technique avoids the computational cost of the full-blown saddle point oracle employed by previous work, while at the same time enabling finite-time regret bounds. Our experiments reveal that our method successfully leverages the structural assumptions, while its regret is at worst comparable to that of vanilla UCB.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {228},
numpages = {10},
series = {ICML'20}
}

@article{10.1145/1183236.1183264,
author = {Batory, Don and Benavides, David and Ruiz-Cortes, Antonio},
title = {Automated analysis of feature models: challenges ahead},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183264},
doi = {10.1145/1183236.1183264},
journal = {Commun. ACM},
month = dec,
pages = {45–47},
numpages = {3}
}

@inproceedings{10.1145/2973839.2973852,
author = {Santos, Ismayle S. and Rocha, Lincoln S. and Neto, Pedro A. Santos and Andrade, Rossana M. C.},
title = {Model Verification of Dynamic Software Product Lines},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973852},
doi = {10.1145/2973839.2973852},
abstract = {Dynamic Software Product Lines (DSPLs) extend the concept of Software Product Lines enabling adaptation at runtime according to context changes. Such dynamic behavior is typically designed using adaptation rules, context-triggered actions responsible for features activation and deactivation at runtime. The erroneous specification and the interleaving of adaptation rules (i.e., the parallel execution of adaptation rules) can lead DSPL to reach an undesired (improperly or defective) product configuration at runtime. Thus, in order to improve the reliability of DSPL behavior, design faults must be rigorously identified and eliminated in the early stages of DSPL development. In this paper, we address this issue introducing Dynamic Feature Transition Systems (DFTSs) that allow the modeling and formal verification of the DSPLs adaptive behavior. These transition systems are derived from the adaptation rules and a Context Kripke Structure, which is a context evolution model. Furthermore, we formally define five properties that can be used to identify existing design faults in DSPL design. Aiming to assess the feasibility of our approach, a feasibility study was conducted using two DSPLs, Mobile Visit Guides and Car. In both cases, design faults were automatically detected indicating that our formalism can help in the detection of design faults in the DSPLs adaptive behavior.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {113–122},
numpages = {10},
keywords = {Software Verification, Software Reliability, Model Checking, Dynamic Software Product Line},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@inproceedings{10.1609/aaai.v33i01.33015725,
author = {Zhang, Biqiao and Kong, Yuqing and Essl, Georg and Provost, Emily Mower},
title = {undefined-similarity preservation loss for soft labels: a demonstration on cross-corpus speech emotion recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015725},
doi = {10.1609/aaai.v33i01.33015725},
abstract = {In this paper, we propose a Deep Metric Learning (DML) approach that supports soft labels. DML seeks to learn representations that encode the similarity between examples through deep neural networks. DML generally presupposes that data can be divided into discrete classes using hard labels. However, some tasks, such as our exemplary domain of speech emotion recognition (SER), work with inherently subjective data, data for which it may not be possible to identify a single hard label. We propose a family of loss functions, undefined-Similarity Preservation Loss (undefined-SPL), based on the dual form of undefined-divergence for DML with soft labels. We show that the minimizer of undefined-SPL preserves the pairwise label similarities in the learned feature embeddings. We demonstrate the efficacy of the proposed loss function on the task of cross-corpus SER with soft labels. Our approach, which combines undefined-SPL and classification loss, significantly outperforms a baseline SER system with the same structure but trained with only classification loss in most experiments. We show that the presented techniques are more robust to over-training and can learn an embedding space in which the similarity between examples is meaningful.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {702},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.5555/3504035.3504982,
author = {Ali, Mohamoud and Lee, Yugyung},
title = {CRM sales prediction using continuous time-evolving classification},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Customer Relationship Management (CRM) systems play an important role in helping companies identify and keep sales and service prospects. CRM service providers offer a range of tools and techniques that will help find, sell to and keep customers. To be effective, CRM users usually require extensive training. Predictive CRM using machine learning expands the capabilities of traditional CRM through the provision of predictive insights for CRM users by combining internal and external data. In this paper, we will explore a novel idea of computationally learning salesmanship, its patterns and success factors to drive industry intuitions for a more predictable road to a vehicle sale. The newly discovered patterns and insights are used to act as a virtual guide or trainer for the general CRM user population.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {947},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.1145/3177148.3180085,
author = {Surendranath, Ajay and Jayagopi, Dinesh Babu},
title = {Curriculum Learning for Depth Estimation with Deep Convolutional Neural Networks},
year = {2018},
isbn = {9781450352901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177148.3180085},
doi = {10.1145/3177148.3180085},
abstract = {Curriculum learning is a machine learning technique adapted from the way humans acquire knowledge and skills, initially mastering simple tasks and progressing to more complex tasks. The work explores curriculum training by creating multiple levels of dataset with increasing complexity on which the trainings are performed. The experiments demonstrated that there is an average of 12% improvement test loss when compared to a non-curriculum approach. The experiment also demonstrates the advantage of creating synthetic dataset and how it aids in the overall improvement of accuracy. An improvement of 26% is attained on the test error loss when curriculum trained model was compared to training on a limited real world dataset. The work also goes onto propose a novel learning approach, the Self Paced Learning approach with Error-Diversity (SPL-ED) An overall reduction of 32% in the test loss is observed when compared to the non-curriculum training limited to real-world dataset.},
booktitle = {Proceedings of the 2nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence},
pages = {95–100},
numpages = {6},
keywords = {Depth Estimation, Curriculum Learning},
location = {Rabat, Morocco},
series = {MedPRAI '18}
}

@mastersthesis{10.5555/AAI28038400,
author = {Yan, Yuchen},
advisor = {Andre, Mazzoleni, and Tiegang, Fang, and Scott, Ferguson,},
title = {Clustering-Based Genetic Algorithm Initialization and Crossover Operators for Market-Driven Design},
year = {2020},
isbn = {9798641746289},
publisher = {North Carolina State University},
abstract = {Genetic algorithm, as a heuristic optimization approach, has proved its huge potential and great capability in solving complex and large-scale optimization problems. Previous researches have already investigated the performance improvement by applying customers' responds-based targeted initialization operator and intelligent crossover operator when solving the product line optimization problems. This study investigates the performance of new initialization and crossover approaches which are developed based on previous research methods. The new methods - clustering-based initialization and clustering-based crossover operators are tested on two specific product line optimization problems, including an MP3 product line optimization and an automobile product line optimization. Both problems are multi-objective optimizations which specifically focus on maximizing the market share of preference and profit. Previous targeted initialization and intelligent crossover operators were developed using problem-related information to inform their behavior. For the new initialization and crossover approaches in this work, unsupervised machine learning technique – K-means clustering and neural network structure – autoencoder are implemented so that design strings are divided into several groups based on their corresponding market segments. By doing this, new design population manage to keep their diversification in the product line, therefore, the new crossover operators yield much better performances in multiobjective optimization problem due to this preservation of diversification. The presented results demonstrate a satisfying improvement in solution quality of the algorithm when compared to previous GA operators.},
note = {AAI28038400}
}

@inproceedings{10.1109/ASE.2015.44,
author = {Martinez, Jabier and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques and Traon, Yves le},
title = {Automating the extraction of model-based software product lines from model variants},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.44},
doi = {10.1109/ASE.2015.44},
abstract = {We address the problem of automating 1) the analysis of existing similar model variants and 2) migrating them into a software product line. Our approach, named MoVa2PL, considers the identification of variability and commonality in model variants, as well as the extraction of a CVL-compliant Model-based Software Product Line (MSPL) from the features identified on these variants. MoVa2PL builds on a generic representation of models making it suitable to any MOF-based models. We apply our approach on variants of the open source ArgoUML UML modeling tool as well as on variants of an Inflight Entertainment System. Evaluation with these large and complex case studies contributed to show how our feature identification with structural constraints discovery and the MSPL generation process are implemented to make the approach valid (i.e., the extracted software product line can be used to regenerate all variants considered) and sound (i.e., derived variants which did not exist are at least structurally valid).},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {396–406},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1007/978-3-030-64583-0_47,
author = {Fortez, Giovanna and Robledo, Franco and Romero, Pablo and Viera, Omar},
title = {A Fast Genetic Algorithm for the Max Cut-Clique Problem},
year = {2020},
isbn = {978-3-030-64582-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64583-0_47},
doi = {10.1007/978-3-030-64583-0_47},
abstract = {In Marketing, the goal is to understand the psychology of the customer in order to maximize sales. A common approach is to combine web semantic, sniffing, historical information of the customer, and machine learning techniques.In this paper, we exploit the historical information of sales in order to assist product placement. The rationale is simple: if two items are sold jointly, they should be close. This concept is formalized in a combinatorial optimization problem, called Max Cut-Clique or MCC for short.The hardness of the MCC promotes the development of heuristics. The literature offers a GRASP/VND methodology as well as an Iterated Local Search (ILS) implementation. In this work, a novel Genetic Algorithm is proposed to deal with the MCC. A comparison with respect to previous heuristics reveals that our proposal is competitive with state-of-the-art solutions.},
booktitle = {Machine Learning, Optimization, and Data Science: 6th International Conference, LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part I},
pages = {528–539},
numpages = {12},
keywords = {Metaheuristics, Max cut-clique, Combinatorial optimization problem, Marketing},
location = {Siena, Italy}
}

@article{10.1016/j.jss.2017.03.044,
author = {Nuez-Varela, Alberto S. and Prez-Gonzalez, Hctor G. and Martnez-Perez, Francisco E. and Soubervielle-Montalvo, Carlos},
title = {Source code metrics},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.044},
doi = {10.1016/j.jss.2017.03.044},
abstract = {Three major programming paradigms measured by source code metrics were identified.The CK metrics and the object oriented paradigm are the most studied subjects.Java benchmark systems are the most commonly measured systems in research.Technology on metrics extraction mechanisms are not up to research advances.Empirical studies have a major impact on the code metrics community. ContextSource code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. ObjectivesThis paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. MethodA systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. ResultsAlmost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. ConclusionsObject oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.},
journal = {J. Syst. Softw.},
month = jun,
pages = {164–197},
numpages = {34},
keywords = {Systematic mapping study, Source code metrics, Software metrics, Object-oriented metrics, Feature-oriented metrics, Aspect-oriented metrics}
}

@inproceedings{10.1007/978-3-030-78270-2_74,
author = {Yun, Yue and Dai, Huan and Cao, Ruoqi and Zhang, Yupei and Shang, Xuequn},
title = {Self-paced Graph Memory Network for Student GPA Prediction and Abnormal Student Detection},
year = {2021},
isbn = {978-3-030-78269-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78270-2_74},
doi = {10.1007/978-3-030-78270-2_74},
abstract = {Student learning performance prediction (SLPP) is a crucial step in high school education. However, traditional methods fail to consider abnormal students. In this study, we organized every student’s learning data as a graph to use the schema of graph memory networks (GMNs). To distinguish the students and make GMNs learn robustly, we proposed to train GMNs in an “easy-to-hard” process, leading to self-paced graph memory network (SPGMN). SPGMN chooses the low-difficult samples as a batch to tune the model parameters in each training iteration. This approach not only improves the robustness but also rearranges the student sample from normal to abnormal. The experiment results show that SPGMN achieves a higher prediction accuracy and more robustness in comparison with traditional methods. The resulted student sequence reveals the abnormal student has a different pattern in course selection to normal students.},
booktitle = {Artificial Intelligence in Education: 22nd International Conference, AIED 2021, Utrecht, The Netherlands, June 14–18, 2021, Proceedings, Part II},
pages = {417–421},
numpages = {5},
keywords = {Abnormal student detection, Graph memory networks, Self-paced learning, Student learning performance prediction},
location = {Utrecht, The Netherlands}
}

@article{10.1016/j.scico.2012.06.007,
author = {Cetina, Carlos and Giner, Pau and Fons, Joan and Pelechano, Vicente},
title = {Prototyping Dynamic Software Product Lines to evaluate run-time reconfigurations},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.06.007},
doi = {10.1016/j.scico.2012.06.007},
abstract = {Dynamic Software Product Lines (DSPL) encompass systems that are capable of modifying their own behavior with respect to changes in their operating environment by using run-time reconfigurations. A failure in these reconfigurations can directly impact the user experience since the reconfigurations are performed when the system is already under the users control. In this work, we prototype a Smart Hotel DSPL to evaluate the reliability-based risk of the DSPL reconfigurations, specifically, the probability of malfunctioning (Availability) and the consequences of malfunctioning (Severity). This DSPL prototype was performed with the participation of human subjects by means of a Smart Hotel case study which was deployed with real devices. Moreover, we successfully identified and addressed two challenges associated with the involvement of human subjects in DSPL prototyping: enabling participants to (1) trigger the run-time reconfigurations and to (2) understand the effects of the reconfigurations. The evaluation of the case study reveals positive results regarding both Availability and Severity. However, the participant feedback highlights issues with recovering from a failed reconfiguration or a reconfiguration triggered by mistake. To address these issues, we discuss some guidelines learned in the case study. Finally, although the results achieved by the DSPL may be considered satisfactory for its particular domain, DSPL engineers must provide users with more control over the reconfigurations or the users will not be comfortable with DSPLs.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {2399–2413},
numpages = {15},
keywords = {Variability modeling, Smart Hotel, Dynamic Software Product Line}
}

@article{10.1016/j.cor.2021.105482,
author = {Shao, Weishi and Shao, Zhongshi and Pi, Dechang},
title = {Effective constructive heuristics for distributed no-wait flexible flow shop scheduling problem},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {136},
number = {C},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2021.105482},
doi = {10.1016/j.cor.2021.105482},
journal = {Comput. Oper. Res.},
month = dec,
numpages = {17},
keywords = {Factory assignment rules, Dispatch rules, Constructive heuristics, Distributed no-wait flexible flow shop scheduling problem, Makespan, Variable neighborhood search}
}

@article{10.1016/j.engappai.2019.08.015,
author = {Tavasoli, Hanane and Oommen, B. John and Yazidi, Anis},
title = {On utilizing weak estimators to achieve the online classification of data streams},
year = {2019},
issue_date = {Nov 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {86},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2019.08.015},
doi = {10.1016/j.engappai.2019.08.015},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
pages = {11–31},
numpages = {21},
keywords = {Classification in data streams, Non-stationary environments, Learning automata, Weak estimators}
}

@inproceedings{10.1109/ASE.2011.6100118,
author = {Soltani, Samaneh and Asadi, Mohsen and Hatala, Marek and Gasevic, Dragan and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on stakeholders' business concerns},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100118},
doi = {10.1109/ASE.2011.6100118},
abstract = {In Software Product Line Engineering, concrete products of a family can be generated through a configuration process over a feature model. The configuration process selects features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from all the available features in the feature model is a cumbersome task because 1) the stakeholders may have diverse business concerns and limited resources that they can spend on a product and 2) features may have negative and positive contributions on different business concern. Many configurations techniques have been proposed to facilitate software developers' tasks through automated product derivation. However, most of the current proposals for automatic configuration are not devised to cope with business oriented requirements and stakeholders' resource limitations. We propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy the stakeholders' business concerns and resource limitations. We also provide tooling support to facilitate the use of our framework.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {536–539},
numpages = {4},
series = {ASE '11}
}

@article{10.1016/j.infsof.2019.08.007,
author = {Nogueira Teixeira, Eld\^{a}nae and Aleixo, Fellipe Ara\'{u}jo and Am\^{a}ncio, Francisco Dione de Sousa and OliveiraJr, Edson and Kulesza, Uir\'{a} and Werner, Cl\'{a}udia},
title = {Software process line as an approach to support software process reuse: A systematic literature review},
year = {2019},
issue_date = {Dec 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {116},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.08.007},
doi = {10.1016/j.infsof.2019.08.007},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {22},
keywords = {Process variability management, Software process line, Process reuse, Software process, Systematic review}
}

@article{10.1016/j.eswa.2021.114781,
author = {Kerr, Emmett and McGinnity, T.M. and Coleman, Sonya and Shepherd, Andrea},
title = {Human vital sign determination using tactile sensing and fuzzy triage system},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {175},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114781},
doi = {10.1016/j.eswa.2021.114781},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {16},
keywords = {Human vital sign detection, Classification, Artificial intelligence, Tactile sensing, Signal processing, Automated triage, Fuzzy systems}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {software product line, service-oriented architecture, service selection, optimization, feature-oriented development},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.5555/3504035.3504869,
author = {Fan, Xin and Liu, Risheng and Huyan, Kang and Feng, Yuyao and Luo, Zhongxuan},
title = {Self-reinforced cascaded regression for face alignment},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Cascaded regression is prevailing in face alignment thanks to its accuracy and robustness, but typically demands manually annotated examples having low discrepancy between shape-indexed features and shape updates. In this paper, we propose a self-reinforced strategy that iteratively expands the quantity and improves the quality of training examples, thus upgrading the performance of cascaded regression itself. The reinforced term evaluates the example quality upon the consistence on both local appearance and global geometry of human faces, and constitutes the example evolution by the philosophy of "survival of the fittest". We train a set of discriminative classifiers, each associated with one landmark label, to prune those examples with inconsistent local appearance, and further validate the geometric relationship among groups of labeled landmarks against the common global geometry derived from a projective invariant. We embed this generic strategy into typical cascaded regressions, and the alignment results on several benchmark data sets demonstrate its effectiveness to predict good examples starting from a small subset.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {834},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.1145/2897053.2897058,
author = {Sharifloo, Amir Molzam and Metzger, Andreas and Quinton, Cl\'{e}ment and Baresi, Luciano and Pohl, Klaus},
title = {Learning and evolution in dynamic software product lines},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897058},
doi = {10.1145/2897053.2897058},
abstract = {A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {158–164},
numpages = {7},
keywords = {machine learning, evolution, dynamic software product lines, adaptation},
location = {Austin, Texas},
series = {SEAMS '16}
}

@inproceedings{10.1145/2635868.2635876,
author = {Schultis, Klaus-Benedikt and Elsner, Christoph and Lohmann, Daniel},
title = {Architecture challenges for internal software ecosystems: a large-scale industry case study},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635876},
doi = {10.1145/2635868.2635876},
abstract = {The idea of software ecosystems encourages organizations to open software projects for external businesses, governing the cross-organizational development by architectural and other measures. Even within a single organization, this paradigm can be of high value for large-scale decentralized software projects that involve various internal, yet self-contained organizational units. However, this intra-organizational decentralization causes architecture challenges that must be understood to reason about suitable architectural measures. We present an in-depth case study on collaboration and architecture challenges in two of these large-scale software projects at Siemens. We performed a total of 46 hours of semi-structured interviews with 17 leading software architects from all involved organizational units. Our major findings are: (1) three collaboration models on a continuum that ranges from high to low coupling, (2) a classification of architecture challenges, together with (3) a qualitative and quantitative exposure of the identified recurring issues along each collaboration model. Our study results provide valuable insights for both industry and academia: Practitioners that find themselves in one of the collaboration models can use empirical evidence on challenges to make informed decisions about counteractive measures. Researchers can focus their attention on challenges faced by practitioners to make software engineering more effective.},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {542–552},
numpages = {11},
keywords = {software product line, software architecture, decentralized software engineering, collaboration, case study, Software ecosystem},
location = {Hong Kong, China},
series = {FSE 2014}
}

@article{10.1007/s10664-013-9253-0,
author = {Ihme, Tuomas and Pikkarainen, Minna and Teppola, Susanna and K\"{a}\"{a}ri\"{a}inen, Jukka and Biot, Olivier},
title = {Challenges and industry practices for managing software variability in small and medium sized enterprises},
year = {2014},
issue_date = {August    2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9253-0},
doi = {10.1007/s10664-013-9253-0},
abstract = {Software variability is an ability to change (configure, customize, extend) software artefacts (e.g. code, product, domain requirements, models, design, documentation, test cases) for a specific context. Optimized variability management can lead a software company to 1) shorter development lead time, 2) improved customer and improved user satisfaction, 3) reduced complexity of product management (more variability, same $) and 4) reduced costs (same variability, less $). However, it is not easy for software companies, especially small and medium size of enterprises to deal with variability. In this paper we present variability challenges and used practices collected from five SMEs. Our study indicates that increased product complexity can lead growing SMEs to the time-consuming decision-making. Many of the analyzed medium size of companies also expect improved tool support to help them to boost their productivity when managing increasingly complex products and increasing amount of variants In fact, in many of the analysed SMEs, a high level of automation in design, release management and testing are or become a key factor for market success By introducing the challenges and used practices related to variability the paper deepens understanding of this highly relevant but relatively under-researched phenomenon and contributes to the literature on software product line engineering.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1144–1168},
numpages = {25},
keywords = {Variability practices, Software variability challenges}
}

@article{10.1007/s13748-020-00205-3,
author = {Ram\'{\i}rez, Aurora and Delgado-P\'{e}rez, Pedro and Ferrer, Javier and Romero, Jos\'{e} Ra\'{u}l and Medina-Bulo, Inmaculada and Chicano, Francisco},
title = {A systematic literature review of the SBSE research community in Spain},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {2},
url = {https://doi.org/10.1007/s13748-020-00205-3},
doi = {10.1007/s13748-020-00205-3},
abstract = {Since its appearance in 2001, search-based software engineering has allowed software engineers to use optimisation techniques to automate distinctive human problems related to software management and development. The scientific community in Spain has not been alien to these advances. Their contributions cover both the optimisation of software engineering tasks and the proposal of new search algorithms. This review compiles the research efforts of this community in the area. With this aim, we propose a protocol to describe the review process, including the search sources, inclusion and exclusion criteria of candidate papers, the data extraction procedure and the categorisation of primary studies. After retrieving more than 3700 papers, 232 primary studies have been selected, whose analysis gives a precise picture of the current research state of the community, trends and future challenges. With 145 authors from 19 distinct institutions, results show that a diversity of tasks, including software planning, requirements, design and testing, and a large variety of techniques has been used, from exact search to evolutionary computation and swarm intelligence. Further, since 2015, specific scientific events have helped to bring together the community, improving collaborations, financial funding and internationalisation.},
journal = {Prog. in Artif. Intell.},
month = jun,
pages = {113–128},
numpages = {16},
keywords = {Spanish community, Research trends, Systematic review, Search-based software engineering}
}

@inproceedings{10.5555/3053577.3053583,
author = {Hubaux, Arnaud and Jannach, Dietmar and Drescher, Conrad and Murta, Leonardo and M\"{a}nnist\"{o}, Tomi and Czarnecki, Krzysztof and Heymans, Patrick and Nguyen, Tien and Zanker, Markus},
title = {Unifying software and product configuration: a research roadmap},
year = {2012},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {For more than 30 years, knowledge-based product configuration systems have been successfully applied in many industrial domains. Correspondingly, a large number of advanced techniques and algorithms have been developed in academia and industry to support different aspects of configuration reasoning. While traditional research in the field focused on the configuration of physical artefacts, recognition of the business value of customizable software products led to the emergence of software product line engineering. Despite the significant overlap in research interests, the two fields mainly evolved in isolation. Only limited attempts were made at combining the approaches developed in the different fields. In this paper, we first aim to give an overview of commonalities and differences between software product line engineering and product configuration. We then identify opportunities for cross-fertilization between these fields and finally develop a research agenda to combine their respective techniques. Ultimately, this should lead to a unified configuration approach.},
booktitle = {Proceedings of the 2012 International Conference on Configuration - Volume 958},
pages = {31–35},
numpages = {5},
location = {Montpellier, France},
series = {CONFWS'12}
}

@inproceedings{10.1145/3469213.3469219,
author = {Ren, Jing and Huang, Xishi},
title = {Rapid Transformation Estimation Using Deep Learning for Defect Detection},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3469219},
doi = {10.1145/3469213.3469219},
abstract = {Defect detection is a crucial step in the manufacturing of vehicle parts such as the engine. One major method for defect detection is to use image registration and image difference to identify and segment the defects. The key technology of this approach is to extract the accurate transformation information between the template image and the testing images. In this paper, we propose a novel deep neural network (DNN) method to learn the transformations from the training dataset. Wavelet transformation is introduced to denoise the images and reduce the image size for fast image registration. The results show that the trained DNN models are able to effectively predict the transformation between the template image and the actual test image in real time with high accuracy.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {6},
numpages = {5},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@article{10.1145/3369393,
author = {Ding, Yuhang and Fan, Hehe and Xu, Mingliang and Yang, Yi},
title = {Adaptive Exploration for Unsupervised Person Re-identification},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3369393},
doi = {10.1145/3369393},
abstract = {Due to domain bias, directly deploying a deep person re-identification (re-ID) model trained on one dataset often achieves considerably poor accuracy on another dataset. In this article, we propose an Adaptive Exploration (AE) method to address the domain-shift problem for re-ID in an unsupervised manner. Specifically, in the target domain, the re-ID model is inducted to (1) maximize distances between all person images and (2) minimize distances between similar person images. In the first case, by treating each person image as an individual class, a non-parametric classifier with a feature memory is exploited to encourage person images to move far away from each other. In the second case, according to a similarity threshold, our method adaptively selects neighborhoods for each person image in the feature space. By treating these similar person images as the same class, the non-parametric classifier forces them to stay closer. However, a problem of the adaptive selection is that, when an image has too many neighborhoods, it is more likely to attract other images as its neighborhoods. As a result, a minority of images may select a large number of neighborhoods while a majority of images has only a few neighborhoods. To address this issue, we additionally integrate a balance strategy into the adaptive selection. We evaluate our methods with two protocols. The first one is called “target-only re-ID”, in which only the unlabeled target data is used for training. The second one is called “domain adaptive re-ID”, in which both the source data and the target data are used during training. Experimental results on large-scale re-ID datasets demonstrate the effectiveness of our method. Our code has been released at https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {3},
numpages = {19},
keywords = {unsupervised learning, domain adaptation, deep learning, Person re-identification}
}

@inproceedings{10.5555/3524938.3525699,
author = {Rosenfeld, Nir and Oshiba, Kojin and Singer, Yaron},
title = {Predicting choice with set-dependent aggregation},
year = {2020},
publisher = {JMLR.org},
abstract = {Providing users with alternatives to choose from is an essential component of many online platforms, making the accurate prediction of choice vital to their success. A renewed interest in learning choice models has led to improved modeling power, but most current methods are either limited in the type of choice behavior they capture, cannot be applied to large-scale data, or both.Here we propose a learning framework for predicting choice that is accurate, versatile, and theoretically grounded. Our key modeling point is that to account for how humans choose, predictive models must be expressive enough to accommodate complex choice patterns but structured enough to retain statistical efficiency. Building on recent results in economics, we derive a class of models that achieves this balance, and propose a neural implementation that allows for scalable end-to-end training. Experiments on three large choice datasets demonstrate the utility of our approach.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {761},
numpages = {10},
series = {ICML'20}
}

@inproceedings{10.5555/3504035.3504690,
author = {Neill, James O' and Buitelaar, Paul},
title = {Few shot transfer learning between word relatedness and similarity tasks using a gated recurrent siamese network},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Word similarity and word relatedness are fundamental to natural language processing and more generally, understanding how humans relate concepts in semantic memory. A growing number of datasets are being proposed as evaluation benchmarks, however, the heterogeneity and focus of each respective dataset makes it difficult to draw plausible conclusions as to how a unified semantic model would perform. Additionally, we want to identify the transferability of knowledge obtained from one task to another, within the same domain and across domains. Hence, this paper first presents an evaluation and comparison of eight chosen datasets tested using the best performing regression models. As a baseline, we present regression models that incorporate both lexical features and word embeddings to produce consistent and competitive results compared to the state of the art. We present our main contribution, the best performing model across seven of the eight datasets - a Gated Recurrent Siamese Network that learns relationships between lexical word definitions. A parameter transfer learning strategy is employed for the Siamese Network. Subsequently, we present a secondary contribution which is the best performing non-sequential model: an Inductive and Transductive Transfer Learning strategy for transferring decision trees within a Random Forest to a target task that is learned from only few instances. The method involves measuring semantic distance between hidden factored matrix representations of decision tree traversal matrices.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {655},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.5555/1170746.1171715,
author = {Seo, Jinwook and Bakay, M. and Zhao, Po and Chen, Yi-Wen and Clarkson, P. and Shneiderman, B. and Hoffman, E. P.},
title = {Interactive color mosaic and dendrogram displays for signal/noise optimization in microarray data analysis},
year = {2003},
isbn = {0780379659},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Data analysis and visualization is strongly influenced by noise and noise filters. There are multiple sources of "noise" in microarray data analysis, but signal/noise ratios are rarely optimized, or even considered. Here, we report a noise analysis of a novel 13 million oligonucleotide dataset - 25 human U133A (/spl sim/500,000 features) profiles of patient muscle biopsies. We use our recently described interactive visualization tool, the hierarchical clustering explorer (HCE) to systemically address the effect of different noise filters on resolution of arrays into "correct" biological groups (unsupervised clustering into three patient groups of known diagnosis). We varied probe set interpretation methods (MAS 5.0, RMA), "present call" filters, and clustering linkage methods, and investigated the results in HCE. HCE's interactive features enabled us to quickly see the impact of these three variables. Dendrogram displays showed the clustering results systematically, and color mosaic displays provided a visual support for the results. We show that each of these three variables has a strong effect on unsupervised clustering. For this dataset, the strength of the biological variable was maximized, and noise minimized, using MAS 5.0, 10% present call filter, and average group linkage. We propose a general method of using interactive tools to identify the optimal signal/noise balance or the optimal combination of these three variables to maximize the effect of the desired biological variable on data interpretation.},
booktitle = {Proceedings of the 2003 International Conference on Multimedia and Expo - Volume 3 (ICME '03) - Volume 03},
pages = {461–464},
numpages = {4},
series = {ICME '03}
}

@inproceedings{10.1109/ICPC.2017.21,
author = {Tang, Yutian and Leung, Hareton},
title = {Constructing feature model by identifying variability-aware modules},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.21},
doi = {10.1109/ICPC.2017.21},
abstract = {Modeling variability, known as building feature models, should be an essential step in the whole process of product line development, maintenance and testing. The work on feature model recovery serves as a foundation and further contributes to product line development and variability-aware analysis. Different from the architecture recovery process even though they somewhat share the same process, the variability is not considered in all architecture recovery techniques. In this paper, we proposed a feature model recovery technique VMS, which gives a variability-aware analysis on the program and further constructs modules for feature model mining. With our work, we bring the variability information into architecture and build the feature model directly from the source base. Our experimental results suggest that our approach performs competitively and outperforms six other representative approaches for architecture recovery.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {263–274},
numpages = {12},
keywords = {variability-aware modularity, product line, feature modules, feature model recovery, configuration},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@article{10.1016/j.jss.2019.02.027,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Modelling equivalence classes of feature models with concept lattices to assist their extraction from product descriptions},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.027},
doi = {10.1016/j.jss.2019.02.027},
journal = {J. Syst. Softw.},
month = jun,
pages = {1–23},
numpages = {23},
keywords = {Feature models, Variability modelling, Formal concept analysis, Reverse engineering, Software product lines}
}

@inproceedings{10.1007/978-3-030-32692-0_49,
author = {Peng, Shiqi and Lai, Bolin and Yao, Guangyu and Zhang, Xiaoyun and Zhang, Ya and Wang, Yan-Feng and Zhao, Hui},
title = {Learning-Based Bone Quality Classification Method for Spinal Metastasis},
year = {2019},
isbn = {978-3-030-32691-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32692-0_49},
doi = {10.1007/978-3-030-32692-0_49},
abstract = {Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries. Early detection of spinal metastasis is critical for accurate staging and optimal treatment. The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists. In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images. We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance. MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks. Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary classification sub-tasks, i.e., whether blastic and whether lytic, and leverage a multiple layer perceptron to combine their predictions. In order to make the model more robust and generalize better, self-paced learning is adopted to gradually involve from easy to more complex samples into the training process. The proposed learning-based method is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our method significantly outperforms an 121-layer DenseNet classifier in sensitivities by +12.54%, +7.23% and +29.06% for blastic, mixed and lytic lesions, respectively, meanwhile +12.33%, +23.21% and +34.25% at vertebrae level.},
booktitle = {Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings},
pages = {426–434},
numpages = {9},
keywords = {Spinal metastasis, Bone quality classification, Multi-task learning, Self-paced learning},
location = {Shenzhen, China}
}

@inproceedings{10.5555/3524938.3525504,
author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U. and Jaggi, Martin},
title = {Extrapolation for large-batch training in deep learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Deep learning networks are typically trained by Stochastic Gradient Descent (SGD) methods that iteratively improve the model parameters by estimating a gradient on a very small fraction of the training data. A major roadblock faced when increasing the batch size to a substantial fraction of the training data for reducing training time is the persistent degradation in performance (generalization gap). To address this issue, recent work propose to add small perturbations to the model parameters when computing the stochastic gradients and report improved generalization performance due to smoothing effects. However, this approach is poorly understood; it requires often model-specific noise and fine-tuning.To alleviate these drawbacks, we propose to use instead computationally efficient extrapolation (extragradient) to stabilize the optimization trajectory while still benefiting from smoothing to avoid sharp minima. This principled approach is well grounded from an optimization perspective and we show that a host of variations can be covered in a unified framework that we propose. We prove the convergence of this novel scheme and rigorously evaluate its empirical performance on ResNet, LSTM, and Transformer. We demonstrate that in a variety of experiments the scheme allows scaling to much larger batch sizes than before whilst reaching or surpassing SOTA accuracy.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {566},
numpages = {11},
series = {ICML'20}
}

@inproceedings{10.5555/3524938.3525278,
author = {Gopi, Sivakanth and Gulhane, Pankaj and Kulkarni, Janardhan and Shen, Judy Hanwen and Shokouhi, Milad and Yekhanin, Sergey},
title = {Differentially private set union},
year = {2020},
publisher = {JMLR.org},
abstract = {We study the basic operation of set union in the global model of differential privacy. In this problem, we are given a universe U of items, possibly of infinite size, and a database D of users. Each user i contributes a subset Wi ⊆ U of items. We want an (ε,δ)-differentially private Algorithm which outputs a subset S ⊂ UiWi such that the size of S is as large as possible. The problem arises in countless real world applications, and is particularly ubiquitous in natural language processing (NLP) applications. For example, discovering words, sentences, n-grams etc., from private text data belonging to users is an instance of the set union problem. In this paper we design new algorithms for this problem that significantly outperform the best known algorithms.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {340},
numpages = {10},
series = {ICML'20}
}

@article{10.1016/j.engappai.2018.06.010,
author = {Chin, Cheng Siong and Ji, Xi},
title = {Adaptive online sequential extreme learning machine for frequency-dependent noise data on offshore oil rig},
year = {2018},
issue_date = {Sep 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2018.06.010},
doi = {10.1016/j.engappai.2018.06.010},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {226–241},
numpages = {16},
keywords = {Root mean square error, Training time, Noise prediction, Oil-rig, Extreme learning machine, Multiple frequency dependent data}
}

@inproceedings{10.1145/3442391.3442411,
author = {Fischer, Stefan and Ramler, Rudolf and Klammer, Claus and Rabiser, Rick},
title = {Testing of Highly Configurable Cyber-Physical Systems – A Multiple Case Study},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442411},
doi = {10.1145/3442391.3442411},
abstract = {Cyber-physical systems, i.e., systems that seamlessly integrate computation and physical components, are typically highly-configurable systems. Testing such systems is particularly challenging because they comprise a large number of heterogeneous components that can be configured and combined in different ways. Despite a plethora of work investigating software testing in general and software product line testing in particular, variability in tests and how industry does actually manage testing highly configurable cyber-physical systems is not well understood. In this paper, we report the results of a multiple case study we conducted with three companies developing and maintaining highly-configurable cyber-physical systems focusing on their testing practices, with a particular focus on how they manage variability in tests. We conclude that experienced-based selection of configurations for testing is currently predominant. Variability modeling techniques are not utilized and the dependencies between configuration options are only partially modeled at best. However, the companies are aware of the situation and have the need and desire to cover more configuration combinations by automated tests. This in turn raises many questions, which might also be of interest to the scientific community and motivate future research.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {19},
numpages = {10},
keywords = {variability testing, interview, industry case, configuration testing},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.5555/3524938.3525116,
author = {Choo, Davin and Grunau, Christoph and Portmann, Julian and Rozho\v{n}, V\'{a}clav},
title = {k-means++: few more steps yield constant approximation},
year = {2020},
publisher = {JMLR.org},
abstract = {The k-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is a state-of-the-art algorithm for solving the k-means clustering problem and is known to give an O(log k)-approximation in expectation. Recently, Lattanzi and Sohler (ICML 2019) proposed augmenting k-means++ with O(k log log k) local search steps to yield a constant approximation (in expectation) to the k-means clustering problem. In this paper, we improve their analysis to show that, for any arbitrarily small constant ε &gt; 0, with only εk additional local search steps, one can achieve a constant approximation guarantee (with high probability in k), resolving an open problem in their paper.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {178},
numpages = {9},
series = {ICML'20}
}

@article{10.1016/j.infsof.2014.04.002,
author = {Machado, Ivan Do Carmo and Mcgregor, John D. and Cavalcanti, Yguarat\~{a} Cerqueira and De Almeida, Eduardo Santana},
title = {On strategies for testing software product lines: A systematic literature review},
year = {2014},
issue_date = {October, 2014},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {56},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2014.04.002},
doi = {10.1016/j.infsof.2014.04.002},
abstract = {Context: Testing plays an important role in the quality assurance process for software product line engineering. There are many opportunities for economies of scope and scale in the testing activities, but techniques that can take advantage of these opportunities are still needed. Objective: The objective of this study is to identify testing strategies that have the potential to achieve these economies, and to provide a synthesis of available research on SPL testing strategies, to be applied towards reaching higher defect detection rates and reduced quality assurance effort. Method: We performed a literature review of two hundred seventy-six studies published from the year 1998 up to the 1st semester of 2013. We used several filters to focus the review on the most relevant studies and we give detailed analyses of the core set of studies. Results: The analysis of the reported strategies comprised two fundamental aspects for software product line testing: the selection of products for testing, and the actual test of products. Our findings indicate that the literature offers a large number of techniques to cope with such aspects. However, there is a lack of reports on realistic industrial experiences, which limits the inferences that can be drawn. Conclusion: This study showed a number of leveraged strategies that can support both the selection of products, and the actual testing of products. Future research should also benefit from the problems and advantages identified in this study.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1183–1199},
numpages = {17},
keywords = {Systematic literature review, Software testing, Software quality, Software product lines}
}

@article{10.1016/j.engappai.2019.103394,
author = {Izakian, Zahedeh and Mesgari, M. Saadi and Weibel, Robert},
title = {A feature extraction based trajectory segmentation approach based on multiple movement parameters},
year = {2020},
issue_date = {Feb 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2019.103394},
doi = {10.1016/j.engappai.2019.103394},
journal = {Eng. Appl. Artif. Intell.},
month = feb,
numpages = {16},
keywords = {Sliding window, Movement parameter, Trajectory clustering, Trajectory segmentation}
}

@article{10.1145/182063.182067,
author = {Perez, R. A. and Lilkendey, J. T. and Koh, S. W.},
title = {Machine learning for a dynamic manufacturing environment},
year = {1994},
issue_date = {Feb. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1558-1144},
url = {https://doi.org/10.1145/182063.182067},
doi = {10.1145/182063.182067},
abstract = {Within the semiconductor wafer manufacturing process, tight quality control is of utmost importance. This is due to such factors as the highly competitive nature of the business [2] and the complex nature of the process itself. Therefore, it is imperative that processing problems are recognized and corrected as quickly as possible. To accomplish this, a Parametric Test facility exists where a critical quality check is performed on the wafers. This is accomplished by measuring a number of electrical parameters at existing test sites on the wafer. Each measurement must fall within an acceptable range of values for its associated parameter to pass the critical check. If test results indicate that a parameter is outside of its acceptable range, the wafer may fail the quality check and be scrapped. If this happens an expert must examine all the parametric data associated with that wafer and attempt to determine the reason for failure and where in the manufacturing process the problem may have occurred.},
journal = {SIGICE Bull.},
month = feb,
pages = {5–9},
numpages = {5}
}

@article{10.1016/j.artint.2019.103203,
author = {Lu, Tyler and Boutilier, Craig},
title = {Preference elicitation and robust winner determination for single- and multi-winner social choice},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {279},
number = {C},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2019.103203},
doi = {10.1016/j.artint.2019.103203},
journal = {Artif. Intell.},
month = feb,
numpages = {32},
keywords = {Decision theory, Minimax regret, Partial preferences, Robust optimization, Preference elicitation, Voting, Social choice}
}

@article{10.4018/IJRSDA.2016070101,
author = {Ripon, Shamim H and Kamal, Sarwar and Hossain, Saddam and Dey, Nilanjan},
title = {Theoretical Analysis of Different Classifiers under Reduction Rough Data Set: A Brief Proposal},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {3},
issn = {2334-4598},
url = {https://doi.org/10.4018/IJRSDA.2016070101},
doi = {10.4018/IJRSDA.2016070101},
abstract = {Rough set plays vital role to overcome the complexities, vagueness, uncertainty, imprecision, and incomplete data during features analysis. Classification is tested on certain dataset that maintain an exact class and review process where key attributes decide the class positions. To assess efficient and automated learning, algorithms are used over training datasets. Generally, classification is supervised learning whereas clustering is unsupervised. Classifications under mathematical models deal with mining rules and machine learning. The Objective of this work is to establish a strong theoretical and manual analysis among three popular classifier namely K-nearest neighbor K-NN, Naive Bayes and Apriori algorithm. Hybridization with rough sets among these three classifiers enables enable to address larger datasets. Performances of three classifiers have tested in absence and presence of rough sets. This work is in the phase of implementation for DNA Deoxyribonucleic Acid datasets and it will design automated system to assess classifier under machine learning environment.},
journal = {Int. J. Rough Sets Data Anal.},
month = jul,
pages = {1–20},
numpages = {20},
keywords = {Rough Set, Naive Bayes, K-NN, DNA, Apriori Algorithm}
}

@inproceedings{10.1145/1600193.1600244,
author = {D\'{\i}az, Oscar and Anfurrutia, Felipe I. and Kortabitarte, Jon},
title = {Using DITA for documenting software product lines},
year = {2009},
isbn = {9781605585758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1600193.1600244},
doi = {10.1145/1600193.1600244},
abstract = {Aligning the software process and the documentation process is a recipe for having both software and documentation in synchrony where changes in software seamlessly ripple along its documentation counterpart. This paper focuses on documentation for Software Product Lines (SPLs). A SPL is not intended to build one application, but a number of them: a product family. In contrast to single-software product development, SPL development is based on the idea that the distinct products of the family share a significant amount of assets. This forces a change in the software process. Likewise, software documentation development should now mimic their code counterpart: product documentation should also be produced out of a common set of assets. Specifically, the paper shows how DITA process and documents are recasted using a feature-oriented approach, a realization mechanism for SPLs. In so doing, documentation artifacts are produced at the same pace and using similar variability mechanisms that those used for code artifacts. This accounts for three main advantages: uniformity, separation of concerns, and timely and accurate delivery of the documentation.},
booktitle = {Proceedings of the 9th ACM Symposium on Document Engineering},
pages = {231–240},
numpages = {10},
keywords = {software product lines, feature oriented programming, documentation, dita},
location = {Munich, Germany},
series = {DocEng '09}
}

@inproceedings{10.1109/ASE.2019.00120,
author = {Reuling, Dennis and Kelter, Udo and Ruland, Sebastian and Lochau, Malte},
title = {SiMPOSE: configurable N-way program merging strategies for superimposition-based analysis of variant-rich software},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00120},
doi = {10.1109/ASE.2019.00120},
abstract = {Modern software often exists in many different, yet similar versions and/or variants, usually derived from a common code base (e.g., via clone-and-own). In the context of product-line engineering, family-based analysis has shown very promising potential for improving efficiency in applying quality-assurance techniques to variant-rich software, as compared to a variant-by-variant approach. Unfortunately, these strategies rely on a product-line representation superimposing all program variants in a syntactically well-formed, semantically sound and variant-preserving manner, which is manually hard to obtain in practice. We demonstrate the SiMPOSE methodology for automatically generating superimpositions of N given program versions and/or variants facilitating family-based analysis of variant-rich software. SiMPOSE is based on a novel N-way model-merging technique operating at the level of control-flow automata (CFA) representations of C programs. CFAs constitute a unified program abstraction utilized by many recent software-analysis tools. We illustrate different merging strategies supported by SiMPOSE, namely variant-by-variant, N-way merging, incremental 2-way merging, and partition-based N/2-way merging, and demonstrate how SiMPOSE can be used to systematically compare their impact on efficiency and effectiveness of family-based unit-test generation. The SiMPOSE tool, the demonstration of its usage as well as related artifacts and documentation can be found at http://pi.informatik.uni-siegen.de/projects/variance/simpose.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1134–1137},
numpages = {4},
keywords = {software testing, program merging, model merging, family-based analyses},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/2897845.2897856,
author = {Meng, Guozhu and Xue, Yinxing and Mahinthan, Chandramohan and Narayanan, Annamalai and Liu, Yang and Zhang, Jie and Chen, Tieming},
title = {Mystique: Evolving Android Malware for Auditing Anti-Malware Tools},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897856},
doi = {10.1145/2897845.2897856},
abstract = {In the arms race of attackers and defenders, the defense is usually more challenging than the attack due to the unpredicted vulnerabilities and newly emerging attacks every day. Currently, most of existing malware detection solutions are individually proposed to address certain types of attacks or certain evasion techniques. Thus, it is desired to conduct a systematic investigation and evaluation of anti-malware solutions and tools based on different attacks and evasion techniques. In this paper, we first propose a meta model for Android malware to capture the common attack features and evasion features in the malware. Based on this model, we develop a framework, MYSTIQUE, to automatically generate malware covering four attack features and two evasion features, by adopting the software product line engineering approach. With the help of MYSTIQUE, we conduct experiments to 1) understand Android malware and the associated attack features as well as evasion techniques; 2) evaluate and compare the 57 off-the-shelf anti-malware tools, 9 academic solutions and 4 App market vetting processes in terms of accuracy in detecting attack features and capability in addressing evasion. Last but not least, we provide a benchmark of Android malware with proper labeling of contained attack and evasion features.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {365–376},
numpages = {12},
keywords = {malware generation, evolutionary algorithm, defense capability, android feature model},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@article{10.1016/j.infsof.2021.106674,
author = {Domingo, \'{A}frica and Echeverr\'{\i}a, Jorge and Pastor, \'{O}scar and Cetina, Carlos},
title = {Evaluating the influence of scope on feature location},
year = {2021},
issue_date = {Dec 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {140},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106674},
doi = {10.1016/j.infsof.2021.106674},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {15},
keywords = {Model-driven engineering, Controlled experiment, Feature location}
}

@article{10.1145/3356773.3356806,
author = {Stegh\"{o}fer, Jan-Philipp and Niu, Nan and Guo, Jin L.C. and Mahmoud, Anas},
title = {SST'19 - Software and Systems Traceability: Summary of the 10th International Workshop at the 41st International Conference on Software Engineering (ICSE), May 27, 2019},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3356773.3356806},
doi = {10.1145/3356773.3356806},
abstract = {Traceability is the ability to relate di erent artifacts during the development and operation of a system to each other. It enables program comprehension, change impact analysis, and facilitates the cooperation of engineers from di erent disciplines. The 10th International Workshop on Software and Systems Traceability (former International Workshop on Traceability in Emerging Forms of Software Engineering, TEFSE), explored the role and impact of traceability in modern software and systems development. The event brought together researchers and practitioners to examine the challenges of recovering, maintaining, and utilizing traceability for the myriad forms of software and systems engineering artifacts. SST'19 was a highly interactive working event focused on discussing the main problems related to software traceability in particular in the context of opportunities and challenges posed by the recent progress in Arti cial Intelligence techniques and proposing possible solutions for such problems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {43–47},
numpages = {5},
keywords = {traceability, requirements, artificial intelligence}
}

@article{10.1287/msom.2018.0721,
author = {Mart\'{\i}nez-de-Alb\'{e}niz, Victor and Valdivia, Ana},
title = {Measuring and Exploiting the Impact of Exhibition Scheduling on Museum Attendance},
year = {2019},
issue_date = {Fall 2019},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {21},
number = {4},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2018.0721},
doi = {10.1287/msom.2018.0721},
abstract = {Problem definition: Attendance at a museum fluctuates over time and is largely dependent on the exhibitions on display. Schedules can be adjusted to maximize the museum's objectives. Academic/practical relevance: In this paper, we build a model to study and manage the impact of exhibitions on the number of museum visitors. Methodology: We first estimate the model using data collected from two large museums in Barcelona. We then build an optimization framework in which the museum operator can choose the best duration and synchronization of exhibitions. Results: We find that, in addition to seasonality, exhibition type, display location, and life cycle affect attendance significantly. We characterize the optimal schedule: optimal durations increase with exhibition attractiveness and seasonality factors, and multiple exhibitions should be synchronized when there are no penalties from congestion. Managerial implications: Operations research techniques and in particular exhibition scheduling can be a valuable lever for museums to increase their impact on visitors.},
journal = {Manufacturing &amp; Service Operations Management},
month = oct,
pages = {761–779},
numpages = {19},
keywords = {content life cycle, consumer engagement, cultural content, art}
}

@inproceedings{10.1145/3302333.3302350,
author = {Garc\'{\i}a, Sergio and Str\"{u}ber, Daniel and Brugali, Davide and Di Fava, Alessandro and Schillinger, Philipp and Pelliccione, Patrizio and Berger, Thorsten},
title = {Variability Modeling of Service Robots: Experiences and Challenges},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302350},
doi = {10.1145/3302333.3302350},
abstract = {Sensing, planning, controlling, and reasoning, are human-like capabilities that can be artificially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specifically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specific variability challenges in robotics software engineering and to inspire other researchers to advance this field.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {6},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3319535.3339815,
author = {Cao, Yulong and Xiao, Chaowei and Cyr, Benjamin and Zhou, Yimeng and Park, Won and Rampazzi, Sara and Chen, Qi Alfred and Fu, Kevin and Mao, Z. Morley},
title = {Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3339815},
doi = {10.1145/3319535.3339815},
abstract = {In Autonomous Vehicles (AVs), one fundamental pillar is perception,which leverages sensors like cameras and LiDARs (Light Detection and Ranging) to understand the driving environment. Due to its direct impact on road safety, multiple prior efforts have been made to study its the security of perception systems. In contrast to prior work that concentrates on camera-based perception, in this work we perform the first security study of LiDAR-based perception in AV settings, which is highly important but unexplored. We consider LiDAR spoofing attacks as the threat model and set the attack goal as spoofing obstacles close to the front of a victim AV. We find that blindly applying LiDAR spoofing is insufficient to achieve this goal due to the machine learning-based object detection process.Thus, we then explore the possibility of strategically controlling the spoofed attack to fool the machine learning model. We formulate this task as an optimization problem and design modeling methods for the input perturbation function and the objective function.We also identify the inherent limitations of directly solving the problem using optimization and design an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75%. As a case study to understand the attack impact at the AV driving decision level, we construct and evaluate two attack scenarios that may damage road safety and mobility.We also discuss defense directions at the AV system, sensor, and machine learning model levels.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2267–2281},
numpages = {15},
keywords = {sensor attack, autonomous driving, adversarial machine learning},
location = {London, United Kingdom},
series = {CCS '19}
}

@article{10.1016/j.jss.2013.06.034,
author = {Alf\'{e}rez, G. H. and Pelechano, V. and Mazo, R. and Salinesi, C. and Diaz, D.},
title = {Dynamic adaptation of service compositions with variability models},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.034},
doi = {10.1016/j.jss.2013.06.034},
abstract = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
journal = {J. Syst. Softw.},
month = may,
pages = {24–47},
numpages = {24},
keywords = {Web service composition, Verification, Variability, Models at runtime, Dynamic software product line, Dynamic adaptation, Constraint programming, Autonomic computing}
}

@article{10.1016/j.eswa.2017.07.021,
author = {Zhou, Feng and Jiao, Jianxin Roger and Yang, Xi Jessie and Lei, Baiying},
title = {Augmenting feature model through customer preference mining by hybrid sentiment analysis},
year = {2017},
issue_date = {December 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {89},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.07.021},
doi = {10.1016/j.eswa.2017.07.021},
abstract = {We use sentiment analysis of online product reviewers to extract customer preference information.The proposed sentiment analysis method is a hybrid combination of various affective lexicons.We adopt the commented features from product users to enhance the basic feature.We incorporate the customer preference information as attribute into the model.We demonstrate the feasibility and potential of the proposed method via an application case. A feature model is an essential tool to identify variability and commonality within a product line of an enterprise, assisting stakeholders to configure product lines and to discover opportunities for reuse. However, the number of product variants needed to satisfy individual customer needs is still an open question, as feature models do not incorporate any direct customer preference information. In this paper, we propose to incorporate customer preference information into feature models using sentiment analysis of user-generated online product reviews. The proposed sentiment analysis method is a hybrid combination of affective lexicons and a rough-set technique. It is able to predict sentence sentiments for individual product features with acceptable accuracy, and thus augment a feature model by integrating positive and negative opinions of the customers. Such opinionated customer preference information is regarded as one attribute of the features, which helps to decide the number of variants needed within a product line. Finally, we demonstrate the feasibility and potential of the proposed method via an application case of Kindle Fire HD tablets.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {306–317},
numpages = {12},
keywords = {Sentiment analysis, Product line planning, Feature model, Customer preference mining}
}

@article{10.1016/j.engappai.2018.06.002,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Yu, Yang and Yuan, Bo-Wen and Tang, Jia-Fu},
title = {Integration of an improved dynamic ensemble selection approach to enhance one-vs-one scheme},
year = {2018},
issue_date = {Sep 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2018.06.002},
doi = {10.1016/j.engappai.2018.06.002},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {43–53},
numpages = {11},
keywords = {Multi-class classification, Decomposition strategy, One-vs-one, Heterogeneous ensemble, Dynamic selection}
}

@inproceedings{10.1007/978-3-031-04673-5_8,
author = {Ibias, Alfredo and Llana, Luis and N\'{u}\~{n}ez, Manuel},
title = {Using Ant Colony Optimisation to&nbsp;Select Features Having Associated Costs},
year = {2021},
isbn = {978-3-031-04672-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-04673-5_8},
doi = {10.1007/978-3-031-04673-5_8},
abstract = {Software Product Lines (SPLs) strongly facilitate the automation of software development processes. They combine features to create programs (called products) that fulfil certain properties. Testing SPLs is an intensive process where choosing the proper products to include in the testing process can be a critical task. In fact, selecting the best combination of features from an SPL is a complex problem that is frequently addressed in the literature. In this paper we use evolutionary algorithms to find a combination of features with low testing cost that include a target feature, to facilitate the integration testing of such feature. Specifically, we use an Ant Colony Optimisation algorithm to find one of the cheapest (in terms of testing) combination of features that contains a specific feature. Our results show that our framework overcomes the limitations of both brute force and random search algorithms.},
booktitle = {Testing Software and Systems: 33rd IFIP WG 6.1 International Conference, ICTSS 2021, London, UK, November 10–12, 2021, Proceedings},
pages = {106–122},
numpages = {17},
keywords = {Feature selection, Ant Colony Optimisation, Integration testing, Software Product Lines},
location = {London, United Kingdom}
}

@inproceedings{10.1145/3001867.3001874,
author = {Queiroz, Rodrigo and Berger, Thorsten and Czarnecki, Krzysztof},
title = {Towards predicting feature defects in software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001874},
doi = {10.1145/3001867.3001874},
abstract = {Defect-prediction techniques can enhance the quality assurance activities for software systems. For instance, they can be used to predict bugs in source files or functions. In the context of a software product line, such techniques could ideally be used for predicting defects in features or combinations of features, which would allow developers to focus quality assurance on the error-prone ones. In this preliminary case study, we investigate how defect prediction models can be used to identify defective features using machine-learning techniques. We adapt process metrics and evaluate and compare three classifiers using an open-source product line. Our results show that the technique can be effective. Our best scenario achieves an accuracy of 73 % for accurately predicting features as defective or clean using a Naive Bayes classifier. Based on the results we discuss directions for future work.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {58–62},
numpages = {5},
keywords = {software product lines, features, defect prediction},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1016/j.jss.2017.11.004,
author = {Carvalho, Michelle Larissa Luciano and da Silva, Matheus Lessa Gonalves and Gomes, Gecynalda Soares da Silva and Santos, Alcemir Rodrigues and Machado, Ivan do Carmo and Souza, Magno Lu de Jesus and de Almeida, Eduardo Santana},
title = {On the implementation of dynamic software product lines},
year = {2018},
issue_date = {February 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.11.004},
doi = {10.1016/j.jss.2017.11.004},
abstract = {A set of criteria to characterize mechanisms suitable to implement dynamic variability.A characterization of thirteen DSPL-ready variability mechanisms.Empirical evaluation of OOP and AOP from the perspective of DSPL evolution.Evidence showing that AOP is a feasible strategy to implement DSPL projects. Dynamic Software Product Line (DSPL) engineering is a paradigm aimed at handling adaptations at runtime. An inherent challenge in DSPL engineering is to reduce the design complexity of adaptable software, particularly in terms of evolution. Existing research only recently started to investigate evolution in this field, but does not assess the impact of different implementations under software quality in evolutionary scenarios. This work presents a characterization of thirteen dynamic variability mechanisms. Based on such characterization, we implemented a DSPL using Object-oriented Programming (OOP) mechanisms. From this implementation, we evidenced that DSPL requires changes and extensions to design, in terms of functionality and adaptation capabilities. Since Aspect-oriented Programming (AOP) was well ranked according to characterization and some studies have demonstrated the likely synergies between AOP and DSPL, we decided to compare it with OOP. We empirically evaluated how OOP and AOP could affect source code quality from the viewpoint of an evolving DSPL. As a result, AOP yields better results in terms of size, SoC, cohesion, and coupling measures. Conversely, AOP provides lower change propagation impact. Although the packages in AOP were more susceptible to changes than in OOP, we could indicate that AOP may be a feasible strategy for DSPL implementation.},
journal = {J. Syst. Softw.},
month = feb,
pages = {74–100},
numpages = {27},
keywords = {Variability mechanisms, Software evolution, Evidence-based software engineering, Dynamic software product lines}
}

@article{10.1287/msom.2019.0805,
author = {Mi\v{s}i\'{c}, Velibor V. and Perakis, Georgia},
title = {Data Analytics in Operations Management: A Review},
year = {2020},
issue_date = {January-February 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {22},
number = {1},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2019.0805},
doi = {10.1287/msom.2019.0805},
abstract = {Research in operations management has traditionally focused on models for understanding, mostly at a strategic level, how firms should operate. Spurred by the growing availability of data and recent advances in machine learning and optimization methodologies, there has been an increasing application of data analytics to problems in operations management. In this paper, we review recent applications of data analytics to operations management in three major areas—supply chain management, revenue management, and healthcare operations—and highlight some exciting directions for the future.},
journal = {Manufacturing &amp; Service Operations Management},
month = jan,
pages = {158–169},
numpages = {12},
keywords = {operations management, machine learning, data analytics}
}

@article{10.1016/j.future.2019.12.027,
author = {Al-Sayed, Mustafa M. and Hassan, Hesham A. and Omara, Fatma A.},
title = {An intelligent cloud service discovery framework},
year = {2020},
issue_date = {May 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.12.027},
doi = {10.1016/j.future.2019.12.027},
journal = {Future Gener. Comput. Syst.},
month = may,
pages = {438–466},
numpages = {29},
keywords = {Non-functional features, Functional features, Cloud service discovery, Information retrieval, OBDA, Cloud ontology, Cloud computing}
}

@inproceedings{10.1109/COASE.2019.8842887,
author = {Qian, Yu and Arinez, Jorge and Xiao, Guoxian and Chang, Qing},
title = {Improved Production Performance Through Manufacturing System Learning},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/COASE.2019.8842887},
doi = {10.1109/COASE.2019.8842887},
abstract = {With the advancement of computing technologies in the manufacturing domain, more information is available on factory intranets. This paper introduces a framework to optimize the performance of a production line through multiple plant comparisons and learning among identical or similar production lines by leveraging the information stored on the factory intranet. In this work, production data from multiple identical production lines are collected and analyzed. A fishbone diagram is introduced to help find differences amongst plants. By taking advantage of the abundant information from multiple plants, the “best” feasible action can be learned on critical machines which offers a new way to optimize the product line in addition to root cause analysis. To predict improvements, machine learning is used including preprocessing, model selection and validation. Consequently, a cost-benefit evaluation is provided to help decision making. A case study is performed based on an automotive industry scenario where the method is demonstrated and an increase in throughput is predicted.},
booktitle = {2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)},
pages = {517–522},
numpages = {6},
location = {Vancouver, BC, Canada}
}

@inproceedings{10.5555/3291291.3291298,
author = {Islam, Nayreet and Azim, Akramul},
title = {Assuring the runtime behavior of self-adaptive cyber-physical systems using feature modeling},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {A self-adaptive cyber-physical system (SACPS) can adjust its behavior and configurations at runtime in response to varying requirements obtained from the system and the environment. With the increasing use of the SACPS in different application domains, such variations are becoming more common. Users today expect the SACPS to guarantee its functional and timing behavior even in adverse environmental situations. However, uncertainties in the SACPS environment impose challenges on assuring the runtime behavior during system design.Software product line engineering (SPLE) is considered as a useful technique for handling varying requirements. In this paper, we present an approach for assuring the runtime behavior of the SACPS by applying an SPLE technique such as feature modeling. By representing the feature-based model at design time, we characterize the possible adaptation requirements to reusable configurations. The proposed approach aims to model two dynamic variability dimensions: 1) environment variability that describes the conditions under which the SACPS must adapt, and 2) structural variability, that defines the resulting architectural configurations. To validate our approach, the experimental analysis is performed using two case studies: 1) a traffic monitoring SACPS and 2) an automotive SACPS. We demonstrate that the proposed feature-based modeling approach can be used to achieve adaptivity which allows the SACPS to assure functional (defining execution of the correct set of adaptive tasks) and non-functional (defining execution of SACPS in the expected mode) correctness at runtime. The experimental results show that the feature-based SACPS demonstrates significant improvement in terms of self-configuration time, self-adaptation time and scalability with less probability of failure in different environmental situations.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {48–59},
numpages = {12},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@article{10.1145/3243316,
author = {Fan, Hehe and Zheng, Liang and Yan, Chenggang and Yang, Yi},
title = {Unsupervised Person Re-identification: Clustering and Fine-tuning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3243316},
doi = {10.1145/3243316},
abstract = {The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {83},
numpages = {18},
keywords = {unsupervised learning, convolutional neural network, clustering, Large-scale person re-identification}
}

@inproceedings{10.1145/3434780.3436640,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a Pe\~{n}alvo, Francisco Jos\'{e} and Theron, Roberto},
title = {Advances in the use of domain engineering to support feature identification and generation of information visualizations},
year = {2021},
isbn = {9781450388504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434780.3436640},
doi = {10.1145/3434780.3436640},
abstract = {Information visualization tools are widely used to better understand large and complex datasets. However, to make the most out of them, it is necessary to rely on proper designs that consider not only the data to be displayed, but also the audience and the context. There are tools that already allow users to configure their displays without requiring programming skills, but this research project aims at exploring the automatic generation of information visualizations and dashboards in order to avoid the configuration process, and select the most suitable features of these tools taking into account their contexts. To address this problem, a domain engineering, and machine learning approach is proposed.},
booktitle = {Eighth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1053–1056},
numpages = {4},
keywords = {Meta-modeling, Machine Learning, Information Dashboards, High-level requirements, Domain engineering, Automatic generation},
location = {Salamanca, Spain},
series = {TEEM'20}
}

@inproceedings{10.1145/3395260.3395275,
author = {Liu, Bingran},
title = {Neural Question Generation based on Seq2Seq},
year = {2020},
isbn = {9781450377072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395260.3395275},
doi = {10.1145/3395260.3395275},
abstract = {Neural Question Generation is the use of deep neural networks to extract target answers from a given article or paragraph and generate questions based on the target answers. There is a problem in the previous NQG(Neural Question Generation) model, and the generated question does not explicitly connect with the context in the target answer, resulting in a large part of the generated question containing the target answer and the accuracy is not high. In this paper, a QG model based on seq2seq is used, which consists of encode and decoder, and adds the attention mechanism and copy mechanism. We use special tags to replace the target answer of the original paragraph, and use the paragraph and target answer as input to reduce the number of incorrect questions, including the correct answer. Through the partial copy mechanism based on character overlap, we can make the generation problem have higher overlap and relevance at the word level and the input document. Experiments show that our proposed model performs better than before.},
booktitle = {Proceedings of the 2020 5th International Conference on Mathematics and Artificial Intelligence},
pages = {119–123},
numpages = {5},
keywords = {Seq2seq model, Question generation, Deep neural network},
location = {Chengdu, China},
series = {ICMAI '20}
}

@inproceedings{10.1007/978-3-030-30490-4_30,
author = {Tarjano, Carlos and Pereira, Valdecy},
title = {Neuro-Spectral Audio Synthesis: Exploiting Characteristics of the Discrete Fourier Transform in the Real-Time Simulation of Musical Instruments Using Parallel Neural Networks},
year = {2019},
isbn = {978-3-030-30489-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30490-4_30},
doi = {10.1007/978-3-030-30490-4_30},
abstract = {Two main approaches are currently prevalent in the digital emulation of musical instruments: manipulation of pre-recorded samples and techniques of real-time synthesis, generally based on physical models with varying degrees of accuracy. Concerning the first, while the processing power of present-day computers enables their use in real-time, many restrictions arising from this sample-based design persist; the huge on disk space requirements and the stiffness of musical articulations being the most prominent. On the other side of the spectrum, pure synthesis approaches, while offering greater flexibility, fail to capture and reproduce certain nuances central to the verisimilitude of the generated sound, offering a dry, synthetic output, at a high computational cost. We propose a method where ensembles of lightweight neural networks working in parallel are learned, from crafted frequency-domain features of an instrument sound spectra, an arbitrary instrument’s voice and articulations realistically and efficiently. We find that our method, while retaining perceptual sound quality on par with sampled approaches, exhibits 1/10 of latency times of industry standard real-time synthesis algorithms, and 1/100 of the disk space requirements of industry standard sample-based digital musical instruments. This method can, therefore, serve as a basis for more efficient implementations in dedicated devices, such as keyboards and electronic drumkits and in general purpose platforms, like desktops and tablets or open-source hardware like Arduino and Raspberry Pi. From a conceptual point of view, this work highlights the advantages of a closer integration of machine learning with other subjects, especially in the endeavor of new product development. Exploiting the synergy between neural networks, digital signal processing techniques and physical modelling, we illustrate the proposed method via the implementation of two virtual instruments: a conventional grand piano and a hibrid stringed instrument.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Text and Time Series: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17–19, 2019, Proceedings, Part IV},
pages = {362–375},
numpages = {14},
keywords = {Real-time audio synthesis, Digital musical instruments, Acoustic modeling, Neural networks},
location = {Munich, Germany}
}

@article{10.5555/3322706.3361988,
author = {Zhou, Zhixin and Amini, Arash A.},
title = {Analysis of spectral clustering algorithms for community detection: the general bipartite setting},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a k-means type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1774–1820},
numpages = {47},
keywords = {sub-Gaussian biclustering, stochastic block model, spectral clustering, regularization of random graphs, graphon clustering, community detection, bipartite networks}
}

@inproceedings{10.5555/1885639.1885667,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Gasevic, Dragan and Soltani, Samaneh},
title = {Stratified analytic hierarchy process: prioritization and selection of software features},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product line engineering allows for the rapid development of variants of a domain specific application by using a common set of reusable assets often known as core assets. Variability modeling is a critical issue in product line engineering, where the use of feature modeling is one of most commonly used formalisms. To support an effective and automated derivation of concrete products for a product family, staged configuration has been proposed in the research literature. In this paper, we propose the integration of well-known requirements engineering principles into stage configuration. Being inspired by the well-established Preview requirements engineering framework, we initially propose an extension of feature models with capabilities for capturing business oriented requirements. This representation enables a more effective capturing of stakeholders' preferences over the business requirements and objectives (e.g.,. implementation costs or security) in the form of fuzzy linguistic variables (e.g., high, medium, and low). On top of this extension, we propose a novel method, the Stratified Analytic Hierarchy process, which first helps to rank and select the most relevant high level business objectives for the target stakeholders (e.g., security over implementation costs), and then helps to rank and select the most relevant features from the feature model to be used as the starting point in the staged configuration process. Besides a complete formalization of the process, we define the place of our proposal in existing software product line lifecycles as well as demonstrate the use of our proposal on the widely-used e-Shop case study. Finally, we report on the results of our user study, which indicates a high appreciation of the proposed method by the participating industrial software developers. The tool support for S-AHP is also introduced.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {300–315},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1007/s10270-020-00814-5,
author = {Mussbacher, Gunter and Combemale, Benoit and Kienzle, J\"{o}rg and Abrah\~{a}o, Silvia and Ali, Hyacinth and Bencomo, Nelly and B\'{u}r, M\'{a}rton and Burgue\~{n}o, Loli and Engels, Gregor and Jeanjean, Pierre and J\'{e}z\'{e}quel, Jean-Marc and K\"{u}hn, Thomas and Mosser, S\'{e}bastien and Sahraoui, Houari and Syriani, Eugene and Varr\'{o}, D\'{a}niel and Weyssow, Martin},
title = {Opportunities in intelligent modeling assistance},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00814-5},
doi = {10.1007/s10270-020-00814-5},
abstract = {Modeling is requiring increasingly larger efforts while becoming indispensable given the complexity of the problems we are solving. Modelers face high cognitive load to understand a multitude of complex abstractions and their relationships. There is an urgent need to better support tool builders to ultimately provide modelers with intelligent modeling assistance that learns from previous modeling experiences, automatically derives modeling knowledge, and provides context-aware assistance. However, current intelligent modeling assistants (IMAs) lack adaptability and flexibility for tool builders, and do not facilitate understanding the differences and commonalities of IMAs for modelers. Such a patchwork of limited IMAs is a lost opportunity to provide modelers with better support for the creative and rigorous aspects of software engineering. In this expert voice, we present a conceptual reference framework (RF-IMA) and its properties to identify the foundations for intelligent modeling assistance. For tool builders, RF-IMA aims to help build IMAs more systematically. For modelers, RF-IMA aims to facilitate comprehension, comparison, and integration of IMAs, and ultimately to provide more intelligent support. We envision a momentum in the modeling community that leads to the implementation of RF-IMA and consequently future IMAs. We identify open challenges that need to be addressed to realize the opportunities provided by intelligent modeling assistance.},
journal = {Softw. Syst. Model.},
month = sep,
pages = {1045–1053},
numpages = {9},
keywords = {Feedback, Development data, Artificial intelligence, Integrated development environment, Intelligent modeling assistance, Model-based software engineering}
}

@article{10.1016/j.ins.2018.06.014,
author = {Ma, Zilu and Liu, Shiqi and Meng, Deyu and Zhang, Yong and Lo, SioLong and Han, Zhi},
title = {On Convergence Properties of Implicit Self-paced Objective},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {462},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.06.014},
doi = {10.1016/j.ins.2018.06.014},
journal = {Inf. Sci.},
month = sep,
pages = {132–140},
numpages = {9},
keywords = {99-00, 00-01, Convergence, Non-convex optimization, Machine learning, Self-paced learning}
}

@inproceedings{10.1109/MODELS.2017.22,
author = {Taentzer, Gabriele and Salay, Rick and Str\"{u}ber, Daniel and Chechik, Marsha},
title = {Transformations of software product lines: a generalizing framework based on category theory},
year = {2017},
isbn = {9781538634929},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS.2017.22},
doi = {10.1109/MODELS.2017.22},
abstract = {Software product lines are used to manage the development of highly complex software with many variants. In the literature, various forms of rule-based product line modifications have been considered. However, when considered in isolation, their expressiveness for specifying combined modifications of feature models and domain models is limited. In this paper, we present a formal framework for product line transformations that is able to combine several kinds of product line modifications presented in the literature. Moreover, it defines new forms of product line modifications supporting various forms of product lines and transformation rules. Our formalization of product line transformations is based on category theory, and concentrates on properties of product line relations instead of their single elements. Our framework provides improved expressiveness and flexibility of software product line transformations while abstracting from the considered type of model.},
booktitle = {Proceedings of the ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems},
pages = {101–111},
numpages = {11},
location = {Austin, Texas},
series = {MODELS '17}
}

@article{10.1016/j.datak.2010.01.002,
author = {Reinhartz-Berger, Iris},
title = {Towards automatization of domain modeling},
year = {2010},
issue_date = {May, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {69},
number = {5},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2010.01.002},
doi = {10.1016/j.datak.2010.01.002},
abstract = {A domain model, which captures the common knowledge and the possible variability allowed among applications in a domain, may assist in the creation of other valid applications in that domain. However, to create such domain models is not a trivial task: it requires expertise in the domain, reaching a very high level of abstraction, and providing flexible, yet formal, artifacts. In this paper an approach, called Semi-automated Domain Modeling (SDM), to create draft domain models from applications in those domains, is presented. SDM takes a repository of application models in a domain and matches, merges, and generalizes them into sound draft domain models that include the commonality and variability allowed in these domains. The similarity of the different elements is measured, with consideration of syntactic, semantic, and structural aspects. Unlike ontology and schema integration, these models capture both structural and behavioral aspects of the domain. Running SDM on small repositories of project management applications and scheduling systems, we found that the approach may provide reasonable draft domain models, whose comprehensibility, correctness, completeness, and consistency levels are satisfactory.},
journal = {Data Knowl. Eng.},
month = may,
pages = {491–515},
numpages = {25},
keywords = {UML, Product line engineering, Metamodeling, Domain engineering, Domain analysis, DSL}
}

@inproceedings{10.1007/978-3-642-30829-1_6,
author = {Khosravi, Ramtin and Sabouri, Hamideh},
title = {Using coordinated actors to model families of distributed systems},
year = {2012},
isbn = {9783642308284},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30829-1_6},
doi = {10.1007/978-3-642-30829-1_6},
abstract = {Software product line engineering enables strategic reuse in development of families of related products. In a component-based approach to product line development, components capture functionalities appearing in one or more products in the family and different assemblies of components yield to various products or configurations. In this approach, an interaction model which effectively factors out the logic handling variability from the functionality of the system greatly enhances the reusability of components. We study the problem of variability modeling for a family of distributed systems expressed in actor model. We define a special type of actors called coordinators whose behavior is described as Reo circuits with the aim of encapsulating the variability logic. We have the benefits of Reo language for expressing coordination logic, while modeling the entire system as an actor-based distributed model. We have applied this model to a case study extracted from an industrial software family in the domain of interactive TV.},
booktitle = {Proceedings of the 14th International Conference on Coordination Models and Languages},
pages = {74–88},
numpages = {15},
location = {Stockholm, Sweden},
series = {COORDINATION'12}
}

@article{10.1016/j.specom.2019.10.003,
author = {Stasak, Brian and Epps, Julien and Goecke, Roland},
title = {Automatic depression classification based on affective read sentences: Opportunities for text-dependent analysis},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.003},
doi = {10.1016/j.specom.2019.10.003},
journal = {Speech Commun.},
month = dec,
pages = {1–14},
numpages = {14},
keywords = {Valence, Speech elicitation, Machine learning, Paralinguistics, Digital medicine, Digital phenotyping}
}

@article{10.1109/TCBB.2015.2476790,
author = {Deng, Su-Ping and Zhu, Lin and Huang, De-Shuang},
title = {Predicting hub genes associated with cervical cancer through gene co-expression networks},
year = {2016},
issue_date = {January/February 2016},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {13},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2476790},
doi = {10.1109/TCBB.2015.2476790},
abstract = {Cervical cancer is the third most common malignancy in women worldwide. It remains a leading cause of cancer-related death for women in developing countries. In order to contribute to the treatment of the cervical cancer, in our work, we try to find a few key genes resulting in the cervical cancer. Employing functions of several bioinformatics tools, we selected 143 differentially expressed genes (DEGs) associated with the cervical cancer. The results of bioinformatics analysis show that these DEGs play important roles in the development of cervical cancer. Through comparing two differential co-expression networks (DCNs) at two different states, we found a common sub-network and two differential sub-networks as well as some hub genes in three sub-networks. Moreover, some of the hub genes have been reported to be related to the cervical cancer. Those hub genes were analyzed from Gene Ontology function enrichment, pathway enrichment and protein binding three aspects. The results can help us understand the development of the cervical cancer and guide further experiments about the cervical cancer.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {27–35},
numpages = {9},
keywords = {hub genes, differentially expressed genes, co-expression network, cervical cancer}
}

@article{10.1016/j.engappai.2015.04.001,
author = {Kwong, C.K. and Jiang, Huimin and Luo, X.G.},
title = {AI-based methodology of integrating affective design, engineering, and marketing for defining design specifications of new products},
year = {2016},
issue_date = {January 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {47},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2015.04.001},
doi = {10.1016/j.engappai.2015.04.001},
abstract = {In the early stage of product design, particularly for consumer products, affective design, engineering, and marketing issues must be taken into considerationand they are commonly performed respectively by product designers, engineers, and marketing personnel. However, they have different concerns and focuses with regard to the new product design. Thus, these three processes are commonly conducted separately, leading to a sub-optimal and even sub-standard design. Such scenario indicates the need to incorporate the concerns of the three processes in the early stage of product design. However, no study has explored the incorporation of the concerns of the three processes into the product design. In this paper, an artificial intelligence (AI)-based methodology for integrating affective design, engineering, and marketing for defining design specifications of new products is proposed by which the concerns of the three processes can be considered simultaneously in the early design stage. The proposed methodology mainly involves development of customer satisfaction and cost models using fuzzy regression, generation of product utility functions using chaos-based fuzzy regression, formulation of a multi-objective optimization model and its solving using a non-dominated sorting genetic algorithm-II (NSGA-II). A case study was conducted for electric iron design to evaluate the effectiveness of the proposed methodology.},
journal = {Eng. Appl. Artif. Intell.},
month = jan,
pages = {49–60},
numpages = {12},
keywords = {NSGA-II, Marketing, Fuzzy regression, Chaos optimization algorithm, Affective design}
}

@inproceedings{10.1007/978-3-030-67658-2_34,
author = {Yamaguchi, Akihiro and Maya, Shigeru and Ueno, Ken},
title = {RLTS: Robust Learning Time-Series Shapelets},
year = {2020},
isbn = {978-3-030-67657-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67658-2_34},
doi = {10.1007/978-3-030-67658-2_34},
abstract = {Shapelets are time-series segments effective for classifying time-series instances. Joint learning of both classifiers and shapelets has been studied in recent years because such a method provides both superior classification performance and interpretable results. For robust learning, we introduce Self-Paced Learning (SPL) and adaptive robust losses into this method. The SPL method can assign latent instance weights by considering not only classification losses but also understandable shapelet discovery. Furthermore, the adaptive robustness introduced into feature vectors is jointly learned with shapelets, a classifier, and latent instance weights. We demonstrate the superiority of AUC and the validity of our approach on UCR time-series datasets.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part I},
pages = {595–611},
numpages = {17},
keywords = {Time-series shapelets, Self-paced learning, Robust losses},
location = {Ghent, Belgium}
}

@inproceedings{10.1007/978-3-319-24888-2_3,
author = {Ma, Guangkai and Gao, Yaozong and Wang, Li and Wu, Ligang and Shen, Dinggang},
title = {Soft-Split Random Forest for Anatomy Labeling},
year = {2015},
isbn = {978-3-319-24887-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-24888-2_3},
doi = {10.1007/978-3-319-24888-2_3},
abstract = {Random Forest (RF) has been widely used in the learning-based labeling. In RF, each sample is directed from the root to each leaf based on the decisions made in the interior nodes, also called splitting nodes. The splitting nodes assign a testing sample to either left or right child based on the learned splitting function. The final prediction is determined as the average of label probability distributions stored in all arrived leaf nodes. For ambiguous testing samples, which often lie near the splitting boundaries, the conventional splitting function, also referred to as hard split function, tends to make wrong assignments, hence leading to wrong predictions. To overcome this limitation, we propose a novel soft-split random forest (SSRF) framework to improve the reliability of node splitting and finally the accuracy of classification. Specifically, a soft split function is employed to assign a testing sample into both left and right child nodes with their certain probabilities, which can effectively reduce influence of the wrong node assignment on the prediction accuracy. As a result, each testing sample can arrive at multiple leaf nodes, and their respective results can be fused to obtain the final prediction according to the weights accumulated along the path from the root node to each leaf node. Besides, considering the importance of context information, we also adopt a Haar-features based context model to iteratively refine the classification map. We have comprehensively evaluated our method on two public datasets, respectively, for labeling hippocampus in MR images and also labeling three organs in Head &amp; Neck CT images. Compared with the hard-split RF (HSRF), our method achieved a notable improvement in labeling accuracy.},
booktitle = {Machine Learning in Medical Imaging: 6th International Workshop, MLMI 2015, Held in Conjunction with MICCAI 2015, Munich, Germany, October 5, 2015, Proceedings},
pages = {17–25},
numpages = {9},
location = {Munich, Germany}
}

@inproceedings{10.1007/978-3-030-87196-3_28,
author = {Wu, Yicheng and Xu, Minfeng and Ge, Zongyuan and Cai, Jianfei and Zhang, Lei},
title = {Semi-supervised Left Atrium Segmentation with Mutual Consistency&nbsp;Training},
year = {2021},
isbn = {978-3-030-87195-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87196-3_28},
doi = {10.1007/978-3-030-87196-3_28},
abstract = {Semi-supervised learning has attracted great attention in the field of machine learning, especially for medical image segmentation tasks, since it alleviates the heavy burden of collecting abundant densely annotated data for training. However, most of existing methods underestimate the importance of challenging regions (e.g. small branches or blurred edges) during training. We believe that these unlabeled regions may contain more crucial information to minimize the uncertainty prediction for the model and should be emphasized in the training process. Therefore, in this paper, we propose a novel Mutual Consistency Network (MC-Net) for semi-supervised left atrium segmentation from 3D MR images. Particularly, our MC-Net consists of one encoder and two slightly different decoders, and the prediction discrepancies of two decoders are transformed as an unsupervised loss by our designed cycled pseudo label scheme to encourage mutual consistency. Such mutual consistency encourages the two decoders to have consistent and low-entropy predictions and enables the model to gradually capture generalized features from these unlabeled challenging regions. We evaluate our MC-Net on the public Left Atrium (LA) database and it obtains impressive performance gains by exploiting the unlabeled data effectively. Our MC-Net outperforms six recent semi-supervised methods for left atrium segmentation, and sets the new state-of-the-art performance on the LA database.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II},
pages = {297–306},
numpages = {10},
keywords = {Cycled pseudo label, Mutual consistency, Semi-supervised learning},
location = {Strasbourg, France}
}

@article{10.1007/s00034-021-01657-1,
author = {Pravin, Sheena Christabel and Palanivelan, M.},
title = {A Hybrid Deep Ensemble for Speech Disfluency Classification},
year = {2021},
issue_date = {Aug 2021},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {40},
number = {8},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-021-01657-1},
doi = {10.1007/s00034-021-01657-1},
abstract = {In this paper, a novel Hybrid Deep Ensemble (HDE) is proposed for automatic speech disfluency classification on a sparse speech dataset. Categorizations of speech disfluencies for diagnosis of speech disorders have so long relied on sophisticated deep learning models. Such a task can be accomplished by a straightforward approach with high accuracy by the proposed model which is an optimal combination of diverse machine learning and deep learning algorithms in a hierarchical arrangement which includes a deep autoencoder that yields the compressed latent features. The proposed model has shown considerable improvement in downgrading processing time overcoming the issues of cumbersome hyper-parameter tuning and huge data demand of the deep learning algorithms with high classification accuracy. Experimental results show that the proposed Hybrid Deep Ensemble has superior performance compared to the individual base learners, and the deep neural network as well. The proposed model and the baseline models were evaluated in terms of Cohen’s kappa coefficient, Hamming loss, Jaccard score, F-score and classification accuracy.},
journal = {Circuits Syst. Signal Process.},
month = aug,
pages = {3968–3995},
numpages = {28},
keywords = {Latent features, Deep autoencoder, Sparse speech dataset, Speech disfluency classification, Hybrid Deep Ensemble}
}

@inproceedings{10.1145/3368826.3377923,
author = {Shaikhha, Amir and Schleich, Maximilian and Ghita, Alexandru and Olteanu, Dan},
title = {Multi-layer optimizations for end-to-end data analytics},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377923},
doi = {10.1145/3368826.3377923},
abstract = {We consider the problem of training machine learning models over multi-relational data. The mainstream approach is to first construct the training dataset using a feature extraction query over input database and then use a statistical software package of choice to train the model. In this paper we introduce Iterative Functional Aggregate Queries (IFAQ), a framework that realizes an alternative approach. IFAQ treats the feature extraction query and the learning task as one program given in the IFAQ's domain-specific language, which captures a subset of Python commonly used in Jupyter notebooks for rapid prototyping of machine learning applications. The program is subject to several layers of IFAQ optimizations, such as algebraic transformations, loop transformations, schema specialization, data layout optimizations, and finally compilation into efficient low-level C++ code specialized for the given workload and data.  We show that a Scala implementation of IFAQ can outperform mlpack, Scikit, and TensorFlow by several orders of magnitude for linear regression and regression tree models over several relational datasets.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {145–157},
numpages = {13},
keywords = {In-Database Machine Learning, Multi-Query Optimization, Query Compilation},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@inproceedings{10.5555/3504035.3504406,
author = {Gong, Tieliang and Wang, Guangtao and Ye, Jieping and Xu, Zongben and Lin, Ming},
title = {Margin based PU learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {The PU learning problem concerns about learning from positive and unlabeled data. A popular heuristic is to iteratively enlarge training set based on some margin-based criterion. However, little theoretical analysis has been conducted to support the success of these heuristic methods. In this work, we show that not all margin-based heuristic rules are able to improve the learned classifiers iteratively. We find that a so-called large positive margin oracle is necessary to guarantee the success of PU learning. Under this oracle, a provable positive-margin based PU learning algorithm is proposed for linear regression and classification under the truncated Gaussian distributions. The proposed algorithm is able to reduce the recovering error geometrically proportional to the positive margin. Extensive experiments on real-world datasets verify our theory and the state-of-the-art performance of the proposed PU learning algorithm.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {371},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1016/j.infsof.2019.05.009,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh and Marston, Chad},
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.009},
doi = {10.1016/j.infsof.2019.05.009},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {131–145},
numpages = {15},
keywords = {Case study, User trust, Business-to-business, Machine learning, Big data}
}

@inproceedings{10.1145/3180155.3180257,
author = {Xue, Yinxing and Li, Yan-Fu},
title = {Multi-objective integer programming approaches for solving optimal feature selection problem: a new perspective on multi-objective optimization problems in SBSE},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180257},
doi = {10.1145/3180155.3180257},
abstract = {The optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study we first expose the mathematical nature of this problem --- multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution).},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1231–1242},
numpages = {12},
keywords = {multi-objective integer programming (MOIP), multi-objective optimization (MOO), optimal feature selection problem},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3426020.3426039,
author = {Nguyen, Huy Toan and Shin, Nu-ri and Yu, Gwang-Hyun and Kwon, Gyeong-Ju and Kwak, Woo-Young and Kim, Jin-Young},
title = {Deep learning-based defective product classification system for smart factory},
year = {2021},
isbn = {9781450389259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426020.3426039},
doi = {10.1145/3426020.3426039},
abstract = {In this paper, the defective product classification based on deep learning for a smart factory is introduced. The proposed system contains PLC (Programmable Logic Controller), Artificial Intelligence (AI) embedded board and cloud service. The AI embedded board is connected and communicated to receive and send commands to PLC via SPI (Serial Peripheral Interface) protocol. The pre-trained defective product classification model is uploaded, saved on a cloud server and downloaded to AI Embedded board for each particular product. The core technique of the system is the AI-based embedded board. Due to the limitation of label data, we use transfer learning method to retrain deep neural networks (DNN). We implement and compare the classification results on different deep neural network including ResNet, DenseNet, and GoogLeNet. We trained these networks by GPU server on casting product classification data. After that, the pre-trained models are optimized and applied on practical embedded board. The experimental results show that our system is able to classify defective products with high accuracy and fast speed.},
booktitle = {The 9th International Conference on Smart Media and Applications},
pages = {80–85},
numpages = {6},
keywords = {defective product classification, deep learning, Smart factory},
location = {Jeju, Republic of Korea},
series = {SMA 2020}
}

@inproceedings{10.1145/3194133.3194143,
author = {Olaechea, Rafael and Atlee, Joanne and Legay, Axel and Fahrenberg, Uli},
title = {Trace checking for dynamic software product lines},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194143},
doi = {10.1145/3194133.3194143},
abstract = {A key objective of self-adaptive systems is to continue to provide optimal quality of service when the environment changes. A dynamic software product line (DSPL) can benefit from knowing how its various product variants would have performed (in terms of quality of service) with respect to the recent history of inputs. We propose a family-based analysis that simulates all the product variants of a DSPL simultaneously, at runtime, on recent environmental inputs to obtain an estimate of the quality of service that each one of the product variants would have had, provided it had been executing. We assessed the efficiency of our DSPL analysis compared to the efficiency of analyzing each product individually on three case studies. We obtained mixed results due to the explosion of quality-of-service values for the product variants of a DSPL. After introducing a simple data abstraction on the values of quality-of- service variables, our DSPL analysis is between 1.4 and 7.7 times faster than analyzing the products one at a time.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {69–75},
numpages = {7},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@inproceedings{10.1109/ASE.2015.16,
author = {Kowal, Matthias and Tschaikowski, Max and Tribastone, Mirco and Schaefer, Ina},
title = {Scaling size and parameter spaces in variability-aware software performance models},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.16},
doi = {10.1109/ASE.2015.16},
abstract = {In software performance engineering, what-if scenarios, architecture optimization, capacity planning, run-time adaptation, and uncertainty management of realistic models typically require the evaluation of many instances. Effective analysis is however hindered by two orthogonal sources of complexity. The first is the infamous problem of state space explosion---the analysis of a single model becomes intractable with its size. The second is due to massive parameter spaces to be explored, but such that computations cannot be reused across model instances. In this paper, we efficiently analyze many queuing models with the distinctive feature of more accurately capturing variability and uncertainty of execution rates by incorporating general (i.e., non-exponential) distributions. Applying product-line engineering methods, we consider a family of models generated by a core that evolves into concrete instances by applying simple delta operations affecting both the topology and the model's parameters. State explosion is tackled by turning to a scalable approximation based on ordinary differential equations. The entire model space is analyzed in a family-based fashion, i.e., at once using an efficient symbolic solution of a super-model that subsumes every concrete instance. Extensive numerical tests show that this is orders of magnitude faster than a naive instance-by-instance analysis.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {407–417},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/2739482.2764681,
author = {Martinez, Jabier and Rossi, Gabriele and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} Fran\c{c}ois D. Assise and Klein, Jacques and Le Traon, Yves},
title = {Estimating and Predicting Average Likability on Computer-Generated Artwork Variants},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764681},
doi = {10.1145/2739482.2764681},
abstract = {Computer assisted human creativity encodes human design decisions in algorithms allowing machines to produce artwork variants. Based on this automated production, one can leverage collective understanding of beauty to rank computer-generated artworks according to their average likability. We present the use of Software Product Line techniques for computer-generated art systems as a case study on leveraging the feedback of human perception within the boundaries of a variability model. Since it is not feasible to get feedback for all variants because of a combinatorial explosion of possible configurations, we propose an approach that is developed in two phases: 1) the creation of a data set using an interactive genetic algorithm and 2) the application of a data mining technique on this dataset to create a ranking enriched with confidence metrics.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1431–1432},
numpages = {2},
keywords = {software product lines, media arts, gentic algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1016/j.sigpro.2019.107332,
author = {Shi, Caijuan and Gu, Zhibin and Duan, Changyu and Tian, Qi},
title = {Multi-view adaptive semi-supervised feature selection with the self-paced learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {168},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2019.107332},
doi = {10.1016/j.sigpro.2019.107332},
journal = {Signal Process.},
month = mar,
numpages = {11},
keywords = {Multi-view learning, Semi-supervised feature selection, Self-paced learning, Graph-based semi-supervised learning}
}

@inproceedings{10.1145/3093241.3093268,
author = {Smith, Jeffrey and Rege, Manjeet},
title = {The Data Warehousing (R) Evolution: Where's it headed next?},
year = {2017},
isbn = {9781450352413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093241.3093268},
doi = {10.1145/3093241.3093268},
abstract = {This paper provides an overview of the history and current state of data warehousing and corporate analytics. It begins with a quick review of the history of the data warehouse and then does a deeper dive into subsets of this space including data integration, the DBMS, business intelligence and analytics, advanced analytics, and information stewardship. It finishes with a quick review of some of the leading trends in data warehousing including Big Data and the Logical Data Warehouse, Hybrid Transaction Analytical Processing and In-Memory Computing.},
booktitle = {Proceedings of the International Conference on Compute and Data Analysis},
pages = {104–108},
numpages = {5},
keywords = {warehouse, intelligence, business, ETL, Data},
location = {Lakeland, FL, USA},
series = {ICCDA '17}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {sampling, machine learning, Performance-influence models},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1609/aaai.v33i01.33015117,
author = {Tang, Ying-Peng and Huang, Sheng-Jun},
title = {Self-paced active learning: query the right thing at the right time},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015117},
doi = {10.1609/aaai.v33i01.33015117},
abstract = {Active learning queries labels from the oracle for the most valuable instances to reduce the labeling cost. In many active learning studies, informative and representative instances are preferred because they are expected to have higher potential value for improving the model. Recently, the results in self-paced learning show that training the model with easy examples first and then gradually with harder examples can improve the performance. While informative and representative instances could be easy or hard, querying valuable but hard examples at early stage may lead to waste of labeling cost. In this paper, we propose a self-paced active learning approach to simultaneously consider the potential value and easiness of an instance, and try to train the model with least cost by querying the right thing at the right time. Experimental results show that the proposed approach is superior to state-of-the-art batch mode active learning methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {628},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1016/j.jss.2014.10.037,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Galindo, Jos\'{e} A. and Parejo, Jos\'{e} A. and Benavides, David and Segura, Sergio and Egyed, Alexander},
title = {An assessment of search-based techniques for reverse engineering feature models},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.10.037},
doi = {10.1016/j.jss.2014.10.037},
abstract = {HighlightsSearch based techniques perform well for reverse engineering feature models.Different algorithms and objectives favour precision and recall differently.The F1 objective function provides a trade-off between precision and recall. Successful software evolves from a single system by adding and changing functionality to keep up with users' demands and to cater to their similar and different requirements. Nowadays it is a common practice to offer a system in many variants such as community, professional, or academic editions. Each variant provides different functionality described in terms of features. Software Product Line Engineering (SPLE) is an effective software development paradigm for this scenario. At the core of SPLE is variability modelling whose goal is to represent the combinations of features that distinguish the system variants using feature models, the de facto standard for such task. As SPLE practices are becoming more pervasive, reverse engineering feature models from the feature descriptions of each individual variant has become an active research subject. In this paper we evaluated, for this reverse engineering task, three standard search based techniques (evolutionary algorithms, hill climbing, and random search) with two objective functions on 74 SPLs. We compared their performance using precision and recall, and found a clear trade-off between these two metrics which we further reified into a third objective function based on Fβ, an information retrieval measure, that showed a clear performance improvement. We believe that this work sheds light on the great potential of search-based techniques for SPLE tasks.},
journal = {J. Syst. Softw.},
month = may,
pages = {353–369},
numpages = {17},
keywords = {Search Based Software Engineering, Reverse engineering, Feature model}
}

@inproceedings{10.1007/978-3-030-77385-4_42,
author = {Halilaj, Lavdim and Dindorkar, Ishan and L\"{u}ttin, J\"{u}rgen and Rothermel, Susanne},
title = {A Knowledge Graph-Based Approach for Situation Comprehension in Driving Scenarios},
year = {2021},
isbn = {978-3-030-77384-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77385-4_42},
doi = {10.1007/978-3-030-77385-4_42},
abstract = {Making an informed and right decision poses huge challenges for drivers in day-to-day traffic situations. This task vastly depends on many subjective and objective factors, including the current driver state, her destination, personal preferences and abilities as well as surrounding environment. In this paper, we present CoSI (Context and Situation Intelligence), a Knowledge Graph (KG)-based approach for fusing and organizing heterogeneous types and sources of information. The KG serves as a coherence layer representing information in the form of entities and their inter-relationships augmented with additional semantic axioms. Harnessing the power of axiomatic rules and reasoning capabilities enables inferring additional knowledge from what is already encoded. Thus, dedicated components exploit and consume the semantically enriched information to perform tasks such as situation classification, difficulty assessment, and trajectory prediction. Further, we generated a synthetic dataset to simulate real driving scenarios with a large range of driving styles and vehicle configurations. We use KG embedding techniques based on a Graph Neural Network (GNN) architecture for a classification task of driving situations and achieve over 95% accuracy whereas vector-based approaches achieve only 75% accuracy for the same task. The results suggest that the KG-based information representation combined with GNN are well suited for situation understanding tasks as required in driver assistance and automated driving systems.},
booktitle = {The Semantic Web: 18th International Conference, ESWC 2021, Virtual Event, June 6–10, 2021, Proceedings},
pages = {699–716},
numpages = {18},
keywords = {Graph neural network, Knowledge graph embedding, Knowledge graph, Situation comprehension}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature model extraction from large collections of informal product descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Product Lines, Feature Models, Domain Analysis},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1155/2021/5089236,
author = {Chen, Yu and Tang, Zhong and Ding, Baiyuan},
title = {Research on the Construction of Intelligent Community Emergency Service Platform Based on Convolutional Neural Network},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/5089236},
doi = {10.1155/2021/5089236},
abstract = {Aiming at the shortcomings of the existing community emergency service platform, such as single function, poor scalability, and strong subjectivity, an intelligent community emergency service platform based on convolutional neural network was constructed. Firstly, the requirements analysis of the emergency service platform was carried out, and the functional demand of the emergency service platform was analyzed from the aspects of community environment, safety, infrastructure, health management, emergency response, and so on. Secondly, through logistics network, big data, cloud computing, artificial intelligence, and all kinds of applications, the intelligent community emergency service platform was designed. Finally, a semantic matching emergency question answering system based on convolutional neural network was developed to provide key technical support for the emergency preparation stage of intelligent community. The results show that the intelligent community emergency service platform plays an important role in preventing community emergency events and taking active and effective measures to ensure the health and safety of community residents.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@inproceedings{10.1145/3127479.3131621,
author = {Traub, Jonas and Bre\ss{}, Sebastian and Rabl, Tilmann and Katsifodimos, Asterios and Markl, Volker},
title = {Optimized on-demand data streaming from sensor nodes},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3131621},
doi = {10.1145/3127479.3131621},
abstract = {Real-time sensor data enables diverse applications such as smart metering, traffic monitoring, and sport analysis. In the Internet of Things, billions of sensor nodes form a sensor cloud and offer data streams to analysis systems. However, it is impossible to transfer all available data with maximal frequencies to all applications. Therefore, we need to tailor data streams to the demand of applications.We contribute a technique that optimizes communication costs while maintaining the desired accuracy. Our technique schedules reads across huge amounts of sensors based on the data-demands of a huge amount of concurrent queries. We introduce user-defined sampling functions that define the data-demand of queries and facilitate various adaptive sampling techniques, which decrease the amount of transferred data. Moreover, we share sensor reads and data transfers among queries. Our experiments with real-world data show that our approach saves up to 87% in data transmissions.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {586–597},
numpages = {12},
keywords = {user-defined sampling, sensor sharing, sensor data, real-time analysis, oversampling, on-demand streaming, adaptive sampling},
location = {Santa Clara, California},
series = {SoCC '17}
}

@article{10.5555/2946645.3007037,
author = {Wei, Ermo and Luke, Sean},
title = {Lenient learning in independent-learner stochastic cooperative games},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We introduce the Lenient Multiagent Reinforcement Learning 2 (LMRL2) algorithm for independent-learner stochastic cooperative games. LMRL2 is designed to overcome a pathology called relative overgeneralization, and to do so while still performing well in games with stochastic transitions, stochastic rewards, and miscoordination. We discuss the existing literature, then compare LMRL2 against other algorithms drawn from the literature which can be used for games of this kind: traditional ("Distributed") Q-learning, Hysteretic Q-learning, WoLF-PHC, SOoN, and (for repeated games only) FMQ. The results show that LMRL2 is very effective in both of our measures (complete and correct policies), and is found in the top rank more often than any other technique. LMRL2 is also easy to tune: though it has many available parameters, almost all of them stay at default settings. Generally the algorithm is optimally tuned with a single parameter, if any. We then examine and discuss a number of side-issues and options for LMRL2.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2914–2955},
numpages = {42},
keywords = {reinforcement learning, multiagent learning, lenient learning, independent learner, game theory}
}

@article{10.1007/s11219-014-9258-y,
author = {Galindo, Jos\'{e} A. and Turner, Hamilton and Benavides, David and White, Jules},
title = {Testing variability-intensive systems using automated analysis: an application to Android},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-014-9258-y},
doi = {10.1007/s11219-014-9258-y},
abstract = {Software product lines are used to develop a set of software products that, while being different, share a common set of features. Feature models are used as a compact representation of all the products (e.g., possible configurations) of the product line. The number of products that a feature model encodes may grow exponentially with the number of features. This increases the cost of testing the products within a product line. Some proposals deal with this problem by reducing the testing space using different techniques. However, a daunting challenge is to explore how the cost and value of test cases can be modeled and optimized in order to have lower-cost testing processes. In this paper, we present TESting vAriAbiLity Intensive Systems (TESALIA), an approach that uses automated analysis of feature models to optimize the testing of variability-intensive systems. We model test value and cost as feature attributes, and then we use a constraint satisfaction solver to prune, prioritize and package product line tests complementing prior work in the software product line testing literature. A prototype implementation of TESALIA is used for validation in an Android example showing the benefits of maximizing the mobile market share (the value function) while meeting a budgetary constraint.},
journal = {Software Quality Journal},
month = jun,
pages = {365–405},
numpages = {41},
keywords = {Testing, Software product lines, Feature models, Automated analysis, Android}
}

@article{10.1007/s11219-021-09550-5,
author = {Alkharabsheh, Khalid and Crespo, Yania and Fern\'{a}ndez-Delgado, Manuel and Viqueira, Jos\'{e} R. and Taboada, Jos\'{e} A.},
title = {Exploratory study of the impact of project domain and size category on the detection of the God class design smell},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09550-5},
doi = {10.1007/s11219-021-09550-5},
abstract = {Design smell detection has proven to be an efficient strategy to improve software quality and consequently decrease maintainability expenses. This work explores the influence of the&nbsp;information  about&nbsp;project context expressed as project domain and size category information, on the automatic detection of the god class design smell by machine learning techniques. A set of experiments using eight classifiers to detect god classes was conducted on a dataset containing 12, 587 classes from 24 Java projects. The results show that classifiers change their behavior when they are used on datasets that differ in these kinds of project information. The results show that god class design smell detection can be improved by feeding machine learning classifiers with this project context information.},
journal = {Software Quality Journal},
month = jun,
pages = {197–237},
numpages = {41},
keywords = {God class, Project context information, Software metrics, Machine learning, Design smell detection}
}

@article{10.1145/2581376,
author = {Behjati, Razieh and Nejati, Shiva and Briand, Lionel C.},
title = {Architecture-Level Configuration of Large-Scale Embedded Software Systems},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2581376},
doi = {10.1145/2581376},
abstract = {Configuration in the domain of Integrated Control Systems (ICS) is largely manual, laborious, and error prone. In this article, we propose a model-based configuration approach that provides automation support for reducing configuration effort and the likelihood of configuration errors in the ICS domain. We ground our approach on component-based specifications of ICS families. We then develop a configuration algorithm using constraint satisfaction techniques over finite domains to generate products that are consistent with respect to their ICS family specifications. We reason about the termination and consistency of our configuration algorithm analytically. We evaluate the effectiveness of our configuration approach by applying it to a real subsea oil production system. Specifically, we have rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {25},
numpages = {43},
keywords = {product configuration, formal specification, constraint satisfaction techniques, consistent configuration, UML/OCL, Model-based product-line engineering}
}

@article{10.1007/s10664-012-9208-x,
author = {Feigenspan, Janet and K\"{a}stner, Christian and Apel, Sven and Liebig, J\"{o}rg and Schulze, Michael and Dachselt, Raimund and Papendieck, Maria and Leich, Thomas and Saake, Gunter},
title = {Do background colors improve program comprehension in the #ifdef hell?},
year = {2013},
issue_date = {August    2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-012-9208-x},
doi = {10.1007/s10664-012-9208-x},
abstract = {Software-product-line engineering aims at the development of variable and reusable software systems. In practice, software product lines are often implemented with preprocessors. Preprocessor directives are easy to use, and many mature tools are available for practitioners. However, preprocessor directives have been heavily criticized in academia and even referred to as "#ifdef hell", because they introduce threats to program comprehension and correctness. There are many voices that suggest to use other implementation techniques instead, but these voices ignore the fact that a transition from preprocessors to other languages and tools is tedious, erroneous, and expensive in practice. Instead, we and others propose to increase the readability of preprocessor directives by using background colors to highlight source code annotated with ifdef directives. In three controlled experiments with over 70 subjects in total, we evaluate whether and how background colors improve program comprehension in preprocessor-based implementations. Our results demonstrate that background colors have the potential to improve program comprehension, independently of size and programming language of the underlying product. Additionally, we found that subjects generally favor background colors. We integrate these and other findings in a tool called FeatureCommander, which facilitates program comprehension in practice and which can serve as a basis for further research.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {699–745},
numpages = {47},
keywords = {Software visualization, Software product lines, Program comprehension, Preprocessors, FeatureCommander, Empirical software engineering}
}

@article{10.1016/j.artmed.2016.05.004,
title = {An ensemble method for extracting adverse drug events from social media},
year = {2016},
issue_date = {June 2016},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {70},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2016.05.004},
doi = {10.1016/j.artmed.2016.05.004},
abstract = {We propose a relation extraction system to distinguish between adverse drug events (ADEs) and non-ADEs on social media.We develop a feature-based method, investigate the effectiveness of feature selection, and analyze the contributions of different features.We investigate whether kernel-based methods can effectively extract ADEs from social media.We propose several classifier ensembles to further enhance ADE extraction capabilities. ObjectiveBecause adverse drug events (ADEs) are a serious health problem and a leading cause of death, it is of vital importance to identify them correctly and in a timely manner. With the development of Web 2.0, social media has become a large data source for information on ADEs. The objective of this study is to develop a relation extraction system that uses natural language processing techniques to effectively distinguish between ADEs and non-ADEs in informal text on social media. Methods and materialsWe develop a feature-based approach that utilizes various lexical, syntactic, and semantic features. Information-gain-based feature selection is performed to address high-dimensional features. Then, we evaluate the effectiveness of four well-known kernel-based approaches (i.e., subset tree kernel, tree kernel, shortest dependency path kernel, and all-paths graph kernel) and several ensembles that are generated by adopting different combination methods (i.e., majority voting, weighted averaging, and stacked generalization). All of the approaches are tested using three data sets: two health-related discussion forums and one general social networking site (i.e., Twitter). ResultsWhen investigating the contribution of each feature subset, the feature-based approach attains the best area under the receiver operating characteristics curve (AUC) values, which are 78.6%, 72.2%, and 79.2% on the three data sets. When individual methods are used, we attain the best AUC values of 82.1%, 73.2%, and 77.0% using the subset tree kernel, shortest dependency path kernel, and feature-based approach on the three data sets, respectively. When using classifier ensembles, we achieve the best AUC values of 84.5%, 77.3%, and 84.5% on the three data sets, outperforming the baselines. ConclusionsOur experimental results indicate that ADE extraction from social media can benefit from feature selection. With respect to the effectiveness of different feature subsets, lexical features and semantic features can enhance the ADE extraction capability. Kernel-based approaches, which can stay away from the feature sparsity issue, are qualified to address the ADE extraction problem. Combining different individual classifiers using suitable combination methods can further enhance the ADE extraction effectiveness.},
journal = {Artif. Intell. Med.},
month = jun,
pages = {62–76},
numpages = {15}
}

@inproceedings{10.5555/3172077.3172181,
author = {Li, Hao and Gong, Maoguo},
title = {Self-paced convolutional neural networks},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks. In order to distinguish the reliable data from the noisy and confusing data, we improve CNNs with self-paced learning (SPL) for enhancing the learning robustness of CNNs. In the proposed self-paced convolutional network (SPCN), each sample is assigned to a weight to reflect the easiness of the sample. Then a dynamic self-paced function is incorporated into the leaning objective of CNN to jointly learn the parameters of CNN and the latent weight variable. SPCN learns the samples from easy to complex and the sample weights can dynamically control the learning rates for converging to better values. To gain more insights of SPCN, theoretical studies are conducted to show that SPCN converges to a stationary solution and is robust to the noisy and confusing data. Experimental results on MNIST and  rectangles  datasets demonstrate that the proposed method outperforms baseline methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2110–2116},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.1016/j.jss.2017.05.052,
author = {Bastos, Jonatas Ferreira and da Mota Silveira Neto, Paulo Anselmo and OLeary, Pdraig and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {Software product lines adoption in small organizations},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.05.052},
doi = {10.1016/j.jss.2017.05.052},
abstract = {Provides a better understanding of SPL adoption in the context of SMEs.A set of empirical studies performed in academic and industry environments.Inputs to establish guidelines for SPL adoption.A discussion of the evidences, with insights to guide future investigations. ContextAn increasing number of studies has demonstrated improvements in product quality, and time-to-market reductions when Software Product Line (SPL) engineering is introduced. However, despite the amount of successful stories about the use of SPL engineering, there is a lack of guidelines to support its adoption, especially to small-sized software organizations. ObjectiveThe aim of this study is to investigate SPL adoption in small organizations and to improve the generalization of evidence through the use of a multi-method approach. MethodThis paper reports on a multi-method study, where results from a mapping study, industrial case study and also expert opinion survey were considered to identify a set of findings. ResultsThe study provides a better understanding of SPL adoption in the context of small to medium-sized organizations, by documenting evidence observed during the transition from single-system development to an SPL approach. This evidence is strengthened by the use of different research methods, which results in 22 findings regarding to the SPL adoption. ConclusionThis research has synthesized the available evidence in SPL adoption and identifies gaps between required strategies, organizational structures, maturity level and existing adoption barriers. These findings are an important step to establish guidelines for SPL adoption.},
journal = {J. Syst. Softw.},
month = sep,
pages = {112–128},
numpages = {17},
keywords = {Survey, Software product lines, SPL adoption, Multi-method approach, Mapping study, Case study, Adoption barriers}
}

@inproceedings{10.1109/ICSE-NIER.2019.00028,
author = {Trubiani, Catia and Apel, Sven},
title = {PLUS: performance learning for uncertainty of software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00028},
doi = {10.1109/ICSE-NIER.2019.00028},
abstract = {Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software performance engineering domain: How to model the variable configuration options (i.e., software and hardware resources) and their intrinsic uncertainties (e.g., resource demand, processor speed) to represent the performance characteristics of software systems? This way, software engineers are exposed to a quantitative evaluation of their systems that supports them in the task of identifying performance critical configurations along with their uncertainties.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {77–80},
numpages = {4},
keywords = {uncertainty, machine learning},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1145/3375959.3375967,
author = {Wolde, Behailu Getachew and Boltana, Abiot Sinamo},
title = {Combinatorial Testing Approach for Cloud Mobility Service},
year = {2020},
isbn = {9781450372633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375959.3375967},
doi = {10.1145/3375959.3375967},
abstract = {Currently, software product becomes an essential component in running many stakeholders' activities. For instance, the industries mostly use cloud services to execute their important business functionality. However, by a few input's parameter interacting, this functionality can be pended. Such constraint poses challenging to cover various features of failure especially in ensuring cloud application. One way is to devise a strategy to cover input parameters' characteristics based on Combinatorial testing approach. This technique includes all possible combinations of test inputs for detecting bugs on the System Under Test (SUT). The paper explains the Combinatorial covering arrays to generate relatively exhaustive testing by modeling features of sample services using Feature IDE plugin in Eclipse IDE. This way, we build the input domain model to represent coverage of the existing mobility service running on NEMo Mobility cloud platform. Using this model, covering arrays is applied to generate t-way test cases by leveraging IPOg algorithm, which is implemented in a CiTLab. As a test case management, the JUnit testing framework uses test stubs to validate the test methods of generated test cases on the specified service (SUT).},
booktitle = {Proceedings of the 2019 2nd Artificial Intelligence and Cloud Computing Conference},
pages = {6–13},
numpages = {8},
keywords = {Software Testing, Feature Model, Combinatorial Testing, Cloud Mobility Service, CiTLAB},
location = {Kobe, Japan},
series = {AICCC '19}
}

@article{10.1016/j.infsof.2019.06.012,
author = {Balera, Juliana Marino and Santiago J\'{u}nior, Valdivino Alexandre de},
title = {A systematic mapping addressing Hyper-Heuristics within Search-based Software Testing},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.06.012},
doi = {10.1016/j.infsof.2019.06.012},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {176–189},
numpages = {14},
keywords = {Meta-heuristics, Genetic Algorithms, Evolutionary Algorithms, Systematic Mapping, Hyper-heuristics, Search-based Software Testing}
}

@inproceedings{10.1145/3053600.3053619,
author = {Mangels, Tatiana and Murarasu, Alin and Oden, Forest and Fishkin, Alexey and Becker, Daniel},
title = {Efficient Analysis at Edge},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053619},
doi = {10.1145/3053600.3053619},
abstract = {Digitalization changes traditional business models by using digital technologies to improve existing offerings and to create new offerings. Current technological trends such as artificial intelligence, autonomous systems, and predictive maintenance are ideal candidate technologies to enable digitalization use cases. Often, these technologies rely on the availability of large amounts of data and the capability to process these data efficiently. In contrast to consumer markets, industrial products must fulfill higher non-functional requirements such as fast response times, 24/7 availability and stability, real-time processing, safety, or security requirements. As a consequence, processing capabilities -- ranging from multicore and manycores to even high end parallel clusters -- have to be exploited to achieve necessary performance and stability needs. In this paper, we introduce a Distributed Multicore Monitoring Framework (MoMo) which is a reference monitoring solution developed at Siemens Corporate Technology. It can be used to easily build efficient and stable diagnostic solutions which can help to understand the correctness, availability, reliability, and performance of large-scale distributed systems based on live data. Due to its small footprint MoMo can be used to analyze data directly at the data source which, for instance, can significantly reduce the network load. While MoMo's efficiency comes from the usage of multicore processors (CPUs) for running analysis in parallel, its usability is guaranteed by its capability to easily integrate with other monitoring frameworks and its usage of SPL - a domain-specific language which allows user to easily define diagnostic algorithms.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {85–90},
numpages = {6},
keywords = {parallel computing, monitoring, data analysis},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@article{10.1016/j.jss.2011.06.026,
author = {Guo, Jianmei and White, Jules and Wang, Guangxin and Li, Jian and Wang, Yinglin},
title = {A genetic algorithm for optimized feature selection with resource constraints in software product lines},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.06.026},
doi = {10.1016/j.jss.2011.06.026},
abstract = {Abstract: Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86-97% of the optimality of other automated feature selection algorithms and in 45-99% less time than existing exact and heuristic feature selection techniques.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2208–2221},
numpages = {14},
keywords = {Software product lines, Product derivation, Optimization, Genetic algorithm, Feature models, Configuration}
}

@inproceedings{10.1109/ASE.2011.6100075,
author = {Apel, Sven and Speidel, Hendrik and Wendler, Philipp and von Rhein, Alexander and Beyer, Dirk},
title = {Detection of feature interactions using feature-aware verification},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100075},
doi = {10.1109/ASE.2011.6100075},
abstract = {A software product line is a set of software products that are distinguished in terms of features (i.e., end-user-visible units of behavior). Feature interactions-- situations in which the combination of features leads to emergent and possibly critical behavior --are a major source of failures in software product lines. We explore how feature-aware verification can improve the automatic detection of feature interactions in software product lines. Feature-aware verification uses product-line-verification techniques and supports the specification of feature properties along with the features in separate and composable units. It integrates the technique of variability encoding to verify a product line without generating and checking a possibly exponential number of feature combinations. We developed the tool suite SPLVERIFIER for feature-aware verification, which is based on standard model-checking technology. We applied it to an e-mail system that incorporates domain knowledge of AT&amp;T. We found that feature interactions can be detected automatically based on specifications that have only local knowledge.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {372–375},
numpages = {4},
series = {ASE '11}
}

@inproceedings{10.1007/978-3-030-73197-7_29,
author = {Du, Yuntao and Chen, Yinghao and Cui, Fengli and Zhang, Xiaowen and Wang, Chongjun},
title = {Cross-Domain Error Minimization for Unsupervised Domain Adaptation},
year = {2021},
isbn = {978-3-030-73196-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73197-7_29},
doi = {10.1007/978-3-030-73197-7_29},
abstract = {Unsupervised domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Previous methods focus on learning domain-invariant features to decrease the discrepancy between the feature distributions as well as minimizing the source error and have made remarkable progress. However, a recently proposed theory reveals that such a strategy is not sufficient for a successful domain adaptation. It shows that besides a small source error, both the discrepancy between the feature distributions and the discrepancy between the labeling functions should be small across domains. The discrepancy between the labeling functions is essentially the cross-domain errors which are ignored by existing methods. To overcome this issue, in this paper, a novel method is proposed to integrate all the objectives into a unified optimization framework. Moreover, the incorrect pseudo labels widely used in previous methods can lead to error accumulation during learning. To alleviate this problem, the pseudo labels are obtained by utilizing structural information of the target domain besides source classifier and we propose a curriculum learning based strategy to select the target samples with more accurate pseudo-labels during training. Comprehensive experiments are conducted, and the results validate that our approach outperforms state-of-the-art methods.},
booktitle = {Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11–14, 2021, Proceedings, Part II},
pages = {429–448},
numpages = {20},
keywords = {Cross-domain errors, Domain adaptation, Transfer learning},
location = {Taipei, Taiwan}
}

@article{10.1007/s10462-018-9675-6,
author = {Vikatos, Pantelis and Gryllos, Prokopios and Makris, Christos},
title = {Marketing campaign targeting using bridge extraction in multiplex social network},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-018-9675-6},
doi = {10.1007/s10462-018-9675-6},
abstract = {In this paper, we introduce a methodology for improving the targeting of marketing campaigns using bridge prediction in communities based on the information of multilayer online social networks. The campaign strategy involves the identification of nodes with high brand loyalty and top-ranking nodes in terms of participation in bridges that will be involved in the evolution of the graph. Our approach is based on an efficient classification model combining topological characteristics of crawled social graphs with sentiment and linguistic traits of user-nodes, popularity in social media as well as meta path-based features of multilayer networks. To validate our approach we present a set of experimental results using a well-defined dataset from Twitter and Foursquare. Our methodology is useful to recommendation systems as well as to marketers who are interested to use social influence and run effective marketing campaigns.},
journal = {Artif. Intell. Rev.},
month = jan,
pages = {703–724},
numpages = {22},
keywords = {Sentiment analysis, Graph mining, Link prediction, Influence metric, Social marketing}
}

@article{10.1007/s11257-011-9114-8,
author = {Wang, Yang and Kobsa, Alfred},
title = {A PLA-based privacy-enhancing user modeling framework and its evaluation},
year = {2013},
issue_date = {March     2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-011-9114-8},
doi = {10.1007/s11257-011-9114-8},
abstract = {Reconciling personalization with privacy has been a continuing interest in user modeling research. This aim has computational, legal and behavioral/attitudinal ramifications. We present a dynamic privacy-enhancing user modeling framework that supports compliance with users' individual privacy preferences and with the privacy laws and regulations that apply to each user. The framework is based on a software product line architecture. It dynamically selects personalization methods during runtime that meet the current privacy constraints. Since dynamic architectural reconfiguration is typically resource-intensive, we conducted a performance evaluation with four implementations of our system that vary two factors. The results demonstrate that at least one implementation of our approach is technically feasible with comparatively modest additional resources, even for websites with the highest traffic today. To gauge user reactions to privacy controls that our framework enables, we also conducted a controlled experiment that allowed one group of users to specify privacy preferences and view the resulting effects on employed personalization methods. We found that users in this treatment group utilized this feature, deemed it useful, and had fewer privacy concerns as measured by higher disclosure of their personal data.},
journal = {User Modeling and User-Adapted Interaction},
month = mar,
pages = {41–82},
numpages = {42},
keywords = {User modeling, User experiment, Product line architecture, Privacy preferences, Privacy laws, Performance evaluation, Disclosure behavior, Compliance}
}

@article{10.1145/3300148,
author = {Li, Miqing and Yao, Xin},
title = {Quality Evaluation of Solution Sets in Multiobjective Optimisation: A Survey},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3300148},
doi = {10.1145/3300148},
abstract = {Complexity and variety of modern multiobjective optimisation problems result in the emergence of numerous search techniques, from traditional mathematical programming to various randomised heuristics. A key issue raised consequently is how to evaluate and compare solution sets generated by these multiobjective search techniques. In this article, we provide a comprehensive review of solution set quality evaluation. Starting with an introduction of basic principles and concepts of set quality evaluation, this article summarises and categorises 100 state-of-the-art quality indicators, with the focus on what quality aspects these indicators reflect. This is accompanied in each category by detailed descriptions of several representative indicators and in-depth analyses of their strengths and weaknesses. Furthermore, issues regarding attributes that indicators possess and properties that indicators are desirable to have are discussed, in the hope of motivating researchers to look into these important issues when designing quality indicators and of encouraging practitioners to bear these issues in mind when selecting/using quality indicators. Finally, future trends and potential research directions in the area are suggested, together with some guidelines on these directions.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {26},
numpages = {38},
keywords = {performance assessment, multobjective optimisation, multi-criteria optimisation, metric, metaheuristic, measure, indicator, heuristic, exact method, evolutionary algorithms, Quality evaluation}
}

@inproceedings{10.5555/3540261.3541724,
author = {Weihs, Luca and Jain, Unnat and Liu, Iou-Jen and Salvador, Jordi and Lazebnik, Svetlana and Kembhavi, Aniruddha and Schwing, Alexander},
title = {Bridging the imitation gap by adaptive insubordination},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In practice, imitation learning is preferred over pure reinforcement learning whenever it is possible to design a teaching agent to provide expert supervision. However, we show that when the teaching agent makes decisions with access to privileged information that is unavailable to the student, this information is marginalized during imitation learning, resulting in an "imitation gap" and, potentially, poor results. Prior work bridges this gap via a progression from imitation learning to reinforcement learning. While often successful, gradual progression fails for tasks that require frequent switches between exploration and memorization. To better address these tasks and alleviate the imitation gap we propose 'Adaptive Insubordination' (ADVISOR). ADVISOR dynamically weights imitation and reward-based reinforcement learning losses during training, enabling on-the-fly switching between imitation and exploration. On a suite of challenging tasks set within gridworlds, multi-agent particle environments, and high-fidelity 3D simulators, we show that on-the-fly switching with ADVISOR outperforms pure imitation, pure reinforcement learning, as well as their sequential and parallel combinations.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1463},
numpages = {13},
series = {NIPS '21}
}

@article{10.1016/j.eswa.2013.12.028,
author = {Segura, Sergio and Parejo, Jos\'{e} A. and Hierons, Robert M. and Benavides, David and Ruiz-Cort\'{e}s, Antonio},
title = {Automated generation of computationally hard feature models using evolutionary algorithms},
year = {2014},
issue_date = {June, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.12.028},
doi = {10.1016/j.eswa.2013.12.028},
abstract = {A feature model is a compact representation of the products of a software product line. The automated extraction of information from feature models is a thriving topic involving numerous analysis operations, techniques and tools. Performance evaluations in this domain mainly rely on the use of random feature models. However, these only provide a rough idea of the behaviour of the tools with average problems and are not sufficient to reveal their real strengths and weaknesses. In this article, we propose to model the problem of finding computationally hard feature models as an optimization problem and we solve it using a novel evolutionary algorithm for optimized feature models (ETHOM). Given a tool and an analysis operation, ETHOM generates input models of a predefined size maximizing aspects such as the execution time or the memory consumption of the tool when performing the operation over the model. This allows users and developers to know the performance of tools in pessimistic cases providing a better idea of their real power and revealing performance bugs. Experiments using ETHOM on a number of analyses and tools have successfully identified models producing much longer executions times and higher memory consumption than those obtained with random models of identical or even larger size.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {3975–3992},
numpages = {18},
keywords = {Software product lines, Search-based testing, Performance testing, Feature models, Evolutionary algorithms, Automated analysis}
}

@article{10.5555/2946645.2946709,
author = {Adi, Yossi and Keshet, Joseph},
title = {StructED: risk minimization in structured prediction},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Structured tasks are distinctive: each task has its own measure of performance, such as the word error rate in speech recognition, the BLEU score in machine translation, the NDCG score in information retrieval, or the intersection-over-union score in visual object segmentation. This paper presents STRUCTED, a software package for learning structured prediction models with training methods that aimed at optimizing the task measure of performance. The package was written in Java and released under the MIT license. It can be downloaded from http://adiyoss.github.io/StructED/.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2282–2286},
numpages = {5},
keywords = {structured prediction, structural SVM, direct loss minimization, CRF}
}

@inproceedings{10.1007/978-3-030-92273-3_25,
author = {Zheng, Jinfang and Xie, Jinyang and Lyu, Chen and Lyu, Lei},
title = {SS-CCN: Scale Self-guided Crowd Counting Network},
year = {2021},
isbn = {978-3-030-92272-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-92273-3_25},
doi = {10.1007/978-3-030-92273-3_25},
abstract = {With the emergence of deep learning, many CNN-based methods have achieved competitive performance in crowd counting, in which how to effectively solve the scale variation problem plays a key role. To tackle with the problem, we present an innovative scale self-guided crowd counting network (SS-CCN) by taking full advantage of scale information in a multi-level network. The proposed SS-CCN highlights crowd information by applying scale enhancement and scale-aware attention modules in multi-level features. Moreover, semantic attention module is applied on deep layers to extract semantic information. Besides, the fine-grained residual module is proposed to further refine the crowd information. Furthermore, we pioneer a scale pyramid loss with different loss functions applied to different scales. Integrating the proposed module, our method can effectively solve the scale variation problem. Extensive experimental results on several public datasets show that our proposed SS-CCN achieves satisfactory and superior performance compared to the state-of-the-art methods.},
booktitle = {Neural Information Processing: 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 8–12, 2021, Proceedings, Part IV},
pages = {299–310},
numpages = {12},
keywords = {Scale pyramid loss, Scale-aware, Attention mechanism, Deep learning, Crowd counting},
location = {Sanur, Bali, Indonesia}
}

@article{10.1016/j.asoc.2016.07.048,
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah and Jalab, Hamid A.},
title = {Extracting features from online software reviews to aid requirements reuse},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.048},
doi = {10.1016/j.asoc.2016.07.048},
abstract = {Display Omitted The extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have the access.Online reviews for software products can be used as input for features extraction to assist requirements reuse.Techniques from unsupervised learning and Natural Language Processing is employed as a propose solutions to Requirements Reuse problem.The approach obtained a precision of 87% (62% average) and a recall of 86% (82% average), when evaluated against the truth data set created manually. Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. In Requirements Reuse (RR), the extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have access to these software artefacts. Due to organisational privacy, SRS are always kept confidential and not easily available to the public. As alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the RR process. The aim of this paper is to propose a semi-automated approach, known as Feature Extraction for Reuse of Natural Language requirements (FENL), to extract phrases that can represent software features from software reviews in the absence of SRS as a way to initiate the RR process. FENL is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. In the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (IR) area. As a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. As for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. The performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and F-Measure.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1297–1315},
numpages = {19},
keywords = {Unsupervised learning, Software engineering, Requirements reuse, Natural language processing, Latent semantic analysis}
}

@inproceedings{10.1007/978-3-319-27343-3_1,
author = {Braubach, Lars and Pokahr, Alexander and Kalinowski, Julian and Jander, Kai},
title = {Tailoring Agent Platforms with Software Product Lines},
year = {2015},
isbn = {9783319273426},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-27343-3_1},
doi = {10.1007/978-3-319-27343-3_1},
abstract = {Agent platforms have been conceived traditionally as middleware, helping to deal with various application challenges like agent programming models, remote messaging, and coordination protocols. A\"{\i} \'{z}middleware is typically a bundle of functionalities necessary to execute multi-agent applications. In contrast to this traditional view, nowadays different use cases also for selected agent concepts have emerged requiring also different kinds of functionalities. Examples include a platform for conducting multi-agent simulations, intelligent agent behavior models for controlling non-player characters NPCs in games and a lightweight version suited for mobile devices. A one-size-fits-all software bundle often does not sufficiently match these requirements, because customers and developers want solutions specifically tailored to their needs, i.e. a small but focused solution is frequently preferred over bloated software with extraneous functionality. Software product lines are an approach suitable for creating a series of similar products from a common code base. In this paper we will show how software product line modeling and technology can help creating tailor-made products from multi-agent platforms. Concretely, the Jadex platform will be analyzed and a feature model as well as an implementation path will be presented.},
booktitle = {Revised Selected Papers of the 13th German Conference on Multiagent System Technologies - Volume 9433},
pages = {3–21},
numpages = {19},
location = {Cottbus, Germany},
series = {MATES 2015}
}

@article{10.1016/j.knosys.2017.03.026,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Garca, Salvador and Tang, Jia-Fu and Herrera, Francisco},
title = {Exploring the effectiveness of dynamic ensemble selection in the one-versus-one scheme},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.03.026},
doi = {10.1016/j.knosys.2017.03.026},
abstract = {The One-versus-One (OVO) strategy is one of the most common and effective techniques to deal with multi-class classification problems. The basic idea of an OVO scheme is to divide a multi-class classification problem into several easier-to-solve binary classification problems with considering each possible pair of classes from the original problem, which is then built into a binary classifier by an independent base learner. In this study, we propose a novel methodology which attempts to select a group of base classifiers in each pairwise dataset for each unknown pattern. To implement this, the Dynamic Ensemble Selection (DES) method based on a competence measure is employed to select the most appropriate ensemble in each binary classification problem derived from the OVO decomposition. In order to verify the validity and effectiveness of our proposed method, we carry out a thorough experimental study. We first compare our proposal with several state-of-the-art approaches. Then, we perform the comparison of several well-known aggregation strategies to combine the binary ensemble obtained by Dynamic Ensemble Selection. Finally, we explore whether further improvement can be achieved by considering the competence-based method in OVO scheme. The extracted findings drawn from the empirical analysis are supported by the proper statistical analysis and indicate that there is a positive synergy between the DES method and the Distance-based Relative Competence Weighting (DRCW) approach for the OVO scheme.},
journal = {Know.-Based Syst.},
month = jun,
pages = {53–63},
numpages = {11},
keywords = {Pairwise learning, One-versus-One, Multi-classification, Dynamic ensemble selection, Decomposition strategies}
}

@inproceedings{10.5555/3504035.3504312,
author = {Huang, Wenjun and Liang, Chao and Yu, Yi and Wang, Zheng and Ruan, Weijian and Hu, Ruimin},
title = {Video-based person re-identification via self paced weighting},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Person re-identification (re-id) is a fundamental technique to associate various person images, captured by different surveillance cameras, to the same person. Compared to the single image based person re-id methods, video-based person re-id has attracted widespread attentions because extra space-time information and more appearance cues that can be used to greatly improve the matching performance. However, most existing video-based person re-id methods equally treat all video frames, ignoring their quality discrepancy caused by object occlusion and motions, which is a common phenomenon in real surveillance scenario. Based on this finding, we propose a novel video-based person re-id method via self paced weighting (SPW). Firstly, we propose a self paced outlier detection method to evaluate the noise degree of video sub sequences. Thereafter, a weighted multi-pair distance metric learning approach is adopted to measure the distance of two person image sequences. Experimental results on two public datasets demonstrate the superiority of the proposed method over current state-of-the-art work.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {277},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1016/j.neucom.2021.04.070,
author = {Yang, Zhao and Liu, Jiehao and Liu, Tie and Zhu, Yuanxin and Wang, Li and Tao, Dapeng},
title = {Equidistant distribution loss for person re-identification},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {455},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.04.070},
doi = {10.1016/j.neucom.2021.04.070},
journal = {Neurocomput.},
month = sep,
pages = {255–264},
numpages = {10},
keywords = {Imbalance learning, Equidistant distribution loss, Person re-identification}
}

@article{10.1016/j.neucom.2019.04.066,
author = {Zhu, Qi and Yuan, Ning and Huang, Jiashuang and Hao, Xiaoke and Zhang, Daoqiang},
title = {Multi-modal AD classification via self-paced latent correlation analysis},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {355},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.04.066},
doi = {10.1016/j.neucom.2019.04.066},
journal = {Neurocomput.},
month = aug,
pages = {143–154},
numpages = {12},
keywords = {Computer-aided diagnosis, Self-paced learning, Low-rank, Feature extraction, Multi-modal fusion}
}

@article{10.1016/j.cviu.2021.103255,
author = {Landi, Federico and Baraldi, Lorenzo and Cornia, Marcella and Corsini, Massimiliano and Cucchiara, Rita},
title = {Multimodal attention networks for low-level vision-and-language navigation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {210},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2021.103255},
doi = {10.1016/j.cviu.2021.103255},
journal = {Comput. Vis. Image Underst.},
month = sep,
numpages = {9},
keywords = {Multi-modal attention, Embodied AI, Vision-and-language navigation, 68T45, 68T40, 68T01}
}

@article{10.1016/j.future.2019.07.013,
author = {Shen, Rongbo and Yan, Kezhou and Tian, Kuan and Jiang, Cheng and Zhou, Ke},
title = {Breast mass detection from the digitized X-ray mammograms based on the combination of deep active learning and self-paced learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.07.013},
doi = {10.1016/j.future.2019.07.013},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {668–679},
numpages = {12},
keywords = {Self-paced learning, Deep active learning, Mass detection, Mammography, Breast cancer}
}

@inproceedings{10.1007/978-3-030-67832-6_30,
author = {Wang, Fei and Ding, Youdong and Liang, Huan and Wen, Jing},
title = {Discriminative and Selective Pseudo-Labeling for Domain Adaptation},
year = {2021},
isbn = {978-3-030-67831-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67832-6_30},
doi = {10.1007/978-3-030-67832-6_30},
abstract = {Unsupervised domain adaptation aims to transfer the knowledge of source domain to a related but not labeled target domain. Due to the lack of label information of target domain, most existing methods train a weak classifier and directly apply to pseudo-labeling which may downgrade adaptation performance. To address this problem, in this paper, we propose a novel discriminative and selective pseudo-labeling (DSPL) method for domain adaptation. Specifically, we first match the marginal distributions of two domains and increase inter-class distance simultaneously. Then a feature transformation method is proposed to learn a low-dimensional transfer subspace which is discriminative enough. Finally, after data has formed good clusters, we introduce a structured prediction based selective pseudo-labeling strategy which is able to sufficiently exploit target data structure. We conduct extensive experiments on three popular visual datasets, demonstrating the good domian adaptation performance of our method.},
booktitle = {MultiMedia Modeling: 27th International Conference, MMM 2021, Prague, Czech Republic, June 22–24, 2021, Proceedings, Part I},
pages = {365–377},
numpages = {13},
keywords = {Discriminative learned subspace, Pseudo-labeling, Unsupervised domain adaptation},
location = {Prague, Czech Republic}
}

@inproceedings{10.1145/3404555.3404582,
author = {Bisma, Mariam and Azam, Farooque and Rasheed, Yawar and Anwar, Muhammad Waseem},
title = {A Model-Driven Framework for Ensuring Role Based Access Control in IoT Devices},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404582},
doi = {10.1145/3404555.3404582},
abstract = {Ensuring security and privacy of IOT devices and the associated/ dependent complex and critical systems is certainly a major concern, especially after proliferation of IoT devices in variety of domains in current era. A considerable level of security can be achieved in these systems using the techniques of Role Based Access Control (RBAC). In contrast to Discretionary Access Control (DAC) where personal identity of the owner/ user matters, RBAC grants access permissions on the basis of roles of the user. Due to the inherent complexity associated with ensuring security in IoT devices and related systems/ services, a level of abstraction is required in the development process, in order to better understand and develop the system accordingly by integrating all the security aspects. This level of abstraction can be achieved by developing the system as per the concepts of Model Driven Development (MDD). In this paper, techniques of Model Driven Architecture (MDA)/ MDD has been used to propose such a Framework/ Meta-Model, which ensures RBAC in order to access the services associated with IoT devices. The proposed Meta-Model can be further extended for the model-based development and automation of such a system that ensure RBAC for IoT devices. Validity of proposed Meta-Model has been proved by creating an M1 level Instance Model of a real-world case study. Results prove, that the proposed Meta-Model is capable to be transformed into a reliable system that ensures RBAC in IoT devices.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {455–460},
numpages = {6},
keywords = {security in IoT, role base access control, model-driven approach, model driven, meta-model, IoT},
location = {Tianjin, China},
series = {ICCAI '20}
}

@inproceedings{10.1007/978-3-642-33666-9_34,
author = {Vierhauser, Michael and Gr\"{u}nbacher, Paul and Heider, Wolfgang and Holl, Gerald and Lettner, Daniela},
title = {Applying a consistency checking framework for heterogeneous models and artifacts in industrial product lines},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_34},
doi = {10.1007/978-3-642-33666-9_34},
abstract = {Product line engineering relies on heterogeneous models and artifacts to define and implement the product line's reusable assets. The complexity and heterogeneity of product line artifacts as well as their interdependencies make it hard to maintain consistency during development and evolution, regardless of the modeling approaches used. Engineers thus need support for detecting and resolving inconsistencies within and between the various artifacts. In this paper we present a framework for checking and maintaining consistency of arbitrary product line artifacts. Our approach is flexible and extensible regarding the supported artifact types and the definition of constraints. We discuss tool support developed for the DOPLER product line tool suite. We report the results of applying the approach to sales support applications of industrial product lines.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {531–545},
numpages = {15},
keywords = {sales support, model-based product lines, consistency checking},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.5555/1964571.1964603,
author = {Hubaux, Arnaud and Boucher, Quentin and Hartmann, Herman and Michel, Rapha\"{e}l and Heymans, Patrick},
title = {Evaluating a textual feature modelling language: four industrial case studies},
year = {2010},
isbn = {9783642194399},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature models are commonly used in software product line engineering as a means to document variability. Since their introduction, feature models have been extended and formalised in various ways. The majority of these extensions are variants of the original tree-based graphical notation. But over time, textual dialects have also been proposed. The textual variability language (TVL) was proposed to combine the advantages of both graphical and textual notations. However, its benefits and limitations have not been empirically evaluated up to now. In this paper, we evaluate TVL with four cases from companies of different sizes and application domains. The study shows that practitioners can benefit from TVL. The participants appreciated the notation, the advantages of a textual language and considered the learning curve to be gentle. The study also reveals some limitations of the current version of TVL.},
booktitle = {Proceedings of the Third International Conference on Software Language Engineering},
pages = {337–356},
numpages = {20},
location = {Eindhoven, The Netherlands},
series = {SLE'10}
}

@article{10.5555/3288992.3288997,
author = {Vinci, Giuseppe and Ventura, Val\'{e}rie and Smith, Matthew A. and Kass, Robert E.},
title = {Adjusted regularization of cortical covariance},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {45},
number = {2},
issn = {0929-5313},
abstract = {It is now common to record dozens to hundreds or more neurons simultaneously, and to ask how the network activity changes across experimental conditions. A natural framework for addressing questions of functional connectivity is to apply Gaussian graphical modeling to neural data, where each edge in the graph corresponds to a non-zero partial correlation between neurons. Because the number of possible edges is large, one strategy for estimating the graph has been to apply methods that aim to identify large sparse effects using an L1$L_{1}$ penalty. However, the partial correlations found in neural spike count data are neither large nor sparse, so techniques that perform well in sparse settings will typically perform poorly in the context of neural spike count data. Fortunately, the correlated firing for any pair of cortical neurons depends strongly on both their distance apart and the features for which they are tuned. We introduce a method that takes advantage of these known, strong effects by allowing the penalty to depend on them: thus, for example, the connection between pairs of neurons that are close together will be penalized less than pairs that are far apart. We show through simulations that this physiologically-motivated procedure performs substantially better than off-the-shelf generic tools, and we illustrate by applying the methodology to populations of neurons recorded with multielectrode arrays implanted in macaque visual cortex areas V1 and V4.},
journal = {J. Comput. Neurosci.},
month = oct,
pages = {83–101},
numpages = {19},
keywords = {Penalized maximum likelihood estimation, Macaque visual cortex, High-dimensional estimation, Graphical lasso, Gaussian graphical model, Functional connectivity, False discovery rate, Bayesian inference}
}

@inproceedings{10.1007/978-3-030-00308-1_33,
author = {O’Keeffe, Simon and Villing, Rudi},
title = {A Benchmark Data Set and Evaluation of Deep Learning Architectures for Ball Detection in the RoboCup SPL},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_33},
doi = {10.1007/978-3-030-00308-1_33},
abstract = {This paper presents a benchmark data set for evaluating ball detection algorithms in the RoboCup Soccer Standard Platform League. We created a labelled data set of images with and without ball derived from vision log files recorded by multiple NAO robots in various lighting conditions. The data set contains 5209 labelled ball image regions and 10924 non-ball regions. Non-ball image regions all contain features that had been classified as a potential ball candidate by an existing ball detector. The data set was used to train and evaluate 252 different Deep Convolutional Neural Network (CNN) architectures for ball detection. In order to control computational requirements, this evaluation focused on networks with 2–5 layers that could feasibly run in the vision and cognition cycle of a NAO robot using two cameras at full frame rate (2&nbsp;\texttimes{}&nbsp;30&nbsp;Hz). The results show that the classification performance of the networks is quite insensitive to the details of the network design including input image size, number of layers and number of outputs at each layer. In an effort to reduce the computational requirements of CNNs we evaluated XNOR-Net architectures which quantize the weights and activations of a neural network to binary values. We examined XNOR-Nets corresponding to the real-valued CNNs we had already tested in order to quantify the effect on classification metrics. The results indicate that ball classification performance degrades by 12% on average when changing from real-valued CNN to corresponding XNOR-Net.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {398–409},
numpages = {12},
keywords = {XNOR-Net, Ball detection, Deep learning, Convolution neural network},
location = {Nagoya, Japan}
}

@article{10.1016/j.future.2015.05.017,
title = {Allocating resources for customizable multi-tenant applications in clouds using dynamic feature placement},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {53},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.05.017},
doi = {10.1016/j.future.2015.05.017},
abstract = {Multi-tenancy, where multiple end users make use of the same application instance, is often used in clouds to reduce hosting costs. A disadvantage of multi-tenancy is however that it makes it difficult to create customizable applications, as all end users use the same application instance. In this article, we describe an approach for the development and management of highly customizable multi-tenant cloud applications. We apply software product line engineering techniques to cloud applications, and use an approach where applications are composed of multiple interacting components, referred to as application features. Using this approach, multiple features can be shared between different applications. Allocating resources for these feature-based applications is complex, as relations between components must be taken into account, and is referred to as the feature placement problem.In this article, we describe dynamic feature placement algorithms that minimize migrations between subsequent invocations, and evaluate them in dynamic scenarios where applications are added and removed throughout the evaluation scenario. We find that the developed algorithm achieves a low cost, while resulting in few resource migrations. In our evaluations, we observe that adding migration-awareness to the management algorithms reduces the number of instance migrations by more than 77 % and reduces the load moved between instances by more than 96 % when compared to a static management approach. Despite this reduction in number of migrations, a cost that is on average less than 3 % more than the optimal cost is achieved. We model customizable SaaS applications using feature modeling.A dynamic, migration-aware management approach is presented.Two ILP-based algorithms and a heuristic algorithm are compared.The dynamic algorithms reduce migrations and remain within 3% of the optimal cost.},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {63–76},
numpages = {14}
}

@inproceedings{10.1007/978-3-030-13709-0_30,
author = {Bourel, Mathias and Canale, Eduardo and Robledo, Franco and Romero, Pablo and St\'{a}bile, Luis},
title = {A GRASP/VND Heuristic for the Max Cut-Clique Problem},
year = {2018},
isbn = {978-3-030-13708-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-13709-0_30},
doi = {10.1007/978-3-030-13709-0_30},
abstract = {In Market Basket Analysis, the goal is to understand the human behavior in order to maximize sales. An evident behavior is to buy correlated items. As a consequence, the determination of a set of items with a large correlation with others is a valuable tool for Market Basket Analysis.In this paper we address a combinatorial optimization problem that formalizes the previous application. Given a simple graph  (where the nodes are items and links represent correlation), we want to find the clique  such that the number of links shared between  and  is maximized. This problem is known in the literature as Max Cut-Clique (MCC).The contributions of this paper are three-fold. First, the computational complexity of the MCC is established. Second, a full GRASP/VND methodology enriched with a Tabu Search is here developed, where the main ingredients are novel local searches and a Restricted Candidate List that trades greediness for randomization in a multi-start fashion. A Tabu Search is also included in order to avoid locally optimum solutions. Finally, a fair comparison with respect to recent heuristics reveals that our proposal is competitive with state-of-the-art solutions.},
booktitle = {Machine Learning, Optimization, and Data Science: 4th International Conference, LOD 2018, Volterra, Italy, September 13-16, 2018, Revised Selected Papers},
pages = {357–367},
numpages = {11},
keywords = {Market Basket Analysis, Combinatorial optimization, Max Cut-Clique, Metaheuristics},
location = {Volterra, Italy}
}

@article{10.1007/s11042-020-10443-1,
author = {Rao, Champakamala Sundar and Karunakara, K.},
title = {A comprehensive review on brain tumor segmentation and classification of MRI images},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10443-1},
doi = {10.1007/s11042-020-10443-1},
abstract = {In the analysis of medical images, one of the challenging tasks is the recognition of brain tumours via medical resonance images (MRIs). The diagnosis process is still tedious due to its complexity and considerable variety in tissues of tumor perception. Therefore, the necessities of tumor identification techniques are improving nowadays for medical applications. In the past decades, different approaches in the segmentation of various precisions and complexity degree have been accomplished, which depends on the simplicity and the benchmark of the technique. An overview of this analysis is to give out the summary of the semi-automatic techniques for brain tumor segmentation and classification utilizing MRI. An enormous amount of MRI based image data is accomplished using deep learning approaches. There are several works, dealing on the conventional approaches for MRI-based segmentation of brain tumor. Alternatively, in this review, we revealed the latest trends in the methods of deep learning. Initially, we explain the several threads in MRI pre-processing, including registration of image, rectification of bias field, and non-brain tissue dismissal. And terminally, the present state evaluation of algorithm is offered and forecasting the growths to systematise the MRI-based brain tumor into a regular cyclic routine in the clinical field are focussed.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {17611–17643},
numpages = {33},
keywords = {Image processing, Tissue, Bias field, Segmentation, Brain tumor, MRI}
}

@article{10.5555/3122009.3242055,
author = {Patrascu, Andrei and Necoara, Ion},
title = {Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {A popular approach for solving stochastic optimization problems is the stochastic gradient descent (SGD) method. Although the SGD iteration is computationally cheap and its practical performance may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate choice of parameters. To avoid some of the drawbacks of SGD, stochastic proximal point (SPP) algorithms have been recently considered. We introduce a new variant of the SPP method for solving stochastic convex problems subject to (in)finite intersection of constraints satisfying a linear regularity condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex Lipschitz continuous objective functions, we prove nonasymptotic convergence rates in terms of the expected value function gap of order O(1/k1/2), where k is the iteration counter. We also derive better nonasymptotic convergence rates in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order O(1/k). Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real problems.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {7204–7245},
numpages = {42},
keywords = {stochastic proximal point, stochastic convex optimization, rates of convergence, nonasymptotic convergence analysis, intersection of convex constraints}
}

@article{10.1016/j.patcog.2021.108164,
author = {Yang, Zhaohui and Shi, Miaojing and Xu, Chao and Ferrari, Vittorio and Avrithis, Yannis},
title = {Training object detectors from few weakly-labeled and many unlabeled images},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108164},
doi = {10.1016/j.patcog.2021.108164},
journal = {Pattern Recogn.},
month = dec,
numpages = {10},
keywords = {Unlabelled set, Semi-supervised learning, Weakly-supervised learning, Object detection}
}

@article{10.1016/j.is.2012.11.010,
author = {Gr\"{o}Ner, Gerd and Bo\v{s}Kovi\'{c}, Marko and Silva Parreiras, Fernando and Ga\v{s}Evi\'{c}, Dragan},
title = {Modeling and validation of business process families},
year = {2013},
issue_date = {July, 2013},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {38},
number = {5},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2012.11.010},
doi = {10.1016/j.is.2012.11.010},
abstract = {Process modeling is an expensive task that needs to encompass requirements of different stakeholders, assure compliance with different standards, and enable the flexible adaptivity to newly emerging requirements in today's dynamic global market. Identifying reusability of process models is a promising direction towards reducing the costs of process modeling. Recent research has offered several solutions. Such solutions promote effective and formally sound methods for variability modeling and configuration management. However, ensuring behavioral validity of reused process models with respect to the original process models (often referred to as reference process models) is still an open research challenge. To address this challenge, in this paper, we propose the notion of business process families by building upon the well-known software engineering discipline-software product line engineering. Business process families comprise (i) a variability modeling perspective, (ii) a process model template (or reference model), and (iii) mappings between (i) and (ii). For business process families, we propose a correct validation algorithm ensuring that each member of a business process family adheres to the core intended behavior that is specified in the process model template. The proposed validation approach is based on the use of Description Logics, variability is represented by using the well-known Feature Models and behavior of process models is considered in terms of control flow patterns. The paper also reports on the experience gained in two external trial cases and results obtained by measuring the tractability of the implementation of the proposed validation approach.},
journal = {Inf. Syst.},
month = jul,
pages = {709–726},
numpages = {18},
keywords = {Validation, Process model variability, Process model configuration, Control flow relations, Business process families}
}

@article{10.1007/s00521-018-3560-8,
author = {Anwar, Zeeshan and Afzal, Hammad and Bibi, Nazia and Abbas, Haider and Mohsin, Athar and Arif, Omar},
title = {A hybrid-adaptive neuro-fuzzy inference system for multi-objective regression test suites optimization},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3560-8},
doi = {10.1007/s00521-018-3560-8},
abstract = {Regression testing is a mandatory activity of software development life cycle, which is performed to ensure that modifications have not caused any adverse effects on the system’s functionality. With every change in software in the maintenance phase, the size of regression test suite grows as new test cases are written to validate changes. The bigger size of regression test suite makes the testing expensive and time-consuming. Optimization of regression test suite is a possible solution to cope with this problem. Various techniques of optimization have been proposed; however, there is no perfect solution for the problem and therefore, requires better solutions to improve the optimization process. This paper presents a novel technique named as hybrid-adaptive neuro-fuzzy inference system tuned with genetic algorithm and particle swarm optimization algorithm that is used to optimize the regression test suites. Evaluation of the proposed approach is performed on benchmark test suites including “previous date problem” and “Siemens print token.” Experimental results are compared with existing state-of-the-art techniques, and results show that the proposed approach is more effective for the reduction in a regression test suites with higher requirement coverage. The size of regression test suites can be reduced up to 48% using the proposed approach without reducing the fault detection rate.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {7287–7301},
numpages = {15},
keywords = {Adaptive neuro-fuzzy inference system, Particle swarm algorithm, Genetic algorithm, Regression test suite optimization}
}

@article{10.1007/s00521-020-05657-1,
author = {James, C. D. and Mondal, Sandeep},
title = {Optimization of decoupling point position using metaheuristic evolutionary algorithms for smart mass customization manufacturing},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {17},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05657-1},
doi = {10.1007/s00521-020-05657-1},
abstract = {In this paper, we present two metaheuristic evolutionary algorithms-based approaches to position the customer order decoupling point (CODP) in smart mass customization (SMC). SMC tries to autonomously mass customize and produce products per customer needs in Industry 4.0. SMC shown here is from the perspective of arriving at a CODP during manufacturing process flow designs meant for fast moving and complex product variants. Learning generally needs several repetitive cycles to break the complexity barrier. We make use of fruit fly and particle swarm optimization (PSO) evolutionary algorithms with the help of MATLAB programming to constantly search better fitting consecutive process modules in manufacturing chain. CODP is optimized by increasing modularity and reducing complexity through evolutionary concept. Learning-based PSO iterations are performed. The methods shown here are recommended for process flow design in a learning-oriented supply chain organization which can involve in-house and outsourced manufacturing steps. Finally, a complexity reduction model is presented which can aid in deploying this concept in design of supply chain and manufacturing flows.},
journal = {Neural Comput. Appl.},
month = sep,
pages = {11125–11155},
numpages = {31},
keywords = {Customer order decoupling point (CODP), Smart mass customization (SMC), Evolutionary algorithm (EA), Optimization, Process flow design, Learning}
}

@article{10.1016/j.patrec.2021.08.011,
author = {Mehta, Nancy and Murala, Subrahmanyam},
title = {MSAR-Net: Multi-scale attention based light-weight image super-resolution},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.08.011},
doi = {10.1016/j.patrec.2021.08.011},
journal = {Pattern Recogn. Lett.},
month = nov,
pages = {215–221},
numpages = {7},
keywords = {65D17, 65D05, 41A10, 41A05, Image super-resolution, Up and down-sampling projection block, Multi-scale attention residual block}
}

@article{10.1016/j.dsp.2021.103205,
author = {Pourebrahim, Yousef and Razzazi, Farbod and Sameti, Hossein},
title = {Semi-supervised parallel shared encoders for speech emotion recognition},
year = {2021},
issue_date = {Nov 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {118},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103205},
doi = {10.1016/j.dsp.2021.103205},
journal = {Digit. Signal Process.},
month = nov,
numpages = {11},
keywords = {Deep neural networks, Domain adaptation, Speech emotion recognition, Semi-supervised learning}
}

@article{10.1016/j.jss.2019.06.003,
author = {Capilla, Rafael and Fuentes, Lidia and Lochau, Malte},
title = {Software variability in dynamic environments},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.003},
doi = {10.1016/j.jss.2019.06.003},
journal = {J. Syst. Softw.},
month = oct,
pages = {62–64},
numpages = {3}
}

@inproceedings{10.1007/978-3-030-98682-7_13,
author = {Antonioni, Emanuele and Suriani, Vincenzo and Solimando, Filippo and Nardi, Daniele and Bloisi, Domenico D.},
title = {Learning from the Crowd: Improving the Decision Making Process in Robot Soccer Using the Audience Noise},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_13},
doi = {10.1007/978-3-030-98682-7_13},
abstract = {Fan input and support is an important component in many individual and team sports, ranging from athletics to basketball. Audience interaction provides a consistent impact on the athletes’ performance. The analysis of the crowd noise can provide a global indication on the ongoing game situation, less conditioned by subjective factors that can influence a single fan. In this work, we exploit the collective intelligence of the audience of a robot soccer match to improve the performance of the robot players. In particular, audio features extracted from the crowd noiseare used in a Reinforcement Learning process to possibly modify the game strategy. The effectiveness of the proposed approach is demonstrated by experiments on registered crowd noise samples from several past RoboCup SPL matches.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {153–164},
numpages = {12},
keywords = {Sound recognition, RoboCup SPL, Crowd noise interpretation},
location = {Sydney, NSW, Australia}
}

@article{10.1287/inte.2020.1065,
author = {Chen, Yixian and Mehrotra, Prakhar and Samala, Nitin Kishore Sai and Ahmadi, Kamilia and Jivane, Viresh and Pang, Linsey and Shrivastav, Monika and Lyman, Nate and Pleiman, Scott},
title = {A Multiobjective Optimization for Clearance in Walmart Brick-and-Mortar Stores},
year = {2021},
issue_date = {January-February 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {51},
number = {1},
issn = {0092-2102},
url = {https://doi.org/10.1287/inte.2020.1065},
doi = {10.1287/inte.2020.1065},
abstract = {We developed a novel multiobjective markdown system and deployed it across many merchandising units at Walmart. The objectives of this system are to (1) clear the stores’ excess inventory by a specified date, (2) improve revenue by minimizing the discounts needed to clear shelves, and (3) reduce the substantial cost to relabel merchandise in the stores. The underlying mathematical approach uses techniques such as deep reinforcement learning, simulation, and optimization to determine the optimal (marked-down) price. Starting in 2019, after six months of extensive testing, we implemented the new approach across all Walmart stores in the United States. The result was a high-performance model with a price-adjustment policy tailored to each store. Walmart increased its sell-through rate (i.e., the number of units sold during the markdown period divided by its inventory at the beginning of the markdown) by 21% and reduced its costs by 7%. Benefits that Walmart accrues include demographics-based store personalization, reductions in operating costs with limited numbers of price adjustments, and a dynamic time window for markdowns.},
journal = {Interfaces},
month = feb,
pages = {76–89},
numpages = {14},
keywords = {Edelman Award, deep reinforcement learning, inventory optimization, price optimization, merchandising}
}

@article{10.1007/s11042-019-7498-3,
author = {Kaur, Taranjit and Saini, Barjinder Singh and Gupta, Savita},
title = {An adaptive fuzzy K-nearest neighbor approach for MR brain tumor image classification using parameter free bat optimization algorithm},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {15},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7498-3},
doi = {10.1007/s11042-019-7498-3},
abstract = {This paper presents an automatic diagnosis system for the tumor grade classification through magnetic resonance imaging (MRI). The diagnosis system involves a region of interest (ROI) delineation using intensity and edge magnitude based multilevel thresholding algorithm. Then the intensity and the texture attributes are extracted from the segregated ROI. Subsequently, a combined approach known as Fisher+ Parameter-Free BAT (PFreeBAT) optimization is employed to derive the optimal feature subset. Finally, a novel learning approach dubbed as PFree BAT enhanced fuzzy K-nearest neighbor (FKNN) is proposed by combining FKNN with PFree BAT for the classification of MR images into two categories: High and Low-Grade. In PFree BAT enhanced FKNN, the model parameters, i.e., neighborhood size k and the fuzzy strength parameter m are adaptively specified by the PFree BAT optimization approach. Integrating PFree BAT with FKNN enhances the classification capability of the FKNN. The diagnostic system is rigorously evaluated on four MR images datasets including images from BRATS 2012 database and the Harvard repository using classification performance metrics. The empirical results illustrate that the diagnostic system reached to ceiling level of accuracy on the test MR image dataset via 5-fold cross-validation mechanism. Additionally, the proposed PFree BAT enhanced FKNN is evaluated on the Parkinson dataset (PD) from the UCI repository having the pre-extracted feature space. The proposed PFree BAT enhanced FKNN reached to an average accuracy of 98% and 97.45%. with and without feature selection on PD dataset. Moreover, solely to contrast, the performance of the proposed PFree BAT enhanced FKNN with the existing FKNN variants the experimentations were also done on six other standard datasets from KEEL repository. The results indicate that the proposed learning strategy achieves the best value of accuracy in contrast to the existing FKNN variants.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {21853–21890},
numpages = {38},
keywords = {Model parameters, Diagnosis system, PFree BAT optimization, Fuzzy K-nearest neighbor}
}

@inproceedings{10.1007/978-3-642-34327-8_33,
author = {Brugali, Davide and Gherardi, Luca and Biziak, A. and Luzzana, Andrea and Zakharov, Alexey},
title = {A reuse-oriented development process for component-based robotic systems},
year = {2012},
isbn = {9783642343261},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34327-8_33},
doi = {10.1007/978-3-642-34327-8_33},
abstract = {State of the art in robot software development mostly relies on class library reuse and only to a limited extent to component-based design. In the BRICS project we have defined a software development process that is based on the two most recent and promising approaches to software reuse, i.e. Software Product Line (SPL) and Model-Driven Engineering (MDE). The aim of this paper is to illustrate the whole software development process that we have defined for developing flexible and reusable component-based robotics libraries, to exemplify it with the case study of robust navigation functionality, and to present the software tools that we have developed for supporting the proposed process.},
booktitle = {Proceedings of the Third International Conference on Simulation, Modeling, and Programming for Autonomous Robots},
pages = {361–374},
numpages = {14},
location = {Tsukuba, Japan},
series = {SIMPAR'12}
}

@inproceedings{10.1007/978-3-030-98682-7_17,
author = {Hasselbring, Arne and Baude, Andreas},
title = {Soccer Field Boundary Detection Using Convolutional Neural Networks},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_17},
doi = {10.1007/978-3-030-98682-7_17},
abstract = {Detecting the field boundary is often one of the first steps in the vision pipeline of soccer robots. Conventional methods make use of a (possibly adaptive) green classifier, selection of boundary points and possibly model fitting. We present an approach to predict the coordinates of the field boundary column-wise in the image using a convolutional neural network. This is combined with a method to let the network predict the uncertainty of its output, which allows to fit a line model in which columns are weighted according to the network’s confidence. Experiments show that the resulting models are accurate enough in different lighting conditions as well as real-time capable. Code and data are available online (, ).},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {202–213},
numpages = {12},
location = {Sydney, NSW, Australia}
}

@article{10.1007/s10827-021-00801-9,
title = {30th Annual Computational Neuroscience Meeting: CNS*2021–Meeting Abstracts},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {49},
number = {Suppl 1},
issn = {0929-5313},
url = {https://doi.org/10.1007/s10827-021-00801-9},
doi = {10.1007/s10827-021-00801-9},
journal = {J. Comput. Neurosci.},
month = dec,
pages = {3–208},
numpages = {206}
}

@inproceedings{10.5555/2818754.2818819,
author = {Henard, Christopher and Papadakis, Mike and Harman, Mark and Le Traon, Yves},
title = {Combining multi-objective search and constraint solving for configuring large software product lines},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) feature selection involves the optimization of multiple objectives in a large and highly constrained search space. We introduce SATIBEA, that augments multi-objective search-based optimization with constraint solving to address this problem, evaluating it on five large real-world SPLs, ranging from 1,244 to 6,888 features with respect to three different solution quality indicators and two diversity metrics. The results indicate that SATIBEA statistically significantly outperforms the current state-of-the-art (p &lt; 0.01) for all five SPLs on all three quality indicators and with maximal effect size (\^{A}12 = 1.0). We also present results that demonstrate the importance of combining constraint solving with search-based optimization and the significant improvement SATIBEA produces over pure constraint solving. Finally, we demonstrate the scalability of SATIBEA: within less than half an hour, it finds thousands of constraint-satisfying optimized software products, even for the largest SPL considered in the literature to date.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {517–528},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.1016/j.infsof.2010.03.014,
author = {Alves, Vander and Niu, Nan and Alves, Carina and Valen\c{c}a, George},
title = {Requirements engineering for software product lines: A systematic literature review},
year = {2010},
issue_date = {August, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.03.014},
doi = {10.1016/j.infsof.2010.03.014},
abstract = {Context: Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed. Objective: This paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement. Method: A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009. Results: The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy. Conclusions: Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {806–820},
numpages = {15},
keywords = {Systematic literature review, Software product lines, Requirements engineering}
}

@article{10.1007/s11042-019-7251-y,
author = {Mei, Jianhan and Wu, Ziming and Chen, Xiang and Qiao, Yu and Ding, Henghui and Jiang, Xudong},
title = {DeepDeblur: text image recovery from blur to sharp},
year = {2019},
issue_date = {Jul 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {13},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7251-y},
doi = {10.1007/s11042-019-7251-y},
abstract = {Digital images could be degraded by a variety of blur during the image acquisition (i.e. relative motion of cameras, electronic noise, capturing defocus, and so on). Blurring images can be computationally modeled as the result of a convolution process with the corresponding blur kernel and thus, image deblurring can be regarded as a deconvolution operation. In this paper, we explore to deblur images by approximating blind deconvolutions using a deep neural network. Different deep neural network structures are investigated to evaluate their deblurring capabilities, which contributes to the optimal design of a network architecture. It is found that shallow and narrow networks are not capable of handling complex motion blur. We thus, present a deep network with 20 layers to cope with text image blur. In addition, a novel network structure with Sequential Highway Connections (SHC) is leveraged to gain superior convergence. The experiment results demonstrate the state-of-the-art performance of the proposed framework with the higher visual quality of the delurred images.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {18869–18885},
numpages = {17},
keywords = {Text Deblurring, Short connection, Convolutional Neural Network (CNN), Blind deconvolution}
}

@article{10.1007/s10270-018-0662-9,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
title = {Tradeoffs in modeling performance of highly configurable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0662-9},
doi = {10.1007/s10270-018-0662-9},
abstract = {Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-influence model can fit different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more configuration options have only a minor influence on the prediction error and that ignoring them when learning a performance-influence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on specific regions of the configuration space and find a sweet spot between accuracy and effort. We further analyzed the causes for the configuration options and their interactions having the observed influences on the systems' performance. We were able to identify several patterns across subject systems, such as dominant configuration options and data pipelines, that explain the influences of highly influential configuration options and interactions, and give further insights into the domain of highly configurable systems.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2265–2283},
numpages = {19},
keywords = {Variability, Software product lines, Performance-influence models, Performance prediction, Machine learning, Highly configurable software systems, Feature interactions}
}

@article{10.1287/orsc.2020.1377,
author = {Pachidi, Stella and Berends, Hans and Faraj, Samer and Huysman, Marleen},
title = {Make Way for the Algorithms: Symbolic Actions and Change in a Regime of Knowing},
year = {2021},
issue_date = {January-February 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {32},
number = {1},
issn = {1526-5455},
url = {https://doi.org/10.1287/orsc.2020.1377},
doi = {10.1287/orsc.2020.1377},
abstract = {When actors deem technological change undesirable, they may act symbolically by pretending to comply while avoiding real change. In our study of the introduction of an algorithmic technology in a sales organization, we found that such symbolic conformity led unintendedly to the full implementation of the suggested technological change. To explain this surprising outcome, we advance a regime-of-knowing lens that helps to analyze deep challenges happening under the surface during the process of technology introduction. A regime of knowing guides what is worth knowing, what actions matter to acquire this knowledge, and who has the authority to make decisions around those issues. We found that both the technologists who introduced the algorithmic technology, and the incumbent workers whose work was affected by the change, used symbolic actions to either defend the established regime of knowing or to advocate a radical change. Although the incumbent workers enacted symbolic conformity by pretending to comply with suggested changes, the technologists performed symbolic advocacy by presenting a positive side of the technological change. Ironically, because the symbolic conformity enabled and was reinforced by symbolic advocacy, reinforcing cycles of symbolic actions yielded a radical change in the sales' regime of knowing: from one focused on a deep understanding of customers via personal contact and strong relationships, to one based on model predictions from the processing of large datasets. We discuss the theoretical implications of these findings for the introduction of technology at work and for knowing in the workplace.},
journal = {Organization Science},
month = jan,
pages = {18–41},
numpages = {24},
keywords = {work, digital transformation, technology introduction, analytics, artificial intelligence, algorithmic technologies, symbolic action, knowing, knowledge}
}

@article{10.1007/s00236-017-0293-6,
author = {Damiani, Ferruccio and Padovani, Luca and Schaefer, Ina and Seidl, Christoph},
title = {A core calculus for dynamic delta-oriented programming},
year = {2018},
issue_date = {June      2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {55},
number = {4},
issn = {0001-5903},
url = {https://doi.org/10.1007/s00236-017-0293-6},
doi = {10.1007/s00236-017-0293-6},
abstract = {Delta-oriented programming (DOP) is a flexible approach to the implementation of software product lines (SPLs). Delta-oriented SPLs consist of a code base (a set of delta modules encapsulating changes to object-oriented programs) and a product line declaration (providing the connection of the delta modules with the product features). In this paper, we present a core calculus that extends DOP with the capability to switch the implemented product configuration at runtime. A dynamic delta-oriented SPL is a delta-oriented SPL with a dynamic reconfiguration graph that specifies how to switch between different feature configurations. Dynamic DOP supports also (unanticipated) software evolution such that at runtime, the product line declaration, the code base and the dynamic reconfiguration graph can be changed in any (unanticipated) way that preserves the currently running product, which is essential when evolution affects existing features. The type system of our dynamic DOP core calculus ensures that the dynamic reconfigurations lead to type safe products and do not cause runtime type errors.},
journal = {Acta Inf.},
month = jun,
pages = {269–307},
numpages = {39}
}

@article{10.3233/JIFS-179775,
author = {Yunchao, Du and Ruikai, Huang and Isaeva, Ekaterina and Rocha, \'{A}lvaro},
title = {Research on the core competitiveness of pharmaceutical listed companies based on fuzzy comprehensive evaluation},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {38},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179775},
doi = {10.3233/JIFS-179775},
abstract = {The pharmaceutical industry is an important part of our national economy, which is the combination of traditional industry and modern industry. Medicine is an industry with high technology, high risk, high investment and high return, and has always been the focus of competition in developed countries. If Chinese pharmaceutical enterprises want to stand out in such fierce competition, they must find out the core competitiveness that fits their own development path. Based on the construction of the evaluation index system of the core competitiveness of the pharmaceutical industry, this study makes an empirical analysis of Hengrui pharmaceutical based on the fuzzy comprehensive evaluation method, discusses the reasons for the formation of its core competitiveness, and puts forward effective ways for listed companies in the pharmaceutical industry to enhance their core competitiveness.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6971–6978},
numpages = {8},
keywords = {Hengrui pharmaceutical, fuzzy comprehensive evaluation, pharmaceutical listed companies, Core competitiveness}
}

@inproceedings{10.1145/3377812.3382153,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {FeatureNET: diversity-driven generation of deep learning models},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382153},
doi = {10.1145/3377812.3382153},
abstract = {We present FeatureNET, an open-source Neural Architecture Search (NAS) tool1 that generates diverse sets of Deep Learning (DL) models. FeatureNET relies on a meta-model of deep neural networks, consisting of generic configurable entities. Then, it uses tools developed in the context of software product lines to generate diverse (maximize the differences between the generated) DL models. The models are translated to Keras and can be integrated into typical machine learning pipelines. FeatureNET allows researchers to generate seamlessly a large variety of models. Thereby, it helps choosing appropriate DL models and performing experiments with diverse models (mitigating potential threats to validity). As a NAS method, FeatureNET successfully generates models performing equally well with handcrafted models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {41–44},
numpages = {4},
keywords = {neural architecture search, configuration search, NAS, AutoML},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1016/j.ins.2019.12.015,
author = {Xiao, Yanshan and Yang, Xiaozhou and Liu, Bo},
title = {A new self-paced method for multiple instance boosting learning},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {515},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.12.015},
doi = {10.1016/j.ins.2019.12.015},
journal = {Inf. Sci.},
month = apr,
pages = {80–90},
numpages = {11},
keywords = {Self-Paced learning, Multiple instance boost learning, Multiple instance learning}
}

@article{10.1016/j.patcog.2009.12.012,
author = {Derrac, Joaqu\'{\i}n and Garc\'{\i}a, Salvador and Herrera, Francisco},
title = {IFS-CoCo: Instance and feature selection based on cooperative coevolution with nearest neighbor rule},
year = {2010},
issue_date = {June, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {43},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2009.12.012},
doi = {10.1016/j.patcog.2009.12.012},
abstract = {Feature and instance selection are two effective data reduction processes which can be applied to classification tasks obtaining promising results. Although both processes are defined separately, it is possible to apply them simultaneously. This paper proposes an evolutionary model to perform feature and instance selection in nearest neighbor classification. It is based on cooperative coevolution, which has been applied to many computational problems with great success. The proposed approach is compared with a wide range of evolutionary feature and instance selection methods for classification. The results contrasted through non-parametric statistical tests show that our model outperforms previously proposed evolutionary approaches for performing data reduction processes in combination with the nearest neighbor rule.},
journal = {Pattern Recogn.},
month = jun,
pages = {2082–2105},
numpages = {24},
keywords = {Nearest neighbor, Instance selection, Feature selection, Evolutionary algorithms, Cooperative coevolution}
}

@article{10.1007/s10922-013-9265-5,
author = {Moens, Hendrik and Truyen, Eddy and Walraven, Stefan and Joosen, Wouter and Dhoedt, Bart and De Turck, Filip},
title = {Cost-Effective Feature Placement of Customizable Multi-Tenant Applications in the Cloud},
year = {2014},
issue_date = {October   2014},
publisher = {Plenum Press},
address = {USA},
volume = {22},
number = {4},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-013-9265-5},
doi = {10.1007/s10922-013-9265-5},
abstract = {Cloud computing technologies can be used to more flexibly provision application resources. By exploiting multi-tenancy, instances can be shared between users, lowering the cost of providing applications. A weakness of current cloud offerings however, is the difficulty of creating customizable applications that retain these advantages. In this article, we define a feature-based cloud resource management model, making use of Software Product Line Engineering techniques, where applications are composed of feature instances using a service-oriented architecture. We focus on how resources can be allocated in a cost-effective way within this model, a problem which we refer to as the feature placement problem. A formal description of this problem, that can be used to allocate resources in a cost-effective way, is provided. We take both the cost of failure to place features, and the cost of using servers into account, making it possible to take energy costs or the cost of public cloud infrastructure into consideration during the placement calculation. Four algorithms that can be used to solve the feature placement problem are defined. We evaluate the algorithm solutions, comparing them with the optimal solution determined using an integer linear problem solver, and evaluating the execution times of the algorithms, making use of both generated inputs and a use case based on three applications. We show that, using our approach a higher degree of multi-tenancy can be achieved, and that for the considered scenarios, taking the relationships between features into account and using application-oriented placement performs 25---40 % better than a purely feature-oriented placement.},
journal = {J. Netw. Syst. Manage.},
month = oct,
pages = {517–558},
numpages = {42},
keywords = {SPLE, Distributed computing, Cloud computing, Application placement}
}

@inproceedings{10.1145/3417990.3421396,
author = {Mussbacher, Gunter and Combemale, Benoit and Abrah\~{a}o, Silvia and Bencomo, Nelly and Burgue\~{n}o, Loli and Engels, Gregor and Kienzle, J\"{o}rg and K\"{u}hn, Thomas and Mosser, S\'{e}bastien and Sahraoui, Houari and Weyssow, Martin},
title = {Towards an assessment grid for intelligent modeling assistance},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421396},
doi = {10.1145/3417990.3421396},
abstract = {The ever-growing complexity of systems, the growing number of stakeholders, and the corresponding continuous emergence of new domain-specific modeling abstractions has led to significantly higher cognitive load on modelers. There is an urgent need to provide modelers with better, more Intelligent Modeling Assistants (IMAs). An important factor to consider is the ability to assess and compare, to learn from existing and inform future IMAs, while potentially combining them. Recently, a conceptual Reference Framework for Intelligent Modeling Assistance (RF-IMA) was proposed. RF-IMA defines the main required components and high-level properties of IMAs. In this paper, we present a detailed, level-wise definition for the properties of RF-IMA to enable a better understanding, comparison, and selection of existing and future IMAs. The proposed levels are a first step towards a comprehensive assessment grid for intelligent modeling assistance. For an initial validation of the proposed levels, we assess the existing landscape of intelligent modeling assistance and three future scenarios of intelligent modeling assistance against these levels.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {48},
numpages = {10},
keywords = {model-based software engineering, intelligent modeling assistance, integrated development environment, feedback, assessment levels, artificial intelligence},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@article{10.1016/j.eswa.2014.12.040,
author = {Fossaceca, John M. and Mazzuchi, Thomas A. and Sarkani, Shahram},
title = {MARK-ELM},
year = {2015},
issue_date = {May 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.12.040},
doi = {10.1016/j.eswa.2014.12.040},
abstract = {Apply Multiple Kernel Boosting and Multiclass KELM to Network Intrusion Detection.Tested approach on several machine learning datasets and the KDD Cup 99 dataset.Utilized Fractional Polynomial Kernels for the Network ID problem for the first time.Requires no feature selection, minimal pre-processing and works on imbalanced data.Achieves superior detection rates and lower false alarm rates than other approaches. Detection of cyber-based attacks on computer networks continues to be a relevant and challenging area of research. Daily reports of incidents appear in public media including major ex-filtrations of data for the purposes of stealing identities, credit card numbers, and intellectual property as well as to take control of network resources. Methods used by attackers constantly change in order to defeat techniques employed by information technology (IT) teams intended to discover or block intrusions. "Zero Day" attacks whose "signatures" are not yet in IT databases are continually being uncovered. Machine learning approaches have been widely used to increase the effectiveness of intrusion detection platforms. While some machine learning techniques are effective at detecting certain types of attacks, there are no known methods that can be applied universally and achieve consistent results for multiple attack types. The focus of our research is the development of a framework that combines the outputs of multiple learners in order to improve the efficacy of network intrusion on data that contains instances of multiple classes of attacks. We have chosen the Extreme Learning Machine (ELM) as the core learning algorithm due to recent research that suggests that ELMs are straightforward to implement, computationally efficient and have excellent learning performance characteristics on par with the Support Vector Machine (SVM), one of the most widely used and best performing machine learning platforms (Liu, Gao, &amp; Li, 2012). We introduce the novel Multiple Adaptive Reduced Kernel Extreme Learning Machine (MARK-ELM) which combines Multiple Kernel Boosting (Xia &amp; Hoi, 2013) with the Multiple Classification Reduced Kernel ELM (Deng, Zheng, &amp; Zhang, 2013). We tested this approach on several machine learning datasets as well as the KDD Cup 99 (Hettich &amp; Bay, 1999) intrusion detection dataset. Our results indicate that MARK-ELM works well for the majority of University of California, Irvine (UCI) Machine Learning Repository small datasets and is scalable for larger datasets. For UCI datasets we achieved performance similar to the MKBoost Support Vector Machine (SVM) approach. In our experiments we demonstrate that MARK-ELM achieves superior detection rates and much lower false alarm rates than other approaches on intrusion detection data.},
journal = {Expert Syst. Appl.},
month = may,
pages = {4062–4080},
numpages = {19},
keywords = {Network Intrusion Detection, Multiple Kernel Learning, Multiclass Classification, Machine Learning, Kernel Selection, KDD Cup 1999, Fractional Polynomial Kernels, Extreme Learning Machine, Ensemble Learning, Cyber security, Adaptive Boosting}
}

@article{10.1007/s00034-021-01674-0,
author = {Naiemi, Fatemeh and Ghods, Vahid and Khalesi, Hassan},
title = {MOSTL: An Accurate Multi-Oriented Scene Text Localization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {40},
number = {9},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-021-01674-0},
doi = {10.1007/s00034-021-01674-0},
abstract = {Automatic text localization in natural environments is the main element of many applications including self-driving cars, identifying vehicles, and providing scene information to visually impaired people. However, text in the natural and irregular scene has different degrees in orientations, shapes, and colors that make it difficult to detect. In this paper, an accurate multi-oriented scene text localization (MOSTL) is presented to obtain high efficiency of detecting text-based on convolutional neural networks. In the proposed method, an improved ReLU layer (i.ReLU) and an improved inception layer (i.inception) were introduced. Firstly, the proposed structure is used to extract low-level visual features. Then, an extra layer has been used to improve the feature extraction. The i.ReLU and i.inception layers have improved valuable information in text detection. The i.ReLU layers cause to extract some low-level features appropriately. The i.inception layers (specially 3 \texttimes{} 3 convolutions) can obtain broadly varying-sized text more effectively than a linear chain of convolution layer (without inception layers). The output of i.ReLU layers and i.inception layers was fed to an extra layer, which enables MOSTL to detect multi-oriented even curved and vertical texts. We conducted text detection experiments on well-known databases including ICDAR 2019, ICDAR 2017, ICDAR 2015, ICDAR 2003, and MSRA-TD500. MOSTL results yielded performance improvement remarkably.},
journal = {Circuits Syst. Signal Process.},
month = sep,
pages = {4452–4473},
numpages = {22},
keywords = {Curved text, Improved ReLU layer, Improved inception layer, Convolutional neural network, Multi-oriented, Object detection, Scene text localization}
}

@article{10.1016/j.compag.2019.105023,
author = {Moon, Taewon and Hong, Seojung and Choi, Ha Young and Jung, Dae Ho and Chang, Se Hong and Son, Jung Eek},
title = {Interpolation of greenhouse environment data using multilayer perceptron},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {166},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2019.105023},
doi = {10.1016/j.compag.2019.105023},
journal = {Comput. Electron. Agric.},
month = nov,
numpages = {8},
keywords = {Spline, Random forest, Multivariate regression, Linear, Data loss}
}

@inproceedings{10.1145/3394486.3403365,
author = {Mehrotra, Prakhar and Pang, Linsey and Gopalswamy, Karthick and Thangali, Avinash and Winters, Timothy and Gupte, Ketki and Kulkarni, Dnyanesh and Potnuru, Sunil and Shastry, Supreeth and Vuyyuri, Harshada},
title = {Price Investment using Prescriptive Analytics and Optimization in Retail},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403365},
doi = {10.1145/3394486.3403365},
abstract = {As the world's largest retailer, Walmart's core mission is to save people money so they can live better. We call the strategy we use to accomplish this goal our Every Day Low Price strategy. By keeping operational expenses as low as possible, we can continually apply a downward pressure on our prices, in turn increasing the amount of traffic, and ultimately, sales within our stores. In this paper, we apply Machine Learning (ML) algorithms and Operations Research techniques for forecasting and optimization to build a new price recommendation system, which improves our ability to generate price recommendations accurately and automatically. Comprised of a demand forecasting step, two optimizations, and causal inference analysis, our system was evaluated in the form of forecast backtests and live pricing experiments, both of which suggested that our approach was more effective than the current rule-based pricing system.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3136–3144},
numpages = {9},
keywords = {prescriptive analytics, optimization, causal inference, bayesian structured time series},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1007/978-3-319-07317-0_8,
author = {Lochau, Malte and Peldszus, Sven and Kowal, Matthias and Schaefer, Ina},
title = {Model-Based Testing},
year = {2014},
isbn = {9783319073163},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-07317-0_8},
doi = {10.1007/978-3-319-07317-0_8},
abstract = {Software more and more pervades our everyday lives. Hence, we have high requirements towards the trustworthiness of the software. Software testing greatly contributes to the quality assurance of modern software systems. However, as today's software system get more and more complex and exist in many different variants, we need rigorous and systematic approaches towards software testing. In this tutorial, we, first, present model-based testing as an approach for systematic test case generation, test execution and test result evaluation for single system testing. The central idea of model-based testing is to base all testing activities on an executable model-based test specification. Second, we consider model-based testing for variant-rich software systems and review two model-based software product line testing techniques. Sample-based testing generates a set of representative variants for testing, and variability-aware product line testing uses a family-based test model which contains the model-based specification of all considered product variants.},
booktitle = {Advanced Lectures of the 14th International School on Formal Methods for Executable Software Models - Volume 8483},
pages = {310–342},
numpages = {33}
}

@inproceedings{10.1007/978-3-030-87199-4_50,
author = {Sedlar, Sara and Alimi, Abib and Papadopoulo, Th\'{e}odore and Deriche, Rachid and Deslauriers-Gauthier, Samuel},
title = {A Spherical Convolutional Neural Network for White Matter Structure Imaging via dMRI},
year = {2021},
isbn = {978-3-030-87198-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87199-4_50},
doi = {10.1007/978-3-030-87199-4_50},
abstract = {Diffusion Magnetic Resonance Imaging (dMRI) is a powerful non-invasive and in-vivo imaging modality for probing brain white matter structure. Convolutional neural networks (CNNs) have been shown to be a powerful tool for many computer vision problems where the signals are acquired on a regular grid and where translational invariance is important. However, as we are considering dMRI signals that are acquired on a sphere, rotational invariance, rather than translational, is desired. In this work, we propose a spherical CNN model with fully spectral domain convolutional and non-linear layers. It provides rotational invariance and is adapted to the real nature of dMRI signals and uniform random distribution of sampling points. The proposed model is positively evaluated on the problem of estimation of neurite orientation dispersion and density imaging (NODDI) parameters on the data from Human Connectome Project (HCP).},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part III},
pages = {529–539},
numpages = {11},
keywords = {White matter micro-structures, Diffusion MRI, Spherical CNN},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-89370-5_19,
author = {Luo, Chao and Bi, Sheng and Dong, Min and Nie, Hongxu},
title = {RGB-D Based Visual Navigation Using Direction Estimation Module},
year = {2021},
isbn = {978-3-030-89369-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89370-5_19},
doi = {10.1007/978-3-030-89370-5_19},
abstract = {Target-driven visual navigation without mapping works to solve navigation problems that given a target object, mobile robots can navigate to the target object. Recently, visual navigation has been researched and improved largely by learning-based methods. However, their methods lack depth information and spatial perception, using only single RGB images. To overcome these problems, two methods are presented in this paper. Firstly, we encode visual features of objects by dynamic graph convolutional network and extract 3D spatial features for objects by 3D geometry, a high level visual feature for agent to easily understand object relationship. Secondly, as human beings, they solve this problem in two steps, first exploring a new environment to find the target object and second planning a path to arrive. Inspired by the way of humans navigation, we propose direction estimation module (DEM) based on RGB-D images. DEM provides direction estimation of the target object to our learning model by a wheel odometry. Given a target object, first stage, our agent explores an unseen scene to detect the target object. Second stage, when detected the target object, we can estimate current location of the target object by 3D geometry, after that, each step of the agent, DEM will estimate new location of target object, and give direction information of the target object from a first-view image. It can guide our agent to navigate to the target object. Our experiment results outperforms the result of state of the art method in the artificial environment AI2-Thor.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III},
pages = {252–264},
numpages = {13},
keywords = {Reinforcement learning, Direction estimation module, Mobile robot, Visual navigation},
location = {Hanoi, Vietnam}
}

@inproceedings{10.1007/978-3-030-69532-3_4,
author = {Huang, Bowen and Zhou, Jinjia and Yan, Xiao and Jing, Ming’e and Wan, Rentao and Fan, Yibo},
title = {CS-MCNet: A Video Compressive Sensing Reconstruction Network with Interpretable Motion Compensation},
year = {2020},
isbn = {978-3-030-69531-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-69532-3_4},
doi = {10.1007/978-3-030-69532-3_4},
abstract = {In this paper, a deep neural network with interpretable motion compensation called CS-MCNet is proposed to realize high-quality and real-time decoding of video compressive sensing. Firstly, explicit multi-hypothesis motion compensation is applied in our network to extract correlation information of adjacent frames (as shown in Fig.&nbsp;1), which improves the recover performance. And then, a residual module further narrows down the gap between reconstruction result and original signal. The overall architecture is interpretable by using algorithm unrolling, which brings the benefits of being able to transfer prior knowledge about the conventional algorithms. As a result, a PSNR of 22&nbsp;dB can be achieved at 64x compression ratio, which is about 4% to 9% better than state-of-the-art methods. In addition, due to the feed-forward architecture, the reconstruction can be processed by our network in real time and up&nbsp;to three orders of magnitude faster than traditional iterative methods.},
booktitle = {Computer Vision – ACCV 2020: 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 – December 4, 2020, Revised Selected Papers, Part II},
pages = {54–67},
numpages = {14},
location = {Kyoto, Japan}
}

@inproceedings{10.1145/2463372.2463545,
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud},
title = {Minimizing test suites in software product lines using weight-based genetic algorithms},
year = {2013},
isbn = {9781450319638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463372.2463545},
doi = {10.1145/2463372.2463545},
abstract = {Test minimization techniques aim at identifying and eliminating redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. In the context of software product line, we can save effort and cost in the selection and minimization of test cases for testing a specific product by modeling the product line. However, minimizing the test suite for a product requires addressing two potential issues: 1) the minimized test suite may not cover all test requirements compared with the original suite; 2) the minimized test suite may have less fault revealing capability than the original suite. In this paper, we apply weight-based Genetic Algorithms (GAs) to minimize the test suite for testing a product, while preserving fault detection capability and testing coverage of the original test suite. The challenge behind is to define an appropriate fitness function, which is able to preserve the coverage of complex testing criteria (e.g., Combinatorial Interaction Testing criterion). Based on the defined fitness function, we have empirically evaluated three different weight-based GAs on an industrial case study provided by Cisco Systems, Inc. Norway. We also presented our results of applying the three weight-based GAs on five existing case studies from the literature. Based on these case studies, we conclude that among the three weight-based GAs, Random-Weighted GA (RWGA) achieved significantly better performance than the other ones.},
booktitle = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation},
pages = {1493–1500},
numpages = {8},
keywords = {weight-based gas, test minimization, feature pairwise coverage, fault detection capability},
location = {Amsterdam, The Netherlands},
series = {GECCO '13}
}

@inproceedings{10.1007/978-3-030-58571-6_2,
author = {Du, Heming and Yu, Xin and Zheng, Liang},
title = {Learning Object Relation Graph and Tentative Policy for Visual Navigation},
year = {2020},
isbn = {978-3-030-58570-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58571-6_2},
doi = {10.1007/978-3-030-58571-6_2},
abstract = {Target-driven visual navigation aims at navigating an agent towards a given target based on the observation of the agent. In this task, it is critical to learn informative visual representation and robust navigation policy. Aiming to improve these two components, this paper proposes three complementary techniques, object relation graph (ORG), trial-driven imitation learning (IL), and a memory-augmented tentative policy network (TPN). ORG improves visual representation learning by integrating object relationships, including category closeness and spatial correlations, e.g., a TV usually co-occurs with a remote spatially. Both Trial-driven IL and TPN underlie robust navigation policy, instructing the agent to escape from deadlock states, such as looping or being stuck. Specifically, trial-driven IL is a type of supervision used in policy network training, while TPN, mimicking the IL supervision in unseen environment, is applied in testing. Experiment in the artificial environment AI2-Thor validates that each of the techniques is effective. When combined, the techniques bring significantly improvement over baseline methods in navigation effectiveness and efficiency in unseen environments. We report 22.8% and 23.5% increase in success rate and Success weighted by Path Length (SPL), respectively. The code is available at .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VII},
pages = {19–34},
numpages = {16},
keywords = {Visual navigation, Tentative policy learning, Imitation learning, Graph},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/3397271.3401041,
author = {Chen, Fanglin and Liu, Xiao and Proserpio, Davide and Troncoso, Isamar and Xiong, Feiyu},
title = {Studying Product Competition Using Representation Learning},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401041},
doi = {10.1145/3397271.3401041},
abstract = {Studying competition and market structure at the product level instead of brand level can provide firms with insights on cannibalization and product line optimization. However, it is computationally challenging to analyze product-level competition for the millions of products available on e-commerce platforms. We introduce Product2Vec, a method based on the representation learning algorithm Word2Vec, to study product-level competition, when the number of products is large. The proposed model takes shopping baskets as inputs and, for every product, generates a low-dimensional embedding that preserves important product information. In order for the product embeddings to be useful for firm strategic decision making, we leverage economic theories and causal inference to propose two modifications to Word2Vec. First of all, we create two measures, complementarity and exchangeability, that allow us to determine whether product pairs are complements or substitutes. Second, we combine these vectors with random utility-based choice models to forecast demand. To accurately estimate price elasticities, i.e., how demand responds to changes in price, we modify Word2Vec by removing the influence of price from the product vectors. We show that, compared with state-of-the-art models, our approach is faster, and can produce more accurate demand forecasts and price elasticities.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1261–1268},
numpages = {8},
keywords = {representation learning, product2vec, product competition},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1016/j.knosys.2013.01.018,
author = {Fern\'{a}Ndez, Alberto and L\'{o}Pez, Victoria and Galar, Mikel and Del Jesus, Mar\'{\i}A Jos\'{e} and Herrera, Francisco},
title = {Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {42},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2013.01.018},
doi = {10.1016/j.knosys.2013.01.018},
abstract = {The imbalanced class problem is related to the real-world application of classification in engineering. It is characterised by a very different distribution of examples among the classes. The condition of multiple imbalanced classes is more restrictive when the aim of the final system is to obtain the most accurate precision for each of the concepts of the problem. The goal of this work is to provide a thorough experimental analysis that will allow us to determine the behaviour of the different approaches proposed in the specialised literature. First, we will make use of binarization schemes, i.e., one versus one and one versus all, in order to apply the standard approaches to solving binary class imbalanced problems. Second, we will apply several ad hoc procedures which have been designed for the scenario of imbalanced data-sets with multiple classes. This experimental study will include several well-known algorithms from the literature such as decision trees, support vector machines and instance-based learning, with the intention of obtaining global conclusions from different classification paradigms. The extracted findings will be supported by a statistical comparative analysis using more than 20 data-sets from the KEEL repository.},
journal = {Know.-Based Syst.},
month = apr,
pages = {97–110},
numpages = {14},
keywords = {Preprocessing, Pairwise learning, Multi-classification, Imbalanced data-sets, Cost-sensitive learning}
}

@inproceedings{10.1007/978-3-319-35122-3_3,
author = {Font, Jaime and Arcega, Lorena and Haugen, \O{}ystein and Cetina, Carlos},
title = {Feature Location in Model-Based Software Product Lines Through a Genetic Algorithm},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_3},
doi = {10.1007/978-3-319-35122-3_3},
abstract = {When following an extractive approach to build a model-based Software Product Line SPL from a set of existing products, features have to be located across the product models. The approaches that produce best results combine model comparisons with the knowledge from the domain experts to locate the features. However, when the domain expert fails to provide accurate information, the semi-automated approach faces challenges. To cope with this issue we propose a genetic algorithm to feature location in model-based SPLs. We have an oracle from an industrial environment that makes it possible to evaluate the results of the approaches. As a result, the proposed approach is able to provide solutions upon inaccurate information on part of the domain expert while the compared approach fails to provide a solution when the information provided by the domain expert is not accurate enough.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {39–54},
numpages = {16},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@article{10.1504/IJAOSE.2008.016800,
author = {Verstraete, Paul and Germain, Bart Saint and Valckenaers, Paul and Brussel, Hendrik Van and Belle, Jan Van and Hadeli},
title = {Engineering manufacturing control systems using PROSA and delegate MAS},
year = {2008},
issue_date = {January 2008},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {2},
number = {1},
issn = {1746-1375},
url = {https://doi.org/10.1504/IJAOSE.2008.016800},
doi = {10.1504/IJAOSE.2008.016800},
abstract = {This paper presents a systematic description of a reusable software architecture for multiagent systems in the domain of manufacturing control. The architectural description consolidates the authors' expertise in this area. Until now, the research has taken a manufacturing control perspective of multiagent systems. The research team has focused on providing benefits to the manufacturing control domain by designing a novel type of control system. This paper takes a software architectural perspective of multiagent manufacturing control. The systematic description specifies a software product line architecture for manufacturing control. The paper describes the assets of the software product line architecture and how these assets can be combined.},
journal = {Int. J. Agent-Oriented Softw. Eng.},
month = jan,
pages = {62–89},
numpages = {28},
keywords = {software reuse, software architecture, multi-agent systems, manufacturing control, agent-based systems, MASs}
}

@inproceedings{10.5555/3060832.3060891,
author = {Pi, Te and Li, Xi and Zhang, Zhongfei and Meng, Deyu and Wu, Fei and Xiao, Jun and Zhuang, Yueting},
title = {Self-paced boost learning for classification},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Effectiveness and robustness are two essential aspects of supervised learning studies. For effective learning, ensemble methods are developed to build a strong effective model from ensemble of weak models. For robust learning, self-paced learning (SPL) is proposed to learn in a self-controlled pace from easy samples to complex ones. Motivated by simultaneously enhancing the learning effectiveness and robustness, we propose a unified framework, Self-Paced Boost Learning (SPBL). With an adaptive from-easy-to-hard pace in boosting process, SPBL asymptotically guides the model to focus more on the insufficiently learned samples with higher reliability. Via a max-margin boosting optimization with self-paced sample selection, SPBL is capable of capturing the intrinsic inter-class discriminative patterns while ensuring the reliability of the samples involved in learning. We formulate SPBL as a fully-corrective optimization for classification. The experiments on several real-world datasets show the superiority of SPBL in terms of both effectiveness and robustness.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1932–1938},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@inproceedings{10.5555/3505326.3505356,
author = {Ravari, Yaser Norouzzadeh and Spronck, Pieter and Sifa, Rafet and Drachen, Anders},
title = {Predicting victory in a hybrid online competitive game: the case of Destiny},
year = {2017},
isbn = {978-1-57735-791-9},
publisher = {AAAI Press},
abstract = {Competitive multi-player game play is a common feature in major commercial titles, and has formed the foundation for esports. In this paper, the question whether it is possible to predict match outcomes in First Person Shooter-type multiplayer competitive games with mixed genres is addressed. The case employed is Destiny, which forms a hybrid title combining Massively Multi-player Online Role-Playing game features and First-Person Shooter games. Destiny provides the opportunity to investigate prediction of the match outcome, as well as the influence of performance metrics on the match results in a hybrid multi-player major commercial title. Two groups of models are presented for predicting match results: One group predicts match results for each individual game mode and the other group predicts match results in general, without considering specific game modes. Models achieve a performance between 63% and 99% in terms of average precision, with a higher performance recorded for the models trained on specific multi-player game modes, of which Destiny has several. We also analyzed performance metrics and their influence for each model. The results show that many key shooter performance metrics such as Kill/Death ratio are relevant across game modes, but also that some performance metrics are mainly important for specific competitive game modes. The results indicate that reliable match prediction is possible in FPS-type esports games.},
booktitle = {Proceedings of the Thirteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
articleno = {30},
numpages = {7},
location = {Little Cottonwood Canyon, Utah, USA},
series = {AIIDE'17}
}

@inproceedings{10.5555/3540261.3542303,
author = {Hahn, Meera and Chaplot, Devendra and Tulsiani, Shubham and Mukadam, Mustafa and Rehg, James M. and Gupta, Abhinav},
title = {No RL, no simulation: learning to navigate without navigating},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most prior methods for learning navigation policies require access to simulation environments, as they need online policy interaction and rely on ground-truth maps for rewards. However, building simulators is expensive (requires manual effort for each and every scene) and creates challenges in transferring learned policies to robotic platforms in the real-world, due to the sim-to-real domain gap. In this paper, we pose a simple question: Do we really need active interaction, ground-truth maps or even reinforcement-learning (RL) in order to solve the image-goal navigation task? We propose a self-supervised approach to learn to navigate from only passive videos of roaming. Our approach, No RL, No Simulator (NRNS), is simple and scalable, yet highly effective. NRNS outperforms RL-based formulations by a significant margin. We present NRNS as a strong baseline for any future image-based navigation tasks that use RL or Simulation.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2042},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.5555/3491440.3491754,
author = {Han, Zhongyi and Gui, Xian-Jin and Cui, Chaoran and Yin, Yilong},
title = {Towards accurate and robust domain adaptation under noisy environments},
year = {2021},
isbn = {9780999241165},
abstract = {In non-stationary environments, learning machines usually confront the domain adaptation scenario where the data distribution does change over time. Previous domain adaptation works have achieved great success in theory and practice. However, they always lose robustness in noisy environments where the labels and features of examples from the source domain become corrupted. In this paper, we report our attempt towards achieving accurate noise-robust domain adaptation. We first give a theoretical analysis that reveals how harmful noises influence unsupervised domain adaptation. To eliminate the effect of label noise, we propose an offline curriculum learning for minimizing a newly-defined empirical source risk. To reduce the impact of feature noise, we propose a proxy distribution based margin discrepancy. We seamlessly transform our methods into an adversarial network that performs efficient joint optimization for them, successfully mitigating the negative influence from both data corruption and distribution shift. A series of empirical studies show that our algorithm remarkably outperforms state of the art, over 10% accuracy improvements in some domain adaptation tasks under noisy environments.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {314},
numpages = {8},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@article{10.1016/j.patcog.2019.107173,
author = {Song, Liangchen and Wang, Cheng and Zhang, Lefei and Du, Bo and Zhang, Qian and Huang, Chang and Wang, Xinggang},
title = {Unsupervised domain adaptive re-identification: Theory and practice},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {102},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107173},
doi = {10.1016/j.patcog.2019.107173},
journal = {Pattern Recogn.},
month = jun,
numpages = {11},
keywords = {Unsupervised domain adaptation, Person re-identification}
}

@article{10.1109/TPAMI.2020.2972281,
author = {Wang, Xin and Huang, Qiuyuan and Celikyilmaz, Asli and Gao, Jianfeng and Shen, Dinghan and Wang, Yuan-Fang and Wang, William Yang and Zhang, Lei},
title = {Vision-Language Navigation Policy Learning and Adaptation},
year = {2021},
issue_date = {Dec. 2021},
publisher = {IEEE Computer Society},
address = {USA},
volume = {43},
number = {12},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2020.2972281},
doi = {10.1109/TPAMI.2020.2972281},
abstract = {Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms baseline methods by 10 percent on Success Rate weighted by Path Length (SPL) and achieves the state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore and adapt to unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7 to 11.7 percent).},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = dec,
pages = {4205–4216},
numpages = {12}
}

@article{10.1007/s11192-019-03307-5,
author = {Tattershall, E. and Nenadic, G. and Stevens, R. D.},
title = {Detecting bursty terms in computer science research},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {122},
number = {1},
issn = {0138-9130},
url = {https://doi.org/10.1007/s11192-019-03307-5},
doi = {10.1007/s11192-019-03307-5},
abstract = {Research topics rise and fall in popularity over time, some more swiftly than others. The fastest rising topics are typically called bursts; for example “deep learning”, “internet of things” and “big data”. Being able to automatically detect and track bursty terms in the literature could give insight into how scientific thought evolves over time. In this paper, we take a trend detection algorithm from stock market analysis and apply it to over 30&nbsp;years of computer science research abstracts, treating the prevalence of each term in the dataset like the price of a stock. Unlike previous work in this domain, we use the free text of abstracts and titles, resulting in a finer-grained analysis. We report a list of bursty terms, and then use historical data to build a classifier to predict whether they will rise or fall in popularity in the future, obtaining accuracy in the region of 80%. The proposed methodology can be applied to any time-ordered collection of text to yield past and present bursty terms and predict their probable fate.},
journal = {Scientometrics},
month = jan,
pages = {681–699},
numpages = {19},
keywords = {MACD, DBLP, Machine learning, Term life cycles, Bibliometrics, Computer science}
}

@inproceedings{10.1007/978-3-030-98682-7_11,
author = {Blumenkamp, Jan and Baude, Andreas and Laue, Tim},
title = {Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_11},
doi = {10.1007/978-3-030-98682-7_11},
abstract = {Deep learning approaches have become the standard solution to many problems in computer vision and robotics, but obtaining sufficient training data in high enough quality is challenging, as human labor is error prone, time consuming, and expensive. Solutions based on simulation have become more popular in recent years, but the gap between simulation and reality is still a major issue. In this paper, we introduce a novel method for augmenting synthetic image data through unsupervised image-to-image translation by applying the style of real world images to simulated images with open source frameworks. The generated dataset is combined with conventional augmentation methods and is then applied to a neural network model running in real-time on autonomous soccer robots. Our evaluation shows a significant improvement compared to models trained on images generated entirely in simulation.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {127–139},
numpages = {13},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/2602576.2602590,
author = {Olsson, Tobias and Toll, Daniel and Wingkvist, Anna and Ericsson, Morgan},
title = {Evaluation of a static architectural conformance checking method in a line of computer games},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602590},
doi = {10.1145/2602576.2602590},
abstract = {We present an evaluation of a simple method to find architectural problems in a product line of computer games. The method uses dependencies (direct, indirect, or no) to automatically classify types in the implementation to high-level components in the product line architecture. We use a commercially available tool to analyse dependencies in the source code. The automatic classification of types is compared to a manual classification by the developer, and all mismatches are reported. To evaluate the method, we inspect the source code and look for a pre-defined set of architectural problems in all types. We compare the set of types that contained problems to the set of types where the manual and automatic classification disagreed to determine precision and recall. We also investigate what changes are needed to correct the found mismatches by either designing and implementing changes in the source code or refining the automatic classification. Our evaluation shows that the simple method is effective at detecting architectural problems in a product line of four games. The method is lightweight, customisable and easy to implement early in the development cycle.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {113–118},
numpages = {6},
keywords = {static conformance checking, product line architecture, model-view-controller, computer game, MVC},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@inproceedings{10.1145/3368089.3409700,
author = {Cambronero, Jos\'{e} P. and Cito, J\"{u}rgen and Rinard, Martin C.},
title = {AMS: generating AutoML search spaces from weak specifications},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409700},
doi = {10.1145/3368089.3409700},
abstract = {We consider a usage model for automated machine learning (AutoML) in which users can influence the generated pipeline by providing a weak pipeline specification: an unordered set of API components from which the AutoML system draws the components it places into the generated pipeline. Such specifications allow users to express preferences over the components that appear in the pipeline, for example a desire for interpretable components to appear in the pipeline. We present AMS, an approach to automatically strengthen weak specifications to include unspecified complementary and functionally related API components, populate the space of hyperparameters and their values, and pair this configuration with a search procedure to produce a strong pipeline specification: a full description of the search space for candidate pipelines. ams uses normalized pointwise mutual information on a code corpus to identify complementary components, BM25 as a lexical similarity score over the target API's documentation to identify functionally related components, and frequency distributions in the code corpus to extract key hyperparameters and values. We show that strengthened specifications can produce pipelines that outperform the pipelines generated from the initial weak specification and an expert-annotated variant, while producing pipelines that still reflect the user preferences captured in the original weak specification.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {763–774},
numpages = {12},
keywords = {search-based software engineering, program mining, automated machine learning},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1007/s10766-016-0417-6,
author = {Allombert, V. and Gava, F. and Tesson, J.},
title = {Multi-ML: Programming Multi-BSP Algorithms in ML},
year = {2017},
issue_date = {April     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {2},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-016-0417-6},
doi = {10.1007/s10766-016-0417-6},
abstract = {bsp is a bridging model between abstract execution and concrete parallel systems. Structure and abstraction brought by bsp allow to have portable parallel programs with scalable performance predictions, without dealing with low-level details of architectures. In the past, we designed bsml for programming bsp algorithms in ml. However, the simplicity of the bsp model does not fit the complexity of today's hierarchical architectures such as clusters of machines with multiple multi-core processors. The multi-bsp model is an extension of the bsp model which brings a tree-based view of nested components of hierarchical architectures. To program multi-bsp algorithms in ml, we propose the multi-ml language as an extension of bsml where a specific kind of recursion is used to go through a hierarchy of computing nodes. We define a formal semantics of the language and present preliminary experiments which show performance improvements with respect to bsml.},
journal = {Int. J. Parallel Program.},
month = apr,
pages = {340–361},
numpages = {22},
keywords = {multi-bsp, ml, bsp, Parallel programming}
}

@article{10.1007/s11554-021-01113-y,
author = {Ngo, Tan D. and Bui, Tuyen T. and Pham, Tuan M. and Thai, Hong T. B. and Nguyen, Giang L. and Nguyen, Tu N.},
title = {Image deconvolution for optical small satellite with deep learning and real-time GPU acceleration},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {5},
issn = {1861-8200},
url = {https://doi.org/10.1007/s11554-021-01113-y},
doi = {10.1007/s11554-021-01113-y},
abstract = {In-orbit optical-imaging instruments may suffer from degradations due to space environment impacts or long-time operation. The degradation causes blurring on the image received from the ground. Degradations come from defocus and spherical aberrations cause blurring on the received image. Image deblurring should be done in pre-processing step to compensate the sensor bad impacts. The aberrations are modeled by Zernike polynomials and treated by deep learning in deblurring method. This paper presents a method to deconvolve the acquired data to improve the image quality. A convolution neural network is trained to estimate the point spread function (PSF) parameters using acquired images over satellite calibration site with specific pattern. Image deconvolution is performed to obtain image signal-to-noise (SNR) and modulation transfer function (MTF) improvement. Technical and image data used for modeling and experiment are used from VNREDSat-1 satellite (the first operational Vietnam Earth observation optical small satellite). The experiment is performed on computers accelerated by graphics processing units (GPU) to ensure fast computation.},
journal = {J. Real-Time Image Process.},
month = oct,
pages = {1697–1710},
numpages = {14},
keywords = {Real-time, Point spread function, Modulation transfer function, Remote sensing, Convolution neural network, Deep learning}
}

@article{10.1016/j.dsp.2021.103106,
author = {Zhang, Hai and Xie, Qiangqiang and Lu, Bei and Gai, Shan},
title = {Dual attention residual group networks for single image deraining},
year = {2021},
issue_date = {Sep 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103106},
doi = {10.1016/j.dsp.2021.103106},
journal = {Digit. Signal Process.},
month = sep,
numpages = {11},
keywords = {Single image rain removal, Channel attention, Residual groups, Spatial attention}
}

@article{10.1007/s11063-020-10286-9,
author = {Li, Li and Zhao, Kaiyi and Li, Sicong and Sun, Ruizhi and Cai, Saihua},
title = {Extreme Learning Machine for Supervised Classification with Self-paced Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10286-9},
doi = {10.1007/s11063-020-10286-9},
abstract = {The extreme learning machine (ELM), a typical machine learning algorithm based on feedforward neural network, has been widely used in classification, clustering, regression and feature learning. However, the traditional ELM learns all samples at once, and sample weights of traditional methods are defined before the learning process and they will not change during the learning process. So, its performance is vulnerable to noisy data and outliers, finding a way to solve this problem is meaningful. In this work, we propose a model of self-paced ELM named SP-ELM for binary classification and multi-classification originated from the self-paced learning paradigm. Concretely, the algorithm takes the importance of samples into account according to the loss of predicted value and real value, and it establishes the model from the simple samples to complex samples. By setting certain restrictions, the influence of complex data on the model is reduced. Four different self-paced regularization terms are adopted in the paper to select the instances. Experimental results demonstrate the effectiveness and of the proposed method by comparing it with other improved ELMs.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1723–1744},
numpages = {22},
keywords = {Accuracy, Self-paced learning, Extreme learning machine, Classification}
}

@article{10.1016/j.sigpro.2020.107466,
author = {Zhu, Qi and Xu, Xiangyu and Yuan, Ning and Zhang, Zheng and Guan, Donghai and Huang, Sheng-Jun and Zhang, Daoqiang},
title = {Latent correlation embedded discriminative multi-modal data fusion},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {171},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2020.107466},
doi = {10.1016/j.sigpro.2020.107466},
journal = {Signal Process.},
month = jun,
numpages = {11},
keywords = {Sparse representation, Self-paced learning, Classification, Multi-modal data fusion}
}

@article{10.1016/j.jbi.2008.12.012,
author = {Saha, Sujan Kumar and Sarkar, Sudeshna and Mitra, Pabitra},
title = {Feature selection techniques for maximum entropy based biomedical named entity recognition},
year = {2009},
issue_date = {October, 2009},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {42},
number = {5},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2008.12.012},
doi = {10.1016/j.jbi.2008.12.012},
abstract = {Named entity recognition is an extremely important and fundamental task of biomedical text mining. Biomedical named entities include mentions of proteins, genes, DNA, RNA, etc which often have complex structures, but it is challenging to identify and classify such entities. Machine learning methods like CRF, MEMM and SVM have been widely used for learning to recognize such entities from an annotated corpus. The identification of appropriate feature templates and the selection of the important feature values play a very important role in the success of these methods. In this paper, we provide a study on word clustering and selection based feature reduction approaches for named entity recognition using a maximum entropy classifier. The identification and selection of features are largely done automatically without using domain knowledge. The performance of the system is found to be superior to existing systems which do not use domain knowledge.},
journal = {J. of Biomedical Informatics},
month = oct,
pages = {905–911},
numpages = {7},
keywords = {Maximum entropy classifier, Machine learning, Feature selection, Feature reduction, Biomedical named entity recognition}
}

@article{10.1016/j.cor.2013.05.010,
author = {Gilbert, Fran\c{c}ois and Marcotte, Patrice and Savard, Gilles},
title = {Logit network pricing},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {41},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2013.05.010},
doi = {10.1016/j.cor.2013.05.010},
abstract = {In this paper, we address a pricing problem defined over a transportation network, and where the underlying flow is assigned according to the logit model. This setting yields a nonlinear optimization problem that yet possesses strong combinatorial features. Taking advantage of the model's analytical properties, we characterize its first-order optimality conditions and introduce rules that simplify the network topology, while leaving the structure of optimal solutions unchanged. Based on these results, a class of unimodal instances is identified. Finally, the connection with the more classical problem known as 'product line pricing' in economics is emphasized.},
journal = {Comput. Oper. Res.},
month = jan,
pages = {291–298},
numpages = {8},
keywords = {Product line design, Network pricing, Discrete choice models}
}

@article{10.1016/j.knosys.2016.05.048,
author = {Zhang, Zhongliang and Krawczyk, Bartosz and Garc\`{\i}a, Salvador and Rosales-P\'{e}rez, Alejandro and Herrera, Francisco},
title = {Empowering one-vs-one decomposition with ensemble learning for multi-class imbalanced data},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.05.048},
doi = {10.1016/j.knosys.2016.05.048},
abstract = {Extending binary ensemble techniques to multi-class imbalanced data.OVO scheme enhancement for multi-class imbalanced data by ensemble learning.A complete experimental study of comparison of the ensemble learning techniques with OVO.Study of the impact of base classifiers used in the proposed scenario. Multi-class imbalance classification problems occur in many real-world applications, which suffer from the quite different distribution of classes. Decomposition strategies are well-known techniques to address the classification problems involving multiple classes. Among them binary approaches using one-vs-one and one-vs-all has gained a significant attention from the research community. They allow to divide multi-class problems into several easier-to-solve two-class sub-problems. In this study we develop an exhaustive empirical analysis to explore the possibility of empowering the one-vs-one scheme for multi-class imbalance classification problems with applying binary ensemble learning approaches. We examine several state-of-the-art ensemble learning methods proposed for addressing the imbalance problems to solve the pairwise tasks derived from the multi-class data set. Then the aggregation strategy is employed to combine the binary ensemble outputs to reconstruct the original multi-class task. We present a detailed experimental study of the proposed approach, supported by the statistical analysis. The results indicate the high effectiveness of ensemble learning with one-vs-one scheme in dealing with the multi-class imbalance classification problems.},
journal = {Know.-Based Syst.},
month = aug,
pages = {251–263},
numpages = {13},
keywords = {Multi-class classification, Imbalanced data, Ensemble learning, Classifier combination, Binary decomposition}
}

@inproceedings{10.1007/978-3-030-58539-6_2,
author = {Chen, Changan and Jain, Unnat and Schissler, Carl and Gari, Sebastia Vicenc Amengual and Al-Halah, Ziad and Ithapu, Vamsi Krishna and Robinson, Philip and Grauman, Kristen},
title = {SoundSpaces: Audio-Visual Navigation in&nbsp;3D&nbsp;Environments},
year = {2020},
isbn = {978-3-030-58538-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58539-6_2},
doi = {10.1007/978-3-030-58539-6_2},
abstract = {Moving around in the world is naturally a multisensory experience, but today’s embodied agents are deaf—restricted to solely their visual perception of the environment. We introduce audio-visual navigation for complex, acoustically and visually realistic 3D environments. By both seeing and hearing, the agent must learn to navigate to a sounding object. We propose a multi-modal deep reinforcement learning approach to train navigation policies end-to-end from a stream of egocentric audio-visual observations, allowing the agent to (1) discover elements of the geometry of the physical space indicated by the reverberating audio and (2) detect and follow sound-emitting targets. We further introduce SoundSpaces: a first-of-its-kind dataset of audio renderings based on geometrical acoustic simulations for two sets of publicly available 3D environments (Matterport3D and Replica), and we instrument Habitat to support the new sensor, making it possible to insert arbitrary sound sources in an array of real-world scanned environments. Our results show that audio greatly benefits embodied visual navigation in 3D spaces, and our work lays groundwork for new research in embodied AI with audio-visual perception. Project: .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI},
pages = {17–36},
numpages = {20},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1007/978-3-030-58539-6_16,
author = {Majumdar, Arjun and Shrivastava, Ayush and Lee, Stefan and Anderson, Peter and Parikh, Devi and Batra, Dhruv},
title = {Improving Vision-and-Language Navigation with Image-Text Pairs from the Web},
year = {2020},
isbn = {978-3-030-58538-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58539-6_16},
doi = {10.1007/978-3-030-58539-6_16},
abstract = {Following a navigation instruction such as ‘Walk down the stairs and stop at the brown sofa’ requires embodied AI agents to ground referenced scene elements referenced (e.g. ‘stairs’) to visual content in the environment (pixels corresponding to ‘stairs’). We ask the following question – can we leverage abundant ‘disembodied’ web-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn the visual groundings that improve performance on a relatively data-starved embodied perception task (Vision-and-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction (‘...stop at the brown sofa’) and a trajectory of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN – outperforming prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful – with their combination resulting in further gains.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI},
pages = {259–274},
numpages = {16},
keywords = {Embodied AI, Transfer learning, Vision-and-language navigation},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/3491440.3492028,
author = {Chen, Cheng and Luo, Luo and Zhang, Weinan and Yu, Yong and Lian, Yijiang},
title = {Efficient and robust high-dimensional linear contextual bandits},
year = {2021},
isbn = {9780999241165},
abstract = {The linear contextual bandits is a sequential decision-making problem where an agent decides among sequential actions given their corresponding contexts. Since large-scale data sets become more and more common, we study the linear contextual bandits in high-dimensional situations. Recent works focus on employing matrix sketching methods to accelerating contextual bandits. However, the matrix approximation error will bring additional terms to the regret bound. In this paper we first propose a novel matrix sketching method which is called Spectral Compensation Frequent Directions (SCFD). Then we propose an efficient approach for contextual bandits by adopting SCFD to approximate the covariance matrices. By maintaining and manipulating sketched matrices, our method only needs O(md) space and O(md) update time in each round, where d is the dimensionality of the data and m is the sketching size. Theoretical analysis reveals that our method has better regret bounds than previous methods in high-dimensional cases. Experimental results demonstrate the effectiveness of our algorithm and verify our theoretical guarantees.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {588},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {deep learning, constraint solving, configurable systems, AutoML},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1016/j.engappai.2013.07.006,
author = {Sanchez-Anguix, Victor and Julian, Vicente and Botti, Vicente and Garc\'{\i}a-Fornes, Ana},
title = {Tasks for agent-based negotiation teams: Analysis, review, and challenges},
year = {2013},
issue_date = {November, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {26},
number = {10},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2013.07.006},
doi = {10.1016/j.engappai.2013.07.006},
abstract = {An agent-based negotiation team is a group of interdependent agents that join together as a single negotiation party due to their shared interests in the negotiation at hand. The reasons to employ an agent-based negotiation team may vary: (i) more computation and parallelization capabilities; (ii) unite agents with different expertise and skills whose joint work makes it possible to tackle complex negotiation domains; (iii) the necessity to represent different stakeholders or different preferences in the same party (e.g., organizations, countries, and married couple). The topic of agent-based negotiation teams has been recently introduced in multi-agent research. Therefore, it is necessary to identify good practices, challenges, and related research that may help in advancing the state-of-the-art in agent-based negotiation teams. For that reason, in this article we review the tasks to be carried out by agent-based negotiation teams. Each task is analyzed and related with current advances in different research areas. The analysis aims to identify special challenges that may arise due to the particularities of agent-based negotiation teams.},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
pages = {2480–2494},
numpages = {15},
keywords = {Negotiation teams, Multiagent systems, Automated negotiation, Agreement technologies}
}

@inproceedings{10.5555/3491440.3492078,
author = {Wang, Xintong and Wellman, Michael P.},
title = {Market manipulation: an adversarial learning framework for detection and evasion},
year = {2021},
isbn = {9780999241165},
abstract = {We propose an adversarial learning framework to capture the evolving game between a regulator who develops tools to detect market manipulation and a manipulator who obfuscates actions to evade detection. The model includes three main parts: (1) a generator that learns to adapt original manipulation order streams to resemble trading patterns of a normal trader while preserving the manipulation intent; (2) a discriminator that differentiates the adversarially adapted manipulation order streams from normal trading activities; and (3) an agent-based simulator that evaluates the manipulation effect of adapted outputs. We conduct experiments on simulated order streams associated with a manipulator and a market-making agent respectively. We show examples of adapted manipulation order streams that mimic a specified market maker's quoting patterns and appear qualitatively different from the original manipulation strategy we implemented in the simulator. These results demonstrate the possibility of automatically generating a diverse set of (unseen) manipulation strategies that can facilitate the training of more robust detection algorithms.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {638},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@article{10.5555/1839525.1839531,
author = {Ueta, Atsushi and Yairi, Takehisa and Kanazaki, Hirofumi and Machida, Kazuo},
title = {Map building without localization by estimation of inter-feature distances},
year = {2010},
issue_date = {December 2010},
publisher = {IOS Press},
address = {NLD},
volume = {14},
number = {4},
issn = {1088-467X},
abstract = {This paper proposes an alternative solution to a mapping problem in two different cases; when bearing measurements to features (landmarks) and odometry are measured and when bearing and range measurements to features are measured. Our approach named M-SEIFD (Mapping by Sequential Estimation of Inter-Feature Distances) first estimates inter-feature distances, then finds global position of all the features by enhanced multi-dimensional scaling (MDS). M-SEIFD is different from the conventional SLAM methods based on Bayesian filtering in that robot self-localization is not compulsory and that M-SEIFD is able to utilize prior information about relative distances among features directly. We show that M-SEIFD is able to achieve a decent map of features both in simulation and in real-world environment with a mobile robot.},
journal = {Intell. Data Anal.},
month = dec,
pages = {515–529},
numpages = {15},
keywords = {multi-dimensional scaling, mobile robot, Mapping}
}

@article{10.1016/j.artint.2007.05.008,
author = {Denundefinedux, Thierry},
title = {Conjunctive and disjunctive combination of belief functions induced by nondistinct bodies of evidence},
year = {2008},
issue_date = {February, 2008},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {172},
number = {2–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2007.05.008},
doi = {10.1016/j.artint.2007.05.008},
abstract = {Dempster's rule plays a central role in the theory of belief functions. However, it assumes the combined bodies of evidence to be distinct, an assumption which is not always verified in practice. In this paper, a new operator, the cautious rule of combination, is introduced. This operator is commutative, associative and idempotent. This latter property makes it suitable to combine belief functions induced by reliable, but possibly overlapping bodies of evidence. A dual operator, the bold disjunctive rule, is also introduced. This operator is also commutative, associative and idempotent, and can be used to combine belief functions issues from possibly overlapping and unreliable sources. Finally, the cautious and bold rules are shown to be particular members of infinite families of conjunctive and disjunctive combination rules based on triangular norms and conorms.},
journal = {Artif. Intell.},
month = feb,
pages = {234–264},
numpages = {31},
keywords = {Transferable belief model, Information fusion, Idempotence, Evidence theory, Distinct evidence, Dempster--Shafer theory}
}

@inproceedings{10.5555/3172077.3172256,
author = {Ren, Yazhou and Zhao, Peng and Sheng, Yongpan and Yao, Dezhong and Xu, Zenglin},
title = {Robust softmax regression for multi-class classification with self-paced learning},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Softmax regression, a generalization of Logistic regression (LR) in the setting of multi-class classification, has been widely used in many machine learning applications. However, the performance of softmax regression is extremely sensitive to the presence of noisy data and outliers. To address this issue, we propose a model of robust softmax regression (RoSR) originated from the self-paced learning (SPL) paradigm for multi-class classification. Concretely, RoSR equipped with the soft weighting scheme is able to evaluate the importance of each data instance. Then, data instances participate in the classification problem according to their weights. In this way, the influence of noisy data and outliers (which are typically with small weights) can be significantly reduced. However, standard SPL may suffer from the imbalanced class influence problem, where some classes may have little influence in the training process if their instances are not sensitive to the loss. To alleviate this problem, we design two novel soft weighting schemes that assign weights and select instances locally for each class. Experimental results demonstrate the effectiveness of the proposed methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2641–2647},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.1007/s10915-017-0457-0,
author = {Khan, Arbaz and Dutt, Pravir and Upadhyay, Chandra Shekhar},
title = {Spectral Element Method for Parabolic Initial Value Problem with Non-Smooth Data: Analysis and Application},
year = {2017},
issue_date = {December  2017},
publisher = {Plenum Press},
address = {USA},
volume = {73},
number = {2–3},
issn = {0885-7474},
url = {https://doi.org/10.1007/s10915-017-0457-0},
doi = {10.1007/s10915-017-0457-0},
abstract = {In this paper, a least-squares spectral element method for parabolic initial value problem for two space dimension on parallel computers is presented. The theory is also valid for three dimension. This method gives exponential accuracy in both space and time. The method is based on minimization of residuals in terms of the partial differential equation and initial condition, in different Sobolev norms, and a term which measures the jump in the function and its derivatives across inter-element boundaries in appropriate fractional Sobolev norms. Rigorous error estimates for this method are given. Some specific numerical examples are solved to show the efficiency of this method.},
journal = {J. Sci. Comput.},
month = dec,
pages = {876–905},
numpages = {30},
keywords = {Parallel preconditioners, Parabolic initial value problem, Least-squares method, Hermite mollifier, Gevrey spaces, Exponential accuracy, Domain decomposition}
}

@inproceedings{10.1007/978-3-030-98682-7_6,
author = {Bestmann, Marc and Engelke, Timon and Fiedler, Niklas and G\"{u}ldenstein, Jasper and Gutsche, Jan and Hagge, Jonas and Vahl, Florian},
title = {TORSO-21 Dataset: Typical Objects in&nbsp;RoboCup Soccer 2021},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_6},
doi = {10.1007/978-3-030-98682-7_6},
abstract = {We present a dataset specifically designed to be used as a benchmark to compare vision systems in the RoboCup Humanoid Soccer domain. The dataset is composed of a collection of images taken in various real-world locations as well as a collection of simulated images. It enables comparing vision approaches with a meaningful and expressive metric. The contributions of this paper consist of providing a comprehensive and annotated dataset, an overview of the recent approaches to vision in RoboCup, methods to generate vision training data in a simulated environment, and an approach to increase the variety of a dataset by automatically selecting a diverse set of images from a larger pool. Additionally, we provide a baseline of YOLOv4 and YOLOv4-tiny on this dataset.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {65–77},
numpages = {13},
keywords = {Deep learning, Vision dataset, Computer vision},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/1944892.1944899,
author = {Galster, Matthias and Avgeriou, Paris},
title = {The notion of variability in software architecture: results from a preliminary exploratory study},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944899},
doi = {10.1145/1944892.1944899},
abstract = {Context: In the software product line domain, the concept of variability is well recognized. However, variability in the context of software architecture still seems to be poorly understood. Objective: In this paper, we aim at contributing to the development of a basic understanding of the notion of variability in the software architecture domain, beyond the idea of product lines. Method: We perform a preliminary exploratory study which consists of two parts: an expert survey among 11 subjects, and a mini focus group with 4 participants. For both parts, we collect and analyze mostly qualitative data. Results: Our observations indicate that there seems to be no common understanding of "variability" in the context of software architecture. On the other hand, some challenges related to variability in software architecture are similar to challenges identified in the product line domain. Conclusions: Variability in software architecture might require more theoretical foundations in order to establish "variability" as an architectural key concept and first-class quality attribute.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {59–67},
numpages = {9},
keywords = {variability, software architecture, questionnaire, product lines, mini focus group},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@article{10.5555/3122009.3176858,
author = {Fenn, Shannon and Moscato, Pablo},
title = {Target curricula via selection of minimum feature sets: a case study in Boolean networks},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider the effect of introducing a curriculum of targets when training Boolean models on supervised Multi Label Classification (MLC) problems. In particular, we consider how to order targets in the absence of prior knowledge, and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models.We show that hierarchical dependencies between targets can be exploited by enforcing an appropriate curriculum using hierarchical loss functions. On several multi-output circuit-inference problems with known target difficulties, Feedforward Boolean Networks (FBNs) trained with such a loss function achieve significantly lower out-of-sample error, up to 10% in some cases. This improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curricula. We also demonstrate the same improvements on three real-world models and two Gene Regulatory Network (GRN) inference problems.We posit a simple a-priori method for identifying an appropriate target order and estimating the strength of target relationships in Boolean MLCs. These methods use intrinsic dimension as a proxy for target difficulty, which is estimated using optimal solutions to a combinatorial optimisation problem known as the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same generalisation gains can be achieved without providing any knowledge of target difficulty.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {4070–4095},
numpages = {26},
keywords = {target curriculum, multi-label classification, k-feature Set, Boolean betworks}
}

@inproceedings{10.1145/3241403.3241414,
author = {Guam\'{a}n, Daniel and P\'{e}rez, Jennifer and D\'{\i}az, Jessica},
title = {Towards a (semi)-automatic reference process to support the reverse engineering and reconstruction of software architectures},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241414},
doi = {10.1145/3241403.3241414},
abstract = {The purpose of this work is to define a reference process to support the software architecture reconstruction in a systematic and automatic way. This process aims to be applied to the construction and maintenance phases within Agile methodologies and Continuous Integration processes, where the quick and continuous test and changes at design or coding level can generate an increase or reduction of technical debt and green software levels. This process is based on phases, activities, techniques, and strategies proposed by related works about reverse engineering and software architecture reconstruction. Specifically, it integrates all of them to create a complete process; which may be a reference process by providing green and technical debt-oriented recommendations during the decision-making of software architecture at design level or coding level. This recommendation phase will be based on algorithms and techniques of Machine Learning, that will allow to apply the process in an Agile way and taking into account previous knowledge.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {8},
numpages = {4},
keywords = {technical debt, software reconstruction, software metrics, software architecture, reverse engineering, machine learning, green software},
location = {Madrid, Spain},
series = {ECSA '18}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00012,
author = {Nuryyev, Batyr and Nadi, Sarah and Bhuiyan, Nazim Uddin and Banderali, Leonardo},
title = {Challenges of implementing software variability in eclipse OMR: an interview study},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00012},
doi = {10.1109/ICSE-SEIP52600.2021.00012},
abstract = {Software variability is the ability of a software system to be customized or configured for a particular context. In this paper, we discuss our experience investigating software variability implementation challenges in practice. Eclipse OMR, developed by IBM, is a set of highly configurable C++ components for building language runtimes; it supports multiple programming languages and target architectures. We conduct an interview study with 6 Eclipse OMR developers and identify 8 challenges incurred by the existing variability implementation, and 3 constraints that need to be taken into account for any reengineering effort. We discuss these challenges and investigate the literature and existing open-source systems for potential solutions. We contribute a solution for one of the challenges, namely adding variability to enumerations and arrays. We also share our experiences and lessons learned working with a large-scale highly configurable industry project. For example, we found that the "latest and greatest" research solutions may not always be favoured by developers due to small practical considerations such as build dependencies, or even C++ version constraints.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {31–40},
numpages = {10},
keywords = {variability implementation, software variability, language runtimes, eclipse OMR},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@article{10.1016/j.infsof.2006.08.001,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Classifying variability modeling techniques},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.001},
doi = {10.1016/j.infsof.2006.08.001},
abstract = {Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {717–739},
numpages = {23},
keywords = {Variability modeling, Variability management, Software product family, Classification}
}

@article{10.3233/KES-170356,
author = {Alidra, Abdelghani and Kimour, Mohamed Tahar},
title = {Adapting large pervasive and context-aware systems. A new evolutionary-based approach},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {21},
number = {2},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-170356},
doi = {10.3233/KES-170356},
abstract = {In order to enable ``anywhere, anytime'' computing, pervasive
systems must autonomously adapt at runtime. The use of dynamic software
product lines has emerged as a promising paradigm where well established
variability management techniques are leveraged at runtime to describe
evolution strategies and adaptation scenarios in terms of combinations of
features. In order to identify the optimal target configuration of the
system under certain circumstances, most existing approaches generate the
set of valid combinations of features and return the best one. Obviously,
while such approaches are well suited to small systems with a reduced number
of configurations, they fail in the case of large modern pervasive systems
because the generation/evaluation of all valid combinations is very costly
in terms of resources and time consumption. In the present article, we
introduce a new scalable, evolutionary-based approach to runtime adaptation
of pervasive systems. To this end, we define the concept of transitive
dependency between features and we exploit it to fasten the generation of
the optimal configuration of the system. We evaluate the scalability of our
proposal by reporting experimental results that show that our genetic
algorithm converges in up to 90% less time than the one from the
literature while preserving the exploration capabilities and solutions
quality. Finally, we illustrate our proposal on the smart homes use case.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {103–121},
numpages = {19},
keywords = {genetic algorithms, features transitive dependencies, dynamic software product lines, online adaptation, pervasive systems, Context-awareness}
}

@article{10.5555/3291125.3309615,
author = {Lamprier, Sylvain and Gisselbrecht, Thibault and Gallinari, Patrick},
title = {Profile-based bandit with unknown profiles},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {Stochastic bandits have been widely studied since decades. A very large panel of settings have been introduced, some of them for the inclusion of some structure between actions. If actions are associated with feature vectors that underlie their usefulness, the discovery of a mapping parameter between such proffles and rewards can help the exploration process of the bandit strategies. This is the setting studied in this paper, but in our case the action profiles (constant feature vectors) are unknown beforehand. Instead, the agent is only given sample vectors, with mean centered on the true profiles, for a subset of actions at each step of the process. In this new bandit instance, policies have thus to deal with a doubled uncertainty, both on the profile estimators and the reward mapping parameters learned so far. We propose a new algorithm, called SampLinUCB, specifically designed for this case. Theoretical convergence guarantees are given for this strategy, according to various profile samples delivery scenarios. Finally, experiments are conducted on both artificial data and a task of focused data capture from online social networks. Obtained results demonstrate the relevance of the approach in various settings.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2060–2099},
numpages = {40},
keywords = {upper confidence bounds, stochastic linear bandits, profile-based exploration}
}

@inbook{10.5555/3454287.3455282,
author = {Saxena, Shreyas and Tuzel, Oncel and DeCoste, Dennis},
title = {Data parameters: a new family of parameters for learning a differentiable curriculum},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent works have shown that learning from easier instances first can help deep neural networks (DNNs) generalize better. However, knowing which data to present during different stages of training is a challenging problem. In this work, we address this problem by introducing data parameters. More specifically, we equip each sample and class in a dataset with a learnable parameter (data parameters), which governs their importance in the learning process. During training, at each iteration, as we update the model parameters, we also update the data parameters. These updates are done by gradient descent and do not require hand-crafted rules or design. When applied to image classification task on CIFAR10, CIFAR100, WebVision and ImageNet datasets, and object detection task on KITTI dataset, learning a dynamic curriculum via data parameters leads to consistent gains, without any increase in model complexity or training time. When applied to a noisy dataset, the proposed method learns to learn from clean images and improves over the state-of-the-art methods by 14%. To the best of our knowledge, our work is the first curriculum learning method to show gains on large scale image classification and detection tasks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {995},
numpages = {11}
}

@article{10.1287/isre.2020.0921,
author = {Lee, Gene Moo and He, Shu and Lee, Joowon and Whinston, Andrew B.},
title = {Matching Mobile Applications for Cross-Promotion},
year = {2020},
issue_date = {September 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {31},
number = {3},
issn = {1526-5536},
url = {https://doi.org/10.1287/isre.2020.0921},
doi = {10.1287/isre.2020.0921},
abstract = {As the mobile app market grows rapidly, with millions of apps and billions of users, search costs are increasing tremendously. Similar to the case of recommender systems, the challenge is how apps can be recommended to the right users and how consumers can find the right apps. This paper studies a new mobile app ad framework, cross-promotion (CP), which is to promote new “target” apps within other “source” apps. With unique random matching experiment data, we empirically test the important determinants of ad effectiveness. We then propose a machine-learning-based framework to optimally match source apps to target apps to improve ad effectiveness in terms of app downloads and postdownload usages. The simulation results show that app analytics capability is essential in building accurate prediction models and in increasing ad effectiveness of CP campaigns and that, at the expense of privacy, individual user data can further improve the matching performance. The paper has important managerial implications because it provides direct guidance to better utilize CP for app developers and to leverage data analytics and machine-learning models for platform managers. It also provides policy implications on the trade-off between utility and privacy in the growing data economy.The mobile applications (apps) market is one of the most successful software markets. As the platform grows rapidly, with millions of apps and billions of users, search costs are increasing tremendously. The challenge is how app developers can target the right users with their apps and how consumers can find the apps that fit their needs. Cross-promotion, advertising a mobile app (target app) in another app (source app), is introduced as a new app-promotion framework to alleviate the issue of search costs. In this paper, we model source app user behaviors (downloads and postdownload usages) with respect to different target apps in cross-promotion campaigns. We construct a novel app similarity measure using latent Dirichlet allocation topic modeling on apps’ production descriptions and then analyze how the similarity between the source and target apps influences users’ app download and usage decisions. To estimate the model, we use a unique data set from a large-scale random matching experiment conducted by a major mobile advertising company in Korea. The empirical results show that consumers prefer more diversified apps when they are making download decisions compared with their usage decisions, which is supported by the psychology literature on people’s variety-seeking behavior. Lastly, we propose an app-matching system based on machine-learning models (on app download and usage prediction) and generalized deferred acceptance algorithms. The simulation results show that app analytics capability is essential in building accurate prediction models and in increasing ad effectiveness of cross-promotion campaigns and that, at the expense of privacy, individual user data can further improve the matching performance. This paper has implications on the trade-off between utility and privacy in the growing mobile economy.},
journal = {Info. Sys. Research},
month = sep,
pages = {865–891},
numpages = {27},
keywords = {mobile analytics, algorithm, deferred acceptance, machine learning, topic modeling, two-sided platform, search cost, matching, cross-promotion, mobile applications}
}

@article{10.1016/j.dsp.2018.12.007,
author = {Yue, Guanghui and Hou, Chunping and Yan, Weiqing and Choi, Lark Kwon and Zhou, Tianwei and Hou, Yonghong},
title = {Blind quality assessment for screen content images via convolutional neural network},
year = {2019},
issue_date = {Aug 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {91},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2018.12.007},
doi = {10.1016/j.dsp.2018.12.007},
journal = {Digit. Signal Process.},
month = aug,
pages = {21–30},
numpages = {10},
keywords = {Convolutional neural network (CNN), Image quality assessment (IQA), Blind/no reference (NR), Screen content image (SCI)}
}

@article{10.1016/j.cie.2019.04.042,
author = {Liang, Jingran and Wang, Yuyan and Zhang, Zhi-Hai and Sun, Yiqi},
title = {Energy efficient production planning and scheduling problem with processing technology selection},
year = {2019},
issue_date = {Jun 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {132},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2019.04.042},
doi = {10.1016/j.cie.2019.04.042},
journal = {Comput. Ind. Eng.},
month = jun,
pages = {260–270},
numpages = {11},
keywords = {Fix-and-optimize, Mixed integer linear programming, Capacitated production planning and scheduling, Energy efficient manufacturing}
}

@article{10.1155/2019/8127869,
author = {Zhu, Qi and Yuan, Ning and Guan, Donghai and Deng, Ke},
title = {Cognitive Driven Multilayer Self-Paced Learning with Misclassified Samples},
year = {2019},
issue_date = {2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2019},
issn = {1076-2787},
url = {https://doi.org/10.1155/2019/8127869},
doi = {10.1155/2019/8127869},
abstract = {In recent years, self-paced learning (SPL) has attracted much attention due to its improvement to nonconvex optimization based machine learning algorithms. As a methodology introduced from human learning, SPL dynamically evaluates the learning difficulty of each sample and provides the weighted learning model against the negative effects from hard-learning samples. In this study, we proposed a cognitive driven SPL method, i.e., retrospective robust self-paced learning (R2SPL), which is inspired by the following two issues in human learning process: the misclassified samples are more impressive in upcoming learning, and the model of the follow-up learning process based on large number of samples can be used to reduce the risk of poor generalization in initial learning phase. We simultaneously estimated the degrees of learning-difficulty and misclassified in each step of SPL and proposed a framework to construct multilevel SPL for improving the robustness of the initial learning phase of SPL. The proposed method can be viewed as a multilayer model and the output of the previous layer can guide constructing robust initialization model of the next layer. The experimental results show that the R2SPL outperforms the conventional self-paced learning models in classification task.},
journal = {Complex.},
month = jan,
numpages = {10}
}

@article{10.1016/j.cie.2021.107687,
author = {Costa, Antonio and Pastore, Erica and Frigerio, Nicla},
title = {The Server Allocation Problem with non-identical machines: A meta-heuristic approach},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {162},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107687},
doi = {10.1016/j.cie.2021.107687},
journal = {Comput. Ind. Eng.},
month = dec,
numpages = {19},
keywords = {Non-identical machines, Optimization, Design, Production system, Server Allocation Problem}
}

@article{10.1007/s10664-018-9652-3,
author = {Vale, Tassio and Almeida, Eduardo Santana},
title = {Experimenting with information retrieval methods in the recovery of feature-code SPL traces},
year = {2019},
issue_date = {Jun 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9652-3},
doi = {10.1007/s10664-018-9652-3},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1328–1368},
numpages = {41},
keywords = {Software traceability, Software product lines, Information retrieval, Controlled-experiment}
}

@article{10.1016/j.ijar.2021.07.015,
author = {Bodewes, Tjebbe and Scutari, Marco},
title = {Learning Bayesian networks from incomplete data with the node-average likelihood},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {138},
number = {C},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2021.07.015},
doi = {10.1016/j.ijar.2021.07.015},
journal = {Int. J. Approx. Reasoning},
month = nov,
pages = {145–160},
numpages = {16},
keywords = {Incomplete data, Score-based structure learning, Bayesian networks}
}

@article{10.1016/j.neucom.2019.07.060,
author = {Zhou, Xinzhe and Fang, Yigeng and Mu, Yadong},
title = {Learning single-shot vehicle orientation estimation from large-scale street panoramas},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {367},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.07.060},
doi = {10.1016/j.neucom.2019.07.060},
journal = {Neurocomput.},
month = nov,
pages = {319–327},
numpages = {9},
keywords = {Computer vision, Deep learning, Autonomous driving}
}

@article{10.1007/s11063-018-9964-8,
author = {Lee, Younghoon and Chung, Minki and Cho, Sungzoon and Choi, Jinhae},
title = {Extraction of Product Evaluation Factors with a Convolutional Neural Network and Transfer Learning},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {1},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-018-9964-8},
doi = {10.1007/s11063-018-9964-8},
abstract = {Earlier studies have indicated that decision-making by a project development team can be improved throughout the design and development process by understanding the key factors that affect customers evaluations of a new product. Aspect extraction could thus be a useful tool for identifying important attributes when evaluating products or services. Aspect extraction based on deep convolutional neural networks has recently been suggested, demonstrating state-of-the-art performance when applied to a customer review of electronic devices. However, this approach is unsuited to the rapidly evolving smartphone industry, which involves a wide range of product lines. Whereas the previous approach required significant amounts of data labeling for each product, we propose a variant of that approach that includes transfer learning. We also propose a novel approach for transferring the architecture sequentially within the product group. The results indicate that the principal key feature of each product is extracted effectively by the proposed method without having to re-train the entire convolutional neural network model. Furthermore, the proposed method performs better than the previous method for each product line.},
journal = {Neural Process. Lett.},
month = aug,
pages = {149–164},
numpages = {16},
keywords = {Domain adaptation, Off-the-shelf features, Transfer learning, Convolutional neural network, Aspect extraction, Product evaluation factor}
}

@inproceedings{10.5555/3504035.3504989,
author = {Ieva, Carlo and Gotlieb, Arnaud and Kaci, Souhila and Lazaar, Nadjib},
title = {Discovering program topoi through clustering},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Understanding source code of large open-source software projects is very challenging when there is only little documentation. New developers face the task of classifying a huge number of files and functions without any help. This paper documents a novel approach to this problem, called FEAT, that automatically extracts topoi from source code by using hierarchical agglomerative clustering. Program topoi summarize the main capabilities of a software system by presenting to developers clustered lists of functions together with an index of their relevant words. The clustering method used in FEAT exploits a new hybrid distance which combines both textual and structural elements automatically extracted from source code and comments. The experimental evaluation of FEAT shows that this approach is suitable to understand open-source software projects of size approaching 2,000 functions and 150 files, which opens the door for its deployment in the open-source community.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {954},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.5555/3172077.3172239,
author = {Murugesan, Keerthiram and Carbonell, aime},
title = {Self-paced multitask learning with shared knowledge},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {This paper introduces self-paced task selection to multitask learning, where instances from more closely related tasks are selected in a progression of easier-to-harder tasks, to emulate an effective human education strategy, but applied to multitask machine learning. We develop the mathematical foundation for the approach based on iterative selection of the most appropriate task, learning the task parameters, and updating the shared knowledge, optimizing a new bi-convex loss function. This proposed method applies quite generally, including to multitask feature learning, multitask learning with alternating structure optimization, etc. Results show that in each of the above formulations self-paced (easier-to-harder) task selection outperforms the baseline version of these methods in all the experiments.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2522–2528},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/1858996.1859010,
author = {Berger, Thorsten and She, Steven and Lotufo, Rafael and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {Variability modeling in the real: a perspective from the operating systems domain},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859010},
doi = {10.1145/1858996.1859010},
abstract = {Variability models represent the common and variable features of products in a product line. Several variability modeling languages have been proposed in academia and industry; however, little is known about the practical use of such languages. We study and compare the constructs, semantics, usage and tools of two variability modeling languages, Kconfig and CDL. We provide empirical evidence for the real-world use of the concepts known from variability modeling research. Since variability models provide basis for automated tools (feature dependency checkers and product configurators), we believe that our findings will be of interest to variability modeling language and tool designers.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {73–82},
numpages = {10},
keywords = {variability modeling, product line architectures, feature models, empirical software engineering, configuration},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1145/3387940.3392089,
author = {Ahlgren, John and Berezin, Maria Eugenia and Bojarczuk, Kinga and Dulskyte, Elena and Dvortsova, Inna and George, Johann and Gucevska, Natalija and Harman, Mark and L\"{a}mmel, Ralf and Meijer, Erik and Sapora, Silvia and Spahr-Summers, Justin},
title = {WES: Agent-based User Interaction Simulation on Real Infrastructure},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392089},
doi = {10.1145/3387940.3392089},
abstract = {We introduce the Web-Enabled Simulation (WES) research agenda, and describe FACEBOOK's WW system. We describe the application of WW to reliability, integrity and privacy at FACEBOOK1, where it is used to simulate social media interactions on an infrastructure consisting of hundreds of millions of lines of code. The WES agenda draws on research from many areas of study, including Search Based Software Engineering, Machine Learning, Programming Languages, Multi Agent Systems, Graph Theory, Game AI, and AI Assisted Game Play. We conclude with a set of open problems and research challenges to motivate wider investigation.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {276–284},
numpages = {9},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1016/j.compag.2021.106358,
author = {Choi, Chanho and Ahn, Houngjong and Yu, Jihun and Han, Jung-Su and Kim, Su-Chul and Park, Young-Jun},
title = {Optimization of gear macro-geometry for reducing gear whine noise in agricultural tractor transmission},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {188},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2021.106358},
doi = {10.1016/j.compag.2021.106358},
journal = {Comput. Electron. Agric.},
month = sep,
numpages = {10},
keywords = {Agricultural tractor, Optimization, Gear macro-geometry, Transmission error, Gear whine noise, Transmission}
}

@article{10.1145/3280848,
author = {Pereira, Fernando Magno Quint\~{a}o and Leobas, Guilherme Vieira and Gamati\'{e}, Abdoulaye},
title = {Static Prediction of Silent Stores},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3280848},
doi = {10.1145/3280848},
abstract = {A store operation is called “silent” if it writes in memory a value that is already there. The ability to detect silent stores is important, because they might indicate performance bugs, might enable code optimizations, and might reveal opportunities of automatic parallelization, for instance. Silent stores are traditionally detected via profiling tools. In this article, we depart from this methodology and instead explore the following question: is it possible to predict silentness by analyzing the syntax of programs? The process of building an answer to this question is interesting in itself, given the stochastic nature of silent stores, which depend on data and coding style. To build such an answer, we have developed a methodology to classify store operations in terms of syntactic features of programs. Based on such features, we develop different kinds of predictors, some of which go much beyond what any trivial approach could achieve. To illustrate how static prediction can be employed in practice, we use it to optimize programs running on nonvolatile memory systems.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {44},
numpages = {26},
keywords = {static analysis, nonvolatile memory, machine learning, code optimization, Silent stores}
}

@inproceedings{10.1145/3417113.3423000,
author = {de Macedo, Jo\~{a}o and Alo\'{\i}sio, Jo\~{a}o and Gon\c{c}alves, Nelson and Pereira, Rui and Saraiva, Jo\~{a}o},
title = {Energy wars - Chrome vs. Firefox: which browser is more energy efficient?},
year = {2021},
isbn = {9781450381284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417113.3423000},
doi = {10.1145/3417113.3423000},
abstract = {This paper presents a preliminary study on the energy consumption of two popular web browsers. In order to properly measure the energy consumption of both environments, we simulate the usage of various applications, which the goal to mimic typical user interactions and usage.Our preliminary results show interesting findings based on observation, such as what type of interactions generate high peaks of energy consumption, and which browser is overall the most efficient. Our goal with this preliminary study is to show to users how very different the efficiency of web browsers can be, and may serve with advances in this area of study.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {159–165},
numpages = {7},
keywords = {web browsers, green software, energy efficiency},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2783258.2783270,
author = {Yan, Feng and Ruwase, Olatunji and He, Yuxiong and Chilimbi, Trishul},
title = {Performance Modeling and Scalability Optimization of Distributed Deep Learning Systems},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783270},
doi = {10.1145/2783258.2783270},
abstract = {Big deep neural network (DNN) models trained on large amounts of data have recently achieved the best accuracy on hard tasks, such as image and speech recognition. Training these DNNs using a cluster of commodity machines is a promising approach since training is time consuming and compute-intensive. To enable training of extremely large DNNs, models are partitioned across machines. To expedite training on very large data sets, multiple model replicas are trained in parallel on different subsets of the training examples with a global parameter server maintaining shared weights across these replicas. The correct choice for model and data partitioning and overall system provisioning is highly dependent on the DNN and distributed system hardware characteristics. These decisions currently require significant domain expertise and time consuming empirical state space exploration.This paper develops performance models that quantify the impact of these partitioning and provisioning decisions on overall distributed system performance and scalability. Also, we use these performance models to build a scalability optimizer that efficiently determines the optimal system configuration that minimizes DNN training time. We evaluate our performance models and scalability optimizer using a state-of-the-art distributed DNN training framework on two benchmark applications. The results show our performance models estimate DNN training time with high estimation accuracy and our scalability optimizer correctly chooses the best configurations, minimizing the training time of distributed DNNs.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1355–1364},
numpages = {10},
keywords = {scalability, performance modeling, optimization, distributed system, deep learning},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@article{10.1016/j.artmed.2006.06.003,
author = {Botros, Andrew and van Dijk, Bas and Killian, Matthijs},
title = {AutoNRTTM: An automated system that measures ECAP thresholds with the Nucleus® FreedomTM cochlear implant via machine intelligence},
year = {2007},
issue_date = {May, 2007},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {40},
number = {1},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2006.06.003},
doi = {10.1016/j.artmed.2006.06.003},
abstract = {Objective: AutoNRT(TM) is an automated system that measures electrically evoked compound action potential (ECAP) thresholds from the auditory nerve with the Nucleus^(R) Freedom(TM) cochlear implant. ECAP thresholds along the electrode array are useful in objectively fitting cochlear implant systems for individual use. This paper provides the first detailed description of the AutoNRT algorithm and its expert systems, and reports the clinical success of AutoNRT to date. Methods: AutoNRT determines thresholds by visual detection, using two decision tree expert systems that automatically recognise ECAPs. The expert systems are guided by a dataset of 5393 neural response measurements. The algorithm approaches threshold from lower stimulus levels, ensuring recipient safety during postoperative measurements. Intraoperative measurements use the same algorithm but proceed faster by beginning at stimulus levels much closer to threshold. When searching for ECAPs, AutoNRT uses a highly specific expert system (specificity of 99% during training, 96% during testing; sensitivity of 91% during training, 89% during testing). Once ECAPs are established, AutoNRT uses an unbiased expert system to determine an accurate threshold. Throughout the execution of the algorithm, recording parameters (such as implant amplifier gain) are automatically optimised when needed. Results: In a study that included 29 intraoperative and 29 postoperative subjects (a total of 418 electrodes), AutoNRT determined a threshold in 93% of cases where a human expert also determined a threshold. When compared to the median threshold of multiple human observers on 77 randomly selected electrodes, AutoNRT performed as accurately as the 'average' clinician. Conclusions: AutoNRT has demonstrated a high success rate and a level of performance that is comparable with human experts. It has been used in many clinics worldwide throughout the clinical trial and commercial launch of Nucleus Custom Sound(TM) Suite, significantly streamlining the clinical procedures associated with cochlear implant use.},
journal = {Artif. Intell. Med.},
month = may,
pages = {15–28},
numpages = {14},
keywords = {Threshold estimation, Pattern recognition, Neural response telemetry, Machine learning, Electrically evoked compound action potential, Decision trees, Cochlear implants, Automated systems}
}

@inproceedings{10.5555/3491440.3491568,
author = {Guo, Dan and Wang, Yang and Song, Peipei and Wang, Meng},
title = {Recurrent relational memory network for unsupervised image captioning},
year = {2021},
isbn = {9780999241165},
abstract = {Unsupervised image captioning with no annotations is an emerging challenge in computer vision, where the existing arts usually adopt GAN (Generative Adversarial Networks) models. In this paper, we propose a novel memory-based network rather than GAN, named Recurrent Relational Memory Network (R2M). Unlike complicated and sensitive adversarial learning that non-ideally performs for long sentence generation, R2M implements a concepts-to-sentence memory translator through two-stage memory mechanisms: fusion and recurrent memories, correlating the relational reasoning between common visual concepts and the generated words for long periods. R2M encodes visual context through unsupervised training on images, while enabling the memory to learn from irrelevant textual corpus via supervised fashion. Our solution enjoys less learnable parameters and higher computational efficiency than GAN-based methods, which heavily bear parameter sensitivity. We experimentally validate the superiority of R2M than state-of-the-arts on all benchmark datasets.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {128},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@inproceedings{10.1007/978-3-030-58604-1_7,
author = {Krantz, Jacob and Wijmans, Erik and Majumdar, Arjun and Batra, Dhruv and Lee, Stefan},
title = {Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments},
year = {2020},
isbn = {978-3-030-58603-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58604-1_7},
doi = {10.1007/978-3-030-58604-1_7},
abstract = {We develop a language-guided navigation task set in a continuous 3D environment where agents must execute low-level actions to follow natural language navigation directions. By being situated in continuous environments, this setting lifts a number of assumptions implicit in prior work that represents environments as a sparse graph of panoramas with edges corresponding to navigability. Specifically, our setting drops the presumptions of known environment topologies, short-range oracle navigation, and perfect agent localization. To contextualize this new task, we develop models that mirror many of the advances made in prior settings as well as single-modality baselines. While some transfer, we find significantly lower absolute performance in the continuous setting – suggesting that performance in prior ‘navigation-graph’ settings may be inflated by the strong implicit assumptions. Code at 
.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII},
pages = {104–120},
numpages = {17},
keywords = {Vision-and-Language Navigation, Embodied agents},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/3001867.3001868,
author = {Lachmann, Remo and Lity, Sascha and Al-Hajjaji, Mustafa and F\"{u}rchtegott, Franz and Schaefer, Ina},
title = {Fine-grained test case prioritization for integration testing of delta-oriented software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001868},
doi = {10.1145/3001867.3001868},
abstract = {Software product line (SPL) testing is a challenging task, due to the huge number of variants sharing common functionalities to be taken into account for efficient testing. By adopting the concept of regression testing, incremental SPL testing strategies cope with this challenge by exploiting the reuse potential of test artifacts between subsequent variants under test. In previous work, we proposed delta-oriented test case prioritization for incremental SPL integration testing, where differences between architecture test model variants allow for reasoning about the order of reusable test cases to be executed. However, the prioritization left two issues open, namely (1) changes to component behavior are ignored, which may also influence component interactions and, (2) the weighting and ordering of similar test cases result in an unintended clustering of test cases. In this paper, we extend the test case prioritization technique by (1) incorporating changes to component behavior allowing for a more fine-grained analysis and (2) defining a dissimilarity measure to avoid clustered test case orders. We prototyped our test case prioritization technique and evaluated its applicability and effectiveness by means of a case study from the automotive domain showing positive results.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {1–10},
numpages = {10},
keywords = {Delta-Oriented Software Product Lines, Model-Based Integration Testing, Test Case Prioritization},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1109/IROS51168.2021.9636743,
author = {Yokoyama, Naoki and Ha, Sehoon and Batra, Dhruv},
title = {Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IROS51168.2021.9636743},
doi = {10.1109/IROS51168.2021.9636743},
abstract = {We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent’s dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exempli-fies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in a real robot to navigate an apartment, and show that they can generalize in a zero-shot manner. A video summary is available here: https://youtu.be/QOQ56XVIYVE},
booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {1562–1569},
numpages = {8},
location = {Prague, Czech Republic}
}

@inproceedings{10.1007/978-3-030-98682-7_5,
author = {Fernandes, Roberto and Rodrigues, Walber M. and Barros, Edna},
title = {Dataset and&nbsp;Benchmarking of&nbsp;Real-Time Embedded Object Detection for&nbsp;RoboCup SSL},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_5},
doi = {10.1007/978-3-030-98682-7_5},
abstract = {When producing a model to object detection in a specific context, the first obstacle is to have a dataset labeling the desired classes. In RoboCup, some leagues already have more than one dataset to train and evaluate a model. However, in the Small Size League (SSL), there is not such dataset available yet. This paper presents an open-source dataset to be used as a benchmark for real-time object detection in SSL. This work also presented a pipeline to train, deploy, and evaluate Convolutional Neural Networks (CNNs) models in a low-power embedded system. This pipeline is used to evaluate the proposed dataset with state-of-art optimized models. In this dataset, the MobileNet SSD v1 achieves 44.88% AP (68.81% AP50) at 94 Frames Per Second (FPS), while running on an SSL robot.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {53–64},
numpages = {12},
keywords = {Dataset, Benchmark, Deep learning, Object detection},
location = {Sydney, NSW, Australia}
}

@article{10.1007/s10796-019-09935-9,
author = {Bunnell, Lawrence and Osei-Bryson, Kweku-Muata and Yoon, Victoria Y.},
title = {RecSys Issues Ontology: A Knowledge Classification of Issues for Recommender Systems Researchers},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {6},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-019-09935-9},
doi = {10.1007/s10796-019-09935-9},
abstract = {Scholarly research has extensively examined a number of issues and challenges affecting recommender systems (e.g. ‘cold-start’, ‘scrutability’, ‘trust’, ‘context’, etc.). However, a comprehensive knowledge classification of the issues involved with recommender systems research has yet to be developed. A holistic knowledge representation of the issues affecting a domain is critical for research advancement. The aim of this study is to advance scholarly research within the domain of recommender systems through formal knowledge classification of issues and their relationships to one another within recommender systems research literature. In this study, we employ a rigorous ontology engineering process for development of a recommender system issues ontology. This ontology provides a formal specification of the issues affecting recommender systems research and development. The ontology answers such questions as, “What issues are associated with ‘trust’ in recommender systems research?”, “What are issues associated with improving and evaluating the ‘performance’ of a recommender system?” or “What ‘contextual’ factors might a recommender systems developer wish to consider in order to improve the relevancy and usefulness of recommendations?” Additionally, as an intermediate representation step in the ontology acquisition process, a concept map of recommender systems issues has been developed to provide conceptual visualization of the issues so that researchers may discern broad themes as well as relationships between concepts. These knowledge representations may aid future researchers wishing to take an integrated approach to addressing the challenges and limitations associated with current recommender systems research.},
journal = {Information Systems Frontiers},
month = dec,
pages = {1377–1418},
numpages = {42},
keywords = {Recommender systems issues, Recommendation agents, Thematic analysis, Concept mapping, Ontology acquisition}
}

@article{10.5555/3288338.3288341,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Finding correlations of features affecting energy consumption and performance of web servers using the HADAS eco-assistant},
year = {2018},
issue_date = {November  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {100},
number = {11},
issn = {0010-485X},
abstract = {The impact of energy consumption on the environment and the economy is raising awareness of "green" software engineering. HADAS is an eco-assistant that makes developers aware of the influence of their designs and implementations on the energy consumption and performance of the final product. In this paper, we extend HADAS to better support the requirements of users: researchers, automatically dumping the energy-consumption of different software solutions; and developers, who want to perform a sustainability analysis of different software solutions. This analysis has been extended by adding Pearson's chi-squared differentials and Bootstrapping statistics, to automatically check the significance of correlations of the energy consumption, or the execution time, with any other variable (e.g., the number of users) that can influence the selection of a particular eco-efficient configuration. We have evaluated our approach by performing a sustainability analysis of the most common web servers (i.e. PHP servers) using the time and energy data measured with the Watts Up? Pro tool previously dumped in HADAS. We show how HADAS helps web server providers to make a trade-off between energy consumption and execution time, allowing them to sell different server configurations with different costs without modifying the hardware.},
journal = {Computing},
month = nov,
pages = {1155–1173},
numpages = {19},
keywords = {Linux, Performance, Web servers, 68M20, 68N30, 68U35, 97K80, Energy efficiency}
}

@article{10.1007/s10489-018-01399-9,
author = {Abolpour Mofrad, Asieh and Yazidi, Anis and Lewi Hammer, Hugo},
title = {On solving the SPL problem using the concept of probability flux},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {7},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-01399-9},
doi = {10.1007/s10489-018-01399-9},
abstract = {The Stochastic Point Location (SPL) problem Oommen is a fundamental learning problem that has recently found a lot of research attention. SPL can be summarized as searching for an unknown point in an interval under faulty feedback. The search is performed via a Learning Mechanism (LM) (algorithm) that interacts with a stochastic Environment which in turn informs it about the direction of the search. Since the Environment is stochastic, the guidance for directions could be faulty. The first solution to the SPL problem, which was pioneered two decades ago by Oommen, relies on discretizing the search interval and performing a controlled random walk on it. The state of the random walk at each step is considered to be the estimation of the point location. The convergence of the latter simplistic estimation strategy is proved for an infinite resolution, i.e., infinite memory. However, this strategy yields rather poor accuracy for low discretization resolutions. In this paper, we present two major contributions to the SPL problem. First, we demonstrate that the estimation of the point location can significantly be improved by resorting to the concept of mutual probability flux between neighboring states along the line. Second, we are able to accurately track the position of the optimal point and simultaneously show a method by which we can estimate the error probability characterizing the Environment. Interestingly, learning this error probability of the Environment takes place in tandem with the unknown location estimation. We present and analyze several experiments discussing the weaknesses and strengths of the different methods.},
journal = {Applied Intelligence},
month = jul,
pages = {2699–2722},
numpages = {24},
keywords = {Estimating environment effectiveness, Flux-based Estimation Solution (FES), Last Transition-based Estimation Solution (LTES), Mutual probability flux, Stochastic Learning Weak Estimation (SLWE), Stochastic Point Location (SPL)}
}

@phdthesis{10.5555/AAI28713793,
author = {Palaparthi, Anil Kumar Reddy and A., Weiss, Jeffrey and D., Rabbitt, Richard and II, Dorval, Alan D., and M, Barkmeier-Kraemer, Julie},
advisor = {R, Titze, Ingo},
title = {Computational Motor Learning and Control of the Vocal Source for Voice Production},
year = {2021},
isbn = {9798780644439},
publisher = {The University of Utah},
abstract = {Voice production is a motor skill and requires the coordinated function of many brain regions, namely the brainstem, cerebellum, basal ganglia, diencephalon, and cerebral hemispheres. The vocal system can be subdivided into three major components: lungs, larynx, and vocal tract. Lung pressure drives the airflow in the trachea towards the larynx. The airflow causes the vocal folds in the larynx (vocal source) to oscillate under certain prephonatory conditions, generating audible pulses of airflow into the vocal tract. The vocal tract filters these pulses and radiates the sound into the air. The intrinsic laryngeal muscles play a significant role in voice production. They position the glottis, the space between the vocal folds in several pre-phonatory positions that facilitate vocal fold vibration. The resulting glottal flow is the vocal source. This project aims to develop a control system that controls the vocal source based on four acoustic and four somatosensory features. Nonlinear control theory and artificial neural networks were used to develop the controllers. A voice simulator with a biomechanical model of the vocal system, LeTalker, was used to model the voice production mechanism. In Aim 1, interrelationships between the intrinsic laryngeal muscles and lung pressure in producing various acoustic and somatosensory features during phonation were obtained. In Aim 2, feedforward and feedback controllers based on acoustic and somatosensory features to control the vocal source were developed. In Aim 3, the controllers' sensitivity and performance were assessed using perturbation analysis. The results demonstrated that the control system was able to generate the lung pressure and muscle activations such that the four acoustic and four somatosensory targets were reached with high accuracy. It was observed that for most of the test cases, the control system produces lung pressure and muscle activations that result in phonation that is within ±15 Hz of the targeted fo and ±2 dB of the targeted SPL. The three studies conducted in this dissertation can be a stepping stone to simulate and study various motor disorders related to voice.},
note = {AAI28713793}
}

@article{10.1155/2021/4327896,
author = {Xie, Shu-Tong and He, Zong-Bao and Chen, Qiong and Chen, Rong-Xin and Kong, Qing-Zhao and Song, Cun-Ying and Huang, Jiwei},
title = {Predicting Learning Behavior Using Log Data in Blended Teaching},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4327896},
doi = {10.1155/2021/4327896},
abstract = {Online and offline blended teaching mode, the future trend of higher education, has recently been widely used in colleges around the globe. In the article, we conducted a study on students’ learning behavior analysis and student performance prediction based on the data about students’ behavior logs in three consecutive years of blended teaching in a college’s “Java Language Programming” course. Firstly, the data from diverse platforms such as MOOC, Rain Classroom, PTA, and cnBlog are integrated and preprocessed. Secondly, a novel multiclass classification framework, combining the genetic algorithm (GA) and the error correcting output codes (ECOC) method, is developed to predict the grade levels of students. In the framework, GA is designed to realize both the feature selection and binary classifier selection to fit the ECOC models. Finally, key factors affecting grades are identified in line with the optimal subset of features selected by GA, which can be analyzed for teaching significance. The results show that the multiclass classification algorithm designed in this article can effectively predict grades compared with other algorithms. In addition, the selected subset of features corresponding to learning behaviors is pedagogically instructive.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@inproceedings{10.5555/645882.672251,
author = {Yacoub, Sherif M.},
title = {Performance Analysis of Component-Based Applications},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Performance analysis is a software engineering activity that involves analyzing a software application with respect to performance quality attributes such as response and execution times. Performance analysis tools provide the necessary support for the analyst to monitor program execution, record and analyze performance data, and locate and understand areas of poor performance. Performance analysis methods and techniques are highly dependent on the properties of the software system to be analyzed. Product line engineering applications possess some special properties that impose constraints on the selection of the performance analysis techniques to be applied and the tools to be used. The development of a component-based reference architecture is crucial to the success of a true product line. The component-based nature facilitates the integration of components and the replacement of a component with another to meet the requirements of an instance application of the product line. In this paper, we discuss performance analysis of component-based software systems and its automation. We discuss how component-based system properties influence the selection of methods and tools used to obtain and analyze performance measures. We use a case study of the document content remastering product line to illustrate the application of a performance analysis method to component-based applications.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {299–315},
numpages = {17},
keywords = {and performance tools, application and component profiling, component-based software engineering (CBSE), performance analysis},
series = {SPLC 2}
}

@article{10.3233/FI-2018-1685,
author = {Sandhu, Jasminder Kaur and Verma, Anil Kumar and Rana, Prashant Singh},
title = {A Novel Framework for Reliable Network Prediction of Small Scale Wireless Sensor Networks (SSWSNs)},
year = {2018},
issue_date = {2018},
publisher = {IOS Press},
address = {NLD},
volume = {160},
number = {3},
issn = {0169-2968},
url = {https://doi.org/10.3233/FI-2018-1685},
doi = {10.3233/FI-2018-1685},
abstract = {In Small Scale Wireless Sensor Networks (SSWSNs), reliability is defined as the capability of a network to perform its intended task under certain conditions for a stated time span. There are many tools for modeling and analyzing the reliability of a network. As the intricacy of various networks is increasing, there is a need for many sophisticated methods for reliability analysis. The term reliability is used as an umbrella term to capture various attributes such as safety, availability, security, and ease of use. The existing methods have many shortcomings which include inadequacy of a novel framework and inefficacy to handle scalable networks. This paper presents a novel framework which predicts the overall reliability of the SSWSNs in terms of performance metrics such as, sent packets, received packets, packets forfeit, packet delivery ratio and throughput. This framework includes various phases starting with scenario generation, construction of a dataset, applying ensemble based machine learning techniques to predict the parameters which cannot be calculated. The ensemble model predicts with an optimum accuracy of 99.9% for data flow, 99.9% for the protocol used and 97.6% for the number of nodes. Finally, to check the robustness of the ensemble model 10-fold cross-validation is used. The dataset used in this work is available as a supplement at .},
journal = {Fundam. Inf.},
month = jan,
pages = {303–341},
numpages = {39},
keywords = {Small Scale Wireless Sensor Networks, Reliability, Machine Learning, Network Prediction, Ensemble}
}

@article{10.1504/IJAISC.2013.056838,
author = {Paulraj, M. P. and Andrew, Allan Melvin},
title = {Classification of interior noise comfort level of Proton model cars using feedforward neural network},
year = {2013},
issue_date = {September 2013},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {3},
number = {4},
issn = {1755-4950},
url = {https://doi.org/10.1504/IJAISC.2013.056838},
doi = {10.1504/IJAISC.2013.056838},
abstract = {In this research, a Proton model cars noise comfort level classification system has been developed to detect the noise comfort level in cars using artificial neural network. This research focuses on developing a database consisting of car sound samples measured from different Proton make models in stationary and moving state. In the stationary condition, the sound pressure level is measured at 1,300 RPM, 2,000 RPM and 3,000 RPM while in moving condition, the sound is recorded using dB Orchestra while the car is moving at constant speed from 30 km/h up to 110 km/h. Subjective test is conducted to find the jury's evaluation for the specific sound sample. The feature set is then feed to the neural network model to classify the comfort level. The spectral power feature gives the highest classification accuracy of 88.42%.},
journal = {Int. J. Artif. Intell. Soft Comput.},
month = sep,
pages = {344–359},
numpages = {16}
}

@article{10.1016/j.specom.2021.06.001,
author = {Akbarzadeh, Sara and Lee, Sungmin and Chen, Fei and Tan, Chin-Tuan},
title = {The effect of speech and noise levels on the quality perceived by cochlear implant and normal hearing listeners},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2021.06.001},
doi = {10.1016/j.specom.2021.06.001},
journal = {Speech Commun.},
month = sep,
pages = {106–113},
numpages = {8},
keywords = {Cochlear implant, Speech level, Noise level, Sound quality perception, NH, CI, SPL, NR, SNR}
}

@inproceedings{10.1007/978-3-030-26142-9_9,
author = {Wang, Yunyun and Zhao, Dan and Li, Yun and Chen, Kejia and Xue, Hui},
title = {The Most Related Knowledge First: A Progressive Domain Adaptation Method},
year = {2019},
isbn = {978-3-030-26141-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26142-9_9},
doi = {10.1007/978-3-030-26142-9_9},
abstract = {In domain adaptation, how to select and transfer related knowledge is critical for learning. Inspired by the fact that human usually transfer from the more related experience to the less related one, in this paper, we propose a novel progressive domain adaptation (PDA) model, which attempts to transfer source knowledge by considering the transfer order based on relevance. Specifically, PDA transfers source instances iteratively from the most related ones to the least related ones, until all related source instances have been adopted. It is an iterative learning process, source instances adopted in each iteration are determined by a gradually annealed weight such that the later iteration will introduce more source instances. Further, a reverse classification performance is used to set the termination of iteration. Experiments on real datasets demonstrate the competiveness of PDA compared with the state-of-arts.},
booktitle = {Trends and Applications in Knowledge Discovery and Data Mining: PAKDD 2019 Workshops, BDM, DLKT, LDRC, PAISI, WeL, Macau, China, April 14–17, 2019, Revised Selected Papers},
pages = {90–102},
numpages = {13},
keywords = {Domain adaptation, Progressive transfer, Iteration, Reverse classification},
location = {Macau, China}
}

@article{10.1007/s11263-019-01278-x,
author = {Ramasinghe, Sameera and Khan, Salman and Barnes, Nick and Gould, Stephen},
title = {Representation Learning on Unit Ball with 3D Roto-translational Equivariance},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {128},
number = {6},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-019-01278-x},
doi = {10.1007/s11263-019-01278-x},
abstract = {Convolution is an integral operation that defines how the shape of one function is modified by another function. This powerful concept forms the basis of hierarchical feature learning in deep neural networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces—such as a sphere (S2) or a unit ball (B3)—entails unique challenges. In this work, we propose a novel ‘volumetric convolution’ operation that can effectively model and convolve arbitrary functions in B3. We develop a theoretical framework for volumetric convolution based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer in deep networks. By construction, our formulation leads to the derivation of a novel formula to measure the symmetry of a function in B3 around an arbitrary axis, that is useful in function analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on one viable use case i.e., 3D object recognition.},
journal = {Int. J. Comput. Vision},
month = jun,
pages = {1612–1634},
numpages = {23},
keywords = {Convolution neural networks, 3D moments, Volumetric convolution, Zernike polynomials, Deep learning}
}

@article{10.1016/j.neucom.2018.04.075,
author = {Xu, Wei and Liu, Wei and Huang, Xiaolin and Yang, Jie and Qiu, Song},
title = {Multi-modal self-paced learning for image classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {309},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.075},
doi = {10.1016/j.neucom.2018.04.075},
journal = {Neurocomput.},
month = oct,
pages = {134–144},
numpages = {11},
keywords = {Image classification, Curriculum learning, Self-paced learning, Multi-modal}
}

@inproceedings{10.1145/3474085.3481541,
author = {Huang, Lianghua and Liu, Yu and Zhou, Xiangzeng and You, Ansheng and Li, Ming and Wang, Bin and Zhang, Yingya and Pan, Pan and Yinghui, Xu},
title = {Once and for All: Self-supervised Multi-modal Co-training on One-billion Videos at Alibaba},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481541},
doi = {10.1145/3474085.3481541},
abstract = {Videos grow to be one of the largest mediums on the Internet. E-commerce platforms like Alibaba need to process millions of video data across multimedia (e.g., visual, audio, image, and text) and on a variety of tasks (e.g., retrieval, tagging, and summary) every day. In this work, we aim to develop a once and for all pretraining technique for diverse modalities and downstream tasks. To achieve this, we make the following contributions: (1) We propose a self-supervised multi-modal co-training framework. It takes cross-modal pseudo-label consistency as the supervision and can jointly learn representations of multiple modalities. (2) We introduce several novel techniques (e.g., sliding-window subset sampling, coarse-to-fine clustering, fast spatial-temporal convolution and parallel data transmission and processing) to optimize the training process, making billion-scale stable training feasible. (3) We construct a large-scale multi-modal dataset consisting of 1.4 billion videos (~0.5 PB) and train our framework on it. The training takes only 4.6 days on an in-house 256 GPUs cluster, and it simultaneously produces pretrained video, audio, image, motion, and text networks. (4) Finetuning from our pretrained models, we obtain significant performance gains and faster convergence on diverse multimedia tasks at Alibaba. Furthermore, we also validate the learned representation on public datasets. Despite the domain gap between our commodity-centric pretraining and the action-centric evaluation data, we show superior results against state-of-the-arts.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1148–1156},
numpages = {9},
keywords = {co-training, multi-modal, once and for all, self-supervised learning},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1007/s10664-019-09705-w,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Apel, Sven},
title = {On the relation of control-flow and performance feature interactions: a case study},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09705-w},
doi = {10.1007/s10664-019-09705-w},
abstract = {Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time-consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that information about potentially interacting features can be obtained by analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted an in-depth qualitative case study on two real-world systems (mbedTLS and SQLite), in which we explored the relation between internal (precisely control-flow) feature interactions, detected through static program analysis, and external (precisely performance) feature interactions, detected by performance-prediction techniques using performance measurements. We found that a relation exists that can potentially be exploited to predict performance interactions.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2410–2437},
numpages = {28},
keywords = {Control-flow feature interaction, Feature, Feature interaction, Feature-interaction prediction, Highly configurable software system, Performance feature interaction, Variability}
}

@article{10.1016/j.patcog.2019.106972,
author = {Dong, Ganggang and Liu, Hongwei and Kuang, Gangyao and Chanussot, Jocelyn},
title = {Target recognition in SAR images via sparse representation in the frequency domain},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {96},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.106972},
doi = {10.1016/j.patcog.2019.106972},
journal = {Pattern Recogn.},
month = dec,
numpages = {10},
keywords = {Sparse representation, Transformed domain, Target recognition}
}

@article{10.1007/s11219-011-9170-7,
author = {Acher, Mathieu and Collet, Philippe and Gaignard, Alban and Lahire, Philippe and Montagnat, Johan and France, Robert B.},
title = {Composing multiple variability artifacts to assemble coherent workflows},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9170-7},
doi = {10.1007/s11219-011-9170-7},
abstract = {The development of scientific workflows is evolving toward the systematic use of service-oriented architectures, enabling the composition of dedicated and highly parameterized software services into processing pipelines. Building consistent workflows then becomes a cumbersome and error-prone activity as users cannot manage such large-scale variability. This paper presents a rigorous and tooled approach in which techniques from Software Product Line (SPL) engineering are reused and extended to manage variability in service and workflow descriptions. Composition can be facilitated while ensuring consistency. Services are organized in a rich catalog which is organized as a SPL and structured according to the common and variable concerns captured for all services. By relying on sound merging techniques on the feature models that make up the catalog, reasoning about the compatibility between connected services is made possible. Moreover, an entire workflow is then seen as a multiple SPL (i.e., a composition of several SPLs). When services are configured within, the propagation of variability choices is then automated with appropriate techniques and the user is assisted in obtaining a consistent workflow. The approach proposed is completely supported by a combination of dedicated tools and languages. Illustrations and experimental validations are provided using medical imaging pipelines, which are representative of current scientific workflows in many domains.},
journal = {Software Quality Journal},
month = sep,
pages = {689–734},
numpages = {46},
keywords = {Composition, Feature models, Scientific workflows, Software product lines}
}

@inproceedings{10.1145/3324884.3416620,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Mastering uncertainty in performance estimations of configurable software systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416620},
doi = {10.1145/3324884.3416620},
abstract = {Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {684–696},
numpages = {13},
keywords = {P4, configurable software systems, performance-influence modeling, probabilistic programming},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1007/978-3-030-58542-6_19,
author = {Wang, Hanqing and Wang, Wenguan and Shu, Tianmin and Liang, Wei and Shen, Jianbing},
title = {Active Visual Information Gathering for Vision-Language Navigation},
year = {2020},
isbn = {978-3-030-58541-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58542-6_19},
doi = {10.1007/978-3-030-58542-6_19},
abstract = {Vision-language navigation (VLN) is the task of entailing an agent to carry out navigational instructions inside photo-realistic environments. One of the key challenges in VLN is how to conduct a robust navigation by mitigating the uncertainty caused by ambiguous instructions and insufficient observation of the environment. Agents trained by current approaches typically suffer from this and would consequently struggle to avoid random and inefficient actions at every step. In contrast, when humans face such a challenge, they can still maintain robust navigation by actively exploring the surroundings to gather more information and thus make more confident navigation decisions. This work draws inspiration from human navigation behavior and endows an agent with an active information gathering ability for a more intelligent vision-language navigation policy. To achieve this, we propose an end-to-end framework for learning an exploration policy that decides i) when and where to explore, ii) what information is worth gathering during exploration, and iii) how to adjust the navigation decision after the exploration. The experimental results show promising exploration strategies emerged from training, which leads to significant boost in navigation performance. On the R2R challenge leaderboard, our agent gets promising results all three VLN settings, i.e., single run, pre-exploration, and beam search.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXII},
pages = {307–322},
numpages = {16},
keywords = {Vision-language navigation, Active exploration},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.cie.2014.12.037,
author = {Kucukkoc, Ibrahim and Zhang, David Z.},
title = {Type-E parallel two-sided assembly line balancing problem},
year = {2015},
issue_date = {June 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {84},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2014.12.037},
doi = {10.1016/j.cie.2014.12.037},
abstract = {Display Omitted Type-E parallel two-sided line balancing problem is introduced for the first time.ACO algorithm is proposed as a possible solution approach for the addressed problem.Parameters of the ACO are optimised through response surface methodology.The cycle time and the total number of workstations are minimised at the same time.The performance of the ACO algorithm is tested through well-known test problems. There are many factors which affect the performance of a complex production system. Efficiency of an assembly line is one of the most important of these factors since assembly lines are generally constructed as the last stage of an entire production system. Parallel two-sided assembly line system is a new research domain in academia though these lines have been utilised to produce large sized products such as automobiles, trucks, and buses in industry for many years. Parallel two-sided assembly lines carry practical advantages of both parallel assembly lines and two-sided assembly lines.The main purpose of this paper is to introduce type-E parallel two-sided assembly line balancing problem for the first time in the literature and to propose a new ant colony optimisation based approach for solving the problem. Different from the existing studies on parallel assembly line balancing problems in the literature, this paper aims to minimise two conflicting objectives, namely cycle time and number of workstations at the same time and proposes a mathematical model for the formal description of the problem. To the best of our knowledge, this is the first study which addresses both conflicting objectives on a parallel two-sided assembly line configuration. The developed ant colony optimisation algorithm is illustrated with an example to explain its procedures. An experimental design is also conducted to calibrate the parameters of the proposed algorithm using response surface methodology. Results obtained from the performed computational study indicate that minimising cycle time as well as number of workstations help increase system efficiency. It is also observed that the proposed algorithm finds promising results for the studied cases of type-E parallel two-sided assembly line balancing problem when the results are compared with those obtained from other three well-known heuristics.},
journal = {Comput. Ind. Eng.},
month = jun,
pages = {56–69},
numpages = {14},
keywords = {Ant colony optimisation, Artificial intelligence, Parallel two-sided assembly lines, Response surface methodology, Type-E assembly line balancing}
}

@article{10.4018/IJSI.2021070105,
author = {Jo, Jun-Hyuk and Lee, Jihyun and Jaffari, Aman and Kim, Eunmi},
title = {Fault Localization With Data Flow Information and an Artificial Neural Network},
year = {2021},
issue_date = {Jul 2021},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {3},
issn = {2166-7160},
url = {https://doi.org/10.4018/IJSI.2021070105},
doi = {10.4018/IJSI.2021070105},
abstract = {Fault localization is a technique for identifying the exact source code line with faults. It typically requires a lot of time and cost because, to locate the fault, a developer must track the execution of the failed program line by line. To reduce the fault localization efforts, many methods have been proposed. However, their localized suspicious code range is wide, and their fault localization effect is not high. To cope with this limitation, this paper computes the degree of fault suspiciousness of statements by using an artificial neural network and information of the executed test case, such as statement coverage, execution result, and definition-use pair. Compared to the approach that uses only statement coverage as input data for training an artificial neural network, the experiment results show higher accuracy in 15 types of faults out of 29 real fault types in the approach that the definition-use pair included.},
journal = {Int. J. Softw. Innov.},
month = jul,
pages = {66–78},
numpages = {13},
keywords = {Artificial Neural Network, Data Flow Coverage, Definition-Use, Du-Pair, Fault Localization, Fault Suspiciousness, Software Testing, Software Verification}
}

@inproceedings{10.1007/978-3-030-29551-6_44,
author = {Lin, Jiping and Zhou, Yu and Kang, Junhao},
title = {Low-Sampling Imagery Data Recovery by Deep Learning Inference and Iterative Approach},
year = {2019},
isbn = {978-3-030-29550-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29551-6_44},
doi = {10.1007/978-3-030-29551-6_44},
abstract = {Block-based compressed sensing (CS) recovery aims to reconstruct the high quality image from only a small number of observations in a block-wise manner. However, when the sampling rate is very low and the existence of additive noise, there are usually some block artifacts and detail blurs which degrades the reconstructed quality. In this paper, we propose an efficient method which takes both the advantages of deep learning (DL) framework and iterative approaches. First, a deep multi-layer perceptron (DMLP) is constructed to obtain the initial reconstructed image. Then, an efficient iterative approach is applied to keep the consistence and smoothness between the adjacent blocks. The proposed method demonstrates its efficacy on benchmark datasets.},
booktitle = {Knowledge Science, Engineering and Management: 12th International Conference, KSEM 2019, Athens, Greece, August 28–30, 2019, Proceedings, Part I},
pages = {488–493},
numpages = {6},
keywords = {Compressed sensing, Deep learning, Iterative approach},
location = {Athens, Greece}
}

@inproceedings{10.5555/3495724.3496081,
author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
title = {Object goal navigation using goal-oriented semantic exploration},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, 'Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allows us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {357},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.1145/3078971.3079003,
author = {Liang, Junwei and Jiang, Lu and Meng, Deyu and Hauptmann, Alexander},
title = {Leveraging Multi-modal Prior Knowledge for Large-scale Concept Learning in Noisy Web Data},
year = {2017},
isbn = {9781450347013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078971.3079003},
doi = {10.1145/3078971.3079003},
abstract = {Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web is associated with rich but noisy contextual information, such as the title and other multi-modal information, which provides weak annotations or labels about the video content. To tackle the problem of large-scale noisy learning, We propose a novel method called Multi-modal WEbly-Labeled Learning (WELL-MM), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human. WELL-MM introduces a novel multi-modal approach to incorporate meaningful prior knowledge called curriculum from the noisy web videos. We empirically study the curriculum constructed from the multi-modal features of the Internet videos and images. The comprehensive experimental results on FCVID and YFCC100M demonstrate that WELL-MM outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data. In addition, the results also verify that WELL-MM is robust to the level of noisiness in the video data. Notably, WELL-MM trained on sufficient noisy web labels is able to achieve a better accuracy to supervised learning methods trained on the clean manually labeled data.},
booktitle = {Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
pages = {32–40},
numpages = {9},
keywords = {big data, concept detection, noisy data, prior knowledge, video understanding, web label, webly-supervised learning},
location = {Bucharest, Romania},
series = {ICMR '17}
}

@article{10.1007/s10664-017-9573-6,
author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
title = {Data-efficient performance learning for configurable systems},
year = {2018},
issue_date = {Jun 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9573-6},
doi = {10.1007/s10664-017-9573-6},
abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1826–1867},
numpages = {42},
keywords = {Performance prediction, Configurable systems, Regression, Model selection, Parameter tuning}
}

@inproceedings{10.5555/3495724.3496497,
author = {Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {Self-paced deep reinforcement learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Curriculum reinforcement learning (CRL) improves the learning speed and stability of an agent by exposing it to a tailored series of tasks throughout learning. Despite empirical successes, an open question in CRL is how to automatically generate a curriculum for a given reinforcement learning (RL) agent, avoiding manual design. In this paper, we propose an answer by interpreting the curriculum generation as an inference problem, where distributions over tasks are progressively learned to approach the target task. This approach leads to an automatic curriculum generation, whose pace is controlled by the agent, with solid theoretical motivation and easily integrated with deep RL algorithms. In the conducted experiments, the curricula generated with the proposed algorithm significantly improve learning performance across several environments and deep RL algorithms, matching or outperforming state-of-the-art existing CRL algorithms.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {773},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.5555/800095.803075,
author = {Gianfagna, Michael A.},
title = {A unified approach to test data analysis},
year = {1978},
publisher = {IEEE Press},
abstract = {To provide cost-effective performance evaluation or engineering feedback from circuit test results often requires that complex analyses be performed on large volumes of non-standard data. Using a large scale data management system and a modular design philosophy, a system to cope with the above requirements has been developed. TDAS (Test Data Analysis System) has provided timely and economic solutions to test data analysis problems which might have been intractable by other means.},
booktitle = {Proceedings of the 15th Design Automation Conference},
pages = {117–124},
numpages = {8},
location = {Las Vegas, Nevada, USA},
series = {DAC '78}
}

@inproceedings{10.5555/3299905.3299978,
author = {Safavi, Saeid and Wang, Wenwu and Plumbley, Mark and Choobbasti, Ali Janalizadeh and Fazekas, George},
title = {Predicting the Perceived Level of Reverberation using Features from Nonlinear Auditory Model},
year = {2018},
publisher = {FRUCT Oy},
address = {Helsinki, Uusimaa, FIN},
abstract = {Perceptual measurements have typically been recognized as the most reliable measurements in assessing perceived levels of reverberation. In this paper, a combination of blind RT60 estimation method and a binaural, nonlinear auditory model is employed to derive signal-based measures (features) that are then utilized in predicting the perceived level of rever- beration. Such measures lack the excess of effort necessary for calculating perceptual measures; not to mention the variations in either stimuli or assessors that may cause such measures to be statistically insigni?cant. As a result, the automatic extraction of objective measurements that can be applied to predict the perceived level of reverberation become of vital signi?cance. Consequently, this work is aimed at discovering measurements such as clarity, reverberance, and RT60 which can automatically be derived directly from audio data. These measurements along with labels from human listening tests are then forwarded to a machine learning system seeking to build a model to estimate the perceived level of reverberation, which is labeled by an expert, autonomously. The data has been labeled by an expert human listener for a unilateral set of ?les from arbitrary audio source types. By examining the results, it can be observed that the automatically extracted features can aid in estimating the perceptual rates.},
booktitle = {Proceedings of the 23rd Conference of Open Innovations Association FRUCT},
articleno = {73},
numpages = {5},
keywords = {Audio signal processing, Feature extraction, Human experiments, Reverberation, machine learning},
location = {Bologna, Italy},
series = {FRUCT'23}
}

@article{10.1007/s11276-018-1837-6,
author = {Yildirim, Ahmet and Zeydan, Engin and Yigit, Ibrahim Onuralp},
title = {A statistical comparative performance analysis of mobile network operators},
year = {2020},
issue_date = {Feb 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {2},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-018-1837-6},
doi = {10.1007/s11276-018-1837-6},
abstract = {Mobile telephony is one of the most widely utilized technologies in the modern world. Records of the usage behaviour of mobile users can provide valuable information for understanding the behaviour of networks for Mobile Network Operators (MNOs). For different reasons, MNOs are interested in knowing how their competitors’ performance varies based on location, phone category, phone Operating System (OS) for various cellular network technology (CNT). This can help MNOs to invest intelligently in locations where they operate with inferior performance. Therefore, Key Performance Indicator (KPI) comparisons among MNOs are of interest for all MNOs. In this article, we investigate cellular network performance statistical comparisons of major Mobile Network Operators (MNOs) in Turkey using a large scale real-world proprietary mobile traffic dataset over a period of 18 months. Focusing our approach on different dimensions of crowd-sourced dataset allows us: (i) to know end-to-end nationwide network performance comparisons of MNOs using real-world measurement data, (ii) to calculate Confidence Intervals (CIs) for the mean difference of KPIs (such as downlink speed, latency, jitter and packet loss) for obtaining useful comparative statistical information of MNO performances and (iii) to observe the existence of significant performance differences between MNOs depending on the region which they are operating, phone category, phone OS as well as CNTs.},
journal = {Wirel. Netw.},
month = feb,
pages = {1105–1124},
numpages = {20},
keywords = {Data analytics, MNOs, Performance, Comparisons, KPIs, Cellular}
}

@inproceedings{10.1007/978-3-030-98682-7_9,
author = {Antonioni, Emanuele and Riccio, Francesco and Nardi, Daniele},
title = {Improving Sample Efficiency in&nbsp;Behavior Learning by&nbsp;Using Sub-optimal Planners for&nbsp;Robots},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_9},
doi = {10.1007/978-3-030-98682-7_9},
abstract = {The design and implementation of behaviors for robots operating in dynamic and complex environments are becoming mandatory in nowadays applications. Reinforcement learning is consistently showing remarkable results in learning effective action policies and in achieving super-human performance in various tasks – without exploiting prior knowledge. However, in robotics, the use of purely learning-based techniques is still subject to strong limitations. Foremost, sample efficiency. Such techniques, in fact, are known to require large training datasets, and long training sessions, in order to develop effective action policies. Hence in this paper, to alleviate such constraint, and to allow learning in such robotic scenarios, we introduce SErP (Sample Efficient robot Policies), an iterative algorithm to improve the sample-efficiency of learning algorithms. SErP exploits a sub-optimal planner (here implemented with a monitor-replanning algorithm) to lead the exploration of the learning agent through its initial iterations. Intuitively, SErP exploits the planner as an expert in order to enable focused exploration and to avoid portions of the search space that are not effective to solve the task of the robot. Finally, to confirm our insights and to show the improvements that SErP carries with, we report the results obtained in two different robotic scenarios: (1) a cartpole scenario and (2) a soccer-robots scenario within the RoboCup@Soccer SPL environment.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {103–114},
numpages = {12},
keywords = {Automated planning, Reinforcement learning, Decision-making},
location = {Sydney, NSW, Australia}
}

@article{10.1016/j.patrec.2021.06.029,
author = {Chaudhary, Sachin and Dudhane, Akshay and Patil, Prashant W. and Murala, Subrahmanyam and Talbar, Sanjay},
title = {Motion estimation in hazy videos},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {150},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.06.029},
doi = {10.1016/j.patrec.2021.06.029},
journal = {Pattern Recogn. Lett.},
month = oct,
pages = {130–138},
numpages = {9},
keywords = {41A05, 41A10, 65D05, 65D17, Scene understanding, Motion estimation}
}

@article{10.1016/j.eswa.2015.07.044,
author = {Bautista, Joaqu\'{\i}n and Alfaro-Pozo, Roc\'{\i}o and Batalla-Garc\'{\i}a, Cristina},
title = {Consideration of human resources in the Mixed-model Sequencing Problem with Work Overload Minimization},
year = {2015},
issue_date = {December 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {22},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.07.044},
doi = {10.1016/j.eswa.2015.07.044},
abstract = {MMSP with work overload minimization and improvement of working conditions.Compliance with saturation conditions of workers imposed by collective agreement.Increase of work pace factor of workers to reduce production losses.Auxiliary processors to complete the required work and to fulfill all conditions.Case study linked to the Nissan Powertrain Plant in Barcelona. Beginning with a variation of the sequencing problem in a mixed-products line (MMSP-W: Mixed-Model Sequencing Problem with Workload Minimization), we propose two new models that incorporate a set of working conditions in regard with human resources of workstations on the line. These conditions come from collective agreements and therefore must be respected by both company and labor unions. The first model takes into account the saturation limit of the workstations, and the second model also includes the activation of the operators throughout the working day. Two computational experiments were carried out using a case study of the Nissan motor plant in Barcelona with two main objectives: (1) to study the repercussions of the saturation limit on the decrease in productivity on the line and (2) to evaluate the recovery of productivity on the line via both activation of operators, while maintaining the same quality in working conditions achieved by limiting the saturation, and auxiliary processors. By results we state that saturation limitation leads an important increase of work overload, which means average economic losses of 28,731.8 Euros/day. However, the productivity reduction may be counteracted by the work pace factor increase, at certain moments of workday, and/or by the incorporation of auxiliary processors into the line.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {8896–8910},
numpages = {15},
keywords = {Activity factor, Manufacturing operations, Mixed-product line, Saturation, Sequencing, Work overload}
}

@article{10.1145/3039207,
author = {Hirzel, Martin and Schneider, Scott and Gedik, Bu\u{g}ra},
title = {SPL: An Extensible Language for Distributed Stream Processing},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/3039207},
doi = {10.1145/3039207},
abstract = {Big data is revolutionizing how all sectors of our economy do business, including telecommunication, transportation, medical, and finance. Big data comes in two flavors: data at rest and data in motion. Processing data in motion is stream processing. Stream processing for big data analytics often requires scale that can only be delivered by a distributed system, exploiting parallelism on many hosts and many cores. One such distributed stream processing system is IBM Streams. Early customer experience with IBM Streams uncovered that another core requirement is extensibility, since customers want to build high-performance domain-specific operators for use in their streaming applications. Based on these two core requirements of distribution and extensibility, we designed and implemented the Streams Processing Language (SPL). This article describes SPL with an emphasis on the language design, distributed runtime, and extensibility mechanism. SPL is now the gateway for the IBM Streams platform, used by our customers for stream processing in a broad range of application domains.},
journal = {ACM Trans. Program. Lang. Syst.},
month = mar,
articleno = {5},
numpages = {39},
keywords = {Stream processing}
}

@inproceedings{10.1007/978-3-642-27872-3_21,
author = {Kumar, K. M. Anil and Suresha, Suresha},
title = {Detection of web users' opinion from normal and short opinionated words},
year = {2010},
isbn = {9783642278716},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-27872-3_21},
doi = {10.1007/978-3-642-27872-3_21},
abstract = {In this paper we present an approach to identify opinion of web users from an opinionated text and to classify web user's opinion into positive or negative. Web users document their opinion in opinionated sites, shopping sites, personal pages etc., to express and share their opinion with other web users. The opinion expressed by web users may be on diverse topics such as politics, sports, products, movies etc. These opinions will be very useful to others such as, leaders of political parties, selection committees of various sports, business analysts and other stake holders of products, directors and producers of movies as well as to the other concerned web users. Today web users express their opinion using normal words and short words. These short words, such as gud for good, grt8 for great etc., are very popular and are used by a large number of web users to document their opinion. We use semantic based approach to find users opinion from both normal and short words. Our approach first detects subjective phrases and uses these phrases along with intensifiers and diminishers to obtain semantic orientation scores. The semantic orientation score of these phrases is used to identify user's opinion from an opinionated text. Our approach provides better results than the other approaches on different data sets.},
booktitle = {Proceedings of the Second International Conference on Data Engineering and Management},
pages = {139–145},
numpages = {7},
keywords = {artificial intelligence, opinion mining, sentiment analysis},
location = {Tiruchirappalli, India},
series = {ICDEM'10}
}

@article{10.5555/2591248.2591263,
author = {Androutsopoulos, Ion and Lampouras, Gerasimos and Galanis, Dimitrios},
title = {Generating natural language descriptions from OWL ontologies: the natural OWL system},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {We present Naturalowl, a natural language generation system that produces texts describing individuals or classes of owl ontologies. Unlike simpler owl verbalizers, which typically express a single axiom at a time in controlled, often not entirely fluent natural language primarily for the benefit of domain experts, we aim to generate fluent and coherent multi-sentence texts for end-users. With a system like Naturalowl, one can publish information in owl on the Web, along with automatically produced corresponding texts in multiple languages, making the information accessible not only to computer programs and domain experts, but also end-users. We discuss the processing stages of Naturalowl, the optional domain-dependent linguistic resources that the system can use at each stage, and why they are useful. We also present trials showing that when the domain-dependent linguistic resources are available, Naturalowl produces significantly better texts compared to a simpler verbalizer, and that the resources can be created with relatively light effort.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {671–715},
numpages = {45}
}

@inproceedings{10.1007/978-3-030-96772-7_15,
author = {Ji, Cheng and Wang, Chu and Song, Mingyan and Wang, Fengmin},
title = {A 3D Dubins Curve Constructing Method Based on Particle Swarm Optimization},
year = {2021},
isbn = {978-3-030-96771-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-96772-7_15},
doi = {10.1007/978-3-030-96772-7_15},
abstract = {The navigation error of aircraft increases in task. Aircraft has to correct the navigation error under structure constraints to avoid path deviation caused by navigation error. Aircraft path planning with navigation correction under the turning radius constraint is a challenge for traditional path planning methods. In this paper, we propose a 3D Dubins curve constructing method which can draw a smooth path in 3D space for the aircraft, next we extend Dynamic Programming for Navigation Error Correction method by 3D Dubins curves to abtain a feasible path under the constraints of turning radius, and then we improve particle swarm optimization method to compute an almost optimal Dubins curve. Finally our algorithm return a feasible smooth path with approximately the optimal length for the path planning problem with navigation correction under the turning radius constraint.},
booktitle = {Parallel and Distributed Computing, Applications and Technologies: 22nd International Conference, PDCAT 2021, Guangzhou, China, December 17–19, 2021, Proceedings},
pages = {150–160},
numpages = {11},
keywords = {Path planning, Dubins curve, Particle swarm optimization},
location = {Guangzhou, China}
}

@inproceedings{10.5555/3540261.3542304,
author = {Zhang, Jiangning and Xu, Chao and Li, Jian and Chen, Wenzhou and Wang, Yabiao and Tai, Ying and Chen, Shuo and Wang, Chengjie and Huang, Feiyue and Liu, Yong},
title = {Analogous to evolutionary algorithm: designing a unified sequence model},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Inspired by biological evolution, we explain the rationality of Vision Transformer by analogy with the proven practical Evolutionary Algorithm (EA) and derive that both of them have consistent mathematical representation. Analogous to the dynamic local population in EA, we improve the existing transformer structure and propose a more efficient EAT model, and design task-related heads to deal with different tasks more flexibly. Moreover, we introduce the spatial-filling curve into the current vision transformer to sequence image data into a uniform sequential format. Thus we can design a unified EAT framework to address multi-modal tasks, separating the network architecture from the data format adaptation. Our approach achieves state-of-the-art results on the ImageNet classification task compared with recent vision transformer works while having smaller parameters and greater throughput. We further conduct multi-modal tasks to demonstrate the superiority of the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves the rank-1 by +3.7 points over the baseline on the CSS dataset.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2043},
numpages = {15},
series = {NIPS '21}
}

@inbook{10.5555/3454287.3454621,
author = {Hwang, Gunpil and Kim, Seohyeon and Bae, Hyeon-Min},
title = {Bat-G net: bat-inspired high-resolution 3D image reconstruction using ultrasonic echoes},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, a bat-inspired high-resolution ultrasound 3D imaging system is presented. Live bats demonstrate that the properly used ultrasound can be used to perceive 3D space. With this in mind, a neural network referred to as a Bat-G network is implemented to reconstruct the 3D representation of target objects from the hyperbolic FM (HFM) chirped ultrasonic echoes. The Bat-G network consists of an encoder emulating a bat's central auditory pathway, and a 3D graphical visualization decoder. For the acquisition of the ultrasound data, a custom-made Bat-I sensor module is used. The Bat-G network shows the uniform 3D reconstruction results and achieves precision, recall, and F1-score of 0.896, 0.899, and 0.895, respectively. The experimental results demonstrate the implementation feasibility of a high-resolution non-optical sound-based imaging system being used by live bats. The project web page (https://sites.google.com/view/batgnet) contains additional content summarizing our research.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {334},
numpages = {12}
}

@inproceedings{10.5555/3367032.3367200,
author = {Terra-Neves, Miguel and Lynce, In\^{e}s and Manquinho, Vasco},
title = {Integrating Pseudo-Boolean constraint reasoning in multi-objective evolutionary algorithms},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Constraint-based reasoning methods thrive in solving problem instances with a tight solution space. On the other hand, evolutionary algorithms are usually effective when it is not hard to satisfy the problem constraints. This dichotomy has been observed in many optimization problems. In the particular case of Multi-Objective Combinatorial Optimization (MOCO), new recently proposed constraint-based algorithms have been shown to outperform more established evolutionary approaches when a given problem instance is hard to satisfy. In this paper, we propose the integration of constraint-based procedures in evolutionary algorithms for solving MOCO. First, a new core-based smart mutation operator is applied to individuals that do not satisfy all problem constraints. Additionally, a new smart improvement operator based on Minimal Correction Subsets is used to improve the quality of the population. Experimental results clearly show that the integration of these operators greatly improves multi-objective evolutionary algorithms MOEA/D and NSGAII. Moreover, even on problem instances with a tight solution space, the newly proposed algorithms outperform the state-of-the-art constraint-based approaches for MOCO.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {1184–1190},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

@inproceedings{10.5555/2050167.2050171,
author = {Nunes, Ingrid and Cowan, Donald and Cirilo, Elder and De Lucena, Carlos J. P.},
title = {A case for new directions in agent-oriented software engineering},
year = {2010},
isbn = {9783642226359},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The state-of-the-art of Agent-oriented Software Engineering (AOSE) is insufficiently reflected in the state-of-practice in developing complex distributed systems. This paper discusses software engineering (SE) areas that have not been widely addressed in the context of AOSE, leading to a lack of mechanisms that support the development of Multiagent Systems (MASs) based on traditional SE principles, such as modularity, reusability and maintainability. This discussion is based on an exploratory study of the development of a family of buyer agents following the belief-desire-intention model and using a Software Product Line architecture. Based on the discussion presented in this paper, we hope to encourage the AOSE community to address particular SE issues on the development of MAS that have not yet been (widely) considered.},
booktitle = {Proceedings of the 11th International Conference on Agent-Oriented Software Engineering},
pages = {37–61},
numpages = {25},
keywords = {agent-oriented software engineering, multi-agent systems, software architectures, software product lines, software reuse},
location = {Toronto, Canada},
series = {AOSE'10}
}

@article{10.1016/j.artmed.2012.03.004,
author = {Mariam, Mai and Delb, Wolfgang and Schick, Bernhard and Strauss, Daniel J.},
title = {Feasibility of an objective electrophysiological loudness scaling: A kernel-based novelty detection approach},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {55},
number = {3},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2012.03.004},
doi = {10.1016/j.artmed.2012.03.004},
abstract = {Objective: The objective of our research is to structure a foundation for an electrophysiological loudness scaling measurement, in particular to estimate an uncomfortable loudness (UCL) level by using the hybrid wavelet-kernel novelty detection (HWND). Methods and materials: Late auditory evoked potentials (LAEPs) were obtained from 20 normal hearing adults. These LAEPs were stimulated by 4 intensity levels (60 decibel (dB) sound pressure level (SPL), 70dB SPL, 80dB SPL, and 90dB SPL). We have extracted the habituation correlates in LAEPs by using HWND. For this, we employed a lattice structure-based wavelet frame decompositions for feature extraction combined with a kernel-based novelty detector. Results: The group results showed that the habituation correlates degrees, i.e., relative changes within the sweep sequences, were significantly different among 60dB SPL, 70dB SPL, 80dB SPL, and 90dB SPL stimulation level, independently from the intensity related amplitude information in the averaged LAEPs. At these particular intensities, 60% of the subjects show the correlation between the novelty measures and the stimulation levels resembles a loudness scaling function, in reverse. In this paper, we have found a correlation in between the novelty measures and loudness perception as well. We have found that high ranges of loudness levels such as loud, upper level and too loud show generally 4.88% of novelty measures and comfortable ranges of loudness levels, i.e., soft, comfortable but soft, comfortable loud and comfortable but loud are generally have 12.29% of novelty measures. Additionally, we demonstrated that our sweep-to-sweep basis of post processing scheme is reliable for habituation extraction and offers an advantage of reducing experimental time as the proposed scheme need less than 20% of single sweeps in comparison to the amount that are commonly used in arithmetical average for a meaningful result. Conclusions: We assessed the feasibility of habituation correlates for an objective loudness scaling. With respect to this first feasibility study, the presented results are promising when using the described signal processing and machine learning methodology. For the group results, the novelty measures approach is able to discriminate 60dB, 70dB, 80dB and 90dB stimulated sweeps. In addition, a correlation between the novelty measures and the subjective loudness scaling is observed. However, more loudness perception and frequency specific experiments need to be conducted to determine the UCL novelty measures threshold as well as clinically oriented studies are necessary to evaluate whether this approach might be used in the objective hearing instrument fitting procedures.},
journal = {Artif. Intell. Med.},
month = jul,
pages = {185–195},
numpages = {11},
keywords = {Adapted filter banks, Event-related potentials, Habituation, Kernel machines, Loudness scaling, Uncomfortable loudness level}
}

@inproceedings{10.5555/1754788.1754805,
author = {Laddaga, Robert and Robertson, Paul and Shrobe, Howie},
title = {Probabilistic dispatch, dynamic domain architecture, and self-adaptive software},
year = {2001},
isbn = {3540007318},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we report on a beginning effort in the self adaptive software research area of improving function or method dispatch. We extend type-signature based method dispatch in a dynamic object oriented programming language with probabilistic dispatch, where the choice of method to use is determined by statistical means. This research direction is part of a larger self adaptive software effort at the MIT Artificial Intelligence Laboratory, called Dynamic Domain Architectures.},
booktitle = {Proceedings of the 2nd International Conference on Self-Adaptive Software: Applications},
pages = {227–237},
numpages = {11},
keywords = {aspect oriented programming, dynamic object languages, method dispatch, probabilistic dispatch, self-adaptive software},
location = {Balatonf\"{u}red, Hungary},
series = {IWSAS'01}
}

@article{10.1007/s10844-017-0479-y,
author = {Cai, Yuanyuan and Zhang, Qingchuan and Lu, Wei and Che, Xiaoping},
title = {A hybrid approach for measuring semantic similarity based on IC-weighted path distance in WordNet},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {1},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-017-0479-y},
doi = {10.1007/s10844-017-0479-y},
abstract = {As a valuable tool for text understanding, semantic similarity measurement enables discriminative semantic-based applications in the fields of natural language processing, information retrieval, computational linguistics and artificial intelligence. Most of the existing studies have used structured taxonomies such as WordNet to explore the lexical semantic relationship, however, the improvement of computation accuracy is still a challenge for them. To address this problem, in this paper, we propose a hybrid WordNet-based approach CSSM-ICSP to measuring concept semantic similarity, which leverage the information content(IC) of concepts to weight the shortest path distance between concepts. To improve the performance of IC computation, we also develop a novel model of the intrinsic IC of concepts, where a variety of semantic properties involved in the structure of WordNet are taken into consideration. In addition, we summarize and classify the technical characteristics of previous WordNet-based approaches, as well as evaluate our approach against these approaches on various benchmarks. The experimental results of the proposed approaches are more correlated with human judgment of similarity in term of the correlation coefficient, which indicates that our IC model and similarity detection approach are comparable or even better for semantic similarity measurement as compared to others.},
journal = {J. Intell. Inf. Syst.},
month = aug,
pages = {23–47},
numpages = {25},
keywords = {Concept semantic similarity, Edge distance, Intrinsic information content, WordNet}
}

@article{10.1016/j.specom.2012.01.002,
author = {Zelinka, Petr and Sigmund, Milan and Schimmel, Jiri},
title = {Impact of vocal effort variability on automatic speech recognition},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {6},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2012.01.002},
doi = {10.1016/j.specom.2012.01.002},
abstract = {The impact of changes in a speaker's vocal effort on the performance of automatic speech recognition has largely been overlooked by researchers and virtually no speech resources exist for the development and testing of speech recognizers at all vocal effort levels. This study deals with speech properties in the whole range of vocal modes - whispering, soft speech, normal speech, loud speech, and shouting. Fundamental acoustic and phonetic changes are documented. The impact of vocal effort variability on the performance of an isolated-word recognizer is shown and effective means of improving the system's robustness are tested. The proposed multiple model framework approach reaches a 50% relative reduction of word error rate compared to the baseline system. A new specialized speech database, BUT-VE1, is presented, which contains speech recordings of 13 speakers at 5 vocal effort levels with manual phonetic segmentation and sound pressure level calibration.},
journal = {Speech Commun.},
month = jul,
pages = {732–742},
numpages = {11},
keywords = {Machine learning, Robust speech recognition, Vocal effort level}
}

@inproceedings{10.1109/ICSE.2019.00112,
author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
title = {Distance-based sampling of software configuration spaces},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00112},
doi = {10.1109/ICSE.2019.00112},
abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1084–1094},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1016/j.neucom.2018.11.060,
author = {Li, Shaoyong and Tang, Chang and Liu, Xinwang and Liu, Yaping and Chen, Jiajia},
title = {Dual graph regularized compact feature representation for unsupervised feature selection},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {331},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.11.060},
doi = {10.1016/j.neucom.2018.11.060},
journal = {Neurocomput.},
month = feb,
pages = {77–96},
numpages = {20},
keywords = {Unsupervised feature selection, Dictionary learning, Local geometrical structure preservation, Feature representation}
}

@inproceedings{10.1007/978-3-030-66823-5_24,
author = {Campari, Tommaso and Eccher, Paolo and Serafini, Luciano and Ballan, Lamberto},
title = {Exploiting Scene-Specific Features for Object Goal Navigation},
year = {2020},
isbn = {978-3-030-66822-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66823-5_24},
doi = {10.1007/978-3-030-66823-5_24},
abstract = {Can the intrinsic relation between an object and the room in which it is usually located help agents in the Visual Navigation Task? We study this question in the context of Object Navigation, a problem in which an agent has to reach an object of a specific class while moving in a complex domestic environment. In this paper, we introduce a new reduced dataset that speeds up the training of navigation models, a notoriously complex task. Our proposed dataset permits the training of models that do not exploit online-built maps in reasonable times even without the use of huge computational resources. Therefore, this reduced dataset guarantees a significant benchmark and it can be used to identify promising models that could be then tried on bigger and more challenging datasets. Subsequently, we propose the SMTSC model, an attention-based model capable of exploiting the correlation between scenes and objects contained in them, highlighting quantitatively how the idea is correct.},
booktitle = {Computer Vision – ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part IV},
pages = {406–421},
numpages = {16},
keywords = {Visual Navigation, ObjectGoal Navigation, Reinforcement Learning},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/3540261.3541282,
author = {Zhang, Jiwen and Wei, Zhongyu and Fan, Jianqing and Peng, Jiajie},
title = {Curriculum learning for vision-and-language navigation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vision-and-Language Navigation (VLN) is a task where an agent navigates in an embodied indoor environment under human instructions. Previous works ignore the distribution of sample difficulty and we argue that this potentially degrade their agent performance. To tackle this issue, we propose a novel curriculum-based training paradigm for VLN tasks that can balance human prior knowledge and agent learning progress about training samples. We develop the principle of curriculum design and re-arrange the benchmark Room-to-Room (R2R) dataset to make it suitable for curriculum training. Experiments show that our method is model-agnostic and can significantly improve the performance, the generalizability, and the training efficiency of current state-of-the-art navigation agents without increasing model complexity.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1021},
numpages = {12},
series = {NIPS '21}
}

@article{10.5555/3044222.3051232,
author = {Montalvillo, Leticia and D\'{\i}az, Oscar},
title = {Requirement-driven evolution in software product lines},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
abstract = {We conducted a systematic mapping study on SPL evolution.We identified 107 relevant contributions on the topic up to mid 2015.We elaborated on the traditional change mini-cycle to classify the contributions.We identified well-established topics, trends and open research issues. CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As SPLs exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the SPL as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring.OBJECTIVE. Research on SPL evolution has not been previously mapped. This work provides a mapping study along Petersen's and Kichenham's guidelines, to identify strong areas of knowledge, trends and gaps.RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, SPL architecture, code assets and products).CONCLUSION. Analyses of the results indicate that "Solution proposals" are the most common type of contribution (31%). Regarding the evolution activity, "Implement change" (43%) and "Analyze and plan change" (37%) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included.},
journal = {J. Syst. Softw.},
month = dec,
pages = {110–143},
numpages = {34},
keywords = {Evolution, Software product lines, Systematic mapping study}
}

@article{10.1007/s00138-017-0839-1,
author = {Biagio, Marco San and Beltr\'{a}n-Gonz\'{a}lez, Carlos and Giunta, Salvatore and Bue, Alessio Del and Murino, Vittorio},
title = {Automatic inspection of aeronautic components},
year = {2017},
issue_date = {August    2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {5–6},
issn = {0932-8092},
url = {https://doi.org/10.1007/s00138-017-0839-1},
doi = {10.1007/s00138-017-0839-1},
abstract = {Industrial processes are costly in terms of time, money and customer satisfaction. The global economic pressures have gradually led businesses to improve these processes to become more competitive. As a result, the demand of intelligent visual inspection systems aimed at ensuring the high quality in production lines is increasing. In this paper, we present a computer vision system that, using only images, is able to address two main problems: (i) model checking: automatically check whether a component meets given specifications or rules, (ii) visual inspection: defect inspection on irregular surfaces, in particular, decolourization and scratches detection. In the experimental results, we show the effectiveness of our system and the readiness of such technologies for their integration in industrial processes.},
journal = {Mach. Vision Appl.},
month = aug,
pages = {591–605},
numpages = {15},
keywords = {Automatic visual inspection, Defects inspection, Image processing, Machine learning, Machine vision, Model checking, Multi-view analysis, Registration}
}

@inproceedings{10.5555/3495724.3496750,
author = {Zhang, Dingwen and Tian, Haibin and Han, Jungong},
title = {Few-cost salient object detection with adversarial-paced learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Detecting and segmenting salient objects from given image scenes has received great attention in recent years. A fundamental challenge in training the existing deep saliency detection models is the requirement of large amounts of annotated data. While gathering large quantities of training data becomes cheap and easy, annotating the data is an expensive process in terms of time, labor and human expertise. To address this problem, this paper proposes to learn the effective salient object detection model based on the manual annotation on a few training images only, thus dramatically alleviating human labor in training models. To this end, we name this task as the few-cost salient object detection and propose an adversarial-paced learning (APL)-based framework to facilitate the few-cost learning scenario. Essentially, APL is derived from the self-paced learning (SPL) regime but it infers the robust learning pace through the data-driven adversarial learning mechanism rather than the heuristic design of the learning regularizer. Comprehensive experiments on four widely-used benchmark datasets demonstrate that the proposed method can effectively approach to the existing supervised deep salient object detection models with only 1k human-annotated training images.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1026},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1016/j.sysarc.2016.07.004,
author = {Malazgirt, Gorker Alp and Yurdakul, Arda},
title = {Prenaut},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {72},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2016.07.004},
doi = {10.1016/j.sysarc.2016.07.004},
abstract = {Prenaut is a design space exploration method for finding the best on-chip architectures given processor cores, Level 1, Level 2, and Level 3 caches.Current design space exploration methods mostly explore cache, processor configurations with a fixed architecture. Our method explores architectures with fixed cache and processor configurations.Prenaut is to build a data oriented design space exploration method that exploits simulation data to its full extent rather than discarding it.Prenaut uses the available simulation data and applies machine learning methods for estimating design parameters. As embedded systems have evolved to appear in many different domains, symmetric multiprocessing (SMP) has been the design choice from low-end to high-end devices. In this paper we present Prenaut, a design space exploration method for finding the best on-chip SMP architectures given processor cores, Level 1, Level 2, and Level 3 caches. Unlike traditional design space exploration tools that are majorly concerned with optimizations in processor, memory and cache structures with a fixed on-chip architecture, Prenaut explores architectures that have not been considered in symmetric multiprocessing domain. These architectures consist of shared instruction caches between cores and heterogeneous cache topologies that feature bypassing a level in the cache hierarchy. The design idea behind Prenaut is to build a data oriented design space exploration method that exploits simulation data to its full extent rather than discarding it. Therefore, Prenaut uses simulation data and applies machine learning methods for estimating design parameters. This provides very rapid estimation of the Pareto set and guides designers through the overall system design process. The design space is pruned by topological clustering of design points which groups similar topologies and new simulation points are selected via an ordered look up table that prevents infeasible random jumps in the design space. For the selected benchmarks, Prenaut can estimate the Pareto set up to 147x faster and the clustering information can reduce the design space up to 82% in comparison with a state-of-the-art evolutionary algorithm.},
journal = {J. Syst. Archit.},
month = jan,
pages = {3–18},
numpages = {16},
keywords = {Clustering, Design space exploration, Machine learning, Symmetric multiprocessing}
}

@inproceedings{10.5555/3386691.3386706,
author = {Lu, Sidi and Luo, Bing and Patel, Tirthak and Yao, Yongtao and Tiwari, Devesh and Shi, Weisong},
title = {Making disk failure predictions SMARTer!},
year = {2020},
isbn = {9781939133120},
publisher = {USENIX Association},
address = {USA},
abstract = {Disk drives are one of the most commonly replaced hardware components and continue to pose challenges for accurate failure prediction. In this work, we present analysis and findings from one of the largest disk failure prediction studies covering a total of 380,000 hard drives over a period of two months across 64 sites of a large leading data center operator. Our proposed machine learning based models predict disk failures with 0.95 F-measure and 0.95 Matthews correlation coefficient (MCC) for 10-days prediction horizon on average.},
booktitle = {Proceedings of the 18th USENIX Conference on File and Storage Technologies},
pages = {151–168},
numpages = {18},
location = {Santa Clara, CA, USA},
series = {FAST'20}
}

@inproceedings{10.1007/978-3-030-32248-9_54,
author = {Han, Shuo and Carass, Aaron and Prince, Jerry L.},
title = {Hierarchical Parcellation of the Cerebellum},
year = {2019},
isbn = {978-3-030-32247-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32248-9_54},
doi = {10.1007/978-3-030-32248-9_54},
abstract = {Parcellation of the cerebellum in an MR image has been used to study regional associations with both motion and cognitive functions. Despite the fact that the division of the cerebellum is defined hierarchically—i.e., the cerebellum can be divided into lobes and the lobes can be further divided into lobules—previous automatic methods to parcellate the cerebellum do not utilize this information. In this work, we propose a method based on convolutional neural networks&nbsp;(CNNs) to explicitly incorporate the hierarchical organization of the cerebellum. The network is constructed in a tree structure with each node representing a cerebellar region and having child nodes that further subdivide the region into finer substructures. Thus, our CNN is aware of the hierarchical organization of the cerebellum. Furthermore, by selecting tree nodes to represent the hierarchical properties of a given training sample, our network can be trained with heterogeneous training data that are labeled to different hierarchical depths. The proposed method was compared with a state-of-the-art cerebellum parcellation network. Our approach shows promising results as a first parcellation method to take the cerebellar hierarchical organization into consideration.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part III},
pages = {484–491},
numpages = {8},
location = {Shenzhen, China}
}

@inproceedings{10.5555/3297863.3297936,
author = {Hanna, Josiah P. and Stone, Peter},
title = {Grounded action transformation for robot learning in simulation},
year = {2017},
publisher = {AAAI Press},
abstract = {Robot learning in simulation is a promising alternative to the prohibitive sample cost of learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the physical robot. This paper proposes a new algorithm for learning in simulation - Grounded Action Transformation - and applies it to learning of humanoid bipedal locomotion. Our approach results in a 43.27% improvement in forward walk velocity compared to a state-of-the art hand-coded walk.1},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {4931–4932},
numpages = {2},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.5555/1577069.1577086,
author = {Li, Junning and Wang, Z. Jane},
title = {Controlling the False Discovery Rate of the Association/Causality Structure Learned with the PC Algorithm},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {In real world applications, graphical statistical models are not only a tool for operations such as classification or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level, and a heuristic modification of the method is able to control the FDR more accurately around the user-specified level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models.},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {475–514},
numpages = {40}
}

@article{10.1016/j.neucom.2019.05.009,
author = {Song, Shaoyue and Yu, Hongkai and Miao, Zhenjiang and Guo, Dazhou and Ke, Wei and Ma, Cong and Wang, Song},
title = {An easy-to-hard learning strategy for within-image co-saliency detection},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {358},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.05.009},
doi = {10.1016/j.neucom.2019.05.009},
journal = {Neurocomput.},
month = sep,
pages = {166–176},
numpages = {11},
keywords = {Within-image co-saliency, Easy-to-hard learning, Multiple instance learning}
}

@inproceedings{10.1007/978-3-642-54804-8_1,
author = {Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Daum, Marcus and Klein, Joachim and M\"{a}rcker, Steffen and Wunderlich, Sascha},
title = {Probabilistic Model Checking and Non-standard Multi-objective Reasoning},
year = {2014},
isbn = {9783642548031},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-54804-8_1},
doi = {10.1007/978-3-642-54804-8_1},
abstract = {Probabilistic model checking is a well-established method for the automated quantitative system analysis. It has been used in various application areas such as coordination algorithms for distributed systems, communication and multimedia protocols, biological systems, resilient systems or security. In this paper, we report on the experiences we made in inter-disciplinary research projects where we contribute with formal methods for the analysis of hardware and software systems. Many performance measures that have been identified as highly relevant by the respective domain experts refer to multiple objectives and require a good balance between two or more cost or reward functions, such as energy and utility. The formalization of these performance measures requires several concepts like quantiles, conditional probabilities and expectations and ratios of cost or reward functions that are not supported by state-ofthe- art probabilistic model checkers. We report on our current work in this direction, including applications in the field of software product line verification.},
booktitle = {Proceedings of the 17th International Conference on Fundamental Approaches to Software Engineering - Volume 8411},
pages = {1–16},
numpages = {16}
}

@inproceedings{10.1145/3422392.3422453,
author = {Vieira, Roger Denis and Farias, Kleinner},
title = {CognIDE: A Psychophysiological Data Integrator Approach for Visual Studio Code},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422453},
doi = {10.1145/3422392.3422453},
abstract = {Wearable devices capable of capturing psychophysiological data are a reality today. Recent studies indicate that the developer's cognitive indicators (e.g., level of attention and meditation) might affect code comprehension and maintenance tasks. However, current Integrated Development Environments (IDEs) and code editors like Visual Studio (VS) Code fall short of providing contextual information of cognitive indicators located throughout source code. This article proposes CognIDE, a tool-supported approach for integrating psychophysiological data related to cognitive indicators into the VS Code. CognIDE help VS code to push a step forward, providing actionable contextual information alongside the evolving source code. The CognIDE was evaluated through a survey with 6 professionals and interviews for investigating its effects on their perception of usefulness, ease of use, and intention to use in real-world settings. With a high acceptance of the professionals, the emerging results show the potential for using CognIDE to identify and prioritize the review of source code with specific cognitive indicators, mainly related to bugs or inadequate understanding of code snippets.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {393–398},
numpages = {6},
keywords = {Bioinformatics, Data Analysis, Data Processing, Neurosciences, Software Engineering},
location = {Natal, Brazil},
series = {SBES '20}
}

@article{10.1007/s10472-019-09645-7,
author = {Liu, Xudong and Truszczynski, Miroslaw},
title = {Voting-based ensemble learning for partial lexicographic preference forests over combinatorial domains},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {87},
number = {1–2},
issn = {1012-2443},
url = {https://doi.org/10.1007/s10472-019-09645-7},
doi = {10.1007/s10472-019-09645-7},
abstract = {We study preference representation models based on partial lexicographic preference trees (PLP-trees). We propose to represent preference relations as forests of small PLP-trees (PLP-forests), and to use voting rules to aggregate orders represented by the individual trees into a single order to be taken as a model of the agent’s preference relation. We show that when learned from examples, PLP-forests have better accuracy than single PLP-trees. We also show that the choice of a voting rule does not have a major effect on the aggregated order, thus rendering the problem of selecting the “right” rule less critical. Next, for the proposed PLP-forest preference models, we develop methods to compute optimal and near-optimal outcomes, the tasks that appear difficult for some other common preference models. Lastly, we compare our models with those based on decision trees, which brings up questions for future research.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = oct,
pages = {137–155},
numpages = {19},
keywords = {Lexicographic preference models, Preference learning, Preference modeling and reasoning, Social choice theory, Computational complexity theory, Voting theory, Maximum satisfiability}
}

@article{10.1007/s11042-020-09907-1,
author = {Zhong, Yuanhong and Zhang, Jing and Zhou, Zhaokun and Cheng, Xinyu and Huang, Guan and Li, Qiang},
title = {Recovery of image and video based on compressive sensing via tensor approximation and Spatio-temporal correlation},
year = {2021},
issue_date = {Feb 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {5},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09907-1},
doi = {10.1007/s11042-020-09907-1},
abstract = {In recent years, block-based compressive sensing (BCS) has been extensively studied because it can reduce computational complexity and data storage by dividing the image into smaller patches, but the performance of the reconstruction algorithm is not satisfactory. In this paper, a new reconstruction model for image and video is proposed. The model makes full use of spatio-temporal correlation and utilizes low-rank tensor approximation to improve the quality of the reconstructed image and video. For image recovery, the proposed model obtains a low-rank approximation of a tensor formed by non-local similar patches, and improves the reconstruction quality from a spatial perspective by combining non-local similarity and low-rank property. For video recovery, the reconstruction process is divided into two phases. In the first phase, each frame of the video sequence is regarded as an independent image to be reconstructed by taking advantage of spatial property. The second phase performs tensor approximation through searching similar patches within frames near the target frame, to achieve reconstruction by putting the spatio-temporal correlation into full play. The resulting model is solved by an efficient Alternating Direction Method of Multipliers (ADMM) algorithm. A series of experiments show that the quality of the proposed model is comparable to the current state-of-the-art recovery methods.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {7433–7450},
numpages = {18},
keywords = {Block-based compressive sensing, Image and video recovery, Low-rank tensor approximation, High order singular value decomposition (HOSVD), Spatio-temporal correlation}
}

@article{10.1016/j.jss.2019.110463,
author = {Pereira, Rui and Car\c{c}\~{a}o, Tiago and Couto, Marco and Cunha, J\'{a}come and Fernandes, Jo\~{a}o Paulo and Saraiva, Jo\~{a}o},
title = {SPELLing out energy leaks: Aiding developers locate energy inefficient code},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {161},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110463},
doi = {10.1016/j.jss.2019.110463},
journal = {J. Syst. Softw.},
month = mar,
numpages = {15},
keywords = {Green Software, Program Analysis, Program Optimization, Green Computing, Fault Localization}
}

@inproceedings{10.5555/3540261.3540707,
author = {Chen, Shizhe and Guhur, Pierre-Louis and Schmid, Cordelia and Laptev, Ivan},
title = {History aware multimodal transformer for vision-and-language navigation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with fine-grained instructions (R2R, RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {446},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.1145/2970276.2975938,
author = {Babur, \"{O}nder},
title = {Statistical analysis of large sets of models},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2975938},
doi = {10.1145/2970276.2975938},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models, e.g. for comparing and merging of model variants into a common domain model. Despite many sophisticated techniques for model comparison, little attention has been given to the initial data analysis and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. We would like to develop a generic approach for model comparison and analysis for large datasets; using techniques from information retrieval, natural language processing and machine learning. We are implementing our approach as an open framework and have so far evaluated it on public datasets involving domain analysis, repository management and model searching scenarios.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {888–891},
numpages = {4},
keywords = {Model-driven engineering, clustering, model comparison, vector space model},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inbook{10.1145/3191315.3191317,
author = {Maier, David and Tekle, K. Tuncay and Kifer, Michael and Warren, David S.},
title = {Datalog: concepts, history, and outlook},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191317},
abstract = {This chapter is a survey of the history and the main concepts of Datalog.We begin with an introduction to the language and its use for database definition and querying. We then look back at the threads from logic languages, databases, artificial intelligence, and expert systems that led to the emergence of Datalog and reminiscence about the origin of the name. We consider the interaction of recursion with other common data language features, such as negation and aggregation, and look at other extensions, such as constraints, updates, and object-oriented features.We provide an overview of the main approaches to Datalog evaluation and their variants, then recount some early implementations of Datalog and of similar deductive database systems.We speculate on the reasons for the decline in the interest in the language in the 1990s and the causes for its later resurgence in a number of application areas.We conclude with several examples of current systems based on or supporting Datalog and briefly examine the performance of some of them.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {3–100},
numpages = {98}
}

@article{10.1007/s11280-019-00766-x,
author = {Hu, Rongyao and Zhu, Xiaofeng and Zhu, Yonghua and Gan, Jiangzhang},
title = {Robust SVM with adaptive graph learning},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-019-00766-x},
doi = {10.1007/s11280-019-00766-x},
abstract = {Support Vector Machine (SVM) has been widely applied in real application due to its efficient performance in the classification task so that a large number of SVM methods have been proposed. In this paper, we present a novel SVM method by taking the dynamic graph learning and the self-paced learning into account. To do this, we propose utilizing self-paced learning to assign important samples with large weights, learning a transformation matrix for conducting feature selection to remove redundant features, and learning a graph matrix from the low-dimensional data of original data to preserve the data structure. As a consequence, both the important samples and the useful features are used to select support vectors in the SVM framework. Experimental analysis on four synthetic and sixteen benchmark data sets demonstrated that our method outperformed state-of-the-art methods in terms of both binary classification and multi-class classification tasks.},
journal = {World Wide Web},
month = may,
pages = {1945–1968},
numpages = {24},
keywords = {Self-paced learning, Feature selection, Graph learning, SVM}
}

@inbook{10.5555/3454287.3454321,
author = {Anderson, Peter and Shrivastava, Ayush and Parikh, Devi and Batra, Dhruv and Lee, Stefan},
title = {Chasing ghosts: instruction following as bayesian state tracking},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A visually-grounded navigation instruction can be interpreted as a sequence of expected observations and actions an agent following the correct trajectory would encounter and perform. Based on this intuition, we formulate the problem of finding the goal location in Vision-and-Language Navigation (VLN) [1] within the framework of Bayesian state tracking – learning observation and motion models conditioned on these expectable events. Together with a mapper that constructs a semantic spatial map on-the-fly during navigation, we formulate an end-to-end differentiable Bayes filter and train it to identify the goal by predicting the most likely trajectory through the map according to the instructions. The resulting navigation policy constitutes a new approach to instruction following that explicitly models a probability distribution over states, encoding strong geometric and algorithmic priors while enabling greater explainability. Our experiments show that our approach outperforms a strong LingUNet [2] baseline when predicting the goal location on the map. On the full VLN task, i.e., navigating to the goal location, our approach achieves promising results with less reliance on navigation constraints.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {34},
numpages = {11}
}

@article{10.1007/s11219-018-9424-8,
author = {Alkharabsheh, Khalid and Crespo, Yania and Manso, Esperanza and Taboada, Jos\'{e} A.},
title = {Software Design Smell Detection: a systematic mapping study},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9424-8},
doi = {10.1007/s11219-018-9424-8},
abstract = {Design Smells are indicators of situations that negatively affect software quality attributes such as understandability, testability, extensibility, reusability, and maintainability in general. Improving maintainability is one of the cornerstones of making software evolution easier. Hence, Design Smell Detection is important in helping developers when making decisions that can improve software evolution processes. After a long period of research, it is important to organize the knowledge produced so far and to identify current challenges and future trends. In this paper, we analyze 18&nbsp;years of research into Design Smell Detection. There is a wide variety of terms that have been used in the literature to describe concepts which are similar to what we have defined as “Design Smells,” such as design defect, design flaw, anomaly, pitfall, antipattern, and disharmony. The aim of this paper is to analyze all these terms and include them in the study. We have used the standard systematic literature review method based on a comprehensive set of 395 articles published in different proceedings, journals, and book chapters. We present the results in different dimensions of Design Smell Detection, such as the type or scope of smell, detection approaches, tools, applied techniques, validation evidence, type of artifact in which the smell is detected, resources used in evaluation, supported languages, and relation between detected smells and software quality attributes according to a quality model. The main contributions of this paper are, on the one hand, the application of domain modeling techniques to obtain a conceptual model that allows the organization of the knowledge on Design Smell Detection and a collaborative web application built on that knowledge and, on the other, finding how tendencies have moved across different kinds of smell detection, as well as different approaches and techniques. Key findings for future trends include the fact that all automatic detection tools described in the literature identify Design Smells as a binary decision (having the smell or not), which is an opportunity to evolve to fuzzy and prioritized decisions. We also find that there is a lack of human experts and benchmark validation processes, as well as demonstrating that Design Smell Detection positively influences quality attributes.},
journal = {Software Quality Journal},
month = sep,
pages = {1069–1148},
numpages = {80},
keywords = {DesignSmell, Antipatterns, Detection tools, Quality models, Systematic mapping study}
}

@inproceedings{10.5555/3015812.3015904,
author = {Sonoda, Akihisa and Todo, Taiki and Yokoo, Makoto},
title = {False-name-proof locations of two facilities: economic and algorithmic approaches},
year = {2016},
publisher = {AAAI Press},
abstract = {This paper considers a mechanism design problem for locating two identical facilities on an interval, in which an agent can pretend to be multiple agents. A mechanism selects a pair of locations on the interval according to the declared single-peaked preferences of agents. An agent's utility is determined by the location of the better one (typically the closer to her ideal point). This model can represent various application domains. For example, assume a company is going to release two models of its product line and performs a questionnaire survey in an online forum to determine their detailed specs. Typically, a customer will buy only one model, but she can answer multiple times by logging onto the forum under several email accounts. We first characterize possible outcomes of mechanisms that satisfy false-name-proofness, as well as some mild conditions. By extending the result, we completely characterize the class of false-name-proof mechanisms when locating two facilities on a circle. We then clarify the approximation ratios of the false-name-proof mechanisms on a line metric for the social and maximum costs.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {615–621},
numpages = {7},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@article{10.1016/j.image.2019.04.017,
author = {Wu, Hehe and Wang, Anhong and Liang, Jie and Li, Suyue and Li, Peihao},
title = {DCSN-Cast: Deep compressed sensing network for wireless video multicast},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0923-5965},
url = {https://doi.org/10.1016/j.image.2019.04.017},
doi = {10.1016/j.image.2019.04.017},
journal = {Image Commun.},
month = aug,
pages = {56–67},
numpages = {12},
keywords = {DCSN-cast, DCSRN-cast, DCSFCN-cast, Fully connected network, Compressed sensing, Deep residual network}
}

@article{10.1287/mksc.1080.0481,
author = {Kannan, P. K. and Pope, Barbara Kline and Jain, Sanjay},
title = {Practice Prize Winner---Pricing Digital Content Product Lines: A Model and Application for the National Academies Press},
year = {2009},
issue_date = {July 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {28},
number = {4},
issn = {1526-548X},
url = {https://doi.org/10.1287/mksc.1080.0481},
doi = {10.1287/mksc.1080.0481},
abstract = {We examine the problem of how a content provider, specifically the National Academies Press (NAP), can optimally price the different forms of its product---print and PDF---that it sells online. Whereas products in the traditional product line generally tend to be substitutes, the different content product forms could range from being substitutes to being complements across customers. Thus the content provider can possibly sell bundles of the product forms, leading to additional revenue. We first discuss NAP's decision context and describe the model we proposed for developing NAP's optimal pricing policies for its different forms. We describe the choice experiment we conducted on the publisher's website that maximally uses the online interface to collect relevant data needed to estimate our model. We show how NAP embraced the results from the model for developing a new business model and how it used the insights derived from the study to set pricing policies and monitor sales performance as a function of pricing. Finally, we perform validation of the model and the implemented policies using dynamic modeling of sales data from NAP's website. The paper illustrates how e-commerce technologies can lead to the development of optimal policies using marketing models.},
journal = {Marketing Science},
month = jul,
pages = {620–636},
numpages = {17},
keywords = {Internet, bundling, choice experiment, digital products, pricing, product form, product line}
}

@article{10.1016/j.knosys.2021.107291,
author = {Mortazavi, Ali and Moloodpoor, Mahsa},
title = {Enhanced Butterfly Optimization Algorithm with a New fuzzy Regulator Strategy and Virtual Butterfly Concept},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {228},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107291},
doi = {10.1016/j.knosys.2021.107291},
journal = {Know.-Based Syst.},
month = sep,
numpages = {20},
keywords = {Butterfly Optimization Algorithm, Fuzzy logic, Engineering optimization problems}
}

@inproceedings{10.5555/3016387.3016586,
author = {Stone, Peter},
title = {What's hot at RoboCup (extended abstract)},
year = {2016},
publisher = {AAAI Press},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4346–4347},
numpages = {2},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@inproceedings{10.1007/978-3-319-47157-0_11,
author = {Wang, Yan and Wu, Xi and Ma, Guangkai and Ma, Zongqing and Fu, Ying and Zhou, Jiliu},
title = {Patch-Based Hippocampus Segmentation Using a Local Subspace Learning Method},
year = {2016},
isbn = {978-3-319-47156-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-47157-0_11},
doi = {10.1007/978-3-319-47157-0_11},
abstract = {Patch-based segmentation methods utilizing multiple atlases have been widely studied for alleviating some misalignments when registering atlases to the target image. However, weights assigned to the fused labels are typically computed based on predefined features (e.g. simple patch intensities), thus being not necessarily optimal. Due to lack of discriminating features for different regions of an anatomical structure, the original feature space defined by image intensities may limit the segmentation accuracy. To address these problems, we propose a novel local subspace learning based patch-wise label propagation method to estimate a voxel-wise segmentation of the target image. Specifically, multi-scale patch intensities and texture features are first extracted from the image patch in order to acquire the abundant appearance information. Then, margin fisher analysis (MFA) is applied to neighboring samples of each voxel to be segmented from the aligned atlases, in order to extract discriminant features. This process can enhance discrimination of features for different local regions in the anatomical structure. Finally, based on extracted discriminant features, the k-nearest neighbor (kNN) classifier is used to determine the final label for the target voxel. Moreover, for the patch-wise label propagation, we first translate label patches into several discrete class labels by using the k-means clustering method, and then apply MFA to ensure that samples with similar label patches achieve a higher similarity and those with dissimilar label patches achieve a lower similarity. To demonstrate segmentation performance, we comprehensively evaluated the proposed method on the ADNI dataset for hippocampus segmentation. Experimental results show that the proposed method outperforms several conventional multi-atlas based segmentation methods.},
booktitle = {Machine Learning in Medical Imaging: 7th International Workshop, MLMI 2016, Held in Conjunction with MICCAI 2016, Athens, Greece, October 17, 2016, Proceedings},
pages = {86–94},
numpages = {9},
keywords = {Target Image, Image Patch, Label Propagation, Spatial Neighborhood, Deformable Image Registration},
location = {Athens, Greece}
}

@article{10.1016/j.compind.2014.05.002,
author = {Yildirim, Orcun and Kardas, Geylani},
title = {A multi-agent system for minimizing energy costs in cement production},
year = {2014},
issue_date = {September, 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {65},
number = {7},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2014.05.002},
doi = {10.1016/j.compind.2014.05.002},
abstract = {A cement production planning system is expected to minimize energy costs. Further, such a system needs to be as autonomous as possible to decrease time loss during the communication between related departments of the plant. Hence, in this paper, we present a multi-agent system (MAS) in which software agents work collaboratively in order to assist production, planning and sales departments of a cement plant for the generation of cost-effective cement production plans. Implemented system was deployed and actively used inside one of the plants of a leading cement company in Turkey. Evaluation result shows that the utilization of the proposed system caused a significant energy cost saving. Moreover, workers in the planning department of the cement plant saved approximately 75% of their working hour by using the system. Total workload of the employees (including all departments) decreased to its half.},
journal = {Comput. Ind.},
month = sep,
pages = {1076–1084},
numpages = {9},
keywords = {Cement production, Mobile agent, Multi-agent system, Production planning, Software agent}
}

@article{10.1007/s00521-018-3478-1,
author = {Gu, Nannan and Fan, Pengying and Fan, Mingyu and Wang, Di},
title = {Structure regularized self-paced learning for robust semi-supervised pattern classification},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3478-1},
doi = {10.1007/s00521-018-3478-1},
abstract = {Semi-supervised classification is a hot topic in pattern recognition and machine learning. However, in presence of heavy noise and outliers, the unlabeled training data could be very challenging or even misleading for the semi-supervised classifier. In this paper, we propose a novel structure regularized self-paced learning method for semi-supervised classification problems, which can efficiently learn partially labeled training data sequentially from the simple to the complex ones. The proposed formulation consists of three components: a cost function defined by a mixture of losses, a functional complexity regularizer, and a self-paced regularizer; and the corresponding optimization algorithm involves three iterative steps: classifier updating, sample importance calculating, and pseudo-labeling. In the proposed method, the cost function for classifier updating and sample importance calculating is defined as a combination of the label fitting loss and manifold smoothness loss. Then, the importance of the pseudo-labeled and unlabeled samples is adaptively calculated by the novel cost. Unlabeled samples with high importance values are pseudo-labeled with their current predictions. In this way, labels are efficiently propagated from the labeled samples to the unlabeled ones in the robust self-paced manner. Experimental results on several benchmark data sets are provided to show the effectiveness of the proposed method.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {6559–6574},
numpages = {16},
keywords = {Semi-supervised classification, Pattern classification, Self-paced learning, Manifold learning, Locally linear coding}
}

@article{10.1016/j.eswa.2018.04.033,
author = {Sreevani and Murthy, C.A. and Chanda, Bhabatosh},
title = {Generation of compound features based on feature interaction for classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {108},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2018.04.033},
doi = {10.1016/j.eswa.2018.04.033},
journal = {Expert Syst. Appl.},
month = oct,
pages = {61–73},
numpages = {13},
keywords = {Feature extraction, Feature selection, Compound features, Semi-features, Information theory, Feature interaction, Mutual information}
}

@inproceedings{10.1145/3396743.3396765,
author = {Kuo, R. J. and Li, C. H.},
title = {Predicting Remaining Useful Life of Ball Bearing Using an Independent Recurrent Neural Network},
year = {2020},
isbn = {9781450377065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396743.3396765},
doi = {10.1145/3396743.3396765},
abstract = {Planning maintenance of facilities is an important role for production line. From preventive maintenance to predictive maintenance, the main purpose is cost down by reducing the chance of the unexpected shot down. Thus, this study intends to apply independent recurrent neural network (IndRNN), which is a kind of deep learning technique, and apply it to predict remaining useful life for the ball bearings using vibration signals. The result of the proposed method is compared with original RNN. The experimental results indicate that IndRNN is able to perform better than the other method in terms of score.},
booktitle = {Proceedings of the 2020 2nd International Conference on Management Science and Industrial Engineering},
pages = {237–241},
numpages = {5},
keywords = {Independent recurrent neural network, Predictive maintenance, Recurrent neural network, Remaining useful life},
location = {Osaka, Japan},
series = {MSIE '20}
}

@article{10.1109/TNET.2021.3056772,
author = {Ruby, Rukhsana and Zhong, Shuxin and ElHalawany, Basem M. and Luo, Hanjiang and Wu, Kaishun},
title = {SDN-Enabled Energy-Aware Routing in Underwater Multi-Modal Communication Networks},
year = {2021},
issue_date = {June 2021},
publisher = {IEEE Press},
volume = {29},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3056772},
doi = {10.1109/TNET.2021.3056772},
abstract = {Despite extensive research efforts, underwater sensor networks (UWSNs) still suffer from serious performance issues due to their inefficient and uncoordinated channel access and resource management. For example, due to the lack of holistic knowledge on the network resources, existing decentralized routing protocols fail to provide globally optimal performance. On the other hand, Software Defined Networking (SDN), as a promising paradigm to provide prominent centralized solutions, can be employed to address the aforementioned issues in UWSNs. Indeed, SDN brings unprecedented opportunities to improve the network performance through the development of advanced algorithms at controllers. In this paper, we study the routing problem in such a network with new features including centralized route decision, global network-state awareness, seamless route discovery while considering the optimization of several long-term global performance metrics. We formulate the entire routing problem of a multi-modal UWSN as an optimization problem while considering the interference phenomenon of ad hoc scenarios and some long-term global performance metrics of an ideal routing protocol. Our formulated problem nicely captures all possible flexibilities of a sensor node no matter it has the full-duplex or half-duplex functionality. Upon the formulation, we recognize the NP-hard nature of the problem for all possible scenarios. We adopt a rounding technique based on the convex programming relaxation concept to solve the formulated routing problem that considers full-duplex scenarios, whereas we solve the problem for half-duplex scenarios using a greedy method upon interpreting it as a submodular function maximization problem. Through extensive simulation via our Python-based in-house simulator, we verify that our proposed globally optimal routing scheme always outperforms three existing decentralized routing protocols (each of these protocols are selected from each of three prominent protocol types, i.e., flooding, cross-layer information and adaptive machine learning based, respectively) in terms of reliability, latency, energy efficiency, lifetime and fairness.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {965–978},
numpages = {14}
}

@inproceedings{10.1007/978-3-030-71827-5_11,
author = {Estienne, Th\'{e}o and Vakalopoulou, Maria and Battistella, Enzo and Carr\'{e}, Alexandre and Henry, Th\'{e}ophraste and Lerousseau, Marvin and Robert, Charlotte and Paragios, Nikos and Deutsch, Eric},
title = {Deep Learning Based Registration Using&nbsp;Spatial Gradients and Noisy Segmentation Labels},
year = {2020},
isbn = {978-3-030-71826-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-71827-5_11},
doi = {10.1007/978-3-030-71827-5_11},
abstract = {Image registration is one of the most challenging problems in medical image analysis. In the recent years, deep learning based approaches became quite popular, providing fast and performing registration strategies. In this short paper, we summarise our work presented on Learn2Reg challenge 2020. The main contributions of our work rely on (i) a symmetric formulation, predicting the transformations from source to target and from target to source simultaneously, enforcing the trained representations to be similar and (ii) integration of variety of publicly available datasets used both for pretraining and for augmenting segmentation labels. Our method reports a mean dice of 0.64 for task 3 and 0.85 for task 4 on the test sets, taking third place on the challenge. Our code and models are publicly available at  and .},
booktitle = {Segmentation, Classification, and Registration of Multi-Modality Medical Imaging Data: MICCAI 2020 Challenges, ABCs 2020, L2R 2020, TN-SCUI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {87–93},
numpages = {7},
location = {Lima, Peru}
}

@article{10.1016/j.jss.2015.12.030,
author = {Liu, Lin and Zhou, Qing and Liu, Jilei and Cao, Zhanqiang},
title = {Requirements cybernetics},
year = {2017},
issue_date = {February 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {124},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.12.030},
doi = {10.1016/j.jss.2015.12.030},
abstract = {Users' behavioral data provides important cue for product improvement.Requirements elicitation process is formulated as a feedback control system.This is an attempt to seek synergies between requirements and cybernetics.Potential control variables of data-driven elicitation are discussed. Users' behavioral data provides important cue for product improvement. Today's web based applications collect various kinds of service data, which is an ideal source of information for product designers to better understand users' needs and behaviors. This paper first discusses the types of data collected so far, and then such data-driven requirements elicitation process is formulated as a feedback control system, where the classical requirements elicitation philosophy turns into a continuous optimization to user behavioral models. To this end, it is important to know how the data collection function reflects user behavior, and how specific data analysis approaches help making design decisions. This is an attempt to seek practical synergies between the two disciplines of requirements and cybernetics, to explore the possibilities of formulating problems in requirements with concepts and frameworks from cybernetics, and understand to what extent that known research results from cybernetics can be applied to address requirements problems. In particular, control frameworks for the user data driven requirements elicitation process are experimented, and potential control variables are discussed. We use two example cases to illustrate the proposed approach, an online dictionary service and a mobile music player service.},
journal = {J. Syst. Softw.},
month = feb,
pages = {187–194},
numpages = {8},
keywords = {Cybernetics, Data analysis, Requirements elicitation}
}

@inproceedings{10.1145/2245276.2231938,
author = {Sardinha, Alberto and Yu, Yijun and Niu, Nan and Rashid, Awais},
title = {EA-tracer: identifying traceability links between code aspects and early aspects},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231938},
doi = {10.1145/2245276.2231938},
abstract = {Early aspects are crosscutting concerns that are identified and addressed at the requirements and architecture level, while code aspects are crosscutting concerns that manifest at the code level. Currently, there are many approaches to address the identification and modularization of these cross-cutting concerns at each level, but very few techniques try to analyze the relationship between early aspects and code aspects. This paper presents a tool for automating the process of identifying traceability links between requirements-level aspects and code aspects, which is a first step towards an in-depth analysis. We also present an empirical evaluation of the tool with a real-life Web-based information system and a software product line for handling data on mobile devices. The results show that we can identify traceability links between early aspects and code aspects with a high accuracy.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1035–1042},
numpages = {8},
location = {Trento, Italy},
series = {SAC '12}
}

@article{10.1017/S0269888909990051,
author = {Recio-garc\'{\i}a, Juan antonio and D\'{\i}az-agudo, Bel\'{e}n and Gonz\'{a}lez-calero, Pedro antonio},
title = {Semantic templates for case-based reasoning systems},
year = {2009},
issue_date = {September 2009},
publisher = {Cambridge University Press},
address = {USA},
volume = {24},
number = {3},
issn = {0269-8889},
url = {https://doi.org/10.1017/S0269888909990051},
doi = {10.1017/S0269888909990051},
abstract = {In this paper, we present an approach to solve the drawbacks of manual composition of software components. Our approach is applied within the jcolibri framework for building case-based reasoning (CBR) applications. We propose a system design process based on reusing templates obtained from previously designed CBR systems. Templates store the control flow of the CBR applications and include semantic annotations conceptualizing its behavior and expertise. We use CBR ontology to formalize syntactical, semantical and pragmatical aspects of the reusable components of the framework. The ontology vocabulary facilitates an annotation process of the components and allows to reason about their composition, facilitating the semi-automatic configuration of complex systems from their composing pieces.},
journal = {Knowl. Eng. Rev.},
month = sep,
pages = {245–264},
numpages = {20}
}

@article{10.1287/ited.2019.0216cs,
author = {Rao, B. Madhu and Xanthopoulos, Petros and Zheng, Qipeng Phil},
title = {Case—Production Scheduling at DeLand Crayon Company },
year = {2020},
issue_date = {January 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {20},
number = {2},
issn = {1532-0545},
url = {https://doi.org/10.1287/ited.2019.0216cs},
doi = {10.1287/ited.2019.0216cs},
abstract = {DeLand Crayon Company (DCC) is a large manufacturer of molded wax crayons. DCC sells three sizes of crayons, namely standard, large and jumbo. Standard-sized crayons, which account for most of the demand, include nine basic colors (red, blue, green, yellow, orange, violet, brown, black, and white), seven popular combination colors (red-orange, yellow-orange, yellow-green, red-violet, blue-green, blue-violet, and pink), 16 fluorescent colors, and four metallic colors. DCC also manufactures several specialty crayons, such as reduced length, hexagonal shaped, golden glitter, glow-in-the-dark, wipe-off, washable, and keno. Demand for non–standard-size crayons, fluorescent, and metallic-colored standard crayons and the specialty crayons is low and uneven, but DCC believes that they must be maintained to create the perception of a broad product line.},
journal = {INFORMS Trans. Edu.},
month = jan,
pages = {99–101},
numpages = {3}
}

@article{10.1016/j.cor.2010.01.011,
author = {McElreath, Mark H. and Mayorga, Maria E. and Kurz, Mary E.},
title = {Metaheuristics for assortment problems with multiple quality levels},
year = {2010},
issue_date = {October, 2010},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {37},
number = {10},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2010.01.011},
doi = {10.1016/j.cor.2010.01.011},
abstract = {The assortment planning problem involves choosing an optimal product line, as defined by a set of products with specific attributes, to offer consumers. Under a locational choice model in which products are differentiated both horizontally (by variety attributes) and vertically (by quality attributes), an optimal assortment, whose attributes have only been partially characterized, may consist of multiple quality levels. Using previous analytical results, we approximate the optimal assortment for make-to-order and static substitution environments. We test the appropriateness and compare the performance of three metaheuristic methods. These metaheuristics can easily be modified to accommodate different consumer preference distribution assumptions.},
journal = {Comput. Oper. Res.},
month = oct,
pages = {1797–1804},
numpages = {8},
keywords = {Assortment planning, Metaheuristics}
}

@article{10.1016/j.specom.2019.10.006,
author = {Michelsanti, Daniel and Tan, Zheng-Hua and Sigurdsson, Sigurdur and Jensen, Jesper},
title = {Deep-learning-based audio-visual speech enhancement in presence of Lombard effect},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.006},
doi = {10.1016/j.specom.2019.10.006},
journal = {Speech Commun.},
month = dec,
pages = {38–50},
numpages = {13},
keywords = {Lombard effect, Audio-visual speech enhancement, Deep learning, Speech quality, Speech intelligibility}
}

@inproceedings{10.1109/ASE.2019.00041,
author = {Zheng, Wujie and Lu, Haochuan and Zhou, Yangfan and Liang, Jianming and Zheng, Haibing and Deng, Yuetang},
title = {iFeedback: exploiting user feedback for real-time issue detection in large-scale online service systems},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00041},
doi = {10.1109/ASE.2019.00041},
abstract = {Large-scale online systems are complex, fast-evolving, and hardly bug-free despite the testing efforts. Backend system monitoring cannot detect many types of issues, such as UI related bugs, bugs with small impact on backend system indicators, or errors from third-party co-operating systems, etc. However, users are good informers of such issues: They will provide their feedback for any types of issues. This experience paper discusses our design of iFeedback, a tool to perform real-time issue detection based on user feedback texts. Unlike traditional approaches that analyze user feedback with computation-intensive natural language processing algorithms, iFeedback is focusing on fast issue detection, which can serve as a system life-condition monitor. In particular, iFeedback extracts word combination-based indicators from feedback texts. This allows iFeedback to perform fast system anomaly detection with sophisticated machine learning algorithms. iFeedback then further summarizes the texts with an aim to effectively present the anomaly to the developers for root cause analysis. We present our representative experiences in successfully applying iFeedback in tens of large-scale production online service systems in ten months.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {352–363},
numpages = {12},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1016/j.engappai.2009.03.001,
author = {Borangiu, Theodor and Gilbert, Pascal and Ivanescu, Nick-Andrei and Rosu, Andrei},
title = {An implementing framework for holonic manufacturing control with multiple robot-vision stations},
year = {2009},
issue_date = {June, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {22},
number = {4–5},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.03.001},
doi = {10.1016/j.engappai.2009.03.001},
abstract = {The paper describes a holonic control architecture and implementing issues for agile job shop assembly with networked intelligent robots, based on the dynamic simulation of material processing and transportation. The holarchy was defined considering the PROSA reference architecture relative to which in-line vision-based quality control was added by help of feature-based descriptions of the material flow. Two solutions for production planning are proposed: a knowledge-based algorithm using production rules, and an OO resolved scheduling rate planner (RSRP) based on variable-timing simulation. Failure- and recovery-management are developed as generic scenarios embedding the CNP mechanism into production self-rescheduling. Aggregate Order Holon execution is realized by OPC-based PLC software integration and event-driven product transportation. The holonic control of multiple networked robot-vision stations also features tolerance to station computer- (IBM PC-type), station controller- (robot controller), quality control- (machine vision) and communication- (LAN) failure. Fault tolerance and high availability at shop-floor level are provided due to the multiple physical communication capabilities of the robot controllers, to their multiple-axis multitasking operating capability, and to hardware redundancy of single points of failure (SPOF). Implementing solutions and experiments are reported for a 6-station robot-vision assembly cell with twin-track closed-loop pallet transportation system and product-racking RD/WR devices. Future developments will consider manufacturing integration at enterprise level.},
journal = {Eng. Appl. Artif. Intell.},
month = jun,
pages = {505–521},
numpages = {17},
keywords = {Applied AI, Holonic manufacturing, Real-time vision, Robotics, Semi-heterarchical control}
}

@inproceedings{10.1007/11431855_34,
author = {Benavides, David and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio},
title = {Automated reasoning on feature models},
year = {2005},
isbn = {3540260951},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11431855_34},
doi = {10.1007/11431855_34},
abstract = {Software Product Line (SPL) Engineering has proved to be an effective method for software production. However, in the SPL community it is well recognized that variability in SPLs is increasing by the thousands. Hence, an automatic support is needed to deal with variability in SPL. Most of the current proposals for automatic reasoning on SPL are not devised to cope with extra–functional features. In this paper we introduce a proposal to model and reason on an SPL using constraint programming. We take into account functional and extra–functional features, improve current proposals and present a running, yet feasible implementation.},
booktitle = {Proceedings of the 17th International Conference on Advanced Information Systems Engineering},
pages = {491–503},
numpages = {13},
location = {Porto, Portugal},
series = {CAiSE'05}
}

@article{10.1145/3322122,
author = {Gong, Chen and Yang, Jian and Tao, Dacheng},
title = {Multi-Modal Curriculum Learning over Graphs},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3322122},
doi = {10.1145/3322122},
abstract = {Curriculum Learning (CL) is a recently proposed learning paradigm that aims to achieve satisfactory performance by properly organizing the learning sequence from simple curriculum examples to more difficult ones. Up to now, few works have been done to explore CL for the data with graph structure. Therefore, this article proposes a novel CL algorithm that can be utilized to guide the Label Propagation (LP) over graphs, of which the target is to “learn” the labels of unlabeled examples on the graphs. Specifically, we assume that different unlabeled examples have different levels of difficulty for propagation, and their label learning should follow a simple-to-difficult sequence with the updated curricula. Furthermore, considering that the practical data are often characterized by multiple modalities, every modality in our method is associated with a “teacher” that not only evaluates the difficulties of examples from its own viewpoint, but also cooperates with other teachers to generate the overall simplest curriculum examples for propagation. By taking the curriculums suggested by the teachers as a whole, the common preference (i.e., commonality) of teachers on selecting the simplest examples can be discovered by a row-sparse matrix, and their distinct opinions (i.e., individuality) are captured by a sparse noise matrix. As a result, an accurate curriculum sequence can be established and the propagation quality can thus be improved. Theoretically, we prove that the propagation risk bound is closely related to the examples’ difficulty information, and empirically, we show that our method can generate higher accuracy than the state-of-the-art CL approach and LP algorithms on various multi-modal tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {35},
numpages = {25},
keywords = {Curriculum learning, label propagation, multi-modal learning, semi-supervised learning}
}

@article{10.1016/j.specom.2021.05.009,
author = {Avila, Anderson R. and O’Shaughnessy, Douglas and Falk, Tiago H.},
title = {Automatic speaker verification from affective speech using Gaussian mixture model based estimation of neutral speech characteristics},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2021.05.009},
doi = {10.1016/j.specom.2021.05.009},
journal = {Speech Commun.},
month = sep,
pages = {21–31},
numpages = {11},
keywords = {Speaker verification, Affective speech, Intra-speaker variability, Transfer learning}
}

@article{10.1016/j.infsof.2019.06.002,
author = {Varghese R, Bright Gee and Raimond, Kumudha and Lovesum, Jeno},
title = {A novel approach for automatic remodularization of software systems using extended ant colony optimization algorithm},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.06.002},
doi = {10.1016/j.infsof.2019.06.002},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {107–120},
numpages = {14},
keywords = {Remodularization, Ant colony optimization, Turbo modularization quality, Software system, Code dependency}
}

@article{10.1016/j.neucom.2017.01.093,
author = {Tareef, Afaf and Song, Yang and Huang, Heng and Wang, Yue and Feng, Dagan and Chen, Mei and Cai, Weidong},
title = {Optimizing the cervix cytological examination based on deep learning and dynamic shape modeling},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {248},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.01.093},
doi = {10.1016/j.neucom.2017.01.093},
abstract = {The task of segmenting nuclei and cytoplasm in Papanicolau smear images is one of the most challenging tasks in automated cervix cytological analysis owing to the high degree of overlapping, the multiform shape of the cells and their complex structures resulting from inconsistent staining, poor contrast, and the presence of inflammatory cells. This article presents a robust variational segmentation framework based on superpixelwise convolutional neutral network and a learned shape prior enabling an accurate analysis of overlapping cervical mass. The cellular components of Pap image are first classified by automatic feature learning and classification model. Then, a learning shape prior model is employed to delineate the actual contour of each individual cytoplasm inside the overlapping mass. The shape prior is dynamically modeled during the segmentation process as a weighted linear combination of shape templates from an over-complete shape dictionary under sparsity constraints. We provide quantitative and qualitative assessment of the proposed method using two databases of 153 cervical cytology images, with 870 cells in total, synthesised by accumulating real isolated cervical cells to generate overlapping cellular masses with a varying number of cells and degree of overlap. The experimental results have demonstrated that our methodology can successfully segment nuclei and cytoplasm from highly overlapping mass. Our segmentation is also competitive when compared to the state-of-the-art methods.},
journal = {Neurocomput.},
month = jul,
pages = {28–40},
numpages = {13},
keywords = {Convolutional neural network, Feature learning, Level set evolution, Overlapping cell segmentation, Sparse approximation}
}

@inproceedings{10.1145/2070821.2070824,
author = {Menzies, Tim and Bird, Christian and Zimmermann, Thomas and Schulte, Wolfram and Kocaganeli, Ekrem},
title = {The inductive software engineering manifesto: principles for industrial data mining},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070824},
doi = {10.1145/2070821.2070824},
abstract = {The practices of industrial and academic data mining are very different. These differences have significant implications for (a) how we manage industrial data mining projects; (b) the direction of academic studies in data mining; and (c) training programs for engineers who seek to use data miners in an industrial setting.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {19–26},
numpages = {8},
keywords = {inductive engineering, industry},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@article{10.1007/s10009-019-00544-0,
author = {Giantamidis, Georgios and Tripakis, Stavros and Basagiannis, Stylianos},
title = {Learning Moore machines from input–output traces},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {1},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-019-00544-0},
doi = {10.1007/s10009-019-00544-0},
abstract = {The problem of learning automata from example traces (but no equivalence or membership queries) is fundamental in automata learning theory and practice. In this paper, we study this problem for finite-state machines with inputs and outputs, and in particular for Moore machines. We develop three algorithms for solving this problem: (1) the PTAP algorithm, which transforms a set of input–output traces into an incomplete Moore machine and then completes the machine with self-loops; (2) the PRPNI algorithm, which uses the well-known RPNI algorithm for automata learning to learn a product of automata encoding a Moore machine; and (3) the MooreMI algorithm, which directly learns a Moore machine using PTAP extended with state merging. We prove that MooreMI has the fundamental identification in the limit property. We compare the algorithms experimentally in terms of the size of the learned machine and several notions of accuracy, introduced in this paper. We also carry out a performance comparison against two existing tools (LearnLib and flexfringe). Finally, we compare with OSTIA, an algorithm that learns a more general class of transducers and find that OSTIA generally does not learn a Moore machine, even when fed with a characteristic sample.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = feb,
pages = {1–29},
numpages = {29},
keywords = {Finite state machine, Moore machine, Mealy machine, Automata learning, Passive learning, Characteristic sample}
}

@article{10.1016/j.neucom.2016.09.070,
author = {Tareef, Afaf and Song, Yang and Cai, Weidong and Huang, Heng and Chang, Hang and Wang, Yue and Fulham, Michael and Feng, Dagan and Chen, Mei},
title = {Automatic segmentation of overlapping cervical smear cells based on local distinctive features and guided shape deformation},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {221},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.09.070},
doi = {10.1016/j.neucom.2016.09.070},
abstract = {Automated segmentation of cells from cervical smears poses great challenge to biomedical image analysis because of the noisy and complex background, poor cytoplasmic contrast and the presence of fuzzy and overlapping cells. In this paper, we propose an automated segmentation method for the nucleus and cytoplasm in a cluster of cervical cells based on distinctive local features and guided sparse shape deformation. Our proposed approach is performed in two stages: segmentation of nuclei and cellular clusters, and segmentation of overlapping cytoplasm. In the first stage, a set of local discriminative shape and appearance cues of image superpixels is incorporated and classified by the Support Vector Machine (SVM) to segment the image into nuclei, cellular clusters, and background. In the second stage, a robust shape deformation framework is proposed, based on Sparse Coding (SC) theory and guided by representative shape features, to construct the cytoplasmic shape of each overlapping cell. Then, the obtained shape is refined by the Distance Regularized Level Set Evolution (DRLSE) model. We evaluated our approach using the ISBI 2014 challenge dataset, which has 135 synthetic cell images for a total of 810 cells. Our results show that our approach outperformed existing approaches in segmenting overlapping cells and obtaining accurate nuclear boundaries. HighlightsA fully automated segmentation method is proposed for overlapping cervical cells.Our approach is based on superpixel-based features and guided shape deformation.Our shape initialization procedure is able to work with the different cell types.The practicality of our approach in segmenting highly overlapping cells is proved.Our approach outperformed existing approaches in nuclei and cytoplasm segmentation.},
journal = {Neurocomput.},
month = jan,
pages = {94–107},
numpages = {14},
keywords = {Distance regularized level set, Feature extraction, Overlapping cervical smear cells, Shape deformation, Sparse coding}
}

@article{10.1016/j.neucom.2019.03.062,
author = {Ren, Yazhou and Que, Xiaofan and Yao, Dezhong and Xu, Zenglin},
title = {Self-paced multi-task clustering},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {350},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.03.062},
doi = {10.1016/j.neucom.2019.03.062},
journal = {Neurocomput.},
month = jul,
pages = {212–220},
numpages = {9},
keywords = {Multi-task clustering, Self-paced learning, Non-convexity, Soft weighting}
}

@article{10.1145/3472291,
author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
title = {A Survey of Deep Active Learning},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3472291},
doi = {10.1145/3472291},
abstract = {Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due.It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {180},
numpages = {40},
keywords = {Deep learning, active learning, deep active learning}
}

@article{10.1007/s10462-009-9117-6,
author = {Hristea, Florentina and Popescu, Marius and Dumitrescu, Monica},
title = {Performing word sense disambiguation at the border between unsupervised and knowledge-based techniques},
year = {2008},
issue_date = {December  2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {1–4},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-009-9117-6},
doi = {10.1007/s10462-009-9117-6},
abstract = {This paper aims to fully present a new word sense disambiguation method that has been introduced in Hristea and Popescu (Fundam Inform 91(3---4):547---562, 2009) and so far tested in the case of adjectives (Hristea and Popescu in Fundam Inform 91(3---4):547---562, 2009) and verbs (Hristea in Int Rev Comput Softw 4(1):58---67, 2009). We hereby extend the method to the case of nouns and draw conclusions regarding its performance with respect to all these parts of speech. The method lies at the border between unsupervised and knowledge-based techniques. It performs unsupervised word sense disambiguation based on an underlying Na\"{\i}ve Bayes model, while using WordNet as knowledge source for feature selection. The performance of the method is compared to that of previous approaches that rely on completely different feature sets. Test results for all involved parts of speech show that feature selection using a knowledge source of type WordNet is more effective in disambiguation than local type features (like part-of-speech tags) are.},
journal = {Artif. Intell. Rev.},
month = dec,
pages = {67–86},
numpages = {20},
keywords = {Bayesian classification, Knowledge-based disambiguation, The EM algorithm, Unsupervised disambiguation, Word sense disambiguation, WordNet}
}

@article{10.5555/1756006.1859931,
author = {Mazumder, Rahul and Hastie, Trevor and Tibshirani, Robert},
title = {Spectral Regularization Algorithms for Learning Large Incomplete Matrices},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {We use convex relaxation techniques to provide a sequence of regularized low-rank solutions for large-scale matrix completion problems. Using the nuclear norm as a regularizer, we provide a simple and very efficient convex algorithm for minimizing the reconstruction error subject to a bound on the nuclear norm. Our algorithm SOFT-IMPUTE iteratively replaces the missing elements with those obtained from a soft-thresholded SVD. With warm starts this allows us to efficiently compute an entire regularization path of solutions on a grid of values of the regularization parameter. The computationally intensive part of our algorithm is in computing a low-rank SVD of a dense matrix. Exploiting the problem structure, we show that the task can be performed with a complexity of order linear in the matrix dimensions. Our semidefinite-programming algorithm is readily scalable to large matrices; for example SOFT-IMPUTE takes a few hours to compute low-rank approximations of a 106 X 106 incomplete matrix with 107 observed entries, and fits a rank-95 approximation to the full Netflix training set in 3.3 hours. Our methods achieve good training and test errors and exhibit superior timings when compared to other competitive state-of-the-art techniques.},
journal = {J. Mach. Learn. Res.},
month = aug,
pages = {2287–2322},
numpages = {36}
}

@inproceedings{10.5555/3367243.3367400,
author = {Ie, Eugene and Jain, Vihan and Wang, Jing and Narvekar, Sanmit and Agarwal, Ritesh and Wu, Rui and Cheng, Heng-Tze and Chandra, Tushar and Boutilier, Craig},
title = {SLATEQ: a tractable decomposition for reinforcement learning with recommendation sets},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Reinforcement learning (RL) methods for recommender systems optimize recommendations for long-term user engagement. However, since users are often presented with slates of multiple items--which may have interacting effects on user choice--methods are required to deal with the combinatorics of the RL action space. We develop SLATEQ, a decomposition of value-based temporal-difference and Q-learning that renders RL tractable with slates. Under mild assumptions on user choice behavior, we show that the long-term value (LTV) of a slate can be decomposed into a tractable function of its component item-wise LTVs. We demonstrate our methods in simulation, and validate the scalability and effectiveness of decomposed TD-learning on YouTube.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {2592–2599},
numpages = {8},
location = {Macao, China},
series = {IJCAI'19}
}

@article{10.1016/j.patrec.2019.03.021,
author = {Ding, Songtao and Qu, Shiru and Xi, Yuling and Sangaiah, Arun Kumar and Wan, Shaohua},
title = {Image caption generation with high-level image features},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {123},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2019.03.021},
doi = {10.1016/j.patrec.2019.03.021},
journal = {Pattern Recogn. Lett.},
month = may,
pages = {89–95},
numpages = {7},
keywords = {Image captioning, Language model, Bottom-up attention mechanism, Faster R-CNN}
}

@inproceedings{10.1145/3341161.3345642,
author = {Hasson, Sharon Grubner and Piorkowski, John and McCulloh, Ian},
title = {Social media as a main source of customer feedback: alternative to customer satisfaction surveys},
year = {2020},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3345642},
doi = {10.1145/3341161.3345642},
abstract = {Customer satisfaction surveys, which have been the most common way of gauging customer feedback, involve high costs, require customer active participation, and typically involve low response rates. The tremendous growth of social media platforms such as Twitter provides businesses an opportunity to continuously gather and analyze customer feedback, with the goal of identifying and rectifying issues. This paper examines the alternative of replacing traditional customer satisfaction surveys with social media data. To evaluate this approach the following steps were taken, using customer feedback data extracted from Twitter: 1) Applying sentiment to each Tweet to compare the overall sentiment across different products and/or services. 2) Constructing a hashtag cooccurrence network to further optimize the customer feedback query process from Twitter. 3) Comparing customer feedback from survey responses with social media feedback, while considering content and added value. We find that social media provides advantages over traditional surveys.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {829–832},
numpages = {4},
keywords = {Twitter, classifier, customers, feedback, machine learning, social media, survey},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@article{10.1016/j.infsof.2021.106620,
author = {Tran, Huynh Khanh Vi and Unterkalmsteiner, Michael and B\"{o}rstler, J\"{u}rgen and Ali, Nauman bin},
title = {Assessing test artifact quality—A tertiary study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106620},
doi = {10.1016/j.infsof.2021.106620},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {22},
keywords = {Software testing, Test case quality, Test suite quality, Test artifact quality, Quality assurance}
}

@inproceedings{10.5555/3304889.3305098,
author = {Zhang, Xuchao and Zhao, Liang and Chen, Zhiqian and Lu, Chang-Tien},
title = {Distributed self-paced learning in alternating direction method of multipliers},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Self-paced learning (SPL) mimics the cognitive process of humans, who generally learn from easy samples to hard ones. One key issue in SPL is the training process required for each instance weight depends on the other samples and thus cannot easily be run in a distributed manner in a large-scale dataset. In this paper, we reformulate the self-paced learning problem into a distributed setting and propose a novel Distributed Self-Paced Learning method (DSPL) to handle large scale datasets. Specifically, both the model and instance weights can be optimized in parallel for each batch based on a consensus alternating direction method of multipliers. We also prove the convergence of our algorithm under mild conditions. Extensive experiments on both synthetic and real datasets demonstrate that our approach is superior to those of existing methods.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {3148–3154},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@article{10.1016/j.neucom.2019.06.072,
author = {Xu, Wei and Liu, Wei and Chi, Haoyuan and Qiu, Song and Jin, Yu},
title = {Self-paced learning with privileged information},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {362},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.072},
doi = {10.1016/j.neucom.2019.06.072},
journal = {Neurocomput.},
month = oct,
pages = {147–155},
numpages = {9},
keywords = {Curriculum learning, Self-paced learning, Learning with privileged information}
}

@article{10.1016/j.neucom.2014.12.100,
author = {Garcia, Lu\'{\i}s P.F. and Carvalho, Andr\'{e} C.P.L.F. de and Lorena, Ana C.},
title = {Noise detection in the meta-learning level},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {176},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.12.100},
doi = {10.1016/j.neucom.2014.12.100},
abstract = {The presence of noise in real data sets can harm the predictive performance of machine learning algorithms. There are several noise filtering techniques whose goal is to improve the quality of the data in classification tasks. These techniques usually scan the data for noise identification in a preprocessing step. Nonetheless, this is a non-trivial task and some noisy data can remain unidentified, while safe data can also be removed. The bias of each filtering technique influences its performance on a particular data set. Therefore, there is no single technique that can be considered the best for all domains or data distribution and choosing a particular filter is not straightforward. Meta-learning has been largely used in the last years to support the recommendation of the most suitable machine learning algorithm(s) for a new data set. This paper presents a meta-learning recommendation system able to predict the expected performance of noise filters in noisy data identification tasks. For such, a meta-base is created, containing meta-features extracted from several corrupted data sets along with the performance of some noise filters when applied to these data sets. Next, regression models are induced from this meta-base to predict the expected performance of the investigated filters in the identification of noisy data. The experimental results show that meta-learning can provide a good recommendation of the most promising filters to be applied to new classification data sets.},
journal = {Neurocomput.},
month = feb,
pages = {14–25},
numpages = {12},
keywords = {Characterization measures, Complexity measures, Meta-learning, Noise identification}
}

@inproceedings{10.1145/602461.602488,
author = {Kishi, Tomoji and Noda, Natsuko},
title = {Aspect-oriented analysis for architectural design},
year = {2001},
isbn = {1581135084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/602461.602488},
doi = {10.1145/602461.602488},
abstract = {It is expected that software architecture remains steady throughout its evolution. In order to design software architecture to have robustness towards evolution, it is important to capture the commonality and differences among potential software that will be developed on the software architecture, and design the software architecture to accommodate the commonality and differences. We have to capture the commonality and differences not only from functional aspect, but also from aspects relate to quality attributes, such as performance and reliability, because requirements on quality attributes also have strong impact on software architecture. In this paper, we propose aspect-oriented analysis method, in which we can capture commonality and differences between software, considering requirements on quality attributes.},
booktitle = {Proceedings of the 4th International Workshop on Principles of Software Evolution},
pages = {126–129},
numpages = {4},
keywords = {aspect-oriented, product-lines, quality attributes, software architecture, software evolution},
location = {Vienna, Austria},
series = {IWPSE '01}
}

@inproceedings{10.1109/ASE.2015.58,
author = {Angerer, Florian and Grimmer, Andreas and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Configuration-aware change impact analysis},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.58},
doi = {10.1109/ASE.2015.58},
abstract = {Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This paper presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining possibly impacted products when changing source code of a product family. The approach further supports engineers, who are adapting specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanism, it provides more precise results than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. Using an industrial product line we report evaluation results on the benefit and performance of the approach.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {385–395},
numpages = {11},
keywords = {change impact analysis, configuration, maintenance, program analysis},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1007/978-3-030-32248-9_56,
author = {Parvathaneni, Prasanna and Bao, Shunxing and Nath, Vishwesh and Woodward, Neil D. and Claassen, Daniel O. and Cascio, Carissa J. and Zald, David H. and Huo, Yuankai and Landman, Bennett A. and Lyu, Ilwoo},
title = {Cortical Surface Parcellation Using Spherical Convolutional Neural Networks},
year = {2019},
isbn = {978-3-030-32247-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32248-9_56},
doi = {10.1007/978-3-030-32248-9_56},
abstract = {We present cortical surface parcellation using spherical deep convolutional neural networks. Traditional multi-atlas cortical surface parcellation requires inter-subject surface registration using geometric features with slow processing speed on a single subject (2–3&nbsp;h). Moreover, even optimal surface registration does not necessarily produce optimal cortical parcellation as parcel boundaries are not fully matched to the geometric features. In this context, a choice of training features is important for accurate cortical parcellation. To utilize the networks efficiently, we propose cortical parcellation-specific input data from an irregular and complicated structure of cortical surfaces. To this end, we align ground-truth cortical parcel boundaries and use their resulting deformation fields to generate new pairs of deformed geometric features and parcellation maps. To extend the capability of the networks, we then smoothly morph cortical geometric features and parcellation maps using the intermediate deformation fields. We validate our method on 427 adult brains for 49 labels. The experimental results show that our method outperforms traditional multi-atlas and naive spherical U-Net approaches, while achieving full cortical parcellation in less than a minute.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part III},
pages = {501–509},
numpages = {9},
keywords = {Cortical surface parcellation, Spherical deformation, Spherical U-Net, Surface registration},
location = {Shenzhen, China}
}

@inproceedings{10.1109/ICSE.2019.00092,
author = {Lazreg, Sami and Cordy, Maxime and Collet, Philippe and Heymans, Patrick and Mosser, S\'{e}bastien},
title = {Multifaceted automated analyses for variability-intensive embedded systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00092},
doi = {10.1109/ICSE.2019.00092},
abstract = {Embedded systems, like those found in the automotive domain, must comply with stringent functional and non-functional requirements. To fulfil these requirements, engineers are confronted with a plethora of design alternatives both at the software and hardware level, out of which they must select the optimal solution wrt. possibly-antagonistic quality attributes (e.g. cost of manufacturing vs. speed of execution). We propose a model-driven framework to assist engineers in this choice. It captures high-level specifications of the system in the form of variable dataflows and configurable hardware platforms. A mapping algorithm then derives the design space, i.e. the set of compatible pairs of application and platform variants, and a variability-aware executable model, which encodes the functional and non-functional behaviour of all viable system variants. Novel verification algorithms then pinpoint the optimal system variants efficiently. The benefits of our approach are evaluated through a real-world case study from the automotive industry.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {854–865},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.5555/3006652.3006710,
author = {G\^{a}rbacea, Cristina and Tsagkias, Manos and de Rijke, Maarten},
title = {Detecting the reputation polarity of microblog posts},
year = {2014},
isbn = {9781614994183},
publisher = {IOS Press},
address = {NLD},
abstract = {We address the task of detecting the reputation polarity of social media updates, that is, deciding whether the content of an update has positive or negative implications for the reputation of a given entity. Typical approaches to this task include sentiment lexicons and linguistic features. However, they fall short in the social media domain because of its unedited and noisy nature, and, more importantly, because reputation polarity is not only encoded in sentiment-bearing words but it is also embedded in other word usage. To this end, automatic methods for extracting discriminative features for reputation polarity detection can play a role. We propose a data-driven, supervised approach for extracting textual features, which we use to train a reputation polarity classifier. Experiments on the RepLab 2013 collection show that our model outperforms the state-of-the-art method based on sentiment analysis by 20% accuracy.},
booktitle = {Proceedings of the Twenty-First European Conference on Artificial Intelligence},
pages = {339–344},
numpages = {6},
location = {Prague, Czech Republic},
series = {ECAI'14}
}

@inproceedings{10.1007/978-3-030-88081-1_16,
author = {Suriani, Vincenzo and Antonioni, Emanuele and Riccio, Francesco and Nardi, Daniele},
title = {Coordination and Cooperation in Robot Soccer},
year = {2021},
isbn = {978-3-030-88080-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88081-1_16},
doi = {10.1007/978-3-030-88081-1_16},
abstract = {Aiming at improving our physical strength and expanding our knowledge, tournaments and competitions have always contributed to our personal growth. Robotics and AI are no exception, and since beginning, competitions have been exploited to improve our understanding of such research areas (e.g. Chess, VideoGames, DARPA). In fact, the research community has launched (and it is involved) in several robotics competitions that provide a two-fold benefit of (i) promoting novel approaches and (ii) valuate proposed solutions systematically and quantitatively. In this paper, we focus on a particular research area of Robotics and AI: we analyze multi-robot systems deployed in a cooperative-adversarial environment being tasked to collaborate to achieve a common goal, while competing against an opposing team. To this end, RoboCup provide the best benchmarking environment by implementing such a challenging problem in the game of soccer. Sports, in fact, represent extremely complex challenge that require a team of robots to show dexterous and fluid movements and to feature high-level cognitive capabilities. Here, we analyse methodologies and approaches to address the problem of coordination and cooperation and we discuss state-of-the-art solutions that achieve effective decision-making processes for multi-robot adversarial scenarios.},
booktitle = {Computational Collective Intelligence: 13th International Conference, ICCCI 2021, Rhodes, Greece, September 29 – October 1, 2021, Proceedings},
pages = {215–227},
numpages = {13},
keywords = {Strategies in robotic games, Robotic competition, Soccer robots RoboCup SPL},
location = {Rhodos, Greece}
}

@article{10.1016/j.patcog.2018.11.030,
author = {Wang, Xiaohong and Jiang, Xudong and Ren, Jianfeng},
title = {Blood vessel segmentation from fundus image by a cascade classification framework},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.11.030},
doi = {10.1016/j.patcog.2018.11.030},
journal = {Pattern Recogn.},
month = apr,
pages = {331–341},
numpages = {11},
keywords = {Fundus image, Retinal vessel segmentation, Cascade classification, Dimensionality reduction}
}

@article{10.1155/2020/7917021,
author = {Zhang, Cheng and He, Dan and Zhang, Qingchen},
title = {A Deep Multiscale Fusion Method via Low-Rank Sparse Decomposition for Object Saliency Detection Based on Urban Data in Optical Remote Sensing Images},
year = {2020},
issue_date = {2020},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2020},
issn = {1530-8669},
url = {https://doi.org/10.1155/2020/7917021},
doi = {10.1155/2020/7917021},
abstract = {The urban data provides a wealth of information that can support the life and work for people. In this work, we research the object saliency detection in optical remote sensing images, which is conducive to the interpretation of urban scenes. Saliency detection selects the regions with important information in the remote sensing images, which severely imitates the human visual system. It plays a powerful role in other image processing. It has successfully made great achievements in change detection, object tracking, temperature reversal, and other tasks. The traditional method has some disadvantages such as poor robustness and high computational complexity. Therefore, this paper proposes a deep multiscale fusion method via low-rank sparse decomposition for object saliency detection in optical remote sensing images. First, we execute multiscale segmentation for remote sensing images. Then, we calculate the saliency value, and the proposal region is generated. The superpixel blocks of the remaining proposal regions of the segmentation map are input into the convolutional neural network. By extracting the depth feature, the saliency value is calculated and the proposal regions are updated. The feature transformation matrix is obtained based on the gradient descent method, and the high-level semantic prior knowledge is obtained by using the convolutional neural network. The process is iterated continuously to obtain the saliency map at each scale. The low-rank sparse decomposition of the transformed matrix is carried out by robust principal component analysis. Finally, the weight cellular automata method is utilized to fuse the multiscale saliency graphs and the saliency map calculated according to the sparse noise obtained by decomposition. Meanwhile, the object priors knowledge can filter most of the background information, reduce unnecessary depth feature extraction, and meaningfully improve the saliency detection rate. The experiment results show that the proposed method can effectively improve the detection effect compared to other deep learning methods.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {14}
}

@inproceedings{10.1007/978-3-030-27544-0_18,
author = {Matamoros, Mauricio and Harbusch, Karin and Paulus, Dietrich},
title = {From Commands to Goal-Based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home},
year = {2018},
isbn = {978-3-030-27543-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27544-0_18},
doi = {10.1007/978-3-030-27544-0_18},
abstract = {On the one hand, speech is a key aspect to people’s communication. On the other, it is widely acknowledged that language proficiency is related to intelligence. Therefore, intelligent robots should be able to understand, at least, people’s orders within their application domain. These insights are not new in RoboCup@Home, but we lack of a long-term plan to evaluate this approach.In this paper we conduct a brief review of the achievements on automated speech recognition and natural language understanding in RoboCup@Home. Furthermore, we discuss main challenges to tackle in spoken human-robot interaction within the scope of this competition. Finally, we contribute by presenting a pipelined road map to engender research in the area of natural language understanding applied to domestic service robotics.},
booktitle = {RoboCup 2018: Robot World Cup XXII},
pages = {217–229},
numpages = {13},
keywords = {Robotic competitions, Natural language understanding, Artificial intelligence and robotics},
location = {Montr\'{e}al, QC, Canada}
}

@article{10.1016/j.jss.2017.01.031,
author = {Lucas, Edson M. and Oliveira, Toacy C. and Farias, Kleinner and Alencar, Paulo S.C.},
title = {CollabRDL},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.01.031},
doi = {10.1016/j.jss.2017.01.031},
abstract = {Extends the Reuse Description Language (RDL) to address collaborative reuse processesIncludes three new commands in RDL, including ROLE, PARALLEL and DOPARALLELCollabRDL can represent parallelism, synchronization, multiple-choice and roleCollabRDL is capable of representing critical workflow patterns Coordinating software reuse activities is a complex problem when considering collaborative software development. This is mainly motivated due to the difficulty in specifying how the artifacts and the knowledge produced in previous projects can be applied in future ones. In addition, modern software systems are developed in group working in separate geographical locations. Therefore, techniques to enrich collaboration on software development are important to improve quality and reduce costs. Unfortunately, the current literature fails to address this problem by overlooking existing reuse techniques. There are many reuse approaches proposed in academia and industry, including Framework Instantiation, Software Product Line, Transformation Chains, and Staged Configuration. But, the current approaches do not support the representation and implementation of collaborative instantiations that involve individual and group roles, the simultaneous performance of multiple activities, restrictions related to concurrency and synchronization of activities, and allocation of activities to reuse actors as a coordination mechanism. These limitations are the main reasons why the Reuse Description Language (RDL) is unable to promote collaborative reuse, i.e., those related to reuse activities in collaborative software development. To overcome these shortcomings, this work, therefore, proposes CollabRDL, a language to coordinate collaborative reuse by providing essential concepts and constructs for allowing group-based reuse activities. For this purpose, we extend RDL by introducing three new commands, including role, parallel, and doparallel. To evaluate CollabRDL we have conducted a case study in which developer groups performed reuse activities collaboratively to instantiate a mainstream Java framework. The results indicated that CollabRDL was able to represent critical workflow patterns, including parallel split pattern, synchronization pattern, multiple-choice pattern, role-based distribution pattern, and multiple instances with decision at runtime. Overall, we believe that the provision of a new language that supports group-based activities in framework instantiation can help enable software organizations to document their coordinated efforts and achieve the benefits of software mass customization with significantly less development time and effort.},
journal = {J. Syst. Softw.},
month = sep,
pages = {505–527},
numpages = {23},
keywords = {Collaboration, Framework, Language, Reuse process, Software reuse}
}

@article{10.1007/s11263-018-1134-y,
author = {Deng, Jiankang and Roussos, Anastasios and Chrysos, Grigorios and Ververas, Evangelos and Kotsia, Irene and Shen, Jie and Zafeiriou, Stefanos},
title = {The Menpo Benchmark for Multi-pose 2D and 3D Facial Landmark Localisation and Tracking},
year = {2019},
issue_date = {Jun 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {6–7},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-018-1134-y},
doi = {10.1007/s11263-018-1134-y},
abstract = {In this article, we present the Menpo 2D and Menpo 3D benchmarks, two new datasets for multi-pose 2D and 3D facial landmark localisation and tracking. In contrast to the previous benchmarks such as 300W and 300VW, the proposed benchmarks contain facial images in both semi-frontal and profile pose. We introduce an elaborate semi-automatic methodology for providing high-quality annotations for both the Menpo 2D and Menpo 3D benchmarks. In Menpo 2D benchmark, different visible landmark configurations are designed for semi-frontal and profile faces, thus making the 2D face alignment full-pose. In Menpo 3D benchmark, a united landmark configuration is designed for both semi-frontal and profile faces based on the correspondence with a 3D face model, thus making face alignment not only full-pose but also corresponding to the real-world 3D space. Based on the considerable number of annotated images, we organised Menpo 2D Challenge and Menpo 3D Challenge for face alignment under large pose variations in conjunction with CVPR 2017 and ICCV 2017, respectively. The results of these challenges demonstrate that recent deep learning architectures, when trained with the abundant data, lead to excellent results. We also provide a very simple, yet effective solution, named Cascade Multi-view Hourglass Model, to 2D and 3D face alignment. In our method, we take advantage of all 2D and 3D facial landmark annotations in a joint way. We not only capitalise on the correspondences between the semi-frontal and profile 2D facial landmarks but also employ joint supervision from both 2D and 3D facial landmarks. Finally, we discuss future directions on the topic of face alignment.},
journal = {Int. J. Comput. Vision},
month = jun,
pages = {599–624},
numpages = {26},
keywords = {2D face alignment, 3D face alignment, Menpo challenge}
}

@article{10.1017/S0890060400141046,
author = {Darr, Timothy P. and Birmingham, William P.},
title = {Part-selection triptych: A representation, problem properties and problem definition, and problem-solving method},
year = {2000},
issue_date = {January 2000},
publisher = {Cambridge University Press},
address = {USA},
volume = {14},
number = {1},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060400141046},
doi = {10.1017/S0890060400141046},
abstract = {In part-selection problems, parts are selected from catalogs and connected to meet the following problem requirements: functionality, specifications, and constraints. This paper formally defines the part-selection problem, enumerates a set of design properties that are useful during a search for a design solution, and provides an algorithm for solving part-selection problems based on a novel set of operators for manipulating portions of the design space.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = jan,
pages = {39–51},
numpages = {13},
keywords = {Configuration, Constraint-satisfaction, Design, Optimization, Part selection}
}

@article{10.1007/s10489-020-01730-3,
author = {Zhu, Wenjie and Peng, Bo and Wu, Han and Wang, Binhao},
title = {Query set centered sparse projection learning for set based image classification},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01730-3},
doi = {10.1007/s10489-020-01730-3},
abstract = {Set based image classification technology has been developed successfully in recent decades. Previous approaches dispose set based image classification by employing all the gallery sets to learn metrics or construct the model using a typical number of parameters. However, they are based on the assumption that the global structure is consistent with the local structure, which is rigid in real applications. Additionally, the participation of all gallery sets increases the influence of outliers. This paper conducts this task via sparse projection learning by employing ℓ2,1 norm from the perspective of the query set. Instead of involving all the image sets, this work devotes to searching for a local region, which is centered with a query set and constructed by the candidates selected from different classes in the gallery sets. By maximizing the inter-class while minimizing the intra-class of the candidates from the gallery sets from the query set, this work can learn a discriminate and sparse projection for image set feature extraction. In order to learn the projection, an alternative updating algorithm to solve the optimization problem is proposed and the convergence and complexity are analyzed. Finally, the distance is measured in the discriminate low-dimensional space using Euclidean distance between the central data point of the query set and the central one of images from the same class. The proposed approach learns the projection in the local set centered with the query set with ℓ2,1 norm, which contributes to more discriminative feature. Compared with the existing algorithms, the experiments on the challenging databases demonstrate that the proposed simple yet effective approach obtains the best classification accuracy with comparable time cost.},
journal = {Applied Intelligence},
month = oct,
pages = {3400–3411},
numpages = {12},
keywords = {Query set, Sparse projection learning, Set based image classification, Discriminate subspace learning}
}

@inproceedings{10.5555/3540261.3541537,
author = {Peng, Jizong and Wang, Ping and Desrosiers, Christian and Pedersoli, Marco},
title = {Self-paced contrastive learning for semi-supervised medical image segmentation with meta-labels},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The contrastive pre-training of a recognition model on a large dataset of unlabeled data often boosts the model's performance on downstream tasks like image classification. However, in domains such as medical imaging, collecting unlabeled data can be challenging and expensive. In this work, we consider the task of medical image segmentation and adapt contrastive learning with meta-label annotations to scenarios where no additional unlabeled data is available. Meta-labels, such as the location of a 2D slice in a 3D MRI scan, often come for free during the acquisition process. We use these meta-labels to pre-train the image encoder, as well as in a semi-supervised learning step that leverages a reduced set of annotated data. A self-paced learning strategy exploiting the weak annotations is proposed to further help the learning process and discriminate useful labels from noise. Results on five medical image segmentation datasets show that our approach: i) highly boosts the performance of a model trained on a few scans, ii) outperforms previous contrastive and semi-supervised approaches, and iii) reaches close to the performance of a model trained on the full data.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1276},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-642-12439-6_14,
author = {Brock, Derek and McClimens, Brian and Wasylyshyn, Christina and Trafton, J. Gregory and McCurry, Malcolm},
title = {Evaluating the utility of auditory perspective-taking in robot speech presentations},
year = {2009},
isbn = {3642124380},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12439-6_14},
doi = {10.1007/978-3-642-12439-6_14},
abstract = {In speech interactions, people routinely reason about each other's auditory perspective and change their manner of speaking accordingly, by adjusting their voice to overcome noise or distance, or by pausing for especially loud sounds and resuming when conditions are more favorable for the listener. In this paper we report the findings of a listening study motivated both by this observation and a prototype auditory interface for a mobile robot that monitors the aural parameters of its environment and infers its user's listening requirements. The results provide significant empirical evidence of the utility of simulated auditory perspective taking and the inferred use of loudness and/or pauses to overcome the potential of ambient noise to mask synthetic speech.},
booktitle = {Proceedings of the 6th International Conference on Auditory Display},
pages = {266–286},
numpages = {21},
keywords = {adaptive auditory display, auditory interaction, auditory perspective-taking, human-robot interaction, listening performance, synthetic speech},
location = {Copenhagen, Denmark},
series = {CMMR/ICAD'09}
}

@article{10.1007/s10845-017-1332-4,
author = {Du, Gang and Xia, Yi and Jiao, Roger J. and Liu, Xiaojie},
title = {Leader-follower joint optimization problems in product family design},
year = {2019},
issue_date = {Mar 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {3},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-017-1332-4},
doi = {10.1007/s10845-017-1332-4},
abstract = {Product family design (PFD) has been traditionally tackled as a single-level multi-objective optimization problem. This paper reveals a complex type of leader-follower joint optimization (LFJO) problems that are widely observed for PFD. Leader-follower decision making is inherent in product family optimization that involves multiple decision makers and encompasses different levels of decision hierarchy, in which many conflicting goals compete to arrive at equilibrium solutions. It is important for PFD to explicitly model such leader-follower decisions in line with a Stackelberg game. Consistent with multiple decision makers across different stages of the PFD process and multiple levels of the PFD decision hierarchy, this paper classifies the leader-follower decisions of PFD using a quartet grid, which serves as a reference model for conceptualization of diverse types of LFJO problems associated with PFD. Coinciding with the bilevel decision structure of game theoretic optimization, each LFJO problem formulation defined from the quartet grid can be quantitatively mapped to a bilevel programming mathematical model to be solved effectively by nested genetic algorithms. A case study of gear reducer PFD is presented to demonstrate the rational and potential of the LFJO quartet grid for dealing with game-theoretic optimization problems underpinning PFD decisions.},
journal = {J. Intell. Manuf.},
month = mar,
pages = {1387–1405},
numpages = {19},
keywords = {Bilevel programming, Game theoretic decision making, Leader-follower joint optimization, Product family design, Quartet grid}
}

@inproceedings{10.1007/978-3-030-58607-2_18,
author = {Qi, Yuankai and Pan, Zizheng and Zhang, Shengping and van den Hengel, Anton and Wu, Qi},
title = {Object-and-Action Aware Model for Visual Language Navigation},
year = {2020},
isbn = {978-3-030-58606-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58607-2_18},
doi = {10.1007/978-3-030-58607-2_18},
abstract = {Vision-and-Language Navigation (VLN) is unique in that it requires turning relatively general natural-language instructions into robot agent actions, on the basis of visible environments. This requires to extract value from two very different types of natural-language information. The first is object description (e.g., ‘table’, ‘door’), each presenting as a tip for the agent to determine the next action by finding the item visible in the environment, and the second is action specification (e.g., ‘go straight’, ‘turn left’) which allows the robot to directly predict the next movements without relying on visual perceptions. However, most existing methods pay few attention to distinguish these information from each other during instruction encoding and mix together the matching between textual object/action encoding and visual perception/orientation features of candidate viewpoints. In this paper, we propose an Object-and-Action Aware Model (OAAM) that processes these two different forms of natural language based instruction separately. This enables each process to match object-centered/action-centered instruction to their own counterpart visual perception/action orientation flexibly. However, one side-issue caused by above solution is that an object mentioned in instructions may be observed in the direction of two or more candidate viewpoints, thus the OAAM may not predict the viewpoint on the shortest path as the next action. To handle this problem, we design a simple but effective path loss to penalize trajectories deviating from the ground truth path. Experimental results demonstrate the effectiveness of the proposed model and path loss, and the superiority of their combination with a 50% SPL score on the R2R dataset and a 40% CLS score on the R4R dataset in unseen environments, outperforming the previous state-of-the-art.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X},
pages = {303–317},
numpages = {15},
keywords = {Vision-and-Language Navigation, Modular network, Reward shaping},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/3540261.3542579,
author = {Ostapenko, Oleksiy and Rodr\'{\i}guez, Pau and Caccia, Massimo and Charlin, Laurent},
title = {Continual learning via local module composition},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modularity is a compelling solution to continual learning (CL), the problem of modeling sequences of related tasks. Learning and then composing modules to solve different tasks provides an abstraction to address the principal challenges of CL including catastrophic forgetting, backward and forward transfer across tasks, and sub-linear model growth. We introduce local module composition (LMC), an approach to modular CL where each module is provided a local structural component that estimates a module's relevance to the input. Dynamic module composition is performed layer-wise based on local relevance scores. We demonstrate that agnosticity to task identities (IDs) arises from (local) structural learning that is module-specific as opposed to the task- and/or model-specific as in previous works, making LMC applicable to more CL settings compared to previous works. In addition, LMC also tracks statistics about the input distribution and adds new modules when outlier samples are detected. In the first set of experiments, LMC performs favorably compared to existing methods on the recent Continual Transfer-learning Benchmark without requiring task identities. In another study, we show that the locality of structural learning allows LMC to interpolate to related but unseen tasks (OOD), as well as to compose modular networks trained independently on different task sequences into a third modular network without any fine-tuning. Finally, in search for limitations of LMC we study it on more challenging sequences of 30 and 100 tasks, demonstrating that local module selection becomes much more challenging in presence of a large number of candidate modules. In this setting best performing LMC spawns much fewer modules compared to an oracle based baseline, however it reaches a lower overall accuracy.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2318},
numpages = {15},
series = {NIPS '21}
}

@article{10.1007/s10639-020-10291-4,
author = {Campo, Marcelo and Amandi, Analia and Biset, Julio Cesar},
title = {A software architecture perspective about Moodle flexibility for supporting empirical research of teaching theories},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1360-2357},
url = {https://doi.org/10.1007/s10639-020-10291-4},
doi = {10.1007/s10639-020-10291-4},
abstract = {Moodle represents a great contribution to the educational world since it provides an evolving platform for Virtual Learning Management Systems (VLMS) that became a standard de facto for most of the educational institutions around the world. Through the pedagogical functions provided, it collects in the many globally spread out databases a huge amount of information regarding the activities that teachers and students perform during the learning process. This reality makes Moodle a natural choice for conducting experimental research by Artificial Intelligence researchers interested in theories for improving learning and teaching; particularly those related with the controversial learning styles concept. Roughly defined, a learning style intends to be a model of the way and media an apprentice acquires knowledge and hence the way a teacher should present that knowledge to the apprentice matching his/her learning style. Independently of the many controversies (be these scientific, psychological or even ethical) about the soundness and real outcomes that such ideas can bring to improve learning, it’s a worthy intriguing research area for many researchers pursuing the ideal automated teacher: the teachbot dream. Behind this goal we have developed Middle, a Moodle plug-in able to infer the learning style of each student taking a course using an advanced version of a Bayesian network model that we previously tested. Middle intends support personalized teaching based on the Felder-Silverman’s ILS model and has been evaluated through controlled experiments and pilot test in high schools and university courses. Such experiments showed promising results that shed some light on learning styles modeling and its potential outcomes. During the experience we found strong limitations in the Moodle design regarding its supposed flexibility to incorporate new functionalities. From a strict software architecture point of view, we found that such flexibility is far from being enough to easier the implementation of the dynamic computational behavior required to support a teachbot. This made our effort much harder than expected, perhaps because of the illusion induced by the widespread use of Moodle. In this article we present our results and experiences extending Moddle with intelligent behavior from a software architecture point of view, focusing on the lessons learnt in such extension. Our experience indicates that this simplicity is far from being so and hence it is worth to share the limitations and how we overcome them.},
journal = {Education and Information Technologies},
month = jan,
pages = {817–842},
numpages = {26},
keywords = {Learning styles, Bayesian networks, Moodle, Virtual learning environments}
}

@inproceedings{10.5555/3540261.3540835,
author = {Yao, Huaxiu and Wang, Yu and Wei, Ying and Zhao, Peilin and Mahdavi, Mehrdad and Lian, Defu and Finn, Chelsea},
title = {Meta-learning with an adaptive task scheduler},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {To benefit the learning of a new task, meta-learning has been proposed to transfer a well-generalized meta-model learned from various meta-training tasks. Existing meta-learning algorithms randomly sample meta-training tasks with a uniform probability, under the assumption that tasks are of equal importance. However, it is likely that tasks are detrimental with noise or imbalanced given a limited number of meta-training tasks. To prevent the meta-model from being corrupted by such detrimental tasks or dominated by tasks in the majority, in this paper, we propose an adaptive task scheduler (ATS) for the meta-training process. In ATS, for the first time, we design a neural scheduler to decide which meta-training tasks to use next by predicting the probability being sampled for each candidate task, and train the scheduler to optimize the generalization capacity of the meta-model to unseen tasks. We identify two meta-model-related factors as the input of the neural scheduler, which characterize the difficulty of a candidate task to the meta-model. Theoretically, we show that a scheduler taking the two factors into account improves the meta-training loss and also the optimization landscape. Under the setting of meta-learning with noise and limited budgets, ATS improves the performance on both miniImageNet and a real-world drug discovery benchmark by up to 13% and 18%, respectively, compared to state-of-the-art task schedulers.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {574},
numpages = {13},
series = {NIPS '21}
}

@article{10.1016/j.robot.2009.03.006,
author = {Cherubini, A. and Giannone, F. and Iocchi, L. and Lombardo, M. and Oriolo, G.},
title = {Policy gradient learning for a humanoid soccer robot},
year = {2009},
issue_date = {July, 2009},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {57},
number = {8},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2009.03.006},
doi = {10.1016/j.robot.2009.03.006},
abstract = {In humanoid robotic soccer, many factors, both at low-level (e.g., vision and motion control) and at high-level (e.g., behaviors and game strategies), determine the quality of the robot performance. In particular, the speed of individual robots, the precision of the trajectory, and the stability of the walking gaits, have a high impact on the success of a team. Consequently, humanoid soccer robots require fine tuning, especially for the basic behaviors. In recent years, machine learning techniques have been used to find optimal parameter sets for various humanoid robot behaviors. However, a drawback of learning techniques is time consumption: a practical learning method for robotic applications must be effective with a small amount of data. In this article, we compare two learning methods for humanoid walking gaits based on the Policy Gradient algorithm. We demonstrate that an extension of the classic Policy Gradient algorithm that takes into account parameter relevance allows for better solutions when only a few experiments are available. The results of our experimental work show the effectiveness of the policy gradient learning method, as well as its higher convergence rate, when the relevance of parameters is taken into account during learning.},
journal = {Robot. Auton. Syst.},
month = jul,
pages = {808–818},
numpages = {11},
keywords = {Humanoid robotics, Machine learning, Motion control}
}

@article{10.1007/s10796-014-9528-z,
author = {Khan, Yasser A. and El-Attar, Mohamed},
title = {Using model transformation to refactor use case models based on antipatterns},
year = {2016},
issue_date = {February  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {1},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-014-9528-z},
doi = {10.1007/s10796-014-9528-z},
abstract = {Use Case modeling is a popular technique for documenting functional requirements of software systems. Refactoring is the process of enhancing the structure of a software artifact without changing its intended behavior. Refactoring, which was first introduced for source code, has been extended for use case models. Antipatterns are low quality solutions to commonly occurring design problems. The presence of antipatterns in a use case model is likely to propagate defects to other software artifacts. Therefore, detection and refactoring of antipatterns in use case models is crucial for ensuring the overall quality of a software system. Model transformation can greatly ease several software development activities including model refactoring. In this paper, a model transformation approach is proposed for improving the quality of use case models. Model transformations which can detect antipattern instances in a given use case model, and refactor them appropriately are defined and implemented. The practicability of the approach is demonstrated by applying it on a case study that pertains to biodiversity database system. The results show that model transformations can efficiently improve quality of use case models by saving time and effort.},
journal = {Information Systems Frontiers},
month = feb,
pages = {171–204},
numpages = {34},
keywords = {Antipatterns, Model transformation, Refactoring, UML, Use case modeling quality attributes, Use cases}
}

@inproceedings{10.1145/3477244.3477985,
author = {van der Sanden, Bram and Li, Yonghui and van den Aker, Joris and Akesson, Benny and Bijlsma, Tjerk and Hendriks, Martijn and Triantafyllidis, Kostas and Verriet, Jacques and Voeten, Jeroen and Basten, Twan},
title = {Model-driven system-performance engineering for cyber-physical systems},
year = {2021},
isbn = {9781450387125},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477244.3477985},
doi = {10.1145/3477244.3477985},
abstract = {System-Performance Engineering (SysPE) encompasses modeling formalisms, methods, techniques, and industrial practices to design systems for performance, where performance is taken integrally into account during the whole system life cycle. Industrial SysPE state of practice is generally model-based. Due to the rapidly increasing complexity of systems, there is a need to develop and establish model-driven methods and techniques. To structure the field of SysPE, we identify (1) industrial challenges motivating the importance of SysPE, (2) scientific challenges that need to be addressed to establish model-driven SysPE, (3) important focus areas for SysPE and (4) best practices. We conducted a survey to collect feedback on our views. The responses were used to update and validate the identified challenges, focus areas, and best practices. The final result is presented in this paper. Interesting observations are that industry sees a need for better design-space exploration support, more than for additional performance modeling and analysis techniques. Also tools and integral methods for SysPE need attention. From the identified focus areas, scheduling and supervisory control is seen as lacking established best practices.},
booktitle = {Proceedings of the 2021 International Conference on Embedded Software},
pages = {11–22},
numpages = {12},
keywords = {CPS, model-driven design, system-performance engineering},
location = {Virtual Event},
series = {EMSOFT '21}
}

@article{10.1145/2000791.2000794,
author = {Anvik, John and Murphy, Gail C.},
title = {Reducing the effort of bug report triage: Recommenders for development-oriented decisions},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000791.2000794},
doi = {10.1145/2000791.2000794},
abstract = {A key collaborative hub for many software development projects is the bug report repository. Although its use can improve the software development process in a number of ways, reports added to the repository need to be triaged. A triager determines if a report is meaningful. Meaningful reports are then organized for integration into the project's development process.To assist triagers with their work, this article presents a machine learning approach to create recommenders that assist with a variety of decisions aimed at streamlining the development process. The recommenders created with this approach are accurate; for instance, recommenders for which developer to assign a report that we have created using this approach have a precision between 70% and 98% over five open source projects. As the configuration of a recommender for a particular project can require substantial effort and be time consuming, we also present an approach to assist the configuration of such recommenders that significantly lowers the cost of putting a recommender in place for a project. We show that recommenders for which developer should fix a bug can be quickly configured with this approach and that the configured recommenders are within 15% precision of hand-tuned developer recommenders.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {10},
numpages = {35},
keywords = {Bug report triage, configuration assistance, machine learning, recommendation, task assignment}
}

@inproceedings{10.1145/2642937.2642939,
author = {Segura, Sergio and S\'{a}nchez, Ana B. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated variability analysis and testing of an E-commerce site.: an experience report},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642939},
doi = {10.1145/2642937.2642939},
abstract = {In this paper, we report on our experience on the development of La Hilandera, an e-commerce site selling haberdashery products and craft supplies in Europe. The store has a huge input space where customers can place almost three millions of different orders which made testing an extremely difficult task. To address the challenge, we explored the applicability of some of the practices for variability management in software product lines. First, we used a feature model to represent the store input space which provided us with a variability view easy to understand, share and discuss with all the stakeholders. Second, we used techniques for the automated analysis of feature models for the detection and repair of inconsistent and missing configuration settings. Finally, we used test selection and prioritization techniques for the generation of a manageable and effective set of test cases. Our findings, summarized in a set of lessons learnt, suggest that variability techniques could successfully address many of the challenges found when developing e-commerce sites.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {139–150},
numpages = {12},
keywords = {automated testing, e-commerce, experience report, feature modelling, variability},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1016/j.neucom.2019.10.024,
author = {Zhang, Ping and Liu, Jingwen and Wang, Xiaoyang and Pu, Tian and Fei, Chun and Guo, Zhengkui},
title = {Stereoscopic video saliency detection based on spatiotemporal correlation and depth confidence optimization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {377},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.10.024},
doi = {10.1016/j.neucom.2019.10.024},
journal = {Neurocomput.},
month = feb,
pages = {256–268},
numpages = {13},
keywords = {Stereoscopic video, Saliency detection, Spatiotemporal correlation, Cepth confidence optimization}
}

@inproceedings{10.1109/ASE.2011.6100096,
author = {Oster, Zachary J. and Santhanam, Ganesh Ram and Basu, Samik},
title = {Automating analysis of qualitative preferences in goal-oriented requirements engineering},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100096},
doi = {10.1109/ASE.2011.6100096},
abstract = {In goal-oriented requirements engineering, a goal model graphically represents relationships between the required goals (functional requirements), tasks (realizations of goals), and optional goals (non-functional properties) involved in designing a system. It may, however, be impossible to find a design that fulfills all required goals and all optional goals. In such cases, it is useful to find designs that provide the required functionality while satisfying the most preferred set of optional goals under the goal model's constraints. We present an approach that considers expressive qualitative preferences over optional goals, as these can model interacting and/or mutually exclusive subgoals. Our framework employs a model checking-based method for reasoning with qualitative preferences to identify the most preferred alternative(s). We evaluate our approach using existing goal models from the literature.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {448–451},
numpages = {4},
series = {ASE '11}
}

@inproceedings{10.1145/3382494.3410677,
author = {Shu, Yangyang and Sui, Yulei and Zhang, Hongyu and Xu, Guandong},
title = {Perf-AL: Performance Prediction for Configurable Software through Adversarial Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410677},
doi = {10.1145/3382494.3410677},
abstract = {Context: Many software systems are highly configurable. Different configuration options could lead to varying performances of the system. It is difficult to measure system performance in the presence of an exponential number of possible combinations of these options.Goal: Predicting software performance by using a small configuration sample.Method: This paper proposes Perf-AL to address this problem via adversarial learning. Specifically, we use a generative network combined with several different regularization techniques (L1 regularization, L2 regularization and a dropout technique) to output predicted values as close to the ground truth labels as possible. With the use of adversarial learning, our network identifies and distinguishes the predicted values of the generator network from the ground truth value distribution. The generator and the discriminator compete with each other by refining the prediction model iteratively until its predicted values converge towards the ground truth distribution.Results: We argue that (i) the proposed method can achieve the same level of prediction accuracy, but with a smaller number of training samples. (ii) Our proposed model using seven real-world datasets show that our approach outperforms the state-of-the-art methods. This help to further promote software configurable performance.Conclusion: Experimental results on seven public real-world datasets demonstrate that PERF-AL outperforms state-of-the-art software performance prediction methods.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {16},
numpages = {11},
keywords = {Software performance prediction, adversarial learning, configurable systems, regularization},
location = {Bari, Italy},
series = {ESEM '20}
}

@article{10.5555/3288443.3288514,
author = {L\"{u}si, Iiris and Bolotnikova, Anastasia and Daneshmand, Morteza and Ozcinar, Cagri and Anbarjafari, Gholamreza},
title = {Optimal image compression via block-based adaptive colour reduction with minimal contour effect},
year = {2018},
issue_date = {Dec 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {23},
issn = {1380-7501},
abstract = {Current image acquisition devices require tremendous amounts of storage for saving the data returned. This paper overcomes the latter drawback through proposing a colour reduction technique which first subdivides the image into patches, and then makes use of fuzzy c-means and fuzzy-logic-based inference systems, in order to cluster and reduce the number of the unique colours present in each patch, iteratively. The colours available in each patch are quantised, and the emergence of false edges is checked for, by means of the Sobel edge detection algorithm, so as to minimise the contour effect. At the compression stage, a methodology taking advantage of block-based singular value decomposition and wavelet difference reduction is adopted. Considering 35000 sample images from various databases, the proposed method outperforms centre cut, moment-preserving threshold, inter-colour correlation, generic K-means and quantisation by dimensionality reduction.},
journal = {Multimedia Tools Appl.},
month = dec,
pages = {30939–30968},
numpages = {30},
keywords = {Adaptive colour reduction, Block processing, Colour image processing, Image compression}
}

@inproceedings{10.1109/WI-IAT.2014.170,
author = {Louati, Amine and Haddad, Joyce El and Pinson, Suzanne},
title = {A Multilevel Agent-Based Approach for Trustworthy Service Selection in Social Networks},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.170},
doi = {10.1109/WI-IAT.2014.170},
abstract = {The growing number of services available within social applications (viz. Social networks) raises a new and challenging search issue: selecting desired services from social networks. Traditional discovery and selection approaches, which are registry-based (e.g., UDDI, ebXML), have manifested their limitations as they often fall behind users' expectations. This is because registries fail to (i) take into consideration non functional properties such as QoS and trust and (ii) capitalize on the information resulting from the previous experiences between agents. To address these shortcomings, we use software agents as they support interactions and offer well-developed capabilities to formally express and interpret semantic information useful to evaluate trust. Trust in a service is a multi-aspect concept that includes a social-based aspect such as judging whether the provider is worthwhile pursuing before using his services (viz. Trust in sociability), expert-based aspect such as estimating whether the service behaves well and as expected (viz. Trust in expertise) and, recommender-based aspect such as assessing whether an agent is reliable and we can rely on its recommendations (viz. Trust in recommendation).},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 03},
pages = {214–221},
numpages = {8},
keywords = {Multi-Agent Systems, Referral Systems, Service Selection, Social Networks, Trust},
series = {WI-IAT '14}
}

@inbook{10.1145/3191315.3191316,
author = {Kifer, Michael and Liu, Yanhong Annie},
title = {Preface},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191316},
abstract = {The idea of this book grew out of a symposium that was held at Stony Brook in September 2012 in celebration of David S. Warren's fundamental contributions to Computer Science and the area of Logic Programming in particular.Logic Programming (LP) is at the nexus of Knowledge Representation, Artificial Intelligence, Mathematical Logic, Databases, and Programming Languages. It is fascinating and intellectually stimulating due to the fundamental interplay among theory, systems, and applications brought about by logic. Logic programs are more declarative in the sense that they strive to be logical specifications of "what" to do rather than "how" to do it, and thus they are high-level and easier to understand and maintain. Yet, without being given an actual algorithm, LP systems implement the logical specifications automatically.Several books cover the basics of LP but focus mostly on the Prolog language with its incomplete control strategy and non-logical features. At the same time, there is generally a lack of accessible yet comprehensive collections of articles covering the key aspects in declarative LP. These aspects include, among others, well-founded vs. stable model semantics for negation, constraints, object-oriented LP, updates, probabilistic LP, and evaluation methods, including top-down vs. bottom-up, and tabling.For systems, the situation is even less satisfactory, lacking accessible literature that can help train the new crop of developers, practitioners, and researchers. There are a few guides on Warren's Abstract Machine (WAM), which underlies most implementations of Prolog, but very little exists on what is needed for constructing a state-of-the-art declarative LP inference engine. Contrast this with the literature on, say, Compilers, where one can first study a book on the general principles and algorithms and then dive in the particulars of a specific compiler. Such resources greatly facilitate the ability to start making meaningful contributions quickly. There is also a dearth of articles about systems that support truly declarative languages, especially those that tie into first-order logic, mathematical programming, and constraint solving.LP helps solve challenging problems in a wide range of application areas, but in-depth analysis of their connection with LP language abstractions and LP implementation methods is lacking. Also, rare are surveys of challenging application areas of LP, such as Bioinformatics, Natural Language Processing, Verification, and Planning.The goal of this book is to help fill in the previously mentioned void in the LP literature. It offers a number of overviews on key aspects of LP that are suitable for researchers and practitioners as well as graduate students. The following chapters in theory, systems, and applications of LP are included.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {xvii–xx}
}

@article{10.1504/IJKEDM.2011.037643,
author = {Reichle, Meike and Bach, Kerstin and Althoff, Klaus-Dieter},
title = {Knowledge engineering within the application-independent architecture SEASALT},
year = {2011},
issue_date = {December 2011},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {1},
number = {3},
issn = {1755-2087},
url = {https://doi.org/10.1504/IJKEDM.2011.037643},
doi = {10.1504/IJKEDM.2011.037643},
abstract = {Sharing Experience using an Agent-based System Architecture Layout (SEASALT) presents an instantiation of the Collaborating Multi-expert Systems (CoMES) approach. It integrates techniques from software engineering and combines them with artificial intelligence methodologies. The approach offers an application-independent architecture that features knowledge acquisition from a web-community, knowledge modularisation and agent-based knowledge maintenance. The paper introduces a travel medicine as application domain which applies SEASALT and describes each part of the novel architecture for extracting, analysing, sharing and providing community experiences in an individualised way.},
journal = {Int. J. Knowl. Eng. Data Min.},
month = dec,
pages = {202–215},
numpages = {14},
keywords = {MAS, agent-based systems, application-independent architecture, artificial intelligence, case-based reasoning, distributed CBR, experience management, expert systems, knowledge acquisition, knowledge engineering, knowledge maintenance, knowledge management, knowledge modularisation, multi-agent-, software engineering, systems, travel medicine, web based communities}
}

@article{10.4018/ijdwm.2014040104,
author = {Liao, Shu-Hsien and Wen, Chih-Hao and Hsian, Pei-Yuan and Li, Chien-Wen and Hsu, Che-Wei},
title = {Mining Customer Knowledge for a Recommendation System in Convenience Stores},
year = {2014},
issue_date = {April 2014},
publisher = {IGI Global},
address = {USA},
volume = {10},
number = {2},
issn = {1548-3924},
url = {https://doi.org/10.4018/ijdwm.2014040104},
doi = {10.4018/ijdwm.2014040104},
abstract = {Taiwan's rapid economic growth with increasing personal income leads increasing numbers of young unmarried people to eat out, and shopping at convenience stores for food is indispensable to the lives of these people. Thus, it is an essential issue for convenience store owners to know how to accurately market appropriate products and to choose effective endorsers for brands or products in order to attract target consumers. Data mining is a business intelligence analysis approach with great potential to help businesses focus on the most important business information contained in a database. Therefore, this study uses the Apriori algorithm as an association rules approach, and clustering analysis for data mining. The authors divide consumers into three groups by their consumer profiles and then find each group's product preference mixes, product endorsers, and product/brand line extensions for new product development. These are developed as a recommendation system for 7-11 convenience stores in Taiwan.},
journal = {Int. J. Data Warehous. Min.},
month = apr,
pages = {55–86},
numpages = {32},
keywords = {Brand and Product Line Extensions, Business Intelligence, Convenience Stores, Data Mining, Endorsers, Recommendation System}
}

@inproceedings{10.1145/3078597.3078602,
author = {Xie, Xiaolong and Tan, Wei and Fong, Liana L. and Liang, Yun},
title = {CuMF_SGD: Parallelized Stochastic Gradient Descent for Matrix Factorization on GPUs},
year = {2017},
isbn = {9781450346993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078597.3078602},
doi = {10.1145/3078597.3078602},
abstract = {Stochastic gradient descent (SGD) is widely used by many machine learning algorithms. It is efficient for big data ap- plications due to its low algorithmic complexity. SGD is inherently serial and its parallelization is not trivial. How to parallelize SGD on many-core architectures (e.g. GPUs) for high efficiency is a big challenge. In this paper, we present cuMF_SGD, a parallelized SGD solution for matrix factorization on GPUs. We first design high-performance GPU computation kernels that accelerate individual SGD updates by exploiting model parallelism. We then design efficient schemes that parallelize SGD updates by exploiting data parallelism. Finally, we scale cuMF SGD to large data sets that cannot fit into one GPU's memory. Evaluations on three public data sets show that cuMF_SGD outperforms existing solutions, including a 64- node CPU system, by a large margin using only one GPU card.},
booktitle = {Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {79–92},
numpages = {14},
keywords = {gpgpu, matrix factorization, parallel computing},
location = {Washington, DC, USA},
series = {HPDC '17}
}

@article{10.1287/msom.2015.0561,
author = {Ferreira, Kris Johnson and Lee, Bin Hong Alex and Simchi-Levi, David},
title = {Analytics for an Online Retailer: Demand Forecasting and Price Optimization},
year = {2016},
issue_date = {February 2016},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {18},
number = {1},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2015.0561},
doi = {10.1287/msom.2015.0561},
abstract = {We present our work with an online retailer, Rue La La, as an example of how a retailer can use its wealth of data to optimize pricing decisions on a daily basis. Rue La La is in the online fashion sample sales industry, where they offer extremely limited-time discounts on designer apparel and accessories. One of the retailer's main challenges is pricing and predicting demand for products that it has never sold before, which account for the majority of sales and revenue. To tackle this challenge, we use machine learning techniques to estimate historical lost sales and predict future demand of new products. The nonparametric structure of our demand prediction model, along with the dependence of a product's demand on the price of competing products, pose new challenges on translating the demand forecasts into a pricing policy. We develop an algorithm to efficiently solve the subsequent multiproduct price optimization that incorporates reference price effects, and we create and implement this algorithm into a pricing decision support tool for Rue La La's daily use. We conduct a field experiment and find that sales does not decrease because of implementing tool recommended price increases for medium and high price point products. Finally, we estimate an increase in revenue of the test group by approximately 9.7% with an associated 90% confidence interval of [2.3%, 17.8%].},
journal = {Manufacturing &amp; Service Operations Management},
month = feb,
pages = {69–88},
numpages = {20},
keywords = {demand forecasting, demand interdependency, flash sales, initial pricing, machine learning, model implementation, online retailing, price optimization, regression trees, revenue management}
}

@article{10.1016/j.patcog.2018.11.028,
author = {Zhao, Lijun and Bai, Huihui and Liang, Jie and Zeng, Bing and Wang, Anhong and Zhao, Yao},
title = {Simultaneous color-depth super-resolution with conditional generative adversarial networks},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.11.028},
doi = {10.1016/j.patcog.2018.11.028},
journal = {Pattern Recogn.},
month = apr,
pages = {356–369},
numpages = {14},
keywords = {Generative adversarial networks, Super-resolution, Image smoothing, Edge detection}
}

@article{10.1016/j.knosys.2018.04.006,
author = {Lee, Gichang and Jeong, Jaeyun and Seo, Seungwan and Kim, CzangYeob and Kang, Pilsung},
title = {Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network},
year = {2018},
issue_date = {July 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {152},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.04.006},
doi = {10.1016/j.knosys.2018.04.006},
abstract = {In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment (positive/negative) of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is a lack of information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network (CNN). In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. To verify the proposed methodology, we evaluated the classification accuracy and the rate of polarity words among high scoring words using two movie review datasets. Experimental results show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores.},
journal = {Know.-Based Syst.},
month = jul,
pages = {70–82},
numpages = {13},
keywords = {Class activation mapping, Convolutional neural network, Sentiment analysis, Weakly supervised learning, Word localization}
}

@inproceedings{10.5555/2887007.2887097,
author = {Ermon, Stefano and Xue, Yexiang and Toth, Russell and Dilkina, Bistra and Bernstein, Richard and Damoulas, Theodoros and Clark, Patrick and DeGloria, Steve and Mude, Andrew and Barrett, Christopher and Gomes, Carla P.},
title = {Learning large-scale dynamic discrete choice models of spatio-temporal preferences with application to migratory pastoralism in East Africa},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Understanding spatio-temporal resource preferences is paramount in the design of policies for sustainable development. Unfortunately, resource preferences are often unknown to policy-makers and have to be inferred from data. In this paper we consider the problem of inferring agents' preferences from observed movement trajectories, and formulate it as an Inverse Reinforcement Learning (IRL) problem. With the goal of informing policy-making, we take a probabilistic approach and consider generative models that can be used to simulate behavior under new circumstances such as changes in resource availability, access policies, or climate. We study the Dynamic Discrete Choice (DDC) models from econometrics and prove that they generalize the Max-Entropy IRL model, a widely used probabilistic approach from the machine learning literature. Furthermore, we develop SPL-GD, a new learning algorithm for DDC models that is considerably faster than the state of the art and scales to very large datasets.We consider an application in the context of pastoralism in the arid and semi-arid regions of Africa, where migratory pastoralists face regular risks due to resource availability, droughts, and resource degradation from climate change and development. We show how our approach based on satellite and survey data can accurately model migratory pastoralism in East Africa and that it considerably outperforms other approaches on a large-scale real-world dataset of pastoralists' movements in Ethiopia collected over 3 years.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {644–650},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.1145/3469096.3469872,
author = {Yang, Eugene and Lewis, David D. and Frieder, Ophir},
title = {On minimizing cost in legal document review workflows},
year = {2021},
isbn = {9781450385961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469096.3469872},
doi = {10.1145/3469096.3469872},
abstract = {Technology-assisted review (TAR) refers to human-in-the-loop machine learning workflows for document review in legal discovery and other high recall review tasks. Attorneys and legal technologists have debated whether review should be a single iterative process (one-phase TAR workflows) or whether model training and review should be separate (two-phase TAR workflows), with implications for the choice of active learning algorithm. The relative cost of manual labeling for different purposes (training vs. review) and of different documents (positive vs. negative examples) is a key and neglected factor in this debate. Using a novel cost dynamics analysis, we show analytically and empirically that these relative costs strongly impact whether a one-phase or two-phase workflow minimizes cost. We also show how category prevalence, classification task difficulty, and collection size impact the optimal choice not only of workflow type, but of active learning method and stopping point.},
booktitle = {Proceedings of the 21st ACM Symposium on Document Engineering},
articleno = {30},
numpages = {10},
keywords = {active learning, cost modeling, high-recall retrieval, total recall},
location = {Limerick, Ireland},
series = {DocEng '21}
}

@inproceedings{10.5555/2772879.2773415,
author = {Genter, Katie and Laue, Tim and Stone, Peter},
title = {The RoboCup 2014 SPL Drop-in Player Competition: Encouraging Teamwork without Pre-coordination},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The Standard Platform League is a soccer league at the annual RoboCup world championships in which teams of five humanoid robots play against each other. In 2014, the Drop-in Player Competition was added to the league to serve as a testbed for cooperation without pre-coordination. Instead of homogeneous robot teams that are programmed by each team to implicitly work together, this competition features ad hoc teams, i.e. teams that consist of robots originating from different RoboCup teams and that are each running different software. In this extended abstract, we provide an overview of this competition, including its motivation and rules.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1745–1746},
numpages = {2},
keywords = {ad hoc teamwork, cooperation, robot soccer},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.5555/3305890.3305916,
author = {Ma, Fan and Meng, Deyu and Xie, Qi and Li, Zina and Dong, Xuanyi},
title = {Self-paced co-training},
year = {2017},
publisher = {JMLR.org},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two different views and exchanges labels of unlabeled instances in an iterative way. During co-training process, labels of unlabeled instances in the training pool are very likely to be false especially in the initial training rounds, while the standard co-training algorithm utilizes a "draw without replacement" manner and does not remove these false labeled instances from training. This issue not only tends to degenerate its performance but also hampers its fundamental theory. Besides, there is no optimization model to explain what objective a co-training process optimizes. To these issues, in this study we design a new co-training algorithm named self-paced co-training (SPaCo) with a "draw with replacement" learning mode. The rationality of SPaCo can be proved under theoretical assumptions utilized in traditional co-training research, and furthermore, the algorithm exactly complies with the alternative optimization process for an optimization model of self-paced curriculum learning, which can be finely explained in robust learning manner. Experimental results substantiate the superiority of the proposed method as compared with current state-of-the-art co-training methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2275–2284},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.1109/ASE.2015.35,
author = {W\"{o}lfl, Andreas and Siegmund, Norbert and Apel, Sven and Kosch, Harald and Krautlager, Johann and Weber-Urbina, Guillermo},
title = {Generating qualifiable avionics software: an experience report},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.35},
doi = {10.1109/ASE.2015.35},
abstract = {We report on our experience with enhancing the data-management component in the avionics software of the NH90 helicopter at Airbus Helicopters. We describe challenges regarding the evolution of avionics software by means of real-world evolution scenarios that arise in industrial practice. A key role plays a legally-binding certification process, called qualification, which is responsible for most of the development effort and cost. To reduce effort and cost, we propose a novel generative approach to develop qualifiable avionics software by combining model-based and product-line technology. Using this approach, we have already generated code that is running on the NH90 helicopter and that is in the process of replacing the current system code. Based on an interview with two professional developers at Airbus and an analysis of the software repository of the NH90, we systematically compare our approach with established development approaches in the avionics domain, in terms of implementation and qualification effort.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {726–736},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1007/s00500-021-05934-8,
author = {Huang, Xuan and Hu, Zhenlong and Lin, Lin},
title = {RETRACTED ARTICLE: Deep clustering based on embedded auto-encoder},
year = {2021},
issue_date = {Jan 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05934-8},
doi = {10.1007/s00500-021-05934-8},
abstract = {Deep clustering is a new research direction that combines deep learning and clustering. It performs feature representation and cluster assignments simultaneously, and its clustering performance is significantly superior to traditional clustering algorithms. The auto-encoder is a neural network model, which can learn the hidden features of the input object to achieve nonlinear dimensionality reduction. This paper proposes the embedded auto-encoder network model; specifically, the auto-encoder is embedded into the encoder unit and the decoder unit of the prototype auto-encoder, respectively. To ensure effectively cluster high-dimensional objects, the encoder of model first encodes the raw features of the input objects, and obtains a cluster-friendly feature representation. Then, in the model training stage, by adding smoothness constraints to the objective function of the encoder, the representation capabilities of the hidden layer coding are significantly improved. Finally, the adaptive self-paced learning threshold is determined according to the median distance between the object and its corresponding the centroid, and the fine-tuning sample of the model is automatically selected. Experimental results on multiple image datasets have shown that our model has fewer parameters, higher efficiency and the comprehensive clustering performance is significantly superior to the state-of-the-art clustering methods.},
journal = {Soft Comput.},
month = jun,
pages = {1075–1090},
numpages = {16},
keywords = {Deep clustering, The embedded auto-encoder, Feature representation}
}

@inproceedings{10.5555/3495724.3496445,
author = {Zhou, Tianyi and Wang, Shengjie and Bilmes, Jeff A.},
title = {Curriculum learning by dynamic instance hardness},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A good teacher can adjust a curriculum based on students' learning history. By analogy, in this paper, we study the dynamics of a deep neural network's (DNN) performance on individual samples during its learning process. The observed properties allow us to develop an adaptive curriculum that leads to faster learning of more accurate models. We introduce dynamic instance hardness (DIH), the exponential moving average of a sample's instantaneous hardness (e.g., a loss, or a change in output) over the training history. A low DIH indicates that a model retains knowledge about a sample over time. For DNNs, we find that a sample's DIH early in training predicts its DIH in later stages. Hence, we can train a model using samples mostly with higher DIH and safely deprioritize those with lower DIH. This motivates a DIH guided curriculum learning (DIHCL) procedure. Compared to existing CL methods: (1) DIH is more stable over time than using only instantaneous hardness, which is noisy due to stochastic training and DNN's non-smoothness; (2) DIHCL is computationally inexpensive since it uses only a byproduct of back-propagation and thus does not require extra inference. On 11 datasets, DIHCL significantly outperforms random mini-batch SGD and recent CL methods in terms of efficiency and final performance.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {721},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1007/s11634-016-0266-6,
author = {Hayashi, Kenichi},
title = {Asymptotic comparison of semi-supervised and supervised linear discriminant functions for heteroscedastic normal populations},
year = {2018},
issue_date = {June      2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {2},
issn = {1862-5347},
url = {https://doi.org/10.1007/s11634-016-0266-6},
doi = {10.1007/s11634-016-0266-6},
abstract = {It has been reported that using unlabeled data together with labeled data to construct a discriminant function works successfully in practice. However, theoretical studies have implied that unlabeled data can sometimes adversely affect the performance of discriminant functions. Therefore, it is important to know what situations call for the use of unlabeled data. In this paper, asymptotic relative efficiency is presented as the measure for comparing analyses with and without unlabeled data under the heteroscedastic normality assumption. The linear discriminant function maximizing the area under the receiver operating characteristic curve is considered. Asymptotic relative efficiency is evaluated to investigate when and how unlabeled data contribute to improving discriminant performance under several conditions. The results show that asymptotic relative efficiency depends mainly on the heteroscedasticity of the covariance matrices and the stochastic structure of observing the labels of the cases.},
journal = {Adv. Data Anal. Classif.},
month = jun,
pages = {315–339},
numpages = {25},
keywords = {62G20, 62H30, 68T10, Area under the ROC curve, Labeling mechanism, Linear discriminant function, Missing data, Receiver operating characteristic curve, Semi-supervised learning}
}

@article{10.1016/j.eswa.2021.114664,
author = {Gan, Yi and He, Yiqi and Gao, Li and He, Weiming},
title = {Propagation path optimization of product attribute design changes based on Petri Net fusion ant colony algorithm},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {173},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114664},
doi = {10.1016/j.eswa.2021.114664},
journal = {Expert Syst. Appl.},
month = jul,
numpages = {12},
keywords = {Expanded petri net, Expanded petri ant colony optimization algorithm, Design of product attribute, Change propagation, Path optimization}
}

@article{10.1007/s00521-019-04435-y,
author = {Kolokas, Nikolaos and Drosou, Anastasios and Tzovaras, Dimitrios},
title = {Text synthesis from keywords: a comparison of recurrent-neural-network-based architectures and hybrid approaches},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {9},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04435-y},
doi = {10.1007/s00521-019-04435-y},
abstract = {This paper concerns an application of recurrent neural networks to text synthesis in the word level, with the help of keywords. First, a Parts Of Speech tagging library is employed to extract verbs and nouns from the texts used in our work, a part of which are then considered, after automatic eliminations, as the aforementioned keywords. Our ultimate aim is to train a recurrent neural network to map the keyword sequence of a text to the entire text. Successive reformulations of the keyword and full-text word sequences are performed, so that they can serve as the input and target of the network as efficiently as possible. The predicted texts are understandable enough, and the model performance depends on the problem difficulty, determined by the percentage of full-text words that are considered as keywords, that ranges from 1/3 to 1/2 approximately, the training memory cost, mainly affected by the network architecture, as well as the similarity between different texts, which determines the best architecture.},
journal = {Neural Comput. Appl.},
month = may,
pages = {4259–4274},
numpages = {16},
keywords = {Deep machine learning, Sequence modeling, Natural language processing, Text mining}
}

@article{10.1016/j.knosys.2020.106660,
author = {Liu, Zhen and Feng, Xiaodong and Wang, Yecheng and Zuo, Wenbo},
title = {Self-paced learning enhanced neural matrix factorization for noise-aware recommendation},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {213},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2020.106660},
doi = {10.1016/j.knosys.2020.106660},
journal = {Know.-Based Syst.},
month = feb,
numpages = {12},
keywords = {Recommendation, Deep learning, Noisy and outlier corruption, Instance weighting, Self-paced learning}
}

@article{10.1007/s10664-019-09761-2,
author = {Xiang, Yi and Yang, Xiaowei and Zhou, Yuren and Zheng, Zibin and Li, Miqing and Huang, Han},
title = {Going deeper with optimal software products selection using many-objective optimization and satisfiability solvers},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09761-2},
doi = {10.1007/s10664-019-09761-2},
abstract = {In search-based software engineering, one actively studied problem is the optimal software product selection from a feature model using multiple (usually more than three) optimization objectives simultaneously. This can be represented as a many-objective optimization problem. The primary goal of solving this problem is to search for diverse and high-quality valid products as rapidly as possible. Previous studies have shown that combining search-based techniques with satisfiability (SAT) solvers was promising for achieving this goal, but it remained open that how different solvers affect the performance of a search algorithm, and that whether the ways to randomize solutions in the solvers make a difference. Moreover, we may need further investigation on the necessity of mixing different types of SAT solving techniques. In this paper, we address the above open research questions by performing a series of empirical studies on 21 features models, most of which are reverse-engineered from industrial software product lines. We examine four conflict-driven clause learning solvers, two stochastic local search solvers, and two different ways to randomize solutions. Experimental results suggest that the performance can be indeed affected by different SAT solvers, and by the ways to randomize solutions in the solvers. This study serves as a practical guideline for choosing and tuning SAT solvers for the many-objective optimal software product selection problem.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {591–626},
numpages = {36},
keywords = {Search-based software engineering, Feature model, Many-objective optimization, Optimal software product selection, Satisfiability solvers}
}

@article{10.1007/s00607-018-0646-1,
author = {Galindo, Jos\'{e} A. and Benavides, David and Trinidad, Pablo and Guti\'{e}rrez-Fern\'{a}ndez, Antonio-Manuel and Ruiz-Cort\'{e}s, Antonio},
title = {Automated analysis of feature models: Quo vadis?},
year = {2019},
issue_date = {May       2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {101},
number = {5},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-018-0646-1},
doi = {10.1007/s00607-018-0646-1},
abstract = {Feature models have been used since the 90s to describe software product lines as a way of reusing common parts in a family of software systems. In 2010, a systematic literature review was published summarizing the advances and settling the basis of the area of automated analysis of feature models (AAFM). From then on, different studies have applied the AAFM in different domains. In this paper, we provide an overview of the evolution of this field since 2010 by performing a systematic mapping study considering 423 primary sources. We found six different variability facets where the AAFM is being applied that define the tendencies: product configuration and derivation; testing and evolution; reverse engineering; multi-model variability-analysis; variability modelling and variability-intensive systems. We also confirmed that there is a lack of industrial evidence in most of the cases. Finally, we present where and when the papers have been published and who are the authors and institutions that are contributing to the field. We observed that the maturity is proven by the increment in the number of journals published along the years as well as the diversity of conferences and workshops where papers are published. We also suggest some synergies with other areas such as cloud or mobile computing among others that can motivate further research in the future.},
journal = {Computing},
month = may,
pages = {387–433},
numpages = {47},
keywords = {68T35, Automated analysis, Feature models, Software product lines, Variability-intensive systems}
}

@inproceedings{10.5555/3155562.3155664,
author = {Krismayer, Thomas and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Mining constraints for event-based monitoring in systems of systems},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = {The full behavior of software-intensive systems of systems (SoS) emerges during operation only. Runtime monitoring approaches have thus been proposed to detect deviations from the expected behavior. They commonly rely on temporal logic or domain-specific languages to formally define requirements, which are then checked by analyzing the stream of monitored events and event data. Some approaches also allow developers to generate constraints from declarative specifications of the expected behavior. However, independent of the approach, deep domain knowledge is required to specify the desired behavior. This knowledge is often not accessible in SoS environments with multiple development teams independently working on different, heterogeneous systems. In this New Ideas Paper we thus describe an approach that automatically mines constraints for runtime monitoring from event logs recorded in SoS. Our approach builds on ideas from specification mining, process mining, and machine learning to mine different types of constraints on event occurrence, event timing, and event data. The approach further presents the mined constraints to users in an existing constraint language and it ranks the constraints using different criteria. We demonstrate the feasibility of our approach by applying it to event logs from a real-world industrial SoS.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {826–831},
numpages = {6},
keywords = {Constraint mining, event-based monitoring, systems of systems},
location = {Urbana-Champaign, IL, USA},
series = {ASE '17}
}

@article{10.1016/j.engappai.2009.03.010,
author = {Vasant, Pandian and Barsoum, Nader},
title = {Hybrid genetic algorithms and line search method for industrial production planning with non-linear fitness function},
year = {2009},
issue_date = {June, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {22},
number = {4–5},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.03.010},
doi = {10.1016/j.engappai.2009.03.010},
abstract = {Many engineering, science, information technology and management optimization problems can be considered as non-linear programming real-world problems where all or some of the parameters and variables involved are uncertain in nature. These can only be quantified using intelligent computational techniques such as evolutionary computation and fuzzy logic. The main objective of this research paper is to solve non-linear fuzzy optimization problem where the technological coefficient in the constraints involved are fuzzy numbers, which was represented by logistic membership functions using the hybrid evolutionary optimization approach. To explore the applicability of the present study, a numerical example is considered to determine the production planning for the decision variables and profit of the company.},
journal = {Eng. Appl. Artif. Intell.},
month = jun,
pages = {767–777},
numpages = {11},
keywords = {Fuzzy constraints, Genetic algorithms, Hybrid optimization, Line search, Uncertainty}
}

@article{10.1016/j.patcog.2011.09.011,
author = {Rasmussen, Peter M. and Hansen, Lars K. and Madsen, Kristoffer H. and Churchill, Nathan W. and Strother, Stephen C.},
title = {Model sparsity and brain pattern interpretation of classification models in neuroimaging},
year = {2012},
issue_date = {June, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2011.09.011},
doi = {10.1016/j.patcog.2011.09.011},
abstract = {Interest is increasing in applying discriminative multivariate analysis techniques to the analysis of functional neuroimaging data. Model interpretation is of great importance in the neuroimaging context, and is conventionally based on a 'brain map' derived from the classification model. In this study we focus on the relative influence of model regularization parameter choices on both the model generalization, the reliability of the spatial patterns extracted from the classification model, and the ability of the resulting model to identify relevant brain networks defining the underlying neural encoding of the experiment. For a support vector machine, logistic regression and Fisher's discriminant analysis we demonstrate that selection of model regularization parameters has a strong but consistent impact on the generalizability and both the reproducibility and interpretable sparsity of the models for both @?"2 and @?"1 regularization. Importantly, we illustrate a trade-off between model spatial reproducibility and prediction accuracy. We show that known parts of brain networks can be overlooked in pursuing maximization of classification accuracy alone with either @?"2 and/or @?"1 regularization. This supports the view that the quality of spatial patterns extracted from models cannot be assessed purely by focusing on prediction accuracy. Our results instead suggest that model regularization parameters must be carefully selected, so that the model and its visualization enhance our ability to interpret the brain.},
journal = {Pattern Recogn.},
month = jun,
pages = {2085–2100},
numpages = {16},
keywords = {Classification, Kernel methods, Machine learning, Model interpretation, NPAIRS resampling, Neuroimaging, Pattern analysis, Regularization, Sparsity}
}

@article{10.4018/IJBDCN.2019070106,
author = {Patil, Vilas K and Nagarale, P.P.},
title = {Prediction of L10 and Leq Noise Levels Due to Vehicular Traffic in Urban Area Using ANN and Adaptive Neuro-Fuzzy Interface System (ANFIS) Approach},
year = {2019},
issue_date = {Jul 2019},
publisher = {IGI Global},
address = {USA},
volume = {15},
number = {2},
issn = {1548-0631},
url = {https://doi.org/10.4018/IJBDCN.2019070106},
doi = {10.4018/IJBDCN.2019070106},
abstract = {Recently in urban areas, road traffic noise is one of the primary sources of noise pollution. Variation in noise level is impacted by the synthesis of traffic and the percentage of heavy vehicles. Presentation to high noise levels may cause serious impact on the health of an individual or community residing near the roadside. Thus, predicting the vehicular traffic noise level is important. The present study aims at the formulation of regression, an artificial neural network (ANN) and an adaptive neuro-fuzzy interface system (ANFIS) model using the data of observed noise levels, traffic volume, and average speed of vehicles for the prediction of L10 and Leq. Measured noise levels are compared to the noise levels predicted by the experimental model. It is observed that the ANFIS approach is more superior when compared to output given by regression and an ANN model. Also, there exists a positive correlation between measured and predicted noise levels. The proposed ANFIS model can be utilized as a tool for traffic direction and planning of new roads in zones of similar land use pattern.},
journal = {Int. J. Bus. Data Commun. Netw.},
month = jul,
pages = {92–105},
numpages = {14},
keywords = {ANFIS, Artificial Neural Network, Modeling, Regression, Vehicular Traffic Noise Prediction}
}

@inproceedings{10.1007/978-3-030-58475-7_41,
author = {Collet, Mathieu and Gotlieb, Arnaud and Lazaar, Nadjib and Carlsson, Mats and Marijan, Dusica and Mossige, Morten},
title = {RobTest: A CP Approach to Generate Maximal Test Trajectories for Industrial Robots},
year = {2020},
isbn = {978-3-030-58474-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58475-7_41},
doi = {10.1007/978-3-030-58475-7_41},
abstract = {Developing industrial robots which are safe, performant, robust and reliable over time is challenging, because their embedded distributed software system involves complex motions with force and torque control and anti-collision surveillance processes. Generating test trajectories which increase the chance to uncover potential failures or downtime is thus crucial to verify the reliability and performance of the robot before delivering it to its final users. Currently, these trajectories are manually created by test engineers, something that renders the process error-prone and time-consuming. In this paper, we present RobTest, a Constraint Programming approach for generating automatically maximal test trajectories for serial industrial robots. RobTest sequentially calls two constraint solvers: a solver over continuous domains to determine the reachability between configurations of the robot’s 3D-space, and a solver over finite domains to generate maximal-load test trajectories among a set of input points and obstacles of the 3D-space. RobTest is developed at ABB Robotics, a large robot manufacturing company, together with test engineers, who are preparing it for integration within the continuous testing process of the robots product-line. This paper reports on initial experimental results with three distinct solvers, namely Gecode, SICStus and Chuffed, where RobTest, has been shown to return near-optimal solutions for trajectories encounting for more than 80 input points and 60 obstacles in less than 5&nbsp;min.},
booktitle = {Principles and Practice of Constraint Programming: 26th International Conference, CP 2020, Louvain-La-Neuve, Belgium, September 7–11, 2020, Proceedings},
pages = {707–723},
numpages = {17},
keywords = {Industrial robotics, Maximal test trajectories, Path planning, Global constraints},
location = {Louvain-la-Neuve, Belgium}
}

@article{10.5555/2608462.2608467,
author = {Przytu\l{}a, \L{}ukasz},
title = {NAO Soccer Robots Path Planning Based on Rough Mereology},
year = {2014},
issue_date = {April 2014},
publisher = {IOS Press},
address = {NLD},
volume = {131},
number = {2},
issn = {0169-2968},
abstract = {Soccer game is a good playground for testing artificial intelligence of robots and methods for spatial reasoning in real conditions. Decision making and path planning are only two of many tasks performed while playing soccer. This paper describes an application of rough mereology introduced by Polkowski and Skowron 1994 for path planning in robotic soccer game. Our path planning method was based on mereological potential fields introduced by Polkowski and O\'{s}mia\l{}owski 2008 and O\'{s}mia\l{}owski 2009 but was redesigned due to conditions of dynamic soccer environment, so an entirely new method was developed.},
journal = {Fundam. Inf.},
month = apr,
pages = {241–251},
numpages = {11},
keywords = {Mereological Potential Field, Nao, Path Planning, Robotics, Rough Mereology, Soccer}
}

@article{10.1007/s11042-019-7381-2,
author = {Dad, Nisrine and En-nahnahi, Noureddine and El Alaoui Ouatik, Said},
title = {Quaternion Harmonic moments and extreme learning machine for color object recognition},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {15},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7381-2},
doi = {10.1007/s11042-019-7381-2},
abstract = {The quaternary orthogonal moments have been widely used as color image descriptors owe to their remarkable color and shape information encapsulation capability. Their computation, however, depends on finding the optimal value of a unit pure quaternion parameter, which is done empirically and with no warranty of optimality. We propose a 2D color object recognition method that relies on the quaternion-valued parameter-free disc-harmonic moment invariants (QHMs) fed into the quaternion extreme learning machine (QELM). The role of this latter is to maintain the correlation between the four parts, real and imaginary, of the quaternary descriptor coefficients. Several datasets are used for recognition experiments. We draw the conclusion that: (1) our quaternion-valued QHMs invariants outperform other quaternary moments, (2) the quaternion-valued moment invariants give results better than the modulus-based moment invariants and (3) the QELM yields results better than the state-of-the-art classifiers.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {20935–20959},
numpages = {25},
keywords = {Quaternion algebra, Color image feature extraction, Disc-Harmonic moments, Zernike moments, Spherical harmonics, Color object recognition, Back-propagation neural networks, Extreme learning machine}
}

@inproceedings{10.5555/3432601.3432616,
author = {Podolskiy, Vladimir and Patrou, Maria and Patros, Panos and Gerndt, Michael and Kent, Kenneth B.},
title = {The weakest link: revealing and modeling the architectural patterns of microservice applications},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {Cloud microservice applications comprise interconnected services packed into containers. Such applications generate complex communication patterns among their microservices. Studying such patterns can support assuring various quality attributes, such as autoscaling for satisfying performance, availability and scalability, or targeted penetration testing for satisfying security and correctness. We study the structure of containerized microservice applications via providing the methodology and the results of a structural graph-based analysis of 103 Docker Compose deployment files from open-sourced Github repositories. Our findings indicate the dominance of a power-law distribution of microservice interconnections. Further analysis highlights the suitability of the Barab\'{a}si-Albert model for generating large random graphs that model the architecture of real microservice applications. The exhibited structures and their usage for engineering microservice applications are discussed.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {113–122},
numpages = {10},
keywords = {application topology, cloud-native application, microservice, software vulnerability},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@article{10.1016/j.neucom.2019.10.018,
author = {Ding, Deqiong and Yang, Xiaogao and Xia, Fei and Ma, Tiefeng and Liu, Haiyun and Tang, Chang},
title = {Unsupervised feature selection via adaptive hypergraph regularized latent representation learning},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {378},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.10.018},
doi = {10.1016/j.neucom.2019.10.018},
journal = {Neurocomput.},
month = feb,
pages = {79–97},
numpages = {19},
keywords = {Unsupervised feature selection, Hypergraph learning, Latent representation learning, Local structure preservation, 00-01, 99-00}
}

@article{10.1016/j.engappai.2011.09.026,
author = {Dumitrache, Alexandru and Borangiu, Theodor},
title = {IMS10-image-based milling toolpaths with tool engagement control for complex geometry},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {25},
number = {6},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2011.09.026},
doi = {10.1016/j.engappai.2011.09.026},
abstract = {This paper presents a NC toolpath generation strategy with tool engagement control for arbitrarily complex discrete part geometry, which reduces machining time and tool wear and can be used in high speed machining. The toolpath computation is based on image models for design part, raw stock and cutting tool, and involves pixel-based simulation of the milling process. Simulation results and comparison with existing methods are presented.},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {1161–1172},
numpages = {12},
keywords = {CNC milling, Image-based CNC toolpaths, NC adaptive high speed machining, Tool engagement control}
}

@inproceedings{10.1007/978-3-030-90439-5_26,
author = {Lakshya},
title = {Behaviour of Sample Selection Techniques Under Explicit Regularization},
year = {2021},
isbn = {978-3-030-90438-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90439-5_26},
doi = {10.1007/978-3-030-90439-5_26},
abstract = {There is a multitude of sample selection-based learning strategies that have been developed for learning with noisy labels. However, It has also been indicated in the literature that perhaps early stopping is better than fully training the model for getting better performance. It leads us to wonder about the behavior of the sample selection strategies under explicit regularization. To this end, we considered four of the most fundamental sample selection-based models MentorNet, Coteaching, Coteaching-plus and JoCor. We provide empirical results of applying explicit L2 regularization to the above-mentioned approaches. We also compared the results with a baseline - a vanilla CNN model trained with just regularization. We show that under explicit regularization, the pre-conceived ranking of the approaches might change. We also show several instances where the baseline was able to outperform some or all of the existing approaches. Moreover, we show that under explicit regularization, the performance gap between the approaches can also reduce.},
booktitle = {Advances in Visual Computing: 16th International Symposium, ISVC 2021, Virtual Event, October 4-6, 2021, Proceedings, Part I},
pages = {331–340},
numpages = {10}
}

@inbook{10.5555/3454287.3455252,
author = {Jeong, Jisoo and Lee, Seungeui and Kim, Jeesoo and Kwak, Nojun},
title = {Consistency-based semi-supervised learning for object detection},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Specifically, the consistency constraint is applied not only for object classification but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage and two-stage detectors and the results show the effectiveness of our method.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {965},
numpages = {10}
}

@article{10.1016/j.specom.2017.04.002,
title = {Fourier model based features for analysis and classification of out-of-breath speech},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {90},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2017.04.002},
doi = {10.1016/j.specom.2017.04.002},
abstract = {A new stressed speech database, named out-of-breath speech (OBS) database, is created, which contains three classes, out-of-breath speech, low out-of-breath speech and normal speech.Four features are proposed using mutual information (MI) on the Fourier parameters for analysis and classification of different classes of OBS database.For multi-class classification, support vector machine (SVM) classifier is used with binary cascade approach.Recognition results show that the proposed features have higher potential to classify the out-of-breath speech, compared to the breathiness, MFCC and TEO-CB-Auto-Env features. This paper presents a new method of feature extraction using Fourier model for analysis of out-of-breath speech. The proposed feature is evaluated using mutual information (MI) on the difference and ratio values of the Fourier parameters, amplitude and frequency. The difference and ratio are calculated between two contiguous values of the Fourier parameters. To analyze the out-of-breath speech, a new stressed speech database, named out-of-breath speech (OBS) database, is created. The database contains three classes of speech, out-of-breath speech, low out-of-breath speech and normal speech. The effectiveness of the proposed features is evaluated with the statistical analysis. The proposed features not only differentiate the normal speech and the out-of-breath speech, but also can discriminate different breath emission levels of speech. Hidden Markov model (HMM) and support vector machine (SVM) are used to evaluate the performance of the proposed features using the OBS database. For multi-class classification problem, SVM classifier is used with binary cascade approach. The performance of the proposed features is compared with the breathiness feature, the mel frequency cepstral coefficient (MFCC) feature and the Teager energy operator (TEO) based critical band TEO autocorrelation envelope (TEO-CB-Auto-Env) feature. The proposed feature outperforms the breathiness feature, the MFCC feature and the TEO-CB-Auto-Env feature.},
journal = {Speech Commun.},
month = jun,
pages = {1–14},
numpages = {14}
}

@inproceedings{10.1007/978-3-030-98682-7_19,
author = {Yao, ZhengBai and Douglas, Will and O’Keeffe, Simon and Villing, Rudi},
title = {Faster YOLO-LITE: Faster Object Detection on Robot and Edge Devices},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_19},
doi = {10.1007/978-3-030-98682-7_19},
abstract = {Mobile robots and many edge AI devices have a need to trade off computational power against power consumption, battery size, and time between charges. Consequently, it is common for such devices to have significantly less computational power than the powerful GPU-based systems typically used to train and evaluate deep neural networks. Object detection is a key aspect of visual perception for robots and edge devices but popular object detection architectures that run fastest on GPU based systems or that are designed to maximize mAP with large input image sizes may not scale well to edge devices. In this work we evaluate the latency and mAP of several model architectures from the YOLO and SSD families on a range of devices representative of robot and edge device capabilities. We also evaluate the effect of runtime framework and show that some unexpected large differences can be found. Based on our evaluations we propose new variations of the YOLO-LITE architecture which we show can provide increased mAP at reduced latency.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {226–237},
numpages = {12},
keywords = {Deep learning, Object detection, Convolutional neural network, Embedded system, Real-time performance, Edge AI},
location = {Sydney, NSW, Australia}
}

@article{10.1016/j.jss.2007.04.011,
author = {Kim, Jintae and Park, Sooyong and Sugumaran, Vijayan},
title = {DRAMA: A framework for domain requirements analysis and modeling architectures in software product lines},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.04.011},
doi = {10.1016/j.jss.2007.04.011},
abstract = {One of the benefits of software product line approach is to improve time-to-market. The changes in market needs cause software requirements to be flexible in product lines. Whenever software requirements are changed, software architecture should be evolved to correspond with them. Therefore, domain architecture should be designed based on domain requirements. It is essential that there is traceability between requirements and architecture, and that the structure of architecture is derived from quality requirements. The purpose of this paper is to provide a framework for modeling domain architecture based on domain requirements within product lines. In particular, we focus on the traceable relationship between requirements and architectural structures. Our framework consists of processes, methods, and a supporting tool. It uses four basic concepts, namely, goal based domain requirements analysis, Analytical Hierarchy Process, Matrix technique, and architecture styles. Our approach is illustrated using HIS (Home Integration System) product line. Finally, industrial examples are used to validate DRAMA.},
journal = {J. Syst. Softw.},
month = jan,
pages = {37–55},
numpages = {19},
keywords = {Domain architecture, Domain requirements, Quality attribute, Traceability}
}

@article{10.1007/s10898-009-9491-2,
author = {Subramanian, Shivaram and Sherali, Hanif D.},
title = {A fractional programming approach for retail category price optimization},
year = {2010},
issue_date = {October   2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {48},
number = {2},
issn = {0925-5001},
url = {https://doi.org/10.1007/s10898-009-9491-2},
doi = {10.1007/s10898-009-9491-2},
abstract = {We present a new mixed-integer programming (MIP) approach to study certain retail category pricing problems that arise in practice. The motivation for this research arises from the need to design innovative analytic retail optimization techniques at Oracle Corporation to not only predict the empirical effect of price changes on the overall sales and revenue of a category, but also to prescribe optimal dynamic pricing recommendations across a category or demand group. A multinomial logit nonlinear optimization model is developed, which is recast as a discrete, nonlinear fractional program (DNFP). The DNFP model employs a bi-level, predictive modeling framework to manage the empirical effects of price elasticity and competition on sales and revenue, and to maximize the gross-margin of the demand group, while satisfying certain practical side-constraints. This model is then transformed by using the Reformulation---Linearization Technique in tandem with a sequential bound-tightening scheme to recover an MIP formulation having a relatively tight underlying linear programming relaxation, which can be effectively solved by any commercial optimization software package. We present sample computational results using randomly generated instances of DNFP having different constraint settings and price range restrictions that are representative of common business requirements, and analyze the empirical effects of certain key modeling parameters. Our results indicate that the proposed retail price optimization methodology can be effectively deployed within practical retail category management applications for solving DNFP instances that typically occur in practice.},
journal = {J. of Global Optimization},
month = oct,
pages = {263–277},
numpages = {15},
keywords = {Embedded demand model, Fractional programming, Mixed-integer programming, Multinomial logit model, Reformulation-Linearization Technique (RLT), Retail category pricing}
}

@inproceedings{10.1007/11527886_77,
author = {Wang, Yang},
title = {Constraint-Sensitive privacy management for personalized web-based systems},
year = {2005},
isbn = {3540278850},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11527886_77},
doi = {10.1007/11527886_77},
abstract = {This research aims at reconciling web personalization with privacy constraints imposed by legal restrictions and by users' privacy preferences. We propose a software product line architecture approach, where our privacyenabling user modeling architecture can dynamically select personalization methods that satisfy current privacy constraints to provide personalization services. A feasibility study is being carried out with the support of an existing user modeling server and a software architecture based development environment.},
booktitle = {Proceedings of the 10th International Conference on User Modeling},
pages = {524–526},
numpages = {3},
location = {Edinburgh, UK},
series = {UM'05}
}

@article{10.1007/s11265-021-01676-w,
author = {Ting, Yu-Ching and Lo, Fang-Wen and Tsai, Pei-Yun},
title = {Implementation for Fetal ECG Detection from Multi-channel Abdominal Recordings with 2D Convolutional Neural Network},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {93},
number = {9},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-021-01676-w},
doi = {10.1007/s11265-021-01676-w},
abstract = {A convolutional neural network (CNN)-based approach for fetal ECG detection from the abdominal ECG recording is proposed. The flow contains a pre-processing phase and a classification phase. In the pre-processing phase, short-time Fourier transform is applied to obtain the spectrogram, which is sent to 2D CNN for classification. The classified results from multiple channels are then fused and high detection accuracy up to 95.2% is achieved and the CNN-based approach outperforms the conventional algorithm. The hardware of this fetal ECG detector composed of the spectrogram processor and 2D CNN classifier is then implemented on the FPGA platform. Because the two dimensions of the spectrogram and the kernel are asymmetric, a pre-fetch mechanism is designed to eliminate the long latency resulted from data buffering for large-size convolution. From the implementation results, it takes 20258 clock cycles for inference and almost 50% computation cycles are reduced. The power consumption is 12.33mW at 324KHz and 1V for real-time operations. The implementation demonstrates the feasibility of real-time applications in wearable devices.},
journal = {J. Signal Process. Syst.},
month = sep,
pages = {1101–1113},
numpages = {13},
keywords = {Fetal electrocardiogram, Convolutional neural network, Short-time Fourier transform}
}

@article{10.1155/2021/4513610,
author = {Chen, Ling-qing and Wu, Mei-ting and Pan, Li-fang and Zheng, Ru-bin and Liu, KunHong},
title = {Grade Prediction in Blended Learning Using Multisource Data},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4513610},
doi = {10.1155/2021/4513610},
abstract = {Today, blended learning is widely carried out in many colleges. Different online learning platforms have accumulated a large number of fine granularity records of students’ learning behavior, which provides us with an excellent opportunity to analyze students’ learning behavior. In this paper, based on the behavior log data in four consecutive years of blended learning in a college’s programming course, we propose a novel multiclassification frame to predict students’ learning outcomes. First, the data obtained from diverse platforms, i.e., MOOC, Cnblogs, Programming Teaching Assistant (PTA) system, and Rain Classroom, are integrated and preprocessed. Second, a novel error-correcting output codes (ECOC) multiclassification framework, based on genetic algorithm (GA) and ternary bitwise calculator, is designed to effectively predict the grade levels of students by optimizing the code-matrix, feature subset, and binary classifiers of ECOC. Experimental results show that the proposed algorithm in this paper significantly outperforms other alternatives in predicting students’ grades. In addition, the performance of the algorithm can be further improved by adding the grades of prerequisite courses.},
journal = {Sci. Program.},
month = jan,
numpages = {15}
}

@inproceedings{10.5555/3304222.3304357,
author = {Liu, Cao and He, Shizhu and Liu, Kang and Zhao, Jun},
title = {Curriculum learning for natural answer generation},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpora. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpora could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-art, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {4223–4229},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@article{10.1016/j.compag.2018.12.010,
author = {Alugubelly, Mamatha and Polepalli, Krishna Reddy and Gade, Sreenivas and Ninomiya, Seishi},
title = {Analysis of similar weather conditions to improve reuse in weather-based decision support systems},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {157},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2018.12.010},
doi = {10.1016/j.compag.2018.12.010},
journal = {Comput. Electron. Agric.},
month = feb,
pages = {154–165},
numpages = {12},
keywords = {Data analysis, Reuse, Decision support systems, IT for agriculture, Agro-informatics, Crop management, Agrometeorology}
}

@article{10.1007/s10009-017-0466-1,
author = {Chadli, Mounir and Kim, Jin H. and Larsen, Kim G. and Legay, Axel and Naujokat, Stefan and Steffen, Bernhard and Traonouez, Louis-Marie},
title = {High-level frameworks for the specification and verification of scheduling problems},
year = {2018},
issue_date = {August    2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-017-0466-1},
doi = {10.1007/s10009-017-0466-1},
abstract = {Over the years, schedulability of Cyber-Physical Systems (CPS) has mainly been performed by analytical methods. These techniques are known to be effective but limited to a few classes of scheduling policies. In a series of recent work, we have shown that schedulability analysis of CPS could be performed with a model-based approach and extensions of verification tools such as UPPAAL. One of our main contributions has been to show that such models are flexible enough to embed various types of scheduling policies, which goes beyond those in the scope of analytical tools. However, the specification of scheduling problems with model-based approaches requires a substantial modeling effort, and a deep understanding of the techniques employed in order to understand their results. In this paper we propose simplicity-driven high-level specification and verification frameworks for various scheduling problems. These frameworks consist of graphical and user-friendly languages for describing scheduling problems. The high-level specifications are then automatically translated to formal models, and results are transformed back into the comprehensible model view. To construct these frameworks we exploit a meta-modeling approach based on the tool generator Cinco . Additionally we propose in this paper two new techniques for scheduling analysis. The first performs runtime monitoring using the CUSUM algorithm to detect alarming change in the system. The second performs optimization using efficient statistical techniques. We illustrate our frameworks and techniques on two case studies.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = aug,
pages = {397–422},
numpages = {26},
keywords = {Energy, Formal methods, Hierarchical scheduling, High-level language, Meta-modeling, Scheduling, Statistical model-checking}
}

@inproceedings{10.5555/3298483.3298512,
author = {Fan, Yanbo and He, Ran and Liang, Jian and Hu, Baogang},
title = {Self-paced learning: an implicit regularization perspective},
year = {2017},
publisher = {AAAI Press},
abstract = {Self-paced learning (SPL) mimics the cognitive mechanism of humans and animals that gradually learns from easy to hard samples. One key issue in SPL is to obtain better weighting strategy that is determined by the minimizer function. Existing methods usually pursue this by artificially designing the explicit form of SPL regularizer. In this paper, we study a group of new regularizer (named self-paced implicit regularizer) that is deduced from robust loss function. Based on the convex conjugacy theory, the minimizer function for self-paced implicit regularizer can be directly learned from the latent loss function, while the analytic form of the regularizer can be even unknown. A general framework (named SPL-IR) for SPL is developed accordingly. We demonstrate that the learning procedure of SPL-IR is associated with latent robust loss functions, thus can provide some theoretical insights for its working mechanism. We further analyze the relation between SPL-IR and half-quadratic optimization and provide a group of self-paced implicit regularizer. Finally, we implement SPL-IR to both supervised and unsupervised tasks, and experimental results corroborate our ideas and demonstrate the correctness and effectiveness of implicit regularizers.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {1877–1883},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.1016/j.ins.2021.06.013,
author = {Yang, Guoli and Kang, Yuanji and Zhu, Xianqiang and Zhu, Cheng and Xiao, Gaoxi},
title = {Info2vec: An aggregative representation method in multi-layer and heterogeneous networks},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {574},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.06.013},
doi = {10.1016/j.ins.2021.06.013},
journal = {Inf. Sci.},
month = oct,
pages = {444–460},
numpages = {17},
keywords = {Multi-layer networks, Representation learning, Cyberspace}
}

@article{10.1007/s10586-017-1108-9,
author = {Ilavarasi, A. K. and Sathiyabhama, B.},
title = {An evolutionary feature set decomposition based anonymization for classification workloads: Privacy Preserving Data Mining},
year = {2017},
issue_date = {Dec 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-017-1108-9},
doi = {10.1007/s10586-017-1108-9},
abstract = {Privacy has become an important concern while publishing micro data about a population. The emerging area called privacy preserving data mining (PPDM) focus on individual privacy without compromising data mining results. An adversarial exploitation of published data poses a risk of information disclosure about individuals. On the other hand, imposing privacy constraints on the data results in substantial information loss and compromises the legitimate data analysis. Motivated by the increasing growth of PPDM algorithms, we first investigate the privacy implications and the crosscutting issues between privacy versus utility of data. We present a privacy model that embeds the anonymization procedure in to a learning algorithm and this has mitigated the additional overheads imposed on data mining tasks. Our primary concern about PPDM is that the utility of data should not be compromised by the transformation applied. Different data mining classification workloads are analyzed with the proposed anonymization procedure for any side effects incurred. It is shown empirically that classification accuracy obtained for most of the datasets outperforms the results obtained with original dataset.},
journal = {Cluster Computing},
month = dec,
pages = {3515–3525},
numpages = {11},
keywords = {Anonymization, Classification, Data mining, Decomposition, Evolutionary partitioning, Privacy}
}

@article{10.1016/j.knosys.2017.02.020,
author = {Prez-Ortiz, M. and Gutirrez, P.A. and Aylln-Tern, M.D. and Heaton, N. and Ciria, R. and Briceo, J. and Hervs-Martnez, C.},
title = {Synthetic semi-supervised learning in imbalanced domains},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.020},
doi = {10.1016/j.knosys.2017.02.020},
abstract = {Liver transplantation is a promising and widely-accepted treatment for patients with terminal liver disease. However, transplantation is restricted by the lack of suitable donors, resulting in significant waiting list deaths. This paper proposes a novel donor-recipient allocation system that uses machine learning to predict graft survival after transplantation using a dataset comprised of donor-recipient pairs from the Kings College Hospital (United Kingdom). The main novelty of the system is that it tackles the imbalanced nature of the dataset by considering semi-supervised learning, analysing its potential for obtaining more robust and equitable models in liver transplantation. We propose two different sources of unsupervised data for this specific problem (recent transplants and virtual donor-recipient pairs) and two methods for using these data during model construction (a semi-supervised algorithm and a label propagation scheme). The virtual pairs and the label propagation method are shown to alleviate the imbalanced distribution. The results of our experiments show that the use of synthetic and real unsupervised information helps to improve and stabilise the performance of the model and leads to fairer decisions with respect to the use of only supervised data. Moreover, the best model is combined with the Model for End-stage Liver Disease score (MELD), which is at the moment the most popular assignation methodology worldwide. By doing this, our decision-support system considers both the compatibility of the donor and the recipient (by our prediction system) and the recipient severity (via the MELD score), supporting then the principles of fairness and benefit.},
journal = {Know.-Based Syst.},
month = may,
pages = {75–87},
numpages = {13},
keywords = {Imbalanced classification, Liver transplantation, Machine learning, Semi-supervised learning, Support vector machines, Survival analysis, Transplant recipient}
}

@inproceedings{10.1109/ICSE-NIER52604.2021.00026,
author = {Shahin, Ramy},
title = {Towards modal software engineering},
year = {2021},
isbn = {9780738133249},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER52604.2021.00026},
doi = {10.1109/ICSE-NIER52604.2021.00026},
abstract = {In this paper we introduce the notion of Modal Software Engineering: automatically turning sequential, deterministic programs into semantically equivalent programs efficiently operating on inputs coming from multiple overlapping worlds. We are drawing an analogy between modal logics, and software application domains where multiple sets of inputs (multiple worlds) need to be processed efficiently. Typically those sets highly overlap, so processing them independently would involve a lot of redundancy, resulting in lower performance, and in many cases intractability. Three application domains are presented: reasoning about feature-based variability of Software Product Lines (SPLs), probabilistic programming, and approximate programming.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {86–90},
numpages = {5},
location = {Virtual Event, Spain},
series = {ICSE-NIER '21}
}

@inproceedings{10.1145/2384616.2384673,
author = {K\"{a}stner, Christian and Ostermann, Klaus and Erdweg, Sebastian},
title = {A variability-aware module system},
year = {2012},
isbn = {9781450315616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384616.2384673},
doi = {10.1145/2384616.2384673},
abstract = {Module systems enable a divide and conquer strategy to software development. To implement compile-time variability in software product lines, modules can be composed in different combinations. However, this way, variability dictates a dominant decomposition. As an alternative, we introduce a variability-aware module system that supports compile-time variability inside a module and its interface. So, each module can be considered a product line that can be type checked in isolation. Variability can crosscut multiple modules. The module system breaks with the antimodular tradition of a global variability model in product-line development and provides a path toward software ecosystems and product lines of product lines developed in an open fashion. We discuss the design and implementation of such a module system on a core calculus and provide an implementation for C as part of the TypeChef project. Our implementation supports variability inside modules from #ifdef preprocessor directives and variable linking at the composition level. With our implementation, we type check all configurations of all modules of the open source product line Busybox with 811~compile-time options, perform linker check of all configurations, and report found type and linker errors -- without resorting to a brute-force strategy.},
booktitle = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications},
pages = {773–792},
numpages = {20},
keywords = {#ifdef, C, composition, conditional compilation, linker, module system, preprocessor, software product lines, variability},
location = {Tucson, Arizona, USA},
series = {OOPSLA '12}
}

@inproceedings{10.1145/3437359.3465596,
author = {Wu, Tsai-Wei and Lien Harrell, Stephen and Lentner, Geoffrey and Younts, Alex and Weekly, Sam and Mertes, Zoey and Maji, Amiya and Smith, Preston and Zhu, Xiao},
title = {Defining Performance of Scientific Application Workloads on the AMD Milan Platform},
year = {2021},
isbn = {9781450382922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437359.3465596},
doi = {10.1145/3437359.3465596},
abstract = {Understanding the capabilities of new architectures is key to informing system purchases and good long-term ROI (Return of Investment) for cluster installations. The newest AMD architecture, Milan, has become available first on Microsoft Azure and we use this early access to measure the performance of this 3rd Generation AMD EPYC processor. In this paper single node performance is gathered for seven popular scientific applications and benchmark test-suites. Quantitative comparisons are carried out between two independent platforms, Milan and its architectural predecessor Rome, for performance evaluations. Our results have shown that Milan architecture have improved performance and met our projections.},
booktitle = {Practice and Experience in Advanced Research Computing 2021: Evolution Across All Dimensions},
articleno = {32},
numpages = {4},
keywords = {HPC, architecture, benchmark, performance, scientific application},
location = {Boston, MA, USA},
series = {PEARC '21}
}

@inproceedings{10.1007/978-3-319-29339-4_24,
author = {Leottau, David L. and Ruiz-del-Solar, Javier and MacAlpine, Patrick and Stone, Peter},
title = {A Study of Layered Learning Strategies Applied to Individual Behaviors in Robot Soccer},
year = {2015},
isbn = {978-3-319-29338-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-29339-4_24},
doi = {10.1007/978-3-319-29339-4_24},
abstract = {Hierarchical task decomposition strategies allow robots and agents in general to address complex decision-making tasks. Layered learning is a hierarchical machine learning paradigm where a complex behavior is learned from a series of incrementally trained sub-tasks. This paper describes how layered learning can be applied to design individual behaviors in the context of soccer robotics. Three different layered learning strategies are implemented and analyzed using a ball-dribbling behavior as a case study. Performance indices for evaluating dribbling speed and ball-control are defined and measured. Experimental results validate the usefulness of the implemented layered learning strategies showing a trade-off between performance and learning speed.},
booktitle = {RoboCup 2015: Robot World Cup XIX},
pages = {290–302},
numpages = {13},
keywords = {Reinforcement learning, Layered learning, Machine learning, Soccer robotics, Biped robot, NAO, Behavior, Dribbling, Fuzzy logic},
location = {Hefei, China}
}

@inproceedings{10.5555/3367243.3367420,
author = {Kim, Yejin and Kim, Kwangseob and Park, Chanyoung and Yu, Hwanjo},
title = {Sequential and diverse recommendation with long tail},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Sequential recommendation is a task that learns a temporal dynamic of a user behaviour in sequential data and predicts items that a user would like afterward. However, diversity has been rarely emphasized in the context of sequential recommendation. Sequential and diverse recommendation must learn temporal preference on diverse items as well as on general items. Thus, we propose a sequential and diverse recommendation model that predicts a ranked list containing general items and also diverse items without compromising significant accuracy. To learn temporal preference on diverse items as well as on general items, we cluster and relocate consumed long tail items to make a pseudo ground truth for diverse items and learn the preference on long tail using recurrent neural network, which enables us to directly learn a ranking function. Extensive online and offline experiments deployed on a commercial platform demonstrate that our models significantly increase diversity while preserving accuracy compared to the state-of-the-art sequential recommendation model, and consequently our models improve user satisfaction.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {2740–2746},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

@article{10.1007/s10951-015-0449-6,
author = {Xu, Shubin and Bean, James C.},
title = {Scheduling parallel-machine batch operations to maximize on-time delivery performance},
year = {2016},
issue_date = {October   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {5},
issn = {1094-6136},
url = {https://doi.org/10.1007/s10951-015-0449-6},
doi = {10.1007/s10951-015-0449-6},
abstract = {In this paper we study the problem of minimizing total weighted tardiness, a proxy for maximizing on-time delivery performance, on parallel nonidentical batch processing machines. We first formulate the (primal) problem as a nonlinear integer programming model. We then show that the primal problem can be solved exactly by solving a corresponding dual problem with a nonlinear relaxation. Since both the primal and the dual problems are NP-hard, we use genetic algorithms, based on random keys and multiple choice encodings, to heuristically solve them. We find that the genetic algorithms consistently outperform a standard mathematical programming package in terms of solution quality and computation time. We also compare the smaller problem instances to a breadth-first tree search algorithm that gives evidence of the quality of the solutions.},
journal = {J. of Scheduling},
month = oct,
pages = {583–600},
numpages = {18},
keywords = {Batching, Genetic algorithms, Nonlinear relaxation, Optimal and approximate algorithms, Parallel-machine scheduling, Total weighted tardiness}
}

@inproceedings{10.5555/3016100.3016151,
author = {Li, Hao and Gong, Maoguo and Meng, Deyu and Miao, Qiguang},
title = {Multi-objective self-paced learning},
year = {2016},
publisher = {AAAI Press},
abstract = {Current self-paced learning (SPL) regimes adopt the greedy strategy to obtain the solution with a gradually increasing pace parameter while where to optimally terminate this increasing process is difficult to determine. Besides, most SPL implementations are very sensitive to initialization and short of a theoretical result to clarify where SPL converges to with pace parameter increasing. In this paper, we propose a novel multi-objective self-paced learning (MOSPL) method to address these issues. Specifically, we decompose the objective functions as two terms, including the loss and the self-paced regularizer, respectively, and treat the problem as the compromise between these two objectives. This naturally reformulates the SPL problem as a standard multi-objective issue. A multi-objective evolutionary algorithm is used to optimize the two objectives simultaneously to facilitate the rational selection of a proper pace parameter. The proposed technique is capable of ameliorating a set of solutions with respect to a range of pace parameters through finely compromising these solutions inbetween, and making them perform robustly even under bad initialization. A good solution can then be naturally achieved from these solutions by making use of some off-the-shelf tools in multi-objective optimization. Experimental results on matrix factorization and action recognition demonstrate the superiority of the proposed method against the existing issues in current SPL research.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {1802–1808},
numpages = {7},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@article{10.1016/j.jss.2019.110428,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Chen, Tao and Kazman, Rick},
title = {Run-time evaluation of architectures: A case study of diversification in IoT},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110428},
doi = {10.1016/j.jss.2019.110428},
journal = {J. Syst. Softw.},
month = jan,
numpages = {28},
keywords = {Run-time architecture evaluation, Runtime architecture evaluation, Software architectures for dynamic environments, Internet of things, IoT, Design diversity}
}

@article{10.1007/s10515-019-00253-7,
author = {Angerer, Florian and Grimmer, Andreas and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Change impact analysis for maintenance and evolution of variable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00253-7},
doi = {10.1007/s10515-019-00253-7},
abstract = {Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This article presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining the possibly impacted source code elements when changing the source code of a product family. The approach also supports engineers, who are adapting the code of specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanisms, it is more precise than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. We report evaluation results on the benefit and performance of the approach using industrial product lines.},
journal = {Automated Software Engg.},
month = jun,
pages = {417–461},
numpages = {45},
keywords = {Change impact analysis, Maintenance, Program analysis, Variability}
}

@inproceedings{10.1007/978-3-030-32239-7_52,
author = {Xie, Yutong and Lu, Hao and Zhang, Jianpeng and Shen, Chunhua and Xia, Yong},
title = {Deep Segmentation-Emendation Model for Gland Instance Segmentation},
year = {2019},
isbn = {978-3-030-32238-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32239-7_52},
doi = {10.1007/978-3-030-32239-7_52},
abstract = {Accurate and automated gland instance segmentation on histology microscopy images can assist pathologists to diagnose the malignancy degree of colorectal adenocarcinoma. To address this problem, many deep convolutional neural network (DCNN) based methods have been proposed, most of which aim to generate better segmentation by improving the model structure and loss function. Few of them, however, focus on further emendating the inferred predictions, thus missing a chance to refine the obtained segmentation results. In this paper, we propose the deep segmentation-emendation (DSE) model for gland instance segmentation. This model consists of a segmentation network (Seg-Net) and an emendation network (Eme-Net). The Seg-Net is dedicated to generating segmentation results, and the Eme-Net learns to predict the inconsistency between the ground truth and the segmentation results generated by Seg-Net. The predictions made by Eme-Net can in turn be used to refine the segmentation result. We evaluated our DSE model against five recent deep learning models on the 2015 MICCAI Gland Segmentation challenge (GlaS) dataset and against two deep learning models on the colorectal adenocarcinoma (CRAG) dataset. Our results indicate that using Eme-Net results in significant improvement in segmentation accuracy, and the proposed DSE model is able to substantially outperform all the rest models in gland instance segmentation on both datasets.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part I},
pages = {469–477},
numpages = {9},
location = {Shenzhen, China}
}

@article{10.1145/3480139,
author = {Chemistruck, Mike and Allen, Andrew and Snyder, John and Raghuvanshi, Nikunj},
title = {Efficient acoustic perception for virtual AI agents},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3480139},
doi = {10.1145/3480139},
abstract = {We model acoustic perception in AI agents efficiently within complex scenes with many sound events. The key idea is to employ perceptual parameters that capture how each sound event propagates through the scene to the agent's location. This naturally conforms virtual perception to human. We propose a simplified auditory masking model that limits localization capability in the presence of distracting sounds. We show that anisotropic reflections as well as the initial sound serve as useful localization cues. Our system is simple, fast, and modular and obtains natural results in our tests, letting agents navigate through passageways and portals by sound alone, and anticipate or track occluded but audible targets. Source code is provided.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = sep,
articleno = {43},
numpages = {13},
keywords = {NPC AI, acoustics, game AI, localization, masking, perception, sound propagation, virtual agents}
}

@inproceedings{10.1145/3501409.3501693,
author = {Yu, Shengzhao and Lei, Ming and Zhan, Yuqi},
title = {Home Smart Fitness System Integrating Fitness Program and Product Design},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501693},
doi = {10.1145/3501409.3501693},
abstract = {Under the influence of COVID-19, the time of the public spent at home increased greatly, during which the consumption of home decoration, sports and fitness increased significantly. Under the epidemic, the public can not participate in centralized fitness and weight loss exercises, and mobile-based fitness apps and home fitness equipment have become the focus of attention of the home-based fitness population during the outbreak of COVID-19. The purpose of this study is to propose the product interaction and design direction of intelligent fitness system in home application scenarios under the concept of hybrid design, in order to avoid the blind fitness of individuals based on fitness applications and enhance the intelligent fitness in the home environment to bring more scientific guidance and richer experience. The focus of this paper is to establish a fitness system that integrates scientific fitness guidance, online fitness intelligent experience, fitness social contact and fitness products into an innovative design concept of the industry. This concept is based on a survey of family fitness needs and future market demand, as well as case studies.The system consists of application carrier module, exercise prescription database, fitness APP and interactive experience module. Furniture provides the possibility for the application carrier of the system, and the huge sports prescription database provides suitable exercise prescriptions for the users to choose according to their fitness environment. Mobile app is the carrier of connecting users and intelligent fitness system, and the interactive experience module of mobile APP will bring functions of virtual fitness coach and social fitness game.In the future, with big data, artificial intelligence technologies and the concept of hybrid design, new smart home fitness system products will become possible.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1610–1616},
numpages = {7},
keywords = {Design trends, Home Smart fitness system, Hybrid design, Insert Fitness App},
location = {Xiamen, China},
series = {EITCE '21}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00107,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-box performance-influence models: a profiling and learning approach (replication package)},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00107},
doi = {10.1109/ICSE-Companion52605.2021.00107},
abstract = {These artifacts refer to the study and implementation of the paper 'White-Box Performance-Influence Models: A Profiling and Learning Approach'. In this document, we describe the idea and process of how to build white-box performance models for configurable software systems. Specifically, we describe the general steps and tools that we have used to implement our approach, the data we have obtained, and the evaluation setup. We further list the available artifacts, such as raw measurements, configurations, and scripts at our software heritage repository.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {232–233},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@article{10.1109/TSE.2002.1019479,
author = {Dobrica, Liliana and Niemel\"{a}, Eila},
title = {A survey on software architecture analysis methods},
year = {2002},
issue_date = {July 2002},
publisher = {IEEE Press},
volume = {28},
number = {7},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2002.1019479},
doi = {10.1109/TSE.2002.1019479},
abstract = {The purpose of the architecture evaluation of a software system is to analyze the architecture to identify potential risks and to verify that the quality requirements have been addressed in the design. This survey shows the state of the research at this moment, in this domain, by presenting and discussing eight of the most representative architecture analysis methods. The selection of the studied methods tries to cover as many particular views of objective reflections as possible to be derived from the general goal. The role of the discussion is to offer guidelines related to the use of the most suitable method for an architecture assessment process. We will concentrate on discovering similarities and differences between these eight available methods by making classifications, comparision and appropriateness studies.},
journal = {IEEE Trans. Softw. Eng.},
month = jul,
pages = {638–653},
numpages = {16},
keywords = {analysis techniques and methods, quality attributes, scenarios, software architecture}
}

@article{10.1155/2021/9956244,
author = {Li, Lei and Zhu, Yuquan and Cai, Tao and Niu, Dejiao and Shi, Huaji and Zou, Tingting and Huang, Chenxi},
title = {A Temporal Pool Learning Algorithm Based on Location Awareness},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/9956244},
doi = {10.1155/2021/9956244},
abstract = {Hierarchical Temporal Memory is a new type of artificial neural network model, which imitates the structure and information processing flow of the human brain. Hierarchical Temporal Memory has strong adaptability and fast learning ability and becomes a hot spot in current research. Hierarchical Temporal Memory obtains and saves the temporal characteristics of input sequences by the temporal pool learning algorithm. However, the current algorithm has some problems such as low learning efficiency and poor learning effect when learning time series data. In this paper, a temporal pool learning algorithm based on location awareness is proposed. The cell selection rules based on location awareness and the dendritic updating rules based on adjacent inputs are designed to improve the learning efficiency and effect of the algorithm. Through the algorithm prototype, three different datasets are used to test and analyze the algorithm performance. The experimental results verify that the algorithm can quickly obtain the complete characteristics of the input sequence. No matter whether there are similar segments in the sequence, the proposed algorithm has higher prediction recall and precision than the existing algorithms.},
journal = {Sci. Program.},
month = jan,
numpages = {12}
}

@article{10.1016/j.eswa.2019.06.060,
author = {Karmy, Juan Pablo and Maldonado, Sebasti\'{a}n},
title = {Hierarchical time series forecasting via Support Vector Regression in the European Travel Retail Industry},
year = {2019},
issue_date = {Dec 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.06.060},
doi = {10.1016/j.eswa.2019.06.060},
journal = {Expert Syst. Appl.},
month = dec,
pages = {59–73},
numpages = {15},
keywords = {Hierarchical time series, Support Vector Regression, Time series analysis, Sales forecasting}
}

@article{10.1016/j.rcim.2019.101850,
author = {Laili, Yuanjun and Lin, Sisi and Tang, Diyin},
title = {Multi-phase integrated scheduling of hybrid tasks in cloud manufacturing environment},
year = {2020},
issue_date = {Feb 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {61},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2019.101850},
doi = {10.1016/j.rcim.2019.101850},
journal = {Robot. Comput.-Integr. Manuf.},
month = feb,
numpages = {18},
keywords = {Cloud manufacturing, Integrated scheduling, Multi-objective optimization, Production line scheduling, Supplier and process selection, Order priority assignment}
}

@inproceedings{10.5555/3172077.3172140,
author = {Han, Longfei and Zhang, Dingwen and Huang, Dong and Chang, Xiaojun and Ren, Jun and Luo, Senlin and Han, Junwei},
title = {Self-paced mixture of regressions},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Mixture of regressions (MoR) is the well-established and effective approach to model discontinuous and heterogeneous data in regression problems. Existing MoR approaches assume smooth joint distribution for its good anlaytic properties. However, such assumption makes existing MoR very sensitive to intra-component outliers (the noisy training data residing in certain components) and the inter-component imbalance (the different amounts of training data in different components). In this paper, we make the earliest effort on Self-paced Learning (SPL) in MoR, i.e., Self-paced mixture of regressions (SPMoR) model. We propose a novel selfpaced regularizer based on the Exclusive LASSO, which improves inter-component balance of training data. As a robust learning regime, SPL pursues confidence sample reasoning. To demonstrate the effectiveness of SPMoR, we conducted experiments on both the sythetic examples and real-world applications to age estimation and glucose estimation. The results show that SPMoR outperforms the state-of-the-arts methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {1816–1822},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/3340531.3411866,
author = {Rozemberczki, Benedek and Sarkar, Rik},
title = {Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411866},
doi = {10.1145/3340531.3411866},
abstract = {In this paper, we propose a flexible notion of characteristic functions defined on graph vertices to describe the distribution of vertex features at multiple scales. We introduce FEATHER, a computationally efficient algorithm to calculate a specific variant of these characteristic functions where the probability weights of the characteristic function are defined as the transition probabilities of random walks. We argue that features extracted by this procedure are useful for node level machine learning tasks. We discuss the pooling of these node representations, resulting in compact descriptors of graphs that can serve as features for graph classification algorithms. We analytically prove that FEATHER describes isomorphic graphs with the same representation and exhibits robustness to data corruption. Using the node feature characteristic functions we define parametric models where evaluation points of the functions are learned parameters of supervised classifiers. Experiments on real world large datasets show that our proposed algorithm creates high quality representations, performs transfer learning efficiently, exhibits robustness to hyperparameter changes and scales linearly with the input size.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {1325–1334},
numpages = {10},
keywords = {graph classification, graph embedding, graph fingerprint, graph neural network, node classification, node embedding},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3377024.3377035,
author = {Kirchhof, J\"{o}rg Christian and Rumpe, Bernhard and Schmalzing, David and Wortmann, Andreas},
title = {Structurally evolving component-port-connector architectures of centrally controlled systems},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377035},
doi = {10.1145/3377024.3377035},
abstract = {The increasing complexity of software variants demands for variation management techniques suitable for industrial practice. As "clone-and-own" with subsequent manual evolution still is a popular method to create software variants, this leads to product lines that are difficult to maintain and evolve and can produce conflicts when changes occur in both, product line and variant. Where general approaches to differencing and merging handcrafted changes to products perform suboptimally, respecting assumptions on the structure of the architecture can reduce the differencing search space and yield better results. We present a novel method for differencing and merging hierarchical component-port-connector architectures based on the Focus calculus. It leverages assumptions on the distribution of components to facilitate calculating differences between architecture versions and deriving delta bundles to update software products to changes in the underlying product line. Through a (preliminary) survey with 27 participants, we compared the results of our method with the results of manually differencing. The survey showed that this method yields deltas and merge results that are considered correct by the participants. Overall, we found that including assumptions about the architectural style and that grouping related deltas into compact bundles can greatly facilitate merging manually created and evolved products back into their underlying product lines.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {23},
numpages = {9},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1016/j.patcog.2016.11.018,
author = {Ma, Xiaolong and Zhu, Xiatian and Gong, Shaogang and Xie, Xudong and Hu, Jianming and Lam, Kin-Man and Zhong, Yisheng},
title = {Person re-identification by unsupervised video matching},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {65},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.11.018},
doi = {10.1016/j.patcog.2016.11.018},
journal = {Pattern Recogn.},
month = may,
pages = {197–210},
numpages = {14},
keywords = {Person re-identification, Action recognition, Gait recognition, Video matching, Temporal sequence matching, Spatio-temporal pyramids, Time shift}
}

@inproceedings{10.1007/978-3-030-91825-5_3,
author = {Das, Susmoy and Sharma, Arpit},
title = {State Space Minimization Preserving Embeddings for Continuous-Time Markov Chains},
year = {2021},
isbn = {978-3-030-91824-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91825-5_3},
doi = {10.1007/978-3-030-91825-5_3},
abstract = {This paper defines embeddings which allow one to construct an action labeled continuous-time Markov chain (ACTMC) from a state labeled continuous-time Markov chain (SCTMC) and vice versa. We prove that these embeddings preserve strong forward bisimulation and strong backward bisimulation. We define weak backward bisimulation for ACTMCs and SCTMCs, and also prove that our embeddings preserve both weak forward and weak backward bisimulation. Next, we define the invertibility criteria and the inverse of these embeddings. Finally, we prove that an ACTMC can be minimized by minimizing its embedded model, i.e. SCTMC and taking the inverse of the embedding. Similarly, we prove that an SCTMC can be minimized by minimizing its embedded model, i.e. ACTMC and taking the inverse of the embedding.},
booktitle = {Performance Engineering and Stochastic Modeling: 17th European Workshop, EPEW 2021, and 26th International Conference, ASMTA 2021, Virtual Event, December 9–10 and December 13–14, 2021, Proceedings},
pages = {44–61},
numpages = {18},
keywords = {Markov chain, Behavioral equivalence, Bisimulation equivalence, Stochastic systems, Embeddings},
location = {Milan, Italy}
}

@article{10.3233/JIFS-169020,
author = {Wang, Ting and Xu, Rui and Han, Xianhua and Chen, Yen-Wei and Ishizaki, Yoshitomo and Miyamoto, Masaru and Hattori, Tomohito and Xiao, Zheng and Li, Kenli},
title = {A principal component analysis based method to automatically inspect wear of throw-away tips},
year = {2016},
issue_date = {2016},
publisher = {IOS Press},
address = {NLD},
volume = {31},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169020},
doi = {10.3233/JIFS-169020},
abstract = {The automatic inspection of throw-away tips is very important for quality control in precision cutting. We proposed an image processing based method for automatic inspection of the processing wear of throw-away tips. After image denoising, the proposed method utilized image-patch based principal component analysis method to enhance the cutting worn region while suppress the background region. Then the enhanced worn region was automatically segmented by a simple thresholding method followed by post-processing. The area of the segmented worn region was used as a measure of cutting wear degree. We collected three datasets of time-series images that recorded the processing of throw-away tips on a product line. One dataset was used to choose optimal parameters of the proposed method, and the other two datasets were used for evaluate its performances. Experimental results showed that the proposed method was able to inspect the cutting wear with high accuracy. Additionally, it was also showed that the proposed method outperformed the conventional thresholding based method.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {903–913},
numpages = {11},
keywords = {Principal component analysis, segmentation, worn region, throw-away tips, automatic inspection}
}

@inproceedings{10.1007/978-3-030-72914-1_11,
author = {Kariyado, Yuta and Arevalo, Camilo and Villegas, Juli\'{a}n},
title = {Auralization of Three-Dimensional Cellular Automata},
year = {2021},
isbn = {978-3-030-72913-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-72914-1_11},
doi = {10.1007/978-3-030-72914-1_11},
abstract = {An auralization tool for exploring three-dimensional cellular automata is presented. This proof-of-concept allows the creation of a sound field comprising individual sound events associated with each cell in a three-dimensional grid. Each sound-event is spatialized depending on the orientation of the listener relative to the three-dimensional model. Users can listen to all cells simultaneously or in sequential slices at will. Conceived to be used as an immersive Virtual Reality (VR) scene, this software application also works as a desktop application for environments where the VR infrastructure is missing. Subjective evaluations indicate that the proposed sonification increases the perceived quality and immersability of the system with respect to a visualization-only system. No subjective differences between the sequential or simultaneous presentations were found.},
booktitle = {Artificial Intelligence in Music, Sound, Art and Design: 10th International Conference, EvoMUSART 2021, Held as Part of EvoStar 2021, Virtual Event, April 7–9, 2021, Proceedings},
pages = {161–170},
numpages = {10},
keywords = {Cellular automata, Game of life, Auralization}
}

@article{10.5555/2747013.2747140,
author = {Chen, Bihuan and Peng, Xin and Yu, Yijun and Zhao, Wenyun},
title = {Uncertainty handling in goal-driven self-optimization - Limiting the negative effect on adaptation},
year = {2014},
issue_date = {April 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {90},
number = {C},
issn = {0164-1212},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose techniques to handle contribution uncertainty and effect uncertainty in goal-driven self-optimization.We integrate these uncertainty handling techniques with preference uncertainty handling to a goal-driven self-optimization framework.We demonstrate the effectiveness of our approach with an evaluation on an online shopping system. Goal-driven self-optimization through feedback loops has shown effectiveness in reducing oscillating utilities due to a large number of uncertain factors in the runtime environments. However, such self-optimization is less satisfactory when there contains uncertainty in the predefined requirements goal models, such as imprecise contributions and unknown quality preferences, or during the switches of goal solutions, such as lack of understanding about the time for the adaptation actions to take effect. In this paper, we propose to handle such uncertainty in goal-driven self-optimization without interrupting the services. Taking the monitored quality values as the feedback, and the estimated earned value as the global indicator of self-optimization, our approach dynamically updates the quantitative contributions from alternative functionalities to quality requirements, tunes the preferences of relevant quality requirements, and determines a proper timing delay for the last adaptation action to take effect. After applying these runtime measures to limit the negative effect of the uncertainty in goal models and their suggested switches, an experimental study on a real-life online shopping system shows the improvements over goal-driven self-optimization approaches without uncertainty handling.},
journal = {J. Syst. Softw.},
month = apr,
pages = {114–127},
numpages = {14},
keywords = {Goal-driven self-optimization, Requirements goal models, Uncertainty}
}

@inproceedings{10.5555/2540128.2540554,
author = {Chen, Zhengzhang and Xie, Yusheng and Cheng, Yu and Zhang, Kunpeng and Agrawal, Ankit and Liao, Wei-Keng and Samatova, Nagiza F. and Choudhary, Alok},
title = {Forecast oriented classification of spatio-temporal extreme events},
year = {2013},
isbn = {9781577356332},
publisher = {AAAI Press},
abstract = {In complex dynamic systems, accurate forecasting of extreme events, such as hurricanes, is a highly underdetermined, yet very important sustainability problem. While physics-based models deserve their own merits, they often provide unreliable predictions for variables highly related to extreme events. In this paper, we propose a new supervised machine learning problem, which we call a forecast oriented classification of spatio-temporal extreme events. We formulate three important real-world extreme event classification tasks, including seasonal forecasting of (a) tropical cyclones in Northern Hemisphere, (b) hurricanes and landfalling hurricanes in North Atlantic, and (c) North African rainfall. Corresponding predictor and predictand data sets are constructed. These data present unique characteristics and challenges that could potentially motivate future Artificial Intelligent and Data Mining research.},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {2952–2954},
numpages = {3},
location = {Beijing, China},
series = {IJCAI '13}
}

@inproceedings{10.1145/3459637.3482458,
author = {Keramati, Mahsa and Zohrevand, Zahra and Gl\"{a}sser, Uwe},
title = {Norma: A Hybrid Feature Alignment for Class-Aware Unsupervised Domain Adaptation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482458},
doi = {10.1145/3459637.3482458},
abstract = {Unsupervised domain adaptation is the problem of transferring extracted knowledge from a labeled source domain to an unlabeled target domain. To achieve discriminative domain adaptation recent studies take advantage of target sample pseudo-labels to impose class-aware distribution alignment across the source and target domains. Still, they have some shortcomings such as making decisions based on inaccurate pseudo-labeled samples that mislead the adaptation process. In this paper, we propose a progressive deep feature alignment, called Norma, to tackle class-aware unsupervised domain adaptation for image classification by enforcing inter-class compactness and intra-class discrepancy through a hybrid learning process. To this end, Norma's optimization process is defined based on a novel triplet loss which not only addresses soft prototype alignment but also pushes away multiple negative centroids. Also, to extract maximum discriminative domain knowledge per iteration, we propose a joint positive and negative learning procedure along with an uncertainty-guided progressive pseudo-labeling on the basis of prototype-based clustering and conditional probability. Our experimental results on several benchmarks demonstrate that Norma outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {833–843},
numpages = {11},
keywords = {adversarial unsupervised domain adaptation, class-aware alignment, deep-metric learning, image classification, negative learning, pseudo-labeling, transfer learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1007/978-3-030-75765-6_49,
author = {Thanthriwatta, Thilina and Rosenblum, David S.},
title = {Instance Selection for Online Updating in Dynamic Recommender Environments},
year = {2021},
isbn = {978-3-030-75764-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-75765-6_49},
doi = {10.1007/978-3-030-75765-6_49},
abstract = {Online recommender systems continuously learn from user interactions that occur in a streaming manner. A fundamental challenge of online recommendation is to select important instances (i.e., user interactions) for model updates to achieve higher prediction accuracy while omitting noisy instances. In this paper, we study&nbsp;(1) how to select the best instances and&nbsp;(2) how to effectively utilize the selected instances in dynamic recommender environments. We present two instance selection strategies based on Self-Paced Learning and rating profiles. We integrate them with Factorization Machines to perform online updates. Moreover, we study the impact of contextual information in online updating. We conducted experiments on a real-world check-in dataset, which contains temporal contextual features. Empirical results demonstrate that ox ur instance selection strategies effectively balance the trade-off between prediction accuracy and efficiency.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 25th Pacific-Asia Conference, PAKDD 2021, Virtual Event, May 11–14, 2021, Proceedings, Part II},
pages = {612–624},
numpages = {13},
keywords = {Instance selection, Context-aware recommender systems, Online recommender systems}
}

@article{10.1016/j.imavis.2016.06.005,
author = {Leng, Mengjun and Moutafis, Panagiotis and Kakadiaris, Ioannis A.},
title = {Joint prototype and metric learning for image set classification: Application to video face identification},
year = {2017},
issue_date = {Feb 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {58},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.06.005},
doi = {10.1016/j.imavis.2016.06.005},
journal = {Image Vision Comput.},
month = feb,
pages = {204–213},
numpages = {10},
keywords = {Image set classification, Metric learning, Prototype learning, Video face recognition}
}

@article{10.1007/s10994-016-5570-z,
author = {Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
title = {A topological insight into restricted Boltzmann machines},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {104},
number = {2–3},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-016-5570-z},
doi = {10.1007/s10994-016-5570-z},
abstract = {Restricted Boltzmann Machines (RBMs) and models derived from them have been successfully used as basic building blocks in deep artificial neural networks for automatic features extraction, unsupervised weights initialization, but also as density estimators. Thus, their generative and discriminative capabilities, but also their computational time are instrumental to a wide range of applications. Our main contribution is to look at RBMs from a topological perspective, bringing insights from network science. Firstly, here we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which naturally have a small-world topology. Secondly, we demonstrate both on synthetic and real-world datasets that by constraining RBMs and GRBMs to a scale-free topology (while still considering local neighborhoods and data distribution), we reduce the number of weights that need to be computed by a few orders of magnitude, at virtually no loss in generative performance. Thirdly, we show that, for a fixed number of weights, our proposed sparse models (which by design have a higher number of hidden neurons) achieve better generative capabilities than standard fully connected RBMs and GRBMs (which by design have a smaller number of hidden neurons), at no additional computational costs.},
journal = {Mach. Learn.},
month = sep,
pages = {243–270},
numpages = {28},
keywords = {Complex networks, Deep learning, Scale-free networks, Small-world networks, Sparse restricted Boltzmann machines}
}

@article{10.1007/s10845-011-0521-9,
author = {Huang, Jing and S\"{u}er, G\"{u}rsel A. and Urs, Shravan B.},
title = {Genetic algorithm for rotary machine scheduling with dependent processing times},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {5},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-011-0521-9},
doi = {10.1007/s10845-011-0521-9},
abstract = {This paper focuses on scheduling a rotary injection molding machine with dependent processing times. The injection machine has n pairs of positions to process n pairs of shoes. It is rotated after every cycle time. Cycle time is the maximum injection time of the jobs currently loaded in the machine. Thus, for all practical purposes, the processing time of a job depends on the combination of the jobs currently assigned to the machine. The uncertainty of processing time makes this problem more complicated than traditional parallel machine scheduling problems. Additionally, since switching jobs leads to mold changes, set-up time is also included in the analysis. We develop a Sequential Genetic Algorithm (SGA) to identify the best schedule with regard to makespan. In this approach, multiple GA evolvers are connected by using a feeding strategy, where each GA evolver identifies the best schedule with minimum makespan for the corresponding product family. A multi-segment (product lines) chromosome representation is applied to represent the product line sequence as well as the job sequence within a product family. Furthermore, an adaptive feeding strategy is also proposed to improve results and reduce computation times. Besides SGA, we also improve the performance of a traditional heuristic procedure by proposing a minimum ΔIT heuristic approach. The experimentation is performed by using four experimental data sets with different demand patterns and nine data sets from a shoe manufacturing plant. The results indicate that our SGA provides better schedule with respect to makespan value, while heuristic procedures take insignificant time to obtain results. Another observation is that adaptive feeding strategy helps to find good results in a shorter time.},
journal = {J. Intell. Manuf.},
month = oct,
pages = {1931–1948},
numpages = {18},
keywords = {Adaptive feeding, Genetic algorithm, Heuristic procedure, Rotary machine scheduling}
}

@article{10.1145/3449356,
author = {Balakrishnan, Aravind and Lee, Jaeyoung and Gaurav, Ashish and Czarnecki, Krzysztof and Sedwards, Sean},
title = {Transfer Reinforcement Learning for Autonomous Driving: From WiseMove to WiseSim},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/3449356},
doi = {10.1145/3449356},
abstract = {Reinforcement learning (RL) is an attractive way to implement high-level decision-making policies for autonomous driving, but learning directly from a real vehicle or a high-fidelity simulator is variously infeasible. We therefore consider the problem of transfer reinforcement learning and study how a policy learned in a simple environment using WiseMove can be transferred to our high-fidelity simulator, WiseMove. WiseMove is a framework to study safety and other aspects of RL for autonomous driving. WiseMove accurately reproduces the dynamics and software stack of our real vehicle. We find that the accurately modelled perception errors in WiseMove contribute the most to the transfer problem. These errors, when even naively modelled in WiseMove, provide an RL policy that performs better in WiseMove than a hand-crafted rule-based policy. Applying domain randomization to the environment in WiseMove yields an even better policy. The final RL policy reduces the failures due to perception errors from 10% to 2.75%. We also observe that the RL policy has significantly less reliance on velocity compared to the rule-based policy, having learned that its measurement is unreliable.},
journal = {ACM Trans. Model. Comput. Simul.},
month = jul,
articleno = {15},
numpages = {26},
keywords = {Transfer reinforcement learning, autonomous driving, deep reinforcement learning, policy distillation}
}

@inproceedings{10.1145/3184558.3191523,
author = {Dalmia, Ayushi and J, Ganesh and Gupta, Manish},
title = {Towards Interpretation of Node Embeddings},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191523},
doi = {10.1145/3184558.3191523},
abstract = {Recently there have been a large number of studies on embedding large-scale information networks using low-dimensional, neighborhood and community aware node representations. Though the performance of these embedding models have been better than traditional methods for graph mining applications, little is known about what these representations encode, or why a particular node representation works better for certain tasks. Our work presented here constitutes the first step in decoding the black-box of vector embeddings of nodes by evaluating their effectiveness in encoding elementary properties of a node such as page rank, degree, closeness centrality, clustering coefficient, etc. We believe that a node representation is effective for an application only if it encodes the application-specific elementary properties of nodes. To unpack the elementary properties encoded in a node representation, we evaluate the representations on the accuracy with which they can model each of these properties. Our extensive study of three state-of-the-art node representation models (DeepWalk, node2vec and LINE) on four different tasks and six diverse graphs reveal that node2vec and LINE best encode the network properties of sparse and dense graphs respectively. We correlate the model performance obtained for elementary property prediction tasks with the high-level downstream applications such as link prediction and node classification, and visualize the task performance vector of each model to understand the semantic similarity between the embeddings learned by various models. Our first study of the node embedding models for outlier detection reveals that node2vec and DeepWalk identify outliers well for sparse and dense graphs respectively. Our analysis highlights that the proposed elementary property prediction tasks help in unearthing the important features responsible for the given node embedding model to perform well for a given downstream task. This understanding would facilitate in picking the right model for a given downstream task.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {945–952},
numpages = {8},
keywords = {graph representation, model interpretability, neural networks},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.1016/j.neucom.2014.06.096,
author = {Liu, Weifeng and Liu, Hongli and Tao, Dapeng and Wang, Yanjiang and Lu, Ke},
title = {Manifold regularized kernel logistic regression for web image annotation},
year = {2016},
issue_date = {Jan 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {172},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.06.096},
doi = {10.1016/j.neucom.2014.06.096},
journal = {Neurocomput.},
month = jan,
pages = {3–8},
numpages = {6},
keywords = {Manifold regularization, Kernel logistic regression, Laplacian eigenmaps, Semi-supervised learning, Image annotation}
}

@inproceedings{10.1145/2737182.2737190,
author = {Feitosa, Daniel and Ampatzoglou, Apostolos and Avgeriou, Paris and Nakagawa, Elisa Yumi},
title = {Investigating Quality Trade-offs in Open Source Critical Embedded Systems},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737182.2737190},
doi = {10.1145/2737182.2737190},
abstract = {During the development of Critical Embedded Systems (CES), quality attributes that are critical for them (e.g., correctness, security, etc.) must be guaranteed. However, this often leads to complex quality trade-offs, since non-critical qualities (e.g., reusability, understandability, etc.) may be compromised. In this study, we aim at empirically investigating the existence of quality trade-offs, on the implemented architecture, among versions of open source CESs, and compare them with those of systems from other application domains. The results of the study suggest that in CES, non-critical quality attributes are usually compromised in favor of critical quality attributes. On the contrary, we have not observed compromises of critical qualities in favor of non-critical ones in either CES or other application domains. Furthermore, quality trade-offs are more frequent among critical quality attributes, compared to trade-offs among non-critical quality attributes. Our study has implications for both practitioners when making trade-offs in practice, as well as researchers that investigate quality trade-offs.},
booktitle = {Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {113–122},
numpages = {10},
keywords = {embedded systems, quality trade-offs, software metrics},
location = {Montr\'{e}al, QC, Canada},
series = {QoSA '15}
}

@inbook{10.5555/3454287.3454826,
author = {Zhang, Jiong and Yu, Hsiang-Fu and Dhillon, Inderjit S.},
title = {AutoAssist: a framework to accelerate training of deep neural networks},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep Neural Networks (DNNs) have yielded superior performance in many contemporary applications. However, the gradient computation in a deep model with millions of instances leads to a lengthy training process even with modern GPU/TPU hardware acceleration. In this paper, we propose AutoAssist, a simple framework to accelerate training of a deep neural network. Typically, as the training procedure evolves, the amount of improvement by a stochastic gradient update varies dynamically with the choice of instances in the mini-batch. In AutoAssist, we utilize this fact and design an instance shrinking operation that is used to filter out instances with relatively low marginal improvement to the current model; thus the computationally intensive gradient computations are performed on informative instances as much as possible. Specifically, we train a very lightweight Assistant model jointly with the original deep network, which we refer to as the Boss. The Assistant model is designed to gauge the importance of a given instance with respect to the current Boss model such that the shrinking operation can be applied in the batch generator. With careful design, we train the Boss and Assistant in a non-blocking and asynchronous fashion such that overhead is minimal. To demonstrate the effectiveness of AutoAssist, we conduct experiments on two contemporary applications: image classification using ResNets with varied number of layers, and neural machine translation using LSTMs, ConvS2S and Transformer models. For each application, we verify that AutoAssist leads to significant reduction in training time; in particular, 30% to 40% of the total operation count can be reduced which leads to faster convergence and a corresponding decrease in training time.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {539},
numpages = {11}
}

@inproceedings{10.5555/3304222.3304408,
author = {Zhang, Yuanxing and Zhang, Yangbin and Bian, Kaigui and Li, Xiaoming},
title = {Towards reading comprehension for long documents},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Machine reading comprehension has gained attention from both industry and academia. It is a very challenging task that involves various domains such as language comprehension, knowledge inference, summarization, etc. Previous studies mainly focus on reading comprehension on short paragraphs, and these approaches fail to perform well on the documents. In this paper, we propose a hierarchical match attention model to instruct the machine to extract answers from a specific short span of passages for the long document reading comprehension (LDRC) task. The model takes advantages from hierarchical-LSTM to learn the paragraph-level representation, and implements the match mechanism (i.e., quantifying the relationship between two contexts) to find the most appropriate paragraph that includes the hint of answers. Then the task can be decoupled into reading comprehension task for short paragraph, such that the answer can be produced. Experiments on the modified SQuAD dataset show that our proposed model outperforms existing reading comprehension models by at least 20% regarding exact match (EM), F1 and the proportion of identified paragraphs which are exactly the short paragraphs where the original answers locate.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {4588–4594},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@article{10.1016/j.engappai.2011.05.007,
author = {Barros, Heitor and Silva, Alan and Costa, Evandro and Bittencourt, Ig Ibert and Holanda, Olavo and Sales, Leandro},
title = {Steps, techniques, and technologies for the development of intelligent applications based on Semantic Web Services: A case study in e-learning systems},
year = {2011},
issue_date = {December, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {24},
number = {8},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2011.05.007},
doi = {10.1016/j.engappai.2011.05.007},
abstract = {Semantic Web Services domain has gained special attention in academia and industry. It has been adopted as a promise to enable automation of all aspects of Web Services provision and uses, such as service creation, selection, discovery, composition, and invocation. However, the development of intelligent systems based on Semantic Web Services (SWS) is still a complex and time-consuming task, mainly with respect to the choice and integration of technologies. In this paper, we discuss some empirical issues associated with the development process for such systems and propose a systematic way for building intelligent applications based on SWS by providing the development process with steps, techniques and technologies. In addition, one experiment concerning the implementation of a real e-learning system using the proposed approach is described. The evaluation results from this experiment showed that our approach has been effective and relevant in terms of improvements in the development process of intelligent applications based on SWS.},
journal = {Eng. Appl. Artif. Intell.},
month = dec,
pages = {1355–1367},
numpages = {13},
keywords = {Grinv Middleware, Intelligent Tutoring System, Ontology, Semantic Web Services}
}

@article{10.3233/THC-218026,
author = {Zhou, Zhiming and Huang, Haihui and Liang, Yong},
title = {Cancer classification and biomarker selection via a penalized logsum network-based logistic regression model},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {29},
number = {S1},
issn = {0928-7329},
url = {https://doi.org/10.3233/THC-218026},
doi = {10.3233/THC-218026},
journal = {Technol. Health Care},
month = jan,
pages = {287–295},
numpages = {9},
keywords = {Regularization, gene selection, log-sum penalty, network-based knowledge}
}

@article{10.1145/264645.264658,
author = {Kieras, David E. and Wood, Scott D. and Meyer, David E.},
title = {Predictive engineering models based on the EPIC architecture for a multimodal high-performance human-computer interaction task},
year = {1997},
issue_date = {Sept. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/264645.264658},
doi = {10.1145/264645.264658},
abstract = {Engineering models of human performance permit some aspects of usability of interface designs to be predicted from an analysis of the task, and thus they can replace to some extent expensive user-testing data. We successfully predicted human performance in telephone operator tasks with engineering models constructed in the EPIC (Executive Process-Interactive Control) architecture for human information processing, which is especially suited for modeling multimodal, complex tasks, and has demonstrated success in other task domains. Several models were constructed on an a priori basis to represent different hypotheses about how operators coordinate their activities to produce rapid task performance. The models predicted  the total time with useful accuracy and clarified  some important properties of the task. The best model was based directly on the GOMS analysis of the task and made simple assumptions about the operator's task strategy, suggesting that EPIC models are a feasible approach to predicting performance in multimodal high-performance tasks.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
pages = {230–275},
numpages = {46},
keywords = {cognitive models, usability engineering}
}

@article{10.1007/s10916-018-0978-6,
author = {Rajkumar, S. and Muttan, S. and Sapthagirivasan, V. and Jaya, V. and Vignesh, S. S.},
title = {Development of Improved Software Intelligent System for Audiological Solutions},
year = {2018},
issue_date = {July      2018},
publisher = {Plenum Press},
address = {USA},
volume = {42},
number = {7},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-018-0978-6},
doi = {10.1007/s10916-018-0978-6},
abstract = {Of late, there has been an increase in hearing impairment cases and to provide the most advantageous solutions to them is an uphill task for audiologists. Significant difficulty faced by the audiologists is in effective programming of hearing aids to provide enhanced satisfaction to the users. The main aim of our study was to develop a software intelligent system (SIS): (i) to perform the required audiological investigations for finding the degree and type of hearing loss, and (ii) to suggest appropriate values of hearing aid parameters for enhancing the speech intelligibility and the satisfaction level among the hearing aid users. In this paper, we present a Neuro-Fuzzy based SIS to automatically predict and suggest the hearing-aid parameters such as gain values, compression ratio and threshold knee point, which are needed to be fixed for different octave frequencies of sound inputs during the hearing-aid trial. The test signals for audiological investigations are generated through the standard hardware present in a personal computer system and with the aid of a software algorithm. The proposed system was validated with 243 subjects' data collected at the Government General Hospital, Chennai, India. The calculated sensitivity, specificity and accuracy of the proposed audiometer incorporated in the SIS were 98.6%, 96.4 and 98.2%, respectively, by comparing its interpretations with those of the `gold standard' audiometers. Furthermore, 91% (221 of 243) of the hearing impaired subjects attained satisfaction in the first hearing aid trials itself with the gain values as recommended by the improved SIS. The proposed system reduced around 75% of the `trial and error' time spent by audiologists for enhancing satisfactory usage of the hearing aid. Hence, the proposed SIS could be used to find the degree and type of hearing loss and to recommend hearing aid parameters to provide optimal solutions to the hearing aid users.},
journal = {J. Med. Syst.},
month = jul,
pages = {1–12},
numpages = {12},
keywords = {Artificial intelligence, Compression ratio, Computerized audiometer, Digital in medical, Expert system, Hearing aid, Hearing loss, Neuro-Fuzzy, Prescriptive procedure, Software intelligent system, Sound pressure level, Speech discrimination score, Threshold knee point}
}

@inproceedings{10.1145/3448016.3457281,
author = {Zheng, Kaiping and Chen, Gang and Herschel, Melanie and Ngiam, Kee Yuan and Ooi, Beng Chin and Gao, Jinyang},
title = {PACE: Learning Effective Task Decomposition for Human-in-the-loop Healthcare Delivery},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457281},
doi = {10.1145/3448016.3457281},
abstract = {Human-in-the-loop data analysis involves both machine learning models and humans in analytic tasks. In healthcare applications, human-in-the-loop data analysis is crucial in that the model can handle "easy" tasks and hand over "hard" ones to medical experts for assistance and medical judgment, where easy tasks are the ones for which the model can provide high accuracy and hard tasks vice versa. In this process, how to decompose tasks in an effective manner is an important stage. To achieve task decomposition, classification with a reject option is a solution. However, existing studies either directly implement a reject option or dive into the theoretical details of the rejection mechanism. Different from such studies, we aim to optimize general classifiers with a reject option and hence, optimize task decomposition for healthcare applications.To this end, we first introduce task decomposition for healthcare applications, which is a crucial stage in human-in-the-loop healthcare delivery. We then devise a framework PACE to learn effective task decomposition concentrating on delivering high performance on the easy tasks. PACE is two-level: on the macro level, PACE employs the Self-Paced Learning method to select easy tasks for each training iteration; on the micro level, PACE adapts the weights of selected tasks through its weighted loss revision strategy. Experimental results in two real-world healthcare datasets show that PACE outperforms baselines in terms of their performance on the easy tasks which are expected to be solved by the learning model.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2156–2168},
numpages = {13},
keywords = {healthcare, human-in-the-loop, task decomposition},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{10.1007/s10586-018-2189-9,
author = {Lou, Gaoxiang and Cai, Zongyan},
title = {Improved hybrid immune clonal selection genetic algorithm and its application in hybrid shop scheduling},
year = {2019},
issue_date = {Mar 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-018-2189-9},
doi = {10.1007/s10586-018-2189-9},
abstract = {This paper is based on the multi-objective optimization problem of mixed shop scheduling problem, the strong coupling of the maximum flow and the minimum time, and the deficiencies of the immune genetic algorithm including high computational complexity and high spatial dimension. This paper establishes a mixed shop scheduling mathematical model with the minimization of the maximum total completion time as the target, and puts forward to use the immune clonal selection algorithm to solve the problem. In the algorithm population construction, it uses the grouping strategy, introduces the cross and delete operator, retains the excellent individuals through memory space, deletes the relatively bad individual, and improves the algorithm’s global optimization ability. In order to verify the effectiveness of the proposed algorithm, under the two experimental environments of workpiece machining and automobile shock absorber processing workshop scheduling, simulation experiments are conducted. The experimental results show that the proposed algorithm has better performance, and can achieve smaller maximum total completion time with less iteration. The algorithm can find the global optimal solution of the multi-objective problem, which has a strong practical significance.},
journal = {Cluster Computing},
month = mar,
pages = {3419–3429},
numpages = {11},
keywords = {Hybrid shop scheduling genetic algorithm, Immune clone selection operation, Memory space, Parallel scheduling}
}

@article{10.1007/s10664-018-9597-6,
author = {Quirchmayr, Thomas and Paech, Barbara and Kohl, Roland and Karey, Hannes and Kasdepke, Gunar},
title = {Semi-automatic rule-based domain terminology and software feature-relevant information extraction from natural language user manuals},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9597-6},
doi = {10.1007/s10664-018-9597-6},
abstract = {Mature software systems comprise a vast number of heterogeneous system capabilities which are usually requested by different groups of stakeholders and which evolve over time. Software features describe and bundle low level capabilities logically on an abstract level and thus provide a structured and comprehensive overview of the entire capabilities of a software system. Software features are often not explicitly managed. Quite the contrary, feature-relevant information is often spread across several software engineering artifacts (e.g., user manual, issue tracking systems). It requires huge manual effort to identify and extract feature-relevant information from these artifacts in order to make feature knowledge explicit. In this paper we present a two-step-approach to extract feature-relevant information from a user manual: First we semi-automatically extract a domain terminology from a natural language user manual based on linguistic patterns. Then, we apply natural language processing techniques based on the extracted domain terminology and structural sentence information. Our approach is able to extract atomic feature-relevant information with an F1-score of at least 92.00%. We describe the implementation of the approach as well as evaluations based on example sections of a user manual taken from industry.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {3630–3683},
numpages = {54},
keywords = {Atomic information extraction, NLP, Software feature, Terminology extraction}
}

@article{10.1016/j.eswa.2014.10.043,
author = {Sangsawang, Chatnugrob and Sethanan, Kanchana and Fujimoto, Takahiro and Gen, Mitsuo},
title = {Metaheuristics optimization approaches for two-stage reentrant flexible flow shop with blocking constraint},
year = {2015},
issue_date = {April 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {5},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.10.043},
doi = {10.1016/j.eswa.2014.10.043},
abstract = {This problem is formed as FFS|2-stage,rcrc,block|Cmax.First report consideration both reentrant and blocking constraints.The proposed HGA and HPSO algorithms are very efficient. This paper addresses a problem of the two-stage reentrant flexible flow shop (RFFS) with blocking constraint (FFS|2-stage,rcrc,block|Cmax). The objective is to find the optimal sequences in order to minimize the makespan. In this study, the hybridization of GA (HGA: hybrid genetic algorithm) with adaptive auto-tuning based on fuzzy logic controller and the hybridization of PSO (HPSO: hybrid particle swarm optimization) with Cauchy distribution were developed to solve the problem. The encoding and decoding routines that appropriate for blocking constraint and Relax-Blocking algorithm for improving chromosome and particle were suggested. Experimental results reveal that the HPSO and HGA algorithms give better solutions than the classical metaheuristics, GA and PSO, for all test problems respectively. Additionally, the relative improvement (RI) of the makespan solutions obtained by the proposed algorithms with respect to those of the current practice is performed in order to measure the quality of the makespan solutions generated by the proposed algorithms. The RI results show that the HGA and HPSO algorithms can improve the makespan solution by averages of 15.51% and 15.60%, respectively. We found that the performance of the HGA is not significantly competitive as compared to the HPSO but its computational times are significantly higher than those of the HPSO.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {2395–2410},
numpages = {16},
keywords = {Blocking constraint, Hard disk drive (HDD) manufacturing, Hybrid genetic algorithm (HGA), Hybrid particle swarm optimization (HPSO), Reentrant flexible flow shop (RFFS)}
}

@inproceedings{10.5555/3298023.3298119,
author = {Schmitt, Felix and Bieg, Hans-Joachim and Herman, Michael and Rothkopf, Constantin A.},
title = {I see what you see: inferring sensor and policy models of human real-world motor behavior},
year = {2017},
publisher = {AAAI Press},
abstract = {Human motor behavior is naturally guided by sensing the environment. To predict such sensori-motor behavior, it is necessary to model what is sensed and how actions are chosen based on the obtained sensory measurements. Although several models of human sensing haven been proposed, rarely data of the assumed sensory measurements is available. This makes statistical estimation of sensor models problematic. To overcome this issue, we propose an abstract structural estimation approach building on the ideas of Herman et al.'s Simultaneous Estimation of Rewards and Dynamics (SERD). Assuming optimal fusion of sensory information and rational choice of actions the proposed method allows to infer sensor models even in absence of data of the sensory measurements. To the best of our knowledge, this work presents the first general approach for joint inference of sensor and policy models. Furthermore, we consider its concrete implementation in the important class of sensor scheduling linear quadratic Gaussian problems. Finally, the effectiveness of the approach is demonstrated for prediction of the behavior of automobile drivers. Specifically, we model the glance and steering behavior of driving in the presence of visually demanding secondary tasks. The results show, that prediction benefits from the inference of sensor models. This is the case, especially, if also information is considered, that is contained in gaze switching behavior.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {3797–3803},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@inproceedings{10.1007/978-3-030-27544-0_8,
author = {Szemenyei, Marton and Estivill-Castro, Vladimir},
title = {Real-Time Scene Understanding Using Deep Neural Networks for RoboCup SPL},
year = {2018},
isbn = {978-3-030-27543-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27544-0_8},
doi = {10.1007/978-3-030-27544-0_8},
abstract = {Convolutional neural networks (CNNs) are the state-of-the-art method for most computer vision tasks. But, the deployment of CNNs on mobile or embedded platforms is challenging because of CNNs’ excessive computational requirements. We present an end-to-end neural network solution to scene understanding for robot soccer. We compose two key neural networks: one to perform semantic segmentation on an image, and another to propagate class labels between consecutive frames. We trained our networks on synthetic datasets and fine-tuned them on a set consisting of real images from a Nao robot. Furthermore, we investigate and evaluate several practical methods for increasing the efficiency and performance of our networks. Finally, we present RoboDNN, a C++ neural network library designed for fast inference on the Nao robots.},
booktitle = {RoboCup 2018: Robot World Cup XXII},
pages = {96–108},
numpages = {13},
keywords = {Computer vision, Deep learning, Semantic segmentation, Neural networks},
location = {Montr\'{e}al, QC, Canada}
}

@article{10.1016/j.comnet.2021.108199,
author = {Arce, Pau and Salvo, David and Pi\~{n}ero, Gema and Gonzalez, Alberto},
title = {FIWARE based low-cost wireless acoustic sensor network for monitoring and classification of urban soundscape},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {196},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108199},
doi = {10.1016/j.comnet.2021.108199},
journal = {Comput. Netw.},
month = sep,
numpages = {10},
keywords = {Acoustic sensor networks, Urban sound classification, FIWARE, Edge computing}
}

@article{10.1017/S0890060406060021,
author = {Sim, S. K. and Chua, Patrick S. K. and Tay, M. L. and Gao, Yun},
title = {Recognition of features of parts subjected to motion using ARTMAP incorporated in a flexible vibratory bowl feeder system},
year = {2006},
issue_date = {January 2006},
publisher = {Cambridge University Press},
address = {USA},
volume = {20},
number = {1},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060406060021},
doi = {10.1017/S0890060406060021},
abstract = {The recognition and identification of parts are important processes in modern manufacturing systems. Although machine vision systems have played an important role in these tasks, there are still challenges in performing these tasks in which parts may be in motion and subjected to noise. Using a flexible vibratory bowl feeder system as a test bed to simulate motion of parts subjected to noise, scanned signatures of part features are acquired using fiber optic sensors and a data acquisition system. Because neural networks have been shown to exhibit good pattern recognition capability, ARTMAP, a neural network that learns patterns under supervision, was incorporated into the feeder system. The pattern recognition capability of the feeder system is dependent on a set of parameters that characterized ARTMAP, the sampling rate of the data acquisition system, and the mean speed of the vibrating parts. The parameters that characterized ARTMAP are the size of an input vector, the vigilance, threshold value of the nonlinear noise suppression function, and the learning rate. Through extensive training and testing of the ARTMAP within the feeder system, it was shown that high success rates of recognition of parts features in motion under noisy conditions can be obtained provided these parameters of ARTMAP are appropriately selected.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = jan,
pages = {19–34},
numpages = {16},
keywords = {ART2, ARTMAP, Programmable Vibratory Bowl Feeder}
}

@article{10.1016/j.neucom.2016.05.104,
author = {Relich, Marcin and Pawlewski, Pawel},
title = {A fuzzy weighted average approach for selecting portfolio of new product development projects},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {231},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.05.104},
doi = {10.1016/j.neucom.2016.05.104},
abstract = {New product portfolio selection is a multi-criteria decision making problem including both qualitative and quantitative criteria. Determining the exact values for these criteria is often difficult or even impossible taking into account uncertainty and complexity associated with new product development projects. To assist managers in making portfolio selection decisions, this study proposes a new project portfolio selection model that uses a fuzzy weighted average approach for ranking new product projects and artificial neural networks for estimating project performance. New product development projects are evaluated according to criteria related to marketing, project team, project performance, risk, and strategy. The use of neural networks enables more precise evaluation of project performance criteria and provides additional information in portfolio selection. A case study of the evaluation of new product projects illustrates the usefulness of the proposed approach.},
journal = {Neurocomput.},
month = mar,
pages = {19–27},
numpages = {9},
keywords = {Fuzzy logic, Fuzzy neural system, Multi-criteria decision making, Neural networks, New product screening}
}

@article{10.1016/j.asoc.2016.05.020,
author = {Sachdeva, Jainy and Kumar, Vinod and Gupta, Indra and Khandelwal, Niranjan and Ahuja, Chirag Kamal},
title = {A package-SFERCB-"Segmentation, feature extraction, reduction and classification analysis by both SVM and ANN for brain tumors"},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.020},
doi = {10.1016/j.asoc.2016.05.020},
abstract = {An interactive computer aided dignostic (CAD) system for assisting inexperience young radiologists is developed. The difficulty in brain tumors classification is due to similar size, shape, location, hetrogeniety, presence of oedema, cystic and isointense regions has been the key feature of this research. Genetic Algorithm is employed as it is an easy concept and is well understood by radiologists without going in much depth of engineering.Display Omitted Brain tumors as segmented regions of interests (SROIs) by content based active contour model (CBAC).Feature extraction-intensity and texture based features.Feature reduction by Genetic Algorithm.Classification by Hybrid Models-GA-SVM and GA-ANN. The objective of this experimentation is to develop an interactive CAD system for assisting radiologists in multiclass brain tumor classification. The study is performed on a diversified dataset of 428 post contrast T1-weighted MR images of 55 patients and publically available dataset of 260 post contrast T1-weighted MR images of 10 patients. The first dataset includes primary brain tumors such as Astrocytoma (AS), Glioblastoma Multiforme (GBM), childhood tumor-Medulloblastoma (MED) and Meningioma (MEN), along with secondary tumor-Metastatic (MET). The second dataset consists of Astrocytoma (AS), Low Grade Glioma (LGL) and Meningioma (MEN). The tumor regions are marked by content based active contour (CBAC) model. The regions are than saved as segmented regions of interest (SROIs). 71 intensity and texture feature set is extracted from these SROIs. The features are specifically selected based on the pathological details of brain tumors provided by the radiologist. Genetic Algorithm (GA) selects the set of optimal features from this input set. Two hybrid machine learning models are implemented using GA with support vector machine (SVM) and artificial neural network (ANN) (GA-SVM and GA-ANN) and are tested on two different datasets. GA-SVM is proposed for finding preliminary probability in identifying tumor class and GA-ANN is used for confirmation of accuracy. Test results of the first dataset show that the GA optimization technique has enhanced the overall accuracy of SVM from 79.3% to 91.7% and of ANN from 75.6% to 94.9%. Individual class accuracies delivered by GA-SVM are: AS-89.8%, GBM-83.3%, MED-95.6%, MEN-91.8%, and MET-97.1%. Individual class accuracies delivered by GA-ANN classifier are: AS-96.6%, GBM-86.6%, MED-93.3%, MEN-96%, MET-100%. Similar results are obtained for the second dataset. The overall accuracy of SVM has increased from 80.8% to 89% and that of ANN has increased from 77.5% to 94.1%. Individual class accuracies delivered by GA-SVM are: AS-85.3%, LGL-88.8%, MEN-93%. Individual class accuracies delivered by GA-ANN classifier are: AS-92.6%, LGL-94.4%, MED-95.3%. It is observed from the experiments that GA-ANN classifier has provided better results than GA-SVM. Further, it is observed that along with providing finer results, GA-SVM provides advantage in speed whereas GA-ANN provides advantage in accuracy. The combined results from both the classifiers will benefit the radiologists in forming a better decision for classifying brain tumors.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {151–167},
numpages = {17},
keywords = {Brain tumors, GA-ANN, GA-SVM, Genetic Algorithm (GA)}
}

@article{10.1016/S0005-1098(02)00235-2,
author = {Gevers, Michel and Bombois, Xavier and Codrons, Beno\i{}T and Scorletti, G\'{e}Rard and Anderson, Brian D. O.},
title = {Model validation for control and controller validation in a prediction error identification framework-Part II: illustrations},
year = {2003},
issue_date = {March, 2003},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {3},
issn = {0005-1098},
url = {https://doi.org/10.1016/S0005-1098(02)00235-2},
doi = {10.1016/S0005-1098(02)00235-2},
abstract = {In this paper, we illustrate our new results on model validation for control and controller validation in a prediction error identification framework, developed in a companion paper (Gevers et al., Automatica (2003) 39(3) pii: S005-1098(02)00234-0), through two realistic simulation examples, covering widely different control design applications. The first is the control of a flexible mechanical system (the Landau benchmark example) with a tracking objective, the second is the control of a ferrosilicon production process with a disturbance rejection objective.},
journal = {Automatica},
month = mar,
pages = {417–427},
numpages = {11},
keywords = {Controller validation, Identification for robust control, Model validation, System identification}
}

@article{10.1007/s11390-019-1960-6,
author = {Alqmase, Mohammed and Alshayeb, Mohammad and Ghouti, Lahouari},
title = {Threshold Extraction Framework for Software Metrics},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1960-6},
doi = {10.1007/s11390-019-1960-6},
abstract = {Software metrics are used to measure different attributes of software. To practically measure software attributes using these metrics, metric thresholds are needed. Many researchers attempted to identify these thresholds based on personal experiences. However, the resulted experience-based thresholds cannot be generalized due to the variability in personal experiences and the subjectivity of opinions. The goal of this paper is to propose an automated clustering framework based on the expectation maximization (EM) algorithm where clusters are generated using a simplified 3-metric set (LOC, LCOM, and CBO). Given these clusters, different threshold levels for software metrics are systematically determined such that each threshold reflects a specific level of software quality. The proposed framework comprises two major steps: the clustering step where the software quality historical dataset is decomposed into a fixed set of clusters using the EM algorithm, and the threshold extraction step where thresholds, specific to each software metric in the resulting clusters, are estimated using statistical measures such as the mean (μ) and the standard deviation (σ) of each software metric in each cluster. The paper’s findings highlight the capability of EM-based clustering, using a minimum metric set, to group software quality datasets according to different quality levels.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1063–1078},
numpages = {16},
keywords = {metric threshold, expectation maximization, empirical study}
}

@article{10.1016/j.dss.2012.10.005,
author = {Van Valkenhoef, Gert and Tervonen, Tommi and Zwinkels, Tijs and De Brock, Bert and Hillege, Hans},
title = {ADDIS: A decision support system for evidence-based medicine},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {2},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2012.10.005},
doi = {10.1016/j.dss.2012.10.005},
abstract = {Clinical trials are the main source of information for the efficacy and safety evaluation of medical treatments. Although they are of pivotal importance in evidence-based medicine, there is a lack of usable information systems providing data-analysis and decision support capabilities for aggregate clinical trial results. This is partly caused by unavailability (i) of trial data in a structured format suitable for re-analysis, and (ii) of a complete data model for aggregate level results. In this paper, we develop a unifying data model that enables the development of evidence-based decision support in the absence of a complete data model. We describe the supported decision processes and show how these are implemented in the open source ADDIS software. ADDIS enables semi-automated construction of meta-analyses, network meta-analyses and benefit-risk decision models, and provides visualization of all results.},
journal = {Decis. Support Syst.},
month = may,
pages = {459–475},
numpages = {17},
keywords = {ADE, ADR, ADaM, AMIA, ANSI, ATC, BRIDG, CDASH, CDISC, CDMS, CHMP, CPOE, CRF, CRO, CTIS, CTMS, Clinical trial, DB, DED, DIS, DOI, DSS, Data model, Decision analysis, EAV, EBM, EDC, EHR, EMA, EPAR, Evidence synthesis, Evidence-based medicine, FDA, FDAAA, GCP, GUI, HL7, HSDB, ICD, ICMJE, ICTRP, JAMA, LAB, MCDA, MeSH, MedDRA, NCI, NDA, NIHUS, OBX, OCRe, ODM, OWL, PIM, PMDA, PRM, PhRMA, QRD, RIM, SDTM, SEND, SMAA, SNOMEDCT, SPL, SmPC, TDM, UMLS, WHO, caBIG, eCRF, eLab, ePRO}
}

@article{10.1007/s00291-013-0338-3,
author = {Baur, Alexander and Klein, Robert and Steinhardt, Claudius},
title = {Model-based decision support for optimal brochure pricing: applying advanced analytics in the tour operating industry},
year = {2014},
issue_date = {July      2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {36},
number = {3},
issn = {0171-6468},
url = {https://doi.org/10.1007/s00291-013-0338-3},
doi = {10.1007/s00291-013-0338-3},
abstract = {The research presented in this paper is motivated by an industry project conducted with TUI Deutschland, Germany's leading tour operator. We consider the decision problem of optimally determining hotel room prices to be published in the tour operator's brochure, which is usually valid for a half-year period. In practice, this task is performed manually by a large number of pricing specialists, each of whom is in charge of setting up to 100,000 prices. In this paper, we develop an advanced analytics approach to provide decision support for this task. More precisely, we propose a mixed integer linear programming-based approach, involving state-of-the-art methods from data analysis and optimization. In this context, we formally introduce the brochure pricing problem as a new optimization problem and present several alternative mathematical model formulations. The problem incorporates demand-side behavior by including a general attraction model whose parameters can be obtained from past booking data. Furthermore, we present different real-world scenarios of model-based decision support, showing how the brochure pricing problem and some variants thereof can be integrated into the manual decision making process, given the requirement of using standard optimization software. For example, the model-based approach can help the pricing specialist balance the objective of profit maximization and the disadvantage of a very complicated pricing structure.},
journal = {OR Spectr.},
month = jul,
pages = {557–584},
numpages = {28},
keywords = {Analytics, Choice modeling, Decision support, Integer programming, Optimization, Tour operator}
}

@article{10.1016/j.eswa.2021.115218,
author = {Serrano-P\'{e}rez, Jonathan and Sucar, L. Enrique},
title = {Artificial datasets for hierarchical classification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115218},
doi = {10.1016/j.eswa.2021.115218},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {13},
keywords = {Artificial datasets, Hierarchical classification, Evaluation}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {deep sparse feedforward neural network, highly configurable systems, software performance prediction, sparsity regularization},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1007/s00366-018-0660-0,
author = {Gaud\^{e}ncio, Juliana Helena Daroz and de Almeida, Fabr\'{\i}cio Alves and Sabioni, Rachel Campos and Turrioni, Jo\~{a}o Batista and de Paiva, Anderson Paulo and da Silva Campos, Paulo Henrique},
title = {Fuzzy multivariate mean square error in equispaced pareto frontiers considering manufacturing process optimization problems},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {4},
issn = {0177-0667},
url = {https://doi.org/10.1007/s00366-018-0660-0},
doi = {10.1007/s00366-018-0660-0},
abstract = {This paper proposes a combined approach using the normal boundary intersection (NBI) and multivariate mean square error (MMSE) that is an alternative approach to outperform the traditional NBI driving to an equispaced Pareto Frontier in a low-dimension space with a considerable reduction in the number of iterations. The method participating in the evolutionary stage of creating a uniformly spread Pareto Frontier for a nonlinear multi-objective problem is the NBI using normalized objective functions allied to MMSE. In sequence, the fuzzy MMSE approach is utilized to determine the optimal point of the multi-objective optimization. For sake of comparison, the performance of arc homotopy length, global criterion method, and weighted sums were explored. To illustrate this proposal, a multivariate case of AISI H13 hardened steel-turning process is used. Experimental results indicate that the solution found by NBI-MMSE approach is a more appropriate Pareto frontier that surpassed all the competitors and also provides the best-compromised solution to set the machine input parameters. Further, this algorithm was also tested in benchmark functions to confirm the NBI-MMSE efficiency.},
journal = {Eng. with Comput.},
month = oct,
pages = {1213–1236},
numpages = {24},
keywords = {Principal component analysis, Multivariate mean square error, Normal boundary intersection, Fuzzy decision maker, Hardened steel turing}
}

@article{10.1016/j.eswa.2009.09.003,
author = {Beg, Azam and Chandana Prasad, P. W.},
title = {Prediction of area and length complexity measures for binary decision diagrams},
year = {2010},
issue_date = {April, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2009.09.003},
doi = {10.1016/j.eswa.2009.09.003},
abstract = {Measuring the complexity of functions that represent digital circuits in non-uniform computation models is an important area of computer science theory. This paper presents a comprehensive set of machine learnt models for predicting the complexity properties of circuits represented by binary decision diagrams. The models are created using Monte Carlo data for a wide range of circuit inputs and number of minterms. The models predict number of nodes as representations of circuit size/area and path lengths: average path length, longest path length, and shortest path length. The models have been validated using an arbitrarily-chosen subset of ISCAS-85 and MCNC-91 benchmark circuits. The models yield reasonably low RMS errors for predictions, so they can be used to estimate complexity metrics of circuits without having to synthesize them.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {2864–2873},
numpages = {10},
keywords = {Area complexity, Binary decision diagrams, Circuit complexity, Complexity prediction, Machine learning, Neural network modeling, Path length complexity}
}

@article{10.1016/j.compag.2018.04.005,
author = {Mu\~{n}oz-Benavent, P. and Andreu-Garc\'{\i}a, G. and Valiente-Gonz\'{a}lez, Jos\'{e} M. and Atienza-Vanacloig, V. and Puig-Pons, V. and Espinosa, V.},
title = {Enhanced fish bending model for automatic tuna sizing using computer vision},
year = {2018},
issue_date = {Jul 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {150},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2018.04.005},
doi = {10.1016/j.compag.2018.04.005},
journal = {Comput. Electron. Agric.},
month = jul,
pages = {52–61},
numpages = {10},
keywords = {Underwater stereo-vision, Computer vision, Fisheries management, Automatic fish sizing, Biomass estimation}
}

@article{10.1007/s10664-020-09808-9,
author = {Agrawal, Amritanshu and Menzies, Tim and Minku, Leandro L. and Wagner, Markus and Yu, Zhe},
title = {Better software analytics via “DUO”: Data mining algorithms using/used-by optimizers},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09808-9},
doi = {10.1007/s10664-020-09808-9},
abstract = {This paper claims that a new field of empirical software engineering research and practice is emerging: data mining using/used-by optimizers for empirical studies, or DUO. For example, data miners can generate models that are explored by optimizers. Also, optimizers can advise how to best adjust the control parameters of a data miner. This combined approach acts like an agent leaning over the shoulder of an analyst that advises “ask this question next” or “ignore that problem, it is not relevant to your goals”. Further, those agents can help us build “better” predictive models, where “better” can be either greater predictive accuracy or faster modeling time (which, in turn, enables the exploration of a wider range of options). We also caution that the era of papers that just use data miners is coming to an end. Results obtained from an unoptimized data miner can be quickly refuted, just by applying an optimizer to produce a different (and better performing) model. Our conclusion, hence, is that for software analytics it is possible, useful and necessary to combine data mining and optimization using DUO.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {2099–2136},
numpages = {38},
keywords = {Software analytics, Data mining, Optimization, Evolutionary algorithms}
}

@article{10.1016/j.compeleceng.2017.11.002,
author = {AbuZeina, Dia and Al-Anzi, Fawaz S.},
title = {Employing fisher discriminant analysis for Arabic text classification},
year = {2018},
issue_date = {February 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.11.002},
doi = {10.1016/j.compeleceng.2017.11.002},
abstract = {Linear discriminant analysis (LDA) is proposed for Arabic text classification.LDA employs less dimensions, which is helpful for sizable textual feature vectors.Despite that LDA is semantic loss feature reduction method, it shows useful results. Fisher's discriminant analysis; also called linear discriminant analysis (LDA), is a popular dimensionality reduction technique that is widely used for features extraction. LDA aims at finding an optimal linear transformation based on maximizing a class separability. Even though LDA shows useful results in various pattern recognition problems, such as face recognition, less attention has been devoted to employing this technique in Arabic information retrieval tasks. In particular, the sizable feature vectors in textual data enforces to implement dimensionality reduction techniques such as LDA. In this paper, we empirically investigated an LDA based method for Arabic text classification. We used a corpus that contains 2,000 documents belonging to five categories. The experimental results showed that the performance of semantic loss LDA based method was almost the same as the semantic rich singular value decomposition (SVD), and that is indication that LDA is a promising method for text mining applications. Display Omitted},
journal = {Comput. Electr. Eng.},
month = feb,
pages = {474–486},
numpages = {13},
keywords = {Arabic, Classification, Eigenvectors, Fisher, Linear discriminant analysis, Text}
}

@article{10.1016/j.jss.2019.110402,
author = {Xu, Zhou and Li, Shuai and Xu, Jun and Liu, Jin and Luo, Xiapu and Zhang, Yifeng and Zhang, Tao and Keung, Jacky and Tang, Yutian},
title = {LDFR: Learning deep feature representation for software defect prediction},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110402},
doi = {10.1016/j.jss.2019.110402},
journal = {J. Syst. Softw.},
month = dec,
numpages = {20},
keywords = {Software defect prediction, Deep feature representation, Triplet loss, Weighted cross-entropy loss, Deep neural network, 00-01, 99-00}
}

@article{10.1016/j.compbiomed.2016.01.002,
author = {Bokov, Plamen and Mahut, Bruno and Flaud, Patrice and Delclaux, Christophe},
title = {Wheezing recognition algorithm using recordings of respiratory sounds at the mouth in a pediatric population},
year = {2016},
issue_date = {March 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {70},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2016.01.002},
doi = {10.1016/j.compbiomed.2016.01.002},
abstract = {BackgroundRespiratory diseases in children are a common reason for physician visits. A diagnostic difficulty arises when parents hear wheezing that is no longer present during the medical consultation. Thus, an outpatient objective tool for recognition of wheezing is of clinical value. MethodWe developed a wheezing recognition algorithm from recorded respiratory sounds with a Smartphone placed near the mouth. A total of 186 recordings were obtained in a pediatric emergency department, mostly in toddlers (mean age 20 months). After exclusion of recordings with artefacts and those with a single clinical operator auscultation, 95 recordings with the agreement of two operators on auscultation diagnosis (27 with wheezing and 68 without) were subjected to a two phase algorithm (signal analysis and pattern classifier using machine learning algorithms) to classify records. ResultsThe best performance (71.4% sensitivity and 88.9% specificity) was observed with a Support Vector Machine-based algorithm. We further tested the algorithm over a set of 39 recordings having a single operator and found a fair agreement (kappa=0.28, CI95% 0.12, 0.45) between the algorithm and the operator. ConclusionsThe main advantage of such an algorithm is its use in contact-free sound recording, thus valuable in the pediatric population. We recorded by Smartphone respiratory sounds at the mouth in pediatric population.Two clinical operators validated the presence or absence of wheezing in 97 toddlers.We used Short-Time Fourier Transform and SVM classifier for wheeze recognition.71.4% Sensitivity and 88.9% Specificity were observed for wheeze detection.An independent test found a fair agreement with a clinical operator.},
journal = {Comput. Biol. Med.},
month = mar,
pages = {40–50},
numpages = {11},
keywords = {Automated wheezing detection, Bronchiolitis, Childhood asthma, ROC analysis, Support vector machine}
}

@article{10.1016/j.engappai.2012.02.008,
author = {Ribeiro, Luis and Barata, Jose},
title = {IMS 10-Validation of a co-evolving diagnostic algorithm for evolvable production systems},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {25},
number = {6},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2012.02.008},
doi = {10.1016/j.engappai.2012.02.008},
abstract = {With the systematic implantation and acceptance of IT in the shop-floor a wide range of production paradigms, relying in open interoperable architectures, have been developed. Exploring these technological novelties, they promise to revolutionize the way current plant floors operate and react to emerging opportunities and disturbances. There is a high interest of module providers in the adoption of these open mechatronic architectures as they may provide a new business model where the automation solution can be easily tailored for each customer in due time and ships with a significant part of the control solution (high added value). Final customers on their side can contract operating hours rather than buying modules. Moreover, the automation solution can be swiftly modified to meet changing requirements. The necessary increase in the number of distributed and autonomous components that interact in the execution of processes implies that new diagnostic approaches should be developed to tackle the network layer of these highly dynamic systems. In fact fault propagation events can be harder to understand and can affect the system in unpredictable and pervasive ways. Following this rationale the paper presents a potential diagnostic solution that targets multiagent-based mechatronic systems where their components are highly decoupled from a control point of view. The diagnostic architecture presented tackles the problem of fault propagation while preserving the decoupled nature of the Mechatronic Agent concept. In this context the diagnostic system explores self-organization to enact an emergent response that denotes macro-level coherence. The system's response is the result of an individual probabilistic diagnostic inference based on Hidden Markov Models that capture the propagating nature of a failure. The validation results of the proposed diagnostic approach are detailed for the system's response in simulation (highlighting the main variables that affect the performance of the system) and compared to the system applied to a pilot assembly cell. The simulation model and the performance metrics considered are detailed and discussed along with the main implementation details.},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {1142–1160},
numpages = {19},
keywords = {Agile manufacturing, Distributed diagnostic systems, Mechatronic Agents, Mechatronic networks, Multiagent systems}
}

@article{10.1016/j.eswa.2014.10.026,
author = {Arsene, Octavian and Dumitrache, Ioan and Mihu, Ioana},
title = {Expert system for medicine diagnosis using software agents},
year = {2015},
issue_date = {March 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.10.026},
doi = {10.1016/j.eswa.2014.10.026},
abstract = {In order to simplify the information exchange within the medical diagnosis process, a collaborative software agents framework is presented.The human body systems (e.g. respiratory, cardiovascular) are embedded into distinct software agents.The automated information exchange between different medicine specialists from different areas of expertise.The framework has three key components: knowledge management, uncertainty reasoning and software agents. In order to simplify the information exchange within the medical diagnosis process, a collaborative software agents framework is presented. The human body systems (e.g. respiratory, cardiovascular) are embedded into distinct software agents. The holistic perspective is given by the all connected agents exchanging information. The purpose of the framework is to allow the automated information exchange between different medicine specialists. The key factor of the exchange is sharing concepts between the areas of expertise. Each human body system expert will act over his concepts (evidences, causes, effects), however the information from other systems will be assimilated as well. The framework has three key components: knowledge management, uncertainty reasoning and software agents. The ontology is chosen to address the management of human body systems knowledge. The Bayesian Network is the graphical model for probabilistic knowledge representation and reasoning about partial beliefs under uncertainty. The software agents, as collaboration framework, are in charge of the belief propagation between system instances.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1825–1834},
numpages = {10},
keywords = {Bayesian Network, Ontology, Software agents}
}

@article{10.1145/3230709,
author = {Liu, Wenhe and Chang, Xiaojun and Yan, Yan and Yang, Yi and Hauptmann, Alexander G.},
title = {Few-Shot Text and Image Classification via Analogical Transfer Learning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230709},
doi = {10.1145/3230709},
abstract = {Learning from very few samples is a challenge for machine learning tasks, such as text and image classification. Performance of such task can be enhanced via transfer of helpful knowledge from related domains, which is referred to as transfer learning. In previous transfer learning works, instance transfer learning algorithms mostly focus on selecting the source domain instances similar to the target domain instances for transfer. However, the selected instances usually do not directly contribute to the learning performance in the target domain. Hypothesis transfer learning algorithms focus on the model/parameter level transfer. They treat the source hypotheses as well-trained and transfer their knowledge in terms of parameters to learn the target hypothesis. Such algorithms directly optimize the target hypothesis by the observable performance improvements. However, they fail to consider the problem that instances that contribute to the source hypotheses may be harmful for the target hypothesis, as instance transfer learning analyzed. To relieve the aforementioned problems, we propose a novel transfer learning algorithm, which follows an analogical strategy. Particularly, the proposed algorithm first learns a revised source hypothesis with only instances contributing to the target hypothesis. Then, the proposed algorithm transfers both the revised source hypothesis and the target hypothesis (only trained with a few samples) to learn an analogical hypothesis. We denote our algorithm as Analogical Transfer Learning. Extensive experiments on one synthetic dataset and three real-world benchmark datasets demonstrate the superior performance of the proposed algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {71},
numpages = {20},
keywords = {Transfer learning, classification}
}

@article{10.1017/S0890060404040077,
author = {Zha, Xuan F. and Sriram, Ram D. and Lu, Wen F.},
title = {Evaluation and selection in product design for mass customization: A knowledge decision support approach},
year = {2004},
issue_date = {January 2004},
publisher = {Cambridge University Press},
address = {USA},
volume = {18},
number = {1},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060404040077},
doi = {10.1017/S0890060404040077},
abstract = {Mass customization has been identified as a competitive strategy by an increasing number of companies. Family-based product design is an efficient and effective means to realize sufficient product variety, while satisfying a range of customer demands in support for mass customization. This paper presents a knowledge decision support approach to product family design evaluation and selection for mass customization process. Here, product family design is viewed as a selection problem with the following stages: product family (design alternatives) generation, product family design evaluation, and selection for customization. The fundamental issues underlying product family design for mass customization are discussed. Then, a knowledge support framework and its relevant technologies are developed for module-based product family design for mass customization. A systematic fuzzy clustering and ranking model is proposed and discussed in detail. This model supports the imprecision inherent in decision making with fuzzy customers' preference relations and uses fuzzy analysis techniques for evaluation and selection. A neural network technique is also adopted to adjust the membership function to enhance the model. The focus of this paper is on the development of a knowledge-intensive support scheme and a comprehensive systematic fuzzy clustering and ranking methodology for product family design evaluation and selection. A case study and the scenario of knowledge support for power supply family evaluation, selection, and customization are provided for illustration.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = jan,
pages = {87–109},
numpages = {23},
keywords = {Customer-Driven Design, Design Evaluation, Fuzzy Clustering, Fuzzy Ranking, Knowledge Support, Mass Customization, Multicriteria Decision Making, Product Family Design, Product Platform}
}

@article{10.1145/3345314,
author = {Wang, Qingyong and Zhou, Yun and Ding, Weiping and Zhang, Zhiguo and Muhammad, Khan and Cao, Zehong},
title = {Random Forest with Self-Paced Bootstrap Learning in Lung Cancer Prognosis},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3345314},
doi = {10.1145/3345314},
abstract = {Training gene expression data with supervised learning approaches can provide an alarm sign for early treatment of lung cancer to decrease death rates. However, the samples of gene features involve lots of noises in a realistic environment. In this study, we present a random forest with self-paced learning bootstrap for improvement of lung cancer classification and prognosis based on gene expression data. To be specific, we propose an ensemble learning with random forest approach to improving the model classification performance by selecting multi-classifiers. Then, we investigate the sampling strategy by gradually embedding from high- to low-quality samples by self-paced learning. The experimental results based on five public lung cancer datasets show that our proposed method could select significant genes exactly, which improves classification performance compared to that of existing approaches. We believe that our proposed method has the potential to assist doctors in gene selections and lung cancer prognosis.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {34},
numpages = {12},
keywords = {Lung cancer, bootstrap, classification, random forest, self-paced learning}
}

@article{10.1016/j.asoc.2020.106076,
author = {Ajeil, Fatin H. and Ibraheem, Ibraheem Kasim and Sahib, Mouayad A. and Humaidi, Amjad J.},
title = {Multi-objective path planning of an autonomous mobile robot using hybrid PSO-MFB optimization algorithm},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.106076},
doi = {10.1016/j.asoc.2020.106076},
journal = {Appl. Soft Comput.},
month = apr,
numpages = {13},
keywords = {Autonomous mobile robot, Robot path planning, Particle swarm optimization, Bat algorithm, Collision avoidance}
}

@article{10.1007/s11633-021-1299-7,
author = {Ramalepa, Larona Pitso and Jamisola, Rodrigo S.},
title = {A Review on Cooperative Robotic Arms with Mobile or Drones Bases},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {4},
issn = {1476-8186},
url = {https://doi.org/10.1007/s11633-021-1299-7},
doi = {10.1007/s11633-021-1299-7},
abstract = {This review paper focuses on cooperative robotic arms with mobile or drone bases performing cooperative tasks. This is because cooperative robots are often used as risk-reduction tools to human life. For example, they are used to explore dangerous places such as minefields and disarm explosives. Drones can be used to perform tasks such as aerial photography, military and defense missions, agricultural surveys, etc. The bases of the cooperative robotic arms can be stationary, mobile (ground), or drones. Cooperative manipulators allow faster performance of assigned tasks because of the available “extra hand”. Furthermore, a mobile base increases the reachable ground workspace of cooperative manipulators while a drone base drastically increases this workspace to include the aerial space. The papers in this review are chosen to extensively cover a wide variety of cooperative manipulation tasks and industries that use them. In cooperative manipulation, avoiding self-collision is one of the most important tasks to be performed. In addition, path planning and formation control can be challenging because of the increased number of components to be coordinated.},
journal = {Int. J. Autom. Comput.},
month = aug,
pages = {536–555},
numpages = {20},
keywords = {Cooperative arms, mobile manipulator, aerial manipulator, mobile base, drone base, cooperative tasks}
}

@article{10.1287/inte.1100.0535,
author = {Abdinnour, Sue},
title = {Hawker Beechcraft Uses a New Solution Approach to Balance Assembly Lines},
year = {2011},
issue_date = {March 2011},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {41},
number = {2},
issn = {0092-2102},
url = {https://doi.org/10.1287/inte.1100.0535},
doi = {10.1287/inte.1100.0535},
abstract = {In 2002, Hawker Beechcraft set out to improve the efficiency of its Hawker 800XP assembly line. The line was paced such that an aircraft was moved to the next workstation on the line on a schedule---even if work in a previous workstation had not completed. At the time, work in progress on the assembly line was high, and most work on the aircraft was completed out of station. Hawker Beechcraft's management wanted a more quantitative approach to determine how many workstations are necessary and how to best assign the tasks to the workstations; its goal was to minimize work in progress and out-of-station work. In response to a management request, we proposed a new solution approach based on assembly-line balancing principles but customized to a complex product (aircraft) in a real assembly-line setting. Hawker Beechcraft implemented our recommended solution on the Hawker 800XP product line in 2003. The company also used our new solution approach to balance two other assembly lines, saving over $30 million.},
journal = {Interfaces},
month = mar,
pages = {164–176},
numpages = {13},
keywords = {CPM, algorithm, assembly-line balancing, aviation, facility layout, linear programming, transportation}
}

@inbook{10.5555/3454287.3454459,
author = {Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
title = {Meta-weight-net: learning an explicit mapping for sample weighting},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods. Source code is available at https://github.com/xjtushujun/meta-weight-net.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {172},
numpages = {12}
}

@inproceedings{10.1145/1739230.1739240,
author = {Farias, Kleinner and Garcia, Alessandro and Whittle, Jon},
title = {Assessing the impact of aspects on model composition effort},
year = {2010},
isbn = {9781605589589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1739230.1739240},
doi = {10.1145/1739230.1739240},
abstract = {Model composition is a common operation used in many software development activities---for example, reconciling models developed in parallel by different development teams, or merging models of new features with existing model artifacts. Unfortunately, both commercial and academic model composition tools suffer from the composition conflict problem. That is, models to-be-composed may conflict with each other and these conflicts must be resolved. In practice, detecting and resolving conflicts is a highly-intensive manual activity. In this paper, we investigate whether aspect-orientation reduces conflict resolution effort as improved modularization may better localize conflicts. The main goal of the paper is to conduct an exploratory study to analyze the impact of aspects on conflict resolution. In particular, model compositions are used to express the evolution of architectural models along six releases of a software product line. Well-known composition algorithms, such as override, merge and union, are applied and compared on both AO and non-AO models in terms of their conflict rate and effort to solve the identified conflicts. Our findings identify specific scenarios where aspect-orientation properties, such as obliviousness and quantification, result in a lower (or higher) composition effort.},
booktitle = {Proceedings of the 9th International Conference on Aspect-Oriented Software Development},
pages = {73–84},
numpages = {12},
keywords = {empirical studies, model composition, software architecture, software metrics, software product lines},
location = {Rennes and Saint-Malo, France},
series = {AOSD '10}
}

@article{10.1016/j.jss.2015.08.026,
author = {Vogel-Heuser, Birgit and Fay, Alexander and Schaefer, Ina and Tichy, Matthias},
title = {Evolution of software in automated production systems},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {110},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.08.026},
doi = {10.1016/j.jss.2015.08.026},
abstract = {Automated Production Systems (aPS) impose specific requirements regarding evolution.We present a classification of how Automated Production Systems evolve.We discuss the state of art and research needs for the development phases of aPS.Model-driven engineering and Variability Management are key issues.Cross-discipline analysis of (non)-functional requirements must be improved. Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. Display Omitted},
journal = {J. Syst. Softw.},
month = dec,
pages = {54–84},
numpages = {31},
keywords = {Automated production systems, Automation, Evolution, Software engineering}
}

@article{10.1609/aimag.v38i1.2714,
author = {Gotlieb, Arnaud and Marijan, Dusica},
title = {Using Global Constraints to Automate Regression Testing},
year = {2017},
issue_date = {Spring 2017},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {38},
number = {1},
issn = {0738-4602},
url = {https://doi.org/10.1609/aimag.v38i1.2714},
doi = {10.1609/aimag.v38i1.2714},
abstract = {Communicating or autonomous systems rely on high‐quality software‐based components. that must be thoroughly verified before they are released and deployed in operational settings. Regression testing is a crucial verification process that compares any new release of a software‐based component against its previous versions, by executing available test cases. However, limited testing time makes selection of test cases in regression testing challenging, and some selection criteria must be respected. Validation engineers usually address this problem, coined as test suite reduction (TSR), through manual analysis or by using approximation techniques. In this paper, we address the TSR problem with sound artificial intelligence techniques such as constraint programming (CP) and global constraints. By using distinct cost‐value‐aggregating criteria, we propose several constraint‐optimization models to find a subset of test cases that cover all the test requirements and optimize the overall cost of selected test cases. Our contribution includes reuse of existing preprocessing rules to simplify the problem before solving it and the design of structure‐aware heuristics that take into account the notion of the costs associated with test cases. The work presented in this paper has been motivated by an industrial application in the communication domain. Our overall goal is to develop a constraint‐based approach of test suite reduction that can be deployed to test a complete product line of conferencing systems in continuous delivery mode. By implementing this approach in a software prototype tool and experimentally evaluating it on both randomly generated and industrial instances, we hope to foster a quick adoption of the technology.},
journal = {AI Mag.},
month = mar,
pages = {73–87},
numpages = {15}
}

@article{10.1016/j.knosys.2019.105424,
author = {Liu, Xiaoshuang and Luo, Senlin and Pan, Limin},
title = {Robust boosting via self-sampling},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {193},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105424},
doi = {10.1016/j.knosys.2019.105424},
journal = {Know.-Based Syst.},
month = apr,
numpages = {10},
keywords = {Boosting, Loss function, Robustness, Self-sampling}
}

@article{10.1109/TASLP.2020.3019646,
author = {Nguyen, Thi Ngoc Tho and Gan, Woon-Seng and Ranjan, Rishabh and Jones, Douglas L.},
title = {Robust Source Counting and DOA Estimation Using Spatial Pseudo-Spectrum and Convolutional Neural Network},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3019646},
doi = {10.1109/TASLP.2020.3019646},
abstract = {Many signal processing-based methods for sound source direction-of-arrival estimation produce a spatial pseudo-spectrum of which the local maxima strongly indicate the source directions. Due to different levels of noise, reverberation and different number of overlapping sources, the spatial pseudo-spectra are noisy even after smoothing. In addition, the number of sources is often unknown. As a result, selecting the peaks from these spectra is susceptible to error. Convolutional neural network has been successfully applied to many image processing problems in general and direction-of-arrival estimation in particular. In addition, deep learning-based methods for direction-of-arrival estimation show good generalization to different environments. We propose to use a 2D convolutional neural network with multi-task learning to robustly estimate the number of sources and the directions-of-arrival from short-time spatial pseudo-spectra, which have useful directional information from audio input signals. This approach reduces the tendency of the neural network to learn unwanted association between sound classes and directional information, and helps the network generalize to unseen sound classes. The simulation and experimental results show that the proposed methods outperform other directional-of-arrival estimation methods in different levels of noise and reverberation, and different number of sources.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {2626–2637},
numpages = {12}
}

@article{10.1007/s00500-015-2004-y,
author = {Qin, Jindong and Liu, Xinwang and Pedrycz, Witold},
title = {A multiple attribute interval type-2 fuzzy group decision making and its application to supplier selection with extended LINMAP method},
year = {2017},
issue_date = {June      2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {12},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-2004-y},
doi = {10.1007/s00500-015-2004-y},
abstract = {Supplier selection is a key issue in supply chain management, which directly impacts the manufacturer's performance. The problem can be viewed as a multiple attribute group decision making (MAGDM) that concerns many conflicting evaluation attributes, both being of qualitative and quantitative nature. Due to the increasing complexity and uncertainty of socio-economic environment, some evaluations of attributes are not adequately represented by numerical assessments and type-1 fuzzy sets. In this paper, we develop some linear programming models with the aid of multidimensional analysis of preference (LINMAP) method to solve interval type-2 fuzzy MAGDM problems, in which the information about attribute weights is incompletely known, and all pairwise comparison judgments over alternatives are represented by IT2FSs. First, we introduce a new distance measure based on the centroid interval between the IT2FSs. Then, we construct the linear programming model to determine the interval type-2 fuzzy positive ideal solution (IT2PIS) and corresponding attributes weight vector. Based on it, an extended LINMAP method to solve MAGDM problem under IT2FSs environment is developed. Finally, a supplier selection example is provided to demonstrate the usefulness of the proposed method.},
journal = {Soft Comput.},
month = jun,
pages = {3207–3226},
numpages = {20},
keywords = {Interval type-2 fuzzy sets (IT2FSs), Linear programming techniques for multidimensional analysis of preference (LINMAP) method, Multiple attribute group decision making (MAGDM), Supplier selection}
}

@article{10.1016/j.specom.2019.10.002,
author = {Howson, Phil J. and Monahan, Philip J.},
title = {Perceptual motivation for rhotics as a class},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.002},
doi = {10.1016/j.specom.2019.10.002},
journal = {Speech Commun.},
month = dec,
pages = {15–28},
numpages = {14},
keywords = {Speech Perception, Phonetics-phonology, Rhotic Typology, Rhotics, Natural, Classes}
}

@inproceedings{10.5555/2886521.2886696,
author = {Jiang, Lu and Meng, Deyu and Zhao, Qian and Shan, Shiguang and Hauptmann, Alexander G.},
title = {Self-paced curriculum learning},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training. The two methods share a similar conceptual learning paradigm, but differ in specific learning schemes. In CL, the curriculum is predetermined by prior knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner. In SPL, the curriculum is dynamically determined to adjust to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a unified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training. In comparison to human education, SPCL is analogous to "instructor-student-collaborative" learning mode, as opposed to "instructor-driven" in CL or "student-driven" in SPL. Empirically, we show that the advantage of SPCL on two tasks.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {2694–2700},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@article{10.1016/j.eswa.2012.03.061,
author = {Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S. and Garc\'{\i}a-Torres, M.},
title = {Fast feature selection aimed at high-dimensional data via hybrid-sequential-ranked searches},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {12},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.03.061},
doi = {10.1016/j.eswa.2012.03.061},
abstract = {We address the feature subset selection problem for classification tasks. We examine the performance of two hybrid strategies that directly search on a ranked list of features and compare them with two widely used algorithms, the fast correlation based filter (FCBF) and sequential forward selection (SFS). The proposed hybrid approaches provide the possibility of efficiently applying any subset evaluator, with a wrapper model included, to large and high-dimensional domains. The experiments performed show that our two strategies are competitive and can select a small subset of features without degrading the classification error or the advantages of the strategies under study.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {11094–11102},
numpages = {9},
keywords = {Classification, Data mining, Feature ranking, Feature selection}
}

@article{10.1007/s10618-016-0475-9,
author = {Garcia, Lu\'{\i}s P. and Lorena, Ana C. and Matwin, Stan and Carvalho, Andr\'{e} C.},
title = {Ensembles of label noise filters: a ranking approach},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {5},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-016-0475-9},
doi = {10.1007/s10618-016-0475-9},
abstract = {Label noise can be a major problem in classification tasks, since most machine learning algorithms rely on data labels in their inductive process. Thereupon, various techniques for label noise identification have been investigated in the literature. The bias of each technique defines how suitable it is for each dataset. Besides, while some techniques identify a large number of examples as noisy and have a high false positive rate, others are very restrictive and therefore not able to identify all noisy examples. This paper investigates how label noise detection can be improved by using an ensemble of noise filtering techniques. These filters, individual and ensembles, are experimentally compared. Another concern in this paper is the computational cost of ensembles, once, for a particular dataset, an individual technique can have the same predictive performance as an ensemble. In this case the individual technique should be preferred. To deal with this situation, this study also proposes the use of meta-learning to recommend, for a new dataset, the best filter. An extensive experimental evaluation of the use of individual filters, ensemble filters and meta-learning was performed using public datasets with imputed label noise. The results show that ensembles of noise filters can improve noise filtering performance and that a recommendation system based on meta-learning can successfully recommend the best filtering technique for new datasets. A case study using a real dataset from the ecological niche modeling domain is also presented and evaluated, with the results validated by an expert.},
journal = {Data Min. Knowl. Discov.},
month = sep,
pages = {1192–1216},
numpages = {25},
keywords = {Ensemble filters, Label noise, Noise filters, Noise ranking, Recommendation system}
}

@article{10.1007/s10618-008-0097-y,
author = {Forman, George},
title = {Quantifying counts and costs via classification},
year = {2008},
issue_date = {October   2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {2},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-008-0097-y},
doi = {10.1007/s10618-008-0097-y},
abstract = {Many business applications track changes over time, for example, measuring the monthly prevalence of influenza incidents. In situations where a classifier is needed to identify the relevant incidents, imperfect classification accuracy can cause substantial bias in estimating class prevalence. The paper defines two research challenges for machine learning. The `quantification' task is to accurately estimate the number of positive cases (or class distribution) in a test set, using a training set that may have a substantially different distribution. The `cost quantification' variant estimates the total cost associated with the positive class, where each case is tagged with a cost attribute, such as the expense to resolve the case. Quantification has a very different utility model from traditional classification research. For both forms of quantification, the paper describes a variety of methods and evaluates them with a suitable methodology, revealing which methods give reliable estimates when training data is scarce, the testing class distribution differs widely from training, and the positive class is rare, e.g., 1% positives. These strengths can make quantification practical for business use, even where classification accuracy is poor.},
journal = {Data Min. Knowl. Discov.},
month = oct,
pages = {164–206},
numpages = {43},
keywords = {Class distribution estimation, Class imbalance, Classification, Concept drift, Detecting and tracking trends, Prevalence estimation, Quantification research methodology, Supervised machine learning, Text mining}
}

@inproceedings{10.1145/1321631.1321711,
author = {Botterweck, Goetz and O'Brien, Liam and Thiel, Steffen},
title = {Model-driven derivation of product architectures},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321711},
doi = {10.1145/1321631.1321711},
abstract = {Product Derivation is one of the central activities in Software Product Lines (SPL). One of the main challenges of the process of product derivation is dealing with complexity, which is caused by the large number of artifacts and dependencies between them. Another major challenge is maximizing development efficiency and reducing time-to-market, while at the same time producing high quality products. One approach to overcome these challenges is to automate the derivation process. To this end, this paper focuses on one particular activity of the derivation process; the derivation of the product-specific architecture and describes how this activity can be automated using a model-driven approach. The approach derives the product-specific architecture by selectively copying elements from the product-line architecture. The decision, which elements are included in the derived architecture, is based on a product-specific feature configuration. We present a prototype that implements the derivation as a model transformation described in the Atlas Transformation Language (ATL). We conclude with a short overview of related work and directions for future research},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {469–472},
numpages = {4},
keywords = {ATL, model transformation, model-driven approaches, product derivation, software architectures, software product lines},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@article{10.1007/s10994-018-5710-8,
author = {Muggleton, Stephen and Dai, Wang-Zhou and Sammut, Claude and Tamaddoni-Nezhad, Alireza and Wen, Jing and Zhou, Zhi-Hua},
title = {Meta-Interpretive Learning from noisy images},
year = {2018},
issue_date = {July      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {107},
number = {7},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-018-5710-8},
doi = {10.1007/s10994-018-5710-8},
abstract = {Statistical machine learning is widely used in image classification. However, most techniques (1) require many images to achieve high accuracy and (2) do not provide support for reasoning below the level of classification, and so are unable to support secondary reasoning, such as the existence and position of light sources and other objects outside the image. This paper describes an Inductive Logic Programming approach called Logical Vision which overcomes some of these limitations. LV uses Meta-Interpretive Learning (MIL) combined with low-level extraction of high-contrast points sampled from the image to learn recursive logic programs describing the image. In published work LV was demonstrated capable of high-accuracy prediction of classes such as regular polygon from small numbers of images where Support Vector Machines and Convolutional Neural Networks gave near random predictions in some cases. LV has so far only been applied to noise-free, artificially generated images. This paper extends LV by (a) addressing classification noise using a new noise-telerant version of the MIL system Metagol, (b) addressing attribute noise using primitive-level statistical estimators to identify sub-objects in real images, (c) using a wider class of background models representing classical 2D shapes such as circles and ellipses, (d) providing richer learnable background knowledge in the form of a simple but generic recursive theory of light reflection. In our experiments we consider noisy images in both natural science settings and in a RoboCup competition setting. The natural science settings involve identification of the position of the light source in telescopic and microscopic images, while the RoboCup setting involves identification of the position of the ball. Our results indicate that with real images the new noise-robust version of LV using a single example (i.e. one-shot LV) converges to an accuracy at least comparable to a thirty-shot statistical machine learner on both prediction of hidden light sources in the scientific settings and in the RoboCup setting. Moreover, we demonstrate that a general background recursive theory of light can itself be invented using LV and used to identify ambiguities in the convexity/concavity of objects such as craters in the scientific setting and partial obscuration of the ball in the RoboCup setting.},
journal = {Mach. Learn.},
month = jul,
pages = {1097–1118},
numpages = {22},
keywords = {High-level vision, Inductive logic programming, Meta-interpretive learning, Robocup}
}

@article{10.1007/s10844-021-00675-4,
author = {Vidal, Cristian and Felfernig, Alexander and Galindo, Jos\'{e} and Atas, M\"{u}sl\"{u}m and Benavides, David},
title = {Explanations for over-constrained problems using QuickXPlain with speculative executions},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {57},
number = {3},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-021-00675-4},
doi = {10.1007/s10844-021-00675-4},
abstract = {Conflict detection is used in various scenarios ranging from interactive decision making (e.g., knowledge-based configuration) to the diagnosis of potentially faulty models (e.g., using knowledge base analysis operations). Conflicts can be regarded as sets of restrictions (constraints) causing an inconsistency. Junker’s QuickXPlain is a divide-and-conquer based algorithm for the detection of preferred minimal conflicts. In this article, we present a novel approach to the detection of such conflicts which is based on speculative programming. We introduce a parallelization of QuickXPlain and empirically evaluate this approach on the basis of synthesized knowledge bases representing feature models. The results of this evaluation show significant performance improvements in the parallelized QuickXPlain version.},
journal = {J. Intell. Inf. Syst.},
month = dec,
pages = {491–508},
numpages = {18},
keywords = {Speculative programming, Conflict detection, Explanations, Constraint solving, Configuration, Diagnosis, Feature models}
}

@inproceedings{10.5555/3005729.3005734,
author = {Selgrad, Kai and Lier, Alexander and D\"{o}rntlein, Jan and Reiche, Oliver and Marc Stamminger, Marc},
title = {A High-Performance Image Processing DSL for Heterogeneous Architectures},
year = {2016},
isbn = {9782955747407},
publisher = {European Lisp Scientific Activities Association},
abstract = {Over the last decade a number of high performance, domainspecific languages (DSLs) have started to grow and help tackle the problem of ever diversifying hard- and software employed in fields such as HPC (high performance computing), medical imaging, computer vision etc. Most of those approaches rely on frameworks such as LLVM for efficient code generation and, to reach a broader audience, take input in C-like form. In this paper we present a DSL for image processing that is on-par with competing methods, yet its design principles are in strong contrast to previous approaches. Our tool chain is much simpler, easing the burden on implementors and maintainers, while our output, C-family code, is both adaptable and shows high performance. We believe that our methodology provides a faster evaluation of language features and abstractions in the domains above.},
booktitle = {Proceedings of the 9th European Lisp Symposium on European Lisp Symposium},
articleno = {5},
keywords = {Common Lisp, Domain Specific Language, Generative Programming, Meta Programming},
location = {Krak\'{o}w, Poland},
series = {ELS2016}
}

@article{10.1016/j.dss.2012.03.001,
author = {Schumaker, Robert P. and Zhang, Yulei and Huang, Chun-Neng and Chen, Hsinchun},
title = {Evaluating sentiment in financial news articles},
year = {2012},
issue_date = {June, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {53},
number = {3},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2012.03.001},
doi = {10.1016/j.dss.2012.03.001},
abstract = {Can the choice of words and tone used by the authors of financial news articles correlate to measurable stock price movements? If so, can the magnitude of price movement be predicted using these same variables? We investigate these questions using the Arizona Financial Text (AZFinText) system, a financial news article prediction system, and pair it with a sentiment analysis tool. Through our analysis, we found that subjective news articles were easier to predict in price direction (59.0% versus 50.0% of chance alone) and using a simple trading engine, subjective articles garnered a 3.30% return. Looking further into the role of author tone in financial news articles, we found that articles with a negative sentiment were easiest to predict in price direction (50.9% versus 50.0% of chance alone) and a 3.04% trading return. Investigating negative sentiment further, we found that our system was able to predict price decreases in articles of a positive sentiment 53.5% of the time, and price increases in articles of a negative sentiment 52.4% of the time. We believe that perhaps this result can be attributable to market traders behaving in a contrarian manner, e.g., see good news, sell; see bad news, buy.},
journal = {Decis. Support Syst.},
month = jun,
pages = {458–464},
numpages = {7},
keywords = {Business intelligence, Financial prediction, Sentiment analysis, Text mining}
}

@inproceedings{10.1145/3340531.3412007,
author = {Chih, Hao-Yi and Fan, Yao-Chung and Peng, Wen-Chih and Kuo, Hai-Yuan},
title = {Product Quality Prediction with Convolutional Encoder-Decoder Architecture and Transfer Learning},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412007},
doi = {10.1145/3340531.3412007},
abstract = {Mining data collected from industrial manufacturing process plays an important role for intelligent manufacturing in Industry 4.0. In this paper, we propose a deep convolutional model for predicting wafer fabrication quality in an intelligent integrated-circuit manufacturing application. The wafer fabrication quality prediction is motivated by the need for improving product line efficiency and reducing manufacturing cost by detecting potential defective work-in-process (WIP) wafers. This work considers the following two crucial data characteristics for wafer fabrication. First, our model is designed to learn spatial correlation between quality measurements on WIP wafers and fabrication results through an encoder-decoder neural network. Second, we leverage the fact that different products share the same raw manufacturing process to enable the knowledge transferring between prediction models of different products. Performance evaluation on real data sets is conducted to validate the strengths of our model on quality prediction, model interpretability, and feasibility of transferring knowledge.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {195–204},
numpages = {10},
keywords = {few-shot learning, industrial data mining, multi-task learning, product quality prediction, transfer learning},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1016/j.neucom.2018.04.001,
author = {Lu, Quanmao and Li, Xuelong and Dong, Yongsheng},
title = {Structure preserving unsupervised feature selection},
year = {2018},
issue_date = {August 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {301},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.001},
doi = {10.1016/j.neucom.2018.04.001},
abstract = {Spectral analysis was usually used to guide unsupervised feature selection. However, the performances of these methods are not always satisfactory due to that they may generate continuous pseudo labels to approximate the discrete real labels. In this paper, a novel unsupervised feature selection method is proposed based on self-expression model. Unlike existing spectral analysis based methods, we utilize self-expression model to capture the relationships between the features without learning the cluster labels. Specifically, each feature can be reconstructed by using a linear combination of all the features in the original feature space, and a representative feature should give a large weight to reconstruct other features. Besides, a structure preserved constraint is incorporated into our model for keeping the local manifold structure of the data. Then an efficient alternative iterative algorithm is utilized to solve our proposed model with the theoretical analysis on its convergence. The experimental results on different datasets show the effectiveness of our method.},
journal = {Neurocomput.},
month = aug,
pages = {36–45},
numpages = {10},
keywords = {Self-expression model, Structure preserving, Unsupervised feature selection}
}

@article{10.1109/TASLP.2018.2821903,
author = {Fu, Szu-Wei and Wang, Tao-Wei and Tsao, Yu and Lu, Xugang and Kawai, Hisashi},
title = {End-to-End Waveform Utterance Enhancement for Direct Evaluation Metrics Optimization by Fully Convolutional Neural Networks},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2821903},
doi = {10.1109/TASLP.2018.2821903},
abstract = {Speech enhancement model is used to map a noisy speech to a clean speech. In the training stage, an objective function is often adopted to optimize the model parameters. However, in the existing literature, there is an inconsistency between the model optimization criterion and the evaluation criterion for the enhanced speech. For example, in measuring speech intelligibility, most of the evaluation metric is based on a short-time objective intelligibility STOI measure, while the frame based mean square error MSE between estimated and clean speech is widely used in optimizing the model. Due to the inconsistency, there is no guarantee that the trained model can provide optimal performance in applications. In this study, we propose an end-to-end utterance-based speech enhancement framework using fully convolutional neural networks FCN to reduce the gap between the model optimization and the evaluation criterion. Because of the utterance-based optimization, temporal correlation information of long speech segments, or even at the entire utterance level, can be considered to directly optimize perception-based objective functions. As an example, we implemented the proposed FCN enhancement framework to optimize the STOI measure. Experimental results show that the STOI of a test speech processed by the proposed approach is better than conventional MSE-optimized speech due to the consistency between the training and the evaluation targets. Moreover, by integrating the STOI into model optimization, the intelligibility of human subjects and automatic speech recognition system on the enhanced speech is also substantially improved compared to those generated based on the minimum MSE criterion.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1570–1584},
numpages = {15}
}

@inproceedings{10.5555/3298483.3298519,
author = {Gong, Chen},
title = {Exploring commonality and individuality for multi-modal curriculum learning},
year = {2017},
publisher = {AAAI Press},
abstract = {Curriculum Learning (CL) mimics the cognitive process of humans and favors a learning algorithm to follow the logical learning sequence from simple examples to more difficult ones. Recent studies show that selecting the simplest curriculum examples from different modalities for graph-based label propagation can yield better performance than simply leveraging single modality. However, they forcibly require the curriculums generated by all modalities to be identical to a common curriculum, which discard the individuality of every modality and produce the inaccurate curriculum for the subsequent learning. Therefore, this paper proposes a novel multi-modal CL algorithm by comprehensively investigating both the individuality and commonality of different modalities. By considering the curriculums of multiple modalities altogether, their common preference on selecting the simplest examples can be explored by a row-sparse matrix, and their distinct opinions are captured by a sparse noise matrix. As a consequence, a "soft" fusion of multiple curriculums from different modalities is achieved and the propagation quality can thus be improved. Comprehensive empirical studies reveal that our method can generate higher accuracy than the state-of-the-art multi-modal CL approach and label propagation algorithms on various image classification tasks.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {1926–1933},
numpages = {8},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.1007/s10845-017-1307-5,
author = {Pourjavad, Ehsan and Mayorga, Rene V.},
title = {A comparative study and measuring performance of manufacturing systems with Mamdani fuzzy inference system},
year = {2019},
issue_date = {Mar 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {3},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-017-1307-5},
doi = {10.1007/s10845-017-1307-5},
abstract = {In today's competitive environment, measuring companies' performance properly has become a vital subject not only for investors but also for the companies that are working in the same sector. The achieved results of performance measurement can help managers to identify means of improvement, measure progress and find unknown problems in the company. There are many efficiency frontier analysis methods to evaluate performance; but, each of these methods has its strength as well as major limitations. In this article, a fuzzy approach based on Mamdani fuzzy inference system is presented for performance measurement of manufacturing systems. The generation of fuzzy rules is the biggest consideration in designing the proposed model. In fact, fuzzy inference rules model human reasoning and are embedded in the system, which is an advantage when compared to approaches that combine fuzzy set theory with multi-criteria decision-making methods. A fuzzy inference system is constructed and applied to measure the performance or efficiency of manufacturing systems. Implementation of the proposed model is analyzed and discussed using a real case. The results reveal the usefulness of the proposed model in evaluating the performance of manufacturing companies.},
journal = {J. Intell. Manuf.},
month = mar,
pages = {1085–1097},
numpages = {13},
keywords = {Criteria, Efficiency, Fuzzy inference system, Manufacturing systems, Performance measurement}
}

@article{10.1016/j.infsof.2011.09.003,
author = {Conejero, Jos\'{e} M. and Figueiredo, Eduardo and Garcia, Alessandro and Hern\'{a}ndez, Juan and Jurado, Elena},
title = {On the relationship of concern metrics and requirements maintainability},
year = {2012},
issue_date = {February, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.09.003},
doi = {10.1016/j.infsof.2011.09.003},
abstract = {Context: Maintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability. Objective: In this setting, this paper introduces an empirical analysis where the correlation between crosscutting properties and two ISO/IEC 9126 maintainability attributes, namely changeability and stability, is presented. Method: This correlation is based on the utilization of a set of concern metrics that allows the quantification of crosscutting, scattering and tangling. Results: Our study confirms that a change in a crosscutting concern is more difficult to be accomplished and that artefacts addressing crosscutting concerns are found to be less stable later as the system evolves. Moreover, our empirical analysis reveals that crosscutting properties introduce non-syntactic dependencies between software artefacts, thereby decreasing the quality of software in terms of changeability and stability as well. These subtle dependencies cannot be easily detected without the use of concern metrics. Conclusion: The correlation provides evidence that the presence of certain crosscutting properties negatively affects to changeability and stability. The whole analysis is performed using as target cases three software product lines, where maintainability properties are of upmost importance not only for individual products but also for the core architecture of the product line.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {212–238},
numpages = {27},
keywords = {Concern metrics, Crosscutting, Maintainability, Product lines, Requirements engineering, Stability}
}

@article{10.1016/j.infsof.2019.01.004,
author = {Souza, Eric and Moreira, Ana and Goul\~{a}o, Miguel},
title = {Deriving architectural models from requirements specifications: A systematic mapping study},
year = {2019},
issue_date = {May 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {109},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.01.004},
doi = {10.1016/j.infsof.2019.01.004},
journal = {Inf. Softw. Technol.},
month = may,
pages = {26–39},
numpages = {14},
keywords = {Software architecture, Mapping study, Literature review}
}

@inproceedings{10.5555/3045390.3045466,
author = {Patrini, Giorgio and Nielsen, Frank and Nock, Richard and Carioni, Marcello},
title = {Loss factorization, weakly supervised learning and label noise robustness},
year = {2016},
publisher = {JMLR.org},
abstract = {We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the same loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator -- the focal quantity of this work -- which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation.Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like SGD and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {708–717},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@article{10.1016/j.infsof.2019.05.013,
author = {Chen, Tao and Li, Miqing and Yao, Xin},
title = {Standing on the shoulders of giants: Seeding search-based multi-objective optimization with prior knowledge for software service composition},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.013},
doi = {10.1016/j.infsof.2019.05.013},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {155–175},
numpages = {21},
keywords = {Service composition, Search-based software engineering, Multi-objective optimization, Evolutionary algorithm, Seeding strategy}
}

@inproceedings{10.1007/11430919_29,
author = {Janssens, Gerrit K. and S\"{o}srensen, Kenneth and Lim\`{e}re, Arthur and Vanhoof, Koen},
title = {Analysis of company growth data using genetic algorithms on binary trees},
year = {2005},
isbn = {3540260765},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11430919_29},
doi = {10.1007/11430919_29},
abstract = {This paper investigates why some companies grow faster than others, by data mining a survey of a large number of companies in Flanders (the northern part of Belgium). Faster or slower average growth over a time period is explained by building a classification tree containing several categorical variables (both quantitative and qualitative). The technique used – called genAID – splits the population at different levels. It is inspired by the Automatic Interaction Detector (AID) technique to find trees that explain the variability in average growth but uses a genetic algorithm to overcome some of the drawbacks of AID.Classical AID or other tree-growing techniques usually generate a single tree for interpretation. This approach has been criticized because, due to the artifacts of data, spurious interactions may occur. genAID offers the user-analyst a set of trees, which are the best ones found over a number of generations of the genetic algorithm. The user-analyst is then offered the choice of choosing a tree by trading off explanatory power against either the ease of understanding or the conformity with an existing theory.},
booktitle = {Proceedings of the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining},
pages = {234–239},
numpages = {6},
location = {Hanoi, Vietnam},
series = {PAKDD'05}
}

@inproceedings{10.5555/645654.665656,
author = {Pal, Sankar K.},
title = {Soft Computing Pattern Recognition: Principles, Integrations, and Data Mining},
year = {2001},
isbn = {3540430709},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Relevance of fuzzy logic, artificial neural networks, genetic algorithms and rough sets to pattern recognition and image processing problems is described through examples. Different integrations of these soft computing tools are illustrated. Evolutionary rough fuzzy network which is based on modular principle is explained, as an example of integrating all the four tools for efficient classification and rule generation, with its various characterstics. Significance of soft computing approach in data mining and knowledge discovery is finally discussed along with the scope of future research.},
booktitle = {Proceedings of the Joint JSAI 2001 Workshop on New Frontiers in Artificial Intelligence},
pages = {261–271},
numpages = {11}
}

@inproceedings{10.1145/3490035.3490262,
author = {Bansal, Rahul and Biswas, Soma},
title = {CT-DANN: co-teaching meets DANN for wild unsupervised domain adaptation},
year = {2021},
isbn = {9781450375962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490035.3490262},
doi = {10.1145/3490035.3490262},
abstract = {Unsupervised domain adaptation aims at leveraging supervision from an annotated source domain for performing tasks like classification/segmentation on an unsupervised target domain. However, a large enough related dataset with clean annotations may not be always available in real scenarios, since annotations are usually obtained from crowd sourcing, and thus are noisy. Here, we consider a more realistic and challenging setting, wild unsupervised domain adaptation (WUDA), where the source domain annotations can be noisy. Standard domain adaptation approaches which directly use these noisy source labels and the unlabeled targets for the domain adaptation task perform poorly, due to severe negative transfer from the noisy source domain. In this work, we propose a novel end-to-end framework, termed CT-DANN (Co-teaching meets DANN), which seamlessly integrates a state-of-the-art approach for handling noisy labels (Co-teaching) with a standard domain adaptation framework (DANN). CT-DANN effectively utilizes all the source samples after accounting for both their noisy labels as well as transferability with respect to the target domain. Extensive experiments on three benchmark datasets with different types and levels of noise and comparison with state-of-the-art WUDA approach justify the effectiveness of the proposed framework.},
booktitle = {Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {5},
numpages = {8},
keywords = {co-teaching, noisy source data, source data weighting, wild unsupervised domain adaptation},
location = {Jodhpur, India},
series = {ICVGIP '21}
}

@inbook{10.5555/3454287.3454823,
author = {Wang, Siqi and Zeng, Yijie and Liu, Xinwang and Zhu, En and Yin, Jianping and Xu, Chuanfu and Kloft, Marius},
title = {Effective end-to-end unsupervised outlier detection via inlier priority of discriminative network},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Despite the wide success of deep neural networks (DNN), little progress has been made on end-to-end unsupervised outlier detection (UOD) from high dimensional data like raw images. In this paper, we propose a framework named E3 Outlier, which can perform UOD in a both effective and end-to-end manner: First, instead of the commonly-used autoencoders in previous end-to-end UOD methods, E3 Outlier for the first time leverages a discriminative DNN for better representation learning, by using surrogate supervision to create multiple pseudo classes from original unla-belled data. Next, unlike classic UOD that utilizes data characteristics like density or proximity, we exploit a novel property named inlier priority to enable end-to-end UOD by discriminative DNN. We demonstrate theoretically and empirically that the intrinsic class imbalance of inliers/outliers will make the network prioritize minimizing inliers' loss when inliers/outliers are indiscriminately fed into the network for training, which enables us to differentiate outliers directly from DNN's outputs. Finally, based on inlier priority, we propose the negative entropy based score as a simple and effective outlierness measure. Extensive evaluations show that E3 Outlier significantly advances UOD performance by up to 30% AUROC against state-of-the-art counterparts, especially on relatively difficult benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {536},
numpages = {14}
}

@article{10.1016/j.jss.2019.03.027,
author = {Xu, Zhou and Li, Shuai and Luo, Xiapu and Liu, Jin and Zhang, Tao and Tang, Yutian and Xu, Jun and Yuan, Peipei and Keung, Jacky},
title = {TSTSS: A two-stage training subset selection framework for cross version defect prediction},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.027},
doi = {10.1016/j.jss.2019.03.027},
journal = {J. Syst. Softw.},
month = aug,
pages = {59–78},
numpages = {20},
keywords = {Cross version defect prediction, Spare modeling, Training subset selection, Weighted extreme learning machine, 00–01, 99-00}
}

@inproceedings{10.3115/993268.993299,
author = {Matiasek, Johannes and Trost, Harald},
title = {An HPSG-based generator for German: an experiment in the reusability of linguistic resources},
year = {1996},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/993268.993299},
doi = {10.3115/993268.993299},
abstract = {We describe the development of a generator for German built by reusing and adapting existing linguistic data and software. Reusability is crucial for the successful application of NLP techniques to real-life problems since it helps to cut down on both development and adaptation effort. However, combining resources not designed to work together is not trivial. We describe the problems arising when integrating three preexisting resources (FUF, a unification-based generator, an HPSG Grammar for German, and X2MorF, a two-level morphology component) and the adaptations necessary to come up with a wide coverage tactical generator for German.},
booktitle = {Proceedings of the 16th Conference on Computational Linguistics - Volume 2},
pages = {752–757},
numpages = {6},
location = {Copenhagen, Denmark},
series = {COLING '96}
}

@article{10.1177/1046878111434474,
author = {Cannon, James N. and Cannon, Hugh M. and Schwaiger, Manfred},
title = {Modeling the “Profitable-Product Death Spiral”: Accounting for Strategic Product-Mix Interactions in Marketing Simulation Games},
year = {2012},
issue_date = {December  2012},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {43},
number = {6},
issn = {1046-8781},
url = {https://doi.org/10.1177/1046878111434474},
doi = {10.1177/1046878111434474},
abstract = {Business simulation game designers typically ignore product line interactions in the design of marketing simulation games. This article addresses the failing by modeling Rust, Zeithaml, and Lemon's concept of the profitable-product death spiral, a product-mix interaction theory based on the concept of customer lifetime value (CLV). According to their theory, marketers often enter a cycle of decreasing demand by deleting less profitable products. When customers seek multiple products from the same company, the deletion of less profitable ones will often reduce demand for more profitable products as well, rendering them less profitable. Unchecked, the cycle continues until the company fails. This article discusses how to model the \^{a} death-spiral\^{a} effect by adapting Teach's gravity-flow model to evaluate the product mix as a kind of \^{a} meta-product,\^{a} where desired products function as product attributes.},
journal = {Simul. Gaming},
month = dec,
pages = {761–777},
numpages = {17},
keywords = {business simulation game, customer equity, customer lifetime value, death spiral, gravity-flow model, marketing simulations, meta-product, product line interactions, product-mix strategy, profitable-product death spiral, strategic product-mix decisions}
}

@article{10.1016/j.procs.2021.01.271,
author = {Mourtzis, Dimitris and Angelopoulos, John and Panopoulos, Nikos},
title = {Equipment Design Optimization Based on Digital Twin Under the Framework of Zero-Defect Manufacturing},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {180},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.01.271},
doi = {10.1016/j.procs.2021.01.271},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {525–533},
numpages = {9},
keywords = {Digital Twin, Machine Design, Zero-Defect Manufacturing}
}

@inproceedings{10.5555/2888116.2888161,
author = {Zhao, Qian and Meng, Deyu and Jiang, Lu and Xie, Qi and Xu, Zongben and Hauptmann, Alexander G.},
title = {Self-paced learning for matrix factorization},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Matrix factorization (MF) has been attracting much attention due to its wide applications. However, since MF models are generally non-convex, most of the existing methods are easily stuck into bad local minima, especially in the presence of outliers and missing data. To alleviate this deficiency, in this study we present a new MF learning methodology by gradually including matrix elements into MF training from easy to complex. This corresponds to a recently proposed learning fashion called self-paced learning (SPL), which has been demonstrated to be beneficial in avoiding bad local minima. We also generalize the conventional binary (hard) weighting scheme for SPL to a more effective real-valued (soft) weighting manner. The effectiveness of the proposed self-paced MF method is substantiated by a series of experiments on synthetic, structure from motion and background subtraction data.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {3196–3202},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.5555/3489212.3489216,
author = {Yu, Lingjing and Luo, Bo and Ma, Jun and Zhou, Zhaoyu and Liu, Qingyun},
title = {You are what you broadcast: identification of mobile and IoT devices from (public) WiFi},
year = {2020},
isbn = {978-1-939133-17-5},
publisher = {USENIX Association},
address = {USA},
abstract = {With the rapid growth of mobile devices and WiFi hotspots, security risks arise. In practice, it is critical for administrators of corporate and public wireless networks to identify the type and/or model of devices connected to the network, in order to set access/firewall rules, to check for known vulnerabilities, or to configure IDS accordingly. Mobile devices are not obligated to report their detailed identities when they join a (public) wireless network, while adversaries could easily forge device attributes. In the literature, efforts have been made to utilize features from network traffic for device identification. In this paper, we present OWL, a novel device identification mechanism for both network administrators and normal users. We first extract network traffic features from passively received broadcast and multicast (BC/MC) packets. Embedding representations are learned to model features into six independent and complementary views. We then present a new multi-view wide and deep learning (MvWDL) framework that is optimized on both generalization performance and label-view interaction performance. Meanwhile, a malicious device detection mechanism is designed to assess the inconsistencies across views in the multi-view classifier to identify anomalies. Finally, we demonstrate OWL's performance through experiments, case studies, and qualitative analysis.},
booktitle = {Proceedings of the 29th USENIX Conference on Security Symposium},
articleno = {4},
numpages = {18},
series = {SEC'20}
}

@inproceedings{10.5555/3305890.3305963,
author = {Pad, Pedram and Salehi, Farnood and Celis, Elisa and Thiran, Patrick and Unser, Michael},
title = {Dictionary learning based on sparse distribution tomography},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a new statistical dictionary learning algorithm for sparse signals that is based on an α-stable innovation model. The parameters of the underlying model—that is, the atoms of the dictionary, the sparsity index α and the dispersion of the transform-domain coefficients—are recovered using a new type of probability distribution tomography. Specifically, we drive our estimator with a series of random projections of the data, which results in an efficient algorithm. Moreover, since the projections are achieved using linear combinations, we can invoke the generalized central limit theorem to justify the use of our method for sparse signals that are not necessarily α-stable. We evaluate our algorithm by performing two types of experiments: image in-painting and image denoising. In both cases, we find that our approach is competitive with state-of-the-art dictionary learning techniques. Beyond the algorithm itself, two aspects of this study are interesting in their own right. The first is our statistical formulation of the problem, which unifies the topics of dictionary learning and independent component analysis. The second is a generalization of a classical theorem about isometries of ℓp-norms that constitutes the foundation of our approach.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2731–2740},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{10.1016/j.csi.2017.11.007,
author = {Souri, Alireza and Navimipour, Nima Jafari and Rahmani, Amir Masoud},
title = {Formal verification approaches and standards in the cloud computing},
year = {2018},
issue_date = {May 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {58},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2017.11.007},
doi = {10.1016/j.csi.2017.11.007},
abstract = {Providing a summary of the current challenges related to the formal verification approaches in the cloud computing.Presenting a Systematic Literature Review (SLR) method for the formal verification approaches in the cloud computing topics.Discussing the important factors of formal verification approaches in the cloud environment to improve their problems in the future. Cloud computing as a new internet-based computing model provides different resources as a service dynamically. Today, cloud computing is actually one of the main improvements in the computing procedure. However, by raising user interactions, the complexity of cloud processes is increasing with the advancement of technology. To evaluate the cloud computing challenges, the simulation experiments just satisfy the non-functional properties with a limited majority in forms of QoS factors. In addition, using simulation approaches have not been sufficient for developed complex cloud services that omit some critical test cases in the state space of the model. On the other hand, formal verification is an essential section in the complex information systems development that satisfies both functional and non-functional properties. Therefore, it is essential that the cloud systems use formal verification approaches for increasing the correctness of the system quality in all of the state space of the model. Despite the importance of the formal verification approaches in the cloud environments, to the best of our knowledge, there is not any systematic, comprehensive and detailed survey and review in the field of formal verification approaches and standards in the cloud computing. This paper provides a Systematic Literature Review (SLR) method to examine the current technical studies (published between 2011 and July 2017) in formal verification of the cloud computing. Also, this paper categorizes the formal verification approaches in three classic fields: specification and process algebra, model checking, and theorem proving. The verification approaches are compared with each other according to some technical properties such as specification methods, modeling approaches, verification tools and verification methods. The advantages and disadvantages of each selected study as well as some hints are discussed for solving their problems. The brief contributions of this paper are as follows: (1) providing a comprehensive literature review of the formal verification approaches in the cloud computing, (2) designing a technical taxonomy for the verification approaches in various modeling and specification methods, (3) presenting a technical analysis and comparison for the main challenges of the formal verification in the cloud and (4) highlighting the future open issues in the recent topics.},
journal = {Comput. Stand. Interfaces},
month = may,
pages = {1–22},
numpages = {22},
keywords = {Cloud computing, Formal verification, Specification, Systematic literature review}
}

@article{10.1007/s11263-019-01176-2,
author = {He, Xiangteng and Peng, Yuxin and Zhao, Junjie},
title = {Which and How Many Regions to Gaze: Focus Discriminative Regions for Fine-Grained Visual Categorization},
year = {2019},
issue_date = {September 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {9},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-019-01176-2},
doi = {10.1007/s11263-019-01176-2},
abstract = {Fine-grained visual categorization (FGVC) aims to discriminate similar subcategories that belong to the same superclass. Since the distinctions among similar subcategories are quite subtle and local, it is highly challenging to distinguish them from each other even for humans. So the localization of distinctions is essential for fine-grained visual categorization, and there are two pivotal problems: (1) Which regions are discriminative and representative to distinguish from other subcategories? (2) How many discriminative regions are necessary to achieve the best categorization performance? It is still difficult to address these two problems adaptively and intelligently. Artificial prior and experimental validation are widely used in existing mainstream methods to discover which and how many regions to gaze. However, their applications extremely restrict the usability and scalability of the methods. To address the above two problems, this paper proposes a multi-scale and multi-granularity deep reinforcement learning approach (M2DRL), which learns multi-granularity discriminative region attention and multi-scale region-based feature representation. Its main contributions are as follows: (1) Multi-granularity discriminative localization is proposed to localize the distinctions via a two-stage deep reinforcement learning approach, which discovers the discriminative regions with multiple granularities in a hierarchical manner ("which problem"), and determines the number of discriminative regions in an automatic and adaptive manner ("how many problem"). (2) Multi-scale representation learning helps to localize regions in different scales as well as encode images in different scales, boosting the fine-grained visual categorization performance. (3) Semantic reward function is proposed to drive M2DRL to fully capture the salient and conceptual visual information, via jointly considering attention and category information in the reward function. It allows the deep reinforcement learning to localize the distinctions in a weakly supervised manner or even an unsupervised manner. (4) Unsupervised discriminative localization is further explored to avoid the heavy labor consumption of annotating, and extremely strengthen the usability and scalability of our M2DRL approach. Compared with state-of-the-art methods on two widely-used fine-grained visual categorization datasets, our M2DRL approach achieves the best categorization accuracy.},
journal = {Int. J. Comput. Vision},
month = sep,
pages = {1235–1255},
numpages = {21},
keywords = {Deep reinforcement learning, Fine-grained visual categorization, Multi-granularity discriminative localization, Multi-scale representation learning, Semantic reward, Unsupervised discriminative localization}
}

@article{10.1504/IJAISC.2014.062815,
author = {Bali, Shivani and Jha, P. C. and Kumar, U. Dinesh and Pham, Hoang},
title = {Fuzzy multi-objective build-or-buy approach for component selection of fault tolerant software system under consensus recovery block scheme with mandatory redundancy in critical modules},
year = {2014},
issue_date = {June 2014},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {4},
number = {2/3},
issn = {1755-4950},
url = {https://doi.org/10.1504/IJAISC.2014.062815},
doi = {10.1504/IJAISC.2014.062815},
abstract = {During the last two decades, there has been a growing interest in component-based software engineering CBSE both in academia and industry. In component-based system development, it is common to identify software modules first. Once they are identified, we need to select appropriate software components for each module. These components can either be bought as commercial off-the-shelf COTS components and probably adapted to work in the software system or can be developed in-house. This is a 'build-or-buy' decision. This paper discusses a framework that helps a developer to decide whether to buy or to build software components while designing a fault-tolerant modular software system. This paper proposes optimisation models for optimal component selection for a fault-tolerant modular software system under the consensus recovery block scheme. It is necessary to identify critical modules in the design of a fault-tolerant modular software system and also to develop a system with a built in redundancy for critical modules. Therefore, the first optimisation model is developed for optimal component selection with the dual objective of reliability maximisation and cost minimisation of the overall system under the constraints on the delivery time and criticality of modules. The second optimisation model is an extension of the first optimisation model and discusses the issue of compatibility of components of modules. In practice, it is not possible for management to obtain precise value of reliability, cost, delivery time, etc., therefore both the models are formulated as fuzzy multi-objective optimisation models. A case study of developing a manufacturing system for medium-size enterprise is used to illustrate the proposed methodology.},
journal = {Int. J. Artif. Intell. Soft Comput.},
month = jun,
pages = {98–119},
numpages = {22}
}

@article{10.1007/s11634-014-0179-1,
author = {Groenen, Patrick J. and Roux, Ni\"{e}l J. and Gardner-Lubbe, Sugnet},
title = {Spline-based nonlinear biplots},
year = {2015},
issue_date = {June      2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {2},
issn = {1862-5347},
url = {https://doi.org/10.1007/s11634-014-0179-1},
doi = {10.1007/s11634-014-0179-1},
abstract = {Biplots are helpful tools to establish the relations between samples and variables in a single plot. Most biplots use a projection interpretation of sample points onto linear lines representing variables. These lines can have marker points to make it easy to find the reconstructed value of the sample point on that variable. For classical multivariate techniques such as principal components analysis, such linear biplots are well established. Other visualization techniques for dimension reduction, such as multidimensional scaling, focus on an often nonlinear mapping in a low dimensional space with emphasis on the representation of the samples. In such cases, the linear biplot can be too restrictive to properly describe the relations between the samples and the variables. In this paper, we propose a simple nonlinear biplot that represents the marker points of a variable on a curved line that is governed by splines. Its main attraction is its simplicity of interpretation: the reconstructed value of a sample point on a variable is the value of the closest marker point on the smooth curved line representing the variable. The proposed spline-based biplot can never lead to a worse overall sample fit of the variable as it contains the linear biplot as a special case.},
journal = {Adv. Data Anal. Classif.},
month = jun,
pages = {219–238},
numpages = {20},
keywords = {62H25, Biplot, Multidimensional scaling, Principal components analysis, Splines}
}

@article{10.3233/JIFS-169433,
author = {Srivastava, Smriti and Gopal and Bhardwaj, Saurabh and Thampi, Sabu M. and El-Alfy, El-Sayed M. and Mitra, Sushmita and Trajkovic, Ljiljana},
title = {Multi-scenario dataset for speaker recognition},
year = {2018},
issue_date = {2018},
publisher = {IOS Press},
address = {NLD},
volume = {34},
number = {3},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169433},
doi = {10.3233/JIFS-169433},
abstract = {The present work describes different research techniques for collecting and organizing speech database in different scenario at the institute and successfully structuring the text independent speaker identification database in Indian context. In order to get the Multi-Scenario dataset, each speaker performed multiple sessions recording in reading style with English and Hindi language with same passages but under different conditions. This work analyzed different scenario affecting the performance of speaker recognition system when tested under dissimilar training conditions. Here four different scenarios are considered; sensor and environment, language, aging and health. To study the effect of sensor, language and environment on the performance of ASR system a database of 200 speaker was created. Under different environmental conditions, four different types of sensors in parallel configuration were used to study the sensor mismatch conditions over testing and training phase. The database contains speech samples of the individual in English and Hindi in read speech styles under two environment i.e. a controlled recording chamber and library. To study the aging effect, an aging NSIT speaker database (AG-NSIT-SD) of 53 famous personalities was collected from online source varying over a period of 10–20 years. Further to study the effect of health, a cough and cold NSIT speaker database (CC-NSIT-SD) of 38 speakers was also collected to study the performance of system. Apart from this, the effect of different noise types on the speaker identification was also studied on different sensors.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1385–1392},
numpages = {8},
keywords = {Speaker identification, speaker database, aging database, cough and cold database}
}

@article{10.1016/j.asoc.2019.105586,
author = {Gaud\^{e}ncio, Juliana Helena Daroz and Corr\^{e}a, Jo\~{a}o \'{E}derson and Paes, Vin\'{\i}cius de Carvalho and Campos, Paulo Henrique da Silva and Turrioni, Jo\~{a}o Batista and Paiva, Anderson Paulo de},
title = {Hybrid multiobjective optimization algorithm based on multivariate mean square error and fuzzy decision maker},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {82},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105586},
doi = {10.1016/j.asoc.2019.105586},
journal = {Appl. Soft Comput.},
month = sep,
numpages = {20},
keywords = {Principal component analysis, Normal boundary intersection, Multivariate mean square error, Fuzzy decision maker}
}

@article{10.1007/s00500-021-05766-6,
author = {Agudelo, Oscar Esneider Acosta and Mar\'{\i}n, Carlos Enrique Montenegro and Crespo, Rub\'{e}n Gonz\'{a}lez},
title = {Sound measurement and automatic vehicle classification and counting applied to road traffic noise characterization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {18},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05766-6},
doi = {10.1007/s00500-021-05766-6},
abstract = {Increase in population density in large cities has increased the environmental noise present in these environments, causing negative effects on human health. There are different sources of environmental noise; however, noise from road traffic is the most prevalent in cities. Therefore, it is necessary to have tools that allow noise characterization to establish strategies that permit obtaining levels that do not affect the quality of life of people. This research discusses the implementation of a system that allows the acquisition of data to characterize the noise generated by road traffic. First, the methodology for obtaining acoustic indicators with an electret measurement microphone is described, so that it adjusts to the data collection needs for road traffic noise analyses. Then, an approach for the classification and counting of automatic vehicular traffic through deep learning is presented. Results showed that there were differences of 0.2 dBA in terms of RMSE between a type 1 sound level meter and the measurement microphone used. With reference to vehicle classification and counting for four categories, the approximate error is between 3.3% and -15.5%.},
journal = {Soft Comput.},
month = sep,
pages = {12075–12087},
numpages = {13},
keywords = {Environmental noise, Road traffic, Vehicle, Classification, Deep learning}
}

@article{10.3233/JID210027,
author = {Salinesi, Camille and Achtaich, Asmaa and Souissi, Nissrine and Mazo, Ra\'{u}l and Roudies, Ounsa and Villota, Angela},
title = {State-Constraint Transition: A Language for the Formal Specification of Dynamic Cyber-System Requirements},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {2},
issn = {1092-0617},
url = {https://doi.org/10.3233/JID210027},
doi = {10.3233/JID210027},
abstract = {Existing formal languages for the specification of self-adaptive cyber-physical systems focus on re-configuring the system-to-be depending on its current context, to satisfy the user’s requirements, that is by dynamically composing the software’s structure and behavior. While these approaches specify context-sensitive requirements, they rarely consider their run-time dynamic and scalable nature. The State-Constraint Transition (SCT) modeling language, introduced in this paper, provides an answer to the problems linked to the specification of dynamic requirements by introducing the concept of configuration states, in which requirements are translated into constraints. The expressiveness of existing approaches is thus extended, combining the ease of use of well-established notations, notably those based on characteristics, and those based on Finite-state Machines (FSM), with the computational power and expressiveness of the constraint programming approach. The paper briefly presents the results of the preliminary evaluation, which assesses the expressiveness, scalability, and domain independence of the SCT language.},
journal = {J. Integr. Des. Process Sci.},
month = jan,
pages = {80–99},
numpages = {20},
keywords = {Cyber-physical systems, self-adaptive systems, software product lines, requirements engineering, state-machines, constraint programming, languages}
}

@article{10.1016/j.jss.2011.04.066,
author = {Cirilo, Elder and Nunes, Ingrid and Kulesza, Uir\'{a} and Lucena, Carlos},
title = {Automating the product derivation process of multi-agent systems product lines},
year = {2012},
issue_date = {February, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {2},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.04.066},
doi = {10.1016/j.jss.2011.04.066},
abstract = {Agent-oriented software engineering and software product lines are two promising software engineering techniques. Recent research work has been exploring their integration, namely multi-agent systems product lines (MAS-PLs), to promote reuse and variability management in the context of complex software systems. However, current product derivation approaches do not provide specific mechanisms to deal with MAS-PLs. This is essential because they typically encompass several concerns (e.g., trust, coordination, transaction, state persistence) that are constructed on the basis of heterogeneous technologies (e.g., object-oriented frameworks and platforms). In this paper, we propose the use of multi-level models to support the configuration knowledge specification and automatic product derivation of MAS-PLs. Our approach provides an agent-specific architecture model that uses abstractions and instantiation rules that are relevant to this application domain. In order to evaluate the feasibility and effectiveness of the proposed approach, we have implemented it as an extension of an existing product derivation tool, called GenArch. The approach has also been evaluated through the automatic instantiation of two MAS-PLs, demonstrating its potential and benefits to product derivation and configuration knowledge specification.},
journal = {J. Syst. Softw.},
month = feb,
pages = {258–276},
numpages = {19},
keywords = {Application engineering, Model-driven development, Multi-agent systems, Product derivation tool, Software product lines}
}

@book{10.5555/2600138,
author = {Nettleton, David},
title = {Commercial Data Mining: Processing, Analysis and Modeling for Predictive Analytics Projects},
year = {2014},
isbn = {0124166024},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Whether you are brand new to data mining or working on your tenth predictive analytics project, Commercial Data Mining will be there for you as an accessible reference outlining the entire process and related themes. In this book, you'll learn that your organization does not need a huge volume of data or a Fortune 500 budget to generate business using existing information assets. Expert author David Nettleton guides you through the process from beginning to end and covers everything from business objectives to data sources, and selection to analysis and predictive modeling. Commercial Data Mining includes case studies and practical examples from Nettleton's more than 20 years of commercial experience. Real-world cases covering customer loyalty, cross-selling, and audience prediction in industries including insurance, banking, and media illustrate the concepts and techniques explained throughout the book. Illustrates cost-benefit evaluation of potential projects Includes vendor-agnostic advice on what to look for in off-the-shelf solutions as well as tips on building your own data mining tools Approachable reference can be read from cover to cover by readers of all experience levels Includes practical examples and case studies as well as actionable business insights from author's own experience}
}

@article{10.1007/s10472-014-9418-6,
author = {Berry, Anne and Gutierrez, Alain and Huchard, Marianne and Napoli, Amedeo and Sigayret, Alain},
title = {Hermes: a simple and efficient algorithm for building the AOC-poset of a binary relation},
year = {2014},
issue_date = {October   2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {72},
number = {1–2},
issn = {1012-2443},
url = {https://doi.org/10.1007/s10472-014-9418-6},
doi = {10.1007/s10472-014-9418-6},
abstract = {Given a relation 𝓡 ⊆ 𝓞 𝓐 on a set 𝓞 of objects and a set 𝓐 of attributes, the AOC-poset (Attribute/Object Concept poset), is the partial order defined on the "introducers" of objects and attributes in the corresponding concept lattice. In this paper, we present Hermes, a simple and efficient algorithm for building an AOC-poset which runs in  O (  m   i   n {  n   m ,  n     }), where  n  is the number of objects plus the number of attributes,  m  is the size of the relation, and  n      is the time required to perform matrix multiplication (currently   = 2.376). Finally, we compare the runtime of Hermes with the runtime of other algorithms computing the AOC-poset: Ares, Ceres and Pluton. We characterize the cases where each algorithm is the more relevant.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = oct,
pages = {45–71},
numpages = {27}
}

@article{10.1007/s00354-021-00126-2,
author = {Li, Peipei and Wu, Man and He, Junhong and Hu, Xuegang},
title = {Recurring Drift Detection and Model Selection-Based Ensemble Classification for Data Streams with Unlabeled Data},
year = {2021},
issue_date = {Aug 2021},
publisher = {Ohmsha},
address = {JPN},
volume = {39},
number = {2},
issn = {0288-3635},
url = {https://doi.org/10.1007/s00354-021-00126-2},
doi = {10.1007/s00354-021-00126-2},
abstract = {Data stream classification is widely popular in the field of network monitoring, sensor network and electronic commerce, etc. However, in the real-world applications, recurring concept drifting and label missing in data streams seriously aggravate the difficulty on the classification solutions. And this challenge has received little attention from the research community. Motivated by this, we propose a new ensemble classification approach based on the recurring concept drifting detection and model selection for data streams with unlabeled data. First, we build an ensemble model based on the classifiers and clusters. To improve the classification accuracy, we use the ensemble model to predict each data chunk and partition clusters according to the distribution of predicted class labels. Second, we adopt a new concept drifting detection method based on the divergence of concept distributions between adjoining data chunks to distinguish recurring concept drifts. All historical new concepts will be maintained. Meanwhile, we introduce the time-stamp-based weights for base models in the ensemble model. In the selection of the base model, we consider the time-stamp-based weight and the divergence between concept distributions simultaneously. Finally, extensive experiments conducted on four benchmark data sets show that our approach can quickly adapt to data streams with recurring concept drifts, and improve the classification accuracy compared to several state-of-the-art classification algorithms for data streams with concept drifts and unlabeled data.},
journal = {New Gen. Comput.},
month = aug,
pages = {341–376},
numpages = {36},
keywords = {Data stream classification, Ensemble learning, Recurring concept drift, Unlabeled data}
}

@article{10.1109/TASLP.2016.2536478,
author = {Zhang, Xiao-Lei and Wang, DeLiang},
title = {A deep ensemble learning method for monaural speech separation},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2536478},
doi = {10.1109/TASLP.2016.2536478},
abstract = {Monaural speech separation is a fundamental problem in robust speech processing. Recently, deep neural network (DNN)-based speech separation methods, which predict either clean speech or an ideal time-frequency mask, have demonstrated remarkable performance improvement. However, a single DNN with a given window length does not leverage contextual information sufficiently, and the differences between the two optimization objectives are not well understood. In this paper, we propose a deep ensemble method, named multicontext networks, to address monaural speech separation. The first multicontext network averages the outputs of multiple DNNs whose inputs employ different window lengths. The second multicontext network is a stack of multiple DNNs. Each DNN in a module of the stack takes the concatenation of original acoustic features and expansion of the soft output of the lower module as its input, and predicts the ratio mask of the target speaker; the DNNs in the same module employ different contexts. We have conducted extensive experiments with three speech corpora. The results demonstrate the effectiveness of the proposed method. We have also compared the two optimization objectives systematically and found that predicting the ideal time-frequency mask is more efficient in utilizing clean training speech, while predicting clean speech is less sensitive to SNR variations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {967–977},
numpages = {11},
keywords = {deep neural networks, ensemble learning, mapping-based separation, masking-based separation, monaural speech separation, multicontext networks}
}

@proceedings{10.1145/3001867,
title = {FOSD 2016: Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1145/3204459,
author = {Chen, Tao and Li, Ke and Bahsoon, Rami and Yao, Xin},
title = {FEMOSAA: Feature-Guided and Knee-Driven Multi-Objective Optimization for Self-Adaptive Software},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3204459},
doi = {10.1145/3204459},
abstract = {Self-Adaptive Software (SAS) can reconfigure itself to adapt to the changing environment at runtime, aiming to continually optimize conflicted nonfunctional objectives (e.g., response time, energy consumption, throughput, cost, etc.). In this article, we present Feature-guided and knEe-driven Multi-Objective optimization for Self-Adaptive softwAre (FEMOSAA), a novel framework that automatically synergizes the feature model and Multi-Objective Evolutionary Algorithm (MOEA) to optimize SAS at runtime. FEMOSAA operates in two phases: at design time, FEMOSAA automatically transposes the engineers’ design of SAS, expressed as a feature model, to fit the MOEA, creating new chromosome representation and reproduction operators. At runtime, FEMOSAA utilizes the feature model as domain knowledge to guide the search and further extend the MOEA, providing a larger chance for finding better solutions. In addition, we have designed a new method to search for the knee solutions, which can achieve a balanced tradeoff. We comprehensively evaluated FEMOSAA on two running SAS: One is a highly complex SAS with various adaptable real-world software under the realistic workload trace; another is a service-oriented SAS that can be dynamically composed from services. In particular, we compared the effectiveness and overhead of FEMOSAA against four of its variants and three other search-based frameworks for SAS under various scenarios, including three commonly applied MOEAs, two workload patterns, and diverse conflicting quality objectives. The results reveal the effectiveness of FEMOSAA and its superiority over the others with high statistical significance and nontrivial effect sizes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {5},
numpages = {50},
keywords = {Feature model, multi-objective evolutionary algorithm, multi-objective optimization, performance engineering, search-based software engineering, self-adaptive system}
}

@inproceedings{10.1145/3430984.3430995,
author = {Sajja, Shravan and Aggarwal, Nupur and Mukherjee, Sumanta and Manglik, Kushagra and Dwivedi, Satyam and Raykar, Vikas},
title = {Explainable AI based Interventions for Pre-season Decision Making in Fashion Retail},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3430995},
doi = {10.1145/3430984.3430995},
abstract = {Future of sustainable fashion lies in adoption of AI for a better understanding of consumer shopping behaviour and using this understanding to further optimize product design, development and sourcing to finally reduce the probability of overproducing inventory. Explainability and interpretability are highly effective in increasing the adoption of AI based tools in creative domains like fashion. In a fashion house, stakeholders like buyers, merchandisers and financial planners have a more quantitative approach towards decision making with primary goals of high sales and reduced dead inventory. Whereas, designers have a more intuitive approach based on observing market trends, social media and runways shows. Our goal is to build an explainable new product forecasting tool with capabilities of interventional analysis such that all the stakeholders (with competing goals) can participate in collaborative decision making process of new product design, development and launch.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {281–289},
numpages = {9},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@inproceedings{10.1145/511446.511521,
author = {Boyapati, Vijay and Chevrier, Kristie and Finkel, Avi and Glance, Natalie and Pierce, Tom and Stockton, Robert and Whitmer, Chip},
title = {ChangeDetector: a site-level monitoring tool for the WWW},
year = {2002},
isbn = {1581134495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/511446.511521},
doi = {10.1145/511446.511521},
abstract = {This paper presents a new challenge for Web monitoring tools: to build a system that can monitor entire web sites effectively. Such a system could potentially be used to discover "silent news" hidden within corporate web sites. Examples of silent news include reorganizations in the executive team of a company or in the retirement of a product line. ChangeDetector, an implemented prototype, addresses this challenge by incorporating a number of machine learning techniques. The principal backend components of ChangeDetector all rely on machine learning: intelligent crawling, page classification and entity-based change detection. Intelligent crawling enables ChangeDetector to selectively crawl the most relevant pages of very large sites. Classification allows change detection to be filtered by topic. Entity extraction over changed pages permits change detection to be filtered by semantic concepts, such as person names, dates, addresses, and phone numbers. Finally, the front end presents a flexible way for subscribers to interact with the database of detected changes to pinpoint those changes most likely to be of interest.},
booktitle = {Proceedings of the 11th International Conference on World Wide Web},
pages = {570–579},
numpages = {10},
keywords = {URL monitoring, classification, information extraction, intelligent crawling, machine learning},
location = {Honolulu, Hawaii, USA},
series = {WWW '02}
}

@inproceedings{10.1007/978-3-319-42061-5_1,
author = {Babur, \"{O}nder and Cleophas, Loek and Brand, Mark},
title = {Hierarchical Clustering of Metamodels for Comparative Analysis and Visualization},
year = {2016},
isbn = {9783319420608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-42061-5_1},
doi = {10.1007/978-3-319-42061-5_1},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models or metamodels. A good example is the comparison and merging of metamodel variants into a common metamodel in domain model recovery. Although there are many sophisticated techniques to process the input dataset, little attention has been given to the initial data analysis, visualization and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. In this paper we present a generic approach for metamodel comparison, analysis and visualization as an exploratory first step for domain model recovery. We propose representing metamodels in a vector space model, and applying hierarchical clustering techniques to compare and visualize them as a tree structure. We demonstrate our approach on two Ecore datasets: a collection of 50 state machine metamodels extracted from GitHub as top search results; and $$sim $$~100 metamodels from 16 different domains, obtained from AtlanMod Metamodel Zoo.},
booktitle = {Proceedings of the 12th European Conference on Modelling Foundations and Applications - Volume 9764},
pages = {3–18},
numpages = {16},
keywords = {Hierarchical clustering, Model comparison, Model-Driven Engineering, R, Vector space model}
}

@inproceedings{10.5555/2832581.2832741,
author = {Gu, Bin and Sheng, Victor S. and Li, Shuo},
title = {Bi-parameter space partition for cost-sensitive SVM},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {Model selection is an important problem of costsensitive SVM (CS-SVM). Although using solution path to find global optimal parameters is a powerful method for model selection, it is a challenge to extend the framework to solve two regularization parameters of CS-SVM simultaneously. To overcome this challenge, we make three main steps in this paper. (i) A critical-regions-based biparameter space partition algorithm is proposed to present all piecewise linearities of CS-SVM. (ii) An invariant-regions-based bi-parameter space partition algorithm is further proposed to compute empirical errors for all parameter pairs. (iii) The global optimal solutions for K-fold cross validation are computed by superposing K invariant region based bi-parameter space partitions into one. The three steps constitute the model selection of CS-SVM which can find global optimal parameter pairs in K-fold cross validation. Experimental results on seven normal datsets and four imbalanced datasets, show that our proposed method has better generalization ability and than various kinds of grid search methods, however, with less running time.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {3532–3539},
numpages = {8},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@inproceedings{10.1145/3478384.3478408,
author = {Lostanlen, Vincent and Bernabeu, Antoine and B\'{e}chennec, Jean-Luc and Briday, Mika\"{e}l and Faucou, S\'{e}bastien and Lagrange, Mathieu},
title = {Energy Efficiency is Not Enough:Towards a Batteryless Internet of Sounds},
year = {2021},
isbn = {9781450385695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478384.3478408},
doi = {10.1145/3478384.3478408},
abstract = {This position paper advocates for digital sobriety in the design and usage of wireless acoustic sensors. As of today, these devices all rely on batteries, which are either recharged by a human operator or via solar panels. Yet, batteries contain chemical pollutants and have a shorter lifespan than electronic components: as such, they hinder the autonomy and sustainability of the Internet of Sounds at large. Against this problem, our radical answer is to avoid the use of batteries altogether; and instead, to harvest ambient energy in real time and store it in a supercapacitor allowing a few minutes of operation. We show the inherent limitations of battery-dependent technologies for acoustic sensing. Then, we describe how a low-cost Micro-Controller Unit (MCU) could serve for audio acquisition and feature extraction on the edge. In particular, we stress the advantage of storing intermediate computations in ferroelectric random-access memory (FeRAM), which is nonvolatile, fast, endurant and consumes little. As a proof of concept, we present a simple-minded detector of sine tones in background noise, which relies on a fixed-point implementation of the fast Fourier transform (FFT). We outline future directions towards bioacoustic event detection and urban acoustic monitoring without batteries nor wires.},
booktitle = {Proceedings of the 16th International Audio Mostly Conference},
pages = {147–155},
numpages = {9},
keywords = {acoustic sensor networks, batteryless computing, digital sobriety, intermittent computing, nonvolatile random access memory},
location = {virtual/Trento, Italy},
series = {AM '21}
}

@inproceedings{10.5555/3060832.3060865,
author = {Liang, Junwei and Jiang, Lu and Meng, Deyu and Hauptmann, Alexander},
title = {Learning to detect concepts from webly-labeled video data},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Learning detectors that can recognize concepts, such as people actions, objects, etc., in video content is an interesting but challenging problem. In this paper, we study the problem of automatically learning detectors from the big video data on the web without any additional manual annotations. The contextual information available on the web provides noisy labels to the video content. To leverage the noisy web labels, we propose a novel method called WEbly-Labeled Learning (WELL). It is established on two theories called curriculum learning and self-paced learning and exhibits useful properties that can be theoretically verified. We provide compelling insights on the latent non-convex robust loss that is being minimized on the noisy data. In addition, we propose two novel techniques that not only enable WELL to be applied to big data but also lead to more accurate results. The efficacy and the scalability of WELL have been extensively demonstrated on two public benchmarks, including the largest multimedia dataset and the largest manually-labeled video set. Experimental results show that WELL significantly outperforms the state-of-the-art methods. To the best of our knowledge, WELL achieves by far the best reported performance on these two webly-labeled big video datasets.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1746–1752},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@article{10.1016/j.engappai.2011.02.012,
author = {Wang, Yaonan and Ge, Ji and Zhang, Hui and Zhou, Bowen},
title = {Intelligent injection liquid particle inspection machine based on two-dimensional Tsallis Entropy with modified pulse-coupled neural networks},
year = {2011},
issue_date = {June, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {24},
number = {4},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2011.02.012},
doi = {10.1016/j.engappai.2011.02.012},
abstract = {The Automatic Liquid Particle Inspection Machine (AIM) using 2-D Tsallis Entropy with modified pulse-coupled neural networks (PCNN) is used in order to detect visible foreign particles within injection fluids. According to the motion of the particles in liquid, appropriate mechanisms are utilized which guarantees that the inspection machine will follow detection procedures: ''Rotation, Abruptly Braking, Video Tracking'' to extract tiny objects from complicated sequential images. In order to reduce the influence derived from air bubbles, improved spin/stop techniques are applied. The external capture mode of CCD cameras is used to avoid the possibility of omitting certain particles by trivial displacement. 2-D Tsallis Entropy with modified PCNN is applied in order to segment the difference images, and then to judge the existence of foreign particles according to the continuity and smoothness of their traces. Preliminary experimental results (125ml 0.9% sodium chloride solution and 10% glucose as the samples) indicate that the inspection machine, which is superior to proficient inspectors, can detect the visible foreign particles effectively and that this detection speed and accuracy, as well as the correct detection rate can also facilitate the medicinal construction.},
journal = {Eng. Appl. Artif. Intell.},
month = jun,
pages = {625–637},
numpages = {13},
keywords = {Intelligent inspection machine, Liquid particle inspection, Modified PCNN, Tsallis Entropy, Two-dimensional histogram}
}

@inproceedings{10.1145/1167473.1167475,
author = {Hutchins, DeLesley},
title = {Eliminating distinctions of class: using prototypes to model virtual classes},
year = {2006},
isbn = {1595933484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1167473.1167475},
doi = {10.1145/1167473.1167475},
abstract = {In mainstream OO languages, inheritance can be used to add new methods, or to override existing methods. Virtual classes and feature oriented programming are techniques which extend the mechanism of inheritance so that it is possible to refine nested classes as well. These techniques are attractive for programming in the large, because inheritance becomes a tool for manipulating whole class hierarchies rather than individual classes. Nevertheless, it has proved difficult to design static type systems for virtual classes, because virtual classes introduce dependent types. The compile-time type of an expression may depend on the run-time values of objects in that expression.We present a formal object calculus which implements virtual classes in a type-safe manner. Our type system uses a novel technique based on prototypes, which blur the distinction between compile-time and run-time. At run-time, prototypes act as objects, and they can be used in ordinary computations. At compile-time, they act as types. Prototypes are similar in power to dependent types, and subtyping is shown to be a form of partial evaluation. We prove that prototypes are type-safe but undecidable, and briefly outline a decidable semi-algorithm for dealing with them.},
booktitle = {Proceedings of the 21st Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications},
pages = {1–20},
numpages = {20},
keywords = {abstract interpretation, dependent types, features, mixins, partial evaluation, prototypes, singleton types, virtual classes, virtual types},
location = {Portland, Oregon, USA},
series = {OOPSLA '06}
}

@article{10.1109/TCBB.2019.2961667,
author = {Huang, Hai-Hui and Liang, Yong},
title = {A Novel Cox Proportional Hazards Model for High-Dimensional Genomic Data in Cancer Prognosis},
year = {2019},
issue_date = {Sept.-Oct. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2961667},
doi = {10.1109/TCBB.2019.2961667},
abstract = {The Cox proportional hazards model is a popular method to study the connection between feature and survival time. Because of the high-dimensionality of genomic data, existing Cox models trained on any specific dataset often generalize poorly to other independent datasets. In this paper, we suggest a novel strategy for the Cox model. This strategy is included a new learning technique, self-paced learning (SPL), and a new gene selection method, SCAD-Net penalty. The SPL method is adopted to aid to build a more accurate prediction with its built-in mechanism of learning from easy samples first and adaptively learning from hard samples. The SCAD-Net penalty has fixed the problem of the SCAD method without an inherent mechanism to fuse the prior graphical information. We combined the SPL with the SCAD-Net penalty to the Cox model (SSNC). The simulation shows that the SSNC outperforms the benchmark in terms of prediction and gene selection. The analysis of a large-scale experiment across several cancer datasets shows that the SSNC method not only results in higher prediction accuracies but also identifies markers that satisfactory stability across another validation dataset. The demo code for the proposed method is provided in supplemental file.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {1821–1830},
numpages = {10}
}

@article{10.1016/j.engappai.2009.08.004,
author = {Fuentes-Fern\'{a}ndez, Rub\'{e}n and Garc\'{\i}a-Magari\~{n}o, Iv\'{a}n and G\'{o}mez-Rodr\'{\i}guez, Alma Mar\'{\i}a and Gonz\'{a}lez-Moreno, Juan Carlos},
title = {A technique for defining agent-oriented engineering processes with tool support},
year = {2010},
issue_date = {April, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {23},
number = {3},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.08.004},
doi = {10.1016/j.engappai.2009.08.004},
abstract = {The formalization of engineering processes is necessary for guaranteeing the quality and cost of the products involved. Agent-oriented engineering has already proposed development processes that still need to be further formalized to be applicable by non-researchers. This paper proposes a technique to instantiate processes for specific agent-oriented methodologies. This technique is based on three orthogonal views that are respectively related with lifecycles, disciplines and guidances. In addition, processes are modeled with a tool, which is automatically generated from a process metamodel inspired by the software &amp; systems process engineering metamodel. Accordingly, engineers can choose the methodology-process pair best-suited for the characteristics of their project. The paper illustrates the approach based on the unified development process and the scrum process for the INGENIAS methodology and compares the results with other existing alternatives.},
journal = {Eng. Appl. Artif. Intell.},
month = apr,
pages = {432–444},
numpages = {13},
keywords = {Agent-oriented engineering, Model-driven development, Multi-agent system, Process engineering, Software &amp; systems process engineering metamodel}
}

@inproceedings{10.5555/3005729.3005739,
author = {Lier, Alexander and Franke, Linus and Marc Stamminger, Marc and Selgrad, Kai},
title = {A Case Study in Implementation-Space Exploration},
year = {2016},
isbn = {9782955747407},
publisher = {European Lisp Scientific Activities Association},
abstract = {In this paper we show how a feature-oriented development methodology can be exploited to investigate a large set of possible implementations for a real-time rendering algorithm. We rely on previously published work to explore potential dimensions of the implementation space of an algorithm to be run on a graphics processing unit (GPU) using CUDA. The main contribution of our paper is to provide a clear example of the benefit to be gained from existing methods in a domain that only slowly moves toward higher level abstractions. Our method employs a generative approach and makes heavy use of Common Lisp-macros before the code is ultimately transformed to CUDA.},
booktitle = {Proceedings of the 9th European Lisp Symposium on European Lisp Symposium},
articleno = {10},
numpages = {8},
keywords = {Meta-Programming, Real-time, Rendering algorithms},
location = {Krak\'{o}w, Poland},
series = {ELS2016}
}

@inproceedings{10.1111/cgf.14116,
author = {Ghorbani, S. and Wloka, C. and Etemad, A. and Brubaker, M. A. and Troje, N. F.},
title = {Probabilistic character motion synthesis using a hierarchical deep latent variable model},
year = {2020},
publisher = {Eurographics Association},
address = {Goslar, DEU},
url = {https://doi.org/10.1111/cgf.14116},
doi = {10.1111/cgf.14116},
abstract = {We present a probabilistic framework to generate character animations based on weak control signals, such that the synthesized motions are realistic while retaining the stochastic nature of human movement. The proposed architecture, which is designed as a hierarchical recurrent model, maps each sub-sequence of motions into a stochastic latent code using a variational autoencoder extended over the temporal domain. We also propose an objective function which respects the impact of each joint on the pose and compares the joint angles based on angular distance. We use two novel quantitative protocols and human qualitative assessment to demonstrate the ability of our model to generate convincing and diverse periodic and non-periodic motion sequences without the need for strong control signals.},
booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
articleno = {21},
numpages = {15},
location = {Virtual Event, Canada},
series = {SCA '20}
}

@article{10.1016/j.cad.2013.05.002,
author = {Cai, Kun and Qin, Qing H. and Luo, Zhen and Zhang, Ai J.},
title = {Robust topology optimisation of bi-modulus structures},
year = {2013},
issue_date = {October, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {45},
number = {10},
issn = {0010-4485},
url = {https://doi.org/10.1016/j.cad.2013.05.002},
doi = {10.1016/j.cad.2013.05.002},
abstract = {This study proposes a robust topology optimisation method for the design of bi-modulus structures under uncertain multiple loading conditions (MLC). The objective of the design optimisation is to minimise the standard deviation of the weighted structural compliance. The gradient-based method is applied to perform a sensitivity analysis for the identification of optimal design variables. A material replacement method is used to overcome difficulty in the sensitivity analysis due to the stress-dependent behaviour of the original bi-modulus material. In the material replacement operation, two new isotropic materials are identified to replace the original bi-modulus material according to its two moduli. To reduce the side effects of the material replacement operation on the final design, the local stiffness is modified in terms of the stress state. Typical numerical examples are used to demonstrate the effectiveness of the proposed method to the final design, including the load uncertainty on the optimal bi-modulus layout, as well as other factors, such as loading direction and the ratio between the two moduli of the bi-modulus material. The comparison between layouts of isotropic and bi-modulus materials also shows that the final bi-modulus material distribution is sensitive to loading directions in practical designs.},
journal = {Comput. Aided Des.},
month = oct,
pages = {1159–1169},
numpages = {11},
keywords = {Bi-modulus, Multi-stiffness structures, Robust design, Topology optimisation}
}

@article{10.1287/msom.2015.0563,
author = {Xue, Zhengliang and Wang, Zizhuo and Ettl, Markus},
title = {Pricing Personalized Bundles: A New Approach and An Empirical Study},
year = {2016},
issue_date = {February 2016},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {18},
number = {1},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2015.0563},
doi = {10.1287/msom.2015.0563},
abstract = {This paper studies the pricing strategies for personalized product bundles. In such problems, a seller provides a variety of products for which customers can construct a personalized bundle and send a request for quote RFQ to the seller. The seller, after reviewing the RFQ, has to determine a price based on which the customer either purchases the whole bundle or nothing. Such problems are faced by many companies in practice, and they are very difficult because of the potential unlimited possible configurations of the bundle and the correlations among the individual products. In this paper, we propose a novel top-down and bottom-up approach to solve this problem. In the top-down step, we decompose the bundle into each component and calibrate a value score for each component. In the bottom-up step, we aggregate the components back to the bundle, define important features of the bundle, and segment different RFQs by those bundle features as well as customer attributes. Then we estimate a utility function for each segment based on historical sales data and derive an optimal price for each incoming RFQ. We show that such a model overcomes the aforementioned difficulties and can be implemented efficiently. We test our approach using empirical data from a major information technology service provider and the test result shows that the proposed approach can improve the effectiveness of pricing significantly.},
journal = {Manufacturing &amp; Service Operations Management},
month = feb,
pages = {51–68},
numpages = {18},
keywords = {data analysis, personalized bundle, pricing, utility model}
}

@inproceedings{10.5555/1732323.1732529,
author = {Chiu, Chui-Yu and Chen, K. Y. and Ke, Cheng-Hsin},
title = {Optimal RFID networks scheduling using genetic algorithm and swarm intelligence},
year = {2009},
isbn = {9781424427932},
publisher = {IEEE Press},
abstract = {RFID is an emerging technique for identifying items and all kinds of real world applications. Multi RFID readers are implemented to the product line in many industries and they consist of varied reader resources. But there are some defects with the disposition of the RFID-based application. The phenomenon of incorrect negative reads occurs in a multi-tag and multi-reader environment where a tag that is present is not detected. Collisions occurring between readers cause the faulty or missing reads. The stopgap is to solve the frequency allocation problem for networks of RFID readers. Furthermore, finding the optimal structure of readers and scheduling the readers to reduce the total system transaction time or response time are both challenging problems. In the presence of interdependencies, the optimal scheduling problem to minimize the overall transaction or response time is modeled as a graph partitioning problem (GPP). GPP is a well known NP-complete problem. The more readers exist in the product line, the higher complexity of the problem. Designing a schedule having the maximum parallelism reduces the total transaction time but may not minimize it.In this research, we integrate genetic algorithms with binary particle swarm optimization (GA-BPSO) to solve the Multi RFID networks scheduling problem. Simulation results on a real-world problem show that the GA-BPSO algorithm provides robust solution quality and is suitable for scheduling large scale RFID reader networks.},
booktitle = {Proceedings of the 2009 IEEE International Conference on Systems, Man and Cybernetics},
pages = {1201–1208},
numpages = {8},
keywords = {GA-BPSO, networks of RFID readers, scheduling},
location = {San Antonio, TX, USA},
series = {SMC'09}
}

@article{10.1007/s11554-014-0483-1,
author = {Concha, David and Cabido, Ra\'{u}l and Pantrigo, Juan Jos\'{e} and Montemayor, Antonio S.},
title = {Performance evaluation of a 3D multi-view-based particle filter for visual object tracking using GPUs and multicore CPUs},
year = {2018},
issue_date = {August    2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {2},
issn = {1861-8200},
url = {https://doi.org/10.1007/s11554-014-0483-1},
doi = {10.1007/s11554-014-0483-1},
abstract = {This paper presents a deep and extensive performance analysis of the particle filter (PF) algorithm for a very compute intensive 3D multi-view visual tracking problem. We compare different implementations and parameter settings of the PF algorithm in a CPU platform taking advantage of the multithreading capabilities of the modern processors and a graphics processing unit (GPU) platform using NVIDIA CUDA computing environment as developing framework. We extend our experimental study to each individual stage of the PF algorithm, and evaluate the quality versus performance trade-off among different ways to design these stages. We have observed that the GPU platform performs better than the multithreaded CPU platform when handling a large number of particles, but we also demonstrate that hybrid CPU/GPU implementations can run almost as fast as only GPU solutions.},
journal = {J. Real-Time Image Process.},
month = aug,
pages = {309–327},
numpages = {19},
keywords = {3D visual tracking, GPU computing, Multi-view, Particle filtering, Performance evaluation}
}

@article{10.1016/j.specom.2019.09.003,
author = {Shirzhiyan, Zahra and Shamsi, Elham and Jafarpisheh, Amir Salar and Jafari, Amir Homayoun},
title = {Objective classification of auditory brainstem responses to consonant-vowel syllables using local discriminant bases},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {114},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.09.003},
doi = {10.1016/j.specom.2019.09.003},
journal = {Speech Commun.},
month = nov,
pages = {36–48},
numpages = {13},
keywords = {Speech ABR, Speech encoding, Local discriminant bases}
}

@article{10.1016/j.neucom.2017.07.037,
author = {Li, Honggui and Trocan, Maria},
title = {Deep neural network based single pixel prediction for unified video coding},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {272},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.07.037},
doi = {10.1016/j.neucom.2017.07.037},
abstract = {Classical video prediction methods exploit directly and shallowly the intra-frame, inter-frame and multi-view similarities within the video sequences; the proposed video prediction methods indirectly and intensively transform the frame correlations into nonlinear mappings by using a general deep neural network (DNN) with single output node. Traditional DNN based video prediction algorithms wholly and coarsely forecast the next frame, but the proposed video prediction algorithms severally and precisely anticipate single pixel of future frame in order to achieve high prediction accuracy and low computation cost. First of all, general DNN based prediction algorithms for intra-frame coding, inter-frame coding and multi-view coding are presented respectively. Then, general DNN based prediction algorithm for unified video coding is raised, which relies on the preceding three prediction algorithms. It is evaluated by simulation experiments that the proposed methods hold better performance than state of the art High Efficiency Video Coding (HEVC) in peak signal to noise ratio (PSNR) and bit per pixel (BPP) in the situation of low bitrate transmission. It is also verified by experimental results that the proposed general DNN architecture possesses higher prediction accuracy and lower computation load than those of conventional DNN architectures. It is further testified by experimental results that the proposed methods are very suitable for multi-view videos with small correlations and big disparities.},
journal = {Neurocomput.},
month = jan,
pages = {558–570},
numpages = {13},
keywords = {Deep neural network, Inter-frame coding, Intra-frame coding, Multi-view coding, Unified video coding, Video prediction}
}

@article{10.1016/j.compbiomed.2019.103380,
author = {Kakati, Tulika and Bhattacharyya, Dhruba K. and Barah, Pankaj and Kalita, Jugal K.},
title = {Comparison of Methods for Differential Co-expression Analysis for Disease Biomarker Prediction},
year = {2019},
issue_date = {Oct 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {113},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2019.103380},
doi = {10.1016/j.compbiomed.2019.103380},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {12},
keywords = {Differential co-expression analysis, Empirical study, Gene expression, miRNA expression, Alzheimer's disease, Parkinson's disease, Disease biomarkers}
}

@article{10.1007/s11219-009-9077-8,
author = {Koschke, Rainer and Frenzel, Pierre and Breu, Andreas P. and Angstmann, Karsten},
title = {Extending the reflexion method for consolidating software variants into product lines},
year = {2009},
issue_date = {December  2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-009-9077-8},
doi = {10.1007/s11219-009-9077-8},
abstract = {Software variants emerge from ad-hoc copying in-the-large with adaptations to a specific context. As the number of variants increases, maintaining such software variants becomes more and more difficult and expensive. In contrast to such ad-hoc reuse, software product lines offer organized ways of reuse, taking advantage of similarities of different products. To re-gain control, software variants may be consolidated as organized software product lines. In this paper, we describe a method and supporting tools to compare software variants at the architectural level extending the reflexion method to software variants. Murphy's reflexion method allows one to reconstruct the module view, a static architectural view describing the static components, their interfaces and dependencies and their grouping as layers and subsystems. The method consists of the specification of the module view and the mapping of implementation components onto the module view. An automatic analysis determines differences between the module view and its implementation. We extend the reflexion method from single systems to software variants. Because software variants share a very large amount of code, we can expect components of one variant to re-occur in the other variant either identically or at least similarly. We use similarity metrics to determine this type of correspondence among implementation components between two variants. Because the similarity metrics are expensive to compute, we use clone detection techniques to reduce the number of candidate pairs of implementation components for which the similarity is computed. The correspondence is then used to transfer as much as possible of the mapping for the analyzed variants to the next variant to be analyzed. Furthermore, we describe how to unify the individual product architectures into a software product line architecture.},
journal = {Software Quality Journal},
month = dec,
pages = {331–366},
numpages = {36},
keywords = {Architecture reconstruction, Reverse engineering, Software product lines, Software variants}
}

@article{10.1207/s15327051hci1204_4,
author = {Kieras, David E. and Meyer, David E.},
title = {An overview of the EPIC architecture for cognition and performance with application to human-computer interaction},
year = {1997},
issue_date = {December 1997},
publisher = {L. Erlbaum Associates Inc.},
address = {USA},
volume = {12},
number = {4},
issn = {0737-0024},
url = {https://doi.org/10.1207/s15327051hci1204_4},
doi = {10.1207/s15327051hci1204_4},
abstract = {EPIC (Executive Process-Interactive Control) is a cognitive architecture especially suited for modeling human multimodal and multiple-task performance. The EPIC architecture includes peripheral sensory-motor processors surrounding a production-rule cognitive processor and is being used to construct precise computational models for a variety of human-computer interaction situations. We briefly describe some of these models to demonstrate how EPIC clarifies basic properties of human performance and provides usefully precise accounts of performance speed.},
journal = {Hum.-Comput. Interact.},
month = dec,
pages = {391–438},
numpages = {48}
}

@article{10.1023/A:1009976227802,
author = {Turney, Peter D.},
title = {Learning Algorithms for Keyphrase Extraction},
year = {2000},
issue_date = {May 2000},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {2},
number = {4},
issn = {1386-4564},
url = {https://doi.org/10.1023/A:1009976227802},
doi = {10.1023/A:1009976227802},
abstract = {Many academic journals ask their authors to provide a list of about five to fifteen keywords, to appear on the first page of each article. Since these key words are often phrases of two or more words, we prefer to call them keyphrases. There is a wide variety of tasks for which keyphrases are useful, as we discuss in this paper. We approach the problem of automatically extracting keyphrases from text as a supervised learning task. We treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Our first set of experiments applies the C4.5 decision tree induction algorithm to this learning task. We evaluate the performance of nine different configurations of C4.5. The second set of experiments applies the GenEx algorithm to the task. We developed the GenEx algorithm specifically for automatically extracting keyphrases from text. The experimental results support the claim that a custom-designed algorithm (GenEx), incorporating specialized procedural domain knowledge, can generate better keyphrases than a general-purpose algorithm (C4.5). Subjective human evaluation of the keyphrases generated by GenEx suggests that about 80% of the keyphrases are acceptable to human readers. This level of performance should be satisfactory for a wide variety of applications.},
journal = {Inf. Retr.},
month = may,
pages = {303–336},
numpages = {34},
keywords = {machine learning, summarization, indexing, keywords, keyphrase extraction}
}

@article{10.1007/s11042-012-1248-0,
author = {Hu, Huijun and Li, Yuanxiang and Liu, Maofu and Liang, Wenhao},
title = {Classification of defects in steel strip surface based on multiclass support vector machine},
year = {2014},
issue_date = {March     2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {69},
number = {1},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-012-1248-0},
doi = {10.1007/s11042-012-1248-0},
abstract = {In this paper, we use support vector machine to classify the defects in steel strip surface images. After image binarization, three types of image features, including geometric feature, grayscale feature and shape feature, are extracted by combining the defect target image and its corresponding binary image. For the classification model based on support vector machine, we utilize Gauss radial basis as the kernel function, determine model parameters by cross-validation and employ one-versus-one method for multiclass classifier. Experiment results show that support vector machine model outperforms the traditional classification model based on back-propagation neural network in average classification accuracy.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {199–216},
numpages = {18},
keywords = {Back-propagation neural network, LIBSVM, OpenCV, Steel strip, Support vector machine, Surface defect}
}

@article{10.1155/2021/8822786,
author = {Zhao, Yiqing and Prabhu, M. and Ahmed, Ramyar Rzgar and Sahu, Anoop Kumar and Bruneo, Dario},
title = {Research Trends and Performance of IIoT Communication Network-Architectural Layers of Petrochemical Industry 4.0 for Coping with Circular Economy},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/8822786},
doi = {10.1155/2021/8822786},
abstract = {In the present era, many Petrochemical Industries (PIs) are driven energetically due to IIoT (Industrial Internet of Things) Communication Networks/Architectural Layers (CNs/ALs), abbreviated as PI4.0-CNs/ALs. PI4.0 fruitfully participated to achieve the Circular Economy (CE) by speeding the reutilization, recovery, and recycling of scrap materials by minimizing cost, unproductive operations, energy consumption, emission of flue gases, etc. Recently, it has been ascertained that the identification and measurement of Research Trends (RTs) of CNs-ALs help the PI4.0 to build the future CE. In addressing the said research challenge, the objective of this research dossier is turned towards inculcating into future PI4.0 researchers the RTs of CNs/ALs of PI4.0, so that the researches can be organized over the very weak and moderately performing CNs-ALs to hike the future CE. To materialize the RTs of PI4-CNs/ALs, the authors conducted the Systematic Literature Survey (SLS) focusing on PI4.0-CNs/ALs, i.e., Internet of Things (IoTs), Cyber Physical System (CPS), Virtual Reality (VR), Integration (I), Data Optimization (DO), Enterprise Resource Planning (ERP), Plant Control (PC), Data and Analytics (DA), Network (N), and Information and Data Management (IDM). The authors searched three hundred two research documents, wherein two hundred seventy-five research manuscripts qualified as RQ2. Next, the authors collected the DOIs/URLs corresponding to each CN-AL and explored the Sum of Digit Scoring System (SDSS) to summarize the DOIs/URLs of PI4.0-CNs/ALs. The RTs of DO have been determined as excellent and stronger over 2007-2017 than residue CNs/ALs. Eventually, the authors advised scholars to focus on the research areas of very weak and moderately weak performing CNs/ALs in order to attain future CE.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {32}
}

@article{10.1016/j.neucom.2015.05.134,
author = {Hu, Huijun and Liu, Ya and Liu, Maofu and Nie, Liqiang},
title = {Surface defect classification in large-scale strip steel image collection via hybrid chromosome genetic algorithm},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {181},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.05.134},
doi = {10.1016/j.neucom.2015.05.134},
abstract = {In this paper, hybrid chromosome genetic algorithm is applied to establishing the real-time classification model for surface defects in a large-scale strip steel image collection. After image preprocessing, four types of visual features, comprising geometric feature, shape feature, texture feature and grayscale feature, are extracted from the defect target image and its corresponding preprocessed image. In order to use genetic algorithm to optimize classification model based on hybrid chromosome, the structure of hybrid chromosome is designed to seamlessly integrate the kernel function, visual features and model parameters. And then the chromosome and the SVM classification model will be evolved and optimized according to the genetic operations and the fitness evaluation. In the end, the final SVM classifier is established using the decoding result of the optimal chromosome. The experimental results show that our method is effective and efficient in classifying the surface defects in a large-scale strip steel image collection.},
journal = {Neurocomput.},
month = mar,
pages = {86–95},
numpages = {10},
keywords = {Hybrid chromosome genetic algorithm, Kernel function, SVM model, Strip steel surface defect, Visual feature selection}
}

@article{10.1145/3306346.3322957,
author = {Glauser, Oliver and Wu, Shihao and Panozzo, Daniele and Hilliges, Otmar and Sorkine-Hornung, Olga},
title = {Interactive hand pose estimation using a stretch-sensing soft glove},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3322957},
doi = {10.1145/3306346.3322957},
abstract = {We propose a stretch-sensing soft glove to interactively capture hand poses with high accuracy and without requiring an external optical setup. We demonstrate how our device can be fabricated and calibrated at low cost, using simple tools available in most fabrication labs. To reconstruct the pose from the capacitive sensors embedded in the glove, we propose a deep network architecture that exploits the spatial layout of the sensor itself. The network is trained only once, using an inexpensive off-the-shelf hand pose reconstruction system to gather the training data. The per-user calibration is then performed on-the-fly using only the glove. The glove's capabilities are demonstrated in a series of ablative experiments, exploring different models and calibration methods. Comparing against commercial data gloves, we achieve a 35% improvement in reconstruction accuracy.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {41},
numpages = {15},
keywords = {data glove, hand tracking, sensor array, stretch-sensing}
}

@article{10.1016/j.patcog.2017.04.024,
author = {Ren, Jianfeng and Jiang, Xudong},
title = {Regularized 2-D complex-log spectral analysis and subspace reliability analysis of micro-Doppler signature for UAV detection},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {69},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.04.024},
doi = {10.1016/j.patcog.2017.04.024},
abstract = {The proposed 2-D regularized complex-log-Fourier transform better represents mDS.The proposed subspace reliability analysis better removes unreliable dimensions.The proposed approach demonstrates superior performance for UAV detection. Unmanned aerial vehicle (UAV) has become an important radar target recently because of its wide applications and potential security threats. Traditionally, visual features such as spectrogram were often extracted for human operators to identify the micro-Doppler signature (mDS) of UAVs, i.e. sinusoidal modulation. In this paper, the authors aim to design a system for machine automatic classification of UAVs from other targets, particularly from birds as both UAVs and birds are small and slow-moving radar targets. Most existing mDS representations such as spectrogram, cepstrogram and cadence velocity diagram discard the phase spectrum, and only make use of the magnitude spectrum. Whats more, people often take the logarithm of the spectrum to enlarge the weak mDS but without sufficient care, as noise may be enlarged at the same time. The authors thus propose a regularized 2-D complex-log-Fourier transform to address these problems. Furthermore, the authors propose an object-oriented dimension-reduction technique: subspace reliability analysis, which directly removes the unreliable feature dimensions of two class-conditional covariance matrices in two separate subspaces. On the benchmark dataset, the proposed approach demonstrates better performance than the state-of-the-art approaches. More specifically, the proposed approach significantly reduces the equal error rate of the second best approach, cadence velocity diagram, from 6.68% to 3.27%.},
journal = {Pattern Recogn.},
month = sep,
pages = {225–237},
numpages = {13},
keywords = {2-D regularized complex-log-Fourier transform, Micro-Doppler signature, Radar, Subspace reliability analysis, UAV detection}
}

@article{10.1007/s10009-012-0234-1,
author = {Classen, Andreas and Cordy, Maxime and Heymans, Patrick and Legay, Axel and Schobbens, Pierre-Yves},
title = {Model checking software product lines with SNIP},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-012-0234-1},
doi = {10.1007/s10009-012-0234-1},
abstract = {We present SNIP, an efficient model checker for software product lines (SPLs). Variability in software product lines is generally expressed in terms of features, and the number of potential products is exponential in the number of features. Whereas classical model checkers are only capable of checking properties against each individual product in the product line, SNIP exploits specifically designed algorithms to check all products in a single step. This is done by using a concise mathematical structure for product line behaviour, that exploits similarities and represents the behaviour of all products in a compact manner. Specification of an SPL in SNIP relies on the combination of two specification languages: TVL to describe the variability in the product line, and fPromela to describe the behaviour of the individual products. SNIP is thus one of the first tools equipped with specification languages to formally express both the variability and the behaviours of the products of the product line. The paper assesses SNIP and suggests that this is the first model checker for SPLs that can be used outside the academic arena.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {589–612},
numpages = {24},
keywords = {Feature, Language, Model checking, Product lines, Tool}
}

@article{10.1016/j.infsof.2016.08.005,
author = {Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Requirements monitoring frameworks},
year = {2016},
issue_date = {December 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {80},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.08.005},
doi = {10.1016/j.infsof.2016.08.005},
abstract = {Analyze the characteristics and application areas of monitoring approaches proposed in different domains.Systematically identify frameworks supporting requirements monitoring.Analyze to what extent the monitoring frameworks support requirements monitoring in SoS. ContextSoftware systems today often interoperate with each other, thus forming a system of systems (SoS). Due to the scale, complexity, and heterogeneity of SoS, determining compliance with their requirements is challenging, despite the range of existing monitoring approaches. The fragmented research landscape and the diversity of existing approaches, however, make it hard to understand and analyze existing research regarding its suitability for SoS. ObjectiveThe aims of this paper are thus to systematically identify, describe, and classify existing approaches for requirements-based monitoring of software systems at runtime. Specifically, we (i) analyze the characteristics and application areas of monitoring approaches proposed in different domains, we (ii) systematically identify frameworks supporting requirements monitoring, and finally (iii) analyze their support for requirements monitoring in SoS. MethodWe performed a systematic literature review (SLR) to identify existing monitoring approaches and to classify their key characteristics and application areas. Based on this analysis we selected requirements monitoring frameworks, following a definition by Robinson, and analyzed them regarding their support for requirements monitoring in SoS. ResultsWe identified 330 publications, which we used to produce a comprehensive overview of the landscape of requirements monitoring approaches. We analyzed these publications regarding their support for Robinson's requirements monitoring layers, resulting in 37 identified frameworks. We investigated how well these frameworks support requirements monitoring in SoS. ConclusionsWe conclude that most existing approaches are restricted to certain kinds of checks, particular types of events and data, and mostly also limited to one particular architectural style and technology. This lack of flexibility makes their application in an SoS context difficult. Also, systematic and automated variability management is still missing. Regarding their evaluation, many existing frameworks focus on measuring the performance overhead, while only few frameworks have been assessed in cases studies with real-world systems.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {89–109},
numpages = {21},
keywords = {Requirements monitoring, Systematic literature review, Systems of systems}
}

@inproceedings{10.5555/2898607.2898861,
author = {Meri\c{c}li, \c{C}etin and Veloso, Manuela},
title = {Biped walk learning through playback and corrective demonstration},
year = {2010},
publisher = {AAAI Press},
abstract = {Developing a robust, flexible, closed-loop walking algorithm for a humanoid robot is a challenging task due to the complex dynamics of the general biped walk. Common analytical approaches to biped walk use simplified models of the physical reality. Such approaches are partially successful as they lead to failures of the robot walk in terms of unavoidable falls. Instead of further refining the analytical models, in this work we investigate the use of human corrective demonstrations, as we realize that a human can visually detect when the robot may be falling. We contribute a two-phase biped walk learning approach, which we experiment on the Aldebaran NAO humanoid robot. In the first phase, the robot walks following an analytical simplified walk algorithm, which is used as a black box, and we identify and save a walk cycle as joint motion commands. We then show how the robot can repeatedly and successfully play back the recorded motion cycle, even if in open-loop. In the second phase, we create a closed-loop walk by modifying the recorded walk cycle to respond to sensory data. The algorithm learns joint movement corrections to the open-loop walk based on the corrective feedback provided by a human, and on the sensory data, while walking autonomously. In our experimental results, we show that the learned closed-loop walking policy outperforms a hand-tuned closed-loop policy and the open-loop playback walk, in terms of the distance traveled by the robot without falling.},
booktitle = {Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence},
pages = {1594–1599},
numpages = {6},
location = {Atlanta, Georgia},
series = {AAAI'10}
}

@article{10.1016/j.ins.2012.07.017,
author = {S\'{a}Nchez-Anguix, V\'{\i}Ctor and Juli\'{a}N, Vicente and Botti, Vicente and Garc\'{\i}A-Fornes, Ana},
title = {Studying the impact of negotiation environments on negotiation teams' performance},
year = {2013},
issue_date = {January, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {219},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2012.07.017},
doi = {10.1016/j.ins.2012.07.017},
abstract = {In this article we study the impact of the negotiation environment on the performance of several intra-team strategies (team dynamics) for agent-based negotiation teams that negotiate with an opponent. An agent-based negotiation team is a group of agents that joins together as a party because they share common interests in the negotiation at hand. It is experimentally shown how negotiation environment conditions like the deadline of both parties, the concession speed of the opponent, similarity among team members, and team size affect performance metrics like the minimum utility of team members, the average utility of team members, and the number of negotiation rounds. Our goal is identifying which intra-team strategies work better in different environmental conditions in order to provide useful knowledge for team members to select appropriate intra-team strategies according to environmental conditions.},
journal = {Inf. Sci.},
month = jan,
pages = {17–40},
numpages = {24},
keywords = {Agreement technology, Automated negotiation, Collective decision making, Multi-agent system, Negotiation team}
}

@article{10.1016/j.procs.2019.02.078,
author = {Zatsarinny, A.A. and Gorshenin, A.K. and Kondrashev, V.A. and Volovich, K.I. and Denisov, S.A.},
title = {Toward High Performance Solutions as Services of Research Digital Platform},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {150},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.02.078},
doi = {10.1016/j.procs.2019.02.078},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {622–627},
numpages = {6},
keywords = {high performance computing, cluster, digital platform, virtualization, computational process control, distributed computing, parallel computing, software-defined network, graphics accelerator}
}

@article{10.1016/j.fss.2015.05.012,
author = {Grobelny, Jerzy},
title = {Fuzzy-based linguistic patterns as a tool for the flexible assessment of a priority vector obtained by pairwise comparisons},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {296},
number = {C},
issn = {0165-0114},
url = {https://doi.org/10.1016/j.fss.2015.05.012},
doi = {10.1016/j.fss.2015.05.012},
abstract = {A novel approach to the assessment of a hierarchy vector generated by an expert pairwise comparison matrix is proposed. The quality of a given hierarchy is measured using so-called linguistic patterns, i.e., specially constructed logical expressions, and an expert assessment matrix. The idea of this approach is based on a concept proposed previously as a flexible criterion for the domain of facility layout problems. The adaptation of the linguistic pattern approach to the assessment of a hierarchy vector also (by analogy to the analytic hierarchy process or AHP) facilitates evaluations of expert consistency. The proposed idea is shown to be less sensitive to potential expert errors than both the classical and fuzzy AHP techniques. The proposed approach is described in detail and illustrated with simple examples. Some features of the proposed approach were studied using computer simulation experiments and the results are reported together with the conclusions obtained.},
journal = {Fuzzy Sets Syst.},
month = aug,
pages = {1–20},
numpages = {20},
keywords = {Analytic hierarchy process, Facility layout problem, Fuzzy set theory, Linear ordering, Linguistic variable, Multivalued logic, Possibility theory}
}

@article{10.1109/TFUZZ.2006.876743,
author = {Chang, Ping-Teng and Hung, Kuo-Chen},
title = {/spl alpha/-cut fuzzy arithmetic: simplifying rules and a fuzzy function optimization with a decision variable},
year = {2006},
issue_date = {August 2006},
publisher = {IEEE Press},
volume = {14},
number = {4},
issn = {1063-6706},
url = {https://doi.org/10.1109/TFUZZ.2006.876743},
doi = {10.1109/TFUZZ.2006.876743},
abstract = {The problems of alpha-cut fuzzy arithmetic have been shown, like in interval arithmetic, that distinct states of fuzzy parameters (or fuzzy variable values) may be chosen and produce an overestimated fuzziness. Meanwhile, local extrema of a function may exist inside the support of fuzzy parameters and cause an underestimation of fuzziness and an illegal fuzzy number's result. Previous approaches to overcoming these problems have appeared in literature. Yet, the computational burden of these approaches became even heavier. Thus, this paper is based on the vertex method in literature and extensively proposes newly devised rules observed greatly useful for simplifying the vertex method. These rules are devised through a function partitioned into subfunctions, distinguishing the types of fuzzy parameter/variable occurrences, and types of subfunctions or functions with the various observations. The improved efficiency has been found able to significantly reduce the combination (vertex) test of the vertex method for the fuzzy parameters' alpha-cut endpoints possibly to only a few fuzzy parameters' endpoint combinations. Also as related, a procedure for the fuzzy optimization of fuzzy functions with a fuzzy blurred argument (a single variable) is examined with the vertex method as well. A proper and useful preliminary algorithm is proposed. Numerical examples with results are provided},
journal = {Trans. Fuz Sys.},
month = aug,
pages = {496–510},
numpages = {15},
keywords = {fuzzy arithmetic, fuzzy function optimization, fuzzy number, vertex method}
}

@inproceedings{10.5555/3298483.3298553,
author = {Li, Changsheng and Yan, Junchi and Wei, Fan and Dong, Weishan and Liu, Qingshan and Zha, Hongyuan},
title = {Self-paced multi-task learning},
year = {2017},
publisher = {AAAI Press},
abstract = {Multi-task learning is a paradigm, where multiple tasks are jointly learnt. Previous multi-task learning models usually treat all tasks and instances per task equally during learning. Inspired by the fact that humans often learn from easy concepts to hard ones in the cognitive process, in this paper, we propose a novel multi-task learning framework that attempts to learn the tasks by simultaneously taking into consideration the complexities of both tasks and instances per task. We propose a novel formulation by presenting a new task-oriented regularizer that can jointly prioritize tasks and instances. Thus it can be interpreted as a self-paced learner for multi-task learning. An efficient block coordinate descent algorithm is developed to solve the proposed objective function, and the convergence of the algorithm can be guaranteed. Experimental results on the toy and real-world datasets demonstrate the effectiveness of the proposed approach, compared to the state-of-the-arts.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {2175–2181},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.1016/j.neucom.2019.11.104,
author = {Ren, Yazhou and Huang, Shudong and Zhao, Peng and Han, Minghao and Xu, Zenglin},
title = {Self-paced and auto-weighted multi-view clustering},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {383},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.104},
doi = {10.1016/j.neucom.2019.11.104},
journal = {Neurocomput.},
month = mar,
pages = {248–256},
numpages = {9},
keywords = {Self-paced learning, Multi-view clustering, Soft weighting}
}

@article{10.1016/j.ins.2019.12.046,
author = {Chen, Dongzi and Yang, Qinli and Liu, Jiaming and Zeng, Zhu},
title = {Selective prototype-based learning on concept-drifting data streams},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {516},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.12.046},
doi = {10.1016/j.ins.2019.12.046},
journal = {Inf. Sci.},
month = apr,
pages = {20–32},
numpages = {13},
keywords = {Data stream, Concept drift, Classification, Prototype, 00-01, 99-00}
}

@inproceedings{10.5555/2887007.2887142,
author = {Lev, Omer and Oren, Joel and Boutilier, Craig and Rosenschein, Jeffrey S.},
title = {The pricing war continues: on competitive multi-item pricing},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {We study a game with strategic vendors (the agents) who own multiple items and a single buyer with a submodular valuation function. The goal of the vendors is to maximize their revenue via pricing of the items, given that the buyer will buy the set of items that maximizes his net payoff.We show this game may not always have a pure Nash equilibrium, in contrast to previous results for the special case where each vendor owns a single item. We do so by relating our game to an intermediate, discrete game in which the vendors only choose the available items, and their prices are set exogenously afterwards.We further make use of the intermediate game to provide tight bounds on the price of anarchy for the subset games that have pure Nash equilibria; we find that the optimal PoA reached in the previous special cases does not hold, but only a logarithmic one. Finally, we show that for a special case of submod-ular functions, efficient pure Nash equilibria always exist.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {972–978},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@article{10.1007/s10009-012-0253-y,
author = {Schaefer, Ina and Rabiser, Rick and Clarke, Dave and Bettini, Lorenzo and Benavides, David and Botterweck, Goetz and Pathak, Animesh and Trujillo, Salvador and Villela, Karina},
title = {Software diversity: state of the art and perspectives},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-012-0253-y},
doi = {10.1007/s10009-012-0253-y},
abstract = {Diversity is prevalent in modern software systems to facilitate adapting the software to customer requirements or the execution environment. Diversity has an impact on all phases of the software development process. Appropriate means and organizational structures are required to deal with the additional complexity introduced by software variability. This introductory article to the special section "Software Diversity--Modeling, Analysis and Evolution" provides an overview of the current state of the art in diverse systems development and discusses challenges and potential solutions. The article covers requirements analysis, design, implementation, verification and validation, maintenance and evolution as well as organizational aspects. It also provides an overview of the articles which are part of this special section and addresses particular issues of diverse systems development.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {477–495},
numpages = {19},
keywords = {Software diversity, Software product lines, Variability}
}

@inproceedings{10.1145/2364527.2364535,
author = {Chen, Sheng and Erwig, Martin and Walkingshaw, Eric},
title = {An error-tolerant type system for variational lambda calculus},
year = {2012},
isbn = {9781450310543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364527.2364535},
doi = {10.1145/2364527.2364535},
abstract = {Conditional compilation and software product line technologies make it possible to generate a huge number of different programs from a single software project. Typing each of these programs individually is usually impossible due to the sheer number of possible variants. Our previous work has addressed this problem with a type system for variational lambda calculus (VLC), an extension of lambda calculus with basic constructs for introducing and organizing variation. Although our type inference algorithm is more efficient than the brute-force strategy of inferring the types of each variant individually, it is less robust since type inference will fail for the entire variational expression if any one variant contains a type error. In this work, we extend our type system to operate on VLC expressions containing type errors. This extension directly supports locating ill-typed variants and the incremental development of variational programs. It also has many subtle implications for the unification of variational types. We show that our extended type system possesses a principal typing property and that the underlying unification problem is unitary. Our unification algorithm computes partial unifiers that lead to result types that (1) contain errors in as few variants as possible and (2) are most general. Finally, we perform an empirical evaluation to determine the overhead of this extension compared to our previous work, to demonstrate the improvements over the brute-force approach, and to explore the effects of various error distributions on the inference process.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming},
pages = {29–40},
numpages = {12},
keywords = {error-tolerant type systems, variational lambda calculus, variational type inference, variational types},
location = {Copenhagen, Denmark},
series = {ICFP '12}
}

@inbook{10.5555/3454287.3455259,
author = {Cao, Yuan and Gu, Quanquan},
title = {Generalization bounds of stochastic gradient descent for wide and deep neural networks},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the training and generalization of deep neural networks (DNNs) in the over-parameterized regime, where the network width (i.e., number of hidden nodes per layer) is much larger than the number of training data points. We show that, the expected 0-1 loss of a wide enough ReLU network trained with stochastic gradient descent (SGD) and random initialization can be bounded by the training loss of a random feature model induced by the network gradient at initialization, which we call a neural tangent random feature (NTRF) model. For data distributions that can be classified by NTRF model with sufficiently small error, our result yields a generalization error bound in the order of \~{O}(n-1/2) that is independent of the network width. Our result is more general and sharper than many existing generalization error bounds for over-parameterized neural networks. In addition, we establish a strong connection between our generalization error bound and the neural tangent kernel (NTK) proposed in recent work.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {972},
numpages = {11}
}

@inbook{10.1145/3191315.3191322,
author = {Borraz-S\'{a}nchez, Conrado and Klabjan, Diego and Pasalic, Emir and Aref, Molham},
title = {SolverBlox: algebraic modeling in datalog},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191322},
abstract = {Datalog is a deductive query language for relational databases. We introduce LogiQL, a language based on Datalog and show how it can be used to specify mixedinteger linear optimization models and solve them. Unlike pure algebraic modeling languages, LogiQL allows the user to both specify models, and manipulate and transform the inputs and outputs of the models. This is an advantage over conventional optimization modeling languages that rely on reading data via plug-in tools or importing data from external sources via files. In this chapter, we give a brief overview of LogiQL and describe two mixed integer programming case studies: a production-transportation model and a formulation of the traveling salesman problem.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {331–354},
numpages = {24}
}

@article{10.1007/s11276-020-02382-4,
author = {Deng, Jiaxin and Ou, Weihua and Gou, Jianping and Song, Heping and Wang, Anzhi and Xu, Xing},
title = {Representation separation adversarial networks for cross-modal retrieval},
year = {2020},
issue_date = {Jul 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {5},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-020-02382-4},
doi = {10.1007/s11276-020-02382-4},
abstract = {Cross-modal retrieval aims to search the semantically similar instances from the other modalities by giving a query from one modality. Recently, generative adversarial networks (GANs) has been proposed to model the joint distribution over the data from different modalities and to learn the common representations for cross-modal retrieval. However, most of existing GANs-based methods simply project original representations of different modalities into a common representation space, and ignore the fact that different modalities share the common characteristics and on the other side each modality has the individual characteristics. To address this problem, in this paper, we propose a novel cross-modal retrieval method, called representation separation adversarial networks, which explicitly separates the original representations into common latent representations and private representations. Specifically, we minimize the correlation between the common representations and private representations to ensure independence of them. Then, we reconstruct the original representations via exchanging the common representations of different modalities to encourage the information swap. Finally, the labels are utilized to increase the discriminant of common representations. Comprehensive experimental results on two widely used datasets show that the proposed method achieved better performance than many existing GANs-based methods, and demonstrate that explicitly modeling the private representation for each modality can improve the model to extract common latent representations.},
journal = {Wirel. Netw.},
month = jun,
pages = {3469–3481},
numpages = {13},
keywords = {Cross-modal retrieval, Adversarial learning, Common representation, Private representation, Representation separation}
}

@inproceedings{10.1007/978-3-030-89128-2_5,
author = {Bigazzi, Roberto and Landi, Federico and Cornia, Marcella and Cascianelli, Silvia and Baraldi, Lorenzo and Cucchiara, Rita},
title = {Out of the Box: Embodied Navigation in the Real World},
year = {2021},
isbn = {978-3-030-89127-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89128-2_5},
doi = {10.1007/978-3-030-89128-2_5},
abstract = {The research field of Embodied AI has witnessed substantial progress in visual navigation and exploration thanks to powerful simulating platforms and the availability of 3D data of indoor and photorealistic environments. These two factors have opened the doors to a new generation of intelligent agents capable of achieving nearly perfect PointGoal Navigation. However, such architectures are commonly trained with millions, if not billions, of frames and tested in simulation. Together with great enthusiasm, these results yield a question: how many researchers will effectively benefit from these advances? In this work, we detail how to transfer the knowledge acquired in simulation into the real world. To that end, we describe the architectural discrepancies that damage the Sim2Real adaptation ability of models trained on the Habitat simulator and propose a novel solution tailored towards the deployment in real-world scenarios. We then deploy our models on a LoCoBot, a Low-Cost Robot equipped with a single Intel RealSense camera. Different from previous work, our testing scene is unavailable to the agent in simulation. The environment is also inaccessible to the agent beforehand, so it cannot count on scene-specific semantic priors. In this way, we reproduce a setting in which a research group (potentially from other fields) needs to employ the agent visual navigation capabilities as-a-Service. Our experiments indicate that it is possible to achieve satisfying results when deploying the obtained model in the real world. Our code and models are available at .},
booktitle = {Computer Analysis of Images and Patterns: 19th International Conference, CAIP 2021, Virtual Event, September 28–30, 2021, Proceedings, Part I},
pages = {47–57},
numpages = {11},
keywords = {Embodied AI, Sim2Real, Visual navigation}
}

@inproceedings{10.5555/3466184.3466321,
author = {Rabe, Markus and Chicaiza-Vaca, Jorge and Tordecilla, Rafael D. and Juan, Angel A.},
title = {A simulation-optimization approach for locating automated parcel lockers in urban logistics operations},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Experts propose using an automated parcel locker (APL) for improving urban logistics operations. However, deciding the location of these APLs is not a trivial task, especially when considering a multi-period horizon under uncertainty. Based on a case study developed in Dortmund, Germany, we propose a simulation-optimization approach that integrates a system dynamics simulation model with a multi-period capacitated facility location problem (CFLP). First, we built the causal-loop and stock-flow diagrams to show the APL system's main components and interdependencies. Then, we formulated a multi-period CFLP model to provide the optimal number of APLs to be installed in each period. Finally, Monte Carlo simulation was used to estimate the cost and reliability level for different scenarios with random demands. In our experiments, only one solution reaches a 100% reliability level, with a total cost of 2.7 million euros. Nevertheless, if the budget is lower, our approach offers other good alternatives.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1230–1241},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1016/j.knosys.2010.10.004,
author = {Li, Jingpeng and Burke, Edmund K. and Qu, Rong},
title = {Integrating neural networks and logistic regression to underpin hyper-heuristic search},
year = {2011},
issue_date = {March, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {24},
number = {2},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2010.10.004},
doi = {10.1016/j.knosys.2010.10.004},
abstract = {A hyper-heuristic often represents a heuristic search method that operates over a space of heuristic rules. It can be thought of as a high level search methodology to choose lower level heuristics. Nearly 200 papers on hyper-heuristics have recently appeared in the literature. A common theme in this body of literature is an attempt to solve the problems in hand in the following way: at each decision point, first employ the chosen heuristic(s) to generate a solution, then calculate the objective value of the solution by taking into account all the constraints involved. However, empirical studies from our previous research have revealed that, under many circumstances, there is no need to carry out this costly 2-stage determination and evaluation at all times. This is because many problems in the real world are highly constrained with the characteristic that the regions of feasible solutions are rather scattered and small. Motivated by this observation and with the aim of making the hyper-heuristic search more efficient and more effective, this paper investigates two fundamentally different data mining techniques, namely artificial neural networks and binary logistic regression. By learning from examples, these techniques are able to find global patterns hidden in large data sets and achieve the goal of appropriately classifying the data. With the trained classification rules or estimated parameters, the performance (i.e. the worth of acceptance or rejection) of a resulting solution during the hyper-heuristic search can be predicted without the need to undertake the computationally expensive 2-stage of determination and calculation. We evaluate our approaches on the solutions (i.e. the sequences of heuristic rules) generated by a graph-based hyper-heuristic proposed for exam timetabling problems. Time complexity analysis demonstrates that the neural network and the logistic regression method can speed up the search significantly. We believe that our work sheds light on the development of more advanced knowledge-based decision support systems.},
journal = {Know.-Based Syst.},
month = mar,
pages = {322–330},
numpages = {9},
keywords = {Data mining, Educational timetabling, Hyper-heuristic, Logistic regression, Neural network}
}

@article{10.1016/j.dss.2013.10.008,
author = {Choi, Tsan-Ming and Hui, Chi-Leung and Liu, Na and Ng, Sau-Fun and Yu, Yong},
title = {Fast fashion sales forecasting with limited data and time},
year = {2014},
issue_date = {March, 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2013.10.008},
doi = {10.1016/j.dss.2013.10.008},
abstract = {Fast fashion is a commonly adopted strategy in fashion retailing. Under fast fashion, operational decisions have to be made with a tight schedule and the corresponding forecasting method has to be completed with very limited data within a limited time duration. Motivated by fast fashion business practices, in this paper, an intelligent forecasting algorithm, which combines tools such as the extreme learning machine and the grey model, is developed. Our real data analysis demonstrates that this newly derived algorithm can generate reasonably good forecasting under the given time and data constraints. Further analysis with an artificial dataset shows that the proposed algorithm performs especially well when either (i) the demand trend slope is large, or (ii) the seasonal cycle's variance is large. These two features fit the fast fashion demand pattern very well because the trend factor is significant and the seasonal cycle is usually highly variable in fast fashion. The results from this paper lay the foundation which can help to achieve real time sales forecasting for fast fashion operations in the future. Some managerial implications are also discussed.},
journal = {Decis. Support Syst.},
month = mar,
pages = {84–92},
numpages = {9},
keywords = {Fashion forecasting, Fast fashion, Intelligent forecasting, Quick forecasting, Time series}
}

@inproceedings{10.1007/978-3-030-48077-6_3,
author = {Claris\'{o}, Robert and Cabot, Jordi},
title = {Diverse Scenario Exploration in Model Finders Using Graph Kernels and Clustering},
year = {2020},
isbn = {978-3-030-48076-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-48077-6_3},
doi = {10.1007/978-3-030-48077-6_3},
abstract = {Complex software systems can be described using modeling notations such as UML/OCL or Alloy. Then, some correctness properties of these systems can be checked using model finders, which compute sample scenarios either fulfilling the desired properties or illustrating potential faults. Such scenarios allow designers to validate, verify and test the system under development.Nevertheless, when asked to produce several scenarios, model finders tend to produce similar solutions. This lack of diversity impairs their effectiveness as testing or validation assets. To solve this problem, we propose the use of graph kernels, a family of methods for computing the (dis)similarity among pairs of graphs. With this metric, it is possible to cluster scenarios effectively, improving the usability of model finders and making testing and validation more efficient.},
booktitle = {Rigorous State-Based Methods: 7th International Conference, ABZ 2020, Ulm, Germany, May 27–29, 2020, Proceedings},
pages = {27–43},
numpages = {17},
keywords = {Model-driven engineering, Verification and validation, Testing, Graph kernels, Clustering, Diversity},
location = {Ulm, Germany}
}

@article{10.1007/s10601-017-9279-9,
author = {Alghazi, Anas and Kurz, Mary E.},
title = {Mixed model line balancing with parallel stations, zoning constraints, and ergonomics},
year = {2018},
issue_date = {January   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1383-7133},
url = {https://doi.org/10.1007/s10601-017-9279-9},
doi = {10.1007/s10601-017-9279-9},
abstract = {Assembly lines are cost efficient production systems that mass produce identical products. Due to customer demand, manufacturers use mixed model assembly lines to produce customized products that are not identical. To stay efficient, management decisions for the line such as number of workers and assembly task assignment to stations need to be optimized to increase throughput and decrease cost. In each station, the work to be done depends on the exact product configuration, and is not consistent across all products. In this paper, we propose a mixed model line balancing integer program (IP) that considers parallel workers, zoning, task assignment, and ergonomic constraints with the objective of minimizing the number of workers. Upon observing the limitation of the IP, a Constraint Programming (CP) model is developed to solve larger assembly line balancing problems. Data from an automotive OEM are used to assess the performance of both the MIP and CP models, including sensitivity analysis to measure the computational cost of enabling the different constraints. To the best of our knowledge, we are the first paper to incorporate the different realistic mixed model assembly line constraints and develop a CP model based on the scheduling module of the IBM ILOG Optimizations Studio. Using the OEM data, we show that the CP model outperforms the IP model for bigger problems.},
journal = {Constraints},
month = jan,
pages = {123–153},
numpages = {31},
keywords = {Assembling line balancing, Mixed integer programs, Mixed model assembly lines}
}

@article{10.1007/s10664-020-09912-w,
author = {Damasceno, Carlos Diego Nascimento and Mousavi, Mohammad Reza and Simao, Adenilso da Silva},
title = {Learning by sampling: learning behavioral family models from software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09912-w},
doi = {10.1007/s10664-020-09912-w},
abstract = {Family-based behavioral analysis operates on a single specification artifact, referred to as family model, annotated with feature constraints to express behavioral variability in terms of conditional states and transitions. Family-based behavioral modeling paves the way for efficient model-based analysis of software product lines. Family-based behavioral model learning incorporates feature model analysis and model learning principles to efficiently unify product models into a family model and integrate the behavior of various products into a behavioral family model. Albeit reasonably effective, the exhaustive analysis of product lines is often infeasible due to the potentially exponential number of valid configurations. In this paper, we first present a family-based behavioral model learning techniques, called FFSMDiff. Subsequently, we report on our experience on learning family models by employing product sampling. Using 105 products of six product lines expressed in terms of Mealy machines, we evaluate the precision of family models learned from products selected from different settings of the T-wise product sampling criterion. We show that product sampling can lead to models as precise as those learned by exhaustive analysis and hence, reduce the costs for family model learning.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {46},
keywords = {Software product lines, Model learning, Family model, T-wise sampling}
}

@article{10.1007/s10270-018-00704-x,
author = {Rodrigues, V\'{\i}tor and Donetti, Simone and Damiani, Ferruccio},
title = {Certifying delta-oriented programs},
year = {2019},
issue_date = {October   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-00704-x},
doi = {10.1007/s10270-018-00704-x},
abstract = {A major design concern in modern software development frameworks is to ensure that mechanisms for updating code running on remote devices comply with given safety specifications. This paper presents a delta-oriented approach for implementing product lines where software reuse is achieved at the three levels of state-diagram modeling, C/$$text {C}^{_{_{_{++}}}} $$C++source code and binary code. A safety specification is expressed on the properties of reusable software libraries that can be dynamically loaded at run time after an over-the-air update. The compilation of delta-engineered code is certified using the framework of proof-carrying code in order to guarantee safety of software updates on remote devices. An empirical evaluation of the computational cost associated with formal safety checks is done by means of experimentation.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {2875–2906},
numpages = {32},
keywords = {Delta-oriented programming, Model-driven development, Proof-carrying code, Runtime systems, Safety properties}
}

@inproceedings{10.5555/3042817.3042922,
author = {Das, Mrinal Kanti and Bhattacharya, Suparna and Bhattacharyya, Chiranjib and Gopinath, K.},
title = {Subtle topic models and discovering subtly manifested software concerns automatically},
year = {2013},
publisher = {JMLR.org},
abstract = {In a recent pioneering approach LDA was used to discover cross cutting concerns (CCC) automatically from software codebases. LDA though successful in detecting prominent concerns, fails to detect many useful CCCs including ones that may be heavily executed but elude discovery because they do not have a strong prevalence in source-code. We pose this problem as that of discovering topics that rarely occur in individual documents, which we will refer to as subtle topics. Recently an interesting approach, namely focused topic models(FTM) was proposed in (Williamson et al., 2010) for detecting rare topics. FTM, though successful in detecting topics which occur prominently in very few documents, is unable to detect subtle topics. Discovering subtle topics thus remains an important open problem. To address this issue we propose subtle topic models (STM). STM uses a generalized stick breaking process (GSBP) as a prior for defining multiple distributions over topics. This hierarchical structure on topics allows STM to discover rare topics beyond the capabilities of FTM. The associated inference is non-standard and is solved by exploiting the relationship between GSBP and generalized Dirichlet distribution. Empirical results show that STM is able to discover subtle CCC in two benchmark code-bases, a feat which is beyond the scope of existing topic models, thus demonstrating the potential of the model in automated concern discovery, a known difficult problem in Software Engineering. Furthermore it is observed that even in general text corpora STM outperforms the state of art in discovering subtle topics.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {II–253–II–261},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@article{10.1016/j.cie.2019.03.051,
author = {B\'{a}ez, Sarah\'{\i} and Angel-Bello, Francisco and Alvarez, Ada and Meli\'{a}n-Batista, Bel\'{e}n},
title = {A hybrid metaheuristic algorithm for a parallel machine scheduling problem with dependent setup times},
year = {2019},
issue_date = {May 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2019.03.051},
doi = {10.1016/j.cie.2019.03.051},
journal = {Comput. Ind. Eng.},
month = may,
pages = {295–305},
numpages = {11},
keywords = {Parallel machine scheduling, Total completion time, Setup time, GRASP, Variable neighborhood search}
}

@phdthesis{10.5555/AAI28087948,
author = {Kumar, Lakshmi Magudilu Srishyla and Nie, Yingjiu and DePaolis, Rory},
advisor = {Ayasakanta, Rout,},
title = {Measuring Listening Effort using Physiological, Behavioral and Subjective Methods in Normal Hearing Subjects: Effect of Signal to Noise Ratio and Presentation Level},
year = {2020},
isbn = {9798672128375},
publisher = {James Madison University},
abstract = {The main objective of the study is to compare the effectiveness of pupillometry, working memory and subjective rating scale —the physiological, behavioral, and subjective measures of listening effort— at different signal to noise ratios (SNR) and presentation levels: when administered together. Eleven young normal hearing individuals with mean age of 21.7 years (SD=1.9 years) participated in the study. The HINT sentences were used for speech perception in noise task. The listening effort was quantified using peak pupil dilation, working memory, working memory difference, subjective rating of listening and recall effort. The rating of perceived performance, frustration level and disengagement were also obtained. Using a repeated measure design, we examined how SNR (+6 dB to -10 dB) and presentation level (50- and 65-dB SPL) affect listening effort. Tobii eye-tracker software and custom MATLAB programing were used for stimulus presentation and data analysis. SNR had significant effect on peak pupil dilation, working memory, working memory difference, and subjective rating of listening effort. Speech intelligibility had significant correlation with all of the listening effort measures except working memory difference. The listening effort measures did not correlate significantly when controlled for speech intelligibility indicating different underlying constructs. When effect sizes are compared working memory (η2p = 0.98) was most sensitive to SNR effect, followed by subjective rating of listening effort (η2p = 0.84), working memory difference (η2p = 0.52) and peak pupil dilation (η2p = 0.40). Only peak pupil dilation showed significant effect of presentation level. The physiological, behavioral and subjective measures of listening effort have different underlying constructs and the sensitivity of these measures varies in representing the effect of SNR and presentation level. The individual data trend analysis shows different breakdown points for physiological and behavioral and subjective measures. There is a need to further explore the relationship of listening effort measures across different SNRs also how these relationship changes in persons with hearing loss.},
note = {AAI28087948}
}

@inproceedings{10.1145/2739482.2768422,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Vergilio, Silvia R. and Egyed, Alexander},
title = {Genetic Improvement for Software Product Lines: An Overview and a Roadmap},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768422},
doi = {10.1145/2739482.2768422},
abstract = {Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {823–830},
numpages = {8},
keywords = {evolutionary algorithms, genetic improvement, genetic programming, software product lines, variability},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.5555/3020548.3020606,
author = {Marlin, Benjamin M. and de Freitas, Nando},
title = {Asymptotic efficiency of deterministic estimators for discrete energy-based models: ratio matching and pseudolikelihood},
year = {2011},
isbn = {9780974903972},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Standard maximum likelihood estimation cannot be applied to discrete energy-based models in the general case because the computation of exact model probabilities is intractable. Recent research has seen the proposal of several new estimators designed specifically to overcome this intractability, but virtually nothing is known about their theoretical properties. In this paper, we present a generalized estimator that unifies many of the classical and recently proposed estimators. We use results from the standard asymptotic theory for M-estimators to derive a generic expression for the asymptotic co-variance matrix of our generalized estimator. We apply these results to study the relative statistical efficiency of classical pseudolikelihood and the recently-proposed ratio matching estimator.},
booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence},
pages = {497–505},
numpages = {9},
location = {Barcelona, Spain},
series = {UAI'11}
}

@inproceedings{10.5555/978-3-030-83903-1_fm,
title = {Front Matter},
year = {2021},
isbn = {978-3-030-83902-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Computer Safety, Reliability, and Security: 40th International Conference, SAFECOMP 2021, York, UK, September 8–10, 2021, Proceedings},
pages = {i–xiv},
location = {York, United Kingdom}
}

@article{10.1007/s11263-020-01374-3,
author = {Vasudevan, Arun Balajee and Dai, Dengxin and Van Gool, Luc},
title = {Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {129},
number = {1},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-020-01374-3},
doi = {10.1007/s11263-020-01374-3},
abstract = {The role of robots in society keeps expanding, bringing with it the necessity of interacting and communicating with humans. In order to keep such interaction intuitive, we provide automatic wayfinding based on verbal navigational instructions. Our first contribution is the creation of a large-scale dataset with verbal navigation instructions. To this end, we have developed an interactive visual navigation environment based on Google Street View; we further design an annotation method to highlight mined anchor landmarks and local directions between them in order to help annotators formulate typical, human references to those. The annotation task was crowdsourced on the AMT platform, to construct a new Talk2Nav dataset with 10,&nbsp;714 routes. Our second contribution is a new learning method. Inspired by spatial cognition research on the mental conceptualization of navigational instructions, we introduce a soft dual attention mechanism defined over the segmented language instructions to jointly extract two partial instructions—one for matching the next upcoming visual landmark and the other for matching the local directions to the next landmark. On the similar lines, we also introduce spatial memory scheme to encode the local directional transitions. Our work takes advantage of the advance in two lines of research: mental formalization of verbal navigational instructions and training neural network agents for automatic way finding. Extensive experiments show that our method significantly outperforms previous navigation methods. For demo video, dataset and code, please refer to our .},
journal = {Int. J. Comput. Vision},
month = jan,
pages = {246–266},
numpages = {21},
keywords = {Vision-and-language navigation, Long-range navigation, Spatial memory, Dual attention}
}

@inproceedings{10.5555/3298023.3298125,
author = {Hanna, Josiah P. and Stone, Peter},
title = {Grounded action transformation for robot learning in simulation},
year = {2017},
publisher = {AAAI Press},
abstract = {Robot learning in simulation is a promising alternative to the prohibitive sample cost of learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the physical robot. Grounded simulation learning (GSL) promises to address this issue by altering the simulator to better match the real world. This paper proposes a new algorithm for GSL - Grounded Action Transformation - and applies it to learning of humanoid bipedal locomotion. Our approach results in a 43.27% improvement in forward walk velocity compared to a state-of-the art hand-coded walk. We further evaluate our methodology in controlled experiments using a second, higher-fidelity simulator in place of the real world. Our results contribute to a deeper understanding of grounded simulation learning and demonstrate its effectiveness for learning robot control policies.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {3834–3840},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.1016/j.future.2019.05.056,
author = {Bagozi, Ada and Bianchini, Devis and De Antonellis, Valeria and Garda, Massimiliano and Marini, Alessandro},
title = {A Relevance-based approach for Big Data Exploration},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.05.056},
doi = {10.1016/j.future.2019.05.056},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {51–69},
numpages = {19},
keywords = {Data exploration, Big data, Multi-dimensional data modelling, Human-In-the-Loop Data Analysis, Industry 4.0, Cyber Physical Systems}
}

@article{10.5555/3006046.3006110,
author = {Lorentz, Harri and Hilmola, Olli-Pekka and Malmsten, Jarmo and Srai, Jagjit Singh},
title = {Cluster analysis application for understanding SME manufacturing strategies},
year = {2016},
issue_date = {December 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0957-4174},
abstract = {Uses clustering to detect manufacturing strategies inspired by Miller\'{z}and Roth\'{z}(1994).Analysis based on Finnish SME survey and publicly available financial statements.Application proposes three clusters to be used as manufacturing strategy taxon.Subcontracting cluster shows lowest financial performance among clusters.Engineer-servers cluster seems to be best strategy in post 2008-2009 era. Small and medium size enterprises' (SME) manufacturing strategy configurations are identified in a small developed economy with the aim to explore how SME manufacturing strategy configurations affect business stability and performance during a period of macroeconomic shock. Drawing on a survey-based dataset, our two-step cluster analysis results suggest that three distinctive manufacturing strategy configurations can be observed among the SMEs of the Finnish manufacturing sector, namely: Responsive niche-innovators, Subcontractors, and Engineer-servers. Furthermore, we are able to establish a link between the strategy configurations and business stability and performance. The results support conclusions that the nature of manufacturing strategy taxonomies are driven by the business context, and that volume flexibility, design flexibility and service provision capabilities enable better business outcomes during macroeconomic shocks, in comparison to the more easily achievable conformance quality as well as delivery speed and dependability. In light of this research, best performing cluster under the macroeconomic shock is the Engineer-servers, emphasizing flexibility-oriented broad product line and after sales service, while having less priority concerning low price and volume flexibility. The results offer important insights for managers, but also for other stakeholders in the form of for example expert systems development for SME funding decisions.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {176–188},
numpages = {13},
keywords = {Business cycle, Configuration, Manufacturing strategy, Performance, SME, Taxonomy}
}

@article{10.1016/j.cl.2016.11.002,
author = {Malhotra, Ruchika},
title = {Special issue on search-based techniques and their hybridizations in software engineering},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {P2},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2016.11.002},
doi = {10.1016/j.cl.2016.11.002},
journal = {Comput. Lang. Syst. Struct.},
month = jan,
pages = {151–152},
numpages = {2}
}

@inproceedings{10.1145/2642937.2642971,
author = {Guo, Jianmei and Zulkoski, Edward and Olaechea, Rafael and Rayside, Derek and Czarnecki, Krzysztof and Apel, Sven and Atlee, Joanne M.},
title = {Scaling exact multi-objective combinatorial optimization by parallelization},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642971},
doi = {10.1145/2642937.2642971},
abstract = {Multi-Objective Combinatorial Optimization (MOCO) is fundamental to the development and optimization of software systems. We propose five novel parallel algorithms for solving MOCO problems exactly and efficiently. Our algorithms rely on off-the-shelf solvers to search for exact Pareto-optimal solutions, and they parallelize the search via collaborative communication, divide-and-conquer, or both. We demonstrate the feasibility and performance of our algorithms by experiments on three case studies of software-system designs. A key finding is that one algorithm, which we call FS-GIA, achieves substantial (even super-linear) speedups that scale well up to 64 cores. Furthermore, we analyze the performance bottlenecks and opportunities of our parallel algorithms, which facilitates further research on exact, parallel MOCO.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {409–420},
numpages = {12},
keywords = {multi-objective combinatorial optimization, parallelization},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1016/j.cmpb.2011.07.015,
author = {Cabezas, Mariano and Oliver, Arnau and Llad\'{o}, Xavier and Freixenet, Jordi and Bach Cuadra, Meritxell},
title = {A review of atlas-based segmentation for magnetic resonance brain images},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {104},
number = {3},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2011.07.015},
doi = {10.1016/j.cmpb.2011.07.015},
abstract = {Abstract: Normal and abnormal brains can be segmented by registering the target image with an atlas. Here, an atlas is defined as the combination of an intensity image (template) and its segmented image (the atlas labels). After registering the atlas template and the target image, the atlas labels are propagated to the target image. We define this process as atlas-based segmentation. In recent years, researchers have investigated registration algorithms to match atlases to query subjects and also strategies for atlas construction. In this paper we present a review of the automated approaches for atlas-based segmentation of magnetic resonance brain images. We aim to point out the strengths and weaknesses of atlas-based methods and suggest new research directions. We use two different criteria to present the methods. First, we refer to the algorithms according to their atlas-based strategy: label propagation, multi-atlas methods, and probabilistic techniques. Subsequently, we classify the methods according to their medical target: the brain and its internal structures, tissue segmentation in healthy subjects, tissue segmentation in fetus, neonates and elderly subjects, and segmentation of damaged brains. A quantitative comparison of the results reported in the literature is also presented.},
journal = {Comput. Methods Prog. Biomed.},
month = dec,
pages = {e158–e177},
numpages = {20},
keywords = {Atlas, Automated methods, Brain, Magnetic resonance imaging, Segmentation}
}

@inproceedings{10.5555/3540261.3542031,
author = {Deng, Qi and Gao, Wenzhi},
title = {Minibatch and momentum model-based methods for stochastic weakly convex optimization},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Stochastic model-based methods have received increasing attention lately due to their appealing robustness to the stepsize selection and provable efficiency guarantee. We make two important extensions for improving model-based methods on stochastic weakly convex optimization. First, we propose new minibatch model-based methods by involving a set of samples to approximate the model function in each iteration. For the first time, we show that stochastic algorithms achieve linear speedup over the batch size even for non-smooth and non-convex (particularly, weakly convex) problems. To this end, we develop a novel sensitivity analysis of the proximal mapping involved in each algorithm iteration. Our analysis appears to be of independent interests in more general settings. Second, motivated by the success of momentum stochastic gradient descent, we propose a new stochastic extrapolated model-based method, greatly extending the classic Polyak momentum technique to a wider class of stochastic algorithms for weakly convex optimization. The rate of convergence to some natural stationarity condition is established over a fairly flexible range of extrapolation terms.While mainly focusing on weakly convex optimization, we also extend our work to convex optimization. We apply the minibatch and extrapolated model-based methods to stochastic convex optimization, for which we provide a new complexity bound and promising linear speedup in batch size. Moreover, an accelerated model-based method based on Nesterov's momentum is presented, for which we establish an optimal complexity bound for reaching optimality.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1770},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.5555/646014.677489,
author = {Dampos, Te\'{o}filo Em\'{\i}dio de and Feris, Rog\'{e}rio Schimidt and Junior, Roberto Marcondes Cesar},
title = {Improved Face/spl times/Non-Face Discrimination using Fourier Descriptors through Feature Selection},
year = {2000},
isbn = {0769508782},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This work presents a new method to discriminate face from non-face images using Fourier descriptors. The first step of our approach consists in applying a horizontal edge detection filter in the input image, followed by the extraction of a 1D signal from the computed edge map. Then we calculate the Fourier descriptors from this signal and classify the image using statistical classifiers. In order to improve our results, we applied a feature selection algorithm. Preliminary performance assessment results have shown that this approach is superior than traditional transform based methods. Besides, these results showed that our method might be used to develop a fast face detection system.},
booktitle = {Proceedings of the 13th Brazilian Symposium on Computer Graphics and Image Processing},
pages = {28–35},
numpages = {8},
keywords = {1D signal extraction, Fourier descriptors, face detection, face discrimination, face recognition, feature selection, horizontal edge detection filter, image classification, performance assessment, statistical classifiers, transform based methods},
series = {SIBGRAPI '00}
}

@article{10.1155/2021/5517843,
author = {Zaiyi, Pu and Kun, Yang and Chaobin, Wang and Chen, Chi-Hua},
title = {Distributed Network Image Processing System and Transmission Control Algorithm},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/5517843},
doi = {10.1155/2021/5517843},
abstract = {With the increasing use of Internet technologies, image data is spreading more and more on the Internet. Whether it is a social network or a search engine, a large amount of image data is generated. By studying the distributed network image processing system and transmission control algorithm, this paper proposes a more accurate gradient calculation method based on the SIFT algorithm. It is concluded that the performance of the proposed algorithm is slightly better than that of the original algorithm, so the system is implemented. On the basis of reducing the performance of the original algorithm, the dimension of the image features is effectively reduced. By comparing the influence of the image retrieval system in the single-machine environment and the distributed environment on the image feature extraction rate, it is proved that the system uses five distributed nodes to construct the image transmission system that achieves the best results in terms of machine cost and system performance. The random Gaussian orthogonal matrix is analyzed with good stability and performance. The OMP algorithm has good convergence and reconstruction performance. The MH-BCS-SPL reconstruction algorithm works best, and the PSNR decreases very smoothly in the process of increasing the packet loss rate from 0.1 to 0.6. The experimental results show that different orthogonal bases behave differently under different images. Overall, the BCS-SPL series algorithm has greatly improved the reconstruction effect compared with the traditional OMP algorithm.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {10}
}

@inproceedings{10.1145/3357384.3358163,
author = {Huang, Chao and Shi, Baoxu and Zhang, Xuchao and Wu, Xian and Chawla, Nitesh V.},
title = {Similarity-Aware Network Embedding with Self-Paced Learning},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358163},
doi = {10.1145/3357384.3358163},
abstract = {Network embedding, which aims to learn low-dimensional vector representations for nodes in a network, has shown promising performance for many real-world applications, such as node classification and clustering. While various embedding methods have been developed for network data, they are limited in their assumption that nodes are correlated with their neighboring nodes with the same similarity degree. As such, these methods can be suboptimal for embedding network data. In this paper, we propose a new method named SANE, short for Similarity-Aware Network Embedding, to learn node representations by explicitly considering different similarity degrees between connected nodes in a network. In particular, we develop a new framework based on self-paced learning by accounting for both the explicit relations (i.e., observed links) and implicit relations (i.e., unobserved node similarities) in network representation learning. To justify our proposed model, we perform experiments on two real-world network data. Experiments results show that SNAE outperforms state-of-the-art embedding models on the tasks of node classification and node clustering.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2113–2116},
numpages = {4},
keywords = {deep neural network, network embedding, self-paced learning},
location = {Beijing, China},
series = {CIKM '19}
}

@article{10.1016/j.asoc.2020.106121,
author = {Leelathakul, Nutthanon and Rimcharoen, Sunisa},
title = {Generating Kranok patterns with an interactive evolutionary algorithm},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.106121},
doi = {10.1016/j.asoc.2020.106121},
journal = {Appl. Soft Comput.},
month = apr,
numpages = {17},
keywords = {Interactive evolutionary algorithm, Thai drawing, Kranok pattern, B\'{e}zier curve, Generated art}
}

@inproceedings{10.1109/ASE.2015.45,
author = {Sarkar, Atri and Guo, Jianmei and Siegmund, Norbert and Apel, Sven and Czarnecki, Krzysztof},
title = {Cost-efficient sampling for performance prediction of configurable systems},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.45},
doi = {10.1109/ASE.2015.45},
abstract = {A key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected. It is usually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predict performance based on small samples of measured variants, but it is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort. In this paper, we adapt two widely-used sampling strategies for performance prediction to the domain of configurable systems and evaluate them in terms of sampling cost, which considers prediction accuracy and measurement effort simultaneously. To generate an initial sample, we introduce a new heuristic based on feature frequencies and compare it to a traditional method based on t-way feature coverage. We conduct experiments on six real-world systems and provide guidelines for stakeholders to predict performance by sampling.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {342–352},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1155/2021/9063410,
author = {Zhang, Jianfeng and Lv, Zhihan},
title = {Virtual Viewpoint Film and Television Synthesis Based on the Intelligent Algorithm of Wireless Network Communication for Image Repair},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9063410},
doi = {10.1155/2021/9063410},
abstract = {With the development of the computer vision field, the acquisition of scene depth information is one of the important topics in the three-dimensional reconstruction of the computer vision field, and its significance is particularly important. The purpose of this paper is to study the virtual viewpoint video synthesis for image restoration based on the intelligent algorithm of wireless network communication. Aiming at the hole problem caused by the change of occlusion relationship, this paper proposes a hole-filling method based on background recognition. A threshold segmentation algorithm is used to reduce the filling priority of foreground pixels at the boundary of the hole and fully solve the hole problem. This paper also proposes a wireless sensor network node positioning model with swarm intelligence algorithm, which combines swarm intelligence algorithm with some key issues of wireless sensor network, speeds up the convergence, and improves the traditional intelligence algorithm. According to the experimental data in this paper, the algorithm in this paper is about 20% higher than the traditional algorithm in PSNR. On SSIM, the performance of the algorithm in this paper is 4.6% higher than the traditional algorithm at most, and the lowest is 2.2%.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {15}
}

@article{10.1145/3432195,
author = {Mao, Wenguang and Sun, Wei and Wang, Mei and Qiu, Lili},
title = {DeepRange: Acoustic Ranging via Deep Learning},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3432195},
doi = {10.1145/3432195},
abstract = {Acoustic ranging is a technique for estimating the distance between two objects using acoustic signals, which plays a critical role in many applications, such as motion tracking, gesture/activity recognition, and indoor localization. Although many ranging algorithms have been developed, their performance still degrades significantly under strong noise, interference and hardware limitations. To improve the robustness of the ranging system, in this paper we develop a Deep learning based Ranging system, called DeepRange. We first develop an effective mechanism to generate synthetic training data that captures noise, speaker/mic distortion, and interference in the signals and remove the need of collecting a large volume of training data. We then design a deep range neural network (DRNet) to estimate distance. Our design is inspired by signal processing that ultra-long convolution kernel sizes help to combat the noise and interference. We further apply an ensemble method to enhance the performance. Moreover, we analyze and visualize the network neurons and filters, and identify a few important findings that can be useful for improving the design of signal processing algorithms. Finally, we implement and evaluate DeepRangeusing 11 smartphones with different brands and models, 4 environments (i.e., a lab, a conference room, a corridor, and a cubic area), and 10 users. Our results show that DRNet significantly outperforms existing ranging algorithms.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {143},
numpages = {23},
keywords = {Acoustic Sensing, Convolutional Neural Network, Motion Tracking, Ranging}
}

@article{10.1016/j.ins.2021.10.020,
author = {Shanmugam, Lakshmanan and Joo, Young Hoon},
title = {Investigation on stability of delayed T-S fuzzy interconnected systems via decentralized memory-based sampled-data control and validation through interconnected power systems with DFIG-based wind turbines},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {580},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.10.020},
doi = {10.1016/j.ins.2021.10.020},
journal = {Inf. Sci.},
month = nov,
pages = {934–952},
numpages = {19},
keywords = {Nonlinear interconnected systems, Lyapunov stability theory, Linear matrix inequality, Decentralized sampled-data control system, T-S fuzzy approach}
}

@article{10.1016/j.neucom.2015.09.011,
author = {Tong, Le and Wong, W.K. and Kwong, C.K.},
title = {Differential evolution-based optimal Gabor filter model for fabric inspection},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {173},
number = {P3},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.09.011},
doi = {10.1016/j.neucom.2015.09.011},
abstract = {In this paper, a defect detection model using optimized Gabor filters, which is suitable for real-time operation, is proposed to tackle the woven fabric inspection problem in fashion industry. Based on the analysis of the particular characteristics of fabric defects, the proposed model utilizes composite differential evolution (CoDE) to optimize the parameters of Gabor filters, which can achieve the optimal feature extraction of fabric defects. Together with thresholding and fusion operations, the optimal Gabor filters can successfully segment the defects from the original image background. By using optimal Gabor filters instead of a Gabor filter bank, the computational cost of the detection model can be significantly reduced. The performance of the proposed defect detection model is evaluated off-line through extensive experiments based on various types of fabric. Experimental results reveal that the proposed detection model is effective and robust, and is superior than four existing models in terms of the high detection rate and low false alarm rate.},
journal = {Neurocomput.},
month = jan,
pages = {1386–1401},
numpages = {16},
keywords = {Defect detection, Differential evolution, Fabric quality inspection, Optimal Gabor filters}
}

@inproceedings{10.1007/978-3-030-79382-1_6,
author = {Burgue\~{n}o, Loli and Claris\'{o}, Robert and G\'{e}rard, S\'{e}bastien and Li, Shuai and Cabot, Jordi},
title = {An NLP-Based Architecture for&nbsp;the&nbsp;Autocompletion of Partial Domain Models},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_6},
doi = {10.1007/978-3-030-79382-1_6},
abstract = {Domain models capture the key concepts and relationships of a business domain. Typically, domain models are manually defined by software designers in the initial phases of a software development cycle, based on their interactions with the client and their own domain expertise. Given the key role of domain models in the quality of the final system, it is important that they properly reflect the reality of the business.To facilitate the definition of domain models and improve their quality, we propose to move towards a more assisted domain modeling building process where an NLP-based assistant will provide autocomplete suggestions for the partial model under construction based on the automatic analysis of the textual information available for the project (contextual knowledge) and/or its related business domain (general knowledge). The process will also take into account the feedback collected from the designer’s interaction with the assistant. We have developed a proof-of-concept tool and have performed a preliminary evaluation that shows promising results.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {91–106},
numpages = {16},
keywords = {Domain model, Autocomplete, Modeling recommendations, Assistant, Natural language processing},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1109/ICSE-SEET52601.2021.00022,
author = {Azanza, Maider and Irastorza, Arantza and Medeiros, Raul and D\'{\i}az, Oscar},
title = {Onboarding in software product lines: concept maps as welcome guides},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00022},
doi = {10.1109/ICSE-SEET52601.2021.00022},
abstract = {With a volatile labour and technological market, onboarding is becoming increasingly important. The process of incorporating a new developer, a.k.a. the newcomer, into a software development team is reckoned to be lengthy, frustrating and expensive. Newcomers face personal, interpersonal, process and technical barriers during their incorporation, which, in turn, affects the overall productivity of the whole team. This problem exacerbates for Software Product Lines (SPLs), where their size and variability combine to make onboarding even more challenging, even more so for developers that are transferred from the Application Engineering team into the Domain Engineering team, who will be our target newcomers. This work presents concept maps on the role of sensemaking scaffolds to help to introduce these newcomers into the SPL domain. Concept maps, used as knowledge visualisation tools, have been proven to be helpful for meaningful learning. Our main insight is to capture concepts of the SPL domain and their interrelationships in a concept map, and then, present them incrementally, helping newcomers grasp the SPL and aiding them in exploring it in a guided manner while avoiding information overload. This work's contributions are four-fold. First, concept maps are proposed as a representation to introduce newcomers into the SPL domain. Second, concept maps are presented as the means for a guided exploration of the SPL core assets. Third, a feature-driven concept map construction process is introduced. Last, the usefulness of concept maps as guides for SPL onboarding is tested through a formative evaluation.Link to the online demo: https://rebrand.ly/wacline-cmap},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {122–133},
numpages = {12},
location = {Virtual Event, Spain},
series = {ICSE-JSEET '21}
}

@article{10.1016/j.eswa.2013.10.038,
author = {Wang, Shih-Chang and Yeh, Ming-Feng},
title = {A modified particle swarm optimization for aggregate production planning},
year = {2014},
issue_date = {May, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {6},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.10.038},
doi = {10.1016/j.eswa.2013.10.038},
abstract = {Particle swarm optimization (PSO) originated from bird flocking models. It has become a popular research field with many successful applications. In this paper, we present a scheme of an aggregate production planning (APP) from a manufacturer of gardening equipment. It is formulated as an integer linear programming model and optimized by PSO. During the course of optimizing the problem, we discovered that PSO had limited ability and unsatisfactory performance, especially a large constrained integral APP problem with plenty of equality constraints. In order to enhance its performance and alleviate the deficiencies to the problem solving, a modified PSO (MPSO) is proposed, which introduces the idea of sub-particles, a particular coding principle, and a modified operation procedure of particles to the update rules to regulate the search processes for a particle swarm. In the computational study, some instances of the APP problems are experimented and analyzed to evaluate the performance of the MPSO with standard PSO (SPSO) and genetic algorithm (GA). The experimental results demonstrate that the MPSO variant provides particular qualities in the aspects of accuracy, reliability, and convergence speed than SPSO and GA.},
journal = {Expert Syst. Appl.},
month = may,
pages = {3069–3077},
numpages = {9},
keywords = {Aggregate production planning (APP), Integer linear programming model, Particle swarm optimization (PSO)}
}

@article{10.1007/s00779-011-0468-z,
author = {Broek, Egon L. and Sluis, Frans and Dijkstra, Ton},
title = {Cross-validation of bimodal health-related stress assessment},
year = {2013},
issue_date = {February  2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {2},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-011-0468-z},
doi = {10.1007/s00779-011-0468-z},
abstract = {This study explores the feasibility of objective and ubiquitous stress assessment. 25 post-traumatic stress disorder patients participated in a controlled storytelling (ST) study and an ecologically valid reliving (RL) study. The two studies were meant to represent an early and a late therapy session, and each consisted of a "happy" and a "stress triggering" part. Two instruments were chosen to assess the stress level of the patients at various point in time during therapy: (i) speech, used as an objective and ubiquitous stress indicator and (ii) the subjective unit of distress (SUD), a clinically validated Likert scale. In total, 13 statistical parameters were derived from each of five speech features: amplitude, zero-crossings, power, high-frequency power, and pitch. To model the emotional state of the patients, 28 parameters were selected from this set by means of a linear regression model and, subsequently, compressed into 11 principal components. The SUD and speech model were cross-validated, using 3 machine learning algorithms. Between 90% (2 SUD levels) and 39% (10 SUD levels) correct classification was achieved. The two sessions could be discriminated in 89% (for ST) and 77% (for RL) of the cases. This report fills a gap between laboratory and clinical studies, and its results emphasize the usefulness of Computer Aided Diagnostics (CAD) for mental health care.},
journal = {Personal Ubiquitous Comput.},
month = feb,
pages = {215–227},
numpages = {13},
keywords = {Computer aided diagnostics (CAD), Machine learning, Post-traumatic stress disorder (PTSD), Speech, Stress, Validity}
}

@article{10.1016/j.neucom.2019.08.002,
author = {Li, Zhenglai and Tang, Chang and Chen, Jiajia and Wan, Cheng and Yan, Weiqing and Liu, Xinwang},
title = {Diversity and consistency learning guided spectral embedding for multi-view clustering},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {370},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.08.002},
doi = {10.1016/j.neucom.2019.08.002},
journal = {Neurocomput.},
month = dec,
pages = {128–139},
numpages = {12},
keywords = {Multi-view clustering, Spectral embedding, Diversity and consistency learning, 00-01, 99-00}
}

@article{10.5555/3135535.3135553,
author = {Benba, Achraf and Jilbab, Abdelilah and Hammouch, Ahmed},
title = {Voice assessments for detecting patients with neurological diseases using PCA and NPCA},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1381-2416},
abstract = {In this study, we wanted to discriminate between 30 patients who suffer from Parkinson's disease (PD) and 20 patients with other neurological diseases (ND). All participants were asked to pronounce sustained vowel /a/ hold as long as possible at comfortable level. The analyses were done on these voice samples. Firstly, an initial feature vector extracted from time, frequency and cepstral domains. Then we used principal component analysis (PCA) and nonlinear PCA (NPCA). These techniques reduce the number of parameters and select the most effective ones to be used for classification. Support vector machine and k-nearest neighbor with different kernels was used for classification. We obtained accuracy up to 88% for discrimination between PD patients ND patients using KNN with k equal to three and five.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {673–683},
numpages = {11},
keywords = {Feature selection, KNN, NPCA, Neurological diseases, PCA, SVM}
}

@article{10.4018/IJCSSA.2015010101,
author = {Adi, Tom},
title = {A Framework of Cognition and Conceptual Structures Based on Deep Semantics},
year = {2015},
issue_date = {January 2015},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {1},
issn = {2166-7292},
url = {https://doi.org/10.4018/IJCSSA.2015010101},
doi = {10.4018/IJCSSA.2015010101},
abstract = {The argument is made that artificial intelligence AI systems which simulate cognition fail to reach significant goals because they rely on inadequate frameworks of cognition and conceptual structures that are based on shallow semantics. Shallow semantics examines only the surface meanings of expressions and words while deep semantics incorporates the meanings of the phonetic substructures of words. Shallow semantics is like chemistry that studies molecules such as sugar but never looks at elements carbon, hydrogen, and oxygen. This paper briefly reviews three decades of deep semantics research by the author which produced a new framework of cognition and a new kind of conceptual structures. These have been validated by large-scale smart text retrieval and by constructing a theory of emotions and a consumer choice theory. Deep semantics is used to develop an ontology of cognition itself which is then contrasted with the cognitive frameworks and conceptual structures of AI systems.},
journal = {Int. J. Concept. Struct. Smart Appl.},
month = jan,
pages = {1–19},
numpages = {19},
keywords = {Abstract, Cognition, Conceptual Structure, Concrete, Deep Semantics, Framework of Cognition, Ontology, Shallow Semantics}
}

