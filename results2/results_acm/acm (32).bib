@inproceedings{10.1007/978-3-030-90785-3_16,
author = {Wang, Baoping and Wang, Wennan and Zhu, Linkai and Liu, Wenjian},
title = {Research on Cross-Project Software Defect Prediction Based on Machine Learning},
year = {2021},
isbn = {978-3-030-90784-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90785-3_16},
doi = {10.1007/978-3-030-90785-3_16},
abstract = {In recent years, machine learning technology has developed vigorously. The research on software defect prediction in the field of software engineering is increasingly adopting various algorithms of machine learning. This article has carried out a systematic literature review on the field of defect prediction. First, this article studies the development process of defect prediction, from correlation to prediction model. then this article studies the development process of cross-project defect prediction based on machine learning algorithms (naive Bayes, decision tree, random forest, neural network, etc.). Finally, this paper looks forward to the research difficulties and future directions of software defect prediction, such as imbalance in classification, cost of data labeling, and cross-project data distribution.},
booktitle = {Advances in Web-Based Learning – ICWL 2021: 20th International Conference, ICWL 2021, Macau, China, November 13–14, 2021, Proceedings},
pages = {160–165},
numpages = {6},
keywords = {Machine learning, Software defect prediction model, Metric},
location = {Macau, China}
}

@inproceedings{10.1145/3416508.3417114,
author = {Aljamaan, Hamoud and Alazba, Amal},
title = {Software defect prediction using tree-based ensembles},
year = {2020},
isbn = {9781450381277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416508.3417114},
doi = {10.1145/3416508.3417114},
abstract = {Software defect prediction is an active research area in software engineering. Accurate prediction of software defects assists software engineers in guiding software quality assurance activities. In machine learning, ensemble learning has been proven to improve the prediction performance over individual machine learning models. Recently, many Tree-based ensembles have been proposed in the literature, and their prediction capabilities were not investigated in defect prediction. In this paper, we will empirically investigate the prediction performance of seven Tree-based ensembles in defect prediction. Two ensembles are classified as bagging ensembles: Random Forest and Extra Trees, while the other five ensembles are boosting ensembles: Ada boost, Gradient Boosting, Hist Gradient Boosting, XGBoost and CatBoost. The study utilized 11 publicly available MDP NASA software defect datasets. Empirical results indicate the superiority of Tree-based bagging ensembles: Random Forest and Extra Trees ensembles over other Tree-based boosting ensembles. However, none of the investigated Tree-based ensembles was significantly lower than individual decision trees in prediction performance. Finally, Adaboost ensemble was the worst performing ensemble among all Tree-based ensembles.},
booktitle = {Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {1–10},
numpages = {10},
keywords = {Bagging, Boosting, Classification, Ensemble Learning, Machine Learning, Prediction, Software Defect},
location = {Virtual, USA},
series = {PROMISE 2020}
}

@inproceedings{10.1145/3368926.3369711,
author = {Ha, Duy-An and Chen, Ting-Hsuan and Yuan, Shyan-Ming},
title = {Unsupervised methods for Software Defect Prediction},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369711},
doi = {10.1145/3368926.3369711},
abstract = {Software Defect Prediction (SDP) aims to assess software quality by using machine learning techniques. Recently, by proposing the connectivity-based unsupervised learning method, Zhang et al. have been proven that unsupervised classification has great potential to apply to this problem. Inspiring by this idea, in our work we try to replicate the results of Zhang et al.'s experiment and attempt to improve the performance by examining different techniques at each step of the approach using unsupervised learning methods to solve the SDP problem. Specifically, we try to follow the steps of the experiment described in their work strictly and examine three other clustering methods with four other ways for feature selection besides using all. To the best of our knowledge, these methods are first applied in SDP to evaluate their predictive power. For replicating the results, generally results in our experiments are not as good as the previous work. It may be due to we do not know which features are used in their experiment exactly. Fluid clustering and spectral clustering give better results than Newman clustering and CNM clustering in our experiments. Additionally, the experiments also show that using Kernel Principal Component Analysis (KPCA) or Non-Negative Matrix Factorization (NMF) for feature selection step gives better performance than using all features in the case of unlabeled data. Lastly, to make replicating our work easy, a lightweight framework is created and released on Github.},
booktitle = {Proceedings of the 10th International Symposium on Information and Communication Technology},
pages = {49–55},
numpages = {7},
keywords = {Community Structure Detection, Machine Learning, Software Defect Prediction, Software Engineering, Unsupervised Learning},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT '19}
}

@inproceedings{10.1145/3387940.3391463,
author = {Omri, Safa and Sinz, Carsten},
title = {Deep Learning for Software Defect Prediction: A Survey},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391463},
doi = {10.1145/3387940.3391463},
abstract = {Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (e.g. complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we discuss various approaches in fault prediction, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs' semantics and fault prediction features and make accurate predictions.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {209–214},
numpages = {6},
keywords = {deep learning, machine learning, software defect prediction, software quality assurance, software testing},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3474198.3478215,
author = {Du, Xiaozhi and Yue, Hehe and Dong, Honglei},
title = {Software Defect Prediction Method based on Hybrid Sampling},
year = {2022},
isbn = {9781450390149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474198.3478215},
doi = {10.1145/3474198.3478215},
abstract = {Software defect prediction is an essential technology to provide guidance and assistance for software testers and developers. However, the problem of imbalanced data sets limits the effect and application of the software defect prediction. To address this issue, this paper proposes a software defect prediction method based on hybrid sampling, which combines the strategies of over-sampling with under-sampling. For minority class, over-sampling uses k-means to cluster samples, then adopts SMOTE to generate artificial data based on safe areas of the clustering outcome. For majority class, under-sampling uses logistic regression classifier to get the misclassification probability of each sample and its instance hardness value. Then the samples, whose instance hardness values are lower than the threshold, are removed from the datasets. The experimental results show that our method is superior to the previous methods. Compared with SMOTE-kNN, SMOTE-Tomek, SMOTE and DBSMOTE, the accuracy of our method is improved by 17.60%, 6.99%, 8.66% and 26.18% on average respectively.},
booktitle = {International Conference on Frontiers of Electronics, Information and Computation Technologies},
articleno = {93},
numpages = {9},
keywords = {Data imbalance, Hybrid sampling, Software defect prediction},
location = {Changsha, China},
series = {ICFEICT 2021}
}

@article{10.1007/s00521-020-04960-1,
author = {Wang, Kechao and Liu, Lin and Yuan, Chengjun and Wang, Zhifei},
title = {Software defect prediction model based on LASSO–SVM},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {14},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-04960-1},
doi = {10.1007/s00521-020-04960-1},
abstract = {A software defect report is a bug in the software system that developers and users submit to the software defect library during software development and maintenance. Managing a software defect report that is overwhelming is a challenging task. The traditional method is manual identification, which is time-consuming and laborious and delays the repair of important software defects. Based on the above background, the purpose of this paper is to study the software defect prediction (SDP) model based on LASSO–SVM. In this paper, the problem of poor prediction accuracy of most SDP models is proposed. A SDP model combining minimum absolute value compression and selection method and support vector machine algorithm is proposed. Firstly, the feature selection ability of the minimum absolute value compression and selection method is used to reduce the dimension of the original data set, and the data set not related to SDP is removed. Then, the optimal value of SVM is obtained by using the parameter optimization ability of cross-validation algorithm. Finally, the SDP is completed by the nonlinear computing ability of SVM. The accuracy of simulation results is 93.25% and 66.67%, recall rate is 78.04%, and f-metric is 72.72%. The results show that the proposed defect prediction model has higher prediction accuracy than the traditional defect prediction model, and the prediction speed is faster.},
journal = {Neural Comput. Appl.},
month = jul,
pages = {8249–8259},
numpages = {11},
keywords = {Software defect prediction, Feature selection, Support vector machine, Cross-validation}
}

@article{10.1007/s11334-021-00399-2,
author = {Suresh Kumar, P. and Behera, H. S. and Nayak, Janmenjoy and Naik, Bighnaraj},
title = {Bootstrap aggregation ensemble learning-based reliable approach for software defect prediction by using characterized code feature},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-021-00399-2},
doi = {10.1007/s11334-021-00399-2},
abstract = {To ensure software quality, software defect prediction plays a prominent role for the software developers and practitioners. Software defect prediction can assist us with distinguishing software defect modules and enhance the software quality. In present days, many supervised machine learning algorithms have proved their efficacy to identify defective modules. However, those are limited to prove their major significance due to the limitations such as the adaptation of parameters with the environment and complexity. So, it is important to develop a key methodology to improve the efficiency of the prediction module. In this paper, an ensemble learning technique called&nbsp;Bootstrap&nbsp;aggregating has been proposed for software defect prediction object-oriented modules. The proposed method's accuracy, recall, precision, F-measure, and AUC-ROC efficiency were compared to those of many qualified machine learning algorithms. Simulation results and performance comparison are evident that the proposed method outperformed well compared to other approaches.},
journal = {Innov. Syst. Softw. Eng.},
month = dec,
pages = {355–379},
numpages = {25},
keywords = {Ensemble learning, Software defect prediction, Software reliability, Machine learning}
}

@article{10.1016/j.infsof.2021.106664,
author = {Yao, Jingxiu and Shepperd, Martin},
title = {The impact of using biased performance metrics on software defect prediction research},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106664},
doi = {10.1016/j.infsof.2021.106664},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {14},
keywords = {Software engineering, Machine learning, Software defect prediction, Computational experiment, Classification metrics}
}

@article{10.1016/j.neucom.2019.11.067,
author = {Qiao, Lei and Li, Xuesong and Umer, Qasim and Guo, Ping},
title = {Deep learning based software defect prediction},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {385},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.067},
doi = {10.1016/j.neucom.2019.11.067},
journal = {Neurocomput.},
month = apr,
pages = {100–110},
numpages = {11},
keywords = {Software defect prediction, Deep learning, Software quality, Software metrics, Robustness evaluation}
}

@article{10.1007/s11063-020-10355-z,
author = {Niu, Liang and Wan, Jianwu and Wang, Hongyuan and Zhou, Kaiwei},
title = {Cost-sensitive Dictionary Learning for Software Defect Prediction},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10355-z},
doi = {10.1007/s11063-020-10355-z},
abstract = {In recent years, software defect prediction has been recognized as a cost-sensitive learning problem. To deal with the unequal misclassification losses resulted by different classification errors, some cost-sensitive dictionary learning methods have been proposed recently. Generally speaking, these methods usually define the misclassification costs to measure the unequal losses and then propose to minimize the cost-sensitive reconstruction loss by embedding the cost information into the reconstruction function of dictionary learning. Although promising performance has been achieved, their cost-sensitive reconstruction functions are not well-designed. In addition, no sufficient attentions are paid to the coding coefficients which can also be helpful to reduce the reconstruction loss. To address these issues, this paper proposes a new cost-sensitive reconstruction loss function and introduces an additional cost-sensitive discrimination regularization for the coding coefficients. Both the two terms are jointly optimized in a unified cost-sensitive dictionary learning framework. By doing so, we can achieve the minimum reconstruction loss and thus obtain a more cost-sensitive dictionary for feature encoding of test data. In the experimental part, we have conducted extensive experiments on twenty-five software projects from four benchmark datasets of NASA, AEEEM, ReLink and Jureczko. The results, in comparison with ten state-of-the-art software defect prediction methods, demonstrate the effectiveness of learned cost-sensitive dictionary for software defect prediction.},
journal = {Neural Process. Lett.},
month = dec,
pages = {2415–2449},
numpages = {35},
keywords = {Software defect prediction, Cost-sensitive, Dictionary learning, Discrimination}
}

@article{10.1049/iet-sen.2019.0149,
author = {Deng, Jiehan and Lu, Lu and Qiu, Shaojian},
title = {Software defect prediction via LSTM},
year = {2020},
issue_date = {August 2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {14},
number = {4},
url = {https://doi.org/10.1049/iet-sen.2019.0149},
doi = {10.1049/iet-sen.2019.0149},
abstract = {Software quality plays an important role in the software lifecycle. Traditional software defect prediction approaches mainly focused on using hand‐crafted features to detect defects. However, like human languages, programming languages contain rich semantic and structural information, and the cause of defective code is closely related to its context. Failing to catch this significant information, the performance of traditional approaches is far from satisfactory. In this study, the authors leveraged a long short‐term memory (LSTM) network to automatically learn the semantic and contextual features from the source code. Specifically, they first extract the program's Abstract Syntax Trees (ASTs), which is made up of AST nodes, and then evaluate what and how much information they can preserve for several node types. They traverse the AST of each file and fed them into the LSTM network to automatically the semantic and contextual features of the program, which is then used to determine whether the file is defective. Experimental results on several opensource projects showed that the proposed LSTM method is superior to the state‐of‐the‐art methods.},
journal = {IET Software},
month = aug,
pages = {443–450},
numpages = {8},
keywords = {feature extraction, learning (artificial intelligence), public domain software, program diagnostics, program debugging, software quality, recurrent neural nets, trees (mathematics), program abstract syntax trees, AST node sequence, semantic features, contextual features, LSTM, software quality, software lifecycle, software defect prediction approaches, machine learning techniques, programming languages, human languages, structural information, defective code, long short-term memory network, open source projects, numerical vectors, word embedding techniques}
}

@article{10.1007/s11219-016-9353-3,
author = {Bowes, David and Hall, Tracy and Petri\'{c}, Jean},
title = {Software defect prediction: do different classifiers find the same defects?},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9353-3},
doi = {10.1007/s11219-016-9353-3},
abstract = {During the last 10 years, hundreds of different defect prediction models have been published. The performance of the classifiers used in these models is reported to be similar with models rarely performing above the predictive performance ceiling of about 80% recall. We investigate the individual defects that four classifiers predict and analyse the level of prediction uncertainty produced by these classifiers. We perform a sensitivity analysis to compare the performance of Random Forest, Na\"{\i}ve Bayes, RPart and SVM classifiers when predicting defects in NASA, open source and commercial datasets. The defect predictions that each classifier makes is captured in a confusion matrix and the prediction uncertainty of each classifier is compared. Despite similar predictive performance values for these four classifiers, each detects different sets of defects. Some classifiers are more consistent in predicting defects than others. Our results confirm that a unique subset of defects can be detected by specific classifiers. However, while some classifiers are consistent in the predictions they make, other classifiers vary in their predictions. Given our results, we conclude that classifier ensembles with decision-making strategies not based on majority voting are likely to perform best in defect prediction.},
journal = {Software Quality Journal},
month = jun,
pages = {525–552},
numpages = {28},
keywords = {Machine learning, Prediction modelling, Software defect prediction}
}

@article{10.1002/smr.2362,
author = {Guo, Shikai and Dong, Jian and Li, Hui and Wang, Jiahui},
title = {Software defect prediction with imbalanced distribution by radius‐synthetic minority over‐sampling technique},
year = {2021},
issue_date = {July 2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {33},
number = {7},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2362},
doi = {10.1002/smr.2362},
abstract = {Software defect prediction, which can identify the defect‐prone modules, is an effective technology to ensure the quality of software products. Due to the importance in software maintenance, many learning‐based software defect prediction models are presented in recent years. Actually, the defects usually occupy a very small proportions in software source codes; thus, the imbalanced distributions between defect‐prone modules and non‐defect‐prone modules increase the learning difficulty of the classification task. To address this issue, we present a random over‐sampling mechanism used to generate minority‐class samples from high‐dimensional sampling space to deal with the imbalanced distributions in software defect prediction, in which two constraints are applied to provide a robust way to generate new synthetic samples, that is, scaling the random over‐sampling scope to a reasonable area and distinguishing the majority‐class samples in a critical region. Based on nine open datasets of software projects, we experimentally verify that our presented method is effective on predict the defect‐prone modules, and the effect is superior to the traditional imbalanced processing methods.},
journal = {J. Softw. Evol. Process},
month = jul,
numpages = {21},
keywords = {imbalanced learning, software defect prediction, software quality}
}

@inproceedings{10.1007/978-3-030-58817-5_45,
author = {Balogun, Abdullateef O. and Lafenwa-Balogun, Fatimah B. and Mojeed, Hammed A. and Adeyemo, Victor E. and Akande, Oluwatobi N. and Akintola, Abimbola G. and Bajeh, Amos O. and Usman-Hamza, Fatimah E.},
title = {SMOTE-Based Homogeneous Ensemble Methods for Software Defect Prediction},
year = {2020},
isbn = {978-3-030-58816-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58817-5_45},
doi = {10.1007/978-3-030-58817-5_45},
abstract = {Class imbalance is a prevalent problem in machine learning which affects the prediction performance of classification algorithms. Software Defect Prediction (SDP) is no exception to this latent problem. Solutions such as data sampling and ensemble methods have been proposed to address the class imbalance problem in SDP. This study proposes a combination of Synthetic Minority Oversampling Technique (SMOTE) and homogeneous ensemble (Bagging and Boosting) methods for predicting software defects. The proposed approach was implemented using Decision Tree (DT) and Bayesian Network (BN) as base classifiers on defects datasets acquired from NASA software corpus. The experimental results showed that the proposed approach outperformed other experimental methods. High accuracy of 86.8% and area under operating receiver characteristics curve value of 0.93% achieved by the proposed technique affirmed its ability to differentiate between the defective and non-defective labels without bias.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part VI},
pages = {615–631},
numpages = {17},
keywords = {Software Defect Prediction, Class imbalance, Data sampling, Ensemble methods},
location = {Cagliari, Italy}
}

@article{10.1016/j.jss.2021.111038,
author = {Eken, Beyza and Tosun, Ayse},
title = {Investigating the performance of personalized models for software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {181},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111038},
doi = {10.1016/j.jss.2021.111038},
journal = {J. Syst. Softw.},
month = nov,
numpages = {17},
keywords = {Personalized, Change-level, Defect prediction, Software recommendation systems}
}

@article{10.4018/IJDSST.2020070105,
author = {Kakkar, Misha and Jain, Sarika and Bansal, Abhay and Grover, P. S.},
title = {Nonlinear Geometric Framework for Software Defect Prediction},
year = {2020},
issue_date = {Jul 2020},
publisher = {IGI Global},
address = {USA},
volume = {12},
number = {3},
issn = {1941-6296},
url = {https://doi.org/10.4018/IJDSST.2020070105},
doi = {10.4018/IJDSST.2020070105},
abstract = {Humans use the software in every walk of life thus it is essential to have the best quality software. Software defect prediction models assist in identifying defect prone modules with the help of historical data, which in turn improves software quality. Historical data consists of data related to modules /files/classes which are labeled as buggy or clean. As the number of buggy artifacts as less as compared to clean artifacts, the nature of historical data becomes imbalance. Due to this uneven distribution of the data, it difficult for classification algorithms to build highly effective SDP models. The objective of this study is to propose a new nonlinear geometric framework based on SMOTE and ensemble learning to improve the performance of SDP models. The study combines the traditional SMOTE algorithm and the novel ensemble Support Vector Machine (SVM) is used to develop the proposed framework called SMEnsemble. SMOTE algorithm handles the class imbalance problem by generating synthetic instances of the minority class. Ensemble learning generates multiple classification models to select the best performing SDP model. For experimentation, datasets from three different software repositories that contain both open source as well as proprietary projects are used in the study. The results show that SMEnsemble performs better than traditional methods for identifying the minority class i.e. buggy artifacts. Also, the proposed model performance is better than the latest state of Art SDP model- SMOTUNED. The proposed model is capable of handling imbalance classes when compared with traditional methods. Also, by carefully selecting the number of ensembles high performance can be achieved in less time.},
journal = {Int. J. Decis Support Syst. Technol.},
month = jul,
pages = {85–100},
numpages = {16},
keywords = {Classification, Data Analytics For Software Engineering, Ensemble Learning, Imbalanced Data, Preprocessing, SMOTE, Software Defect Prediction}
}

@inproceedings{10.1007/978-3-030-37352-8_13,
author = {Cui, Mengtian and Huang, Yameng and Luo, Jing},
title = {Software Defect Prediction Model Based on GA-BP Algorithm},
year = {2019},
isbn = {978-3-030-37351-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-37352-8_13},
doi = {10.1007/978-3-030-37352-8_13},
abstract = {The novel software defect prediction model based on GA-BP algorithm was proposed in the paper considering the disadvantage of traditional BP (abbreviated for Back Propagation) neural network, which has the problem of easy to fall into local optimization when constructing software defect prediction model, and finally affects the prediction accuracy. Firstly, the optimization ability of GA (abbreviated for Genetic Algorithms) is introduced to optimize the weights and thresholds of Back Propagation neural network. Then the prediction model was constructed based on the GA-BP. Meanwhile the public dataset MDP from NASA was selected and the tool WEKA was used to clean the data and format conversion and as the result, four datasets is available. In the end, experimental results show that the proposed method in the paper is effective for software defect prediction.},
booktitle = {Cyberspace Safety and Security: 11th International Symposium, CSS 2019, Guangzhou, China, December 1–3, 2019, Proceedings, Part II},
pages = {151–161},
numpages = {11},
keywords = {Software defect prediction, Machine learning, Genetic Algorithms, BP neural network},
location = {Guangzhou, China}
}

@inproceedings{10.1145/3352411.3352412,
author = {Li, Ran and Zhou, Lijuan and Zhang, Shudong and Liu, Hui and Huang, Xiangyang and Sun, Zhong},
title = {Software Defect Prediction Based on Ensemble Learning},
year = {2019},
isbn = {9781450371414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352411.3352412},
doi = {10.1145/3352411.3352412},
abstract = {Software defect prediction is one of the important ways to guarantee the quality of software systems. Combining various algorithms in machine learning to predict software defects has become a hot topic in the current study. The paper uses the datasets of MDP as the experimental research objects and takes ensemble learning as research focus to construct software defect prediction model. With experimenting five different types of ensemble algorithms and analyzing the features and procedures, this paper discusses the best ensemble algorithm which is Random Forest through experimental comparison. Then we utilize the SMOTE over-sampling and Resample methods to improve the quality of datasets to build a complete new software defect prediction model. Therefore, the results show that the model can improve defect classification performance effectively.},
booktitle = {Proceedings of the 2019 2nd International Conference on Data Science and Information Technology},
pages = {1–6},
numpages = {6},
keywords = {Ensemble algorithm, Over-sampling, Software defect prediction, Under-sampling},
location = {Seoul, Republic of Korea},
series = {DSIT 2019}
}

@article{10.1007/s00500-020-05159-1,
author = {Jin, Cong},
title = {Software defect prediction model based on distance metric learning},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {1},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05159-1},
doi = {10.1007/s00500-020-05159-1},
abstract = {Software defect prediction (SDP) is a very important way for analyzing software quality and reducing development costs. The data during software lifecycle can be used to predict software defect. Currently, many SDP models have been proposed; however, their performance was not always ideal. In many existing prediction models based on machine learning, the distance metric between samples has significant impact on the performance of the SDP model. In addition, most samples are usually class imbalanced. To solve these issues, in this paper, a novel distance metric learning based on cost-sensitive learning (CSL) is proposed for reducing the impact of class imbalance of samples, which is then applied to the large margin distribution machine (LDM) to substitute the traditional kernel function. Further, the improvement and optimization of LDM based on CSL are also studied, and the improved LDM is used as the SDP model, called as CS-ILDM. Subsequently, the proposed CS-ILDM is applied to five publicly available data sets from the NASA Metrics Data Program repository and its performance is compared to other existing SDP models. The experimental results confirm that the proposed CS-ILDM not only has good prediction performance, but also can reduce the misprediction cost and avoid the impact of class imbalance of samples.},
journal = {Soft Comput.},
month = jan,
pages = {447–461},
numpages = {15},
keywords = {Software defect prediction, Software attributes, Distance metric learning, Cost-sensitive learning, Misprediction cost, Class imbalance of samples}
}

@article{10.1016/j.eswa.2021.114637,
author = {Jin, Cong},
title = {Cross-project software defect prediction based on domain adaptation learning and optimization},
year = {2021},
issue_date = {Jun 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {171},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114637},
doi = {10.1016/j.eswa.2021.114637},
journal = {Expert Syst. Appl.},
month = jun,
numpages = {14},
keywords = {Software defect prediction, Optimization, Domain adaptation, Cross-project defect prediction, Improved quantum particle swarm optimization}
}

@article{10.1016/j.jss.2018.06.025,
author = {\"{O}zak\i{}nc\i{}, Rana and Tarhan, Ay\c{c}a},
title = {Early software defect prediction: A systematic map and review},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.06.025},
doi = {10.1016/j.jss.2018.06.025},
journal = {J. Syst. Softw.},
month = oct,
pages = {216–239},
numpages = {24},
keywords = {Early defect prediction, Software defect, Software quality, Prediction model, Systematic mapping, Systematic literature review}
}

@inproceedings{10.1007/978-3-030-86472-9_28,
author = {Shakhovska, Natalya and Yakovyna, Vitaliy},
title = {Feature Selection and Software Defect Prediction by Different Ensemble Classifiers},
year = {2021},
isbn = {978-3-030-86471-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86472-9_28},
doi = {10.1007/978-3-030-86472-9_28},
abstract = {Software defect prediction can improve its quality and is actively studied during the last decade. This paper focuses on the improvement of software defect prediction accuracy by proper feature selection techniques and using ensemble classifier. The software code metrics were used to predict the defective modules. JM1 public NASA dataset from PROMISE Software Engineering Repository was used in this study. Boruta, ACE, regsubsets and simple correlation are used for feature selection. The results of selection are formed based on hard voting of all features selectors. A new stacking classifier for software defects prediction is presented in this paper. The stacking classifier for defects prediction algorithm is based on combination of 5 weak classifiers. Random forest algorithm is used to combine the predictions. The obtained prediction accuracy was up to 96.26%.},
booktitle = {Database and Expert Systems Applications: 32nd International Conference, DEXA 2021, Virtual Event, September 27–30, 2021, Proceedings, Part I},
pages = {307–313},
numpages = {7},
keywords = {Ensemble of classifiers, Feature selection, Software defect analysis}
}

@inproceedings{10.1007/978-3-030-62463-7_33,
author = {Lei, Tianwei and Xue, Jingfeng and Han, Weijie},
title = {Cross-Project Software Defect Prediction Based on Feature Selection and Transfer Learning},
year = {2020},
isbn = {978-3-030-62462-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-62463-7_33},
doi = {10.1007/978-3-030-62463-7_33},
abstract = {Cross-project software defect prediction solves the problem that traditional defect prediction can’t get enough data, but how to apply the model learned from the data of different mechanisms to the target data set is a new problem. At the same time, there is the problem that information redundancy in the training process leads to low accuracy. Based on the difference of projects, this paper uses MIC to filter features to solve the problem of information redundancy. At the same time, combined with the TrAdaboost algorithm, which is based on the idea of aggravating multiple classification error samples, this paper proposes a cross-project software prediction method based on feature selection and migration learning. Experimental results show that the algorithm proposed in this paper has better experimental results on AUC and F1.},
booktitle = {Machine Learning for Cyber Security: Third International Conference, ML4CS 2020, Guangzhou, China, October 8–10, 2020, Proceedings, Part III},
pages = {363–371},
numpages = {9},
keywords = {Transfer learning, TrAdaboost, MIC, Cross-project software defect prediction},
location = {Guangzhou, China}
}

@article{10.1016/j.neucom.2019.05.100,
author = {Huo, Xuan and Li, Ming},
title = {On cost-effective software defect prediction: Classification or ranking?},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {363},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.05.100},
doi = {10.1016/j.neucom.2019.05.100},
journal = {Neurocomput.},
month = oct,
pages = {339–350},
numpages = {12},
keywords = {Software mining, Software defect prediction, Ranking model, Classification model}
}

@inproceedings{10.1007/978-3-030-58802-1_25,
author = {Ronchieri, Elisabetta and Canaparo, Marco and Belgiovine, Mauro},
title = {Software Defect Prediction on Unlabelled Datasets: A Comparative Study},
year = {2020},
isbn = {978-3-030-58801-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58802-1_25},
doi = {10.1007/978-3-030-58802-1_25},
abstract = {Background: Defect prediction on unlabelled datasets is a challenging and widespread problem in software engineering. Machine learning is of great value in this context because it provides techniques - called unsupervised - that are applicable to unlabelled datasets. Objective: This study aims at comparing various approaches employed over the years on unlabelled datasets to predict the defective modules, i.e. the ones which need more attention in the testing phase. Our comparison is based on the measurement of performance metrics and on the real defective information derived from software archives. Our work leverages a new dataset that has been obtained by extracting and preprocessing its metrics from a C++ software. Method: Our empirical study has taken advantage of CLAMI with its improvement CLAMI+ that we have applied on high energy physics software datasets. Furthermore, we have used clustering techniques such as the K-means algorithm to find potentially critical modules. Results: Our experimental analysis have been carried out on 1 open source project with 34 software releases. We have applied 17 ML techniques to the labelled datasets obtained by following the CLAMI and CLAMI+ approaches. The two approaches have been evaluated by using different performance metrics, our results show that CLAMI+ performs better than CLAMI. The predictive average accuracy metric is around 95% for 4 ML techniques (4 out of 17) that show a Kappa statistic greater than 0.80. We applied K-means on the same dataset and obtained 2 clusters labelled according to the output of CLAMI and CLAMI+. Conclusion: Based on the results of the different statistical tests, we conclude that no significant performance differences have been found in the selected classification techniques.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part II},
pages = {333–353},
numpages = {21},
keywords = {Machine learning, Unsupervised methods, Defect prediction, Unlabelled dataset},
location = {Cagliari, Italy}
}

@phdthesis{10.5555/AAI28389525,
author = {Rahman, Ashiqur},
advisor = {R, Cordy, James},
title = {Software Defect Prediction Using Rich Contextualized Language Use Vectors},
year = {2020},
isbn = {9798708779250},
publisher = {Queen's University (Canada)},
abstract = {Context. Software defect prediction aims to find defect prone source code, and thus reduce the effort, time and cost involved with ensuring the quality of software systems. Both code and non-code metrics are commonly used in this process to train machine learning algorithms to predict software defects. Studies have shown that such metrics-based approaches are failing to give satisfactory results, and have reached a performance ceiling. This thesis explores the idea of using code profiles as an alternative to traditional metrics to predict software defects. This code profile-based method proves to be more promising than traditional metrics-based approaches.Aims. This thesis aims to improve software defect prediction using code profiles as feature variables in place of traditional metrics. Software code profiles encode the density of language feature use and the context of such use in Rich Contextualized Language Use Vectors (RCLUVs) by analysing the parse tree of the source code. This thesis explores whether code profiles can be used to train machine learning algorithms, and compares the performance of the derived models to traditional metrics-based approaches.Methods. To achieve these aims the learning curves of several machine learning algorithms are analyzed, and the performance of the derived models are evaluated against traditional metrics-based approaches. Two benchmark bug datasets, the Eclipse bug dataset and the Github bug database, are used to train the models.Results. The learning curves of the models show machine learning algorithms can learn from RCLUV-based code profiles. Performance evaluation against existing metrics-based approaches reveals that the code profile-based approach is more promising than traditional metrics-based approaches. However, the predictive performance of both metrics and code profile-based approaches drops in cross-version predictions.Conclusions. Unlike traditional metrics-based approaches, this thesis uses vectors generated by analyzing language feature use from the parse trees of source code as feature variables to train machine learning algorithms. Experimental results using learning algorithms encourages us to use software code profiles as an alternative to traditional metrics to predict software defects.},
note = {AAI28389525}
}

@article{10.1007/s10515-016-0194-x,
author = {Zhang, Zhi-Wu and Jing, Xiao-Yuan and Wang, Tie-Jian},
title = {Label propagation based semi-supervised learning for software defect prediction},
year = {2017},
issue_date = {March     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-016-0194-x},
doi = {10.1007/s10515-016-0194-x},
abstract = {Software defect prediction can automatically predict defect-prone software modules for efficient software test in software engineering. When the previous defect labels of modules are limited, predicting the defect-prone modules becomes a challenging problem. In static software defect prediction, there exist the similarity among software modules, a software module can be approximated by a sparse representation of the other part of the software modules, and class-imbalance problem, the number of defect-free modules is much larger than that of defective ones. In this paper, we propose to use graph based semi-supervised learning technique to predict software defect. By using Laplacian score sampling strategy for the labeled defect-free modules, we construct a class-balance labeled training dataset firstly. And then, we use a nonnegative sparse algorithm to compute the nonnegative sparse weights of a relationship graph which serve as clustering indicators. Lastly, on the nonnegative sparse graph, we use a label propagation algorithm to iteratively predict the labels of unlabeled software modules. We thus propose a nonnegative sparse graph based label propagation approach for software defect classification and prediction, which uses not only few labeled data but also abundant unlabeled ones to improve the generalization capability. We vary the size of labeled software modules from 10 to 30 % of all the datasets in the widely used NASA projects. Experimental results show that the NSGLP outperforms several representative state-of-the-art semi-supervised software defect prediction methods, and it can fully exploit the characteristics of static code metrics and improve the generalization capability of the software defect prediction model.},
journal = {Automated Software Engg.},
month = mar,
pages = {47–69},
numpages = {23},
keywords = {Software defect prediction, Semi-supervised learning, Nonnegative sparse graph based label propagation (NSGLP), Nonnegative sparse graph, Label propagation}
}

@article{10.1155/2021/2323100,
author = {Liu, Wenjian and Wang, Baoping and Wang, Wennan and Ni, Tongguang},
title = {Deep Learning Software Defect Prediction Methods for Cloud Environments Research},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/2323100},
doi = {10.1155/2021/2323100},
abstract = {This paper provides an in-depth study and analysis of software defect prediction methods in a cloud environment and uses a deep learning approach to justify software prediction. A cost penalty term is added to the supervised part of the deep ladder network; that is, the misclassification cost of different classes is added to the model. A cost-sensitive deep ladder network-based software defect prediction model is proposed, which effectively mitigates the negative impact of the class imbalance problem on defect prediction. To address the problem of lack or insufficiency of historical data from the same project, a flow learning-based geodesic cross-project software defect prediction method is proposed. Drawing on data information from other projects, a migration learning approach was used to embed the source and target datasets into a Gaussian manifold. The kernel encapsulates the incremental changes between the differences and commonalities between the two domains. To this point, the subspace is the space of two distributional approximations formed by the source and target data transformations, with traditional in-project software defect classifiers used to predict labels. It is found that real-time defect prediction is more practical because it has a smaller amount of code to review; only individual changes need to be reviewed rather than entire files or packages while making it easier for developers to assign fixes to defects. More importantly, this paper combines deep belief network techniques with real-time defect prediction at a fine-grained level and TCA techniques to deal with data imbalance and proposes an improved deep belief network approach for real-time defect prediction, while trying to change the machine learning classifier underlying DBN for different experimental studies, and the results not only validate the effectiveness of using TCA techniques to solve the data imbalance problem but also show that the defect prediction model learned by the improved method in this paper has better prediction performance.},
journal = {Sci. Program.},
month = jan,
numpages = {11}
}

@article{10.1016/j.infsof.2021.106662,
author = {Feng, Shuo and Keung, Jacky and Yu, Xiao and Xiao, Yan and Zhang, Miao},
title = {Investigation on the stability of SMOTE-based oversampling techniques in software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106662},
doi = {10.1016/j.infsof.2021.106662},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {14},
keywords = {Empirical Software Engineering, SMOTE, Oversampling, Class imbalance, Software defect prediction}
}

@article{10.1016/j.jss.2021.111026,
author = {Zhu, Kun and Ying, Shi and Zhang, Nana and Zhu, Dandan},
title = {Software defect prediction based on enhanced metaheuristic feature selection optimization and a hybrid deep neural network},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111026},
doi = {10.1016/j.jss.2021.111026},
journal = {J. Syst. Softw.},
month = oct,
numpages = {25},
keywords = {Kernel extreme learning machine, Convolutional neural network, Whale optimization algorithm, Metaheuristic feature selection, Software defect prediction}
}

@article{10.1007/s11219-016-9342-6,
author = {Chen, Lin and Fang, Bin and Shang, Zhaowei and Tang, Yuanyan},
title = {Tackling class overlap and imbalance problems in software defect prediction},
year = {2018},
issue_date = {March     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9342-6},
doi = {10.1007/s11219-016-9342-6},
abstract = {Software defect prediction (SDP) is a promising solution to save time and cost in the software testing phase for improving software quality. Numerous machine learning approaches have proven effective in SDP. However, the unbalanced class distribution in SDP datasets could be a problem for some conventional learning methods. In addition, class overlap increases the difficulty for the predictors to learn the defective class accurately. In this study, we propose a new SDP model which combines class overlap reduction and ensemble imbalance learning to improve defect prediction. First, the neighbor cleaning method is applied to remove the overlapping non-defective samples. The whole dataset is then randomly under-sampled several times to generate balanced subsets so that multiple classifiers can be trained on these data. Finally, these individual classifiers are assembled with the AdaBoost mechanism to build the final prediction model. In the experiments, we investigated nine highly unbalanced datasets selected from a public software repository and confirmed that the high rate of overlap between classes existed in SDP data. We assessed the performance of our proposed model by comparing it with other state-of-the-art methods including conventional SDP models, imbalance learning and data cleaning methods. Test results and statistical analysis show that the proposed model provides more reasonable defect prediction results and performs best in terms of G-mean and AUC among all tested models.},
journal = {Software Quality Journal},
month = mar,
pages = {97–125},
numpages = {29},
keywords = {Software defect prediction, Machine learning, Class overlap, Class imbalance}
}

@article{10.1007/s10515-021-00289-8,
author = {Ali, Aftab and Khan, Naveed and Abu-Tair, Mamun and Noppen, Joost and McClean, Sally and McChesney, Ian},
title = {Discriminating features-based cost-sensitive approach for software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-021-00289-8},
doi = {10.1007/s10515-021-00289-8},
abstract = {Correlated quality metrics extracted from a source code repository can be utilized to design a model to automatically predict defects in a software system. It is obvious that the extracted metrics will result in a highly unbalanced data, since the number of defects in a good quality software system should be far less than the number of normal instances. It is also a fact that the selection of the best discriminating features significantly improves the robustness and accuracy of a prediction model. Therefore, the contribution of this paper is twofold, first it selects the best discriminating features that help in accurately predicting a defect in a software component. Secondly, a cost-sensitive logistic regression and decision tree ensemble-based prediction models are applied to the best discriminating features for precisely predicting a defect in a software component. The proposed models are compared with the most recent schemes in the literature in terms of accuracy, area under the curve, and recall. The models are evaluated using 11 datasets and it is evident from the results and analysis that the performance of the proposed prediction models outperforms the schemes in the literature.},
journal = {Automated Software Engg.},
month = nov,
numpages = {18},
keywords = {Recall, AUC, Cost-sensitivity, Discriminating features, Machine learning models, Software bugs/defects}
}

@article{10.1007/s11277-017-5117-z,
author = {Zhou, Lijuan and Li, Ran and Zhang, Shudong and Wang, Hua},
title = {Imbalanced Data Processing Model for Software Defect Prediction},
year = {2018},
issue_date = {Sep 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {102},
number = {2},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-017-5117-z},
doi = {10.1007/s11277-017-5117-z},
abstract = {In the field of software engineering, software defect prediction is the hotspot of the researches which can effectively guarantee the quality during software development. However, the problem of class imbalanced datasets will affect the accuracy of overall classification of software defect prediction, which is the key issue to be solved urgently today. In order to better solve this problem, this paper proposes a model named ASRA which combines attribute selection, sampling technologies and ensemble algorithm. The model adopts the Chi square test of attribute selection and then utilizes the combined sampling technique which includes SMOTE over-sampling and under-sampling to remove the redundant attributes and make the datasets balance. Afterwards, the model ASRA is eventually established by ensemble algorithm named Adaboost with basic classifier J48 decision tree. The data used in the experiments comes from UCI datasets. It can draw the conclusion that the effect of software defect prediction classification which using this model is improved and better than before by comparing the precision P, F-measure and AUC values from the results of the experiments.},
journal = {Wirel. Pers. Commun.},
month = sep,
pages = {937–950},
numpages = {14},
keywords = {Software defect prediction, Sampling, Ensemble algorithm, Class imbalance, Attribute selection}
}

@article{10.1016/j.neucom.2021.05.043,
author = {Harzevili, Nima Shiri and Alizadeh, Sasan H.},
title = {Analysis and modeling conditional mutual dependency of metrics in software defect prediction using latent variables},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {460},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.05.043},
doi = {10.1016/j.neucom.2021.05.043},
journal = {Neurocomput.},
month = oct,
pages = {309–330},
numpages = {22},
keywords = {99–00, 00–01, Latent variable, Naive Bayes classifier, Software defect prediction Software metrics}
}

@article{10.1007/s10664-020-09861-4,
author = {Morasca, Sandro and Lavazza, Luigi},
title = {On the assessment of software defect prediction models via ROC curves},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09861-4},
doi = {10.1007/s10664-020-09861-4},
abstract = {Software defect prediction models are classifiers often built by setting a threshold t on a defect proneness model, i.e., a scoring function. For instance, they classify a software module non-faulty if its defect proneness is below t and positive otherwise. Different values of t may lead to different defect prediction models, possibly with very different performance levels. Receiver Operating Characteristic (ROC) curves provide an overall assessment of a defect proneness model, by taking into account all possible values of t and thus all defect prediction models that can be built based on it. However, using a defect proneness model with a value of t is sensible only if the resulting defect prediction model has a performance that is at least as good as some minimal performance level that depends on practitioners’ and researchers’ goals and needs. We introduce a new approach and a new performance metric (the Ratio of Relevant Areas) for assessing a defect proneness model by taking into account only the parts of a ROC curve corresponding to values of t for which defect proneness models have higher performance than some reference value. We provide the practical motivations and theoretical underpinnings for our approach, by: 1) showing how it addresses the shortcomings of existing performance metrics like the Area Under the Curve and Gini’s coefficient; 2) deriving reference values based on random defect prediction policies, in addition to deterministic ones; 3) showing how the approach works with several performance metrics (e.g., Precision and Recall) and their combinations; 4) studying misclassification costs and providing a general upper bound for the cost related to the use of any defect proneness model; 5) showing the relationships between misclassification costs and performance metrics. We also carried out a comprehensive empirical study on real-life data from the SEACRAFT repository, to show the differences between our metric and the existing ones and how more reliable and less misleading our metric can be.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3977–4019},
numpages = {43},
keywords = {Gini, AUC, Thresholds, ROC, Software defect proneness, Software defect prediction model}
}

@inproceedings{10.1109/ASE.2019.00071,
author = {Gong, Lina and Jiang, Shujuan and Wang, Rongcun and Jiang, Li},
title = {Empirical evaluation of the impact of class overlap on software defect prediction},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00071},
doi = {10.1109/ASE.2019.00071},
abstract = {Software defect prediction (SDP) utilizes the learning models to detect the defective modules in project, and their performance depends on the quality of training data. The previous researches mainly focus on the quality problems of class imbalance and feature redundancy. However, training data often contains some instances that belong to different class but have similar values on features, and this leads to class overlap to affect the quality of training data. Our goal is to investigate the impact of class overlap on software defect prediction. At the same time, we propose an improved K-Means clustering cleaning approach (IKMCCA) to solve both the class overlap and class imbalance problems. Specifically, we check whether K-Means clustering cleaning approach (KMCCA) or neighborhood cleaning learning (NCL) or IKMCCA is feasible to improve defect detection performance for two cases (i) within-project defect prediction (WPDP) (ii) cross-project defect prediction (CPDP). To have an objective estimate of class overlap, we carry out our investigations on 28 open source projects, and compare the performance of state-of-the-art learning models for the above-mentioned cases by using IKMCCA or KMCCA or NCL VS. without cleaning data. The experimental results make clear that learning models obtain significantly better performance in terms of balance, Recall and AUC for both WPDP and CPDP when the overlapping instances are removed. Moreover, it is better to consider both class overlap and class imbalance.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {698–709},
numpages = {12},
keywords = {software defect prediction, machine learning, class overlap, K-Means clustering},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1007/978-3-031-08421-8_41,
author = {Giorgio, Lazzarinetti and Nicola, Massarenti and Fabio, Sgr\`{o} and Andrea, Salafia},
title = {Continuous Defect Prediction in CI/CD Pipelines: A Machine Learning-Based Framework},
year = {2021},
isbn = {978-3-031-08420-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08421-8_41},
doi = {10.1007/978-3-031-08421-8_41},
abstract = {Recent advances in information technology has led to an increasing number of applications to be developed and maintained daily by product teams. Ensuring that a software application works as expected and that it is absent of bugs requires a lot of time and resources. Thanks to the recent adoption of DevOps methodologies, it is often the case where code commits and application builds are centralized and standardized. Thanks to this new approach, it is now possible to retrieve log and build data to ease the development and management operations of product teams. However, even if such approaches include code control to detect unit or integration errors, they do not check for the presence of logical bugs that can raise after code builds. For such reasons in this work we propose a framework for continuous defect prediction based on machine learning algorithms trained on a publicly available dataset. The framework is composed of a machine learning model for detecting the presence of logical bugs in code on the basis of the available data generated by DevOps tools and a dashboard to monitor the software projects status. We also describe the serverless architecture we designed for hosting the aforementioned framework.},
booktitle = {AIxIA 2021 – Advances in Artificial Intelligence: 20th International Conference of the Italian Association for Artificial Intelligence, Virtual Event, December 1–3, 2021, Revised Selected Papers},
pages = {591–606},
numpages = {16},
keywords = {Continuous integration, DevOps, Machine learning, Continuous defect prediction}
}

@inproceedings{10.1145/3474124.3474127,
author = {Rajnish, Kumar and Bhattacharjee, Vandana and Chandrabanshi, Vishnu},
title = {Applying Cognitive and Neural Network Approach over Control Flow Graph for Software Defect Prediction},
year = {2021},
isbn = {9781450389204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474124.3474127},
doi = {10.1145/3474124.3474127},
booktitle = {Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing},
pages = {13–17},
numpages = {5},
keywords = {Software Defect Prediction, Neural Network, Graph Convolutional Network, Cognitive Measures, Cognitive Complexity, CFGs},
location = {Noida, India},
series = {IC3-2021}
}

@inproceedings{10.1145/3377811.3380389,
author = {Chen, Jinyin and Hu, Keke and Yu, Yue and Chen, Zhuangzhi and Xuan, Qi and Liu, Yi and Filkov, Vladimir},
title = {Software visualization and deep transfer learning for effective software defect prediction},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380389},
doi = {10.1145/3377811.3380389},
abstract = {Software defect prediction aims to automatically locate defective code modules to better focus testing resources and human effort. Typically, software defect prediction pipelines are comprised of two parts: the first extracts program features, like abstract syntax trees, by using external tools, and the second applies machine learning-based classification models to those features in order to predict defective modules. Since such approaches depend on specific feature extraction tools, machine learning classifiers have to be custom-tailored to effectively build most accurate models.To bridge the gap between deep learning and defect prediction, we propose an end-to-end framework which can directly get prediction results for programs without utilizing feature-extraction tools. To that end, we first visualize programs as images, apply the self-attention mechanism to extract image features, use transfer learning to reduce the difference in sample distributions between projects, and finally feed the image files into a pre-trained, deep learning model for defect prediction. Experiments with 10 open source projects from the PROMISE dataset show that our method can improve cross-project and within-project defect prediction. Our code and data pointers are available at https://zenodo.org/record/3373409#.XV0Oy5Mza35.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {578–589},
numpages = {12},
keywords = {cross-project defect prediction, deep transfer learning, self-attention, software visualization, within-project defect prediction},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1007/s11219-018-9436-4,
author = {Ji, Haijin and Huang, Song and Wu, Yaning and Hui, Zhanwei and Zheng, Changyou},
title = {A new weighted naive Bayes method based on information diffusion for software defect prediction},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9436-4},
doi = {10.1007/s11219-018-9436-4},
abstract = {Software defect prediction (SDP) plays a significant part in identifying the most defect-prone modules before software testing and allocating limited testing resources. One of the most commonly used classifiers in SDP is naive Bayes (NB). Despite the simplicity of the NB classifier, it can often perform better than more complicated classification models. In NB, the features are assumed to be equally important, and the numeric features are assumed to have a normal distribution. However, the features often do not contribute equivalently to the classification, and they usually do not have a normal distribution after performing a Kolmogorov-Smirnov test; this may harm the performance of the NB classifier. Therefore, this paper proposes a new weighted naive Bayes method based on information diffusion (WNB-ID) for SDP. More specifically, for the equal importance assumption, we investigate six weight assignment methods for setting the feature weights and then choose the most suitable one based on the F-measure. For the normal distribution assumption, we apply the information diffusion model (IDM) to compute the probability density of each feature instead of the acquiescent probability density function of the normal distribution. We carry out experiments on 10 software defect data sets of three types of projects in three different programming languages provided by the PROMISE repository. Several well-known classifiers and ensemble methods are included for comparison. The final experimental results demonstrate the effectiveness and practicability of the proposed method.},
journal = {Software Quality Journal},
month = sep,
pages = {923–968},
numpages = {46},
keywords = {Information diffusion, Feature weighting, Naive Bayes, Software defect prediction}
}

@article{10.1007/s10586-018-1730-1,
author = {Jayanthi, R. and Florence, Lilly},
title = {Software defect prediction techniques using metrics based on neural network classifier},
year = {2019},
issue_date = {Jan 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-018-1730-1},
doi = {10.1007/s10586-018-1730-1},
abstract = {Software industries strive for software quality improvement by consistent bug prediction, bug removal and prediction of fault-prone module. This area has attracted researchers due to its significant involvement in software industries. Various techniques have been presented for software defect prediction. Recent researches have recommended data-mining using machine learning as an important paradigm for software bug prediction. state-of-art software defect prediction task suffer from various issues such as classification accuracy. However, software defect datasets are imbalanced in nature and known fault prone due to its huge dimension. To address this issue, here we present a combined approach for software defect prediction and prediction of software bugs. Proposed approach delivers a concept of feature reduction and artificial intelligence where feature reduction is carried out by well-known principle component analysis (PCA) scheme which is further improved by incorporating maximum-likelihood estimation for error reduction in PCA data reconstruction. Finally, neural network based classification technique is applied which shows prediction results. A framework is formulated and implemented on NASA software dataset where four datasets i.e., KC1, PC3, PC4 and JM1 are considered for performance analysis using MATLAB simulation tool. An extensive experimental study is performed where confusion, precision, recall, classification accuracy etc. parameters are computed and compared with existing software defect prediction techniques. Experimental study shows that proposed approach can provide better performance for software defect prediction.},
journal = {Cluster Computing},
month = jan,
pages = {77–88},
numpages = {12},
keywords = {Software metrics, Software defect prediction, Machine learning techniques, Defect prediction models}
}

@article{10.1049/iet-sen.2017.0148,
author = {Li, Zhiqiang and Jing, Xiao-Yuan and Zhu, Xiaoke},
title = {Progress on approaches to software defect prediction},
year = {2018},
issue_date = {June 2018},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {12},
number = {3},
url = {https://doi.org/10.1049/iet-sen.2017.0148},
doi = {10.1049/iet-sen.2017.0148},
abstract = {Software defect prediction is one of the most popular research topics in software engineering. It aims to predict defect‐prone software modules before defects are discovered, therefore it can be used to better prioritise software quality assurance effort. In recent years, especially for recent 3 years, many new defect prediction studies have been proposed. The goal of this study is to comprehensively review, analyse and discuss the state‐of‐the‐art of defect prediction. The authors survey almost 70 representative defect prediction papers in recent years (January 2014–April 2017), most of which are published in the prominent software engineering journals and top conferences. The selected defect prediction papers are summarised to four aspects: machine learning‐based prediction algorithms, manipulating the data, effort‐aware prediction and empirical studies. The research community is still facing a number of challenges for building methods and many research opportunities exist. The identified challenges can give some practical guidelines for both software engineering researchers and practitioners in future software defect prediction.},
journal = {IET Software},
month = jun,
pages = {161–175},
numpages = {15},
keywords = {empirical studies, effort-aware prediction, data manipulation, machine learning-based prediction algorithms, software quality assurance, defect-prone software modules, software engineering journals, software defect prediction, research and development, quality assurance, software reliability, software quality}
}

@article{10.1007/s00500-021-06096-3,
author = {Pandey, Sushant Kumar and Tripathi, Anil Kumar},
title = {An empirical study toward dealing with noise and class imbalance issues in software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06096-3},
doi = {10.1007/s00500-021-06096-3},
abstract = {The quality of the defect datasets is a critical issue in the domain of software defect prediction (SDP). These datasets are obtained through the mining of software repositories. Recent studies claim over the quality of the defect dataset. It is because of inconsistency between bug/clean fix keyword in fault reports and the corresponding link in the change management logs. Class Imbalance (CI) problem is also a big challenging issue in SDP models. The defect prediction method trained using noisy and imbalanced data leads to inconsistent and unsatisfactory results. Combined analysis over noisy instances and CI problem needs to be required. To the best of our knowledge, there are insufficient studies that have been done over such aspects. In this paper, we deal with the impact of noise and CI problem on five baseline SDP models; we manually added the various noise level (0–80%) and identified its impact on the performance of those SDP models. Moreover, we further provide guidelines for the possible range of tolerable noise for baseline models. We have also suggested the SDP model, which has the highest noise tolerable ability and outperforms over other classical methods. The True Positive Rate (TPR) and False Positive Rate (FPR) values of the baseline models reduce between 20–30% after adding 10–40% noisy instances. Similarly, the ROC (Receiver Operating Characteristics) values of SDP models reduce to 40–50%. The suggested model leads to avoid noise between 40–60% as compared to other traditional models.},
journal = {Soft Comput.},
month = nov,
pages = {13465–13492},
numpages = {28},
keywords = {Fault proneness, Software metrics, Machine learning, Noisy instance, Class imbalance, Software fault prediction, Software testing}
}

@inproceedings{10.1145/3374549.3374553,
author = {Zong, Liang},
title = {Classification Based Software Defect Prediction Model for Finance Software System - An Industry Study},
year = {2020},
isbn = {9781450376495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374549.3374553},
doi = {10.1145/3374549.3374553},
abstract = {Automated software defect prediction is an important and fundamental activity in the domain of software development. Successful software defect prediction can save testing effort thus reduce the time and cost for software development. However, software systems for finance company are inherently large and complex with numerous interfaces with other systems. Thus, identifying and selecting a good model and a set of features is important but challenging problem. In our paper, we first define the problem we want to solve. Then we propose a prediction model based on binary classification and a set of novel features, which is more specific for finance software systems. We collected 15 months real production data and labelled it as our dataset. The experiment shows our model and features can give a better prediction accuracy for finance systems. In addition, we demonstrate how our prediction model helps improve our production quality further. Unlike other research papers, our proposal focuses to solve problem in real finance industry.},
booktitle = {Proceedings of the 2019 3rd International Conference on Software and E-Business},
pages = {60–65},
numpages = {6},
keywords = {Software defect prediction, Machine learning, Finance system, Faulty change},
location = {Tokyo, Japan},
series = {ICSEB '19}
}

@article{10.1504/ijcat.2020.110428,
author = {Bai, Xue and Zhou, Hua and Yang, Hongji and Wang, Dong},
title = {Connecting historical changes for cross-version software defect prediction},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {63},
number = {4},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2020.110428},
doi = {10.1504/ijcat.2020.110428},
abstract = {In the whole software life cycle, software defects are inevitable and increase the cost of software development and evolution. Cross-Version Software Defect Prediction (CVSDP) aims at learning the defect patterns from the historical data of previous software versions to distinguish buggy software modules from clean ones. In CVSDP, metrics are intrinsic properties associated with the external manifestation of defects. However, traditional software defect measures ignore the sequential information of changes during software evolution process which may play a crucial role in CVSDP. Therefore, researchers tried to connect traditional metrics across versions as a new kind of evolution metrics. This study proposes a new way to connect historical sequence of metrics based on change sequence named HCSM and designs a novel deep learning algorithm GDNN as a classifier to process it. Compared to the traditional metrics approaches and other relevant approaches, the proposed approach fits in projects with stable and orderly defect control trend.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {371–383},
numpages = {12},
keywords = {gate recurrent unit, deep neural networks, DNN, deep learning, historical change sequences, software metrics, cross-version defect prediction, software testing}
}

@article{10.1016/j.infsof.2018.02.003,
author = {Mahmood, Zaheed and Bowes, David and Hall, Tracy and Lane, Peter C.R. and Petri\'{c}, Jean},
title = {Reproducibility and replicability of software defect prediction studies},
year = {2018},
issue_date = {Jul 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {99},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.02.003},
doi = {10.1016/j.infsof.2018.02.003},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {148–163},
numpages = {16},
keywords = {Software defect prediction, Reproducibility, Replication}
}

@article{10.1007/s11227-019-03051-w,
author = {NezhadShokouhi, Mohammad Mahdi and Majidi, Mohammad Ali and Rasoolzadegan, Abbas},
title = {Software defect prediction using over-sampling and feature extraction based on Mahalanobis distance},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {1},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-019-03051-w},
doi = {10.1007/s11227-019-03051-w},
abstract = {As the size of software projects becomes larger, software defect prediction (SDP) will play a key role in allocating testing resources reasonably, reducing testing costs, and speeding up the development process. Most SDP methods have used machine learning techniques based on common software metrics such as Halstead and McCabe’s cyclomatic. Datasets produced by these metrics usually do not follow Gaussian distribution, and also, they have overlaps in defect and non-defect classes. In addition, in many of software defect datasets, the number of defective modules (minority class) is considerably less than non-defective modules (majority class). In this situation, the performance of machine learning methods is reduced dramatically. Therefore, we first need to create a balance between minority and majority classes and then transfer the samples into a new space in which pair samples with same class (must-link set) are near to each other as close as possible and pair samples with different classes (cannot-link) stay as far as possible. To achieve the mentioned objectives, in this paper, Mahalanobis distance in two manners will be used. First, the minority class is oversampled based on the Mahalanobis distance such that generated synthetic data are more diverse from other minority data, and minority class distribution is not changed significantly. Second, a feature extraction method based on Mahalanobis distance metric learning is used which try to minimize distances of sample pairs in must-links and maximize the distance of sample pairs in cannot-links. To demonstrate the effectiveness of the proposed method, we performed some experiments on 12 publicly available datasets which are collected NASA repositories and compared its result by some powerful previous methods. The performance is evaluated in F-measure, G-Mean, and Matthews correlation coefficient. Generally, the proposed method has better performance as compared to the mentioned methods.},
journal = {J. Supercomput.},
month = jan,
pages = {602–635},
numpages = {34},
keywords = {Feature extraction, Over-sampling, Mahalanobis distance, Software metrics, Software defect prediction}
}

@inproceedings{10.1145/3238147.3240469,
author = {Qu, Yu and Liu, Ting and Chi, Jianlei and Jin, Yangxu and Cui, Di and He, Ancheng and Zheng, Qinghua},
title = {node2defect: using network embedding to improve software defect prediction},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240469},
doi = {10.1145/3238147.3240469},
abstract = {Network measures have been proved to be useful in predicting software defects. Leveraging the dependency relationships between software modules, network measures can capture various structural features of software systems. However, existing studies have relied on user-defined network measures (e.g., degree statistics or centrality metrics), which are inflexible and require high computation cost, to describe the structural features. In this paper, we propose a new method called node2defect which uses a newly proposed network embedding technique, node2vec, to automatically learn to encode dependency network structure into low-dimensional vector spaces to improve software defect prediction. Specifically, we firstly construct a program's Class Dependency Network. Then node2vec is used to automatically learn structural features of the network. After that, we combine the learned features with traditional software engineering features, for accurate defect prediction. We evaluate our method on 15 open source programs. The experimental results show that in average, node2defect improves the state-of-the-art approach by 9.15% in terms of F-measure.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {844–849},
numpages = {6},
keywords = {software metrics, network embedding, defect prediction, Software defect},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1109/IRI.2018.00047,
author = {Xu, Ling and Wang, Bei and Liu, Ling and Zhou, Mo and Liao, Shengping and Yan, Meng},
title = {Misclassification Cost-Sensitive Software Defect Prediction},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IRI.2018.00047},
doi = {10.1109/IRI.2018.00047},
abstract = {Software defect prediction helps developers focus on defective modules for efficient software quality assurance. A common goal shared by existing software defect prediction methods is to attain low classification error rates. These proposals suffer from two practical problems: (i) Most of the prediction methods rely on a large number of labeled training data. However, collecting labeled data is a difficult and expensive task. It is hard to obtain classification labels over new software projects or existing projects without historical defect data. (ii) Software defect datasets are highly imbalanced. In many real-world applications, the misclassification cost of defective modules is generally several times higher than that of non-defective ones. In this paper, we present a misclassification Cost-sensitive approach to Software Defect Prediction (CSDP). The CSDP approach is novel in two aspects: First, CSDP addresses the problem of unlabeled software detect datasets by combining an unsupervised sampling method with a domain specific misclassification cost model. This preprocessing step selectively samples a small percentage of modules through estimating their classification labels. Second, CSDP builds a cost-sensitive support vector machine model to predict defect-proneness of the rest of modules with both overall classification error rate and domain specific misclassification cost as quality metrics. CSDP is evaluated on four NASA projects. Experimental results highlight three interesting observations: (1) CSDP achieves higher Normalized Expected Cost of Misclassification (NECM) compared with state-of-art supervised learning models under imbalanced training data with limited labeling. (2) CSDP outperforms state-of-art semi-supervised learning methods, which disregards classification costs, especially in recall rate. (3) CSDP enhanced through unsupervised sampling as a preprocessing step prior to training and prediction outperforms the baseline CSDP without the sampling process.},
booktitle = {2018 IEEE International Conference on Information Reuse and Integration (IRI)},
pages = {256–263},
numpages = {8},
location = {Salt Lake City, UT, USA}
}

@article{10.1504/ijcse.2020.106871,
author = {Ghosh, Soumi and Rana, Ajay and Kansal, Vineet},
title = {A benchmarking framework using nonlinear manifold detection techniques for software defect prediction},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {21},
number = {4},
issn = {1742-7185},
url = {https://doi.org/10.1504/ijcse.2020.106871},
doi = {10.1504/ijcse.2020.106871},
abstract = {Prediction of software defects in time improves quality and helps in locating the defect-prone areas accurately. Although earlier considerable methods were applied, actually none of those measures was found to be fool-proof and accurate. Hence, a newer framework includes nonlinear manifold detection model, and its algorithm originated for defect prediction using different techniques of nonlinear manifold detection (nonlinear MDs) along with 14 different machine learning techniques (MLTs) on eight defective software datasets. A critical analysis cum exhaustive comparative estimation revealed that nonlinear manifold detection model has a more accurate and effective impact on defect prediction as compared to feature selection techniques. The outcome of the experiment was statistically tested by Friedman and post hoc analysis using Nemenyi test, which validates that hidden Markov model (HMM) along with nonlinear manifold detection model outperforms and is significantly different from MLTs.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {593–614},
numpages = {21},
keywords = {post hoc analysis, software defect prediction, nonlinear manifold detection, Nemenyi test, machine learning, Friedman test, feature selection, dimensionality reduction}
}

@article{10.5555/2684939.2684969,
author = {Ma, Ying and Pan, Weiwei and Zhu, Shunzhi and Yin, Huayi and Luo, Jian},
title = {An improved semi-supervised learning method for software defect prediction},
year = {2014},
issue_date = {September 2014},
publisher = {IOS Press},
address = {NLD},
volume = {27},
number = {5},
issn = {1064-1246},
abstract = {This paper presents an improved semi-supervised learning approach for defect prediction involving class imbalanced and limited labeled data problem. This approach employs random under-sampling technique to resample the original training set and updating training set in each round for co-train style algorithm. It makes the defect predictor more practical for real applications, by combating these problems. In comparison with conventional machine learning approaches, our method has significant superior performance. Experimental results also show that with the proposed learning approach, it is possible to design better method to tackle the class imbalanced problem in semi-supervised learning.},
journal = {J. Intell. Fuzzy Syst.},
month = sep,
pages = {2473–2480},
numpages = {8},
keywords = {Semi-Supervised Learning, Random Sampling, Defect Prediction, Co-Train, Class Imbalance}
}

@inproceedings{10.1007/978-3-030-34885-4_27,
author = {Ali, Aftab and Abu-Tair, Mamun and Noppen, Joost and McClean, Sally and Lin, Zhiwei and McChesney, Ian},
title = {Contributing Features-Based Schemes for Software Defect Prediction},
year = {2019},
isbn = {978-3-030-34884-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-34885-4_27},
doi = {10.1007/978-3-030-34885-4_27},
abstract = {Automated defect prediction of large and complex software systems is a challenging task. However, by utilising correlated quality metrics, a defect prediction model can be devised to automatically predict the defects in a software system. The robustness and accuracy of a prediction model is highly dependent on the selection of contributing and non-contributing features. Hence, in this regard, the contribution of this paper is twofold, first it separates those features which are contributing towards the development of a defect in a software component from those which are non-contributing features. Secondly, a logistic regression and Ensemble Bagged Trees-based prediction model are applied on the contributing features for accurately predicting a defect in a software component. The proposed models are compared with the most recent scheme in the literature in terms of accuracy and area under the curve (AUC). It is evident from the results and analysis that the performance of the proposed prediction models outperforms the schemes in the literature.},
booktitle = {Artificial Intelligence XXXVI: 39th SGAI International Conference on Artificial Intelligence, AI 2019, Cambridge, UK, December 17–19, 2019, Proceedings},
pages = {350–361},
numpages = {12},
keywords = {Prediction models, Intelligent information retrieval, Machine learning},
location = {Cambridge, United Kingdom}
}

@article{10.1016/j.neucom.2019.03.076,
author = {Zhao, Linchang and Shang, Zhaowei and Zhao, Ling and Zhang, Taiping and Tang, Yuan Yan},
title = {Software defect prediction via cost-sensitive Siamese parallel fully-connected neural networks},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {352},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.03.076},
doi = {10.1016/j.neucom.2019.03.076},
journal = {Neurocomput.},
month = aug,
pages = {64–74},
numpages = {11},
keywords = {Software defect prediction, Few-shot learning, Deep learning, Cost-sensitive learning, Siamese parallel fully-connected networks}
}

@inproceedings{10.1007/978-3-030-29551-6_23,
author = {Miholca, Diana-Lucia and Czibula, Gabriela},
title = {Software Defect Prediction Using a Hybrid Model Based on Semantic Features Learned from the Source Code},
year = {2019},
isbn = {978-3-030-29550-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29551-6_23},
doi = {10.1007/978-3-030-29551-6_23},
abstract = {Software defect prediction has extensive applicability thus being a very active research area in Search-Based Software Engineering. A high proportion of the software defects are caused by violated couplings. In this paper, we investigate the relevance of semantic coupling in assessing the software proneness to defects. We propose a hybrid classification model combining Gradual Relational Association Rules with Artificial Neural Networks, which detects the defective software entities based on semantic features automatically learned from the source code. The experiments we have performed led to results that confirm the interplay between conceptual coupling and software defects proneness.},
booktitle = {Knowledge Science, Engineering and Management: 12th International Conference, KSEM 2019, Athens, Greece, August 28–30, 2019, Proceedings, Part I},
pages = {262–274},
numpages = {13},
keywords = {Conceptual coupling, Machine learning, Software defect prediction},
location = {Athens, Greece}
}

@article{10.1007/s10664-021-09991-3,
author = {Tahir, Amjed and Bennin, Kwabena E. and Xiao, Xun and MacDonell, Stephen G.},
title = {Does class size matter? An in-depth assessment of the effect of class size in software defect prediction},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09991-3},
doi = {10.1007/s10664-021-09991-3},
abstract = {In the past 20 years, defect prediction studies have generally acknowledged the effect of class size on software prediction performance. To quantify the relationship between object-oriented (OO) metrics and defects, modelling has to take into account the direct, and potentially indirect, effects of class size on defects. However, some studies have shown that size cannot be simply controlled or ignored, when building prediction models. As such, there remains a question whether, and when, to control for class size. This study provides a new in-depth examination of the impact of class size on the relationship between OO metrics and software defects or defect-proneness. We assess the impact of class size on the number of defects and defect-proneness in software systems by employing a regression-based mediation (with bootstrapping) and moderation analysis to investigate the direct and indirect effect of class size in count and binary defect prediction. Our results show that the size effect is not always significant for all metrics. Of the seven OO metrics we investigated, size consistently has significant mediation impact only on the relationship between Coupling Between Objects (CBO) and defects/defect-proneness, and a potential moderation impact on the relationship between Fan-out and defects/defect-proneness. Other metrics show mixed results, in that they are significant for some systems but not for others. Based on our results we make three recommendations. One, we encourage researchers and practitioners to examine the impact of class size for the specific data they have in hand and through the use of the proposed statistical mediation/moderation procedures. Two, we encourage empirical studies to investigate the indirect effect of possible additional variables in their models when relevant. Three, the statistical procedures adopted in this study could be used in other empirical software engineering research to investigate the influence of potential mediators/moderators.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {38},
keywords = {Software quality, Metrics, Class size, Defect prediction}
}

@article{10.1016/j.neucom.2018.04.090,
author = {Malhotra, Ruchika and Kamal, Shine},
title = {An empirical study to investigate oversampling methods for improving software defect prediction using imbalanced data},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {343},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.090},
doi = {10.1016/j.neucom.2018.04.090},
journal = {Neurocomput.},
month = may,
pages = {120–140},
numpages = {21},
keywords = {Procedural metrics, Machine learning techniques, MetaCost learners, Oversampling methods, Imbalanced data, Defect prediction}
}

@article{10.1007/s10515-020-00277-4,
author = {Esteves, Geanderson and Figueiredo, Eduardo and Veloso, Adriano and Viggiato, Markos and Ziviani, Nivio},
title = {Understanding machine learning software defect predictions},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3–4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-020-00277-4},
doi = {10.1007/s10515-020-00277-4},
abstract = {Software defects are well-known in software development and might cause several problems for users and developers aside. As a result, researches employed distinct techniques to mitigate the impacts of these defects in the source code. One of the most notable techniques focuses on defect prediction using machine learning methods, which could support developers in handling these defects before they are introduced in the production environment. These studies provide alternative approaches to predict the likelihood of defects. However, most of these works concentrate on predicting defects from a vast set of software features. Another key issue with the current literature is the lack of a satisfactory explanation of the reasons that drive the software to a defective state. Specifically, we use a tree boosting algorithm (XGBoost) that receives as input a training set comprising records of easy-to-compute characteristics of each module and outputs whether the corresponding module is defect-prone. To exploit the link between predictive power and model explainability, we propose a simple model sampling approach that finds accurate models with the minimum set of features. Our principal idea is that features not contributing to increasing the predictive power should not be included in the model. Interestingly, the reduced set of features helps to increase model explainability, which is important to provide information to developers on features related to each module of the code which is more defect-prone. We evaluate our models on diverse projects within Jureczko datasets, and we show that (i) features that contribute most for finding best models may vary depending on the project and (ii) it is possible to find effective models that use few features leading to better understandability. We believe our results are useful to developers as we provide the specific software features that influence the defectiveness of selected projects.},
journal = {Automated Software Engg.},
month = dec,
pages = {369–392},
numpages = {24},
keywords = {SHAP values, Jureczko datasets, Explainable models, Software defects}
}

@article{10.1155/2019/6230953,
author = {Fan, Guisheng and Diao, Xuyang and Yu, Huiqun and Yang, Kang and Chen, Liqiong and Vitiello, Autilia},
title = {Software Defect Prediction via Attention-Based Recurrent Neural Network},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1058-9244},
url = {https://doi.org/10.1155/2019/6230953},
doi = {10.1155/2019/6230953},
abstract = {In order to improve software reliability, software defect prediction is applied to the process of software maintenance to identify potential bugs. Traditional methods of software defect prediction mainly focus on designing static code metrics, which are input into machine learning classifiers to predict defect probabilities of the code. However, the characteristics of these artificial metrics do not contain the syntactic structures and semantic information of programs. Such information is more significant than manual metrics and can provide a more accurate predictive model. In this paper, we propose a framework called defect prediction via attention-based recurrent neural network (DP-ARNN). More specifically, DP-ARNN first parses abstract syntax trees (ASTs) of programs and extracts them as vectors. Then it encodes vectors which are used as inputs of DP-ARNN by dictionary mapping and word embedding. After that, it can automatically learn syntactic and semantic features. Furthermore, it employs the attention mechanism to further generate significant features for accurate defect prediction. To validate our method, we choose seven open-source Java projects in Apache, using F1-measure and area under the curve (AUC) as evaluation criteria. The experimental results show that, in average, DP-ARNN improves the F1-measure by 14% and AUC by 7% compared with the state-of-the-art methods, respectively.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@article{10.1007/s10515-015-0179-1,
author = {Wang, Tiejian and Zhang, Zhiwu and Jing, Xiaoyuan and Zhang, Liqiang},
title = {Multiple kernel ensemble learning for software defect prediction},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-015-0179-1},
doi = {10.1007/s10515-015-0179-1},
abstract = {Software defect prediction aims to predict the defect proneness of new software modules with the historical defect data so as to improve the quality of a software system. Software historical defect data has a complicated structure and a marked characteristic of class-imbalance; how to fully analyze and utilize the existing historical defect data and build more precise and effective classifiers has attracted considerable researchers' interest from both academia and industry. Multiple kernel learning and ensemble learning are effective techniques in the field of machine learning. Multiple kernel learning can map the historical defect data to a higher-dimensional feature space and make them express better, and ensemble learning can use a series of weak classifiers to reduce the bias generated by the majority class and obtain better predictive performance. In this paper, we propose to use the multiple kernel learning to predict software defect. By using the characteristics of the metrics mined from the open source software, we get a multiple kernel classifier through ensemble learning method, which has the advantages of both multiple kernel learning and ensemble learning. We thus propose a multiple kernel ensemble learning (MKEL) approach for software defect classification and prediction. Considering the cost of risk in software defect prediction, we design a new sample weight vector updating strategy to reduce the cost of risk caused by misclassifying defective modules as non-defective ones. We employ the widely used NASA MDP datasets as test data to evaluate the performance of all compared methods; experimental results show that MKEL outperforms several representative state-of-the-art defect prediction methods.},
journal = {Automated Software Engg.},
month = dec,
pages = {569–590},
numpages = {22},
keywords = {Software defect prediction, Multiple kernel learning, Multiple kernel ensemble learning (MKEL), Ensemble learning}
}

@article{10.1504/ijista.2019.102667,
author = {Ghosh, Soumi and Rana, Ajay and Kansal, Vineet},
title = {Statistical assessment of nonlinear manifold detection-based software defect prediction techniques},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {6},
issn = {1740-8865},
url = {https://doi.org/10.1504/ijista.2019.102667},
doi = {10.1504/ijista.2019.102667},
abstract = {Prediction of software defects has immense importance for obtaining desired outcome at minimised cost and so attracted researchers working on this topic applying various techniques, which were not found fully effective. Software datasets comprise of redundant features that hinder effective application of techniques resulting inappropriate defect prediction. Hence, it requires newer application of nonlinear manifold detection techniques (nonlinear MDTs) that has been examined for accurate prediction of defects at lesser time and cost using different classification techniques. In this work, we analysed and tested the effect of nonlinear MDTs to find out accurate and best classification technique for all datasets. Comparison has been made between the results of without or with nonlinear MDTs and paired two-tailed T-test has been performed for statistical testing and verifying the performance of classifiers using nonlinear MDTs on all datasets. Outcome revealed that among all nonlinear MDTs, FastMVU makes most accurate prediction of software defects.},
journal = {Int. J. Intell. Syst. Technol. Appl.},
month = jan,
pages = {579–605},
numpages = {26},
keywords = {software defect prediction, promise repository, nonlinear, manifold detection, machine learning, FastMVU, fast maximum variance unfolding, dimensionality reduction}
}

@article{10.1049/iet-sen.2017.0198,
author = {Zhang, Zhi-Wu and Jing, Xiao-Yuan and Wu, Fei},
title = {Low‐rank representation for semi‐supervised software defect prediction},
year = {2018},
issue_date = {December 2018},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {12},
number = {6},
url = {https://doi.org/10.1049/iet-sen.2017.0198},
doi = {10.1049/iet-sen.2017.0198},
abstract = {Software defect prediction based on machine learning is an active research topic in the field of software engineering. The historical defect data in software repositories may contain noises because automatic defect collection is based on modified logs and defect reports. When the previous defect labels of modules are limited, predicting the defect‐prone modules becomes a challenging problem. In this study, the authors propose a graph‐based semi‐supervised defect prediction approach to solve the problems of insufficient labelled data and noisy data. Graph‐based semi‐supervised learning methods used the labelled and unlabelled data simultaneously and consider them as the nodes of the graph at the training phase. Therefore, they solve the problem of insufficient labelled samples. To improve the stability of noisy defect data, a powerful clustering method, low‐rank representation (LRR), and neighbourhood distance are used to construct the relationship graph of samples. Therefore, they propose a new semi‐supervised defect prediction approach, named low‐rank representation‐based semi‐supervised software defect prediction (LRRSSDP). The widely used datasets from NASA projects and noisy datasets are employed as test data to evaluate the performance. Experimental results show that (i) LRRSSDP outperforms several representative state‐of‐the‐art semi‐supervised defect prediction methods; and (ii) LRRSSDP can maintain robustness in noisy environments.},
journal = {IET Software},
month = dec,
pages = {527–535},
numpages = {9},
keywords = {LRRSSDP, low-rank representation, noisy defect data, insufficient labelled samples, unlabelled data, noisy data, insufficient labelled data, semisupervised defect prediction approach, defect-prone modules, defect reports, automatic defect collection, software repositories, historical defect data, software engineering, semisupervised software defect prediction, program diagnostics, graph theory, pattern clustering, learning (artificial intelligence), software engineering}
}

@article{10.1007/s42979-020-0119-4,
author = {Khuat, Thanh Tung and Le, My Hanh},
title = {Evaluation of Sampling-Based Ensembles of Classifiers on Imbalanced Data for Software Defect Prediction Problems},
year = {2020},
issue_date = {Mar 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {2},
url = {https://doi.org/10.1007/s42979-020-0119-4},
doi = {10.1007/s42979-020-0119-4},
abstract = {Defect prediction in software projects plays a crucial role to reduce quality-based risk and increase the capability of detecting faulty program modules. Hence, classification approaches to anticipate software defect proneness based on static code characteristics have become a hot topic with a great deal of attention in recent years. While several novel studies show that the use of a single classifier causes the performance bottleneck, ensembles of classifiers might effectively enhance classification performance compared to a single classifier. However, the class imbalance property of software defect data severely hinders the classification efficiency of ensemble learning. To cope with this problem, resampling methods are usually combined into ensemble models.
This paper empirically assesses the importance of sampling with regard to ensembles of various classifiers on imbalanced data in software defect prediction problems. Extensive experiments with the combination of seven different kinds of classification algorithms, three sampling methods, and two balanced data learning schemata were conducted over ten datasets. Empirical results indicated the positive effects of combining sampling techniques and the ensemble learning model on the performance of defect prediction regarding datasets with imbalanced class distributions.},
journal = {SN Comput. Sci.},
month = mar,
numpages = {16},
keywords = {Imbalanced data, Ensemble learning, Data balancing, SMOTE, Random oversampling, Random undersampling, Software defect prediction}
}

@article{10.1016/j.asoc.2016.06.023,
author = {Mesquita, Diego P.P. and Rocha, Lincoln S. and Gomes, Joo Paulo P. and Rocha Neto, Ajalmar R.},
title = {Classification with reject option for software defect prediction},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.06.023},
doi = {10.1016/j.asoc.2016.06.023},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose the use of classification with reject option for software defect prediction (SDP) as a way to incorporate additional knowledge in the SDP process.We propose two variants of the extreme learning machine with reject option.It is proposed an ELM with reject option for imbalanced datasets.The proposed method is tested on five real world software datasets.An example is shown to illustrate how the rejected software modules can be further analyzed to improve the final SDP accuracy. ContextSoftware defect prediction (SDP) is an important task in software engineering. Along with estimating the number of defects remaining in software systems and discovering defect associations, classifying the defect-proneness of software modules plays an important role in software defect prediction. Several machine-learning methods have been applied to handle the defect-proneness of software modules as a classification problem. This type of yes or no decision is an important drawback in the decision-making process and if not precise may lead to misclassifications. To the best of our knowledge, existing approaches rely on fully automated module classification and do not provide a way to incorporate extra knowledge during the classification process. This knowledge can be helpful in avoiding misclassifications in cases where system modules cannot be classified in a reliable way. ObjectiveWe seek to develop a SDP method that (i) incorporates a reject option in the classifier to improve the reliability in the decision-making process; and (ii) makes it possible postpone the final decision related to rejected modules for an expert analysis or even for another classifier using extra domain knowledge. MethodWe develop a SDP method called rejoELM and its variant, IrejoELM. Both methods were built upon the weighted extreme learning machine (ELM) with reject option that makes it possible postpone the final decision of non-classified modules, the rejected ones, to another moment. While rejoELM aims to maximize the accuracy for a rejection rate, IrejoELM maximizes the F-measure. Hence, IrejoELM becomes an alternative for classification with reject option for imbalanced datasets. ResultsrejoEM and IrejoELM are tested on five datasets of source code metrics extracted from real world open-source software projects. Results indicate that rejoELM has an accuracy for several rejection rates that is comparable to some state-of-the-art classifiers with reject option. Although IrejoELM shows lower accuracies for several rejection rates, it clearly outperforms all other methods when the F-measure is used as a performance metric. ConclusionIt is concluded that rejoELM is a valid alternative for classification with reject option problems when classes are nearly equally represented. On the other hand, IrejoELM is shown to be the best alternative for classification with reject option on imbalanced datasets. Since SDP problems are usually characterized as imbalanced learning problems, the use of IrejoELM is recommended.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1085–1093},
numpages = {9},
keywords = {Software defect prediction, Extreme learning machines, Classification with reject option}
}

@inproceedings{10.1145/3342999.3343010,
author = {Cui, Mengtian and Sun, Yue and Lu, Yang and Jiang, Yue},
title = {Study on the Influence of the Number of Features on the Performance of Software Defect Prediction Model},
year = {2019},
isbn = {9781450371605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342999.3343010},
doi = {10.1145/3342999.3343010},
abstract = {The software defect prediction model based on machine learning technology is the key to improve the reliability of software. The influence of the number of features on the performance of different software defect prediction models was proposed in this paper. First, a new data sets was built, which is increasing by the number of features based on the NASA public data sets. Then, the eight predictive models are experimented based on these data sets. Next, the influence of the number of features on the performance of different prediction models was analyzed based on the experimental results. Next, the AUC values obtained from the experiment were used to evaluate the performance of different prediction models, and the coefficient of variation C·V values was used to evaluate the performance stability of different prediction models while the number of features changed. In the end, the experiments show that the performance of the predictive model C4.5 is highly susceptible to changes in the number of features, while the performance of the predictive model SMO is relatively stable.},
booktitle = {Proceedings of the 2019 3rd International Conference on Deep Learning Technologies},
pages = {32–37},
numpages = {6},
keywords = {software defect prediction, number of features, machine learning, feature selection},
location = {Xiamen, China},
series = {ICDLT '19}
}

@inproceedings{10.1145/3239576.3239622,
author = {Yang, Zhao and Qian, Hongbing},
title = {Automated Parameter Tuning of Artificial Neural Networks for Software Defect Prediction},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239622},
doi = {10.1145/3239576.3239622},
abstract = {Defect prediction can help predict defect-prone software modules and improve the efficiency and accuracy of defect location and repair, which plays an extremely important role in software quality assurance. Artificial Neural Networks (ANNs), a family of powerful machine learning regression or classification models, have been widely applied for defect prediction. However, the performance of these models will be degraded if they use suboptimal default parameter settings (e.g., the number of units in the hidden layer). This paper utilizes an automated parameter tuning technique-Caret to optimize parameter settings. In our study, 30 datasets are downloaded from the Tera-PROMISE Repository. According to the characteristics of the datasets, we select key features (metrics) as predictors to train defect prediction models. The experiment applies feed-forward, single hidden layer artificial neural network as classifier to build different defect prediction models respectively with optimized parameter settings and with default parameter settings. Confusion matrix and ROC curve are used for evaluating the quality of the models above. The results show that the models trained with optimized parameter settings outperform the models trained with default parameter settings. Hence, we suggest that researchers should pay attention to tuning parameter settings by Caret for ANNs instead of using suboptimal default settings if they select ANNs for training models in the future defect prediction studies.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {203–209},
numpages = {7},
keywords = {Software defect prediction, Metrics, Automated Parameter Tuning, Artificial Neural Networks},
location = {Chengdu, China},
series = {ICAIP '18}
}

@article{10.4018/IJOSSP.2017100102,
author = {Akour, Mohammed and Melhem, Wasen Yahya},
title = {Software Defect Prediction Using Genetic Programming and Neural Networks},
year = {2017},
issue_date = {October 2017},
publisher = {IGI Global},
address = {USA},
volume = {8},
number = {4},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2017100102},
doi = {10.4018/IJOSSP.2017100102},
abstract = {This article describes how classification methods on software defect prediction is widely researched due to the need to increase the software quality and decrease testing efforts. However, findings of past researches done on this issue has not shown any classifier which proves to be superior to the other. Additionally, there is a lack of research that studies the effects and accuracy of genetic programming on software defect prediction. To find solutions for this problem, a comparative software defect prediction experiment between genetic programming and neural networks are performed on four datasets from the NASA Metrics Data repository. Generally, an interesting degree of accuracy is detected, which shows how the metric-based classification is useful. Nevertheless, this article specifies that the application and usage of genetic programming is highly recommended due to the detailed analysis it provides, as well as an important feature in this classification method which allows the viewing of each attributes impact in the dataset.},
journal = {Int. J. Open Source Softw. Process.},
month = oct,
pages = {32–51},
numpages = {20},
keywords = {Testing, Software Defect Prediction, Neural Networks, Nasa Metrics, Machine learning, Genetic Programming, Genetic Algorithm, Classification}
}

@article{10.1155/2021/4997459,
author = {Li, Zhen and Li, Tong and Wu, YuMei and Yang, Liu and Miao, Hong and Wang, DongSheng and Precup, Radu-Emil},
title = {Software Defect Prediction Based on Hybrid Swarm Intelligence and Deep Learning},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/4997459},
doi = {10.1155/2021/4997459},
abstract = {In order to improve software quality and testing efficiency, this paper implements the prediction of software defects based on deep learning. According to the respective advantages and disadvantages of the particle swarm algorithm and the wolf swarm algorithm, the two algorithms are mixed to realize the complementary advantages of the algorithms. At the same time, the hybrid algorithm is used in the search of model hyperparameter optimization, the loss function of the model is used as the fitness function, and the collaborative search ability of the swarm intelligence population is used to find the global optimal solution in multiple local solution spaces. Through the analysis of the experimental results of six data sets, compared with the traditional hyperparameter optimization method and a single swarm intelligence algorithm, the model using the hybrid algorithm has higher and better indicators. And, under the processing of the autoencoder, the performance of the model has been further improved.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {17}
}

@article{10.1016/j.asoc.2015.04.045,
author = {Arar, \"{O}mer Faruk and Ayan, K\"{u}r\c{s}at},
title = {Software defect prediction using cost-sensitive neural network},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {33},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.04.045},
doi = {10.1016/j.asoc.2015.04.045},
abstract = {Software defect prediction model was built by Artificial Neural Network (ANN).ANN connection weights were optimized by Artificial Bee Colony (ABC).Parametric cost-sensitivity feature was added to ANN by using a new error function.Model was applied to five publicly available datasets from the NASA repository.Results were compared with other cost-sensitive and non-cost-sensitive studies. The software development life cycle generally includes analysis, design, implementation, test and release phases. The testing phase should be operated effectively in order to release bug-free software to end users. In the last two decades, academicians have taken an increasing interest in the software defect prediction problem, several machine learning techniques have been applied for more robust prediction. A different classification approach for this problem is proposed in this paper. A combination of traditional Artificial Neural Network (ANN) and the novel Artificial Bee Colony (ABC) algorithm are used in this study. Training the neural network is performed by ABC algorithm in order to find optimal weights. The False Positive Rate (FPR) and False Negative Rate (FNR) multiplied by parametric cost coefficients are the optimization task of the ABC algorithm. Software defect data in nature have a class imbalance because of the skewed distribution of defective and non-defective modules, so that conventional error functions of the neural network produce unbalanced FPR and FNR results. The proposed approach was applied to five publicly available datasets from the NASA Metrics Data Program repository. Accuracy, probability of detection, probability of false alarm, balance, Area Under Curve (AUC), and Normalized Expected Cost of Misclassification (NECM) are the main performance indicators of our classification approach. In order to prevent random results, the dataset was shuffled and the algorithm was executed 10 times with the use of n-fold cross-validation in each iteration. Our experimental results showed that a cost-sensitive neural network can be created successfully by using the ABC optimization algorithm for the purpose of software defect prediction.},
journal = {Appl. Soft Comput.},
month = aug,
pages = {263–277},
numpages = {15},
keywords = {Software quality, Software defect prediction, Machine learning, Cost-sensitive classification, Artificial Neural Network, Artificial Bee Colony}
}

@article{10.1007/s00500-021-06254-7,
author = {Sotto-Mayor, Bruno and Kalech, Meir},
title = {Cross-project smell-based defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {22},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06254-7},
doi = {10.1007/s00500-021-06254-7},
abstract = {Defect prediction is a technique introduced to optimize the testing phase of the software development pipeline by predicting which components in the software may contain defects. Its methodology trains a classifier with data regarding a set of features measured on each component from the target software project to predict whether the component may be defective or not. However, suppose the defective information is not available in the training set. In that case, we need to rely on an alternate approach that uses the training set of external projects to train the classifier. This approached is called cross-project defect prediction. Bad code smells are a category of features that have been previously explored in defect prediction and have been shown to be a good predictor of defects. Code smells are patterns of poor development in the code and indicate flaws in its design and implementation. Although they have been previously studied in the context of defect prediction, they have not been studied as features for cross-project defect prediction. In our experiment, we train defect prediction models for 100 projects to evaluate the predictive performance of the bad code smells. We implemented four cross-project approaches known in the literature and compared the performance of 37 smells with 56 code metrics, commonly used for defect prediction. The results show that the cross-project defect prediction models trained with code smells significantly improved 6.50% on the ROC AUC compared against the code metrics.},
journal = {Soft Comput.},
month = nov,
pages = {14171–14181},
numpages = {11},
keywords = {Software engineering, Software quality, Mining software repositories, Code smell, Defect prediction, Cross-project defect prediction}
}

@article{10.1016/j.procs.2015.02.161,
author = {Arora, Ishani and Tetarwal, Vivek and Saha, Anju},
title = {Open Issues in Software Defect Prediction},
year = {2015},
issue_date = {2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2015.02.161},
doi = {10.1016/j.procs.2015.02.161},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {906–912},
numpages = {7},
keywords = {software testing, software quality, machine learning, defect prediction, data mining}
}

@article{10.3233/KES-210061,
author = {Shatnawi, Raed},
title = {Software fault prediction using machine learning techniques with metric thresholds},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {2},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-210061},
doi = {10.3233/KES-210061},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {159–172},
numpages = {14},
keywords = {machine learning, threshold values, software metrics, Fault prediction}
}

@article{10.1007/s10515-011-0092-1,
author = {Li, Ming and Zhang, Hongyu and Wu, Rongxin and Zhou, Zhi-Hua},
title = {Sample-based software defect prediction with active and semi-supervised learning},
year = {2012},
issue_date = {June      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0092-1},
doi = {10.1007/s10515-011-0092-1},
abstract = {Software defect prediction can help us better understand and control software quality. Current defect prediction techniques are mainly based on a sufficient amount of historical project data. However, historical data is often not available for new projects and for many organizations. In this case, effective defect prediction is difficult to achieve. To address this problem, we propose sample-based methods for software defect prediction. For a large software system, we can select and test a small percentage of modules, and then build a defect prediction model to predict defect-proneness of the rest of the modules. In this paper, we describe three methods for selecting a sample: random sampling with conventional machine learners, random sampling with a semi-supervised learner and active sampling with active semi-supervised learner. To facilitate the active sampling, we propose a novel active semi-supervised learning method ACoForest which is able to sample the modules that are most helpful for learning a good prediction model. Our experiments on PROMISE datasets show that the proposed methods are effective and have potential to be applied to industrial practice.},
journal = {Automated Software Engg.},
month = jun,
pages = {201–230},
numpages = {30},
keywords = {Software defect prediction, Sampling, Quality assurance, Machine learning, Active semi-supervised learning}
}

@inproceedings{10.1145/3377811.3380403,
author = {Tabassum, Sadia and Minku, Leandro L. and Feng, Danyi and Cabral, George G. and Song, Liyan},
title = {An investigation of cross-project learning in online just-in-time software defect prediction},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380403},
doi = {10.1145/3377811.3380403},
abstract = {Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time. We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {554–565},
numpages = {12},
keywords = {class imbalance, concept drift, cross-project learning, online learning, software defect prediction, transfer learning, verification latency},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1016/j.knosys.2015.09.035,
author = {Li, Weiwei and Huang, Zhiqiu and Li, Qing},
title = {Three-way decisions based software defect prediction},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {91},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2015.09.035},
doi = {10.1016/j.knosys.2015.09.035},
abstract = {Based on a two-stage classification method and a two-stage ranking method on three-way decisions, this paper introduces a three-way decisions framework for cost-sensitive software defect prediction. For the classification problem in software defect prediction, traditional two-way decisions methods usually generate a higher classification error and more decision cost. Here, a two-stage classification method that integrates three-way decisions and ensemble learning to predict software defect is proposed. Experimental results on NASA data sets show that our method can obtain a higher accuracy and a lower decision cost. For the ranking problem in software defect prediction, a two-stage ranking method is introduced. In the first stage, all software modules are classified into three different regions based on three-way decisions. A dominance relation rough set based ranking algorithm is next applied to rank the modules in each region. Comparison experiments with 6 other ranking methods present that our proposed method can obtain a better result on FPA measure.},
journal = {Know.-Based Syst.},
month = jan,
pages = {263–274},
numpages = {12},
keywords = {Three-way decisions, Software defect ranking, Software defect classification}
}

@inproceedings{10.1145/3180374.3181331,
author = {Li, Yuting and Su, Jianmin and Yang, Xiaoxing},
title = {Multi-Objective vs. Single-Objective Approaches for Software Defect Prediction},
year = {2018},
isbn = {9781450354318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180374.3181331},
doi = {10.1145/3180374.3181331},
abstract = {Software defect prediction employs attributes of software modules to identify defect-prone modules and thus improves software reliability by allocating testing resources more efficiently. Realizing that single-objective methods might be insufficient for solving defect prediction problems, some researchers have proposed multi-objective learning approaches, and proved better performance of multi-objective than single-objective methods. However, existing compared single-objective methods optimize a completely different goal from goals of multi-objective approaches, which might lead to bias. In this paper, we compare a multi-objective approach that optimizes two objectives and a single-objective approach that directly optimizes a trade-off of the two objectives, in order to further investigate the comparison of multi-objective and single-objective approaches. The conclusion will help to appropriately choose multi-objective or single-objective learning approaches for defect prediction.},
booktitle = {Proceedings of the 2018 2nd International Conference on Management Engineering, Software Engineering and Service Sciences},
pages = {122–127},
numpages = {6},
keywords = {software defect prediction, single-objective learning, effectiveness, cost, Multi-objective learning},
location = {Wuhan, China},
series = {ICMSS 2018}
}

@article{10.1016/j.procs.2018.05.012,
author = {Ghosh, Soumi and Rana, Ajay and Kansal, Vineet},
title = {A Nonlinear Manifold Detection based Model for Software Defect Prediction},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.012},
doi = {10.1016/j.procs.2018.05.012},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {581–594},
numpages = {14},
keywords = {Feature Selection techniques, Friedman test, Nonlinear Manifold Detection techniques, Paired two-tailed T-test, Software Defect Prediction}
}

@inproceedings{10.1145/2896387.2900324,
author = {Rahman, Md. Habibur and Sharmin, Sadia and Sarwar, Sheikh Muhammad and Shoyaib, Mohammad},
title = {Software Defect Prediction Using Feature Space Transformation},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2900324},
doi = {10.1145/2896387.2900324},
abstract = {In software quality estimation research, software defect prediction is a key topic. A defect prediction model is generally constructed using a variety of software attributes and each attribute may have positive, negative or neutral effect on a specific model. Selection of an optimal set of attributes for model development remains a vital yet unexplored issue. In this paper, we have introduced a new feature space transformation process with a normalization technique to improve the defect prediction accuracy. We proposed a feature space transformation technique and classify the instances using Support Vector Machine (SVM) with its histogram intersection kernel. The proposed method is evaluated using the data sets from NASA metric data repository and its application demonstrates acceptable accuracy.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {72},
numpages = {6},
keywords = {Software defect prediction, Feature space transformation, Attribute selection},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@article{10.1016/j.ins.2018.02.027,
author = {Miholca, Diana-Lucia and Czibula, Gabriela and Czibula, Istvan Gergely},
title = {A novel approach for software defect prediction through hybridizing gradual relational association rules with artificial neural networks},
year = {2018},
issue_date = {May 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {441},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.02.027},
doi = {10.1016/j.ins.2018.02.027},
abstract = {The growing complexity of software projects requires increasing consideration of their analysis and testing. Identifying defective software entities is essential for software quality assurance and it also improves activities related to software testing. In this study, we developed a novel supervised classification method called HyGRAR for software defect prediction. HyGRAR is a non-linear hybrid model that combines gradual relational association rule mining and artificial neural networks to discriminate between defective and non-defective software entities. Experiments performed based on 10 open-source data sets demonstrated the excellent performance of the HYGRAR classifier. HyGRAR performed better than most of the previously proposed approaches for software defect prediction in performance evaluations using the same data sets.},
journal = {Inf. Sci.},
month = may,
pages = {152–170},
numpages = {19},
keywords = {Software defect prediction, Machine learning, Gradual relational association rule, Artificial neural network}
}

@inproceedings{10.5555/3432601.3432619,
author = {Jahanshahi, Hadi and Cevik, Mucahit and Ba\c{s}ar, Ay\c{s}e},
title = {Moving from cross-project defect prediction to heterogeneous defect prediction: a partial replication study},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software defect prediction heavily relies on the metrics collected from software projects. Earlier studies often used machine learning techniques to build, validate, and improve bug prediction models using either a set of metrics collected within a project or across different projects. However, techniques applied and conclusions derived by those models are restricted by how identical those metrics are. Knowledge coming from those models will not be extensible to a target project if no sufficient overlapping metrics have been collected in the source projects. To explore the feasibility of transferring knowledge across projects without common labeled metrics, we systematically integrated Heterogeneous Defect Prediction (HDP) by replicating and validating the obtained results. Our main goal is to extend prior research and explore the feasibility of HDP and finally to compare its performance with that of its predecessor, Cross-Project Defect Prediction. We construct an HDP model on different publicly available datasets. Moreover, we propose a new ensemble voting approach in the HDP context to utilize the predictive power of multiple available datasets. The result of our experiment is comparable to that of the original study. However, we also explored the feasibility of HDP in real cases. Our results shed light on the infeasibility of many cases for the HDP algorithm due to its sensitivity to the parameter selection. In general, our analysis gives a deep insight into why and how to perform transfer learning from one domain to another, and in particular, provides a set of guidelines to help researchers and practitioners to disseminate knowledge to the defect prediction domain.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {133–142},
numpages = {10},
keywords = {transfer learning, software quality, heterogeneous metrics, defect prediction},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1145/3106237.3106257,
author = {Fu, Wei and Menzies, Tim},
title = {Revisiting unsupervised learning for defect prediction},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106257},
doi = {10.1145/3106237.3106257},
abstract = {Collecting quality data from software projects can be time-consuming and expensive. Hence, some researchers explore "unsupervised" approaches to quality prediction that does not require labelled data. An alternate technique is to use "supervised" approaches that learn models from project data labelled with, say, "defective" or "not-defective". Most researchers use these supervised models since, it is argued, they can exploit more knowledge of the projects. At FSE-16, Yang et al. reported startling results where unsupervised defect predictors outperformed supervised predictors for effort-aware just-in-time defect prediction. If confirmed, these results would lead to a dramatic simplification of a seemingly complex task (data mining) that is widely explored in the software engineering literature. This paper repeats and refutes those results as follows. (1) There is much variability in the efficacy of the Yang et al. predictors so even with their approach, some supervised data is required to prune weaker predictors away. (2) Their findings were grouped across N projects. When we repeat their analysis on a project-by-project basis, supervised predictors are seen to work better. Even though this paper rejects the specific conclusions of Yang et al., we still endorse their general goal. In our our experiments, supervised predictors did not perform outstandingly better than unsupervised ones for effort-aware just-in-time defect prediction. Hence, they may indeed be some combination of unsupervised learners to achieve comparable performance to supervised ones. We therefore encourage others to work in this promising area.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {72–83},
numpages = {12},
keywords = {software repository mining, empirical studies, defect prediction, data analytics for software engineering},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3028842.3028859,
author = {Gao, Yan and Yang, Chunhui},
title = {Software defect prediction based on manifold learning in subspace selection},
year = {2016},
isbn = {9781450347990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3028842.3028859},
doi = {10.1145/3028842.3028859},
abstract = {Software defects will lead to software running error and system crashes. In order to detect software defect as early as possible at early stage of software development, a series of machine learning approaches have been studied and applied to predict defects in software modules. Unfortunately, the imbalanceof software defect datasets brings great challenge to software defect prediction model training. In this paper, a new manifold learning based subspace learning algorithm, Discriminative Locality Alignment(DLA), is introduced into software defects prediction. Experimental results demonstrate that DLA is consistently superior to LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) in terms of discriminate information extraction and prediction performance. In addition, DLA reveals some attractive intrinsic properties for numeric calculation, e.g. it can overcome the matrix singular problem and small sample size problem in software defect prediction.},
booktitle = {Proceedings of the 1st International Conference on Intelligent Information Processing},
articleno = {17},
numpages = {6},
keywords = {support vector machine, software defect prediction, manifold learning, discriminative locality alignment},
location = {Wuhan, China},
series = {ICIIP '16}
}

@article{10.1016/j.infsof.2017.11.008,
author = {Tong, Haonan and Liu, Bin and Wang, Shihai},
title = {Software defect prediction using stacked denoising autoencoders and two-stage ensemble learning},
year = {2018},
issue_date = {Apr 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {96},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.11.008},
doi = {10.1016/j.infsof.2017.11.008},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {94–111},
numpages = {18},
keywords = {Deep learning, Software metrics, Ensemble learning, Stacked denoising autoencoders, Software defect prediction}
}

@article{10.1155/2019/2384706,
author = {Yang, Xingguang and Yu, Huiqun and Fan, Guisheng and Shi, Kai and Chen, Liqiong and Tramontana, Emiliano},
title = {Local versus Global Models for Just-In-Time Software Defect Prediction},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1058-9244},
url = {https://doi.org/10.1155/2019/2384706},
doi = {10.1155/2019/2384706},
abstract = {Just-in-time software defect prediction (JIT-SDP) is an active topic in software defect prediction, which aims to identify defect-inducing changes. Recently, some studies have found that the variability of defect data sets can affect the performance of defect predictors. By using local models, it can help improve the performance of prediction models. However, previous studies have focused on module-level defect prediction. Whether local models are still valid in the context of JIT-SDP is an important issue. To this end, we compare the performance of local and global models through a large-scale empirical study based on six open-source projects with 227417 changes. The experiment considers three evaluation scenarios of cross-validation, cross-project-validation, and timewise-cross-validation. To build local models, the experiment uses the k-medoids to divide the training set into several homogeneous regions. In addition, logistic regression and effort-aware linear regression (EALR) are used to build classification models and effort-aware prediction models, respectively. The empirical results show that local models perform worse than global models in the classification performance. However, local models have significantly better effort-aware prediction performance than global models in the cross-validation and cross-project-validation scenarios. Particularly, when the number of clusters k is set to 2, local models can obtain optimal effort-aware prediction performance. Therefore, local models are promising for effort-aware JIT-SDP.},
journal = {Sci. Program.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/3383219.3383281,
author = {Khan, Bilal and Iqbal, Danish and Badshah, Sher},
title = {Cross-Project Software Fault Prediction Using Data Leveraging Technique to Improve Software Quality},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383281},
doi = {10.1145/3383219.3383281},
abstract = {Software fault prediction is a process to detect bugs in software projects. Fault prediction in software engineering has attracted much attention from the last decade. The early prognostication of faults in software minimize the cost and effort of errors that come at later stages. Different machine learning techniques have been utilized for fault prediction, that is proven to be utilizable. Despite, the significance of fault prediction most of the companies do not consider fault prediction in practice and do not build useful models due to lack of data or lack of enough data to strengthen the power of fault predictors. However, models trained and tested on less amount of data are difficult to generalize, because they do not consider project size, project differences, and features selection. To overcome these issues, we proposed an instance-based transfer learning through data leveraging using logistic linear regression as a base proposed statistical methodology. In our study, we considered three software projects within the same domain. Finally, we performed a comparative analysis of three different experiments for building models (targeted project). The experimental results of the proposed approach show promising improvements in (SFP).},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {434–438},
numpages = {5},
keywords = {data leveraging, Software fault prediction, Software Quality, Machine learning, Instance-based learning, Cross-project},
location = {Trondheim, Norway},
series = {EASE '20}
}

@article{10.1504/ijcat.2019.100297,
author = {Jayanthi, R. and Florence, M. Lilly},
title = {Improved Bayesian regularisation using neural networks based on feature selection for software defect prediction},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {60},
number = {3},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2019.100297},
doi = {10.1504/ijcat.2019.100297},
abstract = {Demand for software-based applications has grown drastically in various real-time applications. However, software testing schemes have been developed which include manual and automatic testing. Manual testing requires human effort and chances of error may still affect the quality of software. To overcome this issue, automatic software testing techniques based on machine learning techniques have been developed. In this work, we focus on the machine learning scheme for early prediction of software defects using Levenberg-Marquardt algorithm (LM), Back Propagation (BP) and Bayesian Regularisation (BR) techniques. Bayesian regularisation achieves better performance in terms of bug prediction. However, this performance can be enhanced further. Hence, we developed a novel approach for attribute selection-based feature selection technique to improve the performance of BR classification. An extensive study is carried out with the PROMISE repository where we considered KC1 and JM1 datasets. Experimental study shows that the proposed approach achieves better performance in predicting the defects in software.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {225–241},
numpages = {16},
keywords = {adaptive computation process, cross entropy error function, feature subset selection, gradient-based approach, gradient descent optimisation, software metrics, software defect prediction, machine learning techniques, defect prediction model}
}

@article{10.1016/j.infsof.2019.07.003,
author = {Zhou, Tianchi and Sun, Xiaobing and Xia, Xin and Li, Bin and Chen, Xiang},
title = {Improving defect prediction with deep forest},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.07.003},
doi = {10.1016/j.infsof.2019.07.003},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {204–216},
numpages = {13},
keywords = {Empirical evaluation, Cascade strategy, Deep forest, Software defect prediction}
}

@inproceedings{10.1007/978-3-030-59003-1_27,
author = {Shakhovska, Natalya and Yakovyna, Vitaliy and Kryvinska, Natalia},
title = {An Improved Software Defect Prediction Algorithm Using Self-organizing Maps Combined with Hierarchical Clustering and Data Preprocessing},
year = {2020},
isbn = {978-3-030-59002-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59003-1_27},
doi = {10.1007/978-3-030-59003-1_27},
abstract = {An improved software defects prediction algorithm based on combination of Kohonen map and hierarchical clustering is presented in this paper. The need for software reliability assessment and analysis growths rapidly due to increasing dependence of our day-to-day life on software-controlled devices and systems. Software reliability prediction is the only tool available at early stage of software development lifecycle when the debugging cost risk of faulty operation is minimal. Artificial intelligence and machine learning in particular are promising techniques to solve this task. Various classification methods have been used previously to build software defect prediction models, ranging from simple, like logistic regression, to advanced methods, e.g. multivariate adaptive regression splicing. However, the available literature still does not allow to make unambiguous conclusion concerning the choice of the best classifier and trying different dimensions to overcome potential bias is suggested. The purpose of the paper is to analyze the software code metrics to find dependences be-tween software module’s defect-proneness and its metrics. JM1 public NASA dataset from PROMISE Software Engineering Repository was used in this study. To increase the classification accuracy, we combine self-organizing maps with hierarchical clustering and data preprocessing.},
booktitle = {Database and Expert Systems Applications: 31st International Conference, DEXA 2020, Bratislava, Slovakia, September 14–17, 2020, Proceedings, Part I},
pages = {414–424},
numpages = {11},
keywords = {Prediction algorithm, Hierarchical clustering, Software defect analysis},
location = {Bratislava, Slovakia}
}

@article{10.1016/j.jss.2019.03.012,
author = {Ni, Chao and Chen, Xiang and Wu, Fangfang and Shen, Yuxiang and Gu, Qing},
title = {An empirical study on pareto based multi-objective feature selection for software defect prediction},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.012},
doi = {10.1016/j.jss.2019.03.012},
journal = {J. Syst. Softw.},
month = jun,
pages = {215–238},
numpages = {24},
keywords = {Empirical study, Multi-Objective optimization, Feature selection, Search based software engineering, Software defect prediction, xx-xx, xx-xx}
}

@inproceedings{10.1145/2810146.2810150,
author = {Mahmood, Zaheed and Bowes, David and Lane, Peter C. R. and Hall, Tracy},
title = {What is the Impact of Imbalance on Software Defect Prediction Performance?},
year = {2015},
isbn = {9781450337151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810146.2810150},
doi = {10.1145/2810146.2810150},
abstract = {Software defect prediction performance varies over a large range. Menzies suggested there is a ceiling effect of 80% Recall [8]. Most of the data sets used are highly imbalanced. This paper asks, what is the empirical effect of using different datasets with varying levels of imbalance on predictive performance? We use data synthesised by a previous meta-analysis of 600 fault prediction models and their results. Four model evaluation measures (the Mathews Correlation Coefficient (MCC), F-Measure, Precision and Recall) are compared to the corresponding data imbalance ratio. When the data are imbalanced, the predictive performance of software defect prediction studies is low. As the data become more balanced, the predictive performance of prediction models increases, from an average MCC of 0.15, until the minority class makes up 20% of the instances in the dataset, where the MCC reaches an average value of about 0.34. As the proportion of the minority class increases above 20%, the predictive performance does not significantly increase. Using datasets with more than 20% of the instances being defective has not had a significant impact on the predictive performance when using MCC. We conclude that comparing the results of defect prediction studies should take into account the imbalance of the data.},
booktitle = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {4},
numpages = {4},
keywords = {Data Imbalance, Defect Prediction, Machine Learning},
location = {Beijing, China},
series = {PROMISE '15}
}

@inproceedings{10.1109/ICSE.2019.00076,
author = {Cabral, George G. and Minku, Leandro L. and Shihab, Emad and Mujahid, Suhaib},
title = {Class imbalance evolution and verification latency in just-in-time software defect prediction},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00076},
doi = {10.1109/ICSE.2019.00076},
abstract = {Just-in-Time Software Defect Prediction (JIT-SDP) is an SDP approach that makes defect predictions at the software change level. Most existing JIT-SDP work assumes that the characteristics of the problem remain the same over time. However, JIT-SDP may suffer from class imbalance evolution. Specifically, the imbalance status of the problem (i.e., how much underrepresented the defect-inducing changes are) may be intensified or reduced over time. If occurring, this could render existing JIT-SDP approaches unsuitable, including those that rebuild classifiers over time using only recent data. This work thus provides the first investigation of whether class imbalance evolution poses a threat to JIT-SDP. This investigation is performed in a realistic scenario by taking into account verification latency - the often overlooked fact that labeled training examples arrive with a delay. Based on 10 GitHub projects, we show that JIT-SDP suffers from class imbalance evolution, significantly hindering the predictive performance of existing JIT-SDP approaches. Compared to state-of-the-art class imbalance evolution learning approaches, the predictive performance of JIT-SDP approaches was up to 97.2% lower in terms of g-mean. Hence, it is essential to tackle class imbalance evolution in JIT-SDP. We then propose a novel class imbalance evolution approach for the specific context of JIT-SDP. While maintaining top ranked g-means, this approach managed to produce up to 63.59% more balanced recalls on the defect-inducing and clean classes than state-of-the-art class imbalance evolution approaches. We thus recommend it to avoid overemphasizing one class over the other in JIT-SDP.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {666–676},
numpages = {11},
keywords = {verification latency, software defect prediction, online learning, ensembles, concept drift, class imbalance},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1002/stvr.1610,
author = {Wang, Tiejian and Zhang, Zhiwu and Jing, Xiaoyuan and Liu, Yanli},
title = {Non-negative sparse-based SemiBoost for software defect prediction},
year = {2016},
issue_date = {November 2016},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {26},
number = {7},
issn = {0960-0833},
url = {https://doi.org/10.1002/stvr.1610},
doi = {10.1002/stvr.1610},
abstract = {Software defect prediction is an important decision support activity in software quality assurance. The limitation of the labelled modules usually makes the prediction difficult, and the class-imbalance characteristic of software defect data leads to negative influence on decision of classifiers. Semi-supervised learning can build high-performance classifiers by using large amount of unlabelled modules together with the labelled modules. Ensemble learning achieves a better prediction capability for class-imbalance data by using a series of weak classifiers to reduce the bias generated by the majority class. In this paper, we propose a new semi-supervised software defect prediction approach, non-negative sparse-based SemiBoost learning. The approach is capable of exploiting both labelled and unlabelled data and is formulated in a boosting framework. In order to enhance the prediction ability, we design a flexible non-negative sparse similarity matrix, which can fully exploit the similarity of historical data by incorporating the non-negativity constraint into sparse learning for better learning the latent clustering relationship among software modules. The widely used datasets from NASA projects are employed as test data to evaluate the performance of all compared methods. Experimental results show that non-negative sparse-based SemiBoost learning outperforms several representative state-of-the-art semi-supervised software defect prediction methods. Copyright © 2016 John Wiley &amp; Sons, Ltd.},
journal = {Softw. Test. Verif. Reliab.},
month = nov,
pages = {498–515},
numpages = {18},
keywords = {software defect prediction, semi-supervised learning, non-negative sparse based SemiBoost NSSB, ensemble learning}
}

@inproceedings{10.4108/icst.bict.2014.257871,
author = {Malhotra, Ruchika and Raje, Rajeev},
title = {An empirical comparison of machine learning techniques for software defect prediction},
year = {2014},
isbn = {9781631900532},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.bict.2014.257871},
doi = {10.4108/icst.bict.2014.257871},
abstract = {Software systems are exposed to various types of defects. The timely identification of defective classes is essential in early phases of software development to reduce the cost of testing the software. This will guide the software practitioners and researchers for planning of the proper allocation of testing resources. Software metrics can be used in conjunction with defect data to develop models for predicting defective classes. There have been various machine learning techniques proposed in the literature for analyzing complex relationships and extracting useful information from problems in less time. However, more studies comparing these techniques are needed to provide evidence so that confidence is established on the performance of one technique over the other. In this paper we address four issues (i) comparison of the machine learning techniques over unpopular used data sets (ii) use of inappropriate performance measures for measuring the performance of defect prediction models (iii) less use of statistical tests and (iv) validation of models from the same data set from which they are trained. To resolve these issues, in this paper, we compare 18 machine learning techniques for investigating the effect of Object-Oriented metrics on defective classes. The results are validated on six releases of the 'MMS' application package of recent widely used mobile operating system -- Android. The overall results of the study indicate the predictive capability of the machine learning techniques and an endorsement of one particular ML technique to predict defects.},
booktitle = {Proceedings of the 8th International Conference on Bioinspired Information and Communications Technologies},
pages = {320–327},
numpages = {8},
keywords = {object-oriented metrics, machine learning, empirical validation, defect prediction},
location = {Boston, Massachusetts},
series = {BICT '14}
}

@article{10.4018/IJOSSP.2018010101,
author = {Kakkar, Misha and Jain, Sarika and Bansal, Abhay and Grover, P.S.},
title = {Combining Data Preprocessing Methods With Imputation Techniques for Software Defect Prediction},
year = {2018},
issue_date = {January 2018},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2018010101},
doi = {10.4018/IJOSSP.2018010101},
abstract = {Software Defect Prediction SDP models are used to predict, whether software is clean or buggy using the historical data collected from various software repositories. The data collected from such repositories may contain some missing values. In order to estimate missing values, imputation techniques are used, which utilizes the complete observed values in the dataset. The objective of this study is to identify the best-suited imputation technique for handling missing values in SDP dataset. In addition to identifying the imputation technique, the authors have investigated for the most appropriate combination of imputation technique and data preprocessing method for building SDP model. In this study, four combinations of imputation technique and data preprocessing methods are examined using the improved NASA datasets. These combinations are used along with five different machine-learning algorithms to develop models. The performance of these SDP models are then compared using traditional performance indicators. Experiment results show that among different imputation techniques, linear regression gives the most accurate imputed value. The combination of linear regression with correlation based feature selector outperforms all other combinations. To validate the significance of data preprocessing methods with imputation the findings are applied to open source projects. It was concluded that the result is in consistency with the above conclusion.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {1–19},
numpages = {19},
keywords = {Software Defect Prediction, Missing Value Imputation, Instance Selection, Feature Selection}
}

@article{10.1007/s00607-016-0538-1,
author = {Gupta, Shivani and Gupta, Atul},
title = {A set of measures designed to identify overlapped instances in software defect prediction},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {99},
number = {9},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-016-0538-1},
doi = {10.1007/s00607-016-0538-1},
abstract = {The performance of the learning models will intensely rely on the characteristics of the training data. The previous outcomes recommend that the overlapping between classes and the presence of noise have the most grounded impact on the performance of learning algorithm, and software defect datasets are no exceptions. The class overlap problem is concerned with the performance of machine learning classifiers critical problem is class overlap in which data samples appear as valid examples of more than one class which may be responsible for the presence of noise in datasets. We aim to investigate how the presence of overlapped instances in a dataset influences the classifier's performance, and how to deal with class overlapping problem. To have a close estimate of class overlapping, we have proposed four different measures namely, nearest enemy ratio, subconcept ratio, likelihood ratio and soft margin ratio. We performed our investigations using 327 binary defect classification datasets obtained from 54 software projects, where we first identified overlapped datasets using three data complexity measures proposed in the literature. We also include treatment effort into the prediction process. Subsequently, we used our proposed measures to find overlapped instances in the identified overlapped datasets. Our results indicated that by training a classifier on a training data free from overlapped instances led to an improved classifier performance on the test data containing overlapped instances. The classifiers perform significantly better when the evaluation measure takes the effort into account.},
journal = {Computing},
month = sep,
pages = {889–914},
numpages = {26},
keywords = {Software defect prediction, Machine learning, Data mining, Data complexity measures, Class overlapping}
}

@article{10.1155/2020/8852705,
author = {Zheng, Shang and Gai, Jinjing and Yu, Hualong and Zou, Haitao and Gao, Shang and Briola, Daniela},
title = {Software Defect Prediction Based on Fuzzy Weighted Extreme Learning Machine with Relative Density Information},
year = {2020},
issue_date = {2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
issn = {1058-9244},
url = {https://doi.org/10.1155/2020/8852705},
doi = {10.1155/2020/8852705},
abstract = {To identify software modules that are more likely to be defective, machine learning has been used to construct software defect prediction (SDP) models. However, several previous works have found that the imbalanced nature of software defective data can decrease the model performance. In this paper, we discussed the issue of how to improve imbalanced data distribution in the context of SDP, which can benefit software defect prediction with the aim of finding better methods. Firstly, a relative density was introduced to reflect the significance of each instance within its class, which is irrelevant to the scale of data distribution in feature space; hence, it can be more robust than the absolute distance information. Secondly, a K-nearest-neighbors-based probability density estimation (KNN-PDE) alike strategy was utilised to calculate the relative density of each training instance. Furthermore, the fuzzy memberships of sample were designed based on relative density in order to eliminate classification error coming from noise and outlier samples. Finally, two algorithms were proposed to train software defect prediction models based on the weighted extreme learning machine. This paper compared the proposed algorithms with traditional SDP methods on the benchmark data sets. It was proved that the proposed methods have much better overall performance in terms of the measures including G-mean, AUC, and Balance. The proposed algorithms are more robust and adaptive for SDP data distribution types and can more accurately estimate the significance of each instance and assign the identical total fuzzy coefficients for two different classes without considering the impact of data scale.},
journal = {Sci. Program.},
month = jan,
numpages = {18}
}

@inproceedings{10.1145/3028842.3028858,
author = {Gao, Yan and Yang, Chunhui and Liang, Lixin},
title = {Pseudo-samples generation in Gaussian mixture distribution for software defect prediction},
year = {2016},
isbn = {9781450347990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3028842.3028858},
doi = {10.1145/3028842.3028858},
abstract = {In this paper, we present GCRF method based on pseudo-samples generation and conditional random field (CRF) for software defect prediction in Gaussian Mixture Distribution. In the proposed method, firstly, we leverage Gaussian Mixture Distribution (GMM) to generate pseudo-samples, which can increase the samples of minority class for balancing the train dataset. Secondly, we propose to apply CRF model in the balanced train dataset because the CRF model can handle complex features in nonlinear high dimensional subspace. Moreover, in order to avoid explicit modeling of the observed data, the proposed method can incorporate the classification of software defect data with different statistics characteristics into a unified probabilistic framework. Interestingly, the experiments show that the GCRF method achieves much better prediction performance than the other approach as shown in the software defect data classification task.},
booktitle = {Proceedings of the 1st International Conference on Intelligent Information Processing},
articleno = {16},
numpages = {6},
keywords = {software defect prediction, imbalance distribution, gaussian mixture distribution, conditional random field},
location = {Wuhan, China},
series = {ICIIP '16}
}

@proceedings{10.1145/3472674,
title = {MaLTESQuE 2021: Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the fifth edition of the workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE 2021) to be held virtually on August 23, 2021, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2021).},
location = {Athens, Greece}
}

@article{10.1155/2021/5069016,
author = {Balogun, Abdullateef O. and Basri, Shuib and Mahamad, Saipunidzam and Capretz, Luiz Fernando and Imam, Abdullahi Abubakar and Almomani, Malek A. and Adeyemo, Victor E. and Kumar, Ganesh and Dourado, Ant\'{o}nio},
title = {A Novel Rank Aggregation-Based Hybrid Multifilter Wrapper Feature Selection Method in Software Defect Prediction},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/5069016},
doi = {10.1155/2021/5069016},
abstract = {The high dimensionality of software metric features has long been noted as a data quality problem that affects the performance of software defect prediction (SDP) models. This drawback makes it necessary to apply feature selection (FS) algorithm(s) in SDP processes. FS approaches can be categorized into three types, namely, filter FS (FFS), wrapper FS (WFS), and hybrid FS (HFS). HFS has been established as superior because it combines the strength of both FFS and WFS methods. However, selecting the most appropriate FFS (filter rank selection problem) for HFS is a challenge because the performance of FFS methods depends on the choice of datasets and classifiers. In addition, the local optima stagnation and high computational costs of WFS due to large search spaces are inherited by the HFS method. Therefore, as a solution, this study proposes a novel rank aggregation-based hybrid multifilter wrapper feature selection (RAHMFWFS) method for the selection of relevant and irredundant features from software defect datasets. The proposed RAHMFWFS is divided into two stepwise stages. The first stage involves a rank aggregation-based multifilter feature selection (RMFFS) method that addresses the filter rank selection problem by aggregating individual rank lists from multiple filter methods, using a novel rank aggregation method to generate a single, robust, and non-disjoint rank list. In the second stage, the aggregated ranked features are further preprocessed by an enhanced wrapper feature selection (EWFS) method based on a dynamic reranking strategy that is used to guide the feature subset selection process of the HFS method. This, in turn, reduces the number of evaluation cycles while amplifying or maintaining its prediction performance. The feasibility of the proposed RAHMFWFS was demonstrated on benchmarked software defect datasets with Na\"{\i}ve Bayes and Decision Tree classifiers, based on accuracy, the area under the curve (AUC), and F-measure values. The experimental results showed the effectiveness of RAHMFWFS in addressing filter rank selection and local optima stagnation problems in HFS, as well as the ability to select optimal features from SDP datasets while maintaining or enhancing the performance of SDP models. To conclude, the proposed RAHMFWFS achieved good performance by improving the prediction performances of SDP models across the selected datasets, compared to existing state-of-the-arts HFS methods.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@article{10.1049/sfw2.12012,
author = {Zou, Quanyi and Lu, Lu and Qiu, Shaojian and Gu, Xiaowei and Cai, Ziyi},
title = {Correlation feature and instance weights transfer learning for cross project software defect prediction},
year = {2021},
issue_date = {February 2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {15},
number = {1},
url = {https://doi.org/10.1049/sfw2.12012},
doi = {10.1049/sfw2.12012},
abstract = {Due to the differentiation between training and testing data in the feature space, cross‐project defect prediction (CPDP) remains unaddressed within the field of traditional machine learning. Recently, transfer learning has become a research hot‐spot for building classifiers in the target domain using the data from the related source domains. To implement better CPDP models, recent studies focus on either feature transferring or instance transferring to weaken the impact of irrelevant cross‐project data. Instead, this work proposes a dual weighting mechanism to aid the learning process, considering both feature transferring and instance transferring. In our method, a local data gravitation between source and target domains determines instance weight, while features that are highly correlated with the learning task, uncorrelated with other features and minimizing the difference between the domains are rewarded with a higher feature weight. Experiments on 25 real‐world datasets indicate that the proposed approach outperforms the existing CPDP methods in most cases. By assigning weights based on the different contribution of features and instances to the predictor, the proposed approach is able to build a better CPDP model and demonstrates substantial improvements over the state‐of‐the‐art CPDP models.},
journal = {IET Software},
month = jan,
pages = {55–74},
numpages = {20},
keywords = {software reliability, pattern classification, learning (artificial intelligence)}
}

@inproceedings{10.1145/2568225.2568320,
author = {Jing, Xiao-Yuan and Ying, Shi and Zhang, Zhi-Wu and Wu, Shan-Shan and Liu, Jin},
title = {Dictionary learning based software defect prediction},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568320},
doi = {10.1145/2568225.2568320},
abstract = {In order to improve the quality of a software system, software defect prediction aims to automatically identify defective software modules for efficient software test. To predict software defect, those classification methods with static code attributes have attracted a great deal of attention. In recent years, machine learning techniques have been applied to defect prediction. Due to the fact that there exists the similarity among different software modules, one software module can be approximately represented by a small proportion of other modules. And the representation coefficients over the pre-defined dictionary, which consists of historical software module data, are generally sparse. In this paper, we propose to use the dictionary learning technique to predict software defect. By using the characteristics of the metrics mined from the open source software, we learn multiple dictionaries (including defective module and defective-free module sub-dictionaries and the total dictionary) and sparse representation coefficients. Moreover, we take the misclassification cost issue into account because the misclassification of defective modules generally incurs much higher risk cost than that of defective-free ones. We thus propose a cost-sensitive discriminative dictionary learning (CDDL) approach for software defect classification and prediction. The widely used datasets from NASA projects are employed as test data to evaluate the performance of all compared methods. Experimental results show that CDDL outperforms several representative state-of-the-art defect prediction methods.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {414–423},
numpages = {10},
keywords = {sparse representation, dictionary learning, cost-sensitive discriminative dictionary learning (CDDL), Software defect prediction},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3220267.3220286,
author = {El-Shorbagy, Sara Adel and El-Gammal, Wael Mohamed and Abdelmoez, Walid M.},
title = {Using SMOTE and Heterogeneous Stacking in Ensemble learning for Software Defect Prediction},
year = {2018},
isbn = {9781450364690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220267.3220286},
doi = {10.1145/3220267.3220286},
abstract = {Nowadays, there are a lot of classifications models used for predictions in the software engineering field such as effort estimation and defect prediction. One of these models is the ensemble learning machine that improves model performance by combining multiple models in different ways to get a more powerful model.One of the problems facing the prediction model is the misclassification of the minority samples. This problem mainly appears in the case of defect prediction. Our aim is the classification of defects which are considered minority samples during the training phase. This can be improved by implementing the Synthetic Minority Over-Sampling Technique (SMOTE) before the implementation of the ensemble model which leads to over-sample the minority class instances.In this paper, our work propose applying a new ensemble model by combining the SMOTE technique with the heterogeneous stacking ensemble to get the most benefit and performance in training a dataset that focus on the minority subset as in the software prediction study. Our proposed model shows better performance that overcomes other techniques results applied on the minority samples of the defect prediction.},
booktitle = {Proceedings of the 7th International Conference on Software and Information Engineering},
pages = {44–47},
numpages = {4},
keywords = {Stacking, Software Engineering, SMOTE, Machine Learning, Heterogeneous, Ensemble, Defect Prediction, Classification},
location = {Cairo, Egypt},
series = {ICSIE '18}
}

@inproceedings{10.1145/3239576.3239607,
author = {Du, Yuntao and Zhang, Lu and Shi, Jiahao and Tang, Jingjuan and Yin, Ying},
title = {Feature-Grouping-Based Two Steps Feature Selection Algorithm in Software Defect Prediction},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239607},
doi = {10.1145/3239576.3239607},
abstract = {In order to improve the effect of software defect prediction, many algorithms including feature selection, have been proposed. Based on Wrapper and Filter hybrid framework, a feature-grouping-based feature selection algorithm is proposed in this paper. The algorithm is composed of two steps. In the first step, in order to remove the redundant features, we group the features according to the redundancy between the features. The symmetry uncertainty is used as the constant indicator of the correlation and the FCBF-based grouping algorithm is used to group the features. In the second step, a subset of the features are selected from each group to form the final subset of features. Many classical methods select the representative feature from each group. We consider that when the number of intra-group features is large, the representative features are not enough to reflect the information in this group. Therefore, we require that at least one feature be selected within each group, in this step, the PSO algorithm is used for Searching Randomly from each group. We tested on the open source NASA and PROMISE data sets. Using three kinds of classifier. Compared to the other methods tested in this article, our method resulted in 90% improvement in the predictive performance of 30 sets of results on 10 data sets. Compared with the algorithms without feature selection, the AUC values of this method in the Logistic regression, Naive Bayesian, and K-neighbor classifiers are improved by 5.94% and 4.69% And 8.05%. The FCBF algorithm can also be regarded as a kind of first performing feature grouping. Compared with the FCBF algorithm, the AUC values of this method are improved by 4.78%, 6.41% and 4.4% on the basis of Logistic regression, Naive Bayes and K-neighbor. We can also see that for the FCBF-based grouping algorithm, it could be better to choose a characteristic cloud from each group than to choose a representative one.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {173–178},
numpages = {6},
keywords = {Software defect prediction, PSO, Intra-group feature selection, Feature grouping, FCBF-based grouping algorithm},
location = {Chengdu, China},
series = {ICAIP '18}
}

@article{10.1016/j.asoc.2017.05.043,
author = {Arar, mer Faruk and Ayan, Krat},
title = {A feature dependent Naive Bayes approach and its application to the software defect prediction problem},
year = {2017},
issue_date = {October 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.05.043},
doi = {10.1016/j.asoc.2017.05.043},
abstract = {Display Omitted In Naive Bayes, features are assumed to be independent and have equal weight. But, In practice, features are interrelated.In this study, features are included for calculation as pairs using the proposed Feature Dependent Naive Bayes (FDNB) method.Eight data sets from the NASA PROMISE repository were used for the software defect prediction problem.Results were compared with other modified NBs. Increased classification performance was found after use of the proposed FDNB. Naive Bayes is one of the most widely used algorithms in classification problems because of its simplicity, effectiveness, and robustness. It is suitable for many learning scenarios, such as image classification, fraud detection, web mining, and text classification. Naive Bayes is a probabilistic approach based on assumptions that features are independent of each other and that their weights are equally important. However, in practice, features may be interrelated. In that case, such assumptions may cause a dramatic decrease in performance. In this study, by following preprocessing steps, a Feature Dependent Naive Bayes (FDNB) classification method is proposed. Features are included for calculation as pairs to create dependence between one another. This method was applied to the software defect prediction problem and experiments were carried out using widely recognized NASA PROMISE data sets. The obtained results show that this new method is more successful than the standard Naive Bayes approach and that it has a competitive performance with other feature-weighting techniques. A further aim of this study is to demonstrate that to be reliable, a learning model must be constructed by using only training data, as otherwise misleading results arise from the use of the entire data set.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {197–209},
numpages = {13},
keywords = {Software defect prediction, Naive Bayes, Feature independence, Discretization, Data mining}
}

@article{10.1007/s00500-018-3546-6,
author = {Khuat, Thanh Tung and Le, My Hanh},
title = {Binary teaching–learning-based optimization algorithm with a new update mechanism for sample subset optimization in software defect prediction},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {20},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-018-3546-6},
doi = {10.1007/s00500-018-3546-6},
abstract = {Software defect prediction has gained considerable attention in recent years. A broad range of computational methods has been developed for accurate prediction of faulty modules based on code and design metrics. One of the challenges in training classifiers is the highly imbalanced class distribution in available datasets, leading to an undesirable bias in the prediction performance for the minority class. Data sampling is a widespread technique to tackle this problem. However, traditional sampling methods, which depend mainly on random resampling from a given dataset, do not take advantage of useful information available in training sets, such as sample quality and representative instances. To cope with this limitation, evolutionary undersampling methods are usually used for identifying an optimal sample subset for the training dataset. This paper proposes a binary teaching–learning- based optimization algorithm employing a distribution-based solution update rule, namely BTLBOd, to generate a balanced subset of highly valuable examples. This subset is then applied to train a classifier for reliable prediction of potentially defective modules in a software system. Each individual in BTLBOd includes two vectors: a real-valued vector generated by the distribution-based update mechanism, and a binary vector produced from the corresponding real vector by a proposed mapping function. Empirical results showed that the optimal sample subset produced by BTLBOd might ameliorate the classification accuracy of the predictor on highly imbalanced software defect data. Obtained results also demonstrated the superior performance of the proposed sampling method compared to other popular sampling techniques.},
journal = {Soft Comput.},
month = oct,
pages = {9919–9935},
numpages = {17},
keywords = {Software defect prediction, Imbalanced learning, Sample subset optimization, Distribution-based update, Binary teaching–learning-based optimization, Teaching–learning-based optimization}
}

@inproceedings{10.1145/2591062.2591151,
author = {Jing, Xiao-Yuan and Zhang, Zhi-Wu and Ying, Shi and Wang, Feng and Zhu, Yang-Ping},
title = {Software defect prediction based on collaborative representation classification},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591151},
doi = {10.1145/2591062.2591151},
abstract = {In recent years, machine learning techniques have been successfully applied into software defect prediction. Although they can yield reasonably good prediction results, there still exists much room for improvement on the aspect of prediction accuracy. Sparse representation is one of the most advanced machine learning techniques. It performs well with respect to signal compression and classification, but suffers from its time-consuming sparse coding. Compared with sparse representation, collaborative representation classification (CRC) can yield significantly lower computational complexity and competitive classification performance in pattern recognition domains. To achieve better defect prediction results, we introduce the CRC technique in this paper and propose a CRC based software defect prediction (CSDP) approach. We first design a CRC based learner to build a prediction model, whose computational burden is low. Then, we design a CRC based predictor to classify whether the query software modules are defective or defective-free. Experimental results on the widely used NASA datasets demonstrate the effectiveness and efficiency of the proposed approach.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {632–633},
numpages = {2},
keywords = {Software defect prediction, Prediction model, Machine learning, Collaborative representation classification},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.5555/3432601.3432618,
author = {Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Report on evaluation experiments using different machine learning techniques for defect prediction},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {With the emergence of AI, it is of no surprise that the application of Machine Learning techniques has attracted the attention of numerous software maintenance groups around the world. For defect proneness classification in particular, the use of Machine Learning classifiers has been touted as a promising approach. As a consequence, a large volume of research works has been published in the related research literature, utilizing either proprietary data sets or the PROMISE data repository which, for the purposes of this study, focuses only on the use of source code metrics as defect prediction training features. It has been argued though by several researchers, that process metrics may provide a better option as training features than source code metrics. For this paper, we have conducted a detailed extraction of GitHub process metrics from 148 open source systems, and we report on the findings of experiments conducted by using different Machine Learning classification algorithms for defect proneness classification. The main purpose of the paper is not to propose yet another Machine Learning technique for defect proneness classification, but to present to the community a very large data set using process metrics as opposed to source code metrics, and draw some initial interesting conclusions from this statistically significant data set.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {123–132},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1145/2351676.2351734,
author = {Lu, Huihua and Cukic, Bojan and Culp, Mark},
title = {Software defect prediction using semi-supervised learning with dimension reduction},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351734},
doi = {10.1145/2351676.2351734},
abstract = {Accurate detection of fault prone modules offers the path to high quality software products while minimizing non essential assurance expenditures. This type of quality modeling requires the availability of software modules with known fault content developed in similar environment. Establishing whether a module contains a fault or not can be expensive. The basic idea behind semi-supervised learning is to learn from a small number of software modules with known fault content and supplement model training with modules for which the fault information is not available. In this study, we investigate the performance of semi-supervised learning for software fault prediction. A preprocessing strategy, multidimensional scaling, is embedded in the approach to reduce the dimensional complexity of software metrics. Our results show that the semi-supervised learning algorithm with dimension-reduction preforms significantly better than one of the best performing supervised learning algorithms, random forest, in situations when few modules with known fault content are available for training.},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {314–317},
numpages = {4},
keywords = {software metrics, semi-supervised learning, dimension reduction, Software fault prediction},
location = {Essen, Germany},
series = {ASE '12}
}

@article{10.1016/j.asoc.2017.01.050,
author = {Maua, Goran and Galinac Grbac, Tihana},
title = {Co-evolutionary multi-population genetic programming for classification in software defect prediction},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.01.050},
doi = {10.1016/j.asoc.2017.01.050},
abstract = {Evolving diverse ensembles using genetic programming has recently been proposed for classification problems with unbalanced data. Population diversity is crucial for evolving effective algorithms. Multilevel selection strategies that involve additional colonization and migration operations have shown better performance in some applications. Therefore, in this paper, we are interested in analysing the performance of evolving diverse ensembles using genetic programming for software defect prediction with unbalanced data by using different selection strategies. We use colonization and migration operators along with three ensemble selection strategies for the multi-objective evolutionary algorithm. We compare the performance of the operators for software defect prediction datasets with varying levels of data imbalance. Moreover, to generalize the results, gain a broader view and understand the underlying effects, we replicated the same experiments on UCI datasets, which are often used in the evolutionary computing community. The use of multilevel selection strategies provides reliable results with relatively fast convergence speeds and outperforms the other evolutionary algorithms that are often used in this research area and investigated in this paper. This paper also presented a promising ensemble strategy based on a simple convex hull approach and at the same time it raised the question whether ensemble strategy based on the whole population should also be investigated.},
journal = {Appl. Soft Comput.},
month = jun,
pages = {331–351},
numpages = {21},
keywords = {Software defect prediction, Genetic programming, Coevolution, Classification}
}

@inproceedings{10.1145/3489849.3489948,
author = {Lebiedz, Jacek and Wiszniewski, Bogdan},
title = {CAVE applications: from craft manufacturing to product line engineering},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489948},
doi = {10.1145/3489849.3489948},
abstract = {Product line engineering model is suitable for engineering related software products in an efficient manner, taking advantage of their similarities while managing their differences. Our feature driven software product line (SPL) solution based on that model allows for instantiation of different CAVE products based on the set of core assets and driven by a set of common VR features with the minimal budget and time to market.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {57},
numpages = {2},
keywords = {production stations, core assets, VR application features},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {prediction, feature, defect, classification},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.knosys.2015.10.009,
author = {Rana, Zeeshan Ali and Mian, M. Awais and Shamail, Shafay},
title = {Improving Recall of software defect prediction models using association mining},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {90},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2015.10.009},
doi = {10.1016/j.knosys.2015.10.009},
abstract = {Use of software product metrics in defect prediction studies highlights the utility of these metrics. Public availability of software defect data based on the product metrics has resulted in the development of defect prediction models. These models experience a limitation in learning Defect-prone (D) modules because the available datasets are imbalanced. Most of the datasets are dominated by Not Defect-prone (ND) modules as compared to D modules. This affects the ability of classification models to learn the D modules more accurately. This paper presents an association mining based approach that allows the defect prediction models to learn D modules in imbalanced datasets. The proposed algorithm preprocesses data by setting specific metric values as missing and improves the prediction of D modules. The proposed algorithm has been evaluated using 5 public datasets. A Naive Bayes (NB) classifier has been developed before and after the proposed preprocessing. It has been shown that Recall of the classifier after the proposed preprocessing has improved. Stability of the approach has been tested by experimenting the algorithm with different number of bins. The results show that the algorithm has resulted in up to 40% performance gain.},
journal = {Know.-Based Syst.},
month = dec,
pages = {1–13},
numpages = {13},
keywords = {Software defect prediction, PROMISE repository, Naive Bayes, Improving Recall, Imbalanced data, Association mining}
}

@inproceedings{10.1109/MSR.2019.00017,
author = {Dam, Hoa Khanh and Pham, Trang and Ng, Shien Wee and Tran, Truyen and Grundy, John and Ghose, Aditya and Kim, Taeksu and Kim, Chul-Joo},
title = {Lessons learned from using a deep tree-based model for software defect prediction in practice},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00017},
doi = {10.1109/MSR.2019.00017},
abstract = {Defects are common in software systems and cause many problems for software users. Different methods have been developed to make early prediction about the most likely defective modules in large codebases. Most focus on designing features (e.g. complexity metrics) that correlate with potentially defective code. Those approaches however do not sufficiently capture the syntax and multiple levels of semantics of source code, a potentially important capability for building accurate prediction models. In this paper, we report on our experience of deploying a new deep learning tree-based defect prediction model in practice. This model is built upon the tree-structured Long Short Term Memory network which directly matches with the Abstract Syntax Tree representation of source code. We discuss a number of lessons learned from developing the model and evaluating it on two datasets, one from open source projects contributed by our industry partner Samsung and the other from the public PROMISE repository.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {46–57},
numpages = {12},
keywords = {defect prediction, deep learning},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1007/978-3-030-91452-3_12,
author = {Amasaki, Sousuke and Aman, Hirohisa and Yokogawa, Tomoyuki},
title = {Searching for Bellwether Developers for Cross-Personalized Defect Prediction},
year = {2021},
isbn = {978-3-030-91451-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91452-3_12},
doi = {10.1007/978-3-030-91452-3_12},
abstract = {Context: Recent progress in the use of commit data for software defect prediction has driven research on personalized defect prediction. An idea applying one personalized model to another developer came in for seeking an alternative model predicting better than one’s own model. A question arose whether such exemplary developer (bellwether) existed as observed in traditional defect prediction. Objective: To investigate whether bellwether developers existed and how they behaved. Method: Experiments were conducted on 9 OSS projects. Models based on active developers in a project were compared with each other to seek bellwethers, whose models beaten models of the other active developers. Their performance was evaluated with new unseen data from the other active developers and the remaining non-active developers. Results: Bellwether developers were identified in all nine projects. Their performance on new unseen data from the other active developers was not higher than models learned by those developers. The bellwether was only a practical choice for the non-active developers. Conclusion: Bellwethers were a useful prediction model for the non-active developers but not for the other active developers.},
booktitle = {Product-Focused Software Process Improvement: 22nd International Conference, PROFES 2021, Turin, Italy, November 26, 2021, Proceedings},
pages = {183–198},
numpages = {16},
keywords = {Bellwether effect, Transfer learning, Personalized defect prediction},
location = {Turin, Italy}
}

@inproceedings{10.1145/2961111.2962610,
author = {Petri\'{c}, Jean and Bowes, David and Hall, Tracy and Christianson, Bruce and Baddoo, Nathan},
title = {Building an Ensemble for Software Defect Prediction Based on Diversity Selection},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962610},
doi = {10.1145/2961111.2962610},
abstract = {Background: Ensemble techniques have gained attention in various scientific fields. Defect prediction researchers have investigated many state-of-the-art ensemble models and concluded that in many cases these outperform standard single classifier techniques. Almost all previous work using ensemble techniques in defect prediction rely on the majority voting scheme for combining prediction outputs, and on the implicit diversity among single classifiers. Aim: Investigate whether defect prediction can be improved using an explicit diversity technique with stacking ensemble, given the fact that different classifiers identify different sets of defects. Method: We used classifiers from four different families and the weighted accuracy diversity (WAD) technique to exploit diversity amongst classifiers. To combine individual predictions, we used the stacking ensemble technique. We used state-of-the-art knowledge in software defect prediction to build our ensemble models, and tested their prediction abilities against 8 publicly available data sets. Conclusion: The results show performance improvement using stacking ensembles compared to other defect prediction models. Diversity amongst classifiers used for building ensembles is essential to achieving these performance improvements.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {46},
numpages = {10},
keywords = {stacking, software faults, ensembles of learning machines, diversity, Software defect prediction},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@article{10.1016/j.ins.2013.12.031,
author = {Czibula, Gabriela and Marian, Zsuzsanna and Czibula, Istvan Gergely},
title = {Software defect prediction using relational association rule mining},
year = {2014},
issue_date = {April, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {264},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2013.12.031},
doi = {10.1016/j.ins.2013.12.031},
abstract = {This paper focuses on the problem of defect prediction, a problem of major importance during software maintenance and evolution. It is essential for software developers to identify defective software modules in order to continuously improve the quality of a software system. As the conditions for a software module to have defects are hard to identify, machine learning based classification models are still developed to approach the problem of defect prediction. We propose a novel classification model based on relational association rules mining. Relational association rules are an extension of ordinal association rules, which are a particular type of association rules that describe numerical orderings between attributes that commonly occur over a dataset. Our classifier is based on the discovery of relational association rules for predicting whether a software module is or it is not defective. An experimental evaluation of the proposed model on the open source NASA datasets, as well as a comparison to similar existing approaches is provided. The obtained results show that our classifier overperforms, for most of the considered evaluation measures, the existing machine learning based techniques for defect prediction. This confirms the potential of our proposal.},
journal = {Inf. Sci.},
month = apr,
pages = {260–278},
numpages = {19},
keywords = {Software engineering, Defect prediction, Data mining, Association rule}
}

@article{10.1016/j.asoc.2021.107870,
author = {Kabir, Md Alamgir and Keung, Jacky and Turhan, Burak and Bennin, Kwabena Ebo},
title = {Inter-release defect prediction with feature selection using temporal chunk-based learning: An empirical study},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PA},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107870},
doi = {10.1016/j.asoc.2021.107870},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {17},
keywords = {Feature selection, Inter-release defect prediction, Software defect prediction}
}

@article{10.1007/s10664-021-09984-2,
author = {Ulan, Maria and L\"{o}we, Welf and Ericsson, Morgan and Wingkvist, Anna},
title = {Weighted software metrics aggregation and its application to defect prediction},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09984-2},
doi = {10.1007/s10664-021-09984-2},
abstract = {It is a well-known practice in software engineering to aggregate software metrics to assess software artifacts for various purposes, such as their maintainability or their proneness to contain bugs. For different purposes, different metrics might be relevant. However, weighting these software metrics according to their contribution to the respective purpose is a challenging task. Manual approaches based on experts do not scale with the number of metrics. Also, experts get confused if the metrics are not independent, which is rarely the case. Automated approaches based on supervised learning require reliable and generalizable training data, a ground truth, which is rarely available. We propose an automated approach to weighted metrics aggregation that is based on unsupervised learning. It sets metrics scores and their weights based on probability theory and aggregates them. To evaluate the effectiveness, we conducted two empirical studies on defect prediction, one on ca. 200 000 code changes, and another ca. 5 000 software classes. The results show that our approach can be used as an agnostic unsupervised predictor in the absence of a ground truth.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {34},
keywords = {Weighting, Aggregation, Software metrics, Defect prediction, Quantitative methods, Software assessment}
}

@article{10.1002/smr.2330,
author = {Shi, Ke and Lu, Yang and Liu, Guangliang and Wei, Zhenchun and Chang, Jingfei},
title = {MPT‐embedding: An unsupervised representation learning of code for software defect prediction},
year = {2021},
issue_date = {April 2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {33},
number = {4},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2330},
doi = {10.1002/smr.2330},
abstract = {Software project defect prediction can help developers allocate debugging resources. Existing software defect prediction models are usually based on machine learning methods, especially deep learning. Deep learning‐based methods tend to build end‐to‐end models that directly use source code‐based abstract syntax trees (ASTs) as input. They do not pay enough attention to the front‐end data representation. In this paper, we propose a new framework to represent source code called multiperspective tree embedding (MPT‐embedding), which is an unsupervised representation learning method. MPT‐embedding parses the nodes of ASTs from multiple perspectives and encodes the structural information of a tree into a vector sequence. Experiments on both cross‐project defect prediction (CPDP) and within‐project defect prediction (WPDP) show that, on average, MPT‐embedding provides improvements over the state‐of‐the‐art method.Source code‐based automatic representations are more objective and accurate than traditional handcrafted metrics. This article proposed a new framework to represent code called multiperspective tree embedding (MPT‐embedding), which is an unsupervised representation learning method. MPT‐embedding parses the nodes of ASTs from multiple perspectives and encodes the structural information of a tree into a vector sequence. Experiments on the tasks of defect prediction show the effectiveness of the model.


image
image},
journal = {J. Softw. Evol. Process},
month = apr,
numpages = {20},
keywords = {tree embedding, representation learning, defect prediction, deep learning}
}

@article{10.1016/j.jss.2019.110402,
author = {Xu, Zhou and Li, Shuai and Xu, Jun and Liu, Jin and Luo, Xiapu and Zhang, Yifeng and Zhang, Tao and Keung, Jacky and Tang, Yutian},
title = {LDFR: Learning deep feature representation for software defect prediction},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110402},
doi = {10.1016/j.jss.2019.110402},
journal = {J. Syst. Softw.},
month = dec,
numpages = {20},
keywords = {99-00, 00-01, Deep neural network, Weighted cross-entropy loss, Triplet loss, Deep feature representation, Software defect prediction}
}

@article{10.5555/3324436.3324448,
title = {A statistical comparison for evaluating the effectiveness of linear and nonlinear manifold detection techniques for software defect prediction},
year = {2019},
issue_date = {January 2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {12},
number = {3–4},
issn = {1755-0386},
abstract = {Most of the software systems are released without predicting defects and therefore, this paper presents a new effective technique-manifold detection technique MDT is essential and different than earlier applied defect prediction methods like regression, feature selection methods, etc. In this paper, performance of classifiers has been compared with or without MDTs to evaluate the effectiveness of different MDTs linear and nonlinear by reducing the dimensions of software datasets. In this process, eight classifiers were applied to four PROMISE datasets to determine the best performing classifier with respect to prediction performance measuring factors accuracy, precision, recall, F-measure, AUC, misclassification error with or without MDTs. The experimental results statistically tested by paired two-tailed t-test proved that FastMVU is the most accurate result producing technique as compared to all other nonlinear MDTs and Bayesian network BN is the most effective technique for software defect prediction using with or without MDTs.},
journal = {Int. J. Adv. Intell. Paradigms},
month = jan,
pages = {370–391},
numpages = {22}
}

@article{10.1016/j.jss.2016.09.001,
author = {Andreou, Andreas S. and Chatzis, Sotirios P.},
title = {Software defect prediction using doubly stochastic Poisson processes driven by stochastic belief networks},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.09.001},
doi = {10.1016/j.jss.2016.09.001},
abstract = {This research aims at better addressing the challenges related with software defect prediction.We develop a novel Bayesian inference approach driven from appropriate metrics.Formulation of our method is based on a doubly stochastic homogeneous Poisson process.Our model better learns from data with multiple modes in their distributions.We evaluate generalization across software classes, subsequent releases, and projects. Accurate prediction of software defects is of crucial importance in software engineering. Software defect prediction comprises two major procedures: (i) Design of appropriate software metrics to represent characteristic software system properties; and (ii) development of effective regression models for count data, allowing for accurate prediction of the number of software defects. Although significant research effort has been devoted to software metrics design, research in count data regression has been rather limited. More specifically, most used methods have not been explicitly designed to tackle the problem of metrics-driven software defect counts prediction, thus postulating irrelevant assumptions, such as (log-)linearity of the modeled data. In addition, a lack of simple and efficient algorithms for posterior computation has made more elaborate hierarchical Bayesian approaches appear unattractive in the context of software defect prediction. To address these issues, in this paper we introduce a doubly stochastic Poisson process for count data regression, the failure log-rate of which is driven by a novel latent space stochastic feedforward neural network. Our approach yields simple and efficient updates for its complicated conditional distributions by means of sampling importance resampling and error backpropagation. We exhibit the efficacy of our approach using publicly available and benchmark datasets.},
journal = {J. Syst. Softw.},
month = dec,
pages = {72–82},
numpages = {11},
keywords = {Stochastic belief network, Software defect prediction, Sampling importance resampling, Doubly stochastic Poisson process}
}

@inproceedings{10.1145/3273934.3273938,
author = {Amasaki, Sousuke},
title = {Cross-Version Defect Prediction using Cross-Project Defect Prediction Approaches: Does it work?},
year = {2018},
isbn = {9781450365932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3273934.3273938},
doi = {10.1145/3273934.3273938},
abstract = {Background: Specifying and removing defects before release deserve extra cost for the success of software projects. Long-running projects experience multiple releases, and it is a natural choice to adopt cross-version defect prediction (CVDP) that uses information from older versions. A past study shows that feeding multi older versions data may have a positive influence on the performance. The study also suggests that cross-project defect prediction (CPDP) may fit the situation but one CPDP approach was only examined.Aims: To investigate whether feeding multiple older versions data is effective for CVDP using CPDP approaches. The investigation also involves performance comparisons of the CPDP approaches under CVDP situation. Method: We chose a style of replication of the comparative study on CPDP approaches by Herbold et al. under CVDP situation. Results: Feeding multiple older versions had a positive effect for more than a half CPDP approaches. However, almost all of the CPDP approaches did not perform significantly better than a simple rule-based prediction. Although the best CPDP approach could work better than it and with-in project defect prediction, we found no effect of feeding multiple older versions for it. Conclusions: Feeding multiple older versions could improve CPDP approaches under CVDP situation. However, it did not work for the best CPDP approach in the study.},
booktitle = {Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {32–41},
numpages = {10},
keywords = {Comparative Study, Cross-Project Defect Prediction, Cross-Version Defect Prediction},
location = {Oulu, Finland},
series = {PROMISE'18}
}

@inproceedings{10.1145/3460319.3464819,
author = {Zeng, Zhengran and Zhang, Yuqun and Zhang, Haotian and Zhang, Lingming},
title = {Deep just-in-time defect prediction: how far are we?},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464819},
doi = {10.1145/3460319.3464819},
abstract = {Defect prediction aims to automatically identify potential defective code with minimal human intervention and has been widely studied in the literature. Just-in-Time (JIT) defect prediction focuses on program changes rather than whole programs, and has been widely adopted in continuous testing. CC2Vec, state-of-the-art JIT defect prediction tool, first constructs a hierarchical attention network (HAN) to learn distributed vector representations of both code additions and deletions, and then concatenates them with two other embedding vectors representing commit messages and overall code changes extracted by the existing DeepJIT approach to train a model for predicting whether a given commit is defective. Although CC2Vec has been shown to be the state of the art for JIT defect prediction, it was only evaluated on a limited dataset and not compared with all representative baselines. Therefore, to further investigate the efficacy and limitations of CC2Vec, this paper performs an extensive study of CC2Vec on a large-scale dataset with over 310,370 changes (8.3 X larger than the original CC2Vec dataset). More specifically, we also empirically compare CC2Vec against DeepJIT and representative traditional JIT defect prediction techniques. The experimental results show that CC2Vec cannot consistently outperform DeepJIT, and neither of them can consistently outperform traditional JIT defect prediction. We also investigate the impact of individual traditional defect prediction features and find that the added-line-number feature outperforms other traditional features. Inspired by this finding, we construct a simplistic JIT defect prediction approach which simply adopts the added-line-number feature with the logistic regression classifier. Surprisingly, such a simplistic approach can outperform CC2Vec and DeepJIT in defect prediction, and can be 81k X/120k X faster in training/testing. Furthermore, the paper also provides various practical guidelines for advancing JIT defect prediction in the near future.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {427–438},
numpages = {12},
keywords = {Software Defect Prediction, Just-In-Time Prediction, Deep Learning},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1007/978-3-030-78609-0_28,
author = {Sun, Ying and Sun, Yanfei and Wu, Fei and Jing, Xiao-Yuan},
title = {Deep Adversarial Learning Based Heterogeneous Defect Prediction},
year = {2021},
isbn = {978-3-030-78608-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78609-0_28},
doi = {10.1007/978-3-030-78609-0_28},
abstract = {Cross-project defect prediction (CPDP) is a hot study that predicts defects in the new project by utilizing the model trained on the data from other projects. However, existing CPDP methods usually assume that source and target projects have the same metrics. Heterogeneous defect prediction (HDP) is proposed and has attracted increasing attention, which refers to the metric sets from source and target projects are different in CPDP. HDP conducts prediction model using the instances with heterogeneous metrics from external projects and then use this model to predict defect-prone software instances in source project. However, building HDP methods is challenging including the distribution difference between source and target projects with heterogeneous metrics. In this paper, we propose a Deep adversarial learning based HDP (DHDP) approach. DHDP leverages deep neural network to learn nonlinear transformation for each project to obtain common feature represent, which the heterogeneous data from different projects can be compared directly. DHDP consists of two parts: a discriminator and a classifier that compete with each other. A classifier tries to minimize the similarity across classes and maximize the inter-class similarity. A discriminator tries to distinguish the source of instances that is source or target project on the common feature space. Expensive experiments are performed on 10 public projects from two datasets in terms of F-measure and G-measure. The experimental results show that DHDP gains superior prediction performance improvement compared to a range of competing methods.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part I},
pages = {326–337},
numpages = {12},
keywords = {Heterogeneous defect prediction, Metric learning, Adversarial learning},
location = {Dublin, Ireland}
}

@article{10.1007/s10664-012-9218-8,
author = {Okutan, Ahmet and Y\i{}ld\i{}z, Olcay Taner},
title = {Software defect prediction using Bayesian networks},
year = {2014},
issue_date = {February  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-012-9218-8},
doi = {10.1007/s10664-012-9218-8},
abstract = {There are lots of different software metrics discovered and used for defect prediction in the literature. Instead of dealing with so many metrics, it would be practical and easy if we could determine the set of metrics that are most important and focus on them more to predict defectiveness. We use Bayesian networks to determine the probabilistic influential relationships among software metrics and defect proneness. In addition to the metrics used in Promise data repository, we define two more metrics, i.e. NOD for the number of developers and LOCQ for the source code quality. We extract these metrics by inspecting the source code repositories of the selected Promise data repository data sets. At the end of our modeling, we learn the marginal defect proneness probability of the whole software system, the set of most effective metrics, and the influential relationships among metrics and defectiveness. Our experiments on nine open source Promise data repository data sets show that response for class (RFC), lines of code (LOC), and lack of coding quality (LOCQ) are the most effective metrics whereas coupling between objects (CBO), weighted method per class (WMC), and lack of cohesion of methods (LCOM) are less effective metrics on defect proneness. Furthermore, number of children (NOC) and depth of inheritance tree (DIT) have very limited effect and are untrustworthy. On the other hand, based on the experiments on Poi, Tomcat, and Xalan data sets, we observe that there is a positive correlation between the number of developers (NOD) and the level of defectiveness. However, further investigation involving a greater number of projects is needed to confirm our findings.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {154–181},
numpages = {28},
keywords = {Defect prediction, Bayesian networks}
}

@phdthesis{10.5555/AAI29159899,
author = {Arar, \"{O}mer Faruk and undefinedBrahim, \"{O}Z\c{c}elik, and G\"{u}ltekin, \c{C}agil, and Pakize, Erdogmu\c{s}, and Onay, Durdu, Pinar},
advisor = {K\"{u}r\c{s}at, Ayan,},
title = {Makine \"{o}\u{g}renme algoritmalar\i{} kullan\i{}larak yaz\i{}l\i{}m Hata Kestiriminin iyile\c{s}tirilmesi / Using Machine Learning Algorithms to Improve Software Defect Prediction},
year = {2016},
isbn = {9798835589593},
publisher = {Sakarya Universitesi (Turkey)},
abstract = {Yaz\i{}l\i{}m sistemleri g\"{u}nl\"{u}k ya\c{s}ant\i{}m\i{}zda \c{c}ok \"{o}nemli bir role sahiptir ve her ge\c{c}en g\"{u}n kullan\i{}m\i{} daha da yayg\i{}nla\c{s}maktad\i{}r. Makinelerin ve servislerin b\"{u}y\"{u}k \c{c}o\u{g}unlu\u{g}u kendi i\c{c}lerinde farkl\i{} t\"{u}rde yaz\i{}l\i{}m i\c{c}erirler. Yaz\i{}l\i{}m geli\c{s}tiriciler, g\"{u}nl\"{u}k kullan\i{}m\i{}n\i{} yayg\i{}nla\c{s}t\i{}rmak ve rekabette geri kalmamak i\c{c}in m\"{u}mk\"{u}n oldu\u{g}unca h\i{}zl\i{} bir \c{s}ekilde yaz\i{}l\i{}mlar\i{} geli\c{s}tirmektedirler. Yaz\i{}l\i{}m ya\c{s}am d\"{o}ng\"{u}s\"{u}; genellikle analiz, tasar\i{}m, kodlama, test ve kurulum safhalar\i{}ndan olu\c{s}tur. Son kullan\i{}c\i{}ya hatadan ar\i{}nd\i{}r\i{}lm\i{}\c{s} bir yaz\i{}l\i{}m sunabilmek i\c{c}in test safhas\i{} etkili olarak y\"{u}r\"{u}t\"{u}lmelidir. Yaz\i{}l\i{}m metrikleri, kaynak kodun kalitesini yans\i{}tmay\i{} ama\c{c}larlar ve i\c{c}eri\u{g}i ile ilgili niceliksel bilgi verirler. Her bir metrik kodun farkl\i{} bir y\"{o}n\"{u}n\"{u} de\u{g}erlendirir. Kaynak kodun kalitesi seviyesi ile risk seviyesi aras\i{}nda bir ili\c{s}ki vard\i{}r. Son 20 y\i{}ll\i{}k d\"{o}nemde, akademisyenler, yaz\i{}l\i{}m hata kestirimi problemine giderek artan bir ilgi g\"{o}stermi\c{s}ler, daha g\"{u}rb\"{u}z bir kestirim i\c{c}in \c{c}e\c{s}itli makine \"{o}\u{g}renmesi yakla\c{s}\i{}mlar\i{} uygulanm\i{}\c{s}t\i{}r. Bu \c{c}al\i{}\c{s}mada da bu problem i\c{c}in \c{c}e\c{s}itli makine \"{o}\u{g}renmesi modelleri \"{o}nerilmi\c{s}tir. Yapay Sinir A\u{g}\i{} ve Yapay Ar\i{} Kolonisi kombinasyonu, Lojistik Regresyon-tabanl\i{} Bender Metot ve Naive Bayes bu \c{c}al\i{}\c{s}mada kullan\i{}lan algoritmalard\i{}r. \"{O}nerilen yakla\c{s}\i{}mlar, herkese a\c{c}\i{}k NASA Metrik Veri Program\i{} ve PROMISE havuzunda bulunan veri setlerine uygulanm\i{}\c{s}t\i{}r. undefinedstatistiki olarak g\"{u}venilir sonu\c{c}lar elde etmek ve \"{o}rneklem yanl\i{}l\i{}\u{g}\i{}n\i{} azaltmak i\c{c}in deneyler n-k\"{u}me \c{c}apraz validasyon ile kurgulanm\i{}\c{s}t\i{}r. Performans\i{} artt\i{}rmak i\c{c}in \"{o}nerilen modellere \"{o}zellik se\c{c}imi, normalizasyon ve ayr\i{}kla\c{s}t\i{}rma gibi \c{c}e\c{s}itli veri \"{o}n i\c{s}leme teknikleri uygulanm\i{}\c{s}t\i{}r. Deneylerden elde edilen sonu\c{c}lar di\u{g}er \c{c}al\i{}\c{s}malar ile kar\c{s}\i{}la\c{s}t\i{}r\i{}lm\i{}\c{s}t\i{}r. Bu \c{c}al\i{}\c{s}ma, \"{o}zellikle, yaz\i{}l\i{}m geli\c{s}tiricileri ve test personelinin kullan\i{}m\i{} y\"{o}n\"{u}yle katk\i{} yapmaktad\i{}r. Yaz\i{}l\i{}m geli\c{s}tiricileri, d\"{u}zenlemeye ihtiya\c{c} duyulan s\i{}n\i{}f veya mod\"{u}lleri g\"{o}r\"{u}rler; dolay\i{}s\i{}yla, bu mod\"{u}llerin kalitesinin artt\i{}r\i{}lmas\i{}na ve risk seviyelerinin azalt\i{}lmas\i{}na katk\i{} yapm\i{}\c{s} olurlar. Test personeli, daha \c{c}ok test yo\u{g}unla\c{s}mas\i{} gerektiren mod\"{u}lleri tespit eder ve bunun neticesinde mod\"{u}llerin \"{o}nceliklendirmesinin risk seviyelerine g\"{o}re yap\i{}lmas\i{} sa\u{g}lanm\i{}\c{s} olunur.},
note = {AAI29159899}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {variability, transfer learning, software product lines, machine learning, deep neural networks},
location = {Paris, France},
series = {SPLC '19}
}

@book{10.5555/3175829,
author = {Rashid, Ekbal and Rashid, Ekbal},
title = {Enhancing Software Fault Prediction With Machine Learning: Emerging Research and Opportunities},
year = {2017},
isbn = {1522531858},
publisher = {IGI Global},
address = {USA},
edition = {1st},
abstract = {Software development and design is an intricate and complex process that requires a multitude of steps to ultimately create a quality product. One crucial aspect of this process is minimizing potential errors through software fault prediction. Enhancing Software Fault Prediction With Machine Learning: Emerging Research and Opportunities is an innovative source of material on the latest advances and strategies for software quality prediction. Including a range of pivotal topics such as case-based reasoning, rate of improvement, and expert systems, this book is an ideal reference source for engineers, researchers, academics, students, professionals, and practitioners interested in novel developments in software design and analysis.}
}

@inproceedings{10.1109/CIS.2013.61,
author = {Shuai, Bo and Li, Haifeng and Li, Mengjun and Zhang, Quan and Tang, Chaojing},
title = {Software Defect Prediction Using Dynamic Support Vector Machine},
year = {2013},
isbn = {9781479925490},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CIS.2013.61},
doi = {10.1109/CIS.2013.61},
abstract = {In order to solve the problems of traditional SVM classifier for software defect prediction, this paper proposes a novel dynamic SVM method based on improved cost-sensitive SVM (CSSVM) which is optimized by the Genetic Algorithm (GA). Through selecting the geometric classification accuracy as the fitness function, the GA method could improve the performance of CSSVM by enhancing the accuracy of defective modules and reducing the total cost in the whole decision. Experimental results show that the GA-CSSVM method could achieve higher AUC value which denotes better prediction accuracy both for minority and majority samples in the imbalanced software defect data set.},
booktitle = {Proceedings of the 2013 Ninth International Conference on Computational Intelligence and Security},
pages = {260–263},
numpages = {4},
keywords = {software defect, GA, CSSVM, AUC},
series = {CIS '13}
}

@article{10.1016/j.asoc.2020.106686,
author = {Haouari, Ahmed Taha and Souici-Meslati, Labiba and Atil, Fadila and Meslati, Djamel},
title = {Empirical comparison and evaluation of Artificial Immune Systems in inter-release software fault prediction},
year = {2020},
issue_date = {Nov 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {96},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.106686},
doi = {10.1016/j.asoc.2020.106686},
journal = {Appl. Soft Comput.},
month = nov,
numpages = {18},
keywords = {Neural Network, Artificial Immune Recognition System, Inter-projects fault prediction, Software defect prediction, Artificial Immune Systems}
}

@proceedings{10.1145/3416505,
title = {MaLTeSQuE 2020: Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the fourth edition of the workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE 2020) to be held virtually on November 16, 2020, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2020).},
location = {Virtual, USA}
}

@phdthesis{10.5555/AAI28414631,
author = {Gatling, Teia C. and Blackburn, Timothy},
advisor = {Oluwatomi, Adetunji, and Amirhossein, Etemadi,},
title = {Applying Documentation Metrics in Cross Version Defect Prediction Modeling},
year = {2021},
isbn = {9798708754837},
publisher = {The George Washington University},
abstract = {Software documentation such as documented Application Programming Interface (API) and comments embedded with the software code aid in faster debugging of software defects. The existence of this documentation is used as a measurement of software quality. Over the last 40 years, a series of object-oriented metrics-based defect prediction models have been successfully developed. However, documentation metrics in combination with object-oriented metrics in software defect prediction modeling has not been explored in predicting defects. By leveraging a publicly available GitHub dataset that contains both documentation and object-oriented metrics and applying the cross version defect prediction approach, this research determined documentation metrics impact on defects across a project and developed a predictive model using these metric in combination with baseline metrics to improve model performance. As a result, Boosting ensemble method returned improved model performance when combining the documentation metrics with commonly used object-oriented code metrics. Likewise, the Random Forest returned an improved model when using a feature subset. Random Forest using a subset of metrics provided the most promising results with F-Measure performance improvement of 8.9 percent. The results of this research highlight quantitatively the impact documentation metrics have on software defect prediction and that model performance can improve when identifying a subset of metrics. The results also demonstrate the use of data from three previous versions versus solely using the latest version, the models perform within an average of five percentage points of each other. This knowledge can be leveraged by managers to enhance the application of documentation throughout the lifecycle of software.},
note = {AAI28414631}
}

@inproceedings{10.1145/3472674.3473979,
author = {Pravilov, Mikhail and Bogomolov, Egor and Golubev, Yaroslav and Bryksin, Timofey},
title = {Unsupervised learning of general-purpose embeddings for code changes},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473979},
doi = {10.1145/3472674.3473979},
abstract = {Applying machine learning to tasks that operate with code changes requires their numerical representation. In this work, we propose an approach for obtaining such representations during pre-training and evaluate them on two different downstream tasks — applying changes to code and commit message generation. During pre-training, the model learns to apply the given code change in a correct way. This task requires only code changes themselves, which makes it unsupervised. In the task of applying code changes, our model outperforms baseline models by 5.9 percentage points in accuracy. As for the commit message generation, our model demonstrated the same results as supervised models trained for this specific task, which indicates that it can encode code changes well and can be improved in the future by pre-training on a larger dataset of easily gathered code changes.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {7–12},
numpages = {6},
keywords = {Unsupervised learning, Commit message generation, Code changes},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@article{10.1007/s11219-021-09553-2,
author = {Wu, Jie and Wu , Yingbo and Niu, Nan and Zhou, Min},
title = {MHCPDP: multi-source heterogeneous cross-project defect prediction via multi-source transfer learning and autoencoder},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09553-2},
doi = {10.1007/s11219-021-09553-2},
abstract = {Heterogeneous cross-project defect prediction (HCPDP) is aimed at building a defect prediction model for the target project by reusing datasets from source projects, where the source project datasets and target project dataset have different features. Most existing HCPDP methods only remove redundant or unrelated features without exploring the underlying features of cross-project datasets. Additionally, when the&nbsp;transfer learning method is used in HCPDP, these methods ignore the negative effect of transfer learning. In this paper, we propose a novel HCPDP method called multi-source heterogeneous cross-project defect prediction (MHCPDP). To reduce the gap between the target datasets and the source datasets, MHCPDP uses the autoencoder to extract the intermediate features from the original datasets instead of simply removing redundant and unrelated features and adopts a modified autoencoder algorithm to make instance selection for eliminating irrelevant instances from the source domain datasets. Furthermore, by incorporating multiple source projects to increase the number of source datasets, MHCPDP develops a multi-source transfer learning algorithm to reduce the impact of negative transfers and upgrade the performance of the classifier. We comprehensively evaluate MHCPDP on five open source datasets; our experimental results show that MHCPDP not only has significant improvement in two performance metrics but also overcomes the shortcomings of the conventional HCPDP methods.},
journal = {Software Quality Journal},
month = jun,
pages = {405–430},
numpages = {26},
keywords = {Modified autoencoder, Multi-source transfer learning, Heterogeneous cross-project defect prediction, Autoencoder}
}

@article{10.1016/j.infsof.2011.09.007,
author = {Ma, Ying and Luo, Guangchun and Zeng, Xue and Chen, Aiguo},
title = {Transfer learning for cross-company software defect prediction},
year = {2012},
issue_date = {March, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.09.007},
doi = {10.1016/j.infsof.2011.09.007},
abstract = {Context: Software defect prediction studies usually built models using within-company data, but very few focused on the prediction models trained with cross-company data. It is difficult to employ these models which are built on the within-company data in practice, because of the lack of these local data repositories. Recently, transfer learning has attracted more and more attention for building classifier in target domain using the data from related source domain. It is very useful in cases when distributions of training and test instances differ, but is it appropriate for cross-company software defect prediction? Objective: In this paper, we consider the cross-company defect prediction scenario where source and target data are drawn from different companies. In order to harness cross company data, we try to exploit the transfer learning method to build faster and highly effective prediction model. Method: Unlike the prior works selecting training data which are similar from the test data, we proposed a novel algorithm called Transfer Naive Bayes (TNB), by using the information of all the proper features in training data. Our solution estimates the distribution of the test data, and transfers cross-company data information into the weights of the training data. On these weighted data, the defect prediction model is built. Results: This article presents a theoretical analysis for the comparative methods, and shows the experiment results on the data sets from different organizations. It indicates that TNB is more accurate in terms of AUC (The area under the receiver operating characteristic curve), within less runtime than the state of the art methods. Conclusion: It is concluded that when there are too few local training data to train good classifiers, the useful knowledge from different-distribution training data on feature level may help. We are optimistic that our transfer learning method can guide optimal resource allocation strategies, which may reduce software testing cost and increase effectiveness of software testing process.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {248–256},
numpages = {9},
keywords = {Transfer learning, Software defect prediction, Naive Bayes, Machine learning, Different distribution}
}

@inproceedings{10.1145/2875913.2875944,
author = {Qing, He and Biwen, Li and Beijun, Shen and Xia, Yong},
title = {Cross-Project Software Defect Prediction Using Feature-Based Transfer Learning},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875944},
doi = {10.1145/2875913.2875944},
abstract = {Cross-project defect prediction is taken as an effective means of predicting software defects when the data shortage exists in the early phase of software development. Unfortunately, the precision of cross-project defect prediction is usually poor, largely because of the differences between the reference and the target projects. Having realized the project differences, this paper proposes CPDP, a feature-based transfer learning approach to cross-project defect prediction. The core insight of CPDP is to (1) filter and transfer highly-correlated data based on data samples in the target projects, and (2) evaluate and choose learning schemas for transferring data sets. Models are then built for predicting defects in the target projects. We have also conducted an evaluation of the proposed approach on PROMISE datasets. The evaluation results show that, the proposed approach adapts to cross-project defect prediction in that f-measure of 81.8% of projects can get improved, and AUC of 54.5% projects improved. It also achieves similar f-measure and AUC as some inner-project defect prediction approaches.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {74–82},
numpages = {9},
keywords = {transfer learning, feature-based transfer, cross-project defect prediction},
location = {Wuhan, China},
series = {Internetware '15}
}

@article{10.1504/IJWMC.2016.076145,
author = {Li, Feixiang and Rong, Xiaotao and Cui, Zhihua},
title = {A hybrid CRBA-SVM model for software defect prediction},
year = {2016},
issue_date = {April 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {2},
issn = {1741-1084},
url = {https://doi.org/10.1504/IJWMC.2016.076145},
doi = {10.1504/IJWMC.2016.076145},
abstract = {Support vector machine SVM model is becoming an increasingly popular method in software defects prediction. This model has strong non-linear classifying ability. However, SVM model lacks effective method to determine the best parameters. In this paper, a modified bat algorithm, named changing range bat algorithm, is employed to optimise the parameters of SVM model. To test the performance of this new model, several public datasets of software defect prediction are employed and then the results are compared with other five approaches. Experimental results show that the classification ability of hybrid CRBA-SVM model surpasses all other approaches.},
journal = {Int. J. Wire. Mob. Comput.},
month = apr,
pages = {191–196},
numpages = {6}
}

@inproceedings{10.1109/ISISE.2012.114,
author = {Wang, Pei and Jin, Cong and Jin, Shu-Wei},
title = {Software Defect Prediction Scheme Based on Feature Selection},
year = {2012},
isbn = {9780769549514},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISISE.2012.114},
doi = {10.1109/ISISE.2012.114},
abstract = {Predicting defect-prone software modules accurately and effectively are important ways to control the quality of a software system during software development. Feature selection can highly improve the accuracy and efficiency of the software defect prediction model. The main purpose of this paper is to discuss the best size of feature subset for building a prediction model and prove that feature selection method is useful for establishing software defect prediction model. Mutual information is an outstanding indicator of relevance between variables, and it has been used as a measurement in our feature selection algorithm. We also introduce a nonlinear factor to our evaluation function for feature selection to improve its performance. The results of our feature selection algorithm are validated by different machine learning methods. The experiment results show that all the classifiers achieve higher accuracy by using the feature subset provided by our algorithm.},
booktitle = {Proceedings of the 2012 Fourth International Symposium on Information Science and Engineering},
pages = {477–480},
numpages = {4},
keywords = {software defect prediction, mutual information, feature selection},
series = {ISISE '12}
}

@article{10.1016/j.eswa.2019.113156,
author = {Majd, Amirabbas and Vahidi-Asl, Mojtaba and Khalilian, Alireza and Poorsarvi-Tehrani, Pooria and Haghighi, Hassan},
title = {SLDeep: Statement-level software defect prediction using deep-learning model on static code features},
year = {2020},
issue_date = {Jun 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {147},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113156},
doi = {10.1016/j.eswa.2019.113156},
journal = {Expert Syst. Appl.},
month = jun,
numpages = {14},
keywords = {Software metric, Fault prediction model, Machine learning, Software fault proneness, Defect}
}

@article{10.1016/j.compind.2021.103505,
author = {Gashi, Milot and Ofner, Patrick and Ennsbrunner, Helmut and Thalmann, Stefan},
title = {Dealing with missing usage data in defect prediction: A case study of a welding supplier},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2021.103505},
doi = {10.1016/j.compind.2021.103505},
journal = {Comput. Ind.},
month = nov,
numpages = {10},
keywords = {Multi-component systems, Predictive maintenance, Welding industry, End-of-line testing, Defect prediction}
}

@article{10.1145/3467895,
author = {Falessi, Davide and Ahluwalia, Aalok and Penta, Massimiliano DI},
title = {The Impact of Dormant Defects on Defect Prediction: A Study of 19 Apache Projects},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3467895},
doi = {10.1145/3467895},
abstract = {Defect prediction models can be beneficial to prioritize testing, analysis, or code review activities, and has been the subject of a substantial effort in academia, and some applications in industrial contexts. A necessary precondition when creating a defect prediction model is the availability of defect data from the history of projects. If this data is noisy, the resulting defect prediction model could result to be unreliable. One of the causes of noise for defect datasets is the presence of “dormant defects,” i.e., of defects discovered several releases after their introduction. This can cause a class to be labeled as defect-free while it is not, and is, therefore “snoring.” In this article, we investigate the impact of snoring on classifiers' accuracy and the effectiveness of a possible countermeasure, i.e., dropping too recent data from a training set. We analyze the accuracy of 15 machine learning defect prediction classifiers, on data from more than 4,000 defects and 600 releases of 19 open source projects from the Apache ecosystem. Our results show that on average across projects (i) the presence of dormant defects decreases the recall of defect prediction classifiers, and (ii) removing from the training set the classes that in the last release are labeled as not defective significantly improves the accuracy of the classifiers. In summary, this article provides insights on how to create defects datasets by mitigating the negative effect of dormant defects on defect prediction.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {4},
numpages = {26},
keywords = {dataset bias, fix-inducing changes, Defect prediction}
}

@inproceedings{10.1145/2961111.2962620,
author = {Shippey, Thomas and Hall, Tracy and Counsell, Steve and Bowes, David},
title = {So You Need More Method Level Datasets for Your Software Defect Prediction? Voil\`{a}!},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962620},
doi = {10.1145/2961111.2962620},
abstract = {Context: Defect prediction research is based on a small number of defect datasets and most are at class not method level. Consequently our knowledge of defects is limited. Identifying defect datasets for prediction is not easy and extracting quality data from identified datasets is even more difficult. Goal: Identify open source Java systems suitable for defect prediction and extract high quality fault data from these datasets. Method: We used the Boa to identify candidate open source systems. We reduce 50,000 potential candidates down to 23 suitable for defect prediction using a selection criteria based on the system's software repository and its defect tracking system. We use an enhanced SZZ algorithm to extract fault information and calculate metrics using JHawk. Result: We have produced 138 fault and metrics datasets for the 23 identified systems. We make these datasets (the ELFF datasets) and our data extraction tools freely available to future researchers. Conclusions: The data we provide enables future studies to proceed with minimal effort. Our datasets significantly increase the pool of systems currently being used in defect analysis studies.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {12},
numpages = {6},
keywords = {Defects, Defect linking, Defect Prediction, Data Mining, Boa},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/2491411.2494581,
author = {Zhang, Hongyu and Cheung, S. C.},
title = {A cost-effectiveness criterion for applying software defect prediction models},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2494581},
doi = {10.1145/2491411.2494581},
abstract = {Ideally, software defect prediction models should help organize software quality assurance (SQA) resources and reduce cost of finding defects by allowing the modules most likely to contain defects to be inspected first. In this paper, we study the cost-effectiveness of applying defect prediction models in SQA and propose a basic cost-effectiveness criterion. The criterion implies that defect prediction models should be applied with caution. We also propose a new metric FN/(FN+TN) to measure the cost-effectiveness of a defect prediction model.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {643–646},
numpages = {4},
keywords = {evaluation metrics, cost effectiveness, Defect prediction},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1109/COMPSAC.2014.65,
author = {Lu, Huihua and Cukic, Bojan and Culp, Mark},
title = {A Semi-supervised Approach to Software Defect Prediction},
year = {2014},
isbn = {9781479935758},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2014.65},
doi = {10.1109/COMPSAC.2014.65},
abstract = {Accurate detection of software components that need to be exposed to additional verification and validation offers the path to high quality products while minimizing non essential software assurance expenditures. In this type of quality modeling we assume that software modules with known fault content developed in similar environment are available. Supervised learning algorithms are the traditional methods of choice for training on existing modules. The models are then used to predict fault content for newly developed software components prior to product release. However, one needs to realize that establishing whether a module contains a fault or not, only to be used for model training, can be expensive. The basic idea behind semi-supervised learning is to learn from a small number of software modules with known fault content and supplement model training with modules for which the fault information is not available, thus reducing the overall cost of quality assurance. In this study, we investigate the performance of semi-supervised learning for software fault prediction. A preprocessing strategy, multidimensional scaling, is embedded in the approach to reduce the dimensional complexity of software metrics used for prediction. Our results show that the dimension-reduction with semi-supervised learning algorithm preforms significantly better than one of the best performing supervised learning algorithm - random forest - in situations when few modules with known fault content are available. We compare our results with the published benchmarks and clearly demonstrate performance benefits.},
booktitle = {Proceedings of the 2014 IEEE 38th Annual Computer Software and Applications Conference},
pages = {416–425},
numpages = {10},
keywords = {software fault prediction, semi-supervised learning, dimension reduction, software metrics},
series = {COMPSAC '14}
}

@article{10.1049/sfw2.12053,
author = {Huang, Qingan and Ma, Le and Jiang, Siyu and Wu, Guobin and Song, Hengjie and Jiang, Libiao and Zheng, Chunyun},
title = {A cross‐project defect prediction method based on multi‐adaptation and nuclear norm},
year = {2021},
issue_date = {April 2022},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {16},
number = {2},
url = {https://doi.org/10.1049/sfw2.12053},
doi = {10.1049/sfw2.12053},
abstract = {Cross‐project defect prediction (CPDP) is an important research direction in software defect prediction. Traditional CPDP methods based on hand‐crafted features ignore the semantic information in the source code. Existing CPDP methods based on the&nbsp;deep learning model may not fully consider the differences among projects. Additionally, these methods may not accurately classify the samples near the classification boundary. To solve these problems, the authors propose a model based on multi‐adaptation and nuclear norm (MANN) to deal with samples in projects. The feature of samples were embedded into the multi‐core Hilbert space for distribution and the multi‐kernel maximum mean discrepancy method was utilised to reduce differences among projects. More importantly, the nuclear norm module was constructed, which improved the discriminability and diversity of the target sample by calculating and maximizing the nuclear norm of the target sample in the process of domain adaptation, thus improving the performance of MANN. Finally, extensive experiments were conducted on 11 sizeable open‐source projects. The results indicate&nbsp;that the proposed method exceeds the state of the art under the widely used metrics.},
journal = {IET Software},
month = dec,
pages = {200–213},
numpages = {14},
keywords = {unsupervised learning, software reliability, software quality, neural nets}
}

@article{10.4018/IJOSSP.2017010102,
author = {Alsukhni, Emad and Saifan, Ahmad A. and Alawneh, Hanadi},
title = {A New Data Mining-Based Framework to Test Case Prioritization Using Software Defect Prediction},
year = {2017},
issue_date = {January 2017},
publisher = {IGI Global},
address = {USA},
volume = {8},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2017010102},
doi = {10.4018/IJOSSP.2017010102},
abstract = {Test cases do not have the same importance when used to detect faults in software; therefore, it is more efficient to test the system with the test cases that have the ability to detect the faults. This research proposes a new framework that combines data mining techniques to prioritize the test cases. It enhances fault prediction and detection using two different techniques: 1 the data mining regression classifier that depends on software metrics to predict defective modules, and 2 the k-means clustering technique that is used to select and prioritize test cases to identify the fault early. Our approach of test case prioritization yields good results in comparison with other studies. The authors used the Average Percentage of Faults Detection APFD metric to evaluate the proposed framework, which results in 19.9% for all system modules and 25.7% for defective ones. Our results give us an indication that it is effective to start the testing process with the most defective modules instead of testing all modules arbitrary arbitrarily.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {21–41},
numpages = {21},
keywords = {Test Case Prioritization, Software Testing, Software Defect Prediction, Data Mining}
}

@article{10.1016/j.compeleceng.2021.107370,
author = {Zheng, Shang and Gai, Jinjing and Yu, Hualong and Zou, Haitao and Gao, Shang},
title = {Training data selection for imbalanced cross-project defect prediction},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {94},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107370},
doi = {10.1016/j.compeleceng.2021.107370},
journal = {Comput. Electr. Eng.},
month = sep,
numpages = {11},
keywords = {Relative density, Jensen-Shannon divergence, Data selection, Cross-project software prediction}
}

@article{10.1016/j.procs.2018.05.115,
author = {Singh, Ajmer and Bhatia, Rajesh and Singhrova, Anita},
title = {Taxonomy of machine learning algorithms in software fault prediction using object oriented metrics},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.115},
doi = {10.1016/j.procs.2018.05.115},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {993–1001},
numpages = {9},
keywords = {Software fault prediction, Object Oriented Testing, Object Oriented Coupling, software faults prediction, machine learning}
}

@inproceedings{10.1145/3439961.3439979,
author = {Santos, Geanderson and Figueiredo, Eduardo and Veloso, Adriano and Viggiato, Markos and Ziviani, Nivio},
title = {Predicting Software Defects with Explainable Machine Learning},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439979},
doi = {10.1145/3439961.3439979},
abstract = {Most software systems must evolve to cope with stakeholders’ requirements and fix existing defects. Hence, software defect prediction represents an area of interest in both academia and the software industry. As a result, predicting software defects can help the development team to maintain substantial levels of software quality. For this reason, machine learning models have increased in popularity for software defect prediction and have demonstrated effectiveness in many scenarios. In this paper, we evaluate a machine learning approach for selecting features to predict software module defects. We use a tree boosting algorithm that receives as input a training set comprising records of software features encoding characteristics of each module and outputs whether the corresponding module is defective prone. For nine projects within the widely known NASA data program, we build prediction models from a set of easy-to-compute module features. We then sample this sizable model space by randomly selecting software features to compose each model. This significant number of models allows us to structure our work along model understandability and predictive accuracy. We argue that explaining model predictions is meaningful to provide information to developers on features related to each module defective-prone. We show that (i) features that contribute most to finding the best models may vary depending on the project, and (ii) effective models are highly understandable based on a survey with 40 developers.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {18},
numpages = {10},
keywords = {software defects, explainable models, SHAP values, NASA datasets},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@article{10.1109/TSE.2017.2731766,
author = {Bennin, Kwabena Ebo and Keung, Jacky and Phannachitta, Passakorn and Monden, Akito and Mensah, Solomon},
title = {MAHAKIL: Diversity Based Oversampling Approach to Alleviate the Class Imbalance Issue in Software Defect Prediction},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {44},
number = {6},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2017.2731766},
doi = {10.1109/TSE.2017.2731766},
abstract = {Highly imbalanced data typically make accurate predictions difficult. Unfortunately, software defect datasets tend to have fewer defective modules than non-defective modules. Synthetic oversampling approaches address this concern by creating new minority defective modules to balance the class distribution before a model is trained. Notwithstanding the successes achieved by these approaches, they mostly result in over-generalization (high rates of false alarms) and generate near-duplicated data instances (less diverse data). In this study, we introduce MAHAKIL, a novel and efficient synthetic oversampling approach for software defect datasets that is based on the chromosomal theory of inheritance. Exploiting this theory, MAHAKIL interprets two distinct sub-classes as parents and generates a new instance that inherits different traits from each parent and contributes to the diversity within the data distribution. We extensively compare MAHAKIL with SMOTE, Borderline-SMOTE, ADASYN, Random Oversampling and the No sampling approach using 20 releases of defect datasets from the PROMISE repository and five prediction models. Our experiments indicate that MAHAKIL improves the prediction performance for all the models and achieves better and more significant &lt;italic&gt;pf&lt;/italic&gt; values than the other oversampling approaches, based on Brunner's statistical significance test and Cliff's effect sizes. Therefore, MAHAKIL is strongly recommended as an efficient alternative for defect prediction models built on highly imbalanced datasets.},
journal = {IEEE Trans. Softw. Eng.},
month = jun,
pages = {534–550},
numpages = {17}
}

@inproceedings{10.1109/ICSE-C.2017.72,
author = {Wu, Fei and Jing, Xiao-Yuan and Dong, Xiwei and Cao, Jicheng and Xu, Mingwei and Zhang, Hongyu and Ying, Shi and Xu, Baowen},
title = {Cross-project and within-project semi-supervised software defect prediction problems study using a unified solution},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.72},
doi = {10.1109/ICSE-C.2017.72},
abstract = {When there exists not enough historical defect data for building accurate prediction model, semi-supervised defect prediction (SSDP) and cross-project defect prediction (CPDP) are two feasible solutions. Existing CPDP methods assume that the available source data is well labeled. However, due to expensive human efforts for labeling a large amount of defect data, usually, we can only make use of the suitable unlabeled source data to help build the prediction model. We call CPDP in this scenario as cross-project semi-supervised defect prediction (CSDP). As to within-project semi-supervised defect prediction (WSDP), although some WSDP methods have been developed in recent years, there still exists much room for improvement. In this paper, we aim to provide an effective solution for both CSDP and WSDP problems. We introduce the semi-supervised dictionary learning technique, an effective machine learning technique, into defect prediction and propose a semi-supervised structured dictionary learning (SSDL) approach for CSDP and WSDP. SSDL can make full use of the useful information in limited labeled defect data and a large amount of unlabeled data. Experiments on two public datasets indicate that SSDL can obtain better prediction performance than related SSDP methods in the CSDP scenario.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {195–197},
numpages = {3},
keywords = {within-project semi-supervised defect prediction, semi-supervised structured dictionary learning, cross-project semi-supervised defect prediction},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@inproceedings{10.1145/3461001.3475157,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Ayala, Inmaculada and Kr\"{u}ger, Jacob and Mosser, S\'{e}bastien},
title = {International Workshop on Variability Management for Modern Technologies (VM4ModernTech 2021)},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3475157},
doi = {10.1145/3461001.3475157},
abstract = {Variability is an inherent property of software systems that allows developers to deal with the needs of different customers and environments, creating a family of related systems. Variability can be managed in an opportunistic fashion, for example, using clone-and-own, or by employing a systematic approach, for instance, using a software product line (SPL). In the SPL community, variability management has been discussed for systems in various domains, such as defense, avionics, or finance, and for different platforms, such as desktops, web applications, or embedded systems. Unfortunately, other research communities---particularly those working on modern technologies, such as microservice architectures, cyber-physical systems, robotics, cloud computing, autonomous driving, or ML/AI-based systems---are less aware of the state-of-the-art in variability management, which is why they face similar problems and start to redeveloped the same solutions as the SPL community already did. With the International Workshop on Variability Management for Modern Technologies, we aim to foster and strengthen synergies between the communities researching variability management and modern technologies. More precisely, we aim to attract researchers and practitioners to contribute processes, techniques, tools, empirical studies, and problem descriptions or solutions that are related to reuse and variability management for modern technologies. By inviting different communities and establishing collaborations between them, we hope that the workshop can raise the interest of researchers outside the SPL community for variability management, and thus reduce the extent of costly redevelopments in research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {202},
numpages = {1},
keywords = {variability management, software architecture},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3345629.3351449,
author = {Jahanshahi, Hadi and Jothimani, Dhanya and Ba\c{s}ar, Ay\c{s}e and Cevik, Mucahit},
title = {Does chronology matter in JIT defect prediction? A Partial Replication Study},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3351449},
doi = {10.1145/3345629.3351449},
abstract = {BACKGROUND: Just-In-Time (JIT) models, unlike the traditional defect prediction models, detect the fix-inducing changes (or defect inducing changes). These models are designed based on the assumption that past code change properties are similar to future ones. However, as the system evolves, the expertise of developers and/or the complexity of the system also change.AIM: In this work, we aim to investigate the effect of code change properties on JIT models over time. We also study the impact of using recent data as well as all available data on the performance of JIT models. Further, we analyze the effect of weighted sampling on the performance of fix-inducing properties of JIT models. For this purpose, we used datasets from four open-source projects, namely Eclipse JDT, Mozilla, Eclipse Platform, and PostgreSQL.METHOD: We used five families of change code properties such as size, diffusion, history, experience, and purpose. We used Random Forest to train and test the JIT model and Brier Score (BS) and Area Under Curve (AUC) for performance measurement. We applied the Wilcoxon Signed Rank Test on the output to statistically validate whether the performance of JIT models improves using all the available data or the recent data.RESULTS: Our paper suggest that the predictive power of JIT models does not change by time. Furthermore, we observed that the chronology of data in JIT defect prediction models can be discarded by considering all the available data. On the other hand, the importance score of families of code change properties is found to oscillate over time.CONCLUSION: To mitigate the impact of the evolution of code change properties, it is recommended to use weighted sampling approach in which more emphasis is placed upon the changes occurring closer to the current time. Moreover, since properties such as "Expertise of the Developer" and "Size" evolve with the time, the models obtained from old data may exhibit different characteristics compared to those employing the newer dataset. Hence, practitioners should constantly retrain JIT models to include fresh data.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Just-In-Time prediction, defect prediction, quality assurance, software engineering},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@inproceedings{10.1007/978-3-030-78612-0_5,
author = {Xu, Haitao and Duan, Ruifeng and Yang, Shengsong and Guo, Lei},
title = {An Empirical Study on Data Sampling for Just-in-Time Defect Prediction},
year = {2021},
isbn = {978-3-030-78611-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78612-0_5},
doi = {10.1007/978-3-030-78612-0_5},
abstract = {In this paper, the impact of Data Sampling on Just-in-Time defect prediction is explored. We find that there is a significant negative relationship between the class imbalance ratio of the dataset and the performance of the instant software defect prediction model. Secondly although most software defect data are not as unbalanced as expected, a moderate degree of imbalance is sufficient to affect the performance of traditional learning. This means that if the training data for immediate software defects show moderate or more severe imbalances, one need not expect good defect prediction performance and the data sampling approach to balancing the training data can improve the performance of the model. Finally, the empirical approach shows that although the under-sampling method slightly improves model performance, the different sampling methods do not have a substantial impact on the evaluation of immediate software defect prediction models.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part II},
pages = {54–69},
numpages = {16},
keywords = {Empirical study, Just-in-time defect, Data sampling},
location = {Dublin, Ireland}
}

@inproceedings{10.1007/978-3-030-27878-6_8,
author = {LaCasse, Phillip M. and Otieno, Wilkistar and Maturana, Francisco P.},
title = {Operationalization of a Machine Learning and Fuzzy Inference-Based Defect Prediction Case Study in a Holonic Manufacturing System},
year = {2019},
isbn = {978-3-030-27877-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27878-6_8},
doi = {10.1007/978-3-030-27878-6_8},
abstract = {Industry 4.0 capabilities have enabled manufacturers to collect and analyze smart manufacturing data across a broad array of diverse domains including but not limited to scheduling, production, maintenance, process, and quality. This development necessarily proceeds in a logical sequence by which first the organization develops the capability to capture and store this data and, at best concurrently but frequently lagging, develops and refines the competencies to analyze and effectively utilize it. This research presents an applied case study in surface mount technology (SMT) manufacture of printed circuit board (PCB) assemblies. Parametric data captured at the solder paste inspection (SPI) station is analyzed with machine learning models to identify patterns and relationships that can be harnessed to preempt electrical defects at downline inspection stations. This project is enabled by the recent conclusion of an Industrial Internet of Things (IIoT) capability enhancement at the manufacturing facility from which the data is drawn and is the logical next step in achieving value from the newly-available smart manufacturing data. The operationalization of this analysis is contextualized within the product-resource-order-staff architecture (PROSA) of a Holonic Manufacturing Systems (HMS). A Trigger Holon is nested between the Resource Holarchy and Product Holarchy that, at scheduling, distributes implementation instructions for the defect-prediction model. The Defect Prediction Holon is containerized within the Product Holarchy and provides instructions for corrective action when the model flags a record as exhibiting increased probability of a downline electrical defect.},
booktitle = {Industrial Applications of Holonic and Multi-Agent Systems: 9th International Conference, HoloMAS 2019, Linz, Austria, August 26–29, 2019, Proceedings},
pages = {96–104},
numpages = {9},
keywords = {Holonic manufacturing system, Machine learning, Defect prediction, Printed circuit board, Surface mount technology, Solder paste inspection, Ball grid array},
location = {Linz, Austria}
}

@article{10.1504/IJBIC.2018.092808,
title = {An improved twin support vector machine based on multi-objective cuckoo search for software defect prediction},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {11},
number = {4},
issn = {1758-0366},
url = {https://doi.org/10.1504/IJBIC.2018.092808},
doi = {10.1504/IJBIC.2018.092808},
abstract = {Recently, software defect prediction SDP has drawn much attention as software size becomes larger and consumers hold higher reliability expectations. The premise of SDP is to guide the detection of software bugs and to conserve computational resources. However, in prior research, data imbalances among software defect modules were largely ignored to focus instead on how to improve defect prediction accuracy. In this paper, a novel SDP model based on twin support vector machines TSVM and a multi-objective cuckoo search MOCS is proposed, called MOCSTSVM. We set the probability of detection and the probability of false alarm as the SDP objectives. We use TSVM to predict defected modules and employ MOCS to optimise TSVM for this dual-objective optimisation problem. To test our approach, we conduct a series of experiments on a public dataset from the PROMISE repository. The experimental results demonstrate that our approach achieves good performance compared with other SDP models.},
journal = {Int. J. Bio-Inspired Comput.},
month = jan,
pages = {282–291},
numpages = {10}
}

@inproceedings{10.1145/3475716.3475791,
author = {Gesi, Jiri and Li, Jiawei and Ahmed, Iftekhar},
title = {An Empirical Examination of the Impact of Bias on Just-in-time Defect Prediction},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475791},
doi = {10.1145/3475716.3475791},
abstract = {Background: Just-In-Time (JIT) defect prediction models predict if a commit will introduce defects in the future. DeepJIT and CC2Vec are two state-of-the-art JIT Deep Learning (DL) techniques. Usually, defect prediction techniques are evaluated, treating all training data equally. However, data is usually imbalanced not only in terms of the overall class label (e.g., defect and non-defect) but also in terms of characteristics such as File Count, Edit Count, Multiline Comments, Inward Dependency Sum etc. Prior research has investigated the impact of class imbalance on prediction technique's performance but not the impact of imbalance of other characteristics. Aims: We aim to explore the impact of different commit related characteristic's imbalance on DL defect prediction. Method: We investigated different characteristic's impact on the overall performance of DeepJIT and CC2Vec. We also propose a Siamese network based few-shot learning framework for JIT defect prediction (SifterJIT) combining Siamese network and DeepJIT. Results: Our results show that DeepJIT and CC2Vec lose out on the performance by around 20% when trained and tested on imbalanced data. However, SifterJIT can outperform state-of-the-art DL techniques with an average of 8.65% AUC score, 11% precision, and 6% F1-score improvement. Conclusions: Our results highlight that dataset imbalanced in terms of commit characteristics can significantly impact prediction performance, and few-shot learning based techniques can help alleviate the situation.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {7},
numpages = {12},
keywords = {software engineering, few-shot learning, defect prediction, Deep learning},
location = {Bari, Italy},
series = {ESEM '21}
}

@article{10.1007/s11277-017-5069-3,
author = {Dong, Feng and Wang, Junfeng and Li, Qi and Xu, Guoai and Zhang, Shaodong},
title = {Defect Prediction in Android Binary Executables Using Deep Neural Network},
year = {2018},
issue_date = {October   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {102},
number = {3},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-017-5069-3},
doi = {10.1007/s11277-017-5069-3},
abstract = {Software defect prediction locates defective code to help developers improve the security of software. However, existing studies on software defect prediction are mostly limited to the source code. Defect prediction for Android binary executables (called apks) has never been explored in previous studies. In this paper, we propose an explorative study of defect prediction in Android apks. We first propose smali2vec, a new approach to generate features that capture the characteristics of smali (decompiled files of apks) files in apks. Smali2vec extracts both token and semantic features of the defective files in apks and such comprehensive features are needed for building accurate prediction models. Then we leverage deep neural network (DNN), which is one of the most common architecture of deep learning networks, to train and build the defect prediction model in order to achieve accuracy. We apply our defect prediction model to more than 90,000 smali files from 50 Android apks and the results show that our model could achieve an AUC (the area under the receiver operating characteristic curve) of 85.98% and it is capable of predicting defects in apks. Furthermore, the DNN is proved to have a better performance than the traditional shallow machine learning algorithms (e.g., support vector machine and naive bayes) used in previous studies. The model has been used in our practical work and helped locate many defective files in apks.},
journal = {Wirel. Pers. Commun.},
month = oct,
pages = {2261–2285},
numpages = {25},
keywords = {Software defect prediction, Mobile security, Machine learning, Deep neural network, Android binary executables}
}

@article{10.1007/s11219-015-9287-1,
author = {Ryu, Duksan and Jang, Jong-In and Baik, Jongmoon},
title = {A transfer cost-sensitive boosting approach for cross-project defect prediction},
year = {2017},
issue_date = {March     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-015-9287-1},
doi = {10.1007/s11219-015-9287-1},
abstract = {Software defect prediction has been regarded as one of the crucial tasks to improve software quality by effectively allocating valuable resources to fault-prone modules. It is necessary to have a sufficient set of historical data for building a predictor. Without a set of sufficient historical data within a company, cross-project defect prediction (CPDP) can be employed where data from other companies are used to build predictors. In such cases, a transfer learning technique, which extracts common knowledge from source projects and transfers it to a target project, can be used to enhance the prediction performance. There exists the class imbalance problem, which causes difficulties for the learner to predict defects. The main impacts of imbalanced data under cross-project settings have not been investigated in depth. We propose a transfer cost-sensitive boosting method that considers both knowledge transfer and class imbalance for CPDP when given a small amount of labeled target data. The proposed approach performs boosting that assigns weights to the training instances with consideration of both distributional characteristics and the class imbalance. Through comparative experiments with the transfer learning and the class imbalance learning techniques, we show that the proposed model provides significantly higher defect detection accuracy while retaining better overall performance. As a result, a combination of transfer learning and class imbalance learning is highly effective for improving the prediction performance under cross-project settings. The proposed approach will help to design an effective prediction model for CPDP. The improved defect prediction performance could help to direct software quality assurance activities and reduce costs. Consequently, the quality of software can be managed effectively.},
journal = {Software Quality Journal},
month = mar,
pages = {235–272},
numpages = {38},
keywords = {Transfer learning, Software defect prediction, Cross-project defect prediction, Cost-sensitive learning, Class imbalance, Boosting}
}

@article{10.1007/s00521-019-04651-6,
author = {Dib, M. A. and Oliveira, N. J. and Marques, A. E. and Oliveira, M. C. and Fernandes, J. V. and Ribeiro, B. M. and Prates, P. A.},
title = {Single and ensemble classifiers for defect prediction in sheet metal forming under variability},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {16},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04651-6},
doi = {10.1007/s00521-019-04651-6},
abstract = {This paper presents an approach, based on machine learning techniques, to predict the occurrence of defects in sheet metal forming processes, exposed to sources of scatter in the material properties and process parameters. An empirical analysis of performance of ML techniques is presented, considering both single learning and ensemble models. These are trained using data sets populated with numerical simulation results of two sheet metal forming processes: U-Channel and Square Cup. Data sets were built for three distinct steel sheets. A total of eleven input features, related to the mechanical properties, sheet thickness and process parameters, were considered; also, two types of defects (outputs) were analysed for each process. The sampling data were generated, assuming that the variability of each input feature is described by a normal distribution. For a given type of defect, most single classifiers show similar performances, regardless of the material. When comparing single learning and ensemble models, the latter can provide an efficient alternative. The fact that ensemble predictive models present relatively high performances, combined with the possibility of reconciling model bias and variance, offer a promising direction for its application in industrial environment.},
journal = {Neural Comput. Appl.},
month = aug,
pages = {12335–12349},
numpages = {15},
keywords = {Sheet metal forming, Defect prediction, Ensemble learning, Machine learning}
}

@article{10.1016/j.asoc.2016.04.032,
author = {Malhotra, Ruchika},
title = {An empirical framework for defect prediction using machine learning techniques with Android software},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.04.032},
doi = {10.1016/j.asoc.2016.04.032},
abstract = {Display Omitted Use of appropriate and large number data sets for comparing 18 ML techniques for defect prediction using object-oriented metrics.Effective performance of the predicted models assessed using appropriate performance measures.Reliability of the results evaluated using statistical test and post-hoc analysis.Validating the predicted models using inter-release validation on various releases of seven application packages of Android software. ContextSoftware defect prediction is important for identification of defect-prone parts of a software. Defect prediction models can be developed using software metrics in combination with defect data for predicting defective classes. Various studies have been conducted to find the relationship between software metrics and defect proneness, but there are few studies that statistically determine the effectiveness of the results. ObjectiveThe main objectives of the study are (i) comparison of the machine-learning techniques using data sets obtained from popular open source software (ii) use of appropriate performance measures for measuring the performance of defect prediction models (iii) use of statistical tests for effective comparison of machine-learning techniques and (iv) validation of models over different releases of data sets. MethodIn this study we use object-oriented metrics for predicting defective classes using 18 machine-learning techniques. The proposed framework has been applied to seven application packages of well known, widely used Android operating system viz. Contact, MMS, Bluetooth, Email, Calendar, Gallery2 and Telephony. The results are validated using 10-fold and inter-release validation methods. The reliability and significance of the results are evaluated using statistical test and post-hoc analysis. ResultsThe results show that the area under the curve measure for Nave Bayes, LogitBoost and Multilayer Perceptron is above 0.7 in most of the cases. The results also depict that the difference between the ML techniques is statistically significant. However, it is also proved that the Support Vector Machines based techniques such as Support Vector Machines and voted perceptron do not possess the predictive capability for predicting defects. ConclusionThe results confirm the predictive capability of various ML techniques for developing defect prediction models. The results also confirm the superiority of one ML technique over the other ML techniques. Thus, the software engineers can use the results obtained from this study in the early phases of the software development for identifying defect-prone classes of given software.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1034–1050},
numpages = {17},
keywords = {Statistical tests, Software defect proneness, Object-oriented metrics, Machine-learning, Inter-release validation}
}

@article{10.1155/2021/5558561,
author = {Shao, Yanli and Zhao, Jingru and Wang, Xingqi and Wu, Weiwei and Fang, Jinglong and Gao, Honghao},
title = {Research on Cross-Company Defect Prediction Method to Improve Software Security},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/5558561},
doi = {10.1155/2021/5558561},
abstract = {As the scale and complexity of software increase, software security issues have become the focus of society. Software defect prediction (SDP) is an important means to assist developers in discovering and repairing potential defects that may endanger software security in advance and improving software security and reliability. Currently, cross-project defect prediction (CPDP) and cross-company defect prediction (CCDP) are widely studied to improve the defect prediction performance, but there are still problems such as inconsistent metrics and large differences in data distribution between source and target projects. Therefore, a new CCDP method based on metric matching and sample weight setting is proposed in this study. First, a clustering-based metric matching method is proposed. The multigranularity metric feature vector is extracted to unify the metric dimension while maximally retaining the information contained in the metrics. Then use metric clustering to eliminate metric redundancy and extract representative metrics through principal component analysis (PCA) to support one-to-one metric matching. This strategy not only solves the metric inconsistent and redundancy problem but also transforms the cross-company heterogeneous defect prediction problem into a homogeneous problem. Second, a sample weight setting method is proposed to transform the source data distribution. Wherein the statistical source sample frequency information is set as an impact factor to increase the weight of source samples that are more similar to the target samples, which improves the data distribution similarity between the source and target projects, thereby building a more accurate prediction model. Finally, after the above two-step processing, some classical machine learning methods are applied to build the prediction model, and 12 project datasets in NASA and PROMISE are used for performance comparison. Experimental results prove that the proposed method has superior prediction performance over other mainstream CCDP methods.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {19}
}

@inproceedings{10.1007/978-3-030-30952-7_16,
author = {Liu, Xinyue and Li, Yanhui},
title = {Is Bigger Data Better for Defect Prediction: Examining the Impact of Data Size on Supervised and Unsupervised Defect Prediction},
year = {2019},
isbn = {978-3-030-30951-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30952-7_16},
doi = {10.1007/978-3-030-30952-7_16},
abstract = {Defect prediction could help software practitioners to predict the future occurrence of bugs in the software code regions. In order to improve the accuracy of defect prediction, dozens of supervised and unsupervised methods have been put forward and achieved good results in this field. One limiting factor of defect prediction is that the data size of defect data is not big, which restricts the scope of application with defect prediction models. In this study, we try to construct bigger defect datasets by merging available datasets with the same measurement dimension and check whether bigger data will bring better defect prediction performance with supervised and unsupervised models or not. The results of our experiment reveal that larger-scale dataset doesn’t bring improvements of both supervised and unsupervised classifiers.},
booktitle = {Web Information Systems and Applications: 16th International Conference, WISA 2019, Qingdao, China, September 20-22, 2019, Proceedings},
pages = {138–150},
numpages = {13},
keywords = {Defect prediction, Supervised, Classifier, Data size},
location = {Qingdao, China}
}

@inproceedings{10.1109/QSIC.2012.19,
author = {Wang, Jun and Shen, Beijun and Chen, Yuting},
title = {Compressed C4.5 Models for Software Defect Prediction},
year = {2012},
isbn = {9780769548333},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/QSIC.2012.19},
doi = {10.1109/QSIC.2012.19},
abstract = {Defects in every software must be handled properly, and the number of defects directly reflects the quality of a software. In recent years, researchers have applied data mining and machine learning methods to predicting software defects. However, in their studies, the method in which the machine learning models are directly adopted may not be precise enough. Optimizing the machine learning models used in defects prediction will improve the prediction accuracy. In this paper, aiming at the characteristics of the metrics mined from the open source software, we proposed three new defect prediction models based on C4.5 model. The new models introduce the Spearman's rank correlation coefficient to the basis of choosing root node of the decision tree which makes the models better on defects prediction. In order to verify the effectiveness of the improved models, an experimental scheme is designed. In the experiment, we compared the prediction accuracies of the existing models and the improved models and the result showed that the improved models reduced the size of the decision tree by 49.91% on average and increased the prediction accuracy by 4.58% and 4.87% on two modules used in the experiment.},
booktitle = {Proceedings of the 2012 12th International Conference on Quality Software},
pages = {13–16},
numpages = {4},
keywords = {Software Repository, Defect Prediction, Decision Tree Learner, Data Mining},
series = {QSIC '12}
}

@article{10.1007/s00521-021-05811-3,
author = {Mehta, Sweta and Patnaik, K. Sridhar},
title = {Improved prediction of software defects using ensemble machine learning techniques},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {16},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05811-3},
doi = {10.1007/s00521-021-05811-3},
abstract = {Software testing process is a crucial part in software development. Generally the errors made by developers get fixed at a later stage of the software development process. This increases the impact of the defect. To prevent this, defects need to be predicted during the initial days of the software development, which in turn helps in efficient utilization of the testing resources. Defect prediction process involves classification of software modules into defect prone and non-defect prone. This paper aims to reduce the impact of two major issues faced during defect prediction, i.e., data imbalance and high dimensionality of the defect datasets. In this research work, various software metrics are evaluated using feature selection techniques such as Recursive Feature Elimination (RFE), Correlation-based feature selection, Lasso, Ridge, ElasticNet and Boruta. Logistic Regression, Decision Trees, K-nearest neighbor, Support Vector Machines and Ensemble Learning are some of the algorithms in machine learning that have been used in combination with the feature extraction and feature selection techniques for classifying the modules in software as defect prone and non-defect prone. The proposed model uses combination of Partial Least Square (PLS) Regression and RFE for dimension reduction which is further combined with Synthetic Minority Oversampling Technique due to the imbalanced nature of the used datasets. It has been observed that XGBoost and Stacking Ensemble technique gave best results for all the datasets with defect prediction accuracy more than 0.9 as compared to algorithms used in the research work.},
journal = {Neural Comput. Appl.},
month = aug,
pages = {10551–10562},
numpages = {12},
keywords = {Stacking ensemble classifier, XGBoost, Machine learning algorithms, Data imbalance, Dimension reduction, Defect prediction}
}

@article{10.1016/j.engappai.2021.104504,
author = {Vaish, Rachna and Dwivedi, U.D. and Tewari, Saurabh and Tripathi, S.M.},
title = {Machine learning applications in power system fault diagnosis: Research advancements and perspectives},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2021.104504},
doi = {10.1016/j.engappai.2021.104504},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
numpages = {33},
keywords = {Unsupervised learning, Transfer learning, Supervised learning, Reinforcement learning, Machine learning (ML)}
}

@article{10.1007/s10664-020-09878-9,
author = {Bangash, Abdul Ali and Sahar, Hareem and Hindle, Abram and Ali, Karim},
title = {On the time-based conclusion stability of cross-project defect prediction models},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09878-9},
doi = {10.1007/s10664-020-09878-9},
abstract = {Researchers in empirical software engineering often make claims based on observable data such as defect reports. Unfortunately, in many cases, these claims are generalized beyond the data sets that have been evaluated. Will the researcher’s conclusions hold a year from now for the same software projects? Perhaps not. Recent studies show that in the area of Software Analytics, conclusions over different data sets are usually inconsistent. In this article, we empirically investigate whether conclusions in the area of cross-project defect prediction truly exhibit stability throughout time or not. Our investigation applies a time-aware evaluation approach where models are trained only on the past, and evaluations are executed only on the future. Through this time-aware evaluation, we show that depending on which time period we evaluate defect predictors, their performance, in terms of F-Score, the area under the curve (AUC), and Mathews Correlation Coefficient (MCC), varies and their results are not consistent. The next release of a product, which is significantly different from its prior release, may drastically change defect prediction performance. Therefore, without knowing about the conclusion stability, empirical software engineering researchers should limit their claims of performance within the contexts of evaluation, because broad claims about defect prediction performance might be contradicted by the next upcoming release of a product under analysis.},
journal = {Empirical Softw. Engg.},
month = nov,
pages = {5047–5083},
numpages = {37},
keywords = {Time-aware evaluation, Defect prediction, Conclusion stability}
}

@article{10.1504/IJISTA.2016.076102,
author = {Rong, Xiaotao and Li, Feixiang and Cui, Zhihua},
title = {A model for software defect prediction using support vector machine based on CBA},
year = {2016},
issue_date = {April 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {15},
number = {1},
issn = {1740-8865},
url = {https://doi.org/10.1504/IJISTA.2016.076102},
doi = {10.1504/IJISTA.2016.076102},
abstract = {Software defection prediction is not only crucial for improving software quality, but also helpful for software test effort estimation. As is well-known, 80% of the fault happens in 20% of the modules. Therefore, we need to find out the most error prone modules accurately and correct them in time to save time, money, and energy. Support vector machine SVM is an advanced classification method that fits the defection classification. However, studies show that, the value of parameters of SVM model has a remarkable influence on its classification accuracy and the selection process lacks theory guidance that makes the SVM model uncertainty and low efficiency. In this paper, a CBA-SVM software defect prediction model is proposed, which take advantage of the non-linear computing ability of SVM model and optimisation capacity of bat algorithm with centroid strategy CBA. Through the experimental comparison with other models, CBA-SVM is proved to have a higher accuracy.},
journal = {Int. J. Intell. Syst. Technol. Appl.},
month = apr,
pages = {19–34},
numpages = {16}
}

@inproceedings{10.1109/RAISE.2019.00016,
author = {Humphreys, Jack and Dam, Hoa Khanh},
title = {An explainable deep model for defect prediction},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/RAISE.2019.00016},
doi = {10.1109/RAISE.2019.00016},
abstract = {Self attention transformer encoders represent an effective method for sequence to class prediction tasks as they can disentangle long distance dependencies and have many regularising effects. We achieve results substantially better than state of the art in one such task, namely, defect prediction and with many added benefits. Existing techniques do not normalise for correlations that are inversely proportional to the usefulness of the prediction but do, in fact, go further, specifically exploiting these features which is tantamount to data leakage. Our model is end-to-end trainable and has the potential capability to explain its prediction. This explainability provides insights and potential causes of a model's decisions, the absence of which has stopped defect prediction from gaining any traction in industry.},
booktitle = {Proceedings of the 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {49–55},
numpages = {7},
keywords = {defect prediction, deep learning},
location = {Montreal, Quebec, Canada},
series = {RAISE '19}
}

@inproceedings{10.1145/3379247.3379278,
author = {Ahmed, Md. Razu and Ali, Md. Asraf and Ahmed, Nasim and Zamal, Md. Fahad Bin and Shamrat, F.M. Javed Mehedi},
title = {The Impact of Software Fault Prediction in Real-World Application: An Automated Approach for Software Engineering},
year = {2020},
isbn = {9781450376730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379247.3379278},
doi = {10.1145/3379247.3379278},
abstract = {Software fault prediction and proneness has long been considered as a critical issue for the tech industry and software professionals. In the traditional techniques, it requires previous experience of faults or a faulty module while detecting the software faults inside an application. An automated software fault recovery models enable the software to significantly predict and recover software faults using machine learning techniques. Such ability of the feature makes the software to run more effectively and reduce the faults, time and cost. In this paper, we proposed a software defect predictive development models using machine learning techniques that can enable the software to continue its projected task. Moreover, we used different prominent evaluation benchmark to evaluate the model's performance such as ten-fold cross-validation techniques, precision, recall, specificity, f 1 measure, and accuracy. This study reports a significant classification performance of 98-100% using SVM on three defect datasets in terms of f1 measure. However, software practitioners and researchers can attain independent understanding from this study while selecting automated task for their intended application.},
booktitle = {Proceedings of 2020 6th International Conference on Computing and Data Engineering},
pages = {247–251},
numpages = {5},
keywords = {Software fault, Software engineering, Machine learning, Defect prediction},
location = {Sanya, China},
series = {ICCDE '20}
}

@inproceedings{10.1145/3183440.3183449,
author = {Eken, Beyza},
title = {Assessing personalized software defect predictors},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183449},
doi = {10.1145/3183440.3183449},
abstract = {Software defect prediction models guide developers and testers to identify defect prone software modules in fewer time and effort, compared to manual inspections of the source code. The state-of-the-art predictors on publicly available software engineering data could catch around 70% of the defects. While early studies mostly utilize static code properties of the software, recent studies incorporate the people factor into the prediction models, such as the number of developers that touched a code unit, the experience of the developer, and interaction and cognitive behaviors of developers. Those information could give a stronger clue about the defect-prone parts because they could explain defect injection patterns in software development. Personalization has been emerging in many other systems such as social platforms, web search engines such that people get customized recommendations based on their actions, profiles and interest. Following this point of view, customization in defect prediction with respect to each developer would increase predictions' accuracy and usefulness than traditional, general models. In this thesis, we focus on building a personalized defect prediction framework that gives instant feedback to the developer at change level, based on historical defect and change data. Our preliminary analysis of the personalized prediction models of 121 developers in six open source projects indicate that, a personalized approach is not always the best model when compared to general models built for six projects. Other factors such as project characteristics, developer's historical data, the context and frequency of contributions, and/or development methodologies might affect which model to consider in practice. Eventually, this topic is open to improvement with further empirical studies on each of these factors.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {488–491},
numpages = {4},
keywords = {bug prediction, customization, personalized defect prediction},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1002/smr.2172,
author = {Gong, Lina and Jiang, Shujuan and Jiang, Li},
title = {An improved transfer adaptive boosting approach for mixed‐project defect prediction},
year = {2019},
issue_date = {October 2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {31},
number = {10},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2172},
doi = {10.1002/smr.2172},
abstract = {Software defect prediction (SDP) has been a very important research topic in software engineering, since it can provide high‐quality results when given sufficient historical data of the project. Unfortunately, there are not abundant data to bulid the defect prediction model at the beginning of a project. For this scenario, one possible solution is to use data from other projects in the same company. However, using these data practically would get poor performance because of different distributional characteristics among projects. Also, software has more non‐defective instances than defective instances that may cause a significant bias towards defective instances. Considering these two problems, we propose an improved transfer adaptive boosting (ITrAdaBoost) approach for being given a small number of labeled data in the testing project. In our approach, ITrAdaBoost can not only employ the Matthews correlation coefficient (MCC) as the measure instead of accuracy rate but also use the asymmetric misclassification costs for non‐defective and defective instances. Extensive experiments on 18 public projects from four datasets indicate that: (a) our approach significantly outperforms state‐of‐the‐art cross‐project defect prediction (CPDP) approaches, and (b) our approach can obtain comparable prediction performances in contrast with within project prediction results. Consequently, the proposed approach can build an effective prediction model with a small number of labeled instances for mixed‐project defect prediction (MPDP).For mixed‐project defect prediction, improved transfer adaptive boosting approach (ITrAdaBoost) can not only employ the Matthews correlation coefficient (MCC) as the measure instead of accuracy rate, but also use the asymmetric misclassification costs for nondefective and defective instances. Extensive experiments on 18 public projects from four datasets indicate that (a) our approach significantly outperforms state‐of‐the‐art cross‐project defect prediction (CPDP) approaches, and (b) our approach can obtain comparable prediction performances in contrast with within project prediction results.


image
image},
journal = {J. Softw. Evol. Process},
month = oct,
numpages = {23},
keywords = {transfer learning, software defect prediction, mixed‐project, cross‐project, class imbalance}
}

@inproceedings{10.1109/IRI.2019.00030,
author = {Reddivari, Sandeep and Raman, Jayalakshmi},
title = {Software Quality Prediction: An Investigation Based on Machine Learning},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IRI.2019.00030},
doi = {10.1109/IRI.2019.00030},
abstract = {Irrespective of the type of software system that is being developed, producing and delivering high-quality software within the specified time and budget is crucial for many software businesses. The software process model has a major impact on the quality of the overall system - the longer a defect remains in the system undetected, the harder it becomes to fix. However, predicting the quality of the software in the early phases would immensely assist developers in software maintenance and quality assurance activities, and to allocate effort and resources more efficiently. This paper presents an evaluation of eight machine learning techniques in the context of reliability and maintainability. Reliability is investigated as the number of defects in a system and the maintainability is analyzed as the number of changes made in the system. Software metrics are direct reflections of various characteristics of software and are used in our study as the major attributes for training the models for both defect and maintainability prediction. Among the eight different techniques we experimented with, Random Forest provided the best results with an AUC of over 0.8 during both defect and maintenance prediction.},
booktitle = {2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI)},
pages = {115–122},
numpages = {8},
location = {Los Angeles, CA, USA}
}

@article{10.1504/ijiei.2021.120322,
author = {Lakra, Kirti and Chug, Anuradha},
title = {Application of metaheuristic techniques in software quality prediction: a systematic mapping study},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {4},
issn = {1758-8715},
url = {https://doi.org/10.1504/ijiei.2021.120322},
doi = {10.1504/ijiei.2021.120322},
abstract = {This paper focuses on the systematic review of various metaheuristic techniques employed for analysing different software quality aspects, including fault proneness, defect anticipation, change proneness, maintainability prediction, and software reliability prediction. It is observed that machine learning algorithms are still popular models, but metaheuristic algorithms are also gaining popularity in the field of software quality measurement. This is due to the fact that metaheuristic algorithms are more efficient in solving real-world, search-based, and optimisation problems. Initially, 90 papers were considered and analysed for conducting this study from 2010 to 2020, and 55 studies were shortlisted based on predesigned quality evaluation standards. Resultantly, particle swarm optimisation (PSO), and genetic algorithms came out as the most prominently used metaheuristic techniques for developing software quality models in 36.3% and 27.2% of the shortlisted studies, respectively. The current review will benefit other researchers by providing an insight into the current trends in software quality domain.},
journal = {Int. J. Intell. Eng. Inform.},
month = jan,
pages = {355–399},
numpages = {44},
keywords = {software quality improvement, software maintainability prediction, software reliability prediction, software change prediction, software defect prediction, software fault proneness, software quality, object-oriented metrics, metaheuristic techniques}
}

@article{10.1016/j.eswa.2009.12.056,
author = {Zheng, Jun},
title = {Cost-sensitive boosting neural networks for software defect prediction},
year = {2010},
issue_date = {June, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {6},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2009.12.056},
doi = {10.1016/j.eswa.2009.12.056},
abstract = {Software defect predictors which classify the software modules into defect-prone and not-defect-prone classes are effective tools to maintain the high quality of software products. The early prediction of defect-proneness of the modules can allow software developers to allocate the limited resources on those defect-prone modules such that high quality software can be produced on time and within budget. In the process of software defect prediction, the misclassification of defect-prone modules generally incurs much higher cost than the misclassification of not-defect-prone ones. Most of the previously developed predication models do not consider this cost issue. In this paper, three cost-sensitive boosting algorithms are studied to boost neural networks for software defect prediction. The first algorithm based on threshold-moving tries to move the classification threshold towards the not-fault-prone modules such that more fault-prone modules can be classified correctly. The other two weight-updating based algorithms incorporate the misclassification costs into the weight-update rule of boosting procedure such that the algorithms boost more weights on the samples associated with misclassified defect-prone modules. The performances of the three algorithms are evaluated by using four datasets from NASA projects in terms of a singular measure, the Normalized Expected Cost of Misclassification (NECM). The experimental results suggest that threshold-moving is the best choice to build cost-sensitive software defect prediction models with boosted neural networks among the three algorithms studied, especially for the datasets from projects developed by object-oriented language.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {4537–4543},
numpages = {7},
keywords = {Software defect, Neural networks, Cost-sensitive, Adaboost}
}

@article{10.1016/j.is.2015.02.006,
author = {Siers, Michael J. and Islam, Md Zahidul},
title = {Software defect prediction using a cost sensitive decision forest and voting, and a potential solution to the class imbalance problem},
year = {2015},
issue_date = {July 2015},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {51},
number = {C},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2015.02.006},
doi = {10.1016/j.is.2015.02.006},
abstract = {Software development projects inevitably accumulate defects throughout the development process. Due to the high cost that defects can incur, careful consideration is crucial when predicting which sections of code are likely to contain defects. Classification algorithms used in machine learning can be used to create classifiers which can be used to predict defects. While traditional classification algorithms optimize for accuracy, cost-sensitive classification methods attempt to make predictions which incur the lowest classification cost. In this paper we propose a cost-sensitive classification technique called CSForest which is an ensemble of decision trees. We also propose a cost-sensitive voting technique called CSVoting in order to take advantage of the set of decision trees in minimizing the classification cost. We then investigate a potential solution to class imbalance within our decision forest algorithm. We empirically evaluate the proposed techniques comparing them with six (6) classifier algorithms on six (6) publicly available clean datasets that are commonly used in the research on software defect prediction. Our initial experimental results indicate a clear superiority of the proposed techniques over the existing ones. Author-HighlightsSDP is short for Software Defect Prediction.We show that there is not a clear winner in the studied existing methods for SDP*.A cost-sensitive decision forest and voting technique are proposed.The superiority of the proposed techniques is shown.A proposed framework for the forest algorithm for handling class imbalance.},
journal = {Inf. Syst.},
month = jul,
pages = {62–71},
numpages = {10},
keywords = {Software defect prediction, Forest voting, Decision forest, Cost-sensitive, Class imbalance}
}

@inproceedings{10.1145/3412841.3442019,
author = {Zhao, Kunsong and Xu, Zhou and Yan, Meng and Tang, Yutian and Fan, Ming and Catolino, Gemma},
title = {Just-in-time defect prediction for Android apps via imbalanced deep learning model},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442019},
doi = {10.1145/3412841.3442019},
abstract = {Android mobile apps have played important roles in our daily life and work. To meet new requirements from users, the mobile apps encounter frequent updates, which involves in a large quantity of code commits. Previous studies proposed to apply Just-in-Time (JIT) defect prediction for mobile apps to timely identify whether new code commits can introduce defects into apps, aiming to assure the quality of mobile apps. In general, the number of defective commit instances is much fewer than that of clean ones, in other words, the defect data is class imbalanced. In this work, we propose a novel Imbalanced Deep Learning model, called IDL, to conduct JIT defect prediction task for Android mobile apps. More specifically, we introduce a state-of-the-art cost-sensitive cross-entropy loss function into the deep neural network to learn the high-level feature representation, in which the loss function alleviates the class imbalance issue by taking the prior probability of the two types of classes into account. We conduct experiments on a benchmark defect data consisting of 12 Android mobile apps. The results of rigorous experiments show that our proposed IDL model performs significantly better than 23 comparative imbalanced learning methods in terms of Matthews correlation coefficient performance indicator.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1447–1454},
numpages = {8},
keywords = {JIT defect prediction, imbalanced learning, mobile apps},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@article{10.1016/j.asoc.2016.05.038,
author = {Serdar Bier, M. and Diri, Banu},
title = {Defect prediction for Cascading Style Sheets},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.038},
doi = {10.1016/j.asoc.2016.05.038},
abstract = {Graphical abstractDisplay Omitted Testing is a crucial activity in software development. However exhaustive testing of a given software is impossible in practice because projects have serious time and budget limitations. Therefore, software testing teams need guidance about which modules they should focus on. Defect prediction techniques are useful for this situation because they let testers to identify and focus on defect prone parts of software. These techniques are essential for software teams, because they help teams to efficiently allocate their precious resources in testing phase. Software defect prediction has been an active research area in recent years. Researchers in this field have been using different types of metrics in their prediction models. However, value of extracting static code metrics for style sheet languages has been ignored until now. User experience is a very important part of web applications and its mostly provided using Cascading Style Sheets (CSS). In this research, our aim is to improve defect prediction performance for web applications by utilizing metrics generated from CSS code. We generated datasets from four open source web applications to conduct our experiments. Defect prediction is then performed using three different well-known machine learning algorithms. The results revealed that static code metrics based defect prediction techniques can be performed effectively to improve quality of CSS code in web applications. Therefore we recommend utilizing domain-specific characteristics of applications in defect prediction as they result in significantly high prediction performance with low costs.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1078–1084},
numpages = {7},
keywords = {Web sites, Software quality, Software Metrics, Defect prediction}
}

@article{10.1007/s10462-017-9563-5,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {A study on software fault prediction techniques},
year = {2019},
issue_date = {February  2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {2},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-017-9563-5},
doi = {10.1007/s10462-017-9563-5},
abstract = {Software fault prediction aims to identify fault-prone software modules by using some underlying properties of the software project before the actual testing process begins. It helps in obtaining desired software quality with optimized cost and effort. Initially, this paper provides an overview of the software fault prediction process. Next, different dimensions of software fault prediction process are explored and discussed. This review aims to help with the understanding of various elements associated with fault prediction process and to explore various issues involved in the software fault prediction. We search through various digital libraries and identify all the relevant papers published since 1993. The review of these papers are grouped into three classes: software metrics, fault prediction techniques, and data quality issues. For each of the class, taxonomical classification of different techniques and our observations have also been presented. The review and summarization in the tabular form are also given. At the end of the paper, the statistical analysis, observations, challenges, and future directions of software fault prediction have been discussed.},
journal = {Artif. Intell. Rev.},
month = feb,
pages = {255–327},
numpages = {73},
keywords = {Taxonomic classification, Software metrics, Software fault prediction, Software fault datasets, Fault prediction techniques}
}

@article{10.3233/JIFS-179459,
author = {Bashir, Kamal and Li, Tianrui and Yohannese, Chubato Wondaferaw and Yahaya, Mahama and Kahraman, Cengiz},
title = {SMOTEFRIS-INFFC: Handling the challenge of borderline and noisy examples in imbalanced learning for software defect prediction},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {38},
number = {1},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179459},
doi = {10.3233/JIFS-179459},
abstract = {The object of Software Defect Prediction (SDP) is to identify modules that are prone to defect. This is achieved by training prediction models with datasets obtained by mining software historical depositories. When one acquires data through this approach, it often includes class imbalance which has an unequal class representation among their example. We hypothesize that the imbalance learning is not a problem in itself and decrease in performance is also influenced by other factors related to class distribution in the data. One of these is the existence of noisy and borderline examples. Thus, the objective of our research is to propose a novel preprocessing method using Synthetic Minority Over-Sampling Technique (SMOTE), Fuzzy-rough Instance Selection type II (FRIS-II) and Iterative Noise Filter based on the Fusion of Classifiers (INFFC) which can overcome these problems. The experimental results show that the new proposal significantly outperformed all the methods compared in this study.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {917–933},
numpages = {17},
keywords = {noise filtering, fuzzy rough set, data sampling, Software defect prediction}
}

@inproceedings{10.1145/2601248.2601294,
author = {Rodriguez, Daniel and Herraiz, Israel and Harrison, Rachel and Dolado, Javier and Riquelme, Jos\'{e} C.},
title = {Preliminary comparison of techniques for dealing with imbalance in software defect prediction},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601294},
doi = {10.1145/2601248.2601294},
abstract = {Imbalanced data is a common problem in data mining when dealing with classification problems, where samples of a class vastly outnumber other classes. In this situation, many data mining algorithms generate poor models as they try to optimize the overall accuracy and perform badly in classes with very few samples. Software Engineering data in general and defect prediction datasets are not an exception and in this paper, we compare different approaches, namely sampling, cost-sensitive, ensemble and hybrid approaches to the problem of defect prediction with different datasets preprocessed differently. We have used the well-known NASA datasets curated by Shepperd et al. There are differences in the results depending on the characteristics of the dataset and the evaluation metrics, especially if duplicates and inconsistencies are removed as a preprocessing step.Further Results and replication package: http://www.cc.uah.es/drg/ease14},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {43},
numpages = {10},
keywords = {imbalanced data, defect prediction, data quality},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@article{10.3233/KES-200029,
author = {Singh, Pradeep and Verma, Shrish},
title = {ACO based comprehensive model for software fault prediction},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {24},
number = {1},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-200029},
doi = {10.3233/KES-200029},
abstract = {The comprehensive models can be used for software quality modelling which involves prediction of low-quality modules using interpretable rules. Such comprehensive model can guide the design and testing team to focus on the poor quality modules, thereby, limited resources allocated for software quality inspection can be targeted only towards modules that are likely to be defective. Ant Colony Optimization (ACO) based learner is one potential way to obtain rules that can classify the software modules faulty and not faulty. This paper investigates ACO based mining approach with ROC based rule quality updation to constructs a rule-based software fault prediction model with useful metrics. We have also investigated the effect of feature selection on ACO based and other benchmark algorithms. We tested the proposed method on several publicly available software fault data sets. We compared the performance of ACO based learning with the results of three benchmark classifiers on the basis of area under the receiver operating characteristic curve. The evaluation of performance measure proves that the ACO based learner outperforms other benchmark techniques.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {63–71},
numpages = {9},
keywords = {ACO, fault prediction, Software metric}
}

@article{10.4018/ijssci.2014040101,
author = {Mishra, Bharavi and Shukla, K.K.},
title = {Software Defect Prediction Based on GUHA Data Mining Procedure and Multi-Objective Pareto Efficient Rule Selection},
year = {2014},
issue_date = {April 2014},
publisher = {IGI Global},
address = {USA},
volume = {6},
number = {2},
issn = {1942-9045},
url = {https://doi.org/10.4018/ijssci.2014040101},
doi = {10.4018/ijssci.2014040101},
abstract = {Software defect prediction, if is effective, enables the developers to distribute their testing efforts efficiently and let them focus on defect prone modules. It would be very resource consuming to test all the modules while the defect lies in fraction of modules. Information about fault-proneness of classes and methods can be used to develop new strategies which can help mitigate the overall development cost and increase the customer satisfaction. Several machine learning strategies have been used in recent past to identify defective modules. These models are built using publicly available historical software defect data sets. Most of the proposed techniques are not able to deal with the class imbalance problem efficiently. Therefore, it is necessary to develop a prediction model which consists of small simple and comprehensible rules. Considering these facts, in this paper, the authors propose a novel defect prediction approach named GUHA based Classification Association Rule Mining algorithm G-CARM where "GUHA" stands for General Unary Hypothesis Automaton. G-CARM approach is primarily based on Classification Association Rule Mining, and deploys a two stage process involving attribute discretization, and rule generation using GUHA. GUHA is oldest yet very powerful method of pattern mining. The basic idea of GUHA procedure is to mine the interesting attribute patterns that indicate defect proneness. The new method has been compared against five other models reported in recent literature viz. Naive Bayes, Support Vector Machine, RIPPER, J48 and Nearest Neighbour classifier by using several measures, including AUC and probability of detection. The experimental results indicate that the prediction performance of G-CARM approach is better than other prediction approaches. The authors' approach achieved 76% mean recall and 83% mean precision for defective modules and 93% mean recall and 83% mean precision for non-defective modules on CM1, KC1, KC2 and Eclipse data sets. Further defect rule generation process often generates a large number of rules which require considerable efforts while using these rules as a defect predictor, hence, a rule sub-set selection process is also proposed to select best set of rules according to the requirements. Evolution criteria for defect prediction like sensitivity, specificity, precision often compete against each other. It is therefore, important to use multi-objective optimization algorithms for selecting prediction rules. In this paper the authors report prediction rules that are Pareto efficient in the sense that no further improvements in the rule set is possible without sacrificing some performance criteria. Non-Dominated Sorting Genetic Algorithm has been used to find Pareto front and defect prediction rules.},
journal = {Int. J. Softw. Sci. Comput. Intell.},
month = apr,
pages = {1–29},
numpages = {29},
keywords = {Pareto Optimality, General Unary Hypothesis Automaton GUHA, Fault Prediction, Defect Patterns, Data Mining}
}

@article{10.1016/j.compeleceng.2018.02.043,
author = {Choudhary, Garvit Rajesh and Kumar, Sandeep and Kumar, Kuldeep and Mishra, Alok and Catal, Cagatay},
title = {Empirical analysis of change metrics for software fault prediction},
year = {2018},
issue_date = {Apr 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {67},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2018.02.043},
doi = {10.1016/j.compeleceng.2018.02.043},
journal = {Comput. Electr. Eng.},
month = apr,
pages = {15–24},
numpages = {10},
keywords = {Defect prediction, Software quality, Metrics, Change log, Eclipse, Software fault prediction}
}

@inproceedings{10.1109/ESEM.2017.48,
author = {Yan, Meng and Fang, Yicheng and Lo, David and Xia, Xin and Zhang, Xiaohong},
title = {File-level defect prediction: unsupervised vs. supervised models},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.48},
doi = {10.1109/ESEM.2017.48},
abstract = {Background: Software defect models can help software quality assurance teams to allocate testing or code review resources. A variety of techniques have been used to build defect prediction models, including supervised and unsupervised methods. Recently, Yang et al. [1] surprisingly find that unsupervised models can perform statistically significantly better than supervised models in effort-aware change-level defect prediction. However, little is known about relative performance of unsupervised and supervised models for effort-aware file-level defect prediction. Goal: Inspired by their work, we aim to investigate whether a similar finding holds in effort-aware file-level defect prediction. Method: We replicate Yang et al.'s study on PROMISE dataset with totally ten projects. We compare the effectiveness of unsupervised and supervised prediction models for effort-aware file-level defect prediction. Results: We find that the conclusion of Yang et al. [1] does not hold under within-project but holds under cross-project setting for file-level defect prediction. In addition, following the recommendations given by the best unsupervised model, developers needs to inspect statistically significantly more files than that of supervised models considering the same inspection effort (i.e., LOC). Conclusions: (a) Unsupervised models do not perform statistically significantly better than state-of-art supervised model under within-project setting, (b) Unsupervised models can perform statistically significantly better than state-of-art supervised model under cross-project setting, (c) We suggest that not only LOC but also number of files needed to be inspected should be considered when evaluating effort-aware file-level defect prediction models.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {344–353},
numpages = {10},
keywords = {replication study, inspection effort, effort-aware defect prediction},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3490725.3490739,
author = {Shi, Linlin and Yu, Pengfei and He, Shilie and Zhou, Zhenwei and Meng, Linghui and Liu, Junbin},
title = {Degradation Characteristics Analysis and Fault Prediction of Switching Power Supply Based on Data Mining},
year = {2022},
isbn = {9781450384247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490725.3490739},
doi = {10.1145/3490725.3490739},
abstract = {Fault prediction and health monitoring of DC-DC switching power supply plays an important role in the safe and reliable operation of power electronic equipment. In this paper, a long-term high temperature degradation test was carried out for DC-DC power supply, and the characteristic parameters related to device health, such as input current, input voltage, output current and output voltage, were collected during the test. Through data mining technology, we carry out data preprocessing, feature analysis, health index modeling and fault prediction analysis on the samples collected in the power supply test, so as to study the degradation and fault predictor of the power supply from the real power supply test degradation data. The research results of this paper have important engineering significance for the monitoring and health prediction of power supply.},
booktitle = {Proceedings of the 2021 4th International Conference on Machine Learning and Machine Intelligence},
pages = {89–98},
numpages = {10},
keywords = {feature analysis, Fault Prediction, Data Mining, DC-DC power supply},
location = {Hangzhou, China},
series = {MLMI '21}
}

@proceedings{10.1145/3340482,
title = {MaLTeSQuE 2019: Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@article{10.1007/s11704-017-6015-y,
author = {Zhang, Yun and Lo, David and Xia, Xin and Sun, Jianling},
title = {Combined classifier for cross-project defect prediction: an extended empirical study},
year = {2018},
issue_date = {April     2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {2},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-017-6015-y},
doi = {10.1007/s11704-017-6015-y},
abstract = {To facilitate developers in effective allocation of their testing and debugging efforts, many software defect prediction techniques have been proposed in the literature. These techniques can be used to predict classes that are more likely to be buggy based on the past history of classes, methods, or certain other code elements. These techniques are effective provided that a sufficient amount of data is available to train a prediction model. However, sufficient training data are rarely available for new software projects. To resolve this problem, cross-project defect prediction, which transfers a prediction model trained using data from one project to another, was proposed and is regarded as a new challenge in the area of defect prediction. Thus far, only a few cross-project defect prediction techniques have been proposed. To advance the state of the art, in this study, we investigated seven composite algorithms that integrate multiple machine learning classifiers to improve cross-project defect prediction. To evaluate the performance of the composite algorithms, we performed experiments on 10 open-source software systems from the PROMISE repository, which contain a total of 5,305 instances labeled as defective or clean. We compared the composite algorithms with the combined defect predictor where logistic regression is used as the meta classification algorithm (CODEPLogistic), which is the most recent cross-project defect prediction algorithm in terms of two standard evaluation metrics: cost effectiveness and F-measure. Our experimental results show that several algorithms outperform CODEPLogistic: Maximum voting shows the best performance in terms of F-measure and its average F-measure is superior to that of CODEPLogistic by 36.88%. Bootstrap aggregation (BaggingJ48) shows the best performance in terms of cost effectiveness and its average cost effectiveness is superior to that of CODEPLogistic by 15.34%.},
journal = {Front. Comput. Sci.},
month = apr,
pages = {280–296},
numpages = {17},
keywords = {defect prediction, cross-project, classifier combination}
}

@article{10.1007/s10796-013-9430-0,
author = {Khoshgoftaar, Taghi M. and Gao, Kehan and Napolitano, Amri and Wald, Randall},
title = {A comparative study of iterative and non-iterative feature selection techniques for software defect prediction},
year = {2014},
issue_date = {November  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {5},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-013-9430-0},
doi = {10.1007/s10796-013-9430-0},
abstract = {Two important problems which can affect the performance of classification models are high-dimensionality (an overabundance of independent features in the dataset) and imbalanced data (a skewed class distribution which creates at least one class with many fewer instances than other classes). To resolve these problems concurrently, we propose an iterative feature selection approach, which repeated applies data sampling (in order to address class imbalance) followed by feature selection (in order to address high-dimensionality), and finally we perform an aggregation step which combines the ranked feature lists from the separate iterations of sampling. This approach is designed to find a ranked feature list which is particularly effective on the more balanced dataset resulting from sampling while minimizing the risk of losing data through the sampling step and missing important features. To demonstrate this technique, we employ 18 different feature selection algorithms and Random Undersampling with two post-sampling class distributions. We also investigate the use of sampling and feature selection without the iterative step (e.g., using the ranked list from a single iteration, rather than combining the lists from multiple iterations), and compare these results from the version which uses iteration. Our study is carried out using three groups of datasets with different levels of class balance, all of which were collected from a real-world software system. All of our experiments use four different learners and one feature subset size. We find that our proposed iterative feature selection approach outperforms the non-iterative approach.},
journal = {Information Systems Frontiers},
month = nov,
pages = {801–822},
numpages = {22},
keywords = {Software defect prediction, Iterative feature selection, High dimensionality, Date sampling, Class imbalance}
}

@inproceedings{10.1007/978-3-030-87007-2_28,
author = {Kumar, Lov and Dastidar, Triyasha Ghosh and Murthy Neti, Lalitha Bhanu and Satapathy, Shashank Mouli and Misra, Sanjay and Kocher, Vipul and Padmanabhuni, Srinivas},
title = {Deep-Learning Approach with DeepXplore for Software Defect Severity Level Prediction},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_28},
doi = {10.1007/978-3-030-87007-2_28},
abstract = {Fixing the defects of earlier releases and working on fast and efficient fixing of those software defects is detrimental for the release of further versions. Bug tracking systems like Bugzilla get thousands of software defect reports every day. Manually handling those report to assign severity to the defects is not feasible. Earlier traditional Machine Learning methods have been used to predict the severity level from the defect description. This paper presents different deep learning models to predict defect severity level. Furthermore, the deep neural network was tested using a framework developed similar to that DeepXplore. Different word-embedding techniques, feature-selection techniques, sampling techniques and deep learning models are analyzed and compared for this study. In this paper, we have considered Descriptive statistics, Box-plot, and Significant tests to compare the developed models for defect severity level prediction. The three performance metrics used for testing the models are AUC, Accuracy and Neuron Coverage. This is a preliminary study on DNN testing on this dataset. Thus, the paper focuses on DeepXplore DNN testing technique. However further studies would be undertaken on comparative analysis of different DNN testing techniques on this dataset.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {398–410},
numpages = {13},
keywords = {Word embedding, Deep learning, Feature selection, Imbalance handling, Severity prediction},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3439961.3439991,
author = {Rivero, Luis and Diniz, Jo\~{a}o and Silva, Giovanni and Borralho, Gabriel and Braz Junior, Geraldo and Paiva, Anselmo and Alves, Erika and Oliveira, Milton},
title = {Deployment of a Machine Learning System for Predicting Lawsuits Against Power Companies: Lessons Learned from an Agile Testing Experience for Improving Software Quality},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439991},
doi = {10.1145/3439961.3439991},
abstract = {The advances in Machine Learning (ML) require software organizations to evolve their development processes in order to improve the quality of ML systems. Within the software development process, the testing stage of an ML system is more critical, considering that it is necessary to add data validation, trained model quality evaluation, and model validation to traditional unit, integration tests and system tests. In this paper, we focus on reporting the lessons learned of using model testing and exploratory testing within the context of the agile development process of an ML system that predicts lawsuits proneness in energy supply companies. Through the development of the project, the SCRUM agile methodology was applied and activities related to the development of the ML model and the development of the end-user application were defined. After the testing process of the ML model, we managed to achieve 93.89 accuracy; 95.58 specificity; 88.84 sensitivity; and 87.09 precision. Furthermore, we focused on the quality of use of the application embedding the ML model, by carrying out exploratory testing. As a result, through several iterations, different types of defects were identified and corrected. Our lessons learned support software engineers willing to develop ML systems that consider both the ML model and the end-user application.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {30},
numpages = {10},
keywords = {and Tools, and Testing, Verification, Validation, Software Processes, Methods},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@inproceedings{10.1145/3345629.3345638,
author = {Amasaki, Sousuke and Yokogawa, Tomoyuki and Aman, Hirohisa},
title = {Applying Cross Project Defect Prediction Approaches to Cross-Company Effort Estimation},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345638},
doi = {10.1145/3345629.3345638},
abstract = {BACKGROUND: Prediction systems in software engineering often suffer from the shortage of suitable data within a project. A promising solution is transfer learning that utilizes data from outside the project. Many transfer learning approaches have been proposed for defect prediction known as cross-project defect prediction (CPDP). In contrast, a few approaches have been proposed for software effort estimation known as cross-company software effort estimation (CCSEE). Both CCSEE and CPDP are engaged in a similar problem, and a few CPDP approaches are applicable as CCSEE in actual. It is thus beneficial for improving CCSEE performance to examine how well CPDP approaches can perform as CCSEE approaches. AIMS: To explore how well CPDP approaches work as CCSEE approaches. METHOD: An empirical experiment was conducted for evaluating the performance of CPDP approaches in CCSEE. We examined 7 CPDP approaches which were selected due to the easiness of application. Those approaches were applied to 8 data sets, each of which consists of a few subsets from different domains. The estimation results were evaluated with a common performance measure called SA. RESULTS: there were several CPDP approaches which could improve the estimation accuracy though the degree of improvement was not large. CONCLUSIONS: A straight forward application of selected CPDP approaches did not bring a clear effect. CCSEE may need specific transfer learning approaches for more improvement.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {76–79},
numpages = {4},
keywords = {cross-company effort estimation, cross-project defect prediction, transfer learning},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@article{10.1007/s10664-018-9679-5,
author = {Kondo, Masanari and Bezemer, Cor-Paul and Kamei, Yasutaka and Hassan, Ahmed E. and Mizuno, Osamu},
title = {The impact of feature reduction techniques on defect prediction models},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9679-5},
doi = {10.1007/s10664-018-9679-5},
abstract = {Defect prediction is an important task for preserving software quality. Most prior work on defect prediction uses software features, such as the number of lines of code, to predict whether a file or commit will be defective in the future. There are several reasons to keep the number of features that are used in a defect prediction model small. For example, using a small number of features avoids the problem of multicollinearity and the so-called `curse of dimensionality'. Feature selection and reduction techniques can help to reduce the number of features in a model. Feature selection techniques reduce the number of features in a model by selecting the most important ones, while feature reduction techniques reduce the number of features by creating new, combined features from the original features. Several recent studies have investigated the impact of feature selection techniques on defect prediction. However, there do not exist large-scale studies in which the impact of multiple feature reduction techniques on defect prediction is investigated. In this paper, we study the impact of eight feature reduction techniques on the performance and the variance in performance of five supervised learning and five unsupervised defect prediction models. In addition, we compare the impact of the studied feature reduction techniques with the impact of the two best-performing feature selection techniques (according to prior work). The following findings are the highlights of our study: (1) The studied correlation and consistency-based feature selection techniques result in the best-performing supervised defect prediction models, while feature reduction techniques using neural network-based techniques (restricted Boltzmann machine and autoencoder) result in the best-performing unsupervised defect prediction models. In both cases, the defect prediction models that use the selected/generated features perform better than those that use the original features (in terms of AUC and performance variance). (2) Neural network-based feature reduction techniques generate features that have a small variance across both supervised and unsupervised defect prediction models. Hence, we recommend that practitioners who do not wish to choose a best-performing defect prediction model for their data use a neural network-based feature reduction technique.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1925–1963},
numpages = {39},
keywords = {Restricted Boltzmann machine, Neural network, Feature selection, Feature reduction, Defect prediction}
}

@inproceedings{10.1145/3449365.3449384,
author = {Malhotra, Ruchika and Budhiraja, Anmol and Kumar Singh, Abhinav and Ghoshal, Ishani},
title = {A Novel Feature Selection Approach based on Binary Particle Swarm Optimization and Ensemble Learning for Heterogeneous Defect Prediction},
year = {2021},
isbn = {9781450388108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449365.3449384},
doi = {10.1145/3449365.3449384},
abstract = {Software defect prediction is an integral part of the software development process. Defect prediction helps focus on the grey areas beforehand, thus saving the considerable amount of money that is otherwise wasted in finding and fixing the faults once the software is already in production. One of the popular areas of defect prediction in recent years is Heterogeneous Defect Prediction, which predicts defects in a target project using a source project with different metrics. Through our paper, we provide a novel feature selection based approach, En-BPSO, based on binary particle swarm optimization, coupled with majority voting ensemble classifier based fitness function for heterogeneous defect prediction. The datasets we are using are MORPH and SOFTLAB. The results show that the En-BPSO method provides the highest Friedman mean rank amongst all the feature selection methods used for comparison. En-BPSO technique also helps us dynamically determine the optimal number of features to build an accurate heterogeneous defect prediction model.},
booktitle = {Proceedings of the 2021 3rd Asia Pacific Information Technology Conference},
pages = {115–121},
numpages = {7},
keywords = {Heterogeneous Metrics, Feature Selection, Ensemble Learning, Defect Prediction, Binary Particle Swarm Optimization},
location = {Bangkok, Thailand},
series = {APIT '21}
}

@inproceedings{10.1109/MSR.2017.46,
author = {Madeyski, Lech and Kawalerowicz, Marcin},
title = {Continuous defect prediction: the idea and a related dataset},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.46},
doi = {10.1109/MSR.2017.46},
abstract = {We would like to present the idea of our Continuous Defect Prediction (CDP) research and a related dataset that we created and share. Our dataset is currently a set of more than 11 million data rows, representing files involved in Continuous Integration (CI) builds, that synthesize the results of CI builds with data we mine from software repositories. Our dataset embraces 1265 software projects, 30,022 distinct commit authors and several software process metrics that in earlier research appeared to be useful in software defect prediction. In this particular dataset we use TravisTorrent as the source of CI data. TravisTorrent synthesizes commit level information from the Travis CI server and GitHub open-source projects repositories. We extend this data to a file change level and calculate the software process metrics that may be used, for example, as features to predict risky software changes that could break the build if committed to a repository with CI enabled.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {515–518},
numpages = {4},
keywords = {software repository, open science, mining software repositories, defect prediction, continuous defect prediction},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1007/978-3-030-31726-3_42,
author = {Li, Juanjuan and Jing, Xiao-Yuan and Wu, Fei and Sun, Ying and Yang, Yongguang},
title = {A Cost-Sensitive Shared Hidden Layer Autoencoder for Cross-Project Defect Prediction},
year = {2019},
isbn = {978-3-030-31725-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-31726-3_42},
doi = {10.1007/978-3-030-31726-3_42},
abstract = {Cross-project defect prediction means training a classifier model using the historical data of the other source project, and then testing whether the target project instance is defective or not. Since source and target projects have different data distributions, and data distribution difference will degrade the performance of classifier. Furthermore, the class imbalance of datasets increases the difficulty of classification. Therefore, a cost-sensitive shared hidden layer autoencoder (CSSHLA) method is proposed. CSSHLA learns a common feature representation between source and target projects by shared hidden layer autoencoder, and makes the different data distributions more similar. To solve the class imbalance problem, CSSHLA introduces a cost-sensitive factor to assign different importance weights to different instances. Experiments on 10 projects of PROMISE dataset show that CSSHLA improves the performance of cross-project defect prediction compared with baselines.},
booktitle = {Pattern Recognition and Computer Vision: Second Chinese Conference, PRCV 2019, Xi’an, China, November 8–11, 2019, Proceedings, Part III},
pages = {491–502},
numpages = {12},
keywords = {Cross-project software defect prediction, Cost-sensitive learning, Shared hidden layer autoencoder},
location = {Xi'an, China}
}

@inproceedings{10.1109/ICMLA.2012.145,
author = {Gao, Kehan and Khoshgoftaar, Taghi M. and Napolitano, Amri},
title = {A Hybrid Approach to Coping with High Dimensionality and Class Imbalance for Software Defect Prediction},
year = {2012},
isbn = {9780769549132},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2012.145},
doi = {10.1109/ICMLA.2012.145},
abstract = {High dimensionality and class imbalance are the two main problems affecting many software defect prediction. In this paper, we propose a new technique, named SelectRUSBoost, which is a form of ensemble learning that in-corporates data sampling to alleviate class imbalance and feature selection to resolve high dimensionality. To evaluate the effectiveness of the new technique, we apply it to a group of datasets in the context of software defect prediction. We employ two classification learners and six feature selection techniques. We compare the technique to the approach where feature selection and data sampling are used together, as well as the case where feature selection is used alone (no sampling used at all). The experimental results demonstrate that the SelectRUSBoost technique is more effective in improving classification performance compared to the other approaches.},
booktitle = {Proceedings of the 2012 11th International Conference on Machine Learning and Applications - Volume 02},
pages = {281–288},
numpages = {8},
keywords = {software defect prediction, high dimensionality, class imbalance},
series = {ICMLA '12}
}

@inproceedings{10.1145/3416508.3417118,
author = {Amasaki, Sousuke and Aman, Hirohisa and Yokogawa, Tomoyuki},
title = {An exploratory study on applicability of cross project defect prediction approaches to cross-company effort estimation},
year = {2020},
isbn = {9781450381277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416508.3417118},
doi = {10.1145/3416508.3417118},
abstract = {BACKGROUND: Research on software effort estimation has been active for decades, especially in developing effort estimation models. Effort estimation models need a dataset collected from completed projects similar to a project to be estimated. The similarity suffers from dataset shift, and cross-company software effort estimation (CCSEE) gets an attractive research topic. A recent study on the dataset shift problem examined the applicability and the effectiveness of cross-project defect prediction (CPDP) approaches. It was insufficient to bring a conclusion due to a limited number of examined approaches. AIMS: To investigate the characteristics of CPDP approaches that are applicable and effective for dataset shift problem in effort estimation. METHOD: We first reviewed the characteristics of 24 CPDP approaches to find applicable approaches. Next, we investigated their effectiveness in effort estimation performance with ten dataset configurations. RESULTS: 16 out of 24 CPDP approaches implemented in CrossPare framework were found to be applicable to CCSEE. However, only one approach could improve the effort estimation performance. Most of the others degraded it and were harmful. CONCLUSIONS: Most of the CPDP approaches we examined were helpless for CCSEE.},
booktitle = {Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {71–80},
numpages = {10},
keywords = {cross-company effort estimation, cross-project defect prediction, empirical evaluation},
location = {Virtual, USA},
series = {PROMISE 2020}
}

@inproceedings{10.1145/2786805.2786814,
author = {Nam, Jaechang and Kim, Sunghun},
title = {Heterogeneous defect prediction},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786814},
doi = {10.1145/2786805.2786814},
abstract = {Software defect prediction is one of the most active research areas in software engineering. We can build a prediction model with defect data collected from a software project and predict defects in the same project, i.e. within-project defect prediction (WPDP). Researchers also proposed cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. In recent studies, CPDP is proved to be feasible. However, CPDP requires projects that have the same metric set, meaning the metric sets should be identical between projects. As a result, current techniques for CPDP are difficult to apply across projects with heterogeneous metric sets. To address the limitation, we propose heterogeneous defect prediction (HDP) to predict defects across projects with heterogeneous metric sets. Our HDP approach conducts metric selection and metric matching to build a prediction model between projects with heterogeneous metric sets. Our empirical study on 28 subjects shows that about 68% of predictions using our approach outperform or are comparable to WPDP with statistical significance.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {508–519},
numpages = {12},
keywords = {quality assurance, heterogeneous metrics, Defect prediction},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1007/s11219-019-09468-z,
author = {Eni\c{s}er, Hasan Ferit and Sen, Alper},
title = {Virtualization of stateful services via machine learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09468-z},
doi = {10.1007/s11219-019-09468-z},
abstract = {Today’s enterprise software systems are much more complicated than the past. Increasing numbers of dependent applications, heterogeneous technologies, and wide usage of Service-Oriented Architectures (SOA), where numerous services communicate with each other, makes testing of such systems challenging. For testing these software systems, the concept of service virtualization is gaining popularity. Service virtualization is an automated technique to mimic the behavior of a given real service. Services can be classified as stateless or stateful services. Many services are stateful in nature, yet virtualization of stateful services is harder than virtualization of stateless services. In this work, we introduce two novel stateful service virtualization approaches. We employ classification-based and sequence-to-sequence-based machine learning algorithms in developing our solutions. Classification is a supervised learning method where the task is assigning given inputs to corresponding classes. A sequence-to-sequence model is a deep neural network architecture where the input and the output are sequences. We demonstrate the validity of our approaches on three datasets. Our evaluation shows that we obtain 75 % to 81 % accuracy on subject datasets with classification based method. Our deep neural network-based solution achieves even better accuracy results ranging from 89 to 97 % on subject datasets. Our evaluation on training times of the mentioned techniques show that classification based technique significantly outperforms other methods.},
journal = {Software Quality Journal},
month = mar,
pages = {283–306},
numpages = {24},
keywords = {Machine learning, Service virtualization, Software testing}
}

@inproceedings{10.1007/978-3-030-85607-6_20,
author = {Ardito, Carmelo and Deldjoo, Yashar and Di Sciascio, Eugenio and Nazary, Fatemeh and Sapienza, Gianluca},
title = {ISCADA: Towards a Framework for&nbsp;Interpretable Fault Prediction in&nbsp;Smart Electrical Grids},
year = {2021},
isbn = {978-3-030-85606-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-85607-6_20},
doi = {10.1007/978-3-030-85607-6_20},
abstract = {This paper reports ongoing research for the definition of a data-driven self-healing system using machine learning (ML) techniques that can perform automatic and timely detection of fault types and locations. Specifically, the proposed method makes use of spectrogram-based CNN modeling of the 3-phase voltage signals. Furthermore, to keep human operators informed about why certain decisions were made, i.e., to facilitate the interpretability of the black-box ML model, we propose a novel explanation approach that highlight regions in the input spectrogram that contributed the most for the prediction task at hand (e.g., fault type or location) - or visual explanation.},
booktitle = {Human-Computer Interaction – INTERACT 2021: 18th IFIP TC 13 International Conference, Bari, Italy, August 30 – September 3, 2021, Proceedings, Part V},
pages = {270–274},
numpages = {5},
keywords = {Fault prediction, Interpretability, Self-healing system},
location = {Bari, Italy}
}

@inproceedings{10.1007/978-3-030-16142-2_17,
author = {Li, Heng-Yi and Li, Ming and Zhou, Zhi-Hua},
title = {Towards One Reusable Model for Various Software Defect Mining Tasks},
year = {2019},
isbn = {978-3-030-16141-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-16142-2_17},
doi = {10.1007/978-3-030-16142-2_17},
abstract = {Software defect mining is playing an important role in software quality assurance. Many deep neural network based models have been proposed for software defect mining tasks, and have pushed forward the state-of-the-art mining performance. These deep models usually require a huge amount of task-specific source code for training to capture the code functionality to mine the defects. But such requirement is often hard to be satisfied in practice. On the other hand, lots of free source code and corresponding textual explanations are publicly available in the open source software repositories, which is potentially useful in modeling code functionality. However, no previous studies ever leverage these resources to help defect mining tasks. In this paper, we propose a novel framework to learn one reusable deep model for code functional representation using the huge amount of publicly available task-free source code as well as their textual explanations. And then reuse it for various software defect mining tasks. Experimental results on three major defect mining tasks with real world datasets indicate that by reusing this model in specific tasks, the mining performance outperforms its counterpart that learns deep models from scratch, especially when the training data is insufficient.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 23rd Pacific-Asia Conference, PAKDD 2019, Macau, China, April 14-17, 2019, Proceedings, Part III},
pages = {212–224},
numpages = {13},
keywords = {Software defect mining, Machine learning, Model reuse},
location = {Macau, China}
}

@inproceedings{10.1145/2931037.2931039,
author = {Bowes, David and Hall, Tracy and Harman, Mark and Jia, Yue and Sarro, Federica and Wu, Fan},
title = {Mutation-aware fault prediction},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931039},
doi = {10.1145/2931037.2931039},
abstract = {We introduce mutation-aware fault prediction, which leverages additional guidance from metrics constructed in terms of mutants and the test cases that cover and detect them. We report the results of 12 sets of experiments, applying 4 different predictive modelling techniques to 3 large real-world systems (both open and closed source). The results show that our proposal can significantly (p ≤ 0.05) improve fault prediction performance. Moreover, mutation-based metrics lie in the top 5% most frequently relied upon fault predictors in 10 of the 12 sets of experiments, and provide the majority of the top ten fault predictors in 9 of the 12 sets of experiments.},
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {330–341},
numpages = {12},
keywords = {Software Metrics, Software Fault Prediction, Software Defect Prediction, Mutation Testing, Empirical Study},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@article{10.1007/s10664-016-9468-y,
author = {Herbold, Steffen and Trautsch, Alexander and Grabowski, Jens},
title = {Global vs. local models for cross-project defect prediction},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9468-y},
doi = {10.1007/s10664-016-9468-y},
abstract = {Although researchers invested significant effort, the performance of defect prediction in a cross-project setting, i.e., with data that does not come from the same project, is still unsatisfactory. A recent proposal for the improvement of defect prediction is using local models. With local models, the available data is first clustered into homogeneous regions and afterwards separate classifiers are trained for each homogeneous region. Since the main problem of cross-project defect prediction is data heterogeneity, the idea of local models is promising. Therefore, we perform a conceptual replication of the previous studies on local models with a focus on cross-project defect prediction. In a large case study, we evaluate the performance of local models and investigate their advantages and drawbacks for cross-project predictions. To this aim, we also compare the performance with a global model and a transfer learning technique designed for cross-project defect predictions. Our findings show that local models make only a minor difference in comparison to global models and transfer learning for cross-project defect prediction. While these results are negative, they provide valuable knowledge about the limitations of local models and increase the validity of previously gained research results.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1866–1902},
numpages = {37},
keywords = {Local models, Defect prediction, Cross-project}
}

@inproceedings{10.1145/3377811.3380360,
author = {Li, Ke and Xiang, Zilin and Chen, Tao and Wang, Shuo and Tan, Kay Chen},
title = {Understanding the automated parameter optimization on transfer learning for cross-project defect prediction: an empirical study},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380360},
doi = {10.1145/3377811.3380360},
abstract = {Data-driven defect prediction has become increasingly important in software engineering process. Since it is not uncommon that data from a software project is insufficient for training a reliable defect prediction model, transfer learning that borrows data/konwledge from other projects to facilitate the model building at the current project, namely cross-project defect prediction (CPDP), is naturally plausible. Most CPDP techniques involve two major steps, i.e., transfer learning and classification, each of which has at least one parameter to be tuned to achieve their optimal performance. This practice fits well with the purpose of automated parameter optimization. However, there is a lack of thorough understanding about what are the impacts of automated parameter optimization on various CPDP techniques. In this paper, we present the first empirical study that looks into such impacts on 62 CPDP techniques, 13 of which are chosen from the existing CPDP literature while the other 49 ones have not been explored before. We build defect prediction models over 20 real-world software projects that are of different scales and characteristics. Our findings demonstrate that: (1) Automated parameter optimization substantially improves the defect prediction performance of 77% CPDP techniques with a manageable computational cost. Thus more efforts on this aspect are required in future CPDP studies. (2) Transfer learning is of ultimate importance in CPDP. Given a tight computational budget, it is more cost-effective to focus on optimizing the parameter configuration of transfer learning algorithms (3) The research on CPDP is far from mature where it is 'not difficult' to find a better alternative by making a combination of existing transfer learning and classification techniques. This finding provides important insights about the future design of CPDP techniques.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {566–577},
numpages = {12},
keywords = {automated parameter optimization, classification techniques, cross-project defect prediction, transfer learning},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Challenges, Multiple-case study, Software product lines, Variability management}
}

@article{10.1007/s10489-020-01935-6,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {An empirical study of ensemble techniques for software fault prediction},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {6},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01935-6},
doi = {10.1007/s10489-020-01935-6},
abstract = {Previously, many researchers have performed analysis of various techniques for the software fault prediction (SFP). Oddly, the majority of such studies have shown the limited prediction capability and their performance for given software fault datasets was not persistent. In contrast to this, recently, ensemble techniques based SFP models have shown promising and improved results across different software fault datasets. However, many new as well as improved ensemble techniques have been introduced, which are not explored for SFP. Motivated by this, the paper performs an investigation on ensemble techniques for SFP. We empirically assess the performance of seven ensemble techniques namely, Dagging, Decorate, Grading, MultiBoostAB, RealAdaBoost, Rotation Forest, and Ensemble Selection. We believe that most of these ensemble techniques are not used before for SFP. We conduct a series of experiments on the benchmark fault datasets and use three distinct classification algorithms, namely, naive Bayes, logistic regression, and J48 (decision tree) as base learners to the ensemble techniques. Experimental analysis revealed that rotation forest with J48 as the base learner achieved the highest precision, recall, and G-mean 1 values of 0.995, 0.994, and 0.994, respectively and Decorate achieved the highest AUC value of 0.986. Further, results of statistical tests showed used ensemble techniques demonstrated a statistically significant difference in their performance among the used ones for SFP. Additionally, the cost-benefit analysis showed that SFP models based on used ensemble techniques might be helpful in saving software testing cost and effort for twenty out of twenty-eight used fault datasets.},
journal = {Applied Intelligence},
month = jun,
pages = {3615–3644},
numpages = {30},
keywords = {Empirical analysis, PROMISE data repository, Ensemble techniques, Software fault prediction}
}

@inproceedings{10.1007/978-3-030-62822-2_12,
author = {Zhou, Yangxi and Zhu, Yan and Chen, Liangyu},
title = {Software Defect-Proneness Prediction with Package Cohesion and Coupling Metrics Based on Complex Network Theory},
year = {2020},
isbn = {978-3-030-62821-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-62822-2_12},
doi = {10.1007/978-3-030-62822-2_12},
abstract = {Driven by functionality requirements, software codes are increasingly inflated, and invocations between codes are frequent and random. This makes it difficult for programmers to be thoughtful when modifying code, increasing the risk of defects. In an object-oriented software system, packages take the role of a middle tier that aggregates classes and limits class access. However, as the software system evolves, the logic and correctness of packages are weakened. In this paper, we explore the relation between package metrics and object-oriented software defect-proneness. We use two metrics of package cohesion and coupling based on complex network theory to verify the impact of code structure on software quality. On six Java software systems, the experimental result shows that the cohesion and coupling metrics play a positive role in software defect prediction, and they can correctly and effectively evaluate package organization structure. Meanwhile, our study confirms that compliance with the design principle of high cohesion and low coupling can reduce the risk of software defect-proneness and improve software quality.},
booktitle = {Dependable Software Engineering. Theories, Tools, and Applications: 6th International Symposium, SETTA 2020, Guangzhou, China, November 24–27, 2020, Proceedings},
pages = {186–201},
numpages = {16},
keywords = {Cohesion and coupling metrics, Defect proneness, Software package},
location = {Guangzhou, China}
}

@article{10.1016/j.jss.2016.02.015,
author = {Rana, Rakesh and Staron, Miroslaw and Berger, Christian and Hansson, J\"{o}rgen and Nilsson, Martin and Meding, Wilhelm},
title = {Analyzing defect inflow distribution and applying Bayesian inference method for software defect prediction in large software projects},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.02.015},
doi = {10.1016/j.jss.2016.02.015},
abstract = {Defect inflow distribution of 14 large projects from industry &amp; OSS is analyzed.6 standard distributions are evaluated for their ability to fit the defect inflow.12 out of 14 projects defect inflow data was described best by beta distribution.Historical projects information is useful for early defect prediction using Bayesian inference method. Tracking and predicting quality and reliability is a major challenge in large and distributed software development projects. A number of standard distributions have been successfully used in reliability engineering theory and practice, common among these for modeling software defect inflow being exponential, Weibull, beta and Non-Homogeneous Poisson Process (NHPP). Although standard distribution models have been recognized in reliability engineering practice, their ability to fit defect data from proprietary and OSS software projects is not well understood. Lack of knowledge about underlying defect inflow distribution also leads to difficulty in applying Bayesian based inference methods for software defect prediction. In this paper we explore the defect inflow distribution of total of fourteen large software projects/release from two industrial domain and open source community. We evaluate six standard distributions for their ability to fit the defect inflow data and also assess which information criterion is practical for selecting the distribution with best fit. Our results show that beta distribution provides the best fit to the defect inflow data for all industrial projects as well as majority of OSS projects studied. In the paper we also evaluate how information about defect inflow distribution from historical projects is applied for modeling the prior beliefs/experience in Bayesian analysis which is useful for making software defect predictions early during the software project lifecycle.},
journal = {J. Syst. Softw.},
month = jul,
pages = {229–244},
numpages = {16},
keywords = {Software, SRGM, Defect Inflow}
}

@article{10.1007/s10664-019-09736-3,
author = {Kondo, Masanari and German, Daniel M. and Mizuno, Osamu and Choi, Eun-Hye},
title = {The impact of context metrics on just-in-time defect prediction},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09736-3},
doi = {10.1007/s10664-019-09736-3},
abstract = {Traditional just-in-time defect prediction approaches have been using changed lines of software to predict defective-changes in software development. However, they disregard information around the changed lines. Our main hypothesis is that such information has an impact on the likelihood that the change is defective. To take advantage of this information in defect prediction, we consider n-lines (n = 1,2,…) that precede and follow the changed lines (which we call context lines), and propose metrics that measure them, which we call “Context Metrics.” Specifically, these context metrics are defined as the number of words/keywords in the context lines. In a large-scale empirical study using six open source software projects, we compare the performance of using our context metrics, traditional code churn metrics (e.g., the number of modified subsystems), our extended context metrics which measure not only context lines but also changed lines, and combination metrics that use two extended context metrics at a prediction model for defect prediction. The results show that context metrics that consider the context lines of added-lines achieve the best median value in all cases in terms of a statistical test. Moreover, using few number of context lines is suitable for context metric that considers words, and using more number of context lines is suitable for context metric that considers keywords. Finally, the combination metrics of two extended context metrics significantly outperform all studied metrics in all studied projects w. r. t. the area under the receiver operation characteristic curve (AUC) and Matthews correlation coefficient (MCC).},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {890–939},
numpages = {50},
keywords = {Code churn metrics, Indentation metrics, Changed lines, Context lines, Source code changes, Defect prediction, Just-in-time defect prediction}
}

@inproceedings{10.1145/3412841.3442020,
author = {Hosseini, Seyedrebvar and Turhan, Burak},
title = {A comparison of similarity based instance selection methods for cross project defect prediction},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442020},
doi = {10.1145/3412841.3442020},
abstract = {Context: Previous studies have shown that training data instance selection based on nearest neighborhood (NN) information can lead to better performance in cross project defect prediction (CPDP) by reducing heterogeneity in training datasets. However, neighborhood calculation is computationally expensive and approximate methods such as Locality Sensitive Hashing (LSH) can be as effective as exact methods. Aim: We aim at comparing instance selection methods for CPDP, namely LSH, NN-filter, and Genetic Instance Selection (GIS). Method: We conduct experiments with five base learners, optimizing their hyper parameters, on 13 datasets from PROMISE repository in order to compare the performance of LSH with benchmark instance selection methods NN-Filter and GIS. Results: The statistical tests show six distinct groups for F-measure performance. The top two group contains only LSH and GIS benchmarks whereas the bottom two groups contain only NN-Filter variants. LSH and GIS favor recall more than precision. In fact, for precision performance only three significantly distinct groups are detected by the tests where the top group is comprised of NN-Filter variants only. Recall wise, 16 different groups are identified where the top three groups contain only LSH methods, four of the next six are GIS only and the bottom five contain only NN-Filter. Finally, NN-Filter benchmarks never outperform the LSH counterparts with the same base learner, tuned or non-tuned. Further, they never even belong to the same rank group, meaning that LSH is always significantly better than NN-Filter with the same learner and settings. Conclusions: The increase in performance and the decrease in computational overhead and runtime make LSH a promising approach. However, the performance of LSH is based on high recall and in environments where precision is considered more important NN-Filter should be considered.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1455–1464},
numpages = {10},
keywords = {approximate near neighbour, cross project defect prediction, instance selection, locality sensitive hashing, search based optimisation},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.5555/1332044.1332090,
author = {Catal, Cagatay and Diri, Banu},
title = {Software defect prediction using artificial immune recognition system},
year = {2007},
publisher = {ACTA Press},
address = {USA},
abstract = {Predicting fault-prone modules for software development projects enables companies to reach high reliable systems and minimizes necessary budget, personnel and resource to be allocated to achieve this goal. Researchers have investigated various statistical techniques and machine learning algorithms until now but most of them applied their models to the different datasets which are not public or used different criteria to decide the best predictor model. Artificial Immune Recognition System is a supervised learning algorithm which has been proposed in 2001 for the classification problems and its performance for UCI datasets (University of California machine learning repository) is remarkable.In this paper, we propose a novel software defect prediction model by applying Artificial Immune Recognition System (AIRS) along with the Correlation-Based Feature Selection (CFS) technique. In order to evaluate the performance of the proposed model, we apply it to the five NASA public defect datasets and compute G-mean 1, G-mean 2 and F-measure values to discuss the effectiveness of the model. Experimental results show that AIRS has a great potential for software defect prediction and AIRS along with CFS technique provides relatively better prediction for large scale projects which consist of many modules.},
booktitle = {Proceedings of the 25th Conference on IASTED International Multi-Conference: Software Engineering},
pages = {285–290},
numpages = {6},
keywords = {artificial immune recognition system (AIRS) and correlation-based feature selection, immune systems, quality prediction, software defect prediction},
location = {Innsbruck, Austria},
series = {SE'07}
}

@inproceedings{10.1007/978-3-030-38961-1_8,
author = {Sun, Yuanyuan and Xu, Lele and Guo, Lili and Li, Ye and Wang, Yongming},
title = {A Comparison Study of VAE and GAN for Software Fault Prediction},
year = {2019},
isbn = {978-3-030-38960-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38961-1_8},
doi = {10.1007/978-3-030-38961-1_8},
abstract = {Software fault is an unavoidable problem in software project. How to predict software fault to enhance safety and reliability of system is worth studying. In recent years, deep learning has been widely used in the fields of image, text and voice. However it is seldom applied in the field of software fault prediction. Considering the ability of deep learning, we select the deep learning techniques of VAE and GAN for software fault prediction and compare the performance of them. There is one salient feature of software fault data. The proportion of non-fault data is well above the proportion of fault data. Because of the imbalanced data, it is difficult to get high accuracy to predict software fault. As we known, VAE and GAN are able to generate synthetic samples that obey the distribution of real data. We try to take advantage of their power to generate new fault samples in order to improve the accuracy of software fault prediction. The architectures of VAE and GAN are designed to fit for the high dimensional software fault data. New software fault samples are generated to balance the software fault datasets in order to get better performance for software fault prediction. The models of VAE and GAN are trained on GPU TITAN X. SMOTE is also adopted in order to compare the performance with VAE and GAN. The results in the experiment show that VAE and GAN are useful techniques for software fault prediction and VAE has better performance than GAN on this issue.},
booktitle = {Algorithms and Architectures for Parallel Processing: 19th International Conference, ICA3PP 2019, Melbourne, VIC, Australia, December 9–11, 2019, Proceedings, Part II},
pages = {82–96},
numpages = {15},
keywords = {Software fault prediction, GAN, VAE, Deep learning},
location = {Melbourne, VIC, Australia}
}

@article{10.1007/s10489-021-02346-x,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {Software fault prediction based on the dynamic selection of learning technique: findings from the eclipse project study},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {12},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02346-x},
doi = {10.1007/s10489-021-02346-x},
abstract = {An effective software fault prediction (SFP) model could help developers in the quick and prompt detection of faults and thus help enhance the overall reliability and quality of the software project. Variations in the prediction performance of learning techniques for different software systems make it difficult to select a suitable learning technique for fault prediction modeling. The evaluation of previously presented SFP approaches has shown that single machine learning-based models failed to provide the best accuracy in any context, highlighting the need to use multiple techniques to build the SFP model. To solve this problem, we present and discuss a software fault prediction approach based on selecting the most appropriate learning techniques from a set of competitive and accurate learning techniques for building a fault prediction model. In work, we apply the discussed SFP approach for the five Eclipse project datasets and nine Object-oriented (OO) project datasets and report the findings of the experimental study. We have used different performance measures, i.e., AUC, accuracy, sensitivity, and specificity, to assess the discussed approach’s performance. Further, we have performed a cost-benefit analysis to evaluate the economic viability of the approach. Results showed that the presented approach predicted the software’s faults effectively for the used accuracy, AUC, sensitivity, and specificity measures with the highest achieved values of 0.816, 0.835, 0.98, and 0.903 for AUC, accuracy, sensitivity, and specificity, respectively. The cost-benefit analysis of the approach showed that it could help reduce the overall software testing cost.},
journal = {Applied Intelligence},
month = dec,
pages = {8945–8960},
numpages = {16},
keywords = {Machine learning techniques, Cost-benefit analysis, Dynamic selection, Eclipse project, Software fault prediction}
}

@article{10.3233/JIFS-18473,
author = {Malhotra, Ruchika and Sharma, Anjali},
title = {Empirical assessment of feature selection techniques in defect prediction models using web applications},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {36},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-18473},
doi = {10.3233/JIFS-18473},
abstract = {&nbsp;In order to minimize the over-fitting and related factors that are caused by the high dimensionality of the input data in software defect prediction, the attributes are often optimized using various feature selection techniques. However, the comparative performance of these selection techniques in combination with machine learning algorithms remains largely unexplored using web applications. In this work, we investigate the best possible combination of feature selection technique with machine learning algorithms, with the sample space chosen from open source Apache Click and Rave data sets. Our results are based on 945 defect prediction models derived from parametric, non-parametric and ensemble-based machine learning algorithms, for which the metrics are derived from the various filter and threshold-based ranking techniques. Friedman and Nemenyi post-hoc statistical tests are adopted to identify the performance difference of these models. We find that filter-based feature selection in combination with ensemble-based machine learning algorithms not only poise as the best strategy but also yields a maximum feature set redundancy by 94%, with little or no comprise on the performance index.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6567–6578},
numpages = {12},
keywords = {web application quality, machine learning, feature ranking, Feature selection}
}

@article{10.5555/3197793.3197817,
author = {Shukla, Swapnil and Radhakrishnan, T. and Muthukumaran, K. and Neti, Lalita Bhanu},
title = {Multi-objective cross-version defect prediction},
year = {2018},
issue_date = {March     2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {6},
issn = {1432-7643},
abstract = {Defect prediction models help software project teams to spot defect-prone source files of software systems. Software project teams can prioritize and put up rigorous quality assurance (QA) activities on these predicted defect-prone files to minimize post-release defects so that quality software can be delivered. Cross-version defect prediction is building a prediction model from the previous version of a software project to predict defects in the current version. This is more practical than the other two ways of building models, i.e., cross-project prediction model and cross- validation prediction models, as previous version of same software project will have similar parameter distribution among files. In this paper, we formulate cross-version defect prediction problem as a multi-objective optimization problem with two objective functions: (a) maximizing recall by minimizing misclassification cost and (b) maximizing recall by minimizing cost of QA activities on defect prone files. The two multi-objective defect prediction models are compared with four traditional machine learning algorithms, namely logistic regression, na\"{\i}ve Bayes, decision tree and random forest. We have used 11 projects from the PROMISE repository consisting of a total of 41 different versions of these projects. Our findings show that multi-objective logistic regression is more cost-effective than single-objective algorithms.},
journal = {Soft Comput.},
month = mar,
pages = {1959–1980},
numpages = {22},
keywords = {Search-based software engineering, Multi-objective optimization, Misclassification cost, Cross-version defect prediction, Cost-effectiveness}
}

@article{10.1007/s11390-019-1959-z,
author = {Xu, Zhou and Pang, Shuai and Zhang, Tao and Luo, Xia-Pu and Liu, Jin and Tang, Yu-Tian and Yu, Xiao and Xue, Lei},
title = {Cross Project Defect Prediction via Balanced Distribution Adaptation Based Transfer Learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1959-z},
doi = {10.1007/s11390-019-1959-z},
abstract = {Defect prediction assists the rational allocation of testing resources by detecting the potentially defective software modules before releasing products. When a project has no historical labeled defect data, cross project defect prediction (CPDP) is an alternative technique for this scenario. CPDP utilizes labeled defect data of an external project to construct a classification model to predict the module labels of the current project. Transfer learning based CPDP methods are the current mainstream. In general, such methods aim to minimize the distribution differences between the data of the two projects. However, previous methods mainly focus on the marginal distribution difference but ignore the conditional distribution difference, which will lead to unsatisfactory performance. In this work, we use a novel balanced distribution adaptation (BDA) based transfer learning method to narrow this gap. BDA simultaneously considers the two kinds of distribution differences and adaptively assigns different weights to them. To evaluate the effectiveness of BDA for CPDP performance, we conduct experiments on 18 projects from four datasets using six indicators (i.e., F-measure, g-means, Balance, AUC, EARecall, and EAF-measure). Compared with 12 baseline methods, BDA achieves average improvements of 23.8%, 12.5%, 11.5%, 4.7%, 34.2%, and 33.7% in terms of the six indicators respectively over four datasets.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1039–1062},
numpages = {24},
keywords = {effort-aware indicator, balancing distribution, transfer learning, cross-project defect prediction}
}

@inproceedings{10.1145/3183440.3194992,
author = {Guo, Yuchen and Shepperd, Martin and Li, Ning},
title = {Bridging effort-aware prediction and strong classification: a just-in-time software defect prediction study},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3194992},
doi = {10.1145/3183440.3194992},
abstract = {Context: Most research into software defect prediction ignores the differing amount of effort entailed in searching for defects between software components. The result is sub-optimal solutions in terms of allocating testing resources. Recently effort-aware (EA) defect prediction has sought to redress this deficiency. However, there is a gap between previous classification research and EA prediction.Objective: We seek to transfer strong defect classification capability to efficient effort-aware software defect prediction.Method: We study the relationship between classification performance and the cost-effectiveness curve experimentally (using six open-source software data sets).Results: We observe extremely skewed distributions of change size which contributes to the lack of relationship between classification performance and the ability to find efficient test orderings for defect detection. Trimming allows all effort-aware approaches bridging high classification capability to efficient effort-aware performance.Conclusion: Effort distributions dominate effort-aware models. Trimming is a practical method to handle this problem.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {325–326},
numpages = {2},
keywords = {defect prediction, effort-aware, just-in-time, software},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3380446.3430687,
author = {Fallon, Elias},
title = {Machine Learning in EDA: Opportunities and Challenges},
year = {2020},
isbn = {9781450375191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380446.3430687},
doi = {10.1145/3380446.3430687},
abstract = {Electronic Design Automation software has delivered semiconductor design productivity improvements for decades. The next leap in productivity will come from the addition of machine learning techniques to the toolbox of computational software capabilities employed by EDA developers. Recent research and development into machine learning for EDA point to clear patterns for how it impacts EDA tools, flows, and design challenges. This research has also illustrated some of the challenges that will come with production deployment of machine learning techniques into EDA tools and flows. This talk will detail patterns observed in ML for EDA development, as well as discussing challenges with productization of ML for EDA developments and the opportunities that it presents for researchers.Biography: Elias Fallon is currently Engineering Group Director at Cadence Design Systems, a leading Electronic Design Automation company. He has been involved in EDA for more than 20 years from the founding of Neolinear, Inc, which was acquired by Cadence in 2004. Elias was co-Primary Investigator on the MAGESTIC project, funded by DARPA to investigate the application of Machine Learning to EDA for Package/PCB and Analog IC. Elias also leads an innovation incubation team within the Custom IC R&amp;D group as well as other traditional EDA product teams. Beyond his work developing electronic design automation tools, he has led software quality improvement initiatives within Cadence, partnering with the Carnegie Mellon Software Engineering Institute. Elias graduated from Carnegie Mellon University with an M.S. and B.S. in Electrical and Computer Engineering. Elias, his wife and two children live north of Pittsburgh, PA. https://www.linkedin.com/in/elias-fallon/},
booktitle = {Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {103},
numpages = {1},
keywords = {machine learning, electronic design automation},
location = {Virtual Event, Iceland},
series = {MLCAD '20}
}

@article{10.1007/s10515-019-00259-1,
author = {Li, Zhiqiang and Jing, Xiao-Yuan and Zhu, Xiaoke and Zhang, Hongyu and Xu, Baowen and Ying, Shi},
title = {Heterogeneous defect prediction with two-stage ensemble learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00259-1},
doi = {10.1007/s10515-019-00259-1},
abstract = {Heterogeneous defect prediction (HDP) refers to predicting defect-prone software modules in one project (target) using heterogeneous data collected from other projects (source). Recently, several HDP methods have been proposed. However, these methods do not sufficiently incorporate the two characteristics of the defect data: (1) data could be linear inseparable, and (2) data could be highly imbalanced. These two data characteristics make it challenging to build an effective HDP model. In this paper, we propose a novel Two-Stage Ensemble Learning (TSEL) approach to HDP, which contains two stages: ensemble multi-kernel domain adaptation (EMDA) stage and ensemble data sampling (EDS) stage. In the EMDA stage, we develop an Ensemble Multiple Kernel Correlation Alignment (EMKCA) predictor, which combines the advantage of multiple kernel learning and domain adaptation techniques. In the EDS stage, we employ RESample with replacement (RES) technique to learn multiple different EMKCA predictors and use average ensemble to combine them together. These two stages create an ensemble of defect predictors. Extensive experiments on 30 public projects show that the proposed TSEL approach outperforms a range of competing methods. The improvement is 20.14–33.92% in AUC, 36.05–54.78% in f-measure, and 5.48–19.93% in balance, respectively.},
journal = {Automated Software Engg.},
month = sep,
pages = {599–651},
numpages = {53},
keywords = {Domain adaptation, Data sampling, Class imbalance, Multiple kernel learning, Linear inseparability, Two-stage ensemble learning, Heterogeneous defect prediction}
}

@inproceedings{10.1145/3377024.3380451,
author = {Bencomo, Nelly},
title = {Next steps in variability management due to autonomous behaviour and runtime learning},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3380451},
doi = {10.1145/3377024.3380451},
abstract = {One of the basic principles in product lines is to delay design decisions related to offered functionality and quality to later phases of the life cycle [25]. Instead of deciding on what system to develop in advance, a set of assets and a common reference architecture are specified and implemented during the Domain Engineering process. Later on, during Application Engineering, specific systems are developed to satisfy the requirements reusing the assets and architecture [16]. Traditionally, this is during the Application Engineering when delayed design decisions are solved. The realization of this delay relies heavily on the use of variability in the development of product lines and systems. However, as systems become more interconnected and diverse, software architects cannot easily foresee the software variants and the interconnections between components. Consequently, a generic a priori model is conceived to specify the system's dynamic behaviour and architecture. The corresponding design decisions are left to be solved at runtime [13].Surprisingly, few research initiatives have investigated variability models at runtime [9]. Further, they have been applied only at the level of goals and architecture, which contrasts to the needs claimed by the variability community, i.e., Software Product Lines (SPLC) and Dynamic Software Product Lines (DSPL) [2, 10, 14, 22]. Especially, the vision of DSPL with their ability to support runtime updates with virtually zero downtime for products of a software product line, denotes the obvious need of variability models being used at runtime to adapt the corresponding programs. A main challenge for dealing with runtime variability is that it should support a wide range of product customizations under various scenarios that might be unknown until the execution time, as new product variants can be identified only at runtime [10, 11]. Contemporary variability models face the challenge of representing runtime variability to therefore allow the modification of variation points during the system's execution, and underpin the automation of the system's reconfiguration [15]. The runtime representation of feature models (i.e. the runtime model of features) is required to automate the decision making [9].Software automation and adaptation techniques have traditionally required a priori models for the dynamic behaviour of systems [17]. With the uncertainty present in the scenarios involved, the a priori model is difficult to define [20, 23, 26]. Even if foreseen, its maintenance is labour-intensive and, due to architecture decay, it is also prone to get out-of-date. However, the use of models@runtime does not necessarily require defining the system's behaviour model beforehand. Instead, different techniques such as machine learning, or mining software component interactions from system execution traces can be used to build a model which is in turn used to analyze, plan, and execute adaptations [18], and synthesize emergent software on the fly [7].Another well-known problem posed by the uncertainty that characterize autonomous systems is that different stakeholders (e.g. end users, operators and even developers) may not understand them due to the emergent behaviour. In other words, the running system may surprise its customers and/or developers [4]. The lack of support for explanation in these cases may compromise the trust to stakeholders, who may eventually stop using a system [12, 24]. I speculate that variability models can offer great support for (i) explanation to understand the diversity of the causes and triggers of decisions during execution and their corresponding effects using traceability [5], and (ii) better understand the behaviour of the system and its environment.Further, an extension and potentially reframing of the techniques associated with variability management may be needed to help taming uncertainty and support explanation and understanding of the systems. The use of new techniques such as machine learning exacerbates the current situation. However, at the same time machine learning techniques can also help and be used, for example, to explore the variability space [1]. What can the community do to face the challenges associated?We need to meaningfully incorporate techniques from areas such as artificial intelligence, machine learning, optimization, planning, decision theory, and bio-inspired computing into our variability management techniques to provide explanation and management of the diversity of decisions, their causes and the effects associated. My own previous work has progressed [3, 5, 6, 8, 11, 12, 19, 21] to reflect what was discussed above.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {2},
numpages = {2},
keywords = {variability management, uncertainty, machine learning, dynamic variability, dynamic software product lines, autonomous systems},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@phdthesis{10.5555/AAI28498637,
author = {Khalil, Kasem Mohamed Ahmed and Ashok, Kumar, and Mohammad, Madani, and Michael, Totaro,},
advisor = {A, Bayoumi, Magdy},
title = {Fault Prediction and Self-Healing Paradigm for Intelligent Hardware Systems},
year = {2021},
isbn = {9798522907303},
publisher = {University of Louisiana at Lafayette},
abstract = {As the complexity of hardware systems grows, the failure rate, or the rate at which such systems produce faults, accelerates. Ideally, future hardware should heal faults before the faults can occur and impact a system adversely. Fault prediction is needed to identify a fault before it occurs, and this helps to heal the fault early to avoid losing data or missing some operation. Such systems, also referred to as intelligent hardware systems, are expected to revolutionize the way circuits and systems are designed, and it is the focus of this dissertation. An intelligent hardware system is expected to have mechanisms for self-healing and fault prediction. A novel mechanism for self-healing methods for Embryonic Hardware (EmHW), Network-on-Chip (NoC), and neural network is proposed. The proposed self-healing method is implemented in VHDL on Altera Arria 10 GX FPGA device. The area overhead of the proposed self-healing method for EmHW and NoC is 34% and 31%, respectively, with high reliability and the mean-time-to-failure that prove extended network age. The hardware fault prediction requires low-cost machine learning techniques. A hardware neural network optimization and reconfiguration is proposed for artificial neural networks, Long Short-Term Memory (LSTM), and Convolutional Neural Network (CNN). An Economic LSTM (ELSTM) is proposed, which saves 34% of the area and 35% of the power consumption compared to LSTM. Next, a novel Absolute Average Deviation (ADD) pooling method with very high accuracy for CNN is also. The AAD pooling achieves an accuracy of more than 98%. and has a modest 4%. It is synthesized using Synopsis in 45 nm technology and found to occupy an area of 244.466 nm2, and consume 0.31 mW of power. Two fault prediction methods are presented, and they are based on the proposed machine learning optimization methods. The proposed fault prediction methods are used for early transistor and architectural fault prediction for NoC and EmHW, using fast Fourier transform, Principal Component Analysis (PCA), Relative PCA (RPCA), ELSTM, and CNN. The proposed approaches are implemented using Tensorflow and FPGA device, and the result shows the proposed approach could predict a fault with the accuracy of more than 98%.},
note = {AAI28498637}
}

@inproceedings{10.1145/3338906.3341462,
author = {Caulo, Maria},
title = {A taxonomy of metrics for software fault prediction},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3341462},
doi = {10.1145/3338906.3341462},
abstract = {In the field of Software Fault Prediction (SFP), researchers exploit software metrics to build predictive models using machine learning and/or statistical techniques. SFP has existed for several decades and the number of metrics used has increased dramatically. Thus, the need for a taxonomy of metrics for SFP arises firstly to standardize the lexicon used in this field so that the communication among researchers is simplified and then to organize and systematically classify the used metrics. In this doctoral symposium paper, I present my ongoing work which aims not only to build such a taxonomy as comprehensive as possible, but also to provide a global understanding of the metrics for SFP in terms of detailed information: acronym(s), extended name, univocal description, granularity of the fault prediction (e.g., method and class), category, and research papers in which they were used.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1144–1147},
numpages = {4},
keywords = {taxonomy, software metrics, software fault prediction},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1007/978-3-030-16145-3_23,
author = {Zhang, Wenzhou and Li, Weiwei and Jia, Xiuyi},
title = {Effort-Aware Tri-Training for Semi-supervised Just-in-Time Defect Prediction},
year = {2019},
isbn = {978-3-030-16144-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-16145-3_23},
doi = {10.1007/978-3-030-16145-3_23},
abstract = {In recent years, just-in-time (JIT) defect prediction has gained considerable interest as it enables developers to identify risky changes at check-in time. Previous studies tried to conduct research from both supervised and unsupervised perspectives. Since the label of change is hard to acquire, it would be more desirable for applications if a prediction model doesn’t highly rely on the label information. However, the performance of the unsupervised models proposed by previous work isn’t good in terms of precision and F1 due to the lack of supervised information. To overcome this weakness, we try to study the JIT defect prediction from the semi-supervised perspective, which only requires a few labeled data for training. In this paper, we propose an Effort-Aware Tri-Training (EATT) semi-supervised model for JIT defect prediction based on sample selection. We compare EATT with the state-of-the-art supervised and unsupervised models with respect to different labeled rates. The experimental results on six open-source projects demonstrate that EATT performs better than existing supervised and unsupervised models for effort-aware JIT defect prediction.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 23rd Pacific-Asia Conference, PAKDD 2019, Macau, China, April 14-17, 2019, Proceedings, Part II},
pages = {293–304},
numpages = {12},
keywords = {Defect prediction, Just-in-time, Tri-training, Effort-aware},
location = {Macau, China}
}

@inproceedings{10.1145/3127005.3127015,
author = {Amasaki, Sousuke},
title = {On Applicability of Cross-project Defect Prediction Method for Multi-Versions Projects},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127015},
doi = {10.1145/3127005.3127015},
abstract = {Context: Cross-project defect prediction (CPDP) research has been popular, and many CPDP methods have been proposed so far. As the straightforward use of Cross-project (CP) data was useless, those methods filter, weigh, and adapt CP data for a target project data. This idea would also be useful for a project having past defect data. Objective: To evaluate the applicability of CPDP methods for multi-versions projects. The evaluation focused on the relationship between the performance change and the proximity of older release data to a target project. Method: We conducted experiments that compared the predictive performance between using older version data with and without Nearest Neighbor (NN) filter, a classic CPDP method. Results: NN-filter could not make clear differences in predictive performance. Conclusions: NN-filter was not helpful for improving predictive performance with older release data.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {93–96},
numpages = {4},
keywords = {Cross-Project, Defect Prediction, Experiment},
location = {Toronto, Canada},
series = {PROMISE}
}

@article{10.1016/j.jss.2017.06.070,
author = {Yu, Qiao and Jiang, Shujuan and Zhang, Yanmei},
title = {A feature matching and transfer approach for cross-company defect prediction},
year = {2017},
issue_date = {October 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {132},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.06.070},
doi = {10.1016/j.jss.2017.06.070},
abstract = {A feature matching algorithm is designed to address the heterogeneous features.A feature matching and transfer (FMT) approach for cross-company defect prediction.An empirical study is conducted on 16 datasets from NASA and PROMISE.The results show that FMT is effective for cross-company defect prediction. Software defect prediction has drawn much attention of researchers in software engineering. Traditional defect prediction methods aim to build the prediction model based on historical data. For a new project or a project with limited historical data, we cannot build a good prediction model. Therefore, researchers have proposed the cross-project defect prediction (CPDP) and cross-company defect prediction (CCDP) methods to share the historical data among different projects. However, the features of cross-company datasets are often heterogeneous, which may affect the feasibility of CCDP. To address the heterogeneous features of CCDP, this paper presents a feature matching and transfer (FMT) approach. First, we conduct feature selection for the source project and get the distribution curves of selected features. Similarly, we also get the distribution curves of all features in the target project. Second, according to the distance of different distribution curves, we design a feature matching algorithm to convert the heterogeneous features into the matched features. Finally, we can achieve feature transfer from the source project to the target project. All experiments are conducted on 16 datasets from NASA and PROMISE, and the results show that FMT is effective for CCDP.},
journal = {J. Syst. Softw.},
month = oct,
pages = {366–378},
numpages = {13},
keywords = {Software defect prediction, Heterogeneous features, Feature transfer, Feature matching}
}

@article{10.1504/ijcsm.2021.117600,
author = {Hammad, Mustafa},
title = {Classifying defective software projects based on machine learning and complexity metrics},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {4},
issn = {1752-5055},
url = {https://doi.org/10.1504/ijcsm.2021.117600},
doi = {10.1504/ijcsm.2021.117600},
abstract = {Software defects can lead to software failures or errors at any time. Therefore, software developers and engineers spend a lot of time and effort in order to find possible defects. This paper proposes an automatic approach to predict software defects based on machine learning algorithms. A set of complexity measures values are used to train the classifier. Three public datasets were used to evaluate the ability of mining complexity measures for different software projects to predict possible defects. Experimental results showed that it is possible to min software complexity to build a defect prediction model with a high accuracy rate.},
journal = {Int. J. Comput. Sci. Math.},
month = jan,
pages = {401–412},
numpages = {11},
keywords = {support vector machine, SVM, decision trees, na\"{\i}ve Bayes, neural networks, complexity, machine learning, software metrics, defect prediction, software defects}
}

@inproceedings{10.1145/2499393.2499395,
author = {Herbold, Steffen},
title = {Training data selection for cross-project defect prediction},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499395},
doi = {10.1145/2499393.2499395},
abstract = {Software defect prediction has been a popular research topic in recent years and is considered as a means for the optimization of quality assurance activities. Defect prediction can be done in a within-project or a cross-project scenario. The within-project scenario produces results with a very high quality, but requires historic data of the project, which is often not available. For the cross-project prediction, the data availability is not an issue as data from other projects is readily available, e.g., in repositories like PROMISE. However, the quality of the defect prediction results is too low for practical use. Recent research showed that the selection of appropriate training data can improve the quality of cross-project defect predictions. In this paper, we propose distance-based strategies for the selection of training data based on distributional characteristics of the available data. We evaluate the proposed strategies in a large case study with 44 data sets obtained from 14 open source projects. Our results show that our training data selection strategy improves the achieved success rate of cross-project defect predictions significantly. However, the quality of the results still cannot compete with within-project defect prediction.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {6},
numpages = {10},
keywords = {cross-project prediction, defect-prediction, machine learning},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@article{10.1002/smr.2230,
author = {Ampatzoglou, Apostolos and Arcelli&nbsp;Fontana, Francesca and Palomba, Fabio and Walter, Bartosz},
title = {Introduction to the special issue on “Machine Learning Techniques for Software Quality Evaluation”},
year = {2019},
issue_date = {September 2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {31},
number = {9},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2230},
doi = {10.1002/smr.2230},
journal = {J. Softw. Evol. Process},
month = oct,
numpages = {2}
}

@inproceedings{10.1109/ICMLA.2009.18,
author = {Khoshgoftaar, Taghi M. and Gao, Kehan},
title = {Feature Selection with Imbalanced Data for Software Defect Prediction},
year = {2009},
isbn = {9780769539263},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2009.18},
doi = {10.1109/ICMLA.2009.18},
abstract = {In this paper, we study the learning impact of data sampling followed by attribute selection on the classification models built with binary class imbalanced data within the scenario of software quality engineering. We use a wrapper-based attribute ranking technique to select a subset of attributes, and the random undersampling technique (RUS) on the majority class to alleviate the negative effects of imbalanced data on the prediction models. The datasets used in the empirical study were collected from numerous software projects. Five data preprocessing scenarios were explored in these experiments, including: (1) training on the original, unaltered fit dataset, (2) training on a sampled version of the fit dataset, (3) training on an unsampled version of the fit dataset using only the attributes chosen by feature selection based on the unsampled fit dataset, (4) training on an unsampled version of the fit dataset using only the attributes chosen by feature selection based on a sampled version of the fit dataset, and (5) training on a sampled version of the fit dataset using only the attributes chosen by feature selection based on the sampled version of the fit dataset. We compared the performances of the classification models constructed over these five different scenarios. The results demonstrate that the classification models constructed on the sampled fit data with or without feature selection (case 2 and case 5) significantly outperformed the classification models built with the other cases (unsampled fit data). Moreover, the two scenarios using sampled data (case 2 and case 5) showed very similar performances, but the subset of attributes (case 5) is only around 15% or 30% of the complete set of attributes (case 2).},
booktitle = {Proceedings of the 2009 International Conference on Machine Learning and Applications},
pages = {235–240},
numpages = {6},
keywords = {wrapper-based attribute ranking, software defect prediction, imbalanced data, feature selection},
series = {ICMLA '09}
}

@inproceedings{10.1145/3439961.3439971,
author = {Correia, Jo\~{a}o Lucas and Pereira, Juliana Alves and Mello, Rafael and Garcia, Alessandro and Fonseca, Baldoino and Ribeiro, M\'{a}rcio and Gheyi, Rohit and Kalinowski, Marcos and Cerqueira, Renato and Tiengo, Willy},
title = {Brazilian Data Scientists: Revealing their Challenges and Practices on Machine Learning Model Development},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439971},
doi = {10.1145/3439961.3439971},
abstract = {Data scientists often develop machine learning models to solve a variety of problems in the industry and academy. To build these models, these professionals usually perform activities that are also performed in the traditional software development lifecycle, such as eliciting and implementing requirements. One might argue that data scientists could rely on the engineering of traditional software development to build machine learning models. However, machine learning development presents certain characteristics, which may raise challenges that lead to the need for adopting new practices. The literature lacks in characterizing this knowledge from the perspective of the data scientists. In this paper, we characterize challenges and practices addressing the engineering of machine learning models that deserve attention from the research community. To this end, we performed a qualitative study with eight data scientists across five different companies having different levels of experience in developing machine learning models. Our findings suggest that: (i) data processing and feature engineering are the most challenging stages in the development of machine learning models; (ii) it is essential synergy between data scientists and domain experts in most of stages; and (iii) the development of machine learning models lacks the support of a well-engineered process.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {10},
numpages = {10},
keywords = {Software Engineering, Practitioner, Machine Learning, Empirical Study},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@article{10.1016/j.asoc.2014.11.023,
author = {Malhotra, Ruchika},
title = {A systematic review of machine learning techniques for software fault prediction},
year = {2015},
issue_date = {February 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {27},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2014.11.023},
doi = {10.1016/j.asoc.2014.11.023},
abstract = {Reviews studies from 1991-2013 to assess application of ML techniques for SFP.Identifies seven categories of the ML techniques.Identifies 64 studies to answer the established research questions.Selects primary studies according to the quality assessment of the studies.Systematic literature review performs the following:Summarize ML techniques for SFP models.Assess performance accuracy and capability of ML techniques for constructing SFP models.Provide comparison between the ML and statistical techniques.Provide comparison of performance accuracy of different ML techniques.Summarize the strength and weakness of the ML techniques.Provides future guidelines to software practitioners and researchers. BackgroundSoftware fault prediction is the process of developing models that can be used by the software practitioners in the early phases of software development life cycle for detecting faulty constructs such as modules or classes. There are various machine learning techniques used in the past for predicting faults. MethodIn this study we perform a systematic review of studies from January 1991 to October 2013 in the literature that use the machine learning techniques for software fault prediction. We assess the performance capability of the machine learning techniques in existing research for software fault prediction. We also compare the performance of the machine learning techniques with the statistical techniques and other machine learning techniques. Further the strengths and weaknesses of machine learning techniques are summarized. ResultsIn this paper we have identified 64 primary studies and seven categories of the machine learning techniques. The results prove the prediction capability of the machine learning techniques for classifying module/class as fault prone or not fault prone. The models using the machine learning techniques for estimating software fault proneness outperform the traditional statistical models. ConclusionBased on the results obtained from the systematic review, we conclude that the machine learning techniques have the ability for predicting software fault proneness and can be used by software practitioners and researchers. However, the application of the machine learning techniques in software fault prediction is still limited and more number of studies should be carried out in order to obtain well formed and generalizable results. We provide future guidelines to practitioners and researchers based on the results obtained in this work.},
journal = {Appl. Soft Comput.},
month = feb,
pages = {504–518},
numpages = {15},
keywords = {Systematic literature review, Software fault proneness, Machine learning}
}

@inproceedings{10.1109/CEC45853.2021.9504804,
author = {Roberto Farah, Paulo and Mariani, Thain\'{a} and da Roza, Enrique A. and Silva, Rog\'{e}rio C. and Regina Vergilio, Silvia},
title = {Unsupervised Learning For Refactoring Pattern Detection},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC45853.2021.9504804},
doi = {10.1109/CEC45853.2021.9504804},
abstract = {Software refactoring changes the structure of a program without modifying its external behavior, generally intending to improve software quality attributes. However, refactoring is a complex activity and, many times, a composition of refactorings is necessary. Besides, some code elements are refactored similarly, considering the kind and frequency of refactorings applied. Works in the refactoring literature usually investigate the impact and understanding of an individual refactoring, neglecting that developers have to apply more than one refactoring operation to reach their goals. There is a lack of studies to identify and characterize refactoring patterns. To fulfill this gap, this work explores the use of unsupervised learning, particularly cluster analysis, to group elements (Java classes) that are refactored similarly in software repositories. We used a total of 1435 projects and applied the K-Means algorithm to group classes that received the same refactoring with the same frequency. We obtained a set of seven clusters. Then, the main refactoring compositions associated with each cluster are analyzed to identify the corresponding pattern. Each pattern is described and also characterized using a set of metrics. The great majority of refactoring compositions include only one kind of refactoring, applied with low frequency. If we consider compositions including more than one type of refactorings, combinations of Extract Superclass and Pull Up Method are the most frequent.},
booktitle = {2021 IEEE Congress on Evolutionary Computation (CEC)},
pages = {2377–2384},
numpages = {8},
location = {Krak\'{o}w, Poland}
}

@inproceedings{10.1109/ICMLA.2012.226,
author = {Hall, Tracy and Bowes, David},
title = {The State of Machine Learning Methodology in Software Fault Prediction},
year = {2012},
isbn = {9780769549132},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2012.226},
doi = {10.1109/ICMLA.2012.226},
abstract = {The aim of this paper is to investigate the quality of methodology in software fault prediction studies using machine learning. Over two hundred studies of fault prediction have been published in the last 10 years. There is evidence to suggest that the quality of methodology used in some of these studies does not allow us to have confidence in the predictions reported by them. We evaluate the machine learning methodology used in 21 fault prediction studies. All of these studies use NASA data sets. We score each study from 1 to 10 in terms of the quality of their machine learning methodology (e.g. whether or not studies report randomising their cross validation folds). Only 10 out of the 21 studies scored 5 or more out of 10. Furthermore 1 study scored only 1 out of 10. When we plot these scores over time there is no evidence that the quality of machine learning methodology is better in recent studies. Our results suggest that there remains much to be done by both researchers and reviewers to improve the quality of machine learning methodology used in software fault prediction. We conclude that the results reported in some studies need to be treated with caution.},
booktitle = {Proceedings of the 2012 11th International Conference on Machine Learning and Applications - Volume 02},
pages = {308–313},
numpages = {6},
keywords = {software engineering, methodology, machine learning, fault prediction, experimental techniques},
series = {ICMLA '12}
}

@article{10.1049/iet-sen.2019.0278,
author = {Zhu, Kun and Zhang, Nana and Ying, Shi and Zhu, Dandan},
title = {Within‐project and cross‐project just‐in‐time defect prediction based on denoising autoencoder and convolutional neural network},
year = {2020},
issue_date = {June 2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {14},
number = {3},
url = {https://doi.org/10.1049/iet-sen.2019.0278},
doi = {10.1049/iet-sen.2019.0278},
abstract = {Just‐in‐time defect prediction is an important and useful branch in software defect prediction. At present, deep learning is a research hotspot in the field of artificial intelligence, which can combine basic defect features into deep semantic features and make up for the shortcomings of machine learning algorithms. However, the mainstream deep learning techniques have not been applied yet in just‐in‐time defect prediction. Therefore, the authors propose a novel just‐in‐time defect prediction model named DAECNN‐JDP based on denoising autoencoder and convolutional neural network in this study, which has three main advantages: (i) Different weights for the position vector of each dimension feature are set, which can be automatically trained by adaptive trainable vector. (ii) Through the training of denoising autoencoder, the input features that are not contaminated by noise can be obtained, thus learning more robust feature representation. (iii) The authors leverage a powerful representation‐learning technique, convolution neural network, to construct the basic change features into the abstract deep semantic features. To evaluate the performance of the DAECNN‐JDP model, they conduct extensive within‐project and cross‐project defect prediction experiments on six large open source projects. The experimental results demonstrate that the superiority of DAECNN‐JDP on five evaluation metrics.},
journal = {IET Software},
month = jun,
pages = {185–195},
numpages = {18},
keywords = {cross-project defect prediction experiments, convolution neural network, autoencoder convolutional neural network, just-in-time defect prediction model, mainstream deep learning techniques, basic defect features, software defect prediction, denoising autoencoder, learning (artificial intelligence), neural nets}
}

@article{10.1007/s10664-018-9661-2,
author = {Huang, Qiao and Xia, Xin and Lo, David},
title = {Revisiting supervised and unsupervised models for effort-aware just-in-time defect prediction},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9661-2},
doi = {10.1007/s10664-018-9661-2},
abstract = {Effort-aware just-in-time (JIT) defect prediction aims at finding more defective software changes with limited code inspection cost. Traditionally, supervised models have been used; however, they require sufficient labelled training data, which is difficult to obtain, especially for new projects. Recently, Yang et al. proposed an unsupervised model (i.e., LT) and applied it to projects with rich historical bug data. Interestingly, they reported that, under the same inspection cost (i.e., 20 percent of the total lines of code modified by all changes), it could find about 12% - 27% more defective changes than a state-of-the-art supervised model (i.e., EALR) when using different evaluation settings. This is surprising as supervised models that benefit from historical data are expected to perform better than unsupervised ones. Their finding suggests that previous studies on defect prediction had made a simple problem too complex. Considering the potential high impact of Yang et al.’s work, in this paper, we perform a replication study and present the following new findings: (1) Under the same inspection budget, LT requires developers to inspect a large number of changes necessitating many more context switches. (2) Although LT finds more defective changes, many highly ranked changes are false alarms. These initial false alarms may negatively impact practitioners’ patience and confidence. (3) LT does not outperform EALR when the harmonic mean of Recall and Precision (i.e., F1-score) is considered. Aside from highlighting the above findings, we propose a simple but improved supervised model called CBS+, which leverages the idea of both EALR and LT. We investigate the performance of CBS+ using three different evaluation settings, including time-wise cross-validation, 10-times 10-fold cross-validation and cross-project validation. When compared with EALR, CBS+ detects about 15% - 26% more defective changes, while keeping the number of context switches and initial false alarms close to those of EALR. When compared with LT, the number of defective changes detected by CBS+ is comparable to LT’s result, while CBS+ significantly reduces context switches and initial false alarms before first success. Finally, we discuss how to balance the tradeoff between the number of inspected defects and context switches, and present the implications of our findings for practitioners and researchers.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {2823–2862},
numpages = {40},
keywords = {Research bias, Evaluation metrics, Defect prediction}
}

@inproceedings{10.1109/WORDS.2005.32,
author = {Challagulla, Venkata U. B. and Bastani, Farokh B. and Yen, I-Ling and Paul, Raymond A.},
title = {Empirical Assessment of Machine Learning based Software Defect Prediction Techniques},
year = {2005},
isbn = {0769523471},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WORDS.2005.32},
doi = {10.1109/WORDS.2005.32},
abstract = {The wide-variety of real-time software systems, including telecontrol/telepresence systems, robotic systems, and mission planning systems, can entail dynamic code synthesis based on runtime mission-specific requirements and operating conditions. This necessitates the need for dynamic dependability assessment to ensure that these systems will perform as specified and will not fail in catastrophic ways. One approach in achieving this is to dynamically assess the modules in the synthesized code using software defect prediction techniques. Statistical models, such as Stepwise Multi-linear Regression models and multivariate models, and machine learning approaches, such as Artificial Neural Networks, Instance-based Reasoning, Bayesian-Belief Networks, Decision Trees, and Rule Inductions, have been investigated for predicting software quality. However, there is still no consensus about the best predictor model for software defects. In this paper, we evaluate different predictor models on four different real-time software defect data sets. The results show that a combination of 1R and Instance-based Learning along with the Consistencybased Subset Evaluation technique provides relatively better consistency in accuracy prediction compared to other models. The results also show that "size" and "complexity" metrics are not sufficient for accurately predicting real-time software defects.},
booktitle = {Proceedings of the 10th IEEE International Workshop on Object-Oriented Real-Time Dependable Systems},
pages = {263–270},
numpages = {8},
series = {WORDS '05}
}

@inproceedings{10.1145/3412841.3442027,
author = {Tsimpourlas, Foivos and Rajan, Ajitha and Allamanis, Miltiadis},
title = {Supervised learning over test executions as a test oracle},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442027},
doi = {10.1145/3412841.3442027},
abstract = {The challenge of automatically determining the correctness of test executions is referred to as the test oracle problem and is a key remaining issue for automated testing. The paper aims at solving the test oracle problem in a scalable and accurate way. To achieve this, we use supervised learning over test execution traces. We label a small fraction of the execution traces with their verdict of pass or fail. We use the labelled traces to train a neural network (NN) model to learn to distinguish runtime patterns for passing versus failing executions for a given program.We evaluate our approach using case studies from different application domains - 1. Module from Ethereum Blockchain, 2. Module from PyTorch deep learning framework, 3. Microsoft SEAL encryption library components and 4. Sed stream editor. We found the classification models for all subject programs resulted in high precision, recall and specificity, averaging to 89%, 88% and 92% respectively, while only training with an average 15% of the total traces. Our experiments show that the proposed NN model is promising as a test oracle and is able to learn runtime patterns to distinguish test executions for systems and tests from different application domains.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1521–1531},
numpages = {11},
keywords = {execution trace, neural networks, software testing, test oracle},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.5555/1881763.1881787,
author = {Liu, Guang-Jie and Wang, Wen-Yong},
title = {Research an educational software defect prediction model based on SVM},
year = {2010},
isbn = {3642145329},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We must pay attention and find defects, defects through the prediction to quantify the quality management and quality in order to achieve this goal, requires an estimate of the various defect detection process. Software defects are the departure of software are products' anticipative function. This paper collecting the data of the software defects, then, using the SVM model the predictive values are gained analyzing the predictive results, software are organizations can improve software control measure software process and allocate testing resources effectively.},
booktitle = {Proceedings of the Entertainment for Education, and 5th International Conference on E-Learning and Games},
pages = {215–222},
numpages = {8},
keywords = {software lifecycle, software defect, educational software, SVM},
location = {Changchun, China},
series = {Edutainment'10}
}

@inproceedings{10.1145/3196321.3196331,
author = {Xu, Zhou and Li, Shuai and Tang, Yutian and Luo, Xiapu and Zhang, Tao and Liu, Jin and Xu, Jun},
title = {Cross version defect prediction with representative data via sparse subset selection},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196331},
doi = {10.1145/3196321.3196331},
abstract = {Software defect prediction aims at detecting the defect-prone software modules by mining historical development data from software repositories. If such modules are identified at the early stage of the development, it can save large amounts of resources. Cross Version Defect Prediction (CVDP) is a practical scenario by training the classification model on the historical data of the prior version and then predicting the defect labels of modules of the current version. However, software development is a constantly-evolving process which leads to the data distribution differences across versions within the same project. The distribution differences will degrade the performance of the classification model. In this paper, we approach this issue by leveraging a state-of-the-art Dissimilarity-based Sparse Subset Selection (DS3) method. This method selects a representative module subset from the prior version based on the pairwise dissimilarities between the modules of two versions and assigns each module of the current version to one of the representative modules. These selected modules can well represent the modules of the current version, thus mitigating the distribution differences. We evaluate the effectiveness of DS3 for CVDP performance on total 40 cross-version pairs from 56 versions of 15 projects with three traditional and two effort-aware indicators. The extensive experiments show that DS3 outperforms three baseline methods, especially in terms of two effort-aware indicators.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {132–143},
numpages = {12},
keywords = {cross version defect prediction, pairwise dissimilarities, representative data, sparse subset selection},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/2851613.2851788,
author = {das D\^{o}res, Silvia N. and Alves, Luciano and Ruiz, Duncan D. and Barros, Rodrigo C.},
title = {A meta-learning framework for algorithm recommendation in software fault prediction},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851788},
doi = {10.1145/2851613.2851788},
abstract = {Software fault prediction is a significant part of software quality assurance and it is commonly used to detect faulty software modules based on software measurement data. Several machine learning based approaches have been proposed for generating predictive models from collected data, although none has become standard given the specificities of each software project. Hence, we believe that recommending the best algorithm for each project is much more important and useful than developing a single algorithm for being used in any project. For achieving that goal, we propose in this paper a novel framework for recommending machine learning algorithms that is capable of automatically identifying the most suitable algorithm according to the software project that is being considered. Our solution, namely SFP-MLF, makes use of the meta-learning paradigm in order to learn the best learner for a particular project. Results show that the SFP-MLF framework provides both the best single algorithm recommendation and also the best ranking recommendation for the software fault prediction problem.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1486–1491},
numpages = {6},
keywords = {algorithm recommendation, machine learning, meta-learning, software fault prediction, software quality},
location = {Pisa, Italy},
series = {SAC '16}
}

@article{10.1007/s11219-014-9241-7,
author = {Madeyski, Lech and Jureczko, Marian},
title = {Which process metrics can significantly improve defect prediction models? An empirical study},
year = {2015},
issue_date = {September 2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-014-9241-7},
doi = {10.1007/s11219-014-9241-7},
abstract = {The knowledge about the software metrics which serve as defect indicators is vital for the efficient allocation of resources for quality assurance. It is the process metrics, although sometimes difficult to collect, which have recently become popular with regard to defect prediction. However, in order to identify rightly the process metrics which are actually worth collecting, we need the evidence validating their ability to improve the product metric-based defect prediction models. This paper presents an empirical evaluation in which several process metrics were investigated in order to identify the ones which significantly improve the defect prediction models based on product metrics. Data from a wide range of software projects (both, industrial and open source) were collected. The predictions of the models that use only product metrics (simple models) were compared with the predictions of the models which used product metrics, as well as one of the process metrics under scrutiny (advanced models). To decide whether the improvements were significant or not, statistical tests were performed and effect sizes were calculated. The advanced defect prediction models trained on a data set containing product metrics and additionally Number of Distinct Committers (NDC) were significantly better than the simple models without NDC, while the effect size was medium and the probability of superiority (PS) of the advanced models over simple ones was high (  $$p=.016$$ p = . 016 ,  $$r=-.29$$ r = - . 29 ,  $$hbox {PS}=.76$$ PS = . 76 ), which is a substantial finding useful in defect prediction. A similar result with slightly smaller PS was achieved by the advanced models trained on a data set containing product metrics and additionally all of the investigated process metrics (  $$p=.038$$ p = . 038 ,  $$r=-.29$$ r = - . 29 ,  $$hbox {PS}=.68$$ PS = . 68 ). The advanced models trained on a data set containing product metrics and additionally Number of Modified Lines (NML) were significantly better than the simple models without NML, but the effect size was small (  $$p=.038$$ p = . 038 ,  $$r=.06$$ r = . 06 ). Hence, it is reasonable to recommend the NDC process metric in building the defect prediction models.},
journal = {Software Quality Journal},
month = sep,
pages = {393–422},
numpages = {30},
keywords = {Software metrics, Software defect prediction, Product metrics, Process metrics, Defect prediction models}
}

@inproceedings{10.1109/MSR.2017.20,
author = {Patil, Sangameshwar},
title = {Concept-based classification of software defect reports},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.20},
doi = {10.1109/MSR.2017.20},
abstract = {Automatic identification of the defect type from the textual description of a software defect can significantly speedup as well as improve the software defect management life-cycle. This has been recognized in the research community and multiple solutions based on supervised learning approach have been proposed in the recent literature. However, these approaches need significant amount of labeled training data for use in real-life projects.In this paper, we propose to use Explicit Semantic Analysis (ESA) to carry out concept-based classification of software defect reports. We compute the "semantic similarity" between the defect type labels and the defect report in a concept space spanned by Wikipedia articles and then, assign the defect type which has the highest similarity with the defect report. This approach helps us to circumvent the problem of dependence on labeled training data. Experimental results show that using concept-based classification is a promising approach for software defect classification to avoid the expensive process of creating labeled training data and yet get accuracy comparable to the traditional supervised learning approaches. To the best of our knowledge, this is the first use of Wikipedia and ESA for software defect classification problem.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {182–186},
numpages = {5},
keywords = {text data mining, software defect classification, mining software respositories, explicit semantic analysis},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3183399.3183402,
author = {Koch, Patrick and Schekotihin, Konstantin and Jannach, Dietmar and Hofer, Birgit and Wotawa, Franz and Schmitz, Thomas},
title = {Combining spreadsheet smells for improved fault prediction},
year = {2018},
isbn = {9781450356626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183399.3183402},
doi = {10.1145/3183399.3183402},
abstract = {Spreadsheets are commonly used in organizations as a programming tool for business-related calculations and decision making. Since faults in spreadsheets can have severe business impacts, a number of approaches from general software engineering have been applied to spreadsheets in recent years, among them the concept of code smells. Smells can in particular be used for the task of fault prediction. An analysis of existing spreadsheet smells, however, revealed that the predictive power of individual smells can be limited. In this work we therefore propose a machine learning based approach which combines the predictions of individual smells by using an AdaBoost ensemble classifier. Experiments on two public datasets containing real-world spreadsheet faults show significant improvements in terms of fault prediction accuracy.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {25–28},
numpages = {4},
keywords = {spreadsheet smells, spreadsheet QA, fault prediction},
location = {Gothenburg, Sweden},
series = {ICSE-NIER '18}
}

@inproceedings{10.1109/ICMLA.2010.27,
author = {Wang, Huanjing and Khoshgoftaar, Taghi M. and Napolitano, Amri},
title = {A Comparative Study of Ensemble Feature Selection Techniques for Software Defect Prediction},
year = {2010},
isbn = {9780769543000},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2010.27},
doi = {10.1109/ICMLA.2010.27},
abstract = {Feature selection has become the essential step in many data mining applications. Using a single feature subset selection method may generate local optima. Ensembles of feature selection methods attempt to combine multiple feature selection methods instead of using a single one. We present a comprehensive empirical study examining 17 different ensembles of feature ranking techniques (rankers) including six commonly-used feature ranking techniques, the signal-to-noise filter technique, and 11 threshold-based feature ranking techniques. This study utilized 16 real-world software measurement data sets of different sizes and built 13,600 classification models. Experimental results indicate that ensembles of very few rankers are very effective and even better than ensembles of many or all rankers.},
booktitle = {Proceedings of the 2010 Ninth International Conference on Machine Learning and Applications},
pages = {135–140},
numpages = {6},
keywords = {feature ranking, ensembles, defect prediction},
series = {ICMLA '10}
}

@inproceedings{10.1145/1868328.1868350,
author = {Zhang, Hongyu and Nelson, Adam and Menzies, Tim},
title = {On the value of learning from defect dense components for software defect prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868350},
doi = {10.1145/1868328.1868350},
abstract = {BACKGROUND: Defect predictors learned from static code measures can isolate code modules with a higher than usual probability of defects.AIMS: To improve those learners by focusing on the defect-rich portions of the training sets.METHOD: Defect data CM1, KC1, MC1, PC1, PC3 was separated into components. A subset of the projects (selected at random) were set aside for testing. Training sets were generated for a NaiveBayes classifier in two ways. In sample the dense treatment, the components with higher than the median number of defective modules were used for training. In the standard treatment, modules from any component were used for training. Both samples were run against the test set and evaluated using recall, probability of false alarm, and precision. In addition, under sampling and over sampling was performed on the defect data. Each method was repeated in a 10-by-10 cross-validation experiment.RESULTS: Prediction models learned from defect dense components out-performed standard method, under sampling, as well as over sampling. In statistical rankings based on recall, probability of false alarm, and precision, models learned from dense components won 4--5 times more often than any other method, and also lost the least amount of times.CONCLUSIONS: Given training data where most of the defects exist in small numbers of components, better defect predictors can be trained from the defect dense components.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {14},
numpages = {9},
keywords = {ceiling effect, defect dense components, defect prediction, sampling},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1145/3468264.3468614,
author = {Cito, J\"{u}rgen and Dillig, Isil and Kim, Seohyun and Murali, Vijayaraghavan and Chandra, Satish},
title = {Explaining mispredictions of machine learning models using rule induction},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468614},
doi = {10.1145/3468264.3468614},
abstract = {While machine learning (ML) models play an increasingly prevalent role in many software engineering tasks, their prediction accuracy is often problematic. When these models do mispredict, it can be very difficult to isolate the cause. In this paper, we propose a technique that aims to facilitate the debugging process of trained statistical models. Given an ML model and a labeled data set, our method produces an interpretable characterization of the data on which the model performs particularly poorly. The output of our technique can be useful for understanding limitations of the training data or the model itself; it can also be useful for ensembling if there are multiple models with different strengths. We evaluate our approach through case studies and illustrate how it can be used to improve the accuracy of predictive models used for software engineering tasks within Facebook.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {716–727},
numpages = {12},
keywords = {rule induction, machine learning, explainability},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3340482.3342743,
author = {Foidl, Harald and Felderer, Michael},
title = {Risk-based data validation in machine learning-based software systems},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342743},
doi = {10.1145/3340482.3342743},
abstract = {Data validation is an essential requirement to ensure the reliability and quality of Machine Learning-based Software Systems. However, an exhaustive validation of all data fed to these systems (i.e. up to several thousand features) is practically unfeasible. In addition, there has been little discussion about methods that support software engineers of such systems in determining how thorough to validate each feature (i.e. data validation rigor). Therefore, this paper presents a conceptual data validation approach that prioritizes features based on their estimated risk of poor data quality. The risk of poor data quality is determined by the probability that a feature is of low data quality and the impact of this low (data) quality feature on the result of the machine learning model. Three criteria are presented to estimate the probability of low data quality (Data Source Quality, Data Smells, Data Pipeline Quality). To determine the impact of low (data) quality features, the importance of features according to the performance of the machine learning model (i.e. Feature Importance) is utilized. The presented approach provides decision support (i.e. data validation prioritization and rigor) for software engineers during the implementation of data validation techniques in the course of deploying a trained machine learning model and its software stack.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {13–18},
numpages = {6},
keywords = {Risk-based Testing, Machine Learning, Data Validation},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@inproceedings{10.1109/ASE.2013.6693087,
author = {Jiang, Tian and Tan, Lin and Kim, Sunghun},
title = {Personalized defect prediction},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693087},
doi = {10.1109/ASE.2013.6693087},
abstract = {Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance.This paper proposes personalized defect prediction--building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java--the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {279–289},
numpages = {11},
keywords = {software reliability, personalized defect prediction, machine learning, change classification},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00014,
author = {Idowu, Samuel and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Asset management in machine learning: a survey},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00014},
doi = {10.1109/ICSE-SEIP52600.2021.00014},
abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {51–60},
numpages = {10},
keywords = {machine learning, asset management, SE4AI},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3416505.3423562,
author = {Steinhauer, Martin and Palomba, Fabio},
title = {Speeding up the data extraction of machine learning approaches: a distributed framework},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423562},
doi = {10.1145/3416505.3423562},
abstract = {In the last decade, mining software repositories (MSR) has become one of the most important sources to feed machine learning models. Especially open-source projects on platforms like GitHub are providing a tremendous amount of data and make them easily accessible. Nevertheless, there is still a lack of standardized pipelines to extract data in an automated and fast way. Even though several frameworks and tools exist which can fulfill specific tasks or parts of the data extraction process, none of them allow neither building an automated mining pipeline nor the possibility for full parallelization. As a consequence, researchers interested in using mining software repositories to feed machine learning models are often forced to re-implement commonly used tasks leading to additional development time and libraries may not be integrated optimally.  This preliminary study aims to demonstrate current limitations of existing tools and Git itself which are threatening the prospects of standardization and parallelization. We also introduce the multi-dimensionality aspects of a Git repository and how they affect the computation time. Finally, as a proof of concept, we define an exemplary pipeline for predicting refactoring operations, assessing its performance. Finally, we discuss the limitations of the pipeline and further optimizations to be done.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {13–18},
numpages = {6},
keywords = {Mining Software Repositories, Machine Learning Pipelines, Distributed Mining},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@inproceedings{10.1145/2896839.2896843,
author = {Koroglu, Yavuz and Sen, Alper and Kutluay, Doruk and Bayraktar, Akin and Tosun, Yalcin and Cinar, Murat and Kaya, Hasan},
title = {Defect prediction on a legacy industrial software: a case study on software with few defects},
year = {2016},
isbn = {9781450341547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896839.2896843},
doi = {10.1145/2896839.2896843},
abstract = {Context: Building defect prediction models for software projects is helpful for reducing the effort in locating defects. In this paper, we share our experiences in building a defect prediction model for a large industrial software project. We extract product and process metrics to build models and show that we can build an accurate defect prediction model even when 4% of the software is defective.Objective: Our goal in this project is to integrate a defect predictor into the continuous integration (CI) cycle of a large software project and decrease the effort in testing.Method: We present our approach in the form of an experience report. Specifically, we collected data from seven older versions of the software project and used additional features to predict defects of current versions. We compared several classification techniques including Naive Bayes, Decision Trees, and Random Forest and resampled our training data to present the company with the most accurate defect predictor.Results: Our results indicate that we can focus testing efforts by guiding the test team to only 8% of the software where 53% of actual defects can be found. Our model has 90% accuracy.Conclusion: We produce a defect prediction model with high accuracy for a software with defect rate of 4%. Our model uses Random Forest, that which we show has more predictive power than Naive Bayes, Logistic Regression and Decision Trees in our case.},
booktitle = {Proceedings of the 4th International Workshop on Conducting Empirical Studies in Industry},
pages = {14–20},
numpages = {7},
keywords = {random forest, process metrics, feature selection, experience report, defect prediction},
location = {Austin, Texas},
series = {CESI '16}
}

@article{10.1007/s10586-021-03282-8,
author = {Mustaqeem, Mohd. and Saqib, Mohd.},
title = {Principal component based support vector machine (PC-SVM): a hybrid technique for software defect detection},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-021-03282-8},
doi = {10.1007/s10586-021-03282-8},
abstract = {Defects are the major problems in the current situation and predicting them is also a difficult task. Researchers and scientists have developed many software defects prediction techniques to overcome this very helpful issue. But to some extend there is a need for an algorithm/method to predict defects with more accuracy, reduce time and space complexities. All the previous research conducted on the data without feature reduction lead to the curse of dimensionality. We brought up a machine learning hybrid approach by combining Principal component Analysis (PCA) and Support vector machines (SVM) to overcome the ongoing problem. We have employed PROMISE (CM1: 344 observations, KC1: 2109 observations) data from the directory of NASA to conduct our research. We split the dataset into training (CM1: 240 observations, KC1: 1476 observations) dataset and testing (CM1: 104 observations, KC1: 633 observations) datasets. Using PCA, we find the principal components for feature optimization which reduce the time complexity. Then, we applied SVM for classification due to very native qualities over traditional and conventional methods. We also employed the GridSearchCV method for hyperparameter tuning. In the proposed hybrid model we have found better accuracy (CM1: 95.2%, KC1: 86.6%) than other methods. The proposed model also presents higher evaluation in the terms of other criteria. As a limitation, the only problem with SVM is there is no probabilistic explanation for classification which may very rigid towards classifications. In the future, some other method may also introduce which can overcome this limitation and keep a soft probabilistic based margin for classification on the optimal hyperplane.},
journal = {Cluster Computing},
month = sep,
pages = {2581–2595},
numpages = {15},
keywords = {Software defects detection, PCA, SVM, Feature optimization, Classification, PROMISE dataset}
}

@article{10.1016/j.infsof.2008.04.008,
author = {Chang, Ching-Pao and Chu, Chih-Ping and Yeh, Yu-Fang},
title = {Integrating in-process software defect prediction with association mining to discover defect pattern},
year = {2009},
issue_date = {February, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2008.04.008},
doi = {10.1016/j.infsof.2008.04.008},
abstract = {Rather than detecting defects at an early stage to reduce their impact, defect prevention means that defects are prevented from occurring in advance. Causal analysis is a common approach to discover the causes of defects and take corrective actions. However, selecting defects to analyze among large amounts of reported defects is time consuming, and requires significant effort. To address this problem, this study proposes a defect prediction approach where the reported defects and performed actions are utilized to discover the patterns of actions which are likely to cause defects. The approach proposed in this study is adapted from the Action-Based Defect Prediction (ABDP), an approach uses the classification with decision tree technique to build a prediction model, and performs association rule mining on the records of actions and defects. An action is defined as a basic operation used to perform a software project, while a defect is defined as software flaws and can arise at any stage of the software process. The association rule mining finds the maximum rule set with specific minimum support and confidence and thus the discovered knowledge can be utilized to interpret the prediction models and software process behaviors. The discovered patterns then can be applied to predict the defects generated by the subsequent actions and take necessary corrective actions to avoid defects. The proposed defect prediction approach applies association rule mining to discover defect patterns, and multi-interval discretization to handle the continuous attributes of actions. The proposed approach is applied to a business project, giving excellent prediction results and revealing the efficiency of the proposed approach. The main benefit of using this approach is that the discovered defect patterns can be used to evaluate subsequent actions for in-process projects, and reduce variance of the reported data resulting from different projects. Additionally, the discovered patterns can be used in causal analysis to identify the causes of defects for software process improvement.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {375–384},
numpages = {10},
keywords = {Software defect prediction, Multi-interval discretization, Association rule}
}

@inproceedings{10.1145/3472674.3473978,
author = {De Stefano, Manuel and Pecorelli, Fabiano and Palomba, Fabio and De Lucia, Andrea},
title = {Comparing within- and cross-project machine learning algorithms for code smell detection},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473978},
doi = {10.1145/3472674.3473978},
abstract = {Code smells represent a well-known problem in software engineering, since they are a notorious cause of loss of comprehensibility and maintainability. The most recent efforts in devising automatic machine learning-based code smell detection techniques have achieved unsatisfying results so far. This could be explained by the fact that all these approaches follow a within-project classification, i.e. training and test data are taken from the same source project, which combined with the imbalanced nature of the problem, produces datasets with a very low number of instances belonging to the minority class (i.e. smelly instances). In this paper, we propose a cross-project machine learning approach and compare its performance with a within-project alternative. The core idea is to use transfer learning to increase the overall number of smelly instances in the training datasets. Our results have shown that cross-project classification provides very similar performance with respect to within-project. Despite this finding does not yet provide a step forward in increasing the performance of ML techniques for code smell detection, it sets the basis for further investigations.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {1–6},
numpages = {6},
keywords = {Transfer Learning, Empirical Software Engineering, Code smells},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@article{10.1016/j.engappai.2009.10.001,
author = {Pendharkar, Parag C.},
title = {Exhaustive and heuristic search approaches for learning a software defect prediction model},
year = {2010},
issue_date = {February, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {23},
number = {1},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.10.001},
doi = {10.1016/j.engappai.2009.10.001},
abstract = {In this paper, we propose a software defect prediction model learning problem (SDPMLP) where a classification model selects appropriate relevant inputs, from a set of all available inputs, and learns the classification function. We show that the SDPMLP is a combinatorial optimization problem with factorial complexity, and propose two hybrid exhaustive search and probabilistic neural network (PNN), and simulated annealing (SA) and PNN procedures to solve it. For small size SDPMLP, exhaustive search PNN works well and provides an (all) optimal solution(s). However, for large size SDPMLP, the use of exhaustive search PNN approach is not pragmatic and only the SA-PNN allows us to solve the SDPMLP in a practical time limit. We compare the performance of our hybrid approaches with traditional classification algorithms and find that our hybrid approaches perform better than traditional classification algorithms.},
journal = {Eng. Appl. Artif. Intell.},
month = feb,
pages = {34–40},
numpages = {7},
keywords = {Software engineering, Simulated annealing, Probabilistic neural networks, Heuristics, Exhaustive search}
}

@inproceedings{10.1145/2884781.2884857,
author = {Tantithamthavorn, Chakkrit and McIntosh, Shane and Hassan, Ahmed E. and Matsumoto, Kenichi},
title = {Automated parameter optimization of classification techniques for defect prediction models},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884857},
doi = {10.1145/2884781.2884857},
abstract = {Defect prediction models are classifiers that are trained to identify defect-prone software modules. Such classifiers have configurable parameters that control their characteristics (e.g., the number of trees in a random forest classifier). Recent studies show that these classifiers may underperform due to the use of suboptimal default parameter settings. However, it is impractical to assess all of the possible settings in the parameter spaces. In this paper, we investigate the performance of defect prediction models where Caret --- an automated parameter optimization technique --- has been applied. Through a case study of 18 datasets from systems that span both proprietary and open source domains, we find that (1) Caret improves the AUC performance of defect prediction models by as much as 40 percentage points; (2) Caret-optimized classifiers are at least as stable as (with 35% of them being more stable than) classifiers that are trained using the default settings; and (3) Caret increases the likelihood of producing a top-performing classifier by as much as 83%. Hence, we conclude that parameter settings can indeed have a large impact on the performance of defect prediction models, suggesting that researchers should experiment with the parameters of the classification techniques. Since automated parameter optimization techniques like Caret yield substantially benefits in terms of performance improvement and stability, while incurring a manageable additional computational cost, they should be included in future defect prediction studies.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {321–332},
numpages = {12},
keywords = {software defect prediction, parameter optimization, experimental design, classification techniques},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1155/2020/8858010,
author = {Shen, Zhidong and Chen, Si and Coppolino, Luigi},
title = {A Survey of Automatic Software Vulnerability Detection, Program Repair, and Defect Prediction Techniques},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1939-0114},
url = {https://doi.org/10.1155/2020/8858010},
doi = {10.1155/2020/8858010},
abstract = {Open source software has been widely used in various industries due to its openness and flexibility, but it also brings potential software security problems. Together with the large-scale increase in the number of software and the increase in complexity, the traditional manual methods to deal with these security issues are inefficient and cannot meet the current cyberspace security requirements. Therefore, it is an important research topic for researchers in the field of software security to develop more intelligent technologies to apply to potential security issues in software. The development of deep learning technology has brought new opportunities for the study of potential security issues in software, and researchers have successively proposed many automation methods. In this paper, these automation technologies are evaluated and analysed in detail from three aspects: software vulnerability detection, software program repair, and software defect prediction. At the same time, we point out some problems of these research methods, give corresponding solutions, and finally look forward to the application prospect of deep learning technology in automated software vulnerability detection, automated program repair, and automated defect prediction.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {16}
}

@inproceedings{10.1145/2884781.2884804,
author = {Wang, Song and Liu, Taiyue and Tan, Lin},
title = {Automatically learning semantic features for defect prediction},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884804},
doi = {10.1145/2884781.2884804},
abstract = {Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models.To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs).Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7% in precision, 11.5% in recall, and 14.2% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9% in F1.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {297–308},
numpages = {12},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {variability, process mining, process discovery, configuration workflow, clustering},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2245276.2231975,
author = {Banthia, Deepak and Gupta, Atul},
title = {Investigating fault prediction capabilities of five prediction models for software quality},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231975},
doi = {10.1145/2245276.2231975},
abstract = {Predicting faults in software modules can lead to a high quality and more effective software development process to follow. However, the results of a fault prediction model have to be properly interpreted before incorporating them into any decision making. Most of the earlier studies have used the prediction accuracy as the main criteria to compare amongst competing fault prediction models. However, we show that besides accuracy, other criteria like number of false positives and false negatives can equally be important to choose a candidate model for fault prediction. We have used five NASA software data sets in our experiment. Our results suggest that the performance of Simple Logistic is better than the others on raw data sets whereas the performance of Neural Network was found to be better when we applied dimensionality reduction method on raw data sets. When we used data pre-processing techniques, the prediction accuracy of Random Forest was found to be better in both cases i.e. with and without dimensionality reduction but reliability of Simple Logistic was better than Random Forest because it had less number of fault negatives.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1259–1261},
numpages = {3},
keywords = {quality assurance, fault prediction models, fault prediction, effort estimation, attribute selection},
location = {Trento, Italy},
series = {SAC '12}
}

@article{10.1007/s10515-017-0220-7,
author = {Li, Zhiqiang and Jing, Xiao-Yuan and Wu, Fei and Zhu, Xiaoke and Xu, Baowen and Ying, Shi},
title = {Cost-sensitive transfer kernel canonical correlation analysis for heterogeneous defect prediction},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0220-7},
doi = {10.1007/s10515-017-0220-7},
abstract = {Cross-project defect prediction (CPDP) refers to predicting defects in a target project using prediction models trained from historical data of other source projects. And CPDP in the scenario where source and target projects have different metric sets is called heterogeneous defect prediction (HDP). Recently, HDP has received much research interest. Existing HDP methods only consider the linear correlation relationship among the features (metrics) of the source and target projects, and such models are insufficient to evaluate nonlinear correlation relationship among the features. So these methods may suffer from the linearly inseparable problem in the linear feature space. Furthermore, existing HDP methods do not take the class imbalance problem into consideration. Unfortunately, the imbalanced nature of software defect datasets increases the learning difficulty for the predictors. In this paper, we propose a new cost-sensitive transfer kernel canonical correlation analysis (CTKCCA) approach for HDP. CTKCCA can not only make the data distributions of source and target projects much more similar in the nonlinear feature space, where the learned features have favorable separability, but also utilize the different misclassification costs for defective and defect-free classes to alleviate the class imbalance problem. We perform the Friedman test with Nemenyi's post-hoc statistical test and the Cliff's delta effect size test for the evaluation. Extensive experiments on 28 public projects from five data sources indicate that: (1) CTKCCA significantly performs better than the related CPDP methods; (2) CTKCCA performs better than the related state-of-the-art HDP methods.},
journal = {Automated Software Engg.},
month = jun,
pages = {201–245},
numpages = {45},
keywords = {Transfer learning, Kernel canonical correlation analysis, Heterogeneous defect prediction, Cost-sensitive learning, Class imbalance}
}

@article{10.1007/s10664-014-9346-4,
author = {Ryu, Duksan and Choi, Okjoo and Baik, Jongmoon},
title = {Value-cognitive boosting with a support vector machine for cross-project defect prediction},
year = {2016},
issue_date = {February  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9346-4},
doi = {10.1007/s10664-014-9346-4},
abstract = {It is well-known that software defect prediction is one of the most important tasks for software quality improvement. The use of defect predictors allows test engineers to focus on defective modules. Thereby testing resources can be allocated effectively and the quality assurance costs can be reduced. For within-project defect prediction (WPDP), there should be sufficient data within a company to train any prediction model. Without such local data, cross-project defect prediction (CPDP) is feasible since it uses data collected from similar projects in other companies. Software defect datasets have the class imbalance problem increasing the difficulty for the learner to predict defects. In addition, the impact of imbalanced data on the real performance of models can be hidden by the performance measures chosen. We investigate if the class imbalance learning can be beneficial for CPDP. In our approach, the asymmetric misclassification cost and the similarity weights obtained from distributional characteristics are closely associated to guide the appropriate resampling mechanism. We performed the effect size A-statistics test to evaluate the magnitude of the improvement. For the statistical significant test, we used Wilcoxon rank-sum test. The experimental results show that our approach can provide higher prediction performance than both the existing CPDP technique and the existing class imbalance technique.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {43–71},
numpages = {29},
keywords = {Transfer learning, Cross-project defect prediction, Class imbalance, Boosting}
}

@inproceedings{10.1145/3324884.3415286,
author = {Perera, Anjana},
title = {Using defect prediction to improve the bug detection capability of search-based software testing},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3415286},
doi = {10.1145/3324884.3415286},
abstract = {Automated test generators, such as search based software testing (SBST) techniques, replace the tedious and expensive task of manually writing test cases. SBST techniques are effective at generating tests with high code coverage. However, is high code coverage sufficient to maximise the number of bugs found? We argue that SBST needs to be focused to search for test cases in defective areas rather in non-defective areas of the code in order to maximise the likelihood of discovering the bugs. Defect prediction algorithms give useful information about the bug-prone areas in software. Therefore, we formulate the objective of this thesis: Improve the bug detection capability of SBST by incorporating defect prediction information. To achieve this, we devise two research objectives, i.e., 1) Develop a novel approach (SBSTCL) that allocates time budget to classes based on the likelihood of classes being defective, and 2) Develop a novel strategy (SBSTML) to guide the underlying search algorithm (i.e., genetic algorithm) towards the defective areas in a class. Through empirical evaluation on 434 real reported bugs in the Defects4J dataset, we demonstrate that our novel approach, SBSTCL, is significantly more efficient than the state of the art SBST when they are given a tight time budget in a resource constrained environment.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1170–1174},
numpages = {5},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2875913.2875922,
author = {Tang, Hao and Lan, Tian and Hao, Dan and Zhang, Lu},
title = {Enhancing Defect Prediction with Static Defect Analysis},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875922},
doi = {10.1145/2875913.2875922},
abstract = {In the software development process, how to develop better software at lower cost has been a major issue of concern. One way that helps is to find more defects as early as possible, on which defect prediction can provide effective guidance. The most popular defect prediction technique is to build defect prediction models based on machine learning. To improve the performance of defect prediction model, selecting appropriate features is critical. On the other hand, static analysis is usually used in defect detection. As static defect analyzers detects defects by matching some well-defined "defect patterns", its result is useful for locating defects. However, defect prediction and static defect analysis are supposed to be two parallel areas due to the differences in research motivation, solution and granularity.In this paper, we present a possible approach to improve the performance of defect prediction with the help of static analysis techniques. Specifically, we present to extract features based on defect patterns from static defect analyzers to improve the performance of defect prediction models. Based on this approach, we implemented a defect prediction tool and set up experiments to measure the effect of the features.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {43–51},
numpages = {9},
keywords = {static defect analyzer, predictive model, machine learning, defect pattern, code feature, Defect},
location = {Wuhan, China},
series = {Internetware '15}
}

@article{10.1007/s10270-021-00913-x,
author = {Nguyen, Phuong T. and Di&nbsp;Rocco, Juri and Iovino, Ludovico and Di&nbsp;Ruscio, Davide and Pierantonio, Alfonso},
title = {Evaluation of a machine learning classifier for metamodels},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {6},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-021-00913-x},
doi = {10.1007/s10270-021-00913-x},
abstract = {Modeling is a ubiquitous activity in the process of software development. In recent years, such an activity has reached a high degree of intricacy, guided by the heterogeneity of the components, data sources, and tasks. The democratized use of models has led to the necessity for suitable machinery for mining modeling repositories. Among others, the classification of metamodels into independent categories facilitates personalized searches by boosting the visibility of metamodels. Nevertheless, the manual classification of metamodels is not only a tedious but also an error-prone task. According to our observation, misclassification is the norm which leads to a reduction in reachability as well as reusability of metamodels. Handling such complexity requires suitable tooling to leverage raw data into practical knowledge that can help modelers with their daily tasks. In our previous work, we proposed AURORA as a machine learning classifier for metamodel repositories. In this paper, we present a thorough evaluation of the system by taking into consideration different settings as well as evaluation metrics. More importantly, we improve the original AURORA tool by changing its internal design. Experimental results demonstrate that the proposed amendment is beneficial to the classification of metamodels. We also compared our approach with two baseline algorithms, namely gradient boosted decision tree and support vector machines. Eventually, we see that AURORA outperforms the baselines with respect to various quality metrics.},
journal = {Softw. Syst. Model.},
month = dec,
pages = {1797–1821},
numpages = {25},
keywords = {SVM, GBDT, Neural networks, Machine learning, Model-driven engineering}
}

@article{10.1145/3453444,
author = {Ashmore, Rob and Calinescu, Radu and Paterson, Colin},
title = {Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453444},
doi = {10.1145/3453444},
abstract = {Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic, and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence, and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our article provides a comprehensive survey of the state of the art in the assurance of ML, i.e., in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the machine learning lifecycle, i.e., of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The article begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {111},
numpages = {39},
keywords = {safety-critical systems, machine learning workflow, assurance evidence, assurance, Machine learning lifecycle}
}

@article{10.1016/j.eswa.2015.03.013,
author = {\"{O}zt\"{u}rk, Muhammed Maruf and Cavusoglu, Unal and Zengin, Ahmet},
title = {A novel defect prediction method for web pages using k-means++},
year = {2015},
issue_date = {November 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {19},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.03.013},
doi = {10.1016/j.eswa.2015.03.013},
abstract = {Presents a novel defect clustering method.Shed new light to defect prediction methods.Depicts the prominence of k-means++ for software testing.Unveils the density of error rates of web elements. With the increase of the web software complexity, defect detection and prevention have become crucial processes in the software industry. Over the past decades, defect prediction research has reported encouraging results for reducing software product costs. Despite promising results, these researches have hardly been applied to web based systems using clustering algorithms. An appropriate implementation of the clustering in defect prediction may facilitate to estimate defects in a web page source code. One of the widely used clustering algorithms is k-means whose derived versions such as k-means++ show good performance on large-data sets. Here, we present a new defect clustering method using k-means++ for web page source codes. According to the experimental results, almost half of the defects are detected in the middle of web pages. k-means++ is significantly better than the other four clustering algorithms in three criteria on four data set. We also tested our method on four classifiers and the results have shown that after the clustering, Linear Discriminant Analysis is, in general, better than the other three classifiers.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {6496–6506},
numpages = {11},
keywords = {k-means++, Software testing, Fault clustering, Defect prediction}
}

@article{10.1016/j.asoc.2016.04.009,
author = {Ryu, Duksan and Baik, Jongmoon},
title = {Effective multi-objective nave Bayes learning for cross-project defect prediction},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.04.009},
doi = {10.1016/j.asoc.2016.04.009},
abstract = {Display Omitted We propose novel multi-objective learning techniques considering the class imbalance context for cross-project defect prediction.The proposed approaches (i.e., MONB and MONBNN) show the better diversity compared to existing multi-objective prediction models.The proposed approaches show the similar prediction performance compared to within-project defect prediction models. Software defect prediction predicts fault-prone modules which will be tested thoroughly. Thereby, limited quality control resources can be allocated effectively on them. Without sufficient local data, defects can be predicted via cross-project defect prediction (CPDP) utilizing data from other projects to build a classifier. Software defect datasets have the class imbalance problem, indicating the defect class has much fewer instances than the non-defect class does. Unless defect instances are predicted correctly, software quality could be degraded. In this context, a classifier requires to provide high accuracy of the defect class without severely worsening the accuracy of the non-defect class. This class imbalance principle seamlessly connects to the purpose of the multi-objective (MO) optimization in that MO predictive models aim at balancing many of the competing objectives. In this paper, we target to identify effective multi-objective learning techniques under cross-project (CP) environments. Three objectives are devised considering the class imbalance context. The first objective is to maximize the probability of detection (PD). The second objective is to minimize the probability of false alarm (PF). The third objective is to maximize the overall performance (e.g., Balance). We propose novel MO naive Bayes learning techniques modeled by a Harmony Search meta-heuristic algorithm. Our approaches are compared with single-objective models, other existing MO models and within-project defect prediction models. The experimental results show that the proposed approaches are promising. As a result, they can be effectively applied to satisfy various prediction needs under CP settings.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1062–1077},
numpages = {16},
keywords = {Search-based software engineering, Multi-objective optimization, Harmony Search, Cross-project defect prediction, Class imbalance}
}

@inproceedings{10.1007/978-3-030-64881-7_16,
author = {Sharma, Arnab and Wehrheim, Heike},
title = {Automatic Fairness Testing of Machine Learning Models},
year = {2020},
isbn = {978-3-030-64880-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64881-7_16},
doi = {10.1007/978-3-030-64881-7_16},
abstract = {In recent years, there has been an increased application of machine learning (ML) to decision making systems. This has prompted an urgent need for validating requirements on ML models. Fairness is one such requirement to be ensured in numerous application domains. It specifies a software as “learned” by an ML algorithm to not be biased in the sense of discriminating against some attributes (like gender or age), giving different decisions upon flipping the values of these attributes.In this work, we apply verification-based testing (VBT) to the fairness checking of ML models. Verification-based testing employs verification technology to generate test cases potentially violating the property under interest. For fairness testing, we additionally provide a specification language for the formalization of different fairness requirements. From the ML model under test and fairness specification VBT automatically generates test inputs specific to the specified fairness requirement. The empirical evaluation on several benchmark ML models shows verification-based testing to perform better than existing fairness testing techniques with respect to effectiveness.},
booktitle = {Testing Software and Systems: 32nd IFIP WG 6.1 International Conference, ICTSS 2020, Naples, Italy, December 9–11, 2020, Proceedings},
pages = {255–271},
numpages = {17},
keywords = {SMT solving, Machine learning testing, Fairness},
location = {Naples, Italy}
}

@article{10.1007/s11227-018-2326-5,
author = {Kalsoom, Anum and Maqsood, Muazzam and Ghazanfar, Mustansar Ali and Aadil, Farhan and Rho, Seungmin},
title = {A dimensionality reduction-based efficient software fault prediction using Fisher linear discriminant analysis (FLDA)},
year = {2018},
issue_date = {Sep 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {74},
number = {9},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-018-2326-5},
doi = {10.1007/s11227-018-2326-5},
abstract = {Software quality is an important factor in the success of software companies. Traditional software quality assurance techniques face some serious limitations especially in terms of time and budget. This leads to increase in the use of machine learning classification techniques to predict software faults. Software fault prediction can help developers to uncover software problems in early stages of software life cycle. The extent to which these techniques can be generalized to different sizes of software, class imbalance problem, and identification of discriminative software metrics are the most critical challenges. In this paper, we have analyzed the performance of nine widely used machine learning classifiers--Bayes Net, NB, artificial neural network, support vector machines, K nearest neighbors, AdaBoost, Bagging, Zero R, and Random Forest for software fault prediction. Two standard sampling techniques--SMOTE and Resample with substitution are used to handle the class imbalance problem. We further used FLDA-based feature selection approach in combination with SMOTE and Resample to select most discriminative metrics. Then the top four classifiers based on performance are used for software fault prediction. The experimentation is carried out over 15 publically available datasets (small, medium and large) which are collected from PROMISE repository. The proposed Resample-FLDA method gives better performance as compared to existing methods in terms of precision, recall, f-measure and area under the curve.},
journal = {J. Supercomput.},
month = sep,
pages = {4568–4602},
numpages = {35},
keywords = {Software fault prediction, Robustness, Reliability, Fisher linear discriminant, Fault-tolerance}
}

@inproceedings{10.1145/3452383.3452391,
author = {Kuri, Mohit and Karre, Sai Anirudh and Reddy, Y. Raghu},
title = {Understanding Software Quality Metrics for Virtual Reality Products - A Mapping Study},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452391},
doi = {10.1145/3452383.3452391},
abstract = {Virtual Reality (VR) Software is becoming more mainstream in recent years. It has provided an opportunity for VR practitioners to explore new domains and deliver cutting edge products. The success of the VR products depends primarily on the product contextual relevance and qualities exhibited. However, it is unclear how VR practitioners curb software quality challenges and improve the essence of the VR product over every release. In this paper, we present a Systematic Mapping Study of the software quality metrics adopted by VR practitioners for assessing the quality of their VR products. The study showed that practitioners used unique metrics to measure the quality of their VR products in addition to adopting some of existing enterprise software metrics. Further, we consolidate these metrics into different themes that future practitioners may use for developing VR products.},
booktitle = {Proceedings of the 14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {8},
numpages = {11},
keywords = {Industrial Practices, Metrics, Software Quality, Virtual Reality},
location = {Bhubaneswar, Odisha, India},
series = {ISEC '21}
}

@inproceedings{10.1145/3383219.3383243,
author = {Pham, Van and Lokan, Chris and Kasmarik, Kathryn},
title = {A Better Set of Object-Oriented Design Metrics for Within-Project Defect Prediction},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383243},
doi = {10.1145/3383219.3383243},
abstract = {Background: Using design metrics to predict fault-prone elements of a software design can help to focus attention on classes that need redesign and more extensive testing. However, some design metrics have been pointed out to be theoretically invalid, and the usefulness of some metrics is questioned.Aim: To identify a set of object-oriented metrics that are theoretically valid, and useful for identifying fault-prone classes in a design.Method: Drawing on four well-known sets of design metrics (CK, LK, MOOD and QMOOD), we propose a consolidated set of metrics that covers many aspects of object-oriented software design. We conduct two experiments, first using a single large system and then considering successive releases of that system, to compare the usefulness of the consolidated set with the other four sets for within-project prediction of fault-prone classes.Results: Both experiments suggest the consolidated set is effective at identifying fault-prone classes, outperforming the other metric sets (though at a cost of more false alarms).Conclusion: This paper adds to knowledge about the usefulness of existing sets of design metrics for within-project defect prediction, and identifies a consolidated set of metrics that is more effective than the existing sets at identifying fault-prone classes.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {230–239},
numpages = {10},
keywords = {fault-proneness prediction, design metrics, Object-oriented software design},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/2972958.2972964,
author = {Hosseini, Seyedrebvar and Turhan, Burak and M\"{a}ntyl\"{a}, Mika},
title = {Search Based Training Data Selection For Cross Project Defect Prediction},
year = {2016},
isbn = {9781450347723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2972958.2972964},
doi = {10.1145/2972958.2972964},
abstract = {Context: Previous studies have shown that steered training data or dataset selection can lead to better performance for cross project defect prediction (CPDP). On the other hand, data quality is an issue to consider in CPDP.Aim: We aim at utilising the Nearest Neighbor (NN)-Filter, embedded in a genetic algorithm, for generating evolving training datasets to tackle CPDP, while accounting for potential noise in defect labels.Method: We propose a new search based training data (i.e., instance) selection approach for CPDP called GIS (Genetic Instance Selection) that looks for solutions to optimize a combined measure of F-Measure and GMean, on a validation set generated by (NN)-filter. The genetic operations consider the similarities in features and address possible noise in assigned defect labels. We use 13 datasets from PROMISE repository in order to compare the performance of GIS with benchmark CPDP methods, namely (NN)-filter and naive CPDP, as well as with within project defect prediction (WPDP).Results: Our results show that GIS is significantly better than (NN)-Filter in terms of F-Measure (p -- value ≪ 0.001, Cohen's d = 0.697) and GMean (p -- value ≪ 0.001, Cohen's d = 0.946). It also outperforms the naive CPDP approach in terms of F-Measure (p -- value ≪ 0.001, Cohen's d = 0.753) and GMean (p -- value ≪ 0.001, Cohen's d = 0.994). In addition, the performance of our approach is better than that of WPDP, again considering F-Measure (p -- value ≪ 0.001, Cohen's d = 0.227) and GMean (p -- value ≪ 0.001, Cohen's d = 0.595) values.Conclusions: We conclude that search based instance selection is a promising way to tackle CPDP. Especially, the performance comparison with the within project scenario encourages further investigation of our approach. However, the performance of GIS is based on high recall in the expense of low precision. Using different optimization goals, e.g. targeting high precision, would be a future direction to investigate.},
booktitle = {Proceedings of the The 12th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {3},
numpages = {10},
keywords = {Cross Project Defect Prediction, Genetic Algorithms, Instance Selection, Search Based Optimization, Training Data Selection},
location = {Ciudad Real, Spain},
series = {PROMISE 2016}
}

@inproceedings{10.1145/2695664.2695959,
author = {Xuan, Xiao and Lo, David and Xia, Xin and Tian, Yuan},
title = {Evaluating defect prediction approaches using a massive set of metrics: an empirical study},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695959},
doi = {10.1145/2695664.2695959},
abstract = {To evaluate the performance of a within-project defect prediction approach, people normally use precision, recall, and F-measure scores. However, in machine learning literature, there are a large number of evaluation metrics to evaluate the performance of an algorithm, (e.g., Matthews Correlation Coefficient, G-means, etc.), and these metrics evaluate an approach from different aspects. In this paper, we investigate the performance of within-project defect prediction approaches on a large number of evaluation metrics. We choose 6 state-of-the-art approaches including naive Bayes, decision tree, logistic regression, kNN, random forest and Bayesian network which are widely used in defect prediction literature. And we evaluate these 6 approaches on 14 evaluation metrics (e.g., G-mean, F-measure, balance, MCC, J-coefficient, and AUC). Our goal is to explore a practical and sophisticated way for evaluating the prediction approaches comprehensively. We evaluate the performance of defect prediction approaches on 10 defect datasets from PROMISE repository. The results show that Bayesian network achieves a noteworthy performance. It achieves the best recall, FN-R, G-mean1 and balance on 9 out of the 10 datasets, and F-measure and J-coefficient on 7 out of the 10 datasets.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1644–1647},
numpages = {4},
keywords = {machine learning, evaluation metric, defect prediction},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3358528.3358594,
author = {Lu, Guo and Zhongqing, Yu and Jianqi, Yu and Chuang, Liu},
title = {Data Driven Induction Motor Condition Identification and Fault Prediction},
year = {2019},
isbn = {9781450371926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358528.3358594},
doi = {10.1145/3358528.3358594},
abstract = {With the development of technologies such as sensing and the scale of industrial production, the equipment data can be gathered massively. For induction motors, the health data can be collected is sufficient, but the fault data is not easy to obtain. Therefore, the focus of this paper was determined to identify motor operating conditions and predict possible faults based on motor health data. In this case, an induction motor condition model consisting of a state recognizer and adaptive thresholds was proposed. The health data was used for the training of the induction motor condition model, and an improved SOM-FCM Two-Layer clustering method was used to solve the problem of obtaining the motor data without label. Finally, the validity of the model and method was verified by normal motor variable load state identification and rotor broken motor fault prediction, and the accuracy of 97.5% and 90.2% was obtained respectively.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Technologies},
pages = {221–229},
numpages = {9},
keywords = {Two-Layer clustering, Induction Motor, Fault Prediction, Data Driven, Condition Identification},
location = {Jinan, China},
series = {ICBDT '19}
}

@inproceedings{10.1145/2915970.2916007,
author = {Petri\'{c}, Jean and Bowes, David and Hall, Tracy and Christianson, Bruce and Baddoo, Nathan},
title = {The jinx on the NASA software defect data sets},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2916007},
doi = {10.1145/2915970.2916007},
abstract = {Background: The NASA datasets have previously been used extensively in studies of software defects. In 2013 Shepperd et al. presented an essential set of rules for removing erroneous data from the NASA datasets making this data more reliable to use.Objective: We have now found additional rules necessary for removing problematic data which were not identified by Shepperd et al.Results: In this paper, we demonstrate the level of erroneous data still present even after cleaning using Shepperd et al.'s rules and apply our new rules to remove this erroneous data.Conclusion: Even after systematic data cleaning of the NASA MDP datasets, we found new erroneous data. Data quality should always be explicitly considered by researchers before use.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {13},
numpages = {5},
keywords = {software defect prediction, machine learning, data quality},
location = {Limerick, Ireland},
series = {EASE '16}
}

@article{10.1007/s10619-021-07331-4,
author = {Srinivasan, R. and Subalalitha, C. N.},
title = {Sentimental analysis from imbalanced code-mixed data using machine learning approaches},
year = {2021},
issue_date = {Jun 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {41},
number = {1},
issn = {0926-8782},
url = {https://doi.org/10.1007/s10619-021-07331-4},
doi = {10.1007/s10619-021-07331-4},
abstract = {Knowledge discovery from various perspectives has become a crucial asset in almost all fields. Sentimental analysis is a classification task used to classify the sentence based on the meaning of their context. This paper addresses class imbalance problem which is one of the important issues in sentimental analysis. Not much works focused on sentimental analysis with imbalanced class label distribution. The paper also focusses on another aspect of the problem which involves a concept called “Code Mixing”. Code mixed data consists of text alternating between two or more languages. Class imbalance distribution is a commonly noted phenomenon in a code-mixed data. The existing works have focused more on analyzing the sentiments in a monolingual data but not in a code-mixed data. This paper addresses all these issues and comes up with a solution to analyze sentiments for a class imbalanced code-mixed data using sampling technique combined with levenshtein distance metrics. Furthermore, this paper compares the performances of various machine learning approaches namely, Random Forest Classifier, Logistic Regression, XGBoost classifier, Support Vector Machine and Na\"{\i}ve Bayes Classifier using F1- Score.},
journal = {Distrib. Parallel Databases},
month = mar,
pages = {37–52},
numpages = {16},
keywords = {Code-mixed data, Sentimental analysis, Imbalanced data, Machine learning, Sampling}
}

@inproceedings{10.1007/978-3-030-79379-1_1,
author = {Meinke, Karl and Khosrowjerdi, Hojat},
title = {Use Case Testing: A Constrained Active Machine Learning Approach},
year = {2021},
isbn = {978-3-030-79378-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79379-1_1},
doi = {10.1007/978-3-030-79379-1_1},
abstract = {As a methodology for system design and testing, use cases are well-known and widely used. While current active machine learning (ML) algorithms can effectively automate unit testing, they do not scale up&nbsp;to use case testing of complex systems in an efficient way.We present a new parallel distributed processing (PDP) architecture for a constrained active machine learning (CAML) approach to use case testing. To exploit CAML we introduce a use case modeling language with: (i) compile-time constraints on query generation, and (ii) run-time constraints using dynamic constraint checking. We evaluate this approach by applying a prototype implementation of CAML to use case testing of simulated multi-vehicle autonomous driving scenarios.},
booktitle = {Tests and Proofs: 15th International Conference, TAP 2021, Held as Part of STAF 2021, Virtual Event, June 21–22, 2021, Proceedings},
pages = {3–21},
numpages = {19},
keywords = {Use case testing, Requirements testing, Model checking, Machine learning, Learning-based testing, Constraint solving, Autonomous driving}
}

@inproceedings{10.1109/COMPSAC.2014.66,
author = {Liu, Shulong and Chen, Xiang and Liu, Wangshu and Chen, Jiaqiang and Gu, Qing and Chen, Daoxu},
title = {FECAR: A Feature Selection Framework for Software Defect Prediction},
year = {2014},
isbn = {9781479935758},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2014.66},
doi = {10.1109/COMPSAC.2014.66},
abstract = {Software defect prediction can classify new software entities into either buggy or clean. However the effectiveness of existing methods is influenced by irrelevant and redundant features. In this paper, we propose a new feature selection framework FECAR using Feature Clustering And feature Ranking. This framework firstly partitions original features into k clusters based on FF-Correlation measure. Then it selects relevant features from each cluster based on FC-Relevance measure. In empirical study, we choose Symmetric Uncertainty as FF-Correlation measure, and choose Information Gain, Chi-Square, and Relief as three different FC-Relevance measures. Based on some real projects Eclipse and NASA, we implemented our framework and performed empirical studies to investigate the redundancy rate and the performance of the trained defect predictors. Final results verify the effectiveness of our proposed framework and further provide a guideline for achieving cost-effective feature selection when using our framework.},
booktitle = {Proceedings of the 2014 IEEE 38th Annual Computer Software and Applications Conference},
pages = {426–435},
numpages = {10},
keywords = {Software Defect Prediction, Feature Selection, Feature Clustering, Feature Ranking},
series = {COMPSAC '14}
}

@inproceedings{10.1145/3364641.3364670,
author = {Amaral, Weldson and Rivero, Luis and Junior, Geraldo Braz and Viana, Davi},
title = {Using Machine Learning Technique for Effort Estimation in Software Development},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364670},
doi = {10.1145/3364641.3364670},
abstract = {Estimates in software projects aim to help practitioners predict more realistic values on software development, impacting the quality of software process activities regarding planning and execution. However, software companies have difficulties when carrying out estimations that represent adequately the real effort needed to execute the software project activities. Although, the literature presents techniques to estimate effort, this activity remains complex. Recently, Machine Learning (ML) techniques are been applied to solve this problem. Through ML techniques it is possible to use databases of finished projects (datasets) to help get more precisely estimations. This research aims to propose a methodology to estimate effort using a ML technique based on decision trees: XGBoost. To evaluate our methodology, we conducted tests with four datasets using two metrics: Mean Magnitude Relative Error and Prediction(25). The preliminary results show consistent results for this methodology for software effort estimation based on the employed metrics, which indicates that our methodology is promising. As further work, new datasets must be analyzed using our methodology, and also an approach using synthetic data to improve the ML training.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {240–245},
numpages = {6},
keywords = {Software Projects, Machine Learning, Effort estimation, Boosting},
location = {Fortaleza, Brazil},
series = {SBQS '19}
}

@inproceedings{10.1007/978-3-319-47955-2_19,
author = {Murillo-Morera, Juan and Castro-Herrera, Carlos and Arroyo, Javier and Fuentes-Fern\'{a}ndez, Rub\'{e}n},
title = {An Empirical Validation of Learning Schemes Using an Automated Genetic Defect Prediction Framework},
year = {2016},
isbn = {978-3-319-47954-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-47955-2_19},
doi = {10.1007/978-3-319-47955-2_19},
abstract = {Today, it is common for software projects to collect measurement data through development processes. With these data, defect prediction software can try to estimate the defect proneness of a software module, with the objective of assisting and guiding software practitioners. With timely and accurate defect predictions, practitioners can focus their limited testing resources on higher risk areas. This paper reports a benchmarking study that uses a genetic algorithm that automatically generates and compares different learning schemes (preprocessing + attribute selection + learning algorithms). Performance of the software development defect prediction models (using AUC, Area Under the Curve) was validated using NASA-MDP and PROMISE data sets. Twelve data sets from NASA-MDP (8) and PROMISE (4) projects were analyzed running a -fold cross-validation. We used a genetic algorithm to select the components of the learning schemes automatically, and to evaluate and report those with the best performance. In all, 864 learning schemes were studied. The most common learning schemes were: data preprocessors: Log and CoxBox + attribute selectors: Backward Elimination, BestFirst and LinearForwardSelection + learning algorithms: NaiveBayes, NaiveBayesSimple, SimpleLogistic, MultilayerPerceptron, Logistic, LogitBoost, BayesNet, and OneR. The genetic algorithm reported steady performance and runtime among data sets, according to statistical analysis.},
booktitle = {Advances in Artificial Intelligence - IBERAMIA 2016: 15th Ibero-American Conference on AI, San Jos\'{e}, Costa Rica, November 23-25, 2016, Proceedings},
pages = {222–234},
numpages = {13},
keywords = {Software quality, Fault prediction models, Genetic algorithms, Learning schemes, Learning algorithms, Machine learning},
location = {San Jos\'{e}, Costa Rica}
}

@article{10.1007/s11219-020-09511-4,
author = {Moreno, Valent\'{\i}n and G\'{e}nova, Gonzalo and Parra, Eugenio and Fraga, Anabel},
title = {Application of machine learning techniques to the flexible assessment and improvement of requirements quality},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09511-4},
doi = {10.1007/s11219-020-09511-4},
abstract = {It is already common to compute quantitative metrics of requirements to assess their quality. However, the risk is to build assessment methods and tools that are both arbitrary and rigid in the parameterization and combination of metrics. Specifically, we show that a linear combination of metrics is insufficient to adequately compute a global measure of quality. In this work, we propose to develop a flexible method to assess and improve the quality of requirements that can be adapted to different contexts, projects, organizations, and quality standards, with a high degree of automation. The domain experts contribute with an initial set of requirements that they have classified according to their quality, and we extract their quality metrics. We then use machine learning techniques to emulate the implicit expert’s quality function. We provide also a procedure to suggest improvements in bad requirements. We compare the obtained rule-based classifiers with different machine learning algorithms, obtaining measurements of effectiveness around 85%. We show as well the appearance of the generated rules and how to interpret them. The method is tailorable to different contexts, different styles to write requirements, and different demands in quality. The whole process of inferring and applying the quality rules adapted to each organization is highly automated.},
journal = {Software Quality Journal},
month = dec,
pages = {1645–1674},
numpages = {30},
keywords = {Flexible assessment, Experts’ judgment, Automatic improvement, Automatic classification, Machine learning, Requirements quality}
}

@inproceedings{10.1145/3193977.3193985,
author = {Hardin, Bonnie and Kanewala, Upulee},
title = {Using semi-supervised learning for predicting metamorphic relations},
year = {2018},
isbn = {9781450357296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3193977.3193985},
doi = {10.1145/3193977.3193985},
abstract = {Software testing is difficult to automate, especially in programs which have no oracle, or method of determining which output is correct. Metamorphic testing is a solution this problem. Metamorphic testing uses metamorphic relations to define test cases and expected outputs. A large amount of time is needed for a domain expert to determine which metamorphic relations can be used to test a given program. Metamorphic relation prediction removes this need for such an expert. We propose a method using semi-supervised machine learning to detect which metamorphic relations are applicable to a given code base. We compare this semi-supervised model with a supervised model, and show that the addition of unlabeled data improves the classification accuracy of the MR prediction model.},
booktitle = {Proceedings of the 3rd International Workshop on Metamorphic Testing},
pages = {14–17},
numpages = {4},
keywords = {semi-supervised learning, metamorphic testing, metamorphic relations, machine learning},
location = {Gothenburg, Sweden},
series = {MET '18}
}

@article{10.1007/s11219-018-9409-7,
author = {Minku, Leandro L. and Bener, Ay\c{s}e B. and Turhan, Burak},
title = {Guest editorial: special issue on predictive models for software quality},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9409-7},
doi = {10.1007/s11219-018-9409-7},
journal = {Software Quality Journal},
month = jun,
pages = {521–523},
numpages = {3}
}

@inproceedings{10.1145/3340482.3342744,
author = {Pecorelli, Fabiano and Di Nucci, Dario and De Roover, Coen and De Lucia, Andrea},
title = {On the role of data balancing for machine learning-based code smell detection},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342744},
doi = {10.1145/3340482.3342744},
abstract = {Code smells can compromise software quality in the long term by inducing technical debt. For this reason, many approaches aimed at identifying these design flaws have been proposed in the last decade. Most of them are based on heuristics in which a set of metrics (e.g., code metrics, process metrics) is used to detect smelly code components. However, these techniques suffer of subjective interpretation, low agreement between detectors, and threshold dependability. To overcome these limitations, previous work applied Machine Learning techniques that can learn from previous datasets without needing any threshold definition. However, more recent work has shown that Machine Learning is not always suitable for code smell detection due to the highly unbalanced nature of the problem. In this study we investigate several approaches able to mitigate data unbalancing issues to understand their impact on ML-based approaches for code smell detection. Our findings highlight a number of limitations and open issues with respect to the usage of data balancing in ML-based code smell detection.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {19–24},
numpages = {6},
keywords = {Machine Learning, Data Balancing, Code Smells},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@inproceedings{10.1145/3194104.3194110,
author = {Young, Steven and Abdou, Tamer and Bener, Ayse},
title = {A replication study: just-in-time defect prediction with ensemble learning},
year = {2018},
isbn = {9781450357234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194104.3194110},
doi = {10.1145/3194104.3194110},
abstract = {Just-in-time defect prediction, which is also known as change-level defect prediction, can be used to efficiently allocate resources and manage project schedules in the software testing and debugging process. Just-in-time defect prediction can reduce the amount of code to review and simplify the assignment of developers to bug fixes. This paper reports a replicated experiment and an extension comparing the prediction of defect-prone changes using traditional machine learning techniques and ensemble learning. Using datasets from six open source projects, namely Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL we replicate the original approach to verify the results of the original experiment and use them as a basis for comparison for alternatives in the approach. Our results from the replicated experiment are consistent with the original. The original approach uses a combination of data preprocessing and a two-layer ensemble of decision trees. The first layer uses bagging to form multiple random forests. The second layer stacks the forests together with equal weights. Generalizing the approach to allow the use of any arbitrary set of classifiers in the ensemble, optimizing the weights of the classifiers, and allowing additional layers, we apply a new deep ensemble approach, called deep super learner, to test the depth of the original study. The deep super learner achieves statistically significantly better results than the original approach on five of the six projects in predicting defects as measured by F1 score.},
booktitle = {Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {42–47},
numpages = {6},
keywords = {defect prediction, deep learning},
location = {Gothenburg, Sweden},
series = {RAISE '18}
}

@article{10.1016/j.infsof.2013.05.002,
author = {Rodriguez, Daniel and Ruiz, Roberto and Riquelme, Jose C. and Harrison, Rachel},
title = {A study of subgroup discovery approaches for defect prediction},
year = {2013},
issue_date = {October, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.002},
doi = {10.1016/j.infsof.2013.05.002},
abstract = {Context: Although many papers have been published on software defect prediction techniques, machine learning approaches have yet to be fully explored. Objective: In this paper we suggest using a descriptive approach for defect prediction rather than the precise classification techniques that are usually adopted. This allows us to characterise defective modules with simple rules that can easily be applied by practitioners and deliver a practical (or engineering) approach rather than a highly accurate result. Method: We describe two well-known subgroup discovery algorithms, the SD algorithm and the CN2-SD algorithm to obtain rules that identify defect prone modules. The empirical work is performed with publicly available datasets from the Promise repository and object-oriented metrics from an Eclipse repository related to defect prediction. Subgroup discovery algorithms mitigate against characteristics of datasets that hinder the applicability of classification algorithms and so remove the need for preprocessing techniques. Results: The results show that the generated rules can be used to guide testing effort in order to improve the quality of software development projects. Such rules can indicate metrics, their threshold values and relationships between metrics of defective modules. Conclusions: The induced rules are simple to use and easy to understand as they provide a description rather than a complete classification of the whole dataset. Thus this paper represents an engineering approach to defect prediction, i.e., an approach which is useful in practice, easily understandable and can be applied by practitioners.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1810–1822},
numpages = {13},
keywords = {Subgroup discovery, Rules, Imbalanced datasets, Defect prediction}
}

@article{10.1016/j.jss.2021.111031,
author = {Giray, G\"{o}rkem},
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111031},
doi = {10.1016/j.jss.2021.111031},
journal = {J. Syst. Softw.},
month = oct,
numpages = {35},
keywords = {Systematic literature review, Deep learning, Machine learning, Software process, Software development, Software engineering}
}

@inproceedings{10.5555/1671248.1671311,
author = {Tosun, Ayse and Bener, Ayse},
title = {Reducing false alarms in software defect prediction by decision threshold optimization},
year = {2009},
isbn = {9781424448425},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Software defect data has an imbalanced and highly skewed class distribution. The misclassification costs of two classes are not equal nor are known. It is critical to find the optimum bound, i.e. threshold, which would best separate defective and defect-free classes in software data. We have applied decision threshold optimization on Na\"{\i}ve Bayes classifier in order to find the optimum threshold for software defect data. ROC analyses show that decision threshold optimization significantly decreases false alarms (on the average by 11%) without changing probability of detection rates.},
booktitle = {Proceedings of the 2009 3rd International Symposium on Empirical Software Engineering and Measurement},
pages = {477–480},
numpages = {4},
series = {ESEM '09}
}

@inproceedings{10.1145/3428757.3429152,
author = {Philipp, Robert and Mladenow, Andreas and Strauss, Christine and V\"{o}lz, Alexander},
title = {Machine Learning as a Service: Challenges in Research and Applications},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429152},
doi = {10.1145/3428757.3429152},
abstract = {This study aims to evaluate the current state of research with regards to Machine Learning as a Service (MLaaS) and to identify challenges and research fields of this novel topic. First, a literature review on a basket of eight leading journals was performed. We motivate this study by identifying a lack of studies in the field of MLaaS. The structured literature review was further extended to established scientific databases relevant in this field. We found 30 contributions on MLaaS. As a result of the analysis we grouped them into four key concepts: Platform, Applications; Performance Enhancements and Challenges. Three of the derived concepts are discussed in detail to identify future research areas and to reveal challenges in research as well as in applications.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {396–406},
numpages = {11},
keywords = {Machine Learning as a Service, Machine Learning Platform, Machine Learning, MLaaS, Machine Learning Services},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@inproceedings{10.1145/2639490.2639511,
author = {Mockus, Audris},
title = {Defect prediction and software risk},
year = {2014},
isbn = {9781450328982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639490.2639511},
doi = {10.1145/2639490.2639511},
abstract = {Defect prediction has always fascinated researchers and practitioners. The promise of being able to predict the future and act to improve it is hard to resist. However, the operational data used in predictions are treacherous and the prediction is usually done outside the context of the actual development project, making it impossible to employ it for software quality measurement or improvement. Contextualizing, imputing missing observations, and correcting operational data related to defects is essential to gauge software quality. Such augmented data can then be used with domain- and project-specific considerations to assess risk posed by code, organization, or activities and to suggest risk-specific remediation activities.},
booktitle = {Proceedings of the 10th International Conference on Predictive Models in Software Engineering},
pages = {1},
numpages = {1},
location = {Turin, Italy},
series = {PROMISE '14}
}

@article{10.1007/s10664-015-9396-2,
author = {Zhang, Feng and Mockus, Audris and Keivanloo, Iman and Zou, Ying},
title = {Towards building a universal defect prediction model with rank transformed predictors},
year = {2016},
issue_date = {October   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9396-2},
doi = {10.1007/s10664-015-9396-2},
abstract = {Software defects can lead to undesired results. Correcting defects costs 50 % to 75 % of the total software development budgets. To predict defective files, a prediction model must be built with predictors (e.g., software metrics) obtained from either a project itself (within-project) or from other projects (cross-project). A universal defect prediction model that is built from a large set of diverse projects would relieve the need to build and tailor prediction models for an individual project. A formidable obstacle to build a universal model is the variations in the distribution of predictors among projects of diverse contexts (e.g., size and programming language). Hence, we propose to cluster projects based on the similarity of the distribution of predictors, and derive the rank transformations using quantiles of predictors for a cluster. We fit the universal model on the transformed data of 1,385 open source projects hosted on SourceForge and GoogleCode. The universal model obtains prediction performance comparable to the within-project models, yields similar results when applied on five external projects (one Apache and four Eclipse projects), and performs similarly among projects with different context factors. At last, we investigate what predictors should be included in the universal model. We expect that this work could form a basis for future work on building a universal model and would lead to software support tools that incorporate it into a regular development workflow.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {2107–2145},
numpages = {39},
keywords = {Universal defect prediction model, Software quality, Rank transformation, Large-scale, Defect prediction, Context factors}
}

@article{10.1016/j.infsof.2019.08.005,
author = {Bigonha, Mariza A.S. and Ferreira, Kecia and Souza, Priscila and Sousa, Bruno and Janu\'{a}rio, Marcela and Lima, Daniele},
title = {The usefulness of software metric thresholds for detection of bad smells and fault prediction},
year = {2019},
issue_date = {Nov 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {115},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.08.005},
doi = {10.1016/j.infsof.2019.08.005},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {79–92},
numpages = {14},
keywords = {Fault prediction, Bad smell, Detection strategies, Thresholds, Software quality, Software metrics}
}

@article{10.1016/j.eswa.2018.12.033,
author = {Turabieh, Hamza and Mafarja, Majdi and Li, Xiaodong},
title = {Iterated feature selection algorithms with layered recurrent neural network for software fault prediction},
year = {2019},
issue_date = {May 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2018.12.033},
doi = {10.1016/j.eswa.2018.12.033},
journal = {Expert Syst. Appl.},
month = may,
pages = {27–42},
numpages = {16},
keywords = {Layered recurrent neural network, Feature selection, Software fault prediction}
}

@article{10.1145/3442181,
author = {Sabir, Bushra and Ullah, Faheem and Babar, M. Ali and Gaire, Raj},
title = {Machine Learning for Detecting Data Exfiltration: A Review},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3442181},
doi = {10.1145/3442181},
abstract = {Context: Research at the intersection of cybersecurity, Machine Learning (ML), and Software Engineering (SE) has recently taken significant steps in proposing countermeasures for detecting sophisticated data exfiltration attacks. It is important to systematically review and synthesize the ML-based data exfiltration countermeasures for building a body of knowledge on this important topic. Objective: This article aims at systematically reviewing ML-based data exfiltration countermeasures to identify and classify ML approaches, feature engineering techniques, evaluation datasets, and performance metrics used for these countermeasures. This review also aims at identifying gaps in research on ML-based data exfiltration countermeasures. Method: We used Systematic Literature Review (SLR) method to select and review 92 papers. Results: The review has enabled us to: (a) classify the ML approaches used in the countermeasures into data-driven, and behavior-driven approaches; (b) categorize features into six types: behavioral, content-based, statistical, syntactical, spatial, and temporal; (c) classify the evaluation datasets into simulated, synthesized, and real datasets; and (d) identify 11 performance measures used by these studies. Conclusion: We conclude that: (i) The integration of data-driven and behavior-driven approaches should be explored; (ii) There is a need of developing high quality and large size evaluation datasets; (iii) Incremental ML model training should be incorporated in countermeasures; (iv) Resilience to adversarial learning should be considered and explored during the development of countermeasures to avoid poisoning attacks; and (v) The use of automated feature engineering should be encouraged for efficiently detecting data exfiltration attacks.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {50},
numpages = {47},
keywords = {machine learning, data leakage, data breach, advanced persistent threat, Data exfiltration}
}

@article{10.5555/3057337.3057441,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {A decision tree logic based recommendation system to select software fault prediction techniques},
year = {2017},
issue_date = {March     2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {99},
number = {3},
issn = {0010-485X},
abstract = {Identifying a reliable fault prediction technique is the key requirement for building effective fault prediction model. It has been found that the performance of fault prediction techniques is highly dependent on the characteristics of the fault dataset. To mitigate this issue, researchers have evaluated and compared a plethora of fault prediction techniques by varying the context in terms of domain information, characteristics of input data, complexity, etc. However, the lack of an accepted benchmark makes it difficult to select fault prediction technique for a particular context of prediction. In this paper, we present a recommendation system that facilitates the selection of appropriate technique(s) to build fault prediction model. First, we have reviewed the literature to elicit the various characteristics of the fault dataset and the appropriateness of the machine learning and statistical techniques for the identified characteristics. Subsequently, we have formalized our findings and built a recommendation system that helps in the selection of fault prediction techniques. We performed an initial appraisal of our presented system and found that proposed recommendation system provides useful hints in the selection of the fault prediction techniques.},
journal = {Computing},
month = mar,
pages = {255–285},
numpages = {31},
keywords = {verification, requirements, metrics, etc.), Software fault prediction techniques, Software fault prediction, Recommendation system, Decision tree, 68N30 Mathematical aspects of software engineering (specification}
}

@inproceedings{10.1109/QRS.2015.14,
author = {Yang, Xinli and Lo, David and Xia, Xin and Zhang, Yun and Sun, Jianling},
title = {Deep Learning for Just-in-Time Defect Prediction},
year = {2015},
isbn = {9781467379892},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/QRS.2015.14},
doi = {10.1109/QRS.2015.14},
abstract = {Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time. Nowadays, deep learning is a hot topic in the machine learning literature. Whether deep learning can be used to improve the performance of just-in-time defect prediction is still uninvestigated. In this paper, to bridge this research gap, we propose an approach Deeper which leverages deep learning techniques to predict defect-prone changes. We first build a set of expressive features from a set of initial change features by leveraging a deep belief network algorithm. Next, a machine learning classifier is built on the selected features. To evaluate the performance of our approach, we use datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. We compare our approach with the approach proposed by Kamei et al. The experimental results show that on average across the 6 projects, Deeper could discover 32.22% more bugs than Kamei et al's approach (51.04% versus 18.82% on average). In addition, Deeper can achieve F1-scores of 0.22-0.63, which are statistically significantly higher than those of Kamei et al.'s approach on 4 out of the 6 projects.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Software Quality, Reliability and Security},
pages = {17–26},
numpages = {10},
keywords = {Just-In-Time Defect Prediction, Deep Learning, Deep Belief Network, Cost Effectiveness},
series = {QRS '15}
}

@inproceedings{10.1145/3377816.3381734,
author = {Byun, Taejoon and Rayadurgam, Sanjai},
title = {Manifold for machine learning assurance},
year = {2020},
isbn = {9781450371261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377816.3381734},
doi = {10.1145/3377816.3381734},
abstract = {The increasing use of machine-learning (ML) enabled systems in critical tasks fuels the quest for novel verification and validation techniques yet grounded in accepted system assurance principles. In traditional system development, model-based techniques have been widely adopted, where the central premise is that abstract models of the required system provide a sound basis for judging its implementation. We posit an analogous approach for ML systems using an ML technique that extracts from the high-dimensional training data implicitly describing the required system, a low-dimensional underlying structure---a manifold. It is then harnessed for a range of quality assurance tasks such as test adequacy measurement, test input generation, and runtime monitoring of the target ML system. The approach is built on variational autoencoder, an unsupervised method for learning a pair of mutually near-inverse functions between a given high-dimensional dataset and a low-dimensional representation. Preliminary experiments establish that the proposed manifold-based approach, for test adequacy drives diversity in test data, for test generation yields fault-revealing yet realistic test cases, and for run-time monitoring provides an independent means to assess trustability of the target system's output.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {97–100},
numpages = {4},
keywords = {variational autoencoder, neural networks, machine learning testing},
location = {Seoul, South Korea},
series = {ICSE-NIER '20}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {clafer configurator, ClaferWiki, ClaferMOO visualizer, ClaferMOO, ClaferIG, Clafer},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@article{10.1016/j.cose.2021.102459,
author = {Zhao, Jinxiong and Guo, Sensen and Mu, Dejun},
title = {DouBiGRU-A: Software defect detection algorithm based on attention mechanism and double BiGRU},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {111},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2021.102459},
doi = {10.1016/j.cose.2021.102459},
journal = {Comput. Secur.},
month = dec,
numpages = {10},
keywords = {Flawfinder, RATS, Vulnerability identification, Software defect detection, DouBiGRU-A}
}

@inproceedings{10.1145/3368089.3417043,
author = {Ahmed, Md Sohel and Ishikawa, Fuyuki and Sugiyama, Mahito},
title = {Testing machine learning code using polyhedral region},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417043},
doi = {10.1145/3368089.3417043},
abstract = {To date, although machine learning has been successful in various practical applications, generic methods of testing machine learning code have not been established yet. Here we present a new approach to test machine learning code using the possible input region obtained as a polyhedron. If an ML system generates different output for multiple input in the polyhedron, it is ensured that there exists a bug in the code. This property is known as one of theoretical fundamentals in statistical inference, for example, sparse regression models such as the lasso, and a wide range of machine learning algorithms satisfy this polyhedral condition, to which our testing procedure can be applied. We empirically show that the existence of bugs in lasso code can be effectively detected by our method in the mutation testing framework.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1533–1536},
numpages = {4},
keywords = {Testing, Polyhedral region, Mutation Analysis, Machine learning code, Lasso},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3472163.3472172,
author = {Wiese, Lena and Wiese, Ingmar and Lietz, Kristina},
title = {Software Quality Assessment of a Web Application for Biomedical Data Analysis},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472172},
doi = {10.1145/3472163.3472172},
abstract = {Data Science as a multidisciplinary discipline has seen a massive transformation in the direction of operationalisation of analysis workflows. Yet it can be observed that such a workflow consists of potentially many diverse components: like modules in different programming languages, database backends, or web frontends. In order to achieve high efficiency and reproducibility of the analysis, a sufficiently high level of software engineering for the different components as well as an overall software architecture that integrates and automates the different components is needed. For the use case of gene expression analysis, from a software quality point of view we analyze a newly developed web application that allows user-friendly access to the underlying workflow.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {84–93},
numpages = {10},
keywords = {Web service, Software quality, Gene expression analysis, Data Science workflow},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.5555/3507788.3507798,
author = {Khan, Md Asif and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Garcon, Sylvain and Tauseef, Qasim},
title = {Failure prediction using machine learning in IBM WebSphere liberty continuous integration environment},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {The growing complexity and dependencies of software have increased the importance of testing to ensure that frequent changes do not adversely affect existing functionality. Moreover, continuous integration comes with unique challenges associated with maintaining a stable build environment. Several studies have shown that the testing environment becomes more efficient with proper test case prioritization techniques. However, an application's dynamic behavior makes it challenging to derive test case prioritization techniques for achieving optimal results. With the advance of machine learning, the context of an application execution can be analyzed to select and prioritize test suites more efficiently.Test suite prioritization techniques aim to reorder test suites' execution to deliver high quality, maintainable software at lower costs to meet specific objectives such as revealing failures earlier. The state-of-the-art techniques on test prioritization in a continuous integration environment focus on relatively small, single-language, unit-tested projects. This paper compares and analyzes Machine learning-based test suite prioritization technique on two large-scale dataset collected from a continuous integration environment Google and IBM respectively. We optimize hyperparameters and report on experiments' findings by using different machine learning algorithms for test suite prioritization. Our optimized algorithms prioritize test suites with 93% accuracy on average and require 20% fewer test suites to detect 80% of the failures than the test suites prioritized randomly.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {test prioritization, machine learning, continuous integration, CI},
location = {Toronto, Canada},
series = {CASCON '21}
}

@article{10.1016/j.cosrev.2020.100341,
author = {Kotsiopoulos, Thanasis and Sarigiannidis, Panagiotis and Ioannidis, Dimosthenis and Tzovaras, Dimitrios},
title = {Machine Learning and Deep Learning in smart manufacturing: The Smart Grid paradigm},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2020.100341},
doi = {10.1016/j.cosrev.2020.100341},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {25},
keywords = {Smart Grid, Industrial AI, Deep Learning, Machine Learning, Industry 4.0}
}

@article{10.1155/2020/6688075,
author = {Naseem, Rashid and Khan, Bilal and Ahmad, Arshad and Almogren, Ahmad and Jabeen, Saima and Hayat, Bashir and Shah, Muhammad Arif and Uddin, M. Irfan},
title = {Investigating Tree Family Machine Learning Techniques for a Predictive System to Unveil Software Defects},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1076-2787},
url = {https://doi.org/10.1155/2020/6688075},
doi = {10.1155/2020/6688075},
abstract = {Software defects prediction at the initial period of the software development life cycle remains a critical and important assignment. Defect prediction and correctness leads to the assurance of the quality of software systems and has remained integral to study in the previous years. The quick forecast of imperfect or defective modules in software development can serve the development squad to use the existing assets competently and effectively to provide remarkable software products in a given short timeline. Hitherto, several researchers have industrialized defect prediction models by utilizing statistical and machine learning techniques that are operative and effective approaches to pinpoint the defective modules. Tree family machine learning techniques are well-thought-out to be one of the finest and ordinarily used supervised learning methods. In this study, different tree family machine learning techniques are employed for software defect prediction using ten benchmark datasets. These techniques include Credal Decision Tree (CDT), Cost-Sensitive Decision Forest (CS-Forest), Decision Stump (DS), Forest by Penalizing Attributes (Forest-PA), Hoeffding Tree (HT), Decision Tree (J48), Logistic Model Tree (LMT), Random Forest (RF), Random Tree (RT), and REP-Tree (REP-T). Performance of each technique is evaluated using different measures, i.e., mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), root relative squared error (RRSE), specificity, precision, recall, F-measure (FM), G-measure (GM), Matthew’s correlation coefficient (MCC), and accuracy. The overall outcomes of this paper suggested RF technique by producing best results in terms of reducing error rates as well as increasing accuracy on five datasets, i.e., AR3, PC1, PC2, PC3, and PC4. The average accuracy achieved by RF is 90.2238%. The comprehensive outcomes of this study can be used as a reference point for other researchers. Any assertion concerning the enhancement in prediction through any new model, technique, or framework can be benchmarked and verified.},
journal = {Complex.},
month = jan,
numpages = {21}
}

@inproceedings{10.1145/3340482.3342741,
author = {Nair, Aravind and Meinke, Karl and Eldh, Sigrid},
title = {Leveraging mutants for automatic prediction of metamorphic relations using machine learning},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342741},
doi = {10.1145/3340482.3342741},
abstract = {An oracle is used in software testing to derive the verdict (pass/fail) for a test case. Lack of precise test oracles is one of the major problems in software testing which can hinder judgements about quality. Metamorphic testing is an emerging technique which solves both the oracle problem and the test case generation problem by testing special forms of software requirements known as metamorphic requirements. However, manually deriving the metamorphic requirements for a given program requires a high level of domain expertise, is labor intensive and error prone. As an alternative, we consider the problem of automatic detection of metamorphic requirements using machine learning (ML). For this problem we can apply graph kernels and support vector machines (SVM). A significant problem for any ML approach is to obtain a large labeled training set of data (in this case programs) that generalises well. The main contribution of this paper is a general method to generate large volumes of synthetic training data which can improve ML assisted detection of metamorphic requirements. For training data synthesis we adopt mutation testing techniques. This research is the first to explore the area of data augmentation techniques for ML-based analysis of software code. We also have the goal to enhance black-box testing using white-box methodologies. Our results show that the mutants incorporated into the source code corpus not only efficiently scale the dataset size, but they can also improve the accuracy of classification models.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {1–6},
numpages = {6},
keywords = {Test Case Generation, Source Code Analysis, Mutation Testing, Metamorphic Testing, Machine Learning, Fault Identification, Data augmentation},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@article{10.5555/3271870.3271878,
title = {Software fault prediction using firefly algorithm},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {6},
number = {3–4},
issn = {1758-8715},
abstract = {The software fault prediction SFP literature has shown an immense growth of the research studies involving the artificial neural network ANN based fault prediction models. However, the default gradient descent back propagation neural networks BPNNs have a high risk of getting stuck in the local minima of the search space. A class of nature inspired computing methods overcomes this disadvantage of BPNNs and has helped ANNs to evolve into a class of adaptive ANN. In this work, we propose a hybrid SFP model built using firefly algorithm FA and artificial neural network ANN, along with an empirical comparison with GA and PSO based evolutionary methods in optimising the connection weights of ANN. Seven different datasets were involved and MSE and the confusion matrix parameters were used for performance evaluation. The results have shown that FA-ANN model has performed better than the genetic and particle swarm optimised ANN fault prediction models.},
journal = {Int. J. Intell. Eng. Inform.},
month = jan,
pages = {356–377},
numpages = {22}
}

@inproceedings{10.1109/CEC.2017.7969629,
author = {Ibarguren, Igor and P\'{e}rez, Jes\'{u}s M. and Mugerza, Javier and Rodriguez, Daniel and Harrison, Rachel},
title = {The Consolidated Tree Construction algorithm in imbalanced defect prediction datasets},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC.2017.7969629},
doi = {10.1109/CEC.2017.7969629},
abstract = {In this short paper, we compare well-known rule/tree classifiers in software defect prediction with the CTC decision tree classifier designed to deal with class imbalanced. It is well-known that most software defect prediction datasets are highly imbalance (non-defective instances outnumber defective ones). In this work, we focused only on tree/rule classifiers as these are capable of explaining the decision, i.e., describing the metrics and thresholds that make a module error prone. Furthermore, rules/decision trees provide the advantage that they are easily understood and applied by project managers and quality assurance personnel. The CTC algorithm was designed to cope with class imbalance and noisy datasets instead of using preprocessing techniques (oversampling or undersampling), ensembles or cost weights of misclassification. The experimental work was carried out using the NASA datasets and results showed that induced CTC decision trees performed better or similar to the rest of the rule/tree classifiers.},
booktitle = {2017 IEEE Congress on Evolutionary Computation (CEC)},
pages = {2656–2660},
numpages = {5},
location = {Donostia, San Sebasti\'{a}n, Spain}
}

@article{10.1016/j.asoc.2019.02.008,
author = {Juneja, Kapil},
title = {A fuzzy-filtered neuro-fuzzy framework for software fault prediction for inter-version and inter-project evaluation},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {77},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.02.008},
doi = {10.1016/j.asoc.2019.02.008},
journal = {Appl. Soft Comput.},
month = apr,
pages = {696–713},
numpages = {18},
keywords = {Fuzzy, Classification, Intra project, Inter project, Defect Prediction}
}

@inproceedings{10.1109/ICMLA.2014.63,
author = {Coelho, Rodrigo A. and Guimar\~{a}es, Fabr\'{\i}cio dos R. N. and Esmin, Ahmed A. A.},
title = {Applying Swarm Ensemble Clustering Technique for Fault Prediction Using Software Metrics},
year = {2014},
isbn = {9781479974153},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2014.63},
doi = {10.1109/ICMLA.2014.63},
abstract = {Number of defects remaining in a system provides an insight into the quality of the system. Defect detection systems predict defects by using software metrics and data mining techniques. Clustering analysis is adopted to build the software defect prediction models. Cluster ensembles have emerged as a prominent method for improving robustness, stability and accuracy of clustering solutions. The clustering ensembles combine multiple partitions generated by different clustering algorithms into a single clustering solution. In this paper, the clustering ensemble using Particle Swarm Optimization algorithm (PSO) solution is proposed to improve the prediction quality. An empirical study shows that the PSO can be a good choice to build defect prediction software models.},
booktitle = {Proceedings of the 2014 13th International Conference on Machine Learning and Applications},
pages = {356–361},
numpages = {6},
keywords = {Software defect prediction, Particle swarm optimization, Ensemble clustering, Cluster data},
series = {ICMLA '14}
}

@inproceedings{10.1109/RAMS.2019.8768923,
author = {Cui, Can and Liu, Bin and Li, Guoqi},
title = {A Novel Feature Selection Method for Software Fault Prediction Model},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/RAMS.2019.8768923},
doi = {10.1109/RAMS.2019.8768923},
abstract = {Software fault prediction (SFP) is an active issue in software engineering (SE). At present, machine learning (ML) has been successfully applied to SFP classification problems. However, one of the challenges for building software fault prediction models (SFPM) is processing high dimensional datasets, which include many irrelevant and redundant features. To address this issue, feature selection techniques, mainly contain wrapper methods and filter methods, are used. In the paper, we report an empirical study aimed at providing a novel approach to select feature for SFP.},
booktitle = {2019 Annual Reliability and Maintainability Symposium (RAMS)},
pages = {1–6},
numpages = {6},
location = {Orlando, FL, USA}
}

@article{10.1504/ijics.2020.105155,
author = {Pinto, Joey and Jain, Pooja and Kumar, Tapan},
title = {Fault prediction for distributed computing Hadoop clusters using real-time higher order differential inputs to SVM: Zedacross},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {12},
number = {2–3},
issn = {1744-1765},
url = {https://doi.org/10.1504/ijics.2020.105155},
doi = {10.1504/ijics.2020.105155},
abstract = {Hadoop distributed computing clusters are used worldwide for high-performance computations. Often various hardware and software faults occur, leading to both data and computation time losses. This paper proposes the usage of a fault prediction software called 'Zedacross' which uses machine learning principles combined with cluster monitoring tools. Firstly, the paper suggests a model that uses the resource usage statistics of a normally functioning Hadoop cluster to create a machine learning model that can then be used to predict and detect faults in real time. Secondly, the paper explains the novel idea of using higher order differentials as inputs to SVM for highly accurate fault predictions. Predictions of system faults by observing system resource usage statistics in real-time with minimum delay will play a vital role in deciding the need for job rescheduling tasks or even dynamic up-scaling of the cluster. To demonstrate the effectiveness of the design a Java utility was built to perform cluster fault monitoring. The results obtained after running the system on various test cases demonstrate that the proposed method is accurate and effective.},
journal = {Int. J. Inf. Comput. Secur.},
month = jan,
pages = {181–198},
numpages = {17},
keywords = {SVM, higher order differential, Hadoop, Ganglia, fault prediction}
}

@phdthesis{10.5555/AAI28773561,
author = {Pan, Renjian and Krishnendu, Chakrabarty, and Yiran, Chen, and Hai, Li, and Qiang, Qiu,},
advisor = {Xin, Li,},
title = {Applying Machine Learning to Testing and Diagnosis of Integrated Systems},
year = {2021},
isbn = {9798780614135},
publisher = {Duke University},
address = {USA},
abstract = {The growing complexity of integrated boards and systems makes manufacturing test and diagnosis increasingly expensive. There is a pressing need to reduce test cost and to pinpoint the root causes of integrated systems in a more effective way. In light of machine learning, a number of intelligent test-cost reduction and root-cause analysis methods have been proposed. However, it remains extremely challenging to (i) reduce test cost for black-box testing for integrated systems, and (ii) pinpoint the root causes for integrated systems with little need on labeled test data from repair history. To tackle these challenges, we propose multiple machine-learning-based solutions for black-box test-cost reduction and unsupervised/semi-supervised root-cause analysis in this dissertation.For black-box test-cost reduction, we propose a novel test selection method based on a Bayesian network model. First, it is formulated as a constrained optimization problem. Next, a score-based algorithm is implemented to construct the Bayesian network for black-box tests. Finally, we propose a Bayesian index with the property of Markov blankets, and then an iterative test selection method is developed based on our proposed Bayesian index.For root-cause analysis, we first propose an unsupervised root-cause analysis method in which no repair history is needed. In the first stage, a decision-tree model is trained with system test information to cluster the data in a coarse-grained manner. In the second stage, frequent-pattern mining is applied to extract frequent patterns in each decision-tree node to precisely cluster the data so that each cluster represents only a small number of root causes. The proposed method can accommodate both numerical and categorical test items. A combination of the L-method, cross validation and Silhouette score enables us to automatically determine all hyper-parameters. Two industry case studies with system test data demonstrate that the proposed approach significantly outperforms the state-of-the-art unsupervised root-cause-analysis method.Utilizing transfer learning, we further improve the performance of unsupervised root-cause-analysis. A two-stage clustering method is first developed by exploiting model selection based on the concept of Silhouette score. Next, a data-selection method based on ensemble learning is proposed to transfer valuable information from a source product to improve the diagnosis accuracy on the target product with insufficient data. Two case studies based on industry designs demonstrate that the proposed approach significantly outperforms other state-of-the-art unsupervised root-cause-analysis methods.In addition, we propose a semi-supervised root-cause-analysis method with co-training, where only a small set of labeled data is required. Using random forest as the learning kernel, a co-training technique is proposed to leverage the unlabeled data by automatically pre-labeling a subset of them and retraining each decision tree. In addition, several novel techniques have been proposed to avoid over-fitting and determine hyper-parameters. Two case studies based on industrial designs demonstrate that the proposed approach significantly outperforms the state-of-the-art methods.In summary, this dissertation addresses the most difficult problems in testing and diagnosis of integrated systems with machine learning. A test selection method based on Bayesian networks reduces the test cost for black-box testing. With unsupervised learning, semi-supervised learning and transfer learning, we analysis root causes for integrated systems without much need on historical diagnosis information. The proposed approaches are expected to contribute to the semiconductor industry by effectively reducing the black-box test cost and efficiently diagnosing the integrated systems.},
note = {AAI28773561}
}

@article{10.1145/3183339,
author = {Zhou, Yuming and Yang, Yibiao and Lu, Hongmin and Chen, Lin and Li, Yanhui and Zhao, Yangyang and Qian, Junyan and Xu, Baowen},
title = {How Far We Have Progressed in the Journey? An Examination of Cross-Project Defect Prediction},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3183339},
doi = {10.1145/3183339},
abstract = {Background. Recent years have seen an increasing interest in cross-project defect prediction (CPDP), which aims to apply defect prediction models built on source projects to a target project. Currently, a variety of (complex) CPDP models have been proposed with a promising prediction performance.Problem. Most, if not all, of the existing CPDP models are not compared against those simple module size models that are easy to implement and have shown a good performance in defect prediction in the literature.Objective. We aim to investigate how far we have really progressed in the journey by comparing the performance in defect prediction between the existing CPDP models and simple module size models.Method. We first use module size in the target project to build two simple defect prediction models, ManualDown and ManualUp, which do not require any training data from source projects. ManualDown considers a larger module as more defect-prone, while ManualUp considers a smaller module as more defect-prone. Then, we take the following measures to ensure a fair comparison on the performance in defect prediction between the existing CPDP models and the simple module size models: using the same publicly available data sets, using the same performance indicators, and using the prediction performance reported in the original cross-project defect prediction studies.Result. The simple module size models have a prediction performance comparable or even superior to most of the existing CPDP models in the literature, including many newly proposed models.Conclusion. The results caution us that, if the prediction performance is the goal, the real progress in CPDP is not being achieved as it might have been envisaged. We hence recommend that future studies should include ManualDown/ManualUp as the baseline models for comparison when developing new CPDP models to predict defects in a complete target project.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {1},
numpages = {51},
keywords = {unsupervised, supervised, model, cross-project, Defect prediction}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {software product line, quality evaluation, machine learning, feature model},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3387906.3388618,
author = {Cruz, Daniel and Santana, Amanda and Figueiredo, Eduardo},
title = {Detecting bad smells with machine learning algorithms: an empirical study},
year = {2020},
isbn = {9781450379601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387906.3388618},
doi = {10.1145/3387906.3388618},
abstract = {Bad smells are symptoms of bad design choices implemented on the source code. They are one of the key indicators of technical debts, specifically, design debt. To manage this kind of debt, it is important to be aware of bad smells and refactor them whenever possible. Therefore, several bad smell detection tools and techniques have been proposed over the years. These tools and techniques present different strategies to perform detections. More recently, machine learning algorithms have also been proposed to support bad smell detection. However, we lack empirical evidence on the accuracy and efficiency of these machine learning based techniques. In this paper, we present an evaluation of seven different machine learning algorithms on the task of detecting four types of bad smells. We also provide an analysis of the impact of software metrics for bad smell detection using a unified approach for interpreting the models' decisions. We found that with the right optimization, machine learning algorithms can achieve good performance (F1 score) for two bad smells: God Class (0.86) and Refused Parent Bequest (0.67). We also uncovered which metrics play fundamental roles for detecting each bad smell.},
booktitle = {Proceedings of the 3rd International Conference on Technical Debt},
pages = {31–40},
numpages = {10},
keywords = {software quality, software measurement, machine learning, empirical software engineering, bad smells detection},
location = {Seoul, Republic of Korea},
series = {TechDebt '20}
}

@article{10.1007/s10766-021-00707-0,
author = {\"{O}z, I\c{s}\i{}l and Arslan, Sanem},
title = {Predicting the Soft Error Vulnerability of Parallel Applications Using Machine Learning},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {3},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-021-00707-0},
doi = {10.1007/s10766-021-00707-0},
abstract = {With the widespread use of the multicore systems having smaller transistor sizes, soft errors become an important issue for parallel program execution. Fault injection is a prevalent method to quantify the soft error rates of the applications. However, it is very time consuming to perform detailed fault injection experiments. Therefore, prediction-based techniques have been proposed to evaluate the soft error vulnerability in a faster way. In this work, we present a soft error vulnerability prediction approach for parallel applications using machine learning algorithms. We define a set of features including thread communication, data sharing, parallel programming, and performance characteristics; and train our models based on three ML algorithms. This study uses the parallel programming features, as well as the combination of all features for the first time in vulnerability prediction of parallel programs. We propose two models for the soft error vulnerability prediction: (1) A regression model with rigorous feature selection analysis that estimates correct execution rates, (2) A novel classification model that predicts the vulnerability level of the target programs. We get maximum prediction accuracy rate of 73.2% for the regression-based model, and achieve 89% F-score for our classification model.},
journal = {Int. J. Parallel Program.},
month = jun,
pages = {410–439},
numpages = {30},
keywords = {Machine Learning, Parallel programming, Fault injection, Soft error analysis}
}

@article{10.1016/j.future.2019.09.009,
author = {Lopes, F\'{a}bio and Agnelo, Jo\~{a}o and Teixeira, C\'{e}sar A. and Laranjeiro, Nuno and Bernardino, Jorge},
title = {Automating orthogonal defect classification using machine learning algorithms},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {102},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.09.009},
doi = {10.1016/j.future.2019.09.009},
journal = {Future Gener. Comput. Syst.},
month = jan,
pages = {932–947},
numpages = {16},
keywords = {Text classification, Machine learning, Orthogonal defect classification, Bug reports, Software defects}
}

@article{10.1504/ijbis.2020.108650,
author = {Sarker, Kamal Uddin and Deraman, Aziz Bin and Hasan, Raza},
title = {Ontological practice for software quality control},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {34},
number = {3},
issn = {1746-0972},
url = {https://doi.org/10.1504/ijbis.2020.108650},
doi = {10.1504/ijbis.2020.108650},
abstract = {Recent statistical report on software project has been reflected the significant software project failure rate is existed till now. Failure can be partial, complete or compromising in functional or non-functional factors. The EBSPM public software project data repository has been analysed to reflect the relationship among cost, time, effective hours and functional points to identify eternal influencing factors. The literature review has identified the root causes of failure. Software quality control is a formal approach to make the project successful but until now ambiguous with the integration of quality factors. Our proposed ontology reduces the complexity of project management. This research has established the way of developing an ontology by including maximum quality factors. The way of collaborative working is included in the ontology to ensure formal, transparent, accountable and responsible manner. The ontology model is developed with descriptive logic and diagrams. Theoretical evaluation has been done at the end of the research based on the criteria of standardisation organisations.},
journal = {Int. J. Bus. Inf. Syst.},
month = jan,
pages = {355–372},
numpages = {17},
keywords = {descriptive logic, software quality control, ontology, project failure reasons, understanding project failure, software project failure rate}
}

@inproceedings{10.1007/978-3-030-58811-3_73,
author = {Siavvas, Miltiadis and Tsoukalas, Dimitrios and Marantos, Charalampos and Tsintzira, Angeliki-Agathi and Jankovic, Marija and Soudris, Dimitrios and Chatzigeorgiou, Alexander and Kehagias, Dionysios},
title = {The SDK4ED Platform for Embedded Software Quality Improvement - Preliminary Overview},
year = {2020},
isbn = {978-3-030-58810-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58811-3_73},
doi = {10.1007/978-3-030-58811-3_73},
abstract = {Maintaining high level of quality with respect to important quality attributes is critical for the success of modern software applications. Hence, appropriate tooling is required to help developers and project managers monitor and optimize software quality throughout the overall Software Development Lifecycle (SDLC). Moreover, embedded software engineers and developers need support to manage complex interdependencies and inherent trade-offs between design and run-time qualities. To this end, in an attempt to address these issues, we are developing the SDK4ED Platform as part of the ongoing EU-funded SDK4ED project, a software quality system that enables the monitoring and optimization of software quality, with emphasis on embedded software. The purpose of this technical paper is to provide an overview of the SDK4ED Platform and present the main novel functionalities that have been implemented within the platform until today.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part IV},
pages = {1035–1050},
numpages = {16},
keywords = {Verification and validation, Software quality},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3341105.3374008,
author = {Zakurdaeva, Alla and Weiss, Michael and Muegge, Steven},
title = {Detecting architectural integrity violation patterns using machine learning},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374008},
doi = {10.1145/3341105.3374008},
abstract = {Recent1 years have seen a surge of research into new ways of analyzing software quality. Specifically, a set of studies has been devoted to the impact the architectural relations among files have on system maintainability and file bug-proneness. The literature has proposed a set of rules for determining recurring architectural design flaws that occur in most complex systems, are associated with bugs, and thus incur high maintenance costs. In the present paper we advocate for using machine learning as the means of refining the approach and revealing new patterns of architectural integrity violations. Having trained a machine learning model on the combination of structural and historical information acquired from the Tiki open source project, we have been able to replicate three of the six known types of architectural violations and discover one new type, the Reverse Unstable Interface pattern. The implication of our study is that machine learning can provide valuable insights into the problem and discover novel patterns which would help software analysts to pinpoint specific architectural problems that may be the root causes of elevated bug- and change-proneness.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1480–1487},
numpages = {8},
keywords = {software architecture, machine learning, hotspot patterns, bug-proneness, architectural flaws},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3021460.3021491,
author = {Malhotra, Ruchika},
title = {Software Quality Predictive Modeling: An Effective Assessment of Experimental Data},
year = {2017},
isbn = {9781450348560},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3021460.3021491},
doi = {10.1145/3021460.3021491},
abstract = {A major problem faced by software project managers is to develop good quality software products within tight schedules and budget constraints [1]. Predictive modeling, in the context of software engineering relates to construction of models for estimation of software quality attributes such as defect-proneness, maintainability and effort amongst others. For developing such models, software metrics act as predictor variables as they signify various design characteristics of a software such as coupling, cohesion, inheritance and polymorphism. A number of techniques such as statistical and machine learning are available for developing predictive models.However, conducting effective empirical studies, which develop successful predictive models, is not possible if proper research methodology and steps are not followed. This work introduces a successful stepwise procedure for efficient application of various techniques to predictive modeling. A number of research issues which are important to be addressed while conducting empirical studies such as data collection, validation method, use of statistical tests, use of an effective performance evaluator etc. are also discussed with the help of an example.The tutorial presents an overview of the research process and methodology followed in an empirical research [2]. All steps that are needed to perform an effective empirical study are described. The tutorial would demonstrate the research methodology with the help of an example based on a data set for defect prediction.In this work we focus on various research issues that are stated below:RQ1: Which repositories are available for extracting software engineering data?RQ2: What type of data pre-processing and feature selection techniques should be used before developing predictive models?RQ3: Which possible tools are freely available for mining and analysis of data for developing software quality predictive models?RQ4: Which techniques are available for developing software quality predictive models?RQ5: Which metrics should be used for performance evaluation for models developed for software?RQ6: Which statistical tests can be effectively used for hypothesis testing using search-based techniques?RQ7: How can we effectively use search-based techniques for predictive modeling?RQ8: What are possible fitness functions while using search-based techniques for predictive modeling?RQ9: How would researchers account for the stochastic nature of search-based techniques?The reasons for relevance of this study are manifold. Empirical validation of OO metrics is a critical research area in the present day scenario, with a large number of academicians and research practitioners working towards this direction to predict software quality attributes in the early phases of software development. Thus, we explore the various steps involved in development of an effective software quality predictive model using a modeling technique with an example data set. Performing successful empirical studies in software engineering is important for the following reasons:• To identify defective classes at the initial phases of software development so that more resources can be allocated to these classes to remove errors.• To analyze the metrics which are important for predicting software quality attributes and to use them as quality benchmarks so that the software process can be standardized and delivers effective products.• To efficiently plan testing, walkthroughs, reviews and inspection activities so that limited resources can be properly planned to provide good quality software.• To use and adapt different techniques (statistical, machine learning &amp; search-based) in predicting software quality attributes.• To analyze existing trends for software quality predictive modeling and suggest future directions for researchers.• To document the research methodology so that effective replicated studies can be performed with ease.},
booktitle = {Proceedings of the 10th Innovations in Software Engineering Conference},
pages = {215–216},
numpages = {2},
keywords = {Software quality predictive modeling, Search-based techniques, Object-oriented metrics, Empirical Validation},
location = {Jaipur, India},
series = {ISEC '17}
}

@inproceedings{10.1145/3340482.3342746,
author = {Viuginov, Nickolay and Filchenkov, Andrey},
title = {A machine learning based automatic folding of dynamically typed languages},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342746},
doi = {10.1145/3340482.3342746},
abstract = {The popularity of dynamically typed languages has been growing strongly lately. Elegant syntax of such languages like javascript, python, PHP and ruby pays back when it comes to finding bugs in large codebases. The analysis is hindered by specific capabilities of dynamically typed languages, such as defining methods dynamically and evaluating string expressions. For finding bugs or investigating unfamiliar classes and libraries in modern IDEs and text editors features for folding unimportant code blocks are implemented. In this work, data on user foldings from real projects were collected and two classifiers were trained on their basis. The input to the classifier is a set of parameters describing the structure and syntax of the code block. These classifiers were subsequently used to identify unimportant code fragments. The implemented approach was tested on JavaScript and Python programs and compared with the best existing algorithm for automatic code folding.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {31–36},
numpages = {6},
keywords = {Source code analysis, Python, JavaScript, Dynamically typed languages, Automatic Folding, Abstract Syntax tree},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@article{10.1002/smr.2238,
author = {Naeem, Muhammad Rashid and Lin, Tao and Naeem, Hamad and Liu, Hailu},
title = {A machine learning approach for classification of equivalent mutants},
year = {2020},
issue_date = {May 2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {32},
number = {5},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2238},
doi = {10.1002/smr.2238},
abstract = {Mutation testing is a fault‐based technique to test the quality of test suites by inducing artificial syntactic faults or mutants in a source program. However, some mutants have the same semantics as original program and cannot be detected by any test suite input known as equivalent mutants. Equivalent mutant problem (EMP) is undecidable as it requires manual human effort to identify a mutant as equivalent or killable. The constraint‐based testing (CBT) theory suggests the use of mathematical constraints which can help reveal some equivalent mutants using mutant features. In this paper, we consider three metrics of CBT theory, ie, reachability, necessity, and sufficiency to extract feature constraints from mutant programs. Constraints are extracted using program dependency graphs. Other features such as degree of significance, semantic distance, and information entropy of mutants are also extracted to build a binary classification model. Machine learning algorithms such as Random Forest, GBT, and SVM are applied under two application scenarios (split‐project and cross‐project) on ten Java programs to predict equivalent mutants. The analysis of the study demonstrates that that the proposed techniques not only improves the efficiency of the equivalent mutant detection but also reduces the effort required to perform it with small accuracy loss.},
journal = {J. Softw. Evol. Process},
month = apr,
numpages = {32},
keywords = {static analysis, program semantics, mutation testing, machine learning, equivalent mutants}
}

@article{10.1016/j.jss.2016.05.015,
author = {Chen, Tse-Hsun and Shang, Weiyi and Nagappan, Meiyappan and Hassan, Ahmed E. and Thomas, Stephen W.},
title = {Topic-based software defect explanation},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.05.015},
doi = {10.1016/j.jss.2016.05.015},
abstract = {Some topics are more defect-prone than others.Defect-prone topics are likely to remain so over time.Our topic-based metrics provide additional defect explanatory to baseline metrics.Our metrics outperform state-of-the-art topic-based cohesion and coupling metrics. Researchers continue to propose metrics using measurable aspects of software systems to understand software quality. However, these metrics largely ignore the functionality, i.e., the conceptual concerns, of software systems. Such concerns are the technical concepts that reflect the systems business logic. For instance, while lines of code may be a good general measure for defects, a large file responsible for simple I/O tasks is likely to have fewer defects than a small file responsible for complicated compiler implementation details. In this paper, we study the effect of concerns on software quality. We use a statistical topic modeling approach to approximate software concerns as topics (related words in source code). We propose various metrics using these topics to help explain the file defect-proneness. Case studies on multiple versions of Firefox, Eclipse, Mylyn, and NetBeans show that (i) some topics are more defect-prone than others; (ii) defect-prone topics tend to remain so over time; (iii) our topic-based metrics provide additional explanatory power for software quality over existing structural and historical metrics; and (iv) our topic-based cohesion metric outperforms state-of-the-art topic-based cohesion and coupling metrics in terms of defect explanatory power, while being simpler to implement and more intuitive to interpret.},
journal = {J. Syst. Softw.},
month = jul,
pages = {79–106},
numpages = {28},
keywords = {Topic modeling, Metrics, LDA, Coupling, Cohesion, Code quality}
}

@inproceedings{10.1145/3195546.3206423,
author = {Hamou-Lhadj, Wahab and Nayrolles, Mathieu},
title = {A project on software defect prevention at commit-time: a success story of university-industry research collaboration},
year = {2018},
isbn = {9781450357449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195546.3206423},
doi = {10.1145/3195546.3206423},
abstract = {In this talk, we describe a research collaboration project between Concordia University and Ubisoft. The project consists of investigating techniques for defect prevention at commit-time for increased software quality. The outcome of this project is a tool called CLEVER (Combining Levels of Bug Prevention and Resolution techniques) that uses machine learning to automatically detect coding defects as programmers write code. The main novelty of CLEVER is that it relies on code matching techniques to detect coding mistakes based on a database of historical code defects found in multiple related projects. The tool also proposes fixes based on known patterns.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering Research and Industrial Practice},
pages = {24–25},
numpages = {2},
keywords = {university-industry research project, software maintenance and evolution, machine learning, bug prevention at commit-time},
location = {Gothenburg, Sweden},
series = {SER&amp;IP '18}
}

@inproceedings{10.1145/3368089.3418538,
author = {\v{C}egi\v{n}, J\'{a}n},
title = {Machine learning based test data generation for safety-critical software},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418538},
doi = {10.1145/3368089.3418538},
abstract = {Unit testing focused on Modified Condition/Decision Coverage (MC/DC) criterion is essential in development safety-critical systems. However, design of test data that meets the MC/DC criterion currently needs detailed manual analysis of branching conditions in units under test by test engineers. Multiple state-of-art approaches exist with proven usage even in industrial projects. However, these approaches have multiple shortcomings, one of them being the Path explosion problem which has not been fully solved yet. Machine learning methods as meta-heuristic approximations can model behaviour of programs that are hard to test using traditional approaches, where the Path explosion problem does occur and thus could solve the limitations of the current state-of-art approaches. I believe, motivated by an ongoing collaboration with an industrial partner, that the machine learning methods could be combined with existing approaches to produce an approach suitable for testing of safety-critical projects.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1678–1681},
numpages = {4},
keywords = {unit testing, test data generation, machine learning, MC/DC criterion},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.5555/3432601.3432605,
author = {Krishnakumar, Sanjena and Abdou, Tamer},
title = {Towards interpretable and maintainable supervised learning using shapley values in arrhythmia},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {This paper investigates the application of a model-agnostic interpretability technique, Shapley Additive Explanations (SHAP), to understand and hence, enhance machine learning classification models using Shapley values in the prediction of arrhythmias1. Using the Arrhythmia dataset2, three different feature selection techniques, Information Gain (IG), Recursive Feature Elimination-Random Forest (RFE-RF), and AutoSpearman, were used to select features for machine learning models to predict the arrhythmia class. Four multi-class classification models, Na\"{\i}ve Bayes (NB), k-Nearest Neighbours (kNN), Random Forest (RF), and stacking heterogeneous ensemble (Ensemble) were built, evaluated, and compared. SHAP interpretation method was applied to find reliable explanations for the predictions of the classification models. Additionally, SHAP values were used to find `bellwether' instances to enhance the training of our models in order to improve their performances in the prediction of arrhythmia. The most stable and top-performing classification model was RF, followed by Ensemble in comparison to NB and kNN. SHAP provided robust and reliable explanations for the classification models. Furthermore, improving the training of our models with `bellwether' instances, found using SHAP values, enhanced the overall model performances in terms of accuracy, AUC, and F1 score. In conclusion, we recommend using SHAP value explanations as a robust and reliable method for local model-agnostic interpretability and to enhance machine learning models for arrhythmia prediction.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {23–32},
numpages = {10},
keywords = {shapley value, multi-class classification, machine learning, local model-agnostic interpretation, healthcare, bellwether, arrhythmia, SHAP, LIME},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@article{10.1002/stvr.1570,
author = {Canfora, Gerardo and Lucia, Andrea De and Penta, Massimiliano Di and Oliveto, Rocco and Panichella, Annibale and Panichella, Sebastiano},
title = {Defect prediction as a multiobjective optimization problem},
year = {2015},
issue_date = {June 2015},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {25},
number = {4},
issn = {0960-0833},
url = {https://doi.org/10.1002/stvr.1570},
doi = {10.1002/stvr.1570},
abstract = {In this paper, we formalize the defect-prediction problem as a multiobjective optimization problem. Specifically, we propose an approach, coined as multiobjective defect predictor MODEP, based on multiobjective forms of machine learning techniques-logistic regression and decision trees specifically-trained using a genetic algorithm. The multiobjective approach allows software engineers to choose predictors achieving a specific compromise between the number of likely defect-prone classes or the number of defects that the analysis would likely discover effectiveness, and lines of code to be analysed/tested which can be considered as a proxy of the cost of code inspection. Results of an empirical evaluation on 10 datasets from the PROMISE repository indicate the quantitative superiority of MODEP with respect to single-objective predictors, and with respect to trivial baseline ranking classes by size in ascending or descending order. Also, MODEP outperforms an alternative approach for cross-project prediction, based on local prediction upon clusters of similar classes. Copyright © 2015John Wiley &amp; Sons, Ltd.},
journal = {Softw. Test. Verif. Reliab.},
month = jun,
pages = {426–459},
numpages = {34},
keywords = {multiobjective optimization, defect prediction, cross-project defect prediction, cost-effectiveness}
}

@inproceedings{10.1145/3358331.3358376,
author = {Easttom, Chuck},
title = {A Methodological Approach to Weaponizing Machine Learning},
year = {2019},
isbn = {9781450372022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358331.3358376},
doi = {10.1145/3358331.3358376},
abstract = {The current literature is replete with studies involving the use of machine learning algorithms for defensive security implementations. For example, machine learning has been utilized to enhance antivirus software and intrusion detection systems. The use of machine learning in defensive cybersecurity operations is well documented. However, there is a substantial gap in the literature on the offensive use of machine learning. Particularly, use of machine learning algorithms to enhance cyber warfare operations. Cyber components to modern conflicts, whether those conflicts are cyber or kinetic warfare, are a fact of the modern international political landscape. It is a natural progression to explore applications of machine learning to cyber warfare, particularly weaponized malware.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing},
articleno = {45},
numpages = {5},
keywords = {weaponized malware, machine learning, cyber warfare, Weaponized malware},
location = {Dublin, Ireland},
series = {AIAM 2019}
}

@inproceedings{10.1145/3387940.3391478,
author = {Sildatke, Michael and Karwanni, Hendrik and Kraft, Bodo and Schmidts, Oliver and Z\"{u}ndorf, Albert},
title = {Automated Software Quality Monitoring in Research Collaboration Projects},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391478},
doi = {10.1145/3387940.3391478},
abstract = {In collaborative research projects, both researchers and practitioners work together solving business-critical challenges. These projects often deal with ETL processes, in which humans extract information from non-machine-readable documents by hand. AI-based machine learning models can help to solve this problem.Since machine learning approaches are not deterministic, their quality of output may decrease over time. This fact leads to an overall quality loss of the application which embeds machine learning models. Hence, the software qualities in development and production may differ.Machine learning models are black boxes. That makes practitioners skeptical and increases the inhibition threshold for early productive use of research prototypes. Continuous monitoring of software quality in production offers an early response capability on quality loss and encourages the use of machine learning approaches. Furthermore, experts have to ensure that they integrate possible new inputs into the model training as quickly as possible.In this paper, we introduce an architecture pattern with a reference implementation that extends the concept of Metrics Driven Research Collaboration with an automated software quality monitoring in productive use and a possibility to auto-generate new test data coming from processed documents in production.Through automated monitoring of the software quality and auto-generated test data, this approach ensures that the software quality meets and keeps requested thresholds in productive use, even during further continuous deployment and changing input data.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {603–610},
numpages = {8},
keywords = {Software Architecture, Research Collaboration Management, Research Best Practices, Metrics, Lean Software Development},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3460319.3464844,
author = {Dutta, Saikat and Selvam, Jeeva and Jain, Aryaman and Misailovic, Sasa},
title = {TERA: optimizing stochastic regression tests in machine learning projects},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464844},
doi = {10.1145/3460319.3464844},
abstract = {The stochastic nature of many Machine Learning (ML) algorithms makes testing of ML tools and libraries challenging. ML algorithms allow a developer to control their accuracy and run-time through a set of hyper-parameters, which are typically manually selected in tests. This choice is often too conservative and leads to slow test executions, thereby increasing the cost of regression testing.  We propose TERA, the first automated technique for reducing the cost of regression testing in Machine Learning tools and libraries(jointly referred to as projects) without making the tests more flaky. TERA solves the problem of exploring the trade-off space between execution time of the test and its flakiness as an instance of Stochastic Optimization over the space of algorithm hyper-parameters. TERA presents how to leverage statistical convergence-testing techniques to estimate the level of flakiness of the test for a specific choice of hyper-parameters during optimization.  We evaluate TERA on a corpus of 160 tests selected from 15 popular machine learning projects. Overall, TERA obtains a geo-mean speedup of 2.23x over the original tests, for the minimum passing probability threshold of 99%. We also show that the new tests did not reduce fault detection ability through a mutation study and a study on a set of 12 historical build failures in studied projects.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {413–426},
numpages = {14},
keywords = {Test Optimization, Software Testing, Machine Learning, Bayesian Optimization},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1016/j.jss.2019.110486,
author = {Barbez, Antoine and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {A machine-learning based ensemble method for anti-patterns detection},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {161},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110486},
doi = {10.1016/j.jss.2019.110486},
journal = {J. Syst. Softw.},
month = mar,
numpages = {11},
keywords = {Ensemble methods, Machine learning, Anti-patterns, Software quality}
}

@inproceedings{10.1109/ESEM.2017.50,
author = {Bennin, Kwabena Ebo and Keung, Jacky and Monden, Akito and Phannachitta, Passakorn and Mensah, Solomon},
title = {The significant effects of data sampling approaches on software defect prioritization and classification},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.50},
doi = {10.1109/ESEM.2017.50},
abstract = {Context: Recent studies have shown that performance of defect prediction models can be affected when data sampling approaches are applied to unbalanced training data for building defect prediction models. However, the magnitude (degree and power) of the effect of these sampling methods on the classification and prioritization performances of defect prediction models is still unknown. Goal: To investigate the statistical and practical significance of using resampled data for constructing defect prediction models. Method: We examine the practical effects of six data sampling methods on performances of five defect prediction models. The prediction performances of the models trained on default datasets (no sampling method) are compared with that of the models trained on resampled datasets (application of sampling methods). To decide whether the performance changes are significant or not, robust statistical tests are performed and effect sizes computed. Twenty releases of ten open source projects extracted from the PROMISE repository are considered and evaluated using the AUC, pd, pf and G-mean performance measures. Results: There are statistical significant differences and practical effects on the classification performance (pd, pf and G-mean) between models trained on resampled datasets and those trained on the default datasets. However, sampling methods have no statistical and practical effects on defect prioritization performance (AUC) with small or no effect values obtained from the models trained on the resampled datasets. Conclusions: Existing sampling methods can properly set the threshold between buggy and clean samples, while they cannot improve the prediction of defect-proneness itself. Sampling methods are highly recommended for defect classification purposes when all faulty modules are to be considered for testing.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {364–373},
numpages = {10},
keywords = {statistical significance, sampling methods, imbalanced data, empirical software engineering, defect prediction},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@phdthesis{10.5555/AAI28772454,
author = {Stewart, Michael Allen and J., Laszlo, Michael and Sumitra, Mukherjee,},
advisor = {J, Mitropoulos, Francisco},
title = {Increasing Software Reliability Using Mutation Testing and Machine Learning},
year = {2021},
isbn = {9798492722906},
publisher = {Nova Southeastern University},
abstract = {Mutation testing is a type of software testing proposed in the 1970s where program statements are deliberately changed to introduce simple errors so that test cases can be validated to determine if they can detect the errors. The goal of mutation testing was to reduce complex program errors by preventing the related simple errors. Test cases are executed against the mutant code to determine if one fails, detects the error and ensures the program is correct. One major issue with this type of testing was it became intensive computationally to generate and test all possible mutations for complex programs.This dissertation used machine learning for the selection of mutation operators that reduced the computational cost of testing and improved test suite effectiveness. The goals were to produce mutations that were more resistant to test cases, improve test case evaluation, validate then improve the test suite's effectiveness, realize cost reductions by generating fewer mutations for testing and improving software reliability by detecting more errors. To accomplish these goals, experiments were conducted using sample programs to determine how well the reinforcement learning based algorithm performed with one live mutation, multiple live mutations and no live mutations. The experiments, measured by mutation score, were used to update the algorithm and improved accuracy for predictions. The performance was then evaluated on multiple processor computers.One key result from this research was the development of a reinforcement algorithm to identify mutation operator combinations that resulted in live mutants. During experimentation, the reinforcement learning algorithm identified the optimal mutation operator selections for various programs and test suite scenarios, as well as determined that by using parallel processing and multiple cores the reinforcement learning process for mutation operator selection was practical. With reinforcement learning the mutation operators utilized were reduced by 50–100%. In conclusion, these improvements created a 'live' mutation testing process that evaluated various mutation operators and generated mutants to perform real-time mutation testing while dynamically prioritizing mutation operator recommendations. This has enhanced the software developer's ability to improve testing processes. The contributions of this paper's research supported the shift-left testing approach, where testing is performed earlier in the software development cycle when error resolution is less costly.},
note = {AAI28772454}
}

@inproceedings{10.1145/2908812.2908938,
author = {Panichella, Annibale and Alexandru, Carol V. and Panichella, Sebastiano and Bacchelli, Alberto and Gall, Harald C.},
title = {A Search-based Training Algorithm for Cost-aware Defect Prediction},
year = {2016},
isbn = {9781450342063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908812.2908938},
doi = {10.1145/2908812.2908938},
abstract = {Research has yielded approaches to predict future defects in software artifacts based on historical information, thus assisting companies in effectively allocating limited development resources and developers in reviewing each others' code changes. Developers are unlikely to devote the same effort to inspect each software artifact predicted to contain defects, since the effort varies with the artifacts' size (cost) and the number of defects it exhibits (effectiveness). We propose to use Genetic Algorithms (GAs) for training prediction models to maximize their cost-effectiveness. We evaluate the approach on two well-known models, Regression Tree and Generalized Linear Model, and predict defects between multiple releases of six open source projects. Our results show that regression models trained by GAs significantly outperform their traditional counterparts, improving the cost-effectiveness by up to 240%. Often the top 10% of predicted lines of code contain up to twice as many defects.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
pages = {1077–1084},
numpages = {8},
keywords = {machine learning, genetic algorithm, defect prediction},
location = {Denver, Colorado, USA},
series = {GECCO '16}
}

@article{10.1007/s10515-011-0091-2,
author = {Liparas, Dimitris and Angelis, Lefteris and Feldt, Robert},
title = {Applying the Mahalanobis-Taguchi strategy for software defect diagnosis},
year = {2012},
issue_date = {June      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0091-2},
doi = {10.1007/s10515-011-0091-2},
abstract = {The Mahalanobis-Taguchi (MT) strategy combines mathematical and statistical concepts like Mahalanobis distance, Gram-Schmidt orthogonalization and experimental designs to support diagnosis and decision-making based on multivariate data. The primary purpose is to develop a scale to measure the degree of abnormality of cases, compared to "normal" or "healthy" cases, i.e. a continuous scale from a set of binary classified cases. An optimal subset of variables for measuring abnormality is then selected and rules for future diagnosis are defined based on them and the measurement scale. This maps well to problems in software defect prediction based on a multivariate set of software metrics and attributes. In this paper, the MT strategy combined with a cluster analysis technique for determining the most appropriate training set, is described and applied to well-known datasets in order to evaluate the fault-proneness of software modules. The measurement scale resulting from the MT strategy is evaluated using ROC curves and shows that it is a promising technique for software defect diagnosis. It compares favorably to previously evaluated methods on a number of publically available data sets. The special characteristic of the MT strategy that it quantifies the level of abnormality can also stimulate and inform discussions with engineers and managers in different defect prediction situations.},
journal = {Automated Software Engg.},
month = jun,
pages = {141–165},
numpages = {25},
keywords = {Software testing, Software defect prediction, Mahalanobis-Taguchi strategy, Fault-proneness}
}

@inproceedings{10.1007/978-3-030-33607-3_12,
author = {Shepperd, Martin and Guo, Yuchen and Li, Ning and Arzoky, Mahir and Capiluppi, Andrea and Counsell, Steve and Destefanis, Giuseppe and Swift, Stephen and Tucker, Allan and Yousefi, Leila},
title = {The Prevalence of Errors in Machine Learning Experiments},
year = {2019},
isbn = {978-3-030-33606-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33607-3_12},
doi = {10.1007/978-3-030-33607-3_12},
abstract = {Context: Conducting experiments is central to research machine learning research to benchmark, evaluate and compare learning algorithms. Consequently it is important we conduct reliable, trustworthy experiments.Objective: We investigate the incidence of errors in a sample of machine learning experiments in the domain of software defect prediction. Our focus is simple arithmetical and statistical errors.Method: We analyse 49 papers describing 2456 individual experimental results from a previously undertaken systematic review comparing supervised and unsupervised defect prediction classifiers. We extract the confusion matrices and test for relevant constraints, e.g., the marginal probabilities must sum to one. We also check for multiple statistical significance testing errors.Results: We find that a total of 22 out of 49 papers contain demonstrable errors. Of these 7 were statistical and 16 related to confusion matrix inconsistency (one paper contained both classes of error).Conclusions: Whilst some errors may be of a relatively trivial nature, e.g., transcription errors their presence does not engender confidence. We strongly urge researchers to follow open science principles so errors can be more easily be detected and corrected, thus as a community reduce this worryingly high error rate with our computational experiments.},
booktitle = {Intelligent Data Engineering and Automated Learning – IDEAL 2019: 20th International Conference, Manchester, UK, November 14–16, 2019, Proceedings, Part I},
pages = {102–109},
numpages = {8},
keywords = {Classifier, Computational experiment, Reliability, Error},
location = {Manchester, United Kingdom}
}

@inproceedings{10.1109/ICSE43902.2021.00138,
author = {Wang, Song and Shrestha, Nishtha and Subburaman, Abarna Kucheri and Wang, Junjie and Wei, Moshi and Nagappan, Nachiappan},
title = {Automatic Unit Test Generation for Machine Learning Libraries: How Far Are We?},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00138},
doi = {10.1109/ICSE43902.2021.00138},
abstract = {Automatic unit test generation that explores the input space and produces effective test cases for given programs have been studied for decades. Many unit test generation tools that can help generate unit test cases with high structural coverage over a program have been examined. However, the fact that existing test generation tools are mainly evaluated on general software programs calls into question about its practical effectiveness and usefulness for machine learning libraries, which are statistically-orientated and have fundamentally different nature and construction from general software projects.In this paper, we set out to investigate the effectiveness of existing unit test generation techniques on machine learning libraries. To investigate this issue, we conducted an empirical study on five widely-used machine learning libraries with two popular unit test case generation tools, i.e., EVOSUITE and Randoop. We find that (1) most of the machine learning libraries do not maintain a high-quality unit test suite regarding commonly applied quality metrics such as code coverage (on average is 34.1%) and mutation score (on average is 21.3%), (2) unit test case generation tools, i.e., EVOSUITE and Randoop, lead to clear improvements in code coverage and mutation score, however, the improvement is limited, and (3) there exist common patterns in the uncovered code across the five machine learning libraries that can be used to improve unit test case generation tasks.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1548–1560},
numpages = {13},
keywords = {testing machine learning libraries, test case generation, Empirical software engineering},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2020390.2020405,
author = {Lu, Huihua and Cukic, Bojan and Culp, Mark},
title = {An iterative semi-supervised approach to software fault prediction},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020405},
doi = {10.1145/2020390.2020405},
abstract = {Background: Many statistical and machine learning techniques have been implemented to build predictive fault models. Traditional methods are based on supervised learning. Software metrics for a module and corresponding fault information, available from previous projects, are used to train a fault prediction model. This approach calls for a large size of training data set and enables the development of effective fault prediction models. In practice, data collection costs, the lack of data from earlier projects or product versions may make large fault prediction training data set unattainable. Small size of the training set that may be available from the current project is known to deteriorate the performance of the fault predictive model. In semi-supervised learning approaches, software modules with known or unknown fault content can be used for training.Aims: To implement and evaluate a semi-supervised learning approach in software fault prediction.Methods: We investigate an iterative semi-supervised approach to software quality prediction in which a base supervised learner is used within a semi-supervised application.Results: We varied the size of labeled software modules from 2% to 50% of all the modules in the project. After tracking the performance of each iteration in the semi-supervised algorithm, we observe that semi-supervised learning improves fault prediction if the number of initially labeled software modules exceeds 5%.Conclusion: The semi-supervised approach outperforms the corresponding supervised learning approach when both use random forest as base classification algorithm.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {15},
numpages = {10},
keywords = {fault prediction, semi-supervised learning},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@article{10.4018/IJITSA.2021010104,
author = {Shatnawi, Raed and Mishra, Alok},
title = {An Empirical Study on Software Fault Prediction Using Product and Process Metrics},
year = {2021},
issue_date = {Jan 2021},
publisher = {IGI Global},
address = {USA},
volume = {14},
number = {1},
issn = {1935-570X},
url = {https://doi.org/10.4018/IJITSA.2021010104},
doi = {10.4018/IJITSA.2021010104},
abstract = {Product and process metrics are measured from the development and evolution of software. Metrics are indicators of software fault-proneness and advanced models using machine learning can be provided to the development team to select modules for further inspection. Most fault-proneness classifiers were built from product metrics. However, the inclusion of process metrics adds evolution as a factor to software quality. In this work, the authors propose a process metric measured from the evolution of software to predict fault-proneness in software models. The process metrics measures change-proneness of modules (classes and interfaces). Classifiers are trained and tested for five large open-source systems. Classifiers were built using product metrics alone and using a combination of product and the proposed process metric. The classifiers evaluation shows improvements whenever the process metrics were used. Evolution metrics are correlated with quality of software and helps in improving software quality prediction for future releases.},
journal = {Int. J. Inf. Technol. Syst. Appoach},
month = jan,
pages = {62–78},
numpages = {17},
keywords = {Software Fault, Product Metrics, Process Metrics, CK Metrics}
}

@article{10.1007/s10664-020-09881-0,
author = {Riccio, Vincenzo and Jahangirova, Gunel and Stocco, Andrea and Humbatova, Nargiz and Weiss, Michael and Tonella, Paolo},
title = {Testing machine learning based systems: a systematic mapping},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09881-0},
doi = {10.1007/s10664-020-09881-0},
journal = {Empirical Softw. Engg.},
month = nov,
pages = {5193–5254},
numpages = {62},
keywords = {Machine learning, Software testing, Systematic review, Systematic mapping}
}

@inproceedings{10.1109/ITSC48978.2021.9564437,
author = {Shi, Xiaojie and Dai, Shenghua},
title = {Fault Prediction of Turnout Equipment Based on Double-layer Gated Recurrent Unit Neural Network},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ITSC48978.2021.9564437},
doi = {10.1109/ITSC48978.2021.9564437},
abstract = {Turnout is an important railway equipment, and the failure of the turnout equipment will have a great impact on the safe operation of the train. In order to predict the failure of the turnout equipment in advance, this paper combines the double-layer gated recurrent unit (DL-GRU) neural network with the failure prediction of the turnout. This paper extracts the features of the current curves generated during multiple actions before the turnout fails, and uses the method of kernel principal component analysis (KPCA) to reduce the dimensions of the extracted features. Finally, the time series data set of turnout action current fault feature is established, which is used as the input of the DL-GRU neural network to realize the fault prediction of the turnout. The simulation results show that the DL-GRU network has a high prediction accuracy, compared with LSTM network and single-layer GRU neural network, the DL-GRU has better prediction performance.},
booktitle = {2021 IEEE International Intelligent Transportation Systems Conference (ITSC)},
pages = {2336–2341},
numpages = {6},
location = {Indianapolis, IN, USA}
}

@article{10.1016/j.eswa.2020.114161,
author = {Houssein, Essam H. and Emam, Marwa M. and Ali, Abdelmgeid A. and Suganthan, Ponnuthurai Nagaratnam},
title = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114161},
doi = {10.1016/j.eswa.2020.114161},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {20},
keywords = {Thermography images, Ultrasound images, Mammogram images, Medical imaging modalities, Magnetic resonance imaging (MRI), Machine learning, Histological images, Deep learning, Computer-aided diagnosis system (CAD), Convolutional neural network, Breast cancer classification}
}

@inproceedings{10.1145/3410352.3410747,
author = {Almaghairbe, Rafig and Roper, Marc and Almabruk, Tahani},
title = {Machine Learning Techniques for Automated Software Fault Detection via Dynamic Execution Data: Empirical Evaluation Study},
year = {2020},
isbn = {9781450377362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410352.3410747},
doi = {10.1145/3410352.3410747},
abstract = {The biggest obstacle of automated software testing is the construction of test oracles. Today, it is possible to generate enormous amount of test cases for an arbitrary system that reach a remarkably high level of coverage, but the effectiveness of test cases is limited by the availability of test oracles that can distinguish failing executions. Previous work by the authors has explored the use of unsupervised and semi-supervised learning techniques to develop test oracles so that the correctness of software outputs and behaviours on new test cases can be predicated [1], [2], [10], and experimental results demonstrate the promise of this approach. In this paper, we present an evaluation study for test oracles based on machine-learning approaches via dynamic execution data (firstly, input/output pairs and secondly, amalgamations of input/output pairs and execution traces) by comparing their effectiveness with existing techniques from the specification mining domain (the data invariant detector Daikon [5]). The two approaches are evaluated on a range of mid-sized systems and compared in terms of their fault detection ability and false positive rate. The empirical study also discuss the major limitations and the most important properties related to the application of machine learning techniques as test oracles in practice. The study also gives a road map for further research direction in order to tackle some of discussed limitations such as accuracy and scalability. The results show that in most cases semi-supervised learning techniques performed far better as an automated test classifier than Daikon (especially in the case that input/output pairs were augmented with their execution traces). However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases particularly when input/output pairs were used together with execution traces.},
booktitle = {Proceedings of the 6th International Conference on Engineering &amp; MIS 2020},
articleno = {15},
numpages = {12},
keywords = {Automated Testing Oracles, Empirical Study, Machine Learning Techniques, Specification Mining},
location = {Almaty, Kazakhstan},
series = {ICEMIS'20}
}

@article{10.1016/j.compeleceng.2021.107362,
author = {P, Gouthaman and Sankaranarayanan, Suresh},
title = {Prediction of Risk Percentage in Software Projects by Training Machine Learning Classifiers},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {94},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107362},
doi = {10.1016/j.compeleceng.2021.107362},
journal = {Comput. Electr. Eng.},
month = sep,
numpages = {9},
keywords = {Risk prediction, Machine learning, Incremental, Evolutionary, Waterfall, Agile, Software model}
}

@inproceedings{10.1145/2786805.2804429,
author = {Kim, Mijung and Nam, Jaechang and Yeon, Jaehyuk and Choi, Soonhwang and Kim, Sunghun},
title = {REMI: defect prediction for efficient API testing},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2804429},
doi = {10.1145/2786805.2804429},
abstract = {Quality assurance for common APIs is important since the the reliability of APIs affects the quality of other systems using the APIs. Testing is a common practice to ensure the quality of APIs, but it is a challenging and laborious task especially for industrial projects. Due to a large number of APIs with tight time constraints and limited resources, it is hard to write enough test cases for all APIs. To address these challenges, we present a novel technique, REMI that predicts high risk APIs in terms of producing potential bugs. REMI allows developers to write more test cases for the high risk APIs. We evaluate REMI on a real-world industrial project, Tizen-wearable, and apply REMI to the API development process at Samsung Electronics. Our evaluation results show that REMI predicts the bug-prone APIs with reasonable accuracy (0.681 f-measure on average). The results also show that applying REMI to the Tizen-wearable development process increases the number of bugs detected, and reduces the resources required for executing test cases.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {990–993},
numpages = {4},
keywords = {Quality Assurance, Defect Prediction, API Testing},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1145/2347696.2347709,
author = {Rashid, Ekbal and Patnayak, Srikanta and Bhattacherjee, Vandana},
title = {A survey in the area of machine learning and its application for software quality prediction},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2347696.2347709},
doi = {10.1145/2347696.2347709},
abstract = {This paper explores software quality improvement through early prediction of error patterns. It summarizes a variety of techniques for software quality prediction in the domain of software engineering. The objective of this research is to apply the various machine learning approaches, such as Case-Based Reasoning and Fuzzy logic, to predict software quality. The system predicts the error after accepting the values of certain parameters of the software. This paper advocates the use of case-based reasoning (i.e., CBR) to build a software quality prediction system with the help of human experts. The prediction is based on analogy. We have used different similarity measures to find the best method that increases reliability. This software is compiled using Turbo C++ 3.0 and hence it is very compact and standalone. It can be readily deployed on any configuration without affecting its performance.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–7},
numpages = {7},
keywords = {software quality, similarity, machine learning, function, erffort estimation, analogy, CBR}
}

@inproceedings{10.1145/2979779.2979783,
author = {Maheshwari, Suchi and Agarwal, Sonali},
title = {Three-way decision based Defect Prediction for Object Oriented Software},
year = {2016},
isbn = {9781450342131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2979779.2979783},
doi = {10.1145/2979779.2979783},
abstract = {Early prediction of defective software module plays critical role in the software project development to reduce the overall development time, budgets and increases the customer satisfaction. The bug prediction based on two-way classification method classifies the software module as defective or non-defective. This method provides good accuracy measure but this metric is not sufficient in case if misclassification cost is concerned. Classifying the defective module as non-defective will lead to higher cost of entire software project at the end. In this study, three-way decision based classification method and Random Forest ensemble are used to predict the defect in Object Oriented Software to reduce the misclassification cost which will lead to avoid the cost overrun. The eclipse bug prediction dataset is used and experimental results show that the decision cost is reduced and accuracy is increased using our proposed method.},
booktitle = {Proceedings of the International Conference on Advances in Information Communication Technology &amp; Computing},
articleno = {4},
numpages = {6},
keywords = {Three-way decision, Software defect prediction, Random Forest, Na\"{\i}ve Bayes, Eclipse Bug Prediction dataset},
location = {Bikaner, India},
series = {AICTC '16}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {software product line, quality evaluation, machine learning, feature model},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1007/s00500-020-05226-7,
author = {Khuat, Thanh Tung and Ruta, Dymitr and Gabrys, Bogdan},
title = {Hyperbox-based machine learning algorithms: a comprehensive survey},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05226-7},
doi = {10.1007/s00500-020-05226-7},
abstract = {With the rapid development of digital information, the data volume generated by humans and machines is growing exponentially. Along with this trend, machine learning algorithms have been formed and evolved continuously to discover new information and knowledge from different data sources. Learning algorithms using hyperboxes as fundamental representational and building blocks are a branch of machine learning methods. These algorithms have enormous potential for high scalability and online adaptation of predictors built using hyperbox data representations to the dynamically changing environments and streaming data. This paper aims to give a comprehensive survey of the literature on hyperbox-based machine learning models. In general, according to the architecture and characteristic features of the resulting models, the existing hyperbox-based learning algorithms may be grouped into three major categories: fuzzy min–max neural networks, hyperbox-based hybrid models and other algorithms based on hyperbox representations. Within each of these groups, this paper shows a brief description of the structure of models, associated learning algorithms and an analysis of their advantages and drawbacks. Main applications of these hyperbox-based models to the real-world problems are also described in this paper. Finally, we discuss some open problems and identify potential future research directions in this field.},
journal = {Soft Comput.},
month = jan,
pages = {1325–1363},
numpages = {39},
keywords = {Online learning, Clustering, Data classification, Hybrid classifiers, Fuzzy min–max neural network, Membership function, Hyperboxes}
}

@inproceedings{10.1007/978-3-030-04780-1_26,
author = {Pal, Amrit and Kumar, Manish},
title = {Applying Big Data Intelligence for Real Time Machine Fault Prediction},
year = {2018},
isbn = {978-3-030-04779-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-04780-1_26},
doi = {10.1007/978-3-030-04780-1_26},
abstract = {Continuous use of mechanical systems requires precise maintenance. Automatic monitoring of such systems generates a large amount of data which require intelligent mining methods for processing and information extraction. The problem is to predict the faults generated with ball bearing which severely degrade operating conditions of machinery. We develop a distributed fault prediction model based on big data intelligence that extracts nine essential features from ball bearing dataset through distributed random forest. We also perform a rigorous simulation analysis of the proposed approach and the results ensure the accuracy/correctness of the method. Different types of fault classes are considered for prediction purpose and classification is done in a supervised distributed environment.},
booktitle = {Big Data Analytics: 6th International Conference, BDA 2018, Warangal, India, December 18–21, 2018, Proceedings},
pages = {376–391},
numpages = {16},
keywords = {Distributed environment, Random forest, Ball bearing, Fault prediction, Spark, Parallel processing, Decision tree},
location = {Warangal, India}
}

@inproceedings{10.1145/1370750.1370759,
author = {Ratzinger, Jacek and Sigmund, Thomas and Gall, Harald C.},
title = {On the relation of refactorings and software defect prediction},
year = {2008},
isbn = {9781605580241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370750.1370759},
doi = {10.1145/1370750.1370759},
abstract = {This paper analyzes the influence of evolution activities such as refactoring on software defects. In a case study of five open source projects we used attributes of software evolution to predict defects in time periods of six months. We use versioning and issue tracking systems to extract 110 data mining features, which are separated into refactoring and non-refactoring related features. These features are used as input into classification algorithms that create prediction models for software defects. We found out that refactoring related features as well as non-refactoring related features lead to high quality prediction models. Additionally, we discovered that refactorings and defects have an inverse correlation: The number of software defects decreases, if the number of refactorings increased in the preceding time period. As a result, refactoring should be a significant part of both bug fixes and other evolutionary changes to reduce software defects.},
booktitle = {Proceedings of the 2008 International Working Conference on Mining Software Repositories},
pages = {35–38},
numpages = {4},
keywords = {software evolution, software analysis, mining},
location = {Leipzig, Germany},
series = {MSR '08}
}

@inproceedings{10.5555/2819009.2819026,
author = {Tan, Ming and Tan, Lin and Dara, Sashank and Mayeux, Caleb},
title = {Online defect prediction for imbalanced data},
year = {2015},
publisher = {IEEE Press},
abstract = {Many defect prediction techniques are proposed to improve software reliability. Change classification predicts defects at the change level, where a change is the modifications to one file in a commit. In this paper, we conduct the first study of applying change classification in practice.We identify two issues in the prediction process, both of which contribute to the low prediction performance. First, the data are imbalanced---there are much fewer buggy changes than clean changes. Second, the commonly used cross-validation approach is inappropriate for evaluating the performance of change classification. To address these challenges, we apply and adapt online change classification, resampling, and updatable classification techniques to improve the classification performance.We perform the improved change classification techniques on one proprietary and six open source projects. Our results show that these techniques improve the precision of change classification by 12.2-89.5% or 6.4--34.8 percentage points (pp.) on the seven projects. In addition, we integrate change classification in the development process of the proprietary project. We have learned the following lessons: 1) new solutions are needed to convince developers to use and believe prediction results, and prediction results need to be actionable, 2) new and improved classification algorithms are needed to explain the prediction results, and insensible and unactionable explanations need to be filtered or refined, and 3) new techniques are needed to improve the relatively low precision.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {99–108},
numpages = {10},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3422392.3422427,
author = {Oliveira, Daniel and Assun\c{c}\~{a}o, Wesley K. G. and Souza, Leonardo and Oizumi, Willian and Garcia, Alessandro and Fonseca, Baldoino},
title = {Applying Machine Learning to Customized Smell Detection: A Multi-Project Study},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422427},
doi = {10.1145/3422392.3422427},
abstract = {Code smells are considered symptoms of poor implementation choices, which may hamper the software maintainability. Hence, code smells should be detected as early as possible to avoid software quality degradation. Unfortunately, detecting code smells is not a trivial task. Some preliminary studies investigated and concluded that machine learning (ML) techniques are a promising way to better support smell detection. However, these techniques are hard to be customized to promote an early and accurate detection of specific smell types. Yet, ML techniques usually require numerous code examples to be trained (composing a relevant dataset) in order to achieve satisfactory accuracy. Unfortunately, such a dependency on a large validated dataset is impractical and leads to late detection of code smells. Thus, a prevailing challenge is the early customized detection of code smells taking into account the typical limited training data. In this direction, this paper reports a study in which we collected code smells, from ten active projects, that were actually refactored by developers, differently from studies that rely on code smells inferred by researchers. These smells were used for evaluating the accuracy regarding early detection of code smells by using seven ML techniques. Once we take into account such smells that were considered as important by developers, the ML techniques are able to customize the detection in order to focus on smells observed as relevant in the investigated systems. The results showed that all the analyzed techniques are sensitive to the type of smell and obtained good results for the majority of them, especially JRip and Random Forest. We also observe that the ML techniques did not need a high number of examples to reach their best accuracy results. This finding implies that ML techniques can be successfully used for early detection of smells without depending on the curation of a large dataset.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {233–242},
numpages = {10},
keywords = {code smell, code smell detection, software quality},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1109/ICCAD51958.2021.9643459,
author = {Tseng, Hsiao-Yin and Chiu, I-Wei and Wu, Mu-Ting and Li, James Chien-Mo},
title = {Machine Learning-Based Test Pattern Generation for Neuromorphic Chips},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICCAD51958.2021.9643459},
doi = {10.1109/ICCAD51958.2021.9643459},
abstract = {The demand for neuromorphic chips has skyrocketed in recent years. Thus, efficient manufacturing testing becomes an issue. Conventional testing cannot be applied because some neuromorphic chips do not have scan chains. However, traditional functional testing for neuromorphic chips suffers from long test length and low fault coverage. In this work, we propose a machine learning-based test pattern generation technique with behavior fault models. We use the concept of adversarial attack to generate test patterns to improve the fault coverage of existing functional test patterns. The effectiveness of the proposed technique is demonstrated on two Spiking Neural Network models trained on MNIST. Compared to traditional functional testing, our proposed technique reduces test length by 566x to 8,824x and improves fault coverage by 8.1% to 86.3% on five fault models. Finally, we propose a methodology to solve the scalability issue for the synapse fault models, resulting in 25.7x run time reduction on test pattern generation for synapse faults.},
booktitle = {2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)},
pages = {1–7},
numpages = {7},
location = {Munich, Germany}
}

@article{10.1007/s10994-020-05872-w,
author = {Kuwajima, Hiroshi and Yasuoka, Hirotoshi and Nakae, Toshihiro},
title = {Engineering problems in machine learning systems},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {109},
number = {5},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-020-05872-w},
doi = {10.1007/s10994-020-05872-w},
abstract = {Fatal accidents are a major issue hindering the wide acceptance of safety-critical systems that employ machine learning and deep learning models, such as automated driving vehicles. In order to use machine learning in a safety-critical system, it is necessary to demonstrate the safety and security of the system through engineering processes. However, thus far, no such widely accepted engineering concepts or frameworks have been established for these systems. The key to using a machine learning model in a deductively engineered system is decomposing the data-driven training of machine learning models into requirement, design, and verification, particularly for machine learning models used in safety-critical systems. Simultaneously, open problems and relevant technical fields are not organized in a manner that enables researchers to select a theme and work on it. In this study, we identify, classify, and explore the open problems in engineering (safety-critical) machine learning systems—that is, in terms of requirement, design, and verification of machine learning models and systems—as well as discuss related works and research directions, using automated driving vehicles as an example. Our results show that machine learning models are characterized by a lack of requirements specification, lack of design specification, lack of interpretability, and lack of robustness. We also perform a gap analysis on a conventional system quality standard SQuaRE with the characteristics of machine learning models to study quality models for machine learning systems. We find that a lack of requirements specification and lack of robustness have the greatest impact on conventional quality models.},
journal = {Mach. Learn.},
month = may,
pages = {1103–1126},
numpages = {24},
keywords = {Quality models, Automated driving, Safety critical systems, Systems engineering, Software engineering, Machine learning}
}

@article{10.3233/JIFS-179619,
author = {Lyu, Yi and Jiang, YiJie and Zhang, Weiping},
title = {Examination on avionics system fault prediction technology based on ashy neural network and fuzzy recognition},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {38},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179619},
doi = {10.3233/JIFS-179619},
abstract = {The purpose of this paper is to accurately locate the fault prediction and diagnosis technology, to have a high degree of automation, and to handle it quickly, for the large aircraft avionics system failure presents the feature of multiple coupling, complex impact and rapid spread. At the same time, the fault prediction diagnosis technology is one of the most important contents of the avionics system equipment prediction, so how to quickly and effectively predict the failure of key system parts of avionics is the core essential to ensure the complete operation of the whole system. This paper through establishing the gray neural network model, combining the advantages of gray model to deal with poor information and the characteristics of artificial neural network processing nonlinear data, to realize the fault prediction of avionics system, At the same time, At the same time, through the fuzzy recognition method based on the deterioration degree, established the bridge between the two, in turn, to achieve the health prediction management of system. The method mainly includes: Firstly, by combining gray theory and artificial neural network algorithm with fuzzy recognition to establish a network model that contains gray neural network models and can reflect the excellent characteristics of fuzzy recognition and conduct experimental analysis; Second, on this basis, improve the weight update strategy of the gray neural network by using additional learning rate method which based on momentum and improve the accuracy of the algorithm. Therefore, it can be concluded that the predictions presented in this paper should not be directly imitated when the system disturbance factor is too large or the system is abnormally caused by a serious disturbance suddenly appearing at a certain point in time, but should properly processed the data firstly according to the actual situation. According to the time series of the actual situation, several models are established, and the data correction is explained from the model prediction effect, and the gray model and description are improved. The improved combination of gray neural network and gray neural network can not only improve the prediction accuracy, but also provide a feasible method for such time series prediction, which provides a practical and effective technical method for avionics system fault prediction.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {3939–3947},
numpages = {9},
keywords = {combined forecast, fault prediction, fuzzy recognition, avionics system, Ashy neural network}
}

@inproceedings{10.1145/1629716.1629720,
author = {Chae, Wonseok and Blume, Matthias},
title = {Language support for feature-oriented product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629720},
doi = {10.1145/1629716.1629720},
abstract = {Product line engineering is an emerging paradigm of developing a family of products. While product line analysis and design mainly focus on reasoning about commonality and variability of family members, product line implementation gives its attention to mechanisms of managing variability. In many cases, however, product line methods do not impose any specific synthesis mechanisms on product line implementation, so implementation details are left to developers. In our previous work, we adopted feature-oriented product line engineering to build a family of compilers and managed variations using the Standard ML module system. We demonstrated the applicability of this module system to product line implementation. Although we have benefited from the product line engineering paradigm, it mostly served us as a design paradigm to change the way we think about a set of closely related compilers, not to change the way we build them. The problem was that Standard ML did not fully realize this paradigm at the code level, which caused some difficulties when we were developing a set of compilers.In this paper, we address such issues with a language-based solution. MLPolyR is our choice of an implementation language. It supports three different programming styles. First, its first-class cases facilitate composable extensions at the expression levels. Second, its module language provides extensible and parameterized modules, which make large-scale extensible programming possible. Third, its macro system simplifies specification and composition of feature related code. We will show how the combination of these language features work together to facilitate the product line engineering paradigm.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {3–10},
numpages = {8},
keywords = {feature-oriented programming, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/3387939.3388613,
author = {Scheerer, Max and Klamroth, Jonas and Reussner, Ralf and Beckert, Bernhard},
title = {Towards classes of architectural dependability assurance for machine-learning-based systems},
year = {2020},
isbn = {9781450379625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387939.3388613},
doi = {10.1145/3387939.3388613},
abstract = {Advances in Machine Learning (ML) have brought previously hard to handle problems within arm's reach. However, this power comes at the cost of unassured reliability and lacking transparency. Overcoming this drawback is very hard due to the probabilistic nature of ML. Current approaches mainly tackle this problem by developing more robust learning procedures. Such algorithmic approaches, however, are limited to certain types of uncertainties and cannot deal with all of them, e.g., hardware failure. This paper discusses how this problem can be addressed at architectural rather than algorithmic level to assess systems dependability properties in early development stages. Moreover, we argue that Self-Adaptive Systems (SAS) are more suited to safeguard ML w.r.t. various uncertainties. As a step towards this we propose classes of dependability in which ML-based systems may be categorized and discuss which and how assurances can be made for each class.},
booktitle = {Proceedings of the IEEE/ACM 15th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {31–37},
numpages = {7},
keywords = {software quality, machine learning, dependability, artificial intelligence, architectural-driven self-adaptation},
location = {Seoul, Republic of Korea},
series = {SEAMS '20}
}

@inproceedings{10.1145/2597073.2597078,
author = {Zhang, Feng and Mockus, Audris and Keivanloo, Iman and Zou, Ying},
title = {Towards building a universal defect prediction model},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597078},
doi = {10.1145/2597073.2597078},
abstract = {To predict files with defects, a suitable prediction model must be built for a software project from either itself (within-project) or other projects (cross-project). A universal defect prediction model that is built from the entire set of diverse projects would relieve the need for building models for an individual project. A universal model could also be interpreted as a basic relationship between software metrics and defects. However, the variations in the distribution of predictors pose a formidable obstacle to build a universal model. Such variations exist among projects with different context factors (e.g., size and programming language). To overcome this challenge, we propose context-aware rank transformations for predictors. We cluster projects based on the similarity of the distribution of 26 predictors, and derive the rank transformations using quantiles of predictors for a cluster. We then fit the universal model on the transformed data of 1,398 open source projects hosted on SourceForge and GoogleCode. Adding context factors to the universal model improves the predictive power. The universal model obtains prediction performance comparable to the within-project models and yields similar results when applied on five external projects (one Apache and four Eclipse projects). These results suggest that a universal defect prediction model may be an achievable goal.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {182–191},
numpages = {10},
keywords = {rank transformation, quality, large scale, defect prediction, defect, context factors, bug, Universal defect prediction model},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1145/2889160.2889256,
author = {Tantithamthavorn, Chakkrit},
title = {Towards a better understanding of the impact of experimental components on defect prediction modelling},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889256},
doi = {10.1145/2889160.2889256},
abstract = {Defect prediction models are used to pinpoint risky software modules and understand past pitfalls that lead to defective modules. The predictions and insights that are derived from defect prediction models may not be accurate and reliable if researchers do not consider the impact of experimental components (e.g., datasets, metrics, and classifiers) of defect prediction modelling. Therefore, a lack of awareness and practical guidelines from previous research can lead to invalid predictions and unreliable insights. In this thesis, we investigate the impact that experimental components have on the predictions and insights of defect prediction models. Through case studies of systems that span both proprietary and open-source domains, we find that (1) noise in defect datasets; (2) parameter settings of classification techniques; and (3) model validation techniques have a large impact on the predictions and insights of defect prediction models, suggesting that researchers should carefully select experimental components in order to produce more accurate and reliable defect prediction models.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {867–870},
numpages = {4},
keywords = {experimental components, defect prediction modelling},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3127005.3127013,
author = {Valdivia-Garcia, Harold and Nagappan, Meiyappan},
title = {The Characteristics of False-Negatives in File-level Fault Prediction},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127013},
doi = {10.1145/3127005.3127013},
abstract = {Over the years, a plethora of works has proposed more and more sophisticated machine learning techniques to improve fault prediction models. However, past studies using product metrics from closed-source projects, found a ceiling effect in the performance of fault prediction models. On the other hand, other studies have shown that process metrics are significantly better than product metrics for fault prediction. In our case study therefore we build models that include both product and process metrics taken together. We find that the ceiling effect found in prior studies exists even when we consider process metrics. We then qualitatively investigate the bug reports, source code files, and commit information for the bugs in the files that are false-negative in our fault prediction models trained using product and process metrics. Surprisingly, our qualitative analysis shows that bugs related to false-negative files and true-positive files are similar in terms of root causes, impact and affected components, and consequently such similarities might be exploited to enhance fault prediction models.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {73–82},
numpages = {10},
keywords = {Code Metrics, Post-release Defects, Process Metrics},
location = {Toronto, Canada},
series = {PROMISE}
}

@phdthesis{10.5555/AAI28256855,
author = {Wu, Baijun and Arun, Lakhotia, and Anthony, Maida, and Miao, Jin,},
advisor = {Sheng, Chen,},
title = {Using Machine Learning to Improve Programming Error Reporting},
year = {2020},
isbn = {9798519181839},
publisher = {University of Louisiana at Lafayette},
abstract = {The main purpose of this research is to explore applying machine learning to improve programming error reporting. In the first part of this dissertation, I present the empirical study about how type error were fixed and what students did. The investigation results demonstrate that current error debugging support is far from sufficient in practice, where the located error causes for more than 50% of type errors are incorrect and the corresponding change suggestions are ineffective. I provide a fundamental understanding of why existing error debuggers do not work well for nonstructural type errors. To address this issue, a machine learning-base type error debugger, Learnskell, is developed. The evaluations results show that Learnskell could locate the error causes for nonstructural type errors several times more accurate than the state-of-the-art tools. In the second part, I study how to precisely infer error specifications in C. Error specifications, which specify the value range that each function returns to indicate failures, are widely used to check and propagate errors for the sake of reliability and security. I propose a general method, MLPEx, that can automatically generate error specifications by analyzing only the source code. MLPEx utilizes the idea of transfer learning, and therefore requires zero manual efforts to label data during the learning process. Error specifications are useful to detect bugs. As one example, I present how the results of MLPEx can be used to find new error handling bugs in real-world projects.},
note = {AAI28256855}
}

@article{10.1016/j.scico.2021.102713,
author = {Jain, Shivani and Saha, Anju},
title = {Improving performance with hybrid feature selection and ensemble machine learning techniques for code smell detection},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {212},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2021.102713},
doi = {10.1016/j.scico.2021.102713},
journal = {Sci. Comput. Program.},
month = dec,
numpages = {34},
keywords = {Stacking, Hybrid feature selection, Ensemble machine learning, Machine learning, Code smell}
}

@article{10.1007/s11704-020-9441-1,
author = {Sun, Xiaobing and Zhou, Tianchi and Wang, Rongcun and Duan, Yucong and Bo, Lili and Chang, Jianming},
title = {Experience report: investigating bug fixes in machine learning frameworks/libraries},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {6},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-020-9441-1},
doi = {10.1007/s11704-020-9441-1},
abstract = {Machine learning (ML) techniques and algorithms have been successfully and widely used in various areas including software engineering tasks. Like other software projects, bugs are also common in ML projects and libraries. In order to more deeply understand the features related to bug fixing in ML projects, we conduct an empirical study with 939 bugs from five ML projects by manually examining the bug categories, fixing patterns, fixing scale, fixing duration, and types of maintenance. The results show that (1) there are commonly seven types of bugs in ML programs; (2) twelve fixing patterns are typically used to fix the bugs in ML programs; (3) 68.80% of the patches belong to micro-scale-fix and small-scale-fix; (4) 66.77% of the bugs in ML programs can be fixed within one month; (5) 45.90% of the bug fixes belong to corrective activity from the perspective of software maintenance. Moreover, we perform a questionnaire survey and send them to developers or users of ML projects to validate the results in our empirical study. The results of our empirical study are basically consistent with the feedback from developers. The findings from the empirical study provide useful guidance and insights for developers and users to effectively detect and fix bugs in ML projects.},
journal = {Front. Comput. Sci.},
month = dec,
numpages = {16},
keywords = {questionnaire survey, empirical study, machine learning project, bug fixing}
}

@inproceedings{10.1145/3379597.3387461,
author = {Chen, Yang and Santosa, Andrew E. and Yi, Ang Ming and Sharma, Abhishek and Sharma, Asankhaya and Lo, David},
title = {A Machine Learning Approach for Vulnerability Curation},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387461},
doi = {10.1145/3379597.3387461},
abstract = {Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models' quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59% maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {32–42},
numpages = {11},
keywords = {self-training, open-source software, machine learning, classifiers ensemble, application security},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@mastersthesis{10.5555/AAI27837415,
author = {Kumar, Abhishek and Song, Myoungkyu and Hale, Matthew},
advisor = {Harvey, Siy,},
title = {Validating Machine Learning Applications with Metamorphic Testing},
year = {2020},
isbn = {9798662408258},
publisher = {University of Nebraska at Omaha},
abstract = {As machine learning applications have gone into mainstream use, it is increasingly important to find ways of assessing the reliability of their outputs. Unlike conventional software applications, there is no known approach for systematically testing machine learning applications. It is because of a lack of useful oracles due to the large input space for such applications. One of the most common uses of machine learning is image recognition. Robust image recognition needs to accurately identify images, even in the face of slight distortions to the image. Metamorphic testing is a promising approach for testing such applications. It makes use of successful test cases to generate additional test inputs. The main idea is that a small change in the input from a known test case should lead to a similarly small change (or no change) in the output. The changes to the test cases are constrained by metamorphic relations, which are defined as "expected relation among input and output of multiple executions of a program."In this study, we developed a metamorphic testing framework for testing the robustness of machine learning algorithms. We use a simple metamorphic relation: small changes in the input image should not lead to a different classification by the machine. We implemented five machine learning algorithms and trained them on the same dataset. We then apply a set of affine transformations to the test data to generate follow-up test data. We feed the follow-up test data to the algorithms and compare the output to the original outputs. We progressively increase the transformations to identify the ``breaking point" of the algorithms. Our results on three image databases indicate that machine learning algorithms are more sensitive to image shifting than other transformations such as rotation and shearing. Based on the results of the study, we provide recommendations to use our metamorphic properties to generate follow-up test cases for machine learning algorithms. Finally, we generated a new test dataset with these recommendations to assess the robustness of the algorithms. Within these recommended ranges, we find that deep learning algorithms like convolutional neural networks can outperform other algorithms.},
note = {AAI27837415}
}

@inproceedings{10.1145/3434581.3434619,
author = {Bao, Yang and Rui, Guosheng and Zhang, Song},
title = {A Unsupervised Learning System of Aeroengine Predictive Maintenance Based on Cluster Analysis},
year = {2020},
isbn = {9781450375764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434581.3434619},
doi = {10.1145/3434581.3434619},
abstract = {In this paper, a new cluster analysis system of predictive maintenance is proposed. The aim is to perform predictive maintenance on aero-engines under unsupervised conditions and reduce the cost of traditional periodic maintenance. Using this system and the proposed maintenance strategy to verify the subset from C-MAPSS dataset, the results show that the system obtains 40% extra uptime than regular maintenance. Under the theoretical limit, up to 60% of extra uptime can be obtained. The results show that the system can effectively increase uptime and reduce costs, which is a good supplement to the existing predictive maintenance.},
booktitle = {Proceedings of the 2020 International Conference on Aviation Safety and Information Technology},
pages = {187–191},
numpages = {5},
keywords = {warning system, unsupervised learning, cluster analysis, Predictive maintenance},
location = {Weihai City, China},
series = {ICASIT 2020}
}

@article{10.1504/ijiids.2020.109457,
author = {Anwar, Khalid and Siddiqui, Jamshed and Sohail, Shahab Saquib},
title = {Machine learning-based book recommender system: a survey and new perspectives},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {2–4},
issn = {1751-5858},
url = {https://doi.org/10.1504/ijiids.2020.109457},
doi = {10.1504/ijiids.2020.109457},
abstract = {The exponential growth of recommender systems research has drawn the attention of the scientific community recently. These systems are very useful in reducing information overload and providing users with the items of their need. The major areas where recommender systems have contributed significantly include e-commerce, online auction, and books and conference recommendation for academia and industrialists. Book recommender systems suggest books of interest to users according to their preferences and requirements. In this article, we have surveyed machine learning techniques which have been used in book recommender systems. Moreover, evaluation metrics applied to evaluate recommendation techniques is also studied. Six categories for book recommendation techniques have been identified and discussed which would enable the scientific community to lay a foundation of research in the concerned field. We have also proposed future perspectives to improve recommender system. We hope that researchers exploring recommendation technology in general and book recommendation in particular will be finding this work highly beneficial.},
journal = {Int. J. Intell. Inf. Database Syst.},
month = jan,
pages = {231–248},
numpages = {17},
keywords = {evaluation metrics, association rule mining, classification, machine learning, BRS, book recommender system}
}

@article{10.1016/j.inffus.2018.09.013,
author = {Praveen Kumar, D. and Amgoth, Tarachand and Annavarapu, Chandra Sekhara Rao},
title = {Machine learning algorithms for wireless sensor networks: A survey},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2018.09.013},
doi = {10.1016/j.inffus.2018.09.013},
journal = {Inf. Fusion},
month = sep,
pages = {1–25},
numpages = {25},
keywords = {Data aggregation, Network lifetime, Energy efficiency, Machine learning, Wireless sensor networks}
}

@inproceedings{10.1109/ISSRE.2014.35,
author = {Lu, Huihua and Kocaguneli, Ekrem and Cukic, Bojan},
title = {Defect Prediction between Software Versions with Active Learning and Dimensionality Reduction},
year = {2014},
isbn = {9781479960330},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISSRE.2014.35},
doi = {10.1109/ISSRE.2014.35},
abstract = {Accurate detection of defects prior to product release helps software engineers focus verification activities on defect prone modules, thus improving the effectiveness of software development. A common scenario is to use the defects from prior releases to build the prediction model for the upcoming release, typically through a supervised learning method. As software development is a dynamic process, fault characteristics in subsequent releases may vary. Therefore, supplementing the defect information from prior releases with limited information about the defects from the current release detected early seems to offer intuitive and practical benefits. We propose active learning as a way to automate the development of models which improve the performance of defect prediction between successive releases. Our results show that the integration of active learning with uncertainty sampling consistently outperforms the corresponding supervised learning approach. We further improve the prediction performance with feature compression techniques, where feature selection or dimensionality reduction is applied to defect data prior to active learning. We observe that dimensionality reduction techniques, particularly multidimensional scaling with random forest similarity, work better than feature selection due to their ability to identify and combine essential information in data set features. We present the improvements offered by this methodology through the prediction of defective modules in the three successive versions of Eclipse.},
booktitle = {Proceedings of the 2014 IEEE 25th International Symposium on Software Reliability Engineering},
pages = {312–322},
numpages = {11},
keywords = {Software defect prediction, Machine learning, Dimensionality reduction, Complexity measures, Active learning},
series = {ISSRE '14}
}

@article{10.1007/s10922-020-09512-5,
author = {Le, Duc C. and Zincir-Heywood, Nur},
title = {A Frontier: Dependable, Reliable and Secure Machine Learning for Network/System Management},
year = {2020},
issue_date = {Oct 2020},
publisher = {Plenum Press},
address = {USA},
volume = {28},
number = {4},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-020-09512-5},
doi = {10.1007/s10922-020-09512-5},
abstract = {Modern networks and systems pose many challenges to traditional management approaches. Not only the number of devices and the volume of network traffic are increasing exponentially, but also new network protocols and technologies require new techniques and strategies for monitoring controlling and managing up and coming networks and systems. Moreover, machine learning has recently found its successful applications in many fields due to its capability to learn from data to automatically infer patterns for network analytics. Thus, the deployment of machine learning in network and system management has become imminent. This work provides a review of the applications of machine learning in network and system management. Based on this review, we aim to present the current opportunities and challenges in and highlight the need for dependable, reliable and secure machine learning for network and system management.},
journal = {J. Netw. Syst. Manage.},
month = oct,
pages = {827–849},
numpages = {23},
keywords = {Secure machine learning, Reliable and dependable machine learning, Network and system management}
}

@inproceedings{10.1109/ICSE-NIER52604.2021.00022,
author = {Panichella, Annibale and Liem, Cynthia C. S.},
title = {What are we really testing in mutation testing for machine learning? a critical reflection},
year = {2021},
isbn = {9780738133249},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER52604.2021.00022},
doi = {10.1109/ICSE-NIER52604.2021.00022},
abstract = {Mutation testing is a well-established technique for assessing a test suite's quality by injecting artificial faults into production code. In recent years, mutation testing has been extended to machine learning (ML) systems, and deep learning (DL) in particular; researchers have proposed approaches, tools, and statistically sound heuristics to determine whether mutants in DL systems are killed or not. However, as we will argue in this work, questions can be raised to what extent currently used mutation testing techniques in DL are actually in line with the classical interpretation of mutation testing. We observe that ML model development resembles a test-driven development (TDD) process, in which a training algorithm ('programmer') generates a model (program) that fits the data points (test data) to labels (implicit assertions), up to a certain threshold. However, considering proposed mutation testing techniques for ML systems under this TDD metaphor, in current approaches, the distinction between production and test code is blurry, and the realism of mutation operators can be challenged. We also consider the fundamental hypotheses underlying classical mutation testing: the competent programmer hypothesis and coupling effect hypothesis. As we will illustrate, these hypotheses do not trivially translate to ML system development, and more conscious and explicit scoping and concept mapping will be needed to truly draw parallels. Based on our observations, we propose several action points for better alignment of mutation testing techniques for ML with paradigms and vocabularies of classical mutation testing.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {66–70},
numpages = {5},
keywords = {software testing, mutation testing, mutation operators, machine learning},
location = {Virtual Event, Spain},
series = {ICSE-NIER '21}
}

@inproceedings{10.1145/3318299.3318345,
author = {Li, ZhanJun and Shao, Yan},
title = {A Survey of Feature Selection for Vulnerability Prediction Using Feature-based Machine Learning},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318345},
doi = {10.1145/3318299.3318345},
abstract = {This paper summarized the basic process of software vulnerability prediction using feature-based machine learning for the first time. In addition to sorting out the related types and basis of vulnerability features definition, the advantages and disadvantages of different methods are compared. Finally, this paper analyzed the difficulties and challenges in this research field, and put forward some suggestions for future work.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {36–42},
numpages = {7},
keywords = {machine learning, feature, Software vulnerability prediction},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1109/COMPSAC.2015.58,
author = {Zhang, Yun and Lo, David and Xia, Xin and Sun, Jianling},
title = {An Empirical Study of Classifier Combination for Cross-Project Defect Prediction},
year = {2015},
isbn = {9781467365642},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2015.58},
doi = {10.1109/COMPSAC.2015.58},
abstract = {To help developers better allocate testing and debugging efforts, many software defect prediction techniques have been proposed in the literature. These techniques can be used to predict classes that are more likely to be buggy based on past history of buggy classes. These techniques work well as long as a sufficient amount of data is available to train a prediction model. However, there is rarely enough training data for new software projects. To deal with this problem, cross-project defect prediction, which transfers a prediction model trained using data from one project to another, has been proposed and is regarded as a new challenge for defect prediction. So far, only a few cross-project defect prediction techniques have been proposed. To advance the state-of-the-art, in this work, we investigate 7 composite algorithms, which integrate multiple machine learning classifiers, to improve cross-project defect prediction. To evaluate the performance of the composite algorithms, we perform experiments on 10 open source software systems from the PROMISE repository which contain a total of 5,305 instances labeled as defective or clean. We compare the composite algorithms with CODEP Logistic, which is the latest cross-project defect prediction algorithm proposed by Panichella et al., in terms of two standard evaluation metrics: cost effectiveness and F-measure. Our experiment results show that several algorithms outperform CODEP Logistic: Max performs the best in terms of F-measure and its average F-measure outperforms that of CODEP Logistic by 36.88%. Bagging J48 performs the best in terms of cost effectiveness and its average cost effectiveness outperforms that of CODEP Logistic by 15.34%.},
booktitle = {Proceedings of the 2015 IEEE 39th Annual Computer Software and Applications Conference - Volume 02},
pages = {264–269},
numpages = {6},
keywords = {Defect Prediction, Cross-Project, Classifier Combination},
series = {COMPSAC '15}
}

@article{10.1007/s11219-020-09508-z,
author = {Falcone, Yli\`{e}s and Mariani, Leonardo},
title = {Preface to the special section on improving software quality through formal methods},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09508-z},
doi = {10.1007/s11219-020-09508-z},
journal = {Software Quality Journal},
month = jun,
pages = {693–694},
numpages = {2}
}

@inproceedings{10.1007/978-3-030-41418-4_3,
author = {Nakajima, Shin},
title = {Distortion and Faults in Machine Learning Software},
year = {2019},
isbn = {978-3-030-41417-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-41418-4_3},
doi = {10.1007/978-3-030-41418-4_3},
abstract = {Machine learning software, deep neural networks (DNN) software in particular, discerns valuable information from a large dataset, a set of data, so as to synthesize approximate input-output relations. The outcomes of such DNN programs are dependent on the quality of both learning programs and datasets. However, the quality assurance of DNN software is difficult. The trained machine learning models, defining the functional behavior of the approximate relations, are unknown prior to its development, and the validation is conducted indirectly in terms of the prediction performance. This paper introduces a hypothesis that faults in DNN programs manifest themselves as distortions in trained machine learning models. Relative distortion degrees measured with appropriate observer functions may indicate that the programs have some hidden faults. The proposal is demonstrated with the cases of the MNIST dataset.},
booktitle = {Structured Object-Oriented Formal Language and Method: 9th International Workshop, SOFL+MSVL 2019, Shenzhen, China, November 5, 2019, Revised Selected Papers},
pages = {29–41},
numpages = {13},
location = {Shenzhen, China}
}

@inproceedings{10.1109/ISCID.2013.199,
author = {Xia, Ye and Yan, Guoying and Si, Qianran},
title = {A Study on the Significance of Software Metrics in Defect Prediction},
year = {2013},
isbn = {9780769550794},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISCID.2013.199},
doi = {10.1109/ISCID.2013.199},
abstract = {In the case of metrics-based software defect prediction, an intelligent selection of metrics plays an important role in improving the model performance. In this paper, we use different ways for feature selection and dimensionality reduction to determine the most important software metrics. Three different classifiers are utilized, namely Na\"{\i}ve Bayes, support vector machine and decision tree. On the publicly NASA data, a comparative experiment results show that instead of 22 or more metrics, less than 10 metrics can get better performance.},
booktitle = {Proceedings of the 2013 Sixth International Symposium on Computational Intelligence and Design - Volume 02},
pages = {343–346},
numpages = {4},
keywords = {software metric, feature selection, defect prediction, classifier},
series = {ISCID '13}
}

@inproceedings{10.1145/3349341.3349460,
author = {Wang, Xiaojuan and Wang, Defu and Zhang, Yong and Jin, Lei and Song, Mei},
title = {Unsupervised Learning for Log Data Analysis Based on Behavior and Attribute Features},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349460},
doi = {10.1145/3349341.3349460},
abstract = {In some special application environments, network fault can lead to loss of important information or even mission failures, resulting in unpredictable losses. Therefore, it has certain research significance and practical value to evaluate the network status and predict the possible faults before performing the key tasks. Based on the logs collected by the router board in the real network, this paper analyses the behavior type, attribute information and the corresponding status value, and detects the hidden fault or network attack, so as to provide early warning information for operators. We propose a deep neural network model utilizing Long Short-Term Memory (LSTM) to predict the current number of level-1 logs. By comparing the predicted number of level-1 logs, it can detect abnormal behavior such as a surge in the number of logs. What's more, we perform semantic analysis on attribute information to construct attribute syntax forest, which assists maintenance staff to monitor the system through key fingerprint information in the log. In addition, we adopt attribute information and status value to train the unsupervised learning algorithm models such as Isolation Forest, OneClassSVM and LocalOutlierFactor. What's more, this paper analyses the results to find out the causes of log surge, and to assist operators in subsequent maintenance of the system.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {510–518},
numpages = {9},
keywords = {Unsupervised Machine Learning, Network Fault, Log Analysis, LSTM},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3324884.3416617,
author = {Li, Ke and Xiang, Zilin and Chen, Tao and Tan, Kay Chen},
title = {BiLO-CPDP: bi-level programming for automated model discovery in cross-project defect prediction},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416617},
doi = {10.1145/3324884.3416617},
abstract = {Cross-Project Defect Prediction (CPDP), which borrows data from similar projects by combining a transfer learner with a classifier, have emerged as a promising way to predict software defects when the available data about the target project is insufficient. However, developing such a model is challenge because it is difficult to determine the right combination of transfer learner and classifier along with their optimal hyper-parameter settings. In this paper, we propose a tool, dubbed BiLO-CPDP, which is the first of its kind to formulate the automated CPDP model discovery from the perspective of bi-level programming. In particular, the bi-level programming proceeds the optimization with two nested levels in a hierarchical manner. Specifically, the upper-level optimization routine is designed to search for the right combination of transfer learner and classifier while the nested lower-level optimization routine aims to optimize the corresponding hyper-parameter settings. To evaluate BiLO-CPDP, we conduct experiments on 20 projects to compare it with a total of 21 existing CPDP techniques, along with its single-level optimization variant and Auto-Sklearn, a state-of-the-art automated machine learning tool. Empirical results show that BiLO-CPDP champions better prediction performance than all other 21 existing CPDP techniques on 70% of the projects, while being overwhelmingly superior to Auto-Sklearn and its single-level optimization variant on all cases. Furthermore, the unique bi-level formalization in BiLO-CPDP also permits to allocate more budget to the upper-level, which significantly boosts the performance.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {573–584},
numpages = {12},
keywords = {automated parameter optimization, classification techniques, configurable software and tool, cross-project defect prediction, transfer learning},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1016/j.infsof.2021.106573,
author = {Zhang, Fanlong and Khoo, Siau-cheng},
title = {An empirical study on clone consistency prediction based on machine learning},
year = {2021},
issue_date = {Aug 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {136},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106573},
doi = {10.1016/j.infsof.2021.106573},
journal = {Inf. Softw. Technol.},
month = aug,
numpages = {16},
keywords = {Machine learning, Software maintenance, Clone consistency prediction, Clone consistent change, Code clones}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Product Line Architecture, Machine Learning, Human-computer interaction},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3425174.3425226,
author = {Santos, Sebasti\~{a}o H. N. and da Silveira, Beatriz Nogueira Carvalho and Andrade, Stev\~{a}o A. and Delamaro, M\'{a}rcio and Souza, Simone R. S.},
title = {An Experimental Study on Applying Metamorphic Testing in Machine Learning Applications},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425226},
doi = {10.1145/3425174.3425226},
abstract = {Machine learning techniques have been successfully employed in various areas and, in particular, for the development of healthcare applications, aiming to support in more effective and faster diagnostics (such as cancer diagnosis). However, machine learning models may present uncertainties and errors. Errors in the training process, classification, and evaluation can generate incorrect results and, consequently, to wrong clinical decisions, reducing the professionals' confidence in the use of such techniques. Similar to other application domains, the quality should be guaranteed to produce more reliable models capable of assisting health professionals in their daily activities. Metamorphic testing can be an interesting option to validate machine learning applications. Using this testing approach is possible to define relationships that define changes to be made in the application's input data to identify faults. This paper presents an experimental study to evaluate the effectiveness of metamorphic testing to validate machine learning applications. A Machine learning application to verify breast cancer diagnostic was developed, using an available dataset composed of 569 samples whose data were taken from breast cancer images, and used as the software under test, in which the metamorphic testing was applied. The results indicate that metamorphic testing can be an alternative to support the validation of machine learning applications.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {98–106},
numpages = {9},
keywords = {Metamorphic Test, Machine Learning, Experimental Study},
location = {Natal, Brazil},
series = {SAST '20}
}

@inproceedings{10.1145/2025113.2025156,
author = {Lee, Taek and Nam, Jaechang and Han, DongGyun and Kim, Sunghun and In, Hoh Peter},
title = {Micro interaction metrics for defect prediction},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025156},
doi = {10.1145/2025113.2025156},
abstract = {There is a common belief that developers' behavioral interaction patterns may affect software quality. However, widely used defect prediction metrics such as source code metrics, change churns, and the number of previous defects do not capture developers' direct interactions. We propose 56 novel micro interaction metrics (MIMs) that leverage developers' interaction information stored in the Mylyn data. Mylyn is an Eclipse plug-in, which captures developers' interactions such as file editing and selection events with time spent. To evaluate the performance of MIMs in defect prediction, we build defect prediction (classification and regression) models using MIMs, traditional metrics, and their combinations. Our experimental results show that MIMs significantly improve defect classification and regression accuracy.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {311–321},
numpages = {11},
keywords = {mylyn, micro interaction metrics, defect prediction},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inproceedings{10.1145/2020390.2020406,
author = {Paikari, Elham and Sun, Bo and Ruhe, Guenther and Livani, Emadoddin},
title = {Customization support for CBR-based defect prediction},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020406},
doi = {10.1145/2020390.2020406},
abstract = {Background: The prediction performance of a case-based reasoning (CBR) model is influenced by the combination of the following parameters: (i) similarity function, (ii) number of nearest neighbor cases, (iii) weighting technique used for attributes, and (iv) solution algorithm. Each combination of the above parameters is considered as an instantiation of the general CBR-based prediction method. The selection of an instantiation for a new data set with specific characteristics (such as size, defect density and language) is called customization of the general CBR method.Aims: For the purpose of defect prediction, we approach the question which combinations of parameters works best at which situation. Three more specific questions were studied:(RQ1) Does one size fit all? Is one instantiation always the best?(RQ2) If not, which individual and combined parameter settings occur most frequently in generating the best prediction results?(RQ3) Are there context-specific rules to support the customization?Method: In total, 120 different CBR instantiations were created and applied to 11 data sets from the PROMISE repository. Predictions were evaluated in terms of their mean magnitude of relative error (MMRE) and percentage Pred(α) of objects fulfilling a prediction quality level α. For the third research question, dependency network analysis was performed.Results: Most frequent parameter options for CBR instantiations were neural network based sensitivity analysis (as the weighting technique), un-weighted average (as the solution algorithm), and maximum number of nearest neighbors (as the number of nearest neighbors). Using dependency network analysis, a set of recommendations for customization was provided.Conclusion: An approach to support customization is provided. It was confirmed that application of context-specific rules across groups of similar data sets is risky and produces poor results.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {16},
numpages = {10},
keywords = {case-based reasoning, customization, defect prediction, dependency network analysis, instantiation},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@article{10.1007/s11390-020-9668-1,
author = {Elmidaoui, Sara and Cheikhi, Laila and Idri, Ali and Abran, Alain},
title = {Machine Learning Techniques for Software Maintainability Prediction: Accuracy Analysis},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-9668-1},
doi = {10.1007/s11390-020-9668-1},
abstract = {Maintaining software once implemented on the end-user side is laborious and, over its lifetime, is most often considerably more expensive than the initial software development. The prediction of software maintainability has emerged as an important research topic to address industry expectations for reducing costs, in particular, maintenance costs. Researchers and practitioners have been working on proposing and identifying a variety of techniques ranging from statistical to machine learning (ML) for better prediction of software maintainability. This review has been carried out to analyze the empirical evidence on the accuracy of software product maintainability prediction (SPMP) using ML techniques. This paper analyzes and discusses the findings of 77 selected studies published from 2000 to 2018 according to the following criteria: maintainability prediction techniques, validation methods, accuracy criteria, overall accuracy of ML techniques, and the techniques offering the best performance. The review process followed the well-known systematic review process. The results show that ML techniques are frequently used in predicting maintainability. In particular, artificial neural network (ANN), support vector machine/regression (SVM/R), regression &amp; decision trees (DT), and fuzzy &amp; neuro fuzzy (FNF) techniques are more accurate in terms of PRED and MMRE. The N-fold and leave-one-out cross-validation methods, and the MMRE and PRED accuracy criteria are frequently used in empirical studies. In general, ML techniques outperformed non-machine learning techniques, e.g., regression analysis (RA) techniques, while FNF outperformed SVM/R, DT, and ANN in most experiments. However, while many techniques were reported superior, no specific one can be identified as the best.},
journal = {J. Comput. Sci. Technol.},
month = oct,
pages = {1147–1174},
numpages = {28},
keywords = {maintainability prediction, machine learning technique, accuracy value, accuracy criterion}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00049,
author = {Zhu, Junjie and Long, Teng and Memon, Atif},
title = {Automatically authoring regression tests for machine-learning based systems},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00049},
doi = {10.1109/ICSE-SEIP52600.2021.00049},
abstract = {Two key design characteristics of machine learning (ML) systems---their ever-improving nature, and learning-based emergent functional behavior---create a moving target, posing new challenges for authoring/maintaining functional regression tests. We identify four specific challenges and address them by developing a new general methodology to automatically author and maintain tests. In particular, we use the volume of production data to periodically refresh our large corpus of test inputs and expected outputs; we use perturbation of the data to obtain coverage-adequate tests; and we use clustering to help identify patterns of failures that are indicative of software bugs. We demonstrate our methodology on an ML-based context-aware Speller. Our coverage-adequate, approx. 1 million regression test cases, automatically authored and maintained for Speller (1) are virtually maintenance free, (2) detect a higher number of Speller failures than previous manually-curated tests, (3) have better coverage of previously unknown functional boundaries of the ML component, and (4) lend themselves to automatic failure triaging by clustering and prioritizing subcategories of tests with over-represented failures. We identify several systematic failure patterns which were due to previously undetected bugs in the Speller, e.g., (1) when the user misses the first letter in a short word, and (2) when the user mistakenly inserts a character in the last token of an address; these have since been fixed.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {374–383},
numpages = {10},
keywords = {spelling correction, ML-based testing, ML testing},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/2365324.2365335,
author = {Lu, Huihua and Cukic, Bojan},
title = {An adaptive approach with active learning in software fault prediction},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365335},
doi = {10.1145/2365324.2365335},
abstract = {Background: Software quality prediction plays an important role in improving the quality of software systems. By mining software metrics, predictive models can be induced that provide software managers with insights into quality problems they need to tackle as effectively as possible.Objective: Traditional, supervised learning approaches dominate software quality prediction. Resulting models tend to be project specific. On the other hand, in situations where there are no previous releases, supervised learning approaches are not very useful because large training data sets are needed to develop accurate predictive models.Method: This paper eases the limitations of supervised learning approaches and offers good prediction performance. We propose an adaptive approach in which supervised learning and active learning are coupled together. NaiveBayes classifier is used as the base learner.Results: We track the performance at each iteration of the adaptive learning algorithm and compare it with the performance of supervised learning. Our results show that proposed scheme provides good fault prediction performance over time, i.e., it eventually outperforms the corresponding supervised learning approach. On the other hand, adaptive learning classification approach reduces the variance in prediction performance in comparison with the corresponding supervised learning algorithm.Conclusion: The adaptive approach outperforms the corresponding supervised learning approach when both use Naive-Bayes as base learner. Additional research is needed to investigate whether this observation remains valid with other base classifiers.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {79–88},
numpages = {10},
keywords = {active learning, adaptive learning, software fault prediction},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@inproceedings{10.1007/978-3-642-02481-8_80,
author = {Santos, Igor and Nieves, Javier and Penya, Yoseba K. and Bringas, Pablo G.},
title = {Optimising Machine-Learning-Based Fault Prediction in Foundry Production},
year = {2009},
isbn = {9783642024801},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-02481-8_80},
doi = {10.1007/978-3-642-02481-8_80},
abstract = {Microshrinkages are known as probably the most difficult defects to avoid in high-precision foundry. The presence of this failure renders the casting invalid, with the subsequent cost increment. Modelling the foundry process as an expert knowledge cloud allows properly-trained machine learning algorithms to foresee the value of a certain variable, in this case the probability that a microshrinkage appears within a casting. Extending previous research that presented outstanding results with a Bayesian-network-based approach, we have adapted and tested an artificial neural network and the K-nearest neighbour algorithm for the same objective. Finally, we compare the obtained results and show that Bayesian networks are more suitable than the rest of the counterparts for the prediction of microshrinkages.},
booktitle = {Proceedings of the 10th International Work-Conference on Artificial Neural Networks: Part II: Distributed Computing, Artificial Intelligence, Bioinformatics, Soft Computing, and Ambient Assisted Living},
pages = {554–561},
numpages = {8},
keywords = {fault prediction, data mining, Machine learning},
location = {Salamanca, Spain},
series = {IWANN '09}
}

@article{10.1016/j.mejo.2021.105198,
author = {Abazyan, Suren and Melikyan, Vazgen},
title = {Enhanced pin-access prediction and design optimization with machine learning integration},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {116},
number = {C},
issn = {0026-2692},
url = {https://doi.org/10.1016/j.mejo.2021.105198},
doi = {10.1016/j.mejo.2021.105198},
journal = {Microelectron. J.},
month = oct,
numpages = {5},
keywords = {Prediction and optimization, Machine learning, Pin access}
}

@article{10.3233/JIFS-210229,
author = {Narendiranath Babu, T. and Singh, Prabhu Pal and Somesh, M. and Jha, Harshit Kumar and Rama Prabha, D. and Venkatesan, S. and Ramesh Babu, V.},
title = {Vibration analysis of planetary gearbox using empirical mode decomposition and automatic fault prediction using artificial neural network},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-210229},
doi = {10.3233/JIFS-210229},
abstract = {The planetary gearbox works on an epicyclic gear train consisting of sun gear meshed with planets gears and ring gear. It got advantages due to its large torque to weight ratio and reduced vibrations. It is mostly employed in analog clocks, automobile automatic gearbox, Lathe machines, and other heavy industries. Therefore, it was imperative to analyze the various faults occurring in a gearbox. Furthermore, come up with a method so that failures can be avoided at the early stage. It was also a reason why it became the field of intensive research. Moreover, the technology of neural networks emerged recently, where machine learning models are trained to detect uneven vibrations on their own. This attracted many researchers to perform the study to devise their own methods of prediction. The central concept of fault prediction by the neural network without human beings’ interference inspired this study. Most industries always wanted to know if their operation line is working fine or not. In this study, an attempt was made to apply the method of deep learning on one of the most critical gearboxes because of its components and functionality. A significant part of the study also involved filtering the vibration data obtained while testing. Comparative analysis of the variation of the peak of acceleration was performed for healthy and faulty conditions.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6407–6427},
numpages = {21},
keywords = {deep learning, neural networks, Planetary gearbox}
}

@inproceedings{10.1007/978-3-030-79463-7_36,
author = {Kawalerowicz, Marcin and Madeyski, Lech},
title = {Jaskier: A Supporting Software Tool for&nbsp;Continuous Build Outcome Prediction Practice},
year = {2021},
isbn = {978-3-030-79462-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79463-7_36},
doi = {10.1007/978-3-030-79463-7_36},
abstract = {Continuous Defect Prediction (CDP) is an assisting software development practice that combines Software Defect Prediction (SDP) with machine learning aided modelling and continuous developer feedback. Jaskier is a set of software tools developed under the supervision and with the participation of the authors of the article that implements a lightweight version of CDP called Continuous Build Outcome Prediction (CBOP). CBOP uses classification to label the possible build results based on historical data and metrics derived from the software repository. This paper contains a detailed description of the tool that was already started to be used in the production environment of a real software project where the CBOP practice is being evaluated.},
booktitle = {Advances and Trends in Artificial Intelligence. From Theory to Practice: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part II},
pages = {426–438},
numpages = {13},
keywords = {Continuous integration, Software defect prediction},
location = {Kuala Lumpur, Malaysia}
}

@article{10.5555/1991856.1991869,
author = {Jiang, Yuan and Li, Ming and Zhou, Zhi-Hua},
title = {Software defect detection with rocus},
year = {2011},
issue_date = {March 2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {2},
issn = {1000-9000},
abstract = {Software defect detection aims to automatically identify defective software modules for efficient software test in order to improve the quality of a software system. Although many machine learning methods have been successfully applied to the task, most of them fail to consider two practical yet important issues in software defect detection. First, it is rather difficult to collect a large amount of labeled training data for learning a well-performing model; second, in a software system there are usually much fewer defective modules than defect-free modules, so learning would have to be conducted over an imbalanced data set. In this paper, we address these two practical issues simultaneously by prcposing a novel semi-supervised learning approach named ROCUS. This method exploits the abundant unlabeled examples to improve the detection accuracy, as well as employs under-sampling to tackle the class-imbalance problem in the learning process. Experimental results of real-world software defect detection tasks show that ROCUS is effective for software defect cetection. Its performance is better than a semi-supervised learning method that ignores the class-imbalance nature of the task and a class-imbalance learning method that does not make effective use of unlabeled data.},
journal = {J. Comput. Sci. Technol.},
month = mar,
pages = {328–342},
numpages = {15},
keywords = {software defect detection, semi-supervised learning, machine learning, data mining, class-imbalance}
}

@inproceedings{10.1109/ICTAI.2011.155,
author = {Lounis, Hakim and Gayed, Tamer Fares and Boukadoum, Mounir},
title = {Machine-Learning Models for Software Quality: A Compromise between Performance and Intelligibility},
year = {2011},
isbn = {9780769545967},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2011.155},
doi = {10.1109/ICTAI.2011.155},
abstract = {Building powerful machine-learning assessment models is an important achievement of empirical software engineering research, but it is not the only one. Intelligibility of such models is also needed, especially, in a domain, software engineering, where exploration and knowledge capture is still a challenge. Several algorithms, belonging to various machine-learning approaches, are selected and run on software data collected from medium size applications. Some of these approaches produce models with very high quantitative performances, others give interpretable, intelligible, and "glass-box" models that are very complementary. We consider that the integration of both, in automated decision-making systems for assessing software product quality, is desirable to reach a compromise between performance and intelligibility.},
booktitle = {Proceedings of the 2011  IEEE 23rd International Conference on Tools with Artificial Intelligence},
pages = {919–921},
numpages = {3},
keywords = {software product quality, reusability, metrics, maintainability, machine-learning, assessment models},
series = {ICTAI '11}
}

@inproceedings{10.1145/3395363.3404364,
author = {Guo, Zichen and Liu, Jiawei and He, Tieke and Li, Zhuoyang and Zhangzhu, Peitian},
title = {TauJud: test augmentation of machine learning in judicial documents},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3404364},
doi = {10.1145/3395363.3404364},
abstract = {The booming of big data makes the adoption of machine learning ubiquitous in the legal field. As we all know, a large amount of test data can better reflect the performance of the model, so the test data must be naturally expanded. In order to solve the high cost problem of labeling data in natural language processing, people in the industry have improved the performance of text classification tasks through simple data amplification techniques. However, the data amplification requirements in the judgment documents are interpretable and logical, as observed from CAIL2018 test data with over 200,000 judicial documents. Therefore, we have designed a test augmentation tool called TauJud specifically for generating more effective test data with uniform distribution over time and location for model evaluation and save time in marking data. The demo can be found at https://github.com/governormars/TauJud.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {549–552},
numpages = {4},
keywords = {Test Augmentation, Machine Learning, Judicial Documents},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3468264.3468615,
author = {Dutta, Saikat and Shi, August and Misailovic, Sasa},
title = {FLEX: fixing flaky tests in machine learning projects by updating assertion bounds},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468615},
doi = {10.1145/3468264.3468615},
abstract = {Many machine learning (ML) algorithms are inherently random – multiple executions using the same inputs may produce slightly different results each time. Randomness impacts how developers write tests that check for end-to-end quality of their implementations of these ML algorithms. In particular, selecting the proper thresholds for comparing obtained quality metrics with the reference results is a non-intuitive task, which may lead to flaky test executions.  We present FLEX, the first tool for automatically fixing flaky tests due to algorithmic randomness in ML algorithms. FLEX fixes tests that use approximate assertions to compare actual and expected values that represent the quality of the outputs of ML algorithms. We present a technique for systematically identifying the acceptable bound between the actual and expected output quality that also minimizes flakiness. Our technique is based on the Peak Over Threshold method from statistical Extreme Value Theory, which estimates the tail distribution of the output values observed from several runs. Based on the tail distribution, FLEX updates the bound used in the test, or selects the number of test re-runs, based on a desired confidence level.  We evaluate FLEX on a corpus of 35 tests collected from the latest versions of 21 ML projects. Overall, FLEX identifies and proposes a fix for 28 tests. We sent 19 pull requests, each fixing one test, to the developers. So far, 9 have been accepted by the developers.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {603–614},
numpages = {12},
keywords = {Machine Learning, Flaky tests, Extreme Value Theory},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1109/ICTAI.2010.27,
author = {Khoshgoftaar, Taghi M. and Gao, Kehan and Seliya, Naeem},
title = {Attribute Selection and Imbalanced Data: Problems in Software Defect Prediction},
year = {2010},
isbn = {9780769542638},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2010.27},
doi = {10.1109/ICTAI.2010.27},
abstract = {The data mining and machine learning community is often faced with two key problems: working with imbalanced data and selecting the best features for machine learning. This paper presents a process involving a feature selection technique for selecting the important attributes and a data sampling technique for addressing class imbalance. The application domain of this study is software engineering, more specifically, software quality prediction using classification models. When using feature selection and data sampling together, different scenarios should be considered. The four possible scenarios are: (1) feature selection based on original data, and modeling (defect prediction) based on original data; (2) feature selection based on original data, and modeling based on sampled data; (3) feature selection based on sampled data, and modeling based on original data; and (4) feature selection based on sampled data, and modeling based on sampled data. The research objective is to compare the software defect prediction performances of models based on the four scenarios. The case study consists of nine software measurement data sets obtained from the PROMISE software project repository. Empirical results suggest that feature selection based on sampled data performs significantly better than feature selection based on original data, and that defect prediction models perform similarly regardless of whether the training data was formed using sampled or original data.},
booktitle = {Proceedings of the 2010 22nd IEEE International Conference on Tools with Artificial Intelligence - Volume 01},
pages = {137–144},
numpages = {8},
keywords = {software measurements, feature selection, defect prediction, data sampling},
series = {ICTAI '10}
}

@inbook{10.5555/3454287.3455252,
author = {Jeong, Jisoo and Lee, Seungeui and Kim, Jeesoo and Kwak, Nojun},
title = {Consistency-based semi-supervised learning for object detection},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Specifically, the consistency constraint is applied not only for object classification but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage and two-stage detectors and the results show the effectiveness of our method.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {965},
numpages = {10}
}

@article{10.1016/j.asoc.2016.08.025,
author = {Erturk, Ezgi and Akcapinar Sezer, Ebru},
title = {Iterative software fault prediction with a hybrid approach},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.025},
doi = {10.1016/j.asoc.2016.08.025},
abstract = {Display Omitted To make software fault prediction (SFP) more beneficial, it should be into service at the beginning of the project.A novel prediction methodology based on existing methods (i.e. FIS, ANN) are proposed here.Version based development of software projects are considered to design an iterative prediction approach.Proposed methodology is developed as Eclipse plugin.Experiments show that proposed methodology gives promising results to use SFP in daily routine of software development phases. In this study, we consider a software fault prediction task that can assist a developer during the lifetime of a project. We aim to improve the performance of software fault prediction task while keeping it as applicable. Initial predictions are constructed by Fuzzy Inference Systems (FISs), whereas subsequent predictions are performed by data-driven methods. In this paper, an Artificial Neural Network and Adaptive Neuro Fuzzy Inference System are employed. We propose an iterative prediction model that begins with a FIS when no data are available for the software project and continues with a data-driven method when adequate data become available. To prove the usability of this iterative prediction approach, software fault prediction experiments are performed using expert knowledge for the initial version and information about previous versions for subsequent versions. The datasets employed in this paper comprise different versions of Ant, jEdit, Camel, Xalan, Log4j and Lucene projects from the PROMISE repository. The metrics of the models are common object-oriented metrics, such as coupling between objects, weighted methods per class and response for a class. The results of the models are evaluated according to the receiver operating characteristics with the area under the curve approach. The results indicate that the iterative software fault prediction is successful and can be transformed into a tool that can automatically locate fault-prone modules due to its well-organized information flow. We also implement the proposed methodology as a plugin for the Eclipse environment.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1020–1033},
numpages = {14},
keywords = {Software fault prediction, Iterative prediction, Fuzzy inference systems, Artificial neural network, Adaptive neuro fuzzy inference system}
}

@article{10.1016/j.infsof.2019.106214,
author = {Alsolai, Hadeel and Roper, Marc},
title = {A systematic literature review of machine learning techniques for software maintainability prediction},
year = {2020},
issue_date = {Mar 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {119},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106214},
doi = {10.1016/j.infsof.2019.106214},
journal = {Inf. Softw. Technol.},
month = mar,
numpages = {25},
keywords = {Dataset, Metric, Machine learning, Software maintainability prediction, Systematic literature review}
}

@inproceedings{10.1109/ICSE43902.2021.00100,
author = {Velez, Miguel and Jamshidi, Pooyan and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {White-Box Analysis over Machine Learning: Modeling Performance of Configurable Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00100},
doi = {10.1109/ICSE43902.2021.00100},
abstract = {Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly accurate performance-influence models to the most accurate and expensive black-box approach, but at a reduced cost and with additional benefits from interpretable and local models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1072–1084},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1109/ACIT-CSI.2015.104,
author = {Kawata, Kazuya and Amasaki, Sousuke and Yokogawa, Tomoyuki},
title = {Improving Relevancy Filter Methods for Cross-Project Defect Prediction},
year = {2015},
isbn = {9781467396424},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ACIT-CSI.2015.104},
doi = {10.1109/ACIT-CSI.2015.104},
abstract = {Context: Cross-project defect prediction (CPDP)research has been popular. One of the techniques for CPDP isa relevancy filter which utilizes clustering algorithms to selecta useful subset of the cross-project data. Their performanceheavily relies on the quality of clustering, and using an advancedclustering algorithm instead of simple ones used in the past studiescan contribute to the performance improvement. Objective:To propose and examine a new relevancy filter method usingan advanced clustering method DBSCAN (Density-Based SpatialClustering). Method: We conducted an experiment that examinedthe predictive performance of the proposed method. Theexperiments compared three relevancy filter methods, namely,Burak-filter, Peters-filter, and the proposed method with 56project data and four prediction models. Results: The predictiveperformance measures supported the proposed method. It wasbetter than Burak-filter and Peters-filter in terms of AUC andg-measure. Conclusion: The proposed method achieved betterprediction than the conventional methods. The results suggestedthat exploring advanced clustering algorithms could contributeto cross-project defect prediction.},
booktitle = {Proceedings of the 2015 3rd International Conference on Applied Computing and Information Technology/2nd International Conference on Computational Science and Intelligence},
pages = {2–7},
numpages = {6},
series = {ACIT-CSI '15}
}

@inproceedings{10.5555/2818754.2818850,
author = {Ghotra, Baljinder and McIntosh, Shane and Hassan, Ahmed E.},
title = {Revisiting the impact of classification techniques on the performance of defect prediction models},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Defect prediction models help software quality assurance teams to effectively allocate their limited resources to the most defect-prone software modules. A variety of classification techniques have been used to build defect prediction models ranging from simple (e.g., logistic regression) to advanced techniques (e.g., Multivariate Adaptive Regression Splines (MARS)). Surprisingly, recent research on the NASA dataset suggests that the performance of a defect prediction model is not significantly impacted by the classification technique that is used to train it. However, the dataset that is used in the prior study is both: (a) noisy, i.e., contains erroneous entries and (b) biased, i.e., only contains software developed in one setting. Hence, we set out to replicate this prior study in two experimental settings. First, we apply the replicated procedure to the same (known-to-be noisy) NASA dataset, where we derive similar results to the prior study, i.e., the impact that classification techniques have appear to be minimal. Next, we apply the replicated procedure to two new datasets: (a) the cleaned version of the NASA dataset and (b) the PROMISE dataset, which contains open source software developed in a variety of settings (e.g., Apache, GNU). The results in these new datasets show a clear, statistically distinct separation of groups of techniques, i.e., the choice of classification technique has an impact on the performance of defect prediction models. Indeed, contrary to earlier research, our results suggest that some classification techniques tend to produce defect prediction models that outperform others.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {789–800},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.1007/s00521-021-05892-0,
author = {Rajakumar, M. P. and Ramya, J. and Maheswari, B. Uma},
title = {Health monitoring and fault prediction using a lightweight deep convolutional neural network optimized by Levy flight optimization algorithm},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {19},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05892-0},
doi = {10.1007/s00521-021-05892-0},
abstract = {Agricultural machines (AMs) refer to equipment usually used in agriculture such as tractors, hand tools, and power tools. It reduces the labor work, increases farms produce, enhances goods quality, and reduces farming time and cost-saving. However, the faults in the fuel system, blades, engine of the AM will often result in degraded vehicle performance, compromising the vehicle’s efficiency and strength. To overcome these problems, fault detection algorithms are developed to identify the faults even before they occur with high classification accuracy. The deep convolutional neural network (DCNN) is a popular deep learning model that offers a high classification recognition rate, and it is widely adopted in similar fields for monitoring the health status of machines. Very few state-of-the-art works are available to identify the health state of agricultural machines using deep learning techniques and extracting the acoustic features from an audio recording. The acoustic signal-based agricultural machine health monitoring and fault prediction model using smartphones is a cost-effective option that is deployed in this proposed work. To optimize the network structure of the DCNN, this paper proposes a Levy flight optimization algorithm (LFOA). The DCNN-LFOA model is implemented on the smartphone’s on-board device (OBD) along with the health monitoring application. The LFOA algorithm minimizes the number of neurons in the DCNN hidden layer and the number of input features from the audio recordings and enhances the classification accuracy. The LFOA algorithm provides the optimal solution which is essential in developing a lightweight DCNN model to implement in the edge processor (smartphone). The experimental results prove that the proposed model gives improved accuracy for the six faults to be classified and serves as a new research model to identify the health condition of the vehicles.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {12513–12534},
numpages = {22},
keywords = {Short-term audio signals, Fault detection, Levy flight optimization algorithm, Convolutional neural network, Agricultural vehicles}
}

@inproceedings{10.1145/1868328.1868336,
author = {Mende, Thilo},
title = {Replication of defect prediction studies: problems, pitfalls and recommendations},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868336},
doi = {10.1145/1868328.1868336},
abstract = {Background: The main goal of the PROMISE repository is to enable reproducible, and thus verifiable or refutable research. Over time, plenty of data sets became available, especially for defect prediction problems.Aims: In this study, we investigate possible problems and pitfalls that occur during replication. This information can be used for future replication studies, and serve as a guideline for researchers reporting novel results.Method: We replicate two recent defect prediction studies comparing different data sets and learning algorithms, and report missing information and problems.Results: Even with access to the original data sets, replicating previous studies may not lead to the exact same results. The choice of evaluation procedures, performance measures and presentation has a large influence on the reproducibility. Additionally, we show that trivial and random models can be used to identify overly optimistic evaluation measures.Conclusions: The best way to conduct easily reproducible studies is to share all associated artifacts, e.g. scripts and programs used. When this is not an option, our results can be used to simplify the replication task for other researchers.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {5},
numpages = {10},
keywords = {defect prediction model, replication},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@article{10.2478/acss-2020-0017,
author = {Abramov, Kirill and Grundspenkis, Janis},
title = {Suitability Determination of Machine Learning Techniques for the Operational Quality Assessment of Geophysical Survey Results},
year = {2020},
issue_date = {Dec 2020},
publisher = {Walter de Gruyter GmbH},
address = {Berlin, DEU},
volume = {25},
number = {2},
issn = {2255-8691},
url = {https://doi.org/10.2478/acss-2020-0017},
doi = {10.2478/acss-2020-0017},
abstract = {Well logging, also known as a geophysical survey, is one of the main components of a nuclear fuel cycle. This survey follows directly after the drilling process, and the operational quality assessment of its results is a very serious problem. Any mistake in this survey can lead to the culling of the whole well. This paper examines the feasibility of applying machine learning techniques to quickly assess the well logging quality results. The studies were carried out by a reference well modelling for the selected uranium deposit of the Republic of Kazakhstan and further comparing it with the results of geophysical surveys recorded earlier. The parameters of the geophysical methods and the comparison rules for them were formulated after the reference well modelling process. The classification trees and the artificial neural networks were used during the research process and the results obtained for both methods were compared with each other. The results of this paper may be useful to the enterprises engaged in the geophysical well surveys and data processing obtained during the logging process.},
journal = {Appl. Comput. Syst.},
month = dec,
pages = {153–162},
numpages = {10},
keywords = {well logging, neural networks, machine learning, Classification trees}
}

@inproceedings{10.1145/1985793.1985859,
author = {Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
title = {Dealing with noise in defect prediction},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985859},
doi = {10.1145/1985793.1985859},
abstract = {Many software defect prediction models have been built using historical defect data obtained by mining software repositories (MSR). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises.This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding FP (false positive) or FN (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20%-35% of both FP and FN noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {481–490},
numpages = {10},
keywords = {noise resistance, defect prediction, data quality, buggy files, buggy changes},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@article{10.1109/TSE.2010.90,
author = {Song, Qinbao and Jia, Zihan and Shepperd, Martin and Ying, Shi and Liu, Jin},
title = {A General Software Defect-Proneness Prediction Framework},
year = {2011},
issue_date = {May 2011},
publisher = {IEEE Press},
volume = {37},
number = {3},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2010.90},
doi = {10.1109/TSE.2010.90},
abstract = {BACKGROUND—Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE—We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD—The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. RESULTS—The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. CONCLUSIONS—Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework.},
journal = {IEEE Trans. Softw. Eng.},
month = may,
pages = {356–370},
numpages = {15},
keywords = {software defect-proneness prediction, scheme evaluation., machine learning, Software defect prediction}
}

@inproceedings{10.5555/3049877.3049895,
author = {Mezouar, Mariam El and Zhang, Feng and Zou, Ying},
title = {Local versus global models for effort-aware defect prediction},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software entities (e.g., files or classes) do not have the same density of defects and therefore do not require the same amount of effort for inspection. With limited resources, it is critical to reveal as many defects as possible. To satisfy such need, effort-aware defect prediction models have been proposed. However, the performance of prediction models is commonly affected by a large amount of possible variability in the training data. Prior studies have inspected whether using a subset of the original training data (i.e., local models) could improve the performance of prediction models in the context of defect prediction and effort estimation in comparison with global models (i.e., trained on the whole dataset). However, no consensus has been reached and the comparison has not been performed in the context of effort-aware defect prediction.In this study, we compare local and global effort-aware defect prediction models using 15 projects from the widely used AEEEM and PROMISE datasets. We observe that although there is at least one local model that can outperform the global model, there always exists another local model that performs very poorly in all the projects. We further find that the poor performing local model is built on the subset of the training set with a low ratio of defective entities. By excluding such subset of the training set and building a local effort-aware model with the remaining training set, the local model usually underperforms the global model in 11 out of the 15 studied projects. A close inspection on the failure of local effort-aware models reveals that the major challenge comes from defective entities with small size (i.e., few lines of code), as such entities tend to be correctly predicted by the global model but missed by the local model. Further work should pay special attention to the small but defective entities.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {178–187},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@article{10.1007/s10664-011-9173-9,
author = {D'Ambros, Marco and Lanza, Michele and Robbes, Romain},
title = {Evaluating defect prediction approaches: a benchmark and an extensive comparison},
year = {2012},
issue_date = {August    2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4–5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-011-9173-9},
doi = {10.1007/s10664-011-9173-9},
abstract = {Reliably predicting software defects is one of the holy grails of software engineering. Researchers have devised and implemented a plethora of defect/bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available dataset consisting of several software systems, and provide an extensive comparison of well-known bug prediction approaches, together with novel approaches we devised. We evaluate the performance of the approaches using different performance indicators: classification of entities as defect-prone or not, ranking of the entities, with and without taking into account the effort to review an entity. We performed three sets of experiments aimed at (1) comparing the approaches across different systems, (2) testing whether the differences in performance are statistically significant, and (3) investigating the stability of approaches across different learners. Our results indicate that, while some approaches perform better than others in a statistically significant manner, external validity in defect prediction is still an open problem, as generalizing results to different contexts/learners proved to be a partially unsuccessful endeavor.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {531–577},
numpages = {47},
keywords = {Source code metrics, Defect prediction, Change metrics}
}

@inproceedings{10.1109/CEC48606.2020.9185555,
author = {Brester, Christina and Niska, Harri and Ciszek, Robert and Kolehmainen, Mikko},
title = {Weather-based Fault Prediction in Electricity Networks with Artificial Neural Networks},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC48606.2020.9185555},
doi = {10.1109/CEC48606.2020.9185555},
abstract = {Predicting weather-related outages in electricity networks is an important issue for distribution system operators. In this study, we apply a data-driven approach and train artificial neural networks to predict faults in the electricity network. In our experiments, we utilize the meteorological data and fault records collected for the period of1.1.2011-31.12.2013 in central Finland. Assuming that there might be long-term dependencies between weather conditions and faults in the network, we investigate simple recurrent neural networks, long short-term memory networks, and traditional multilayer perceptrons. Taking into account the meteorological observations preceding faults and varying this period from several hours to several days, we found that 6 hours prior to faults included the sufficient information to make accurate predictions. Also, there was no need in more complicated recurrent neural networks as multilayer perceptron was able to predict events with the large number of faults more accurately. Besides, while forecasting all types of faults and wind-related faults only, oversampling allowed the model to predict rare high peaks.},
booktitle = {2020 IEEE Congress on Evolutionary Computation (CEC)},
pages = {1–8},
numpages = {8},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s10664-015-9376-6,
author = {Herzig, Kim and Just, Sascha and Zeller, Andreas},
title = {The impact of tangled code changes on defect prediction models},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9376-6},
doi = {10.1007/s10664-015-9376-6},
abstract = {When interacting with source control management system, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing version histories, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found between 7 % and 20 % of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6 % of all source files are incorrectly associated with bug reports. These incorrect bug file associations seem to not significantly impact models classifying source files to have at least one bug or no bugs. But our experiments show that untangling tangled code changes can result in more accurate regression bug prediction models when compared to models trained and tested on tangled bug datasets--in our experiments, the statistically significant accuracy improvements lies between 5 % and 200 %. We recommend better change organization to limit the impact of tangled changes.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {303–336},
numpages = {34},
keywords = {Untangling, Defect prediction, Data noise}
}

@article{10.1016/j.eswa.2014.10.025,
author = {Erturk, Ezgi and Sezer, Ebru Akcapinar},
title = {A comparison of some soft computing methods for software fault prediction},
year = {2015},
issue_date = {March 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.10.025},
doi = {10.1016/j.eswa.2014.10.025},
abstract = {Software fault prediction is implemented with ANN, SVM and ANFIS.First ANFIS implementation is applied to solve fault prediction problem.Parameters are discussed in neuro fuzzy approach.Experiments show that the application of ANFIS to the software fault prediction problem is highly reasonable. The main expectation from reliable software is the minimization of the number of failures that occur when the program runs. Determining whether software modules are prone to fault is important because doing so assists in identifying modules that require refactoring or detailed testing. Software fault prediction is a discipline that predicts the fault proneness of future modules by using essential prediction metrics and historical fault data. This study presents the first application of the Adaptive Neuro Fuzzy Inference System (ANFIS) for the software fault prediction problem. Moreover, Artificial Neural Network (ANN) and Support Vector Machine (SVM) methods, which were experienced previously, are built to discuss the performance of ANFIS. Data used in this study are collected from the PROMISE Software Engineering Repository, and McCabe metrics are selected because they comprehensively address the programming effort. ROC-AUC is used as a performance measure. The results achieved were 0.7795, 0.8685, and 0.8573 for the SVM, ANN and ANFIS methods, respectively.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1872–1879},
numpages = {8},
keywords = {Support Vector Machines, Software fault prediction, McCabe metrics, Artificial Neural Networks, Adaptive neuro fuzzy systems}
}

@article{10.1007/s11219-008-9053-8,
author = {Kastro, Yomi and Bener, Ay\c{s}e Basar},
title = {A defect prediction method for software versioning},
year = {2008},
issue_date = {December  2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-008-9053-8},
doi = {10.1007/s11219-008-9053-8},
abstract = {New methodologies and tools have gradually made the life cycle for software development more human-independent. Much of the research in this field focuses on defect reduction, defect identification and defect prediction. Defect prediction is a relatively new research area that involves using various methods from artificial intelligence to data mining. Identifying and locating defects in software projects is a difficult task. Measuring software in a continuous and disciplined manner provides many advantages such as the accurate estimation of project costs and schedules as well as improving product and process qualities. This study aims to propose a model to predict the number of defects in the new version of a software product with respect to the previous stable version. The new version may contain changes related to a new feature or a modification in the algorithm or bug fixes. Our proposed model aims to predict the new defects introduced into the new version by analyzing the types of changes in an objective and formal manner as well as considering the lines of code (LOC) change. Defect predictors are helpful tools for both project managers and developers. Accurate predictors may help reducing test times and guide developers towards implementing higher quality codes. Our proposed model can aid software engineers in determining the stability of software before it goes on production. Furthermore, such a model may provide useful insight for understanding the effects of a feature, bug fix or change in the process of defect detection.},
journal = {Software Quality Journal},
month = dec,
pages = {543–562},
numpages = {20},
keywords = {Software defects, Neural networks, Defect prediction}
}

@inproceedings{10.1145/2499393.2499397,
author = {Tass\'{e}, Jos\'{e}e},
title = {Using code change types in an analogy-based classifier for short-term defect prediction},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499397},
doi = {10.1145/2499393.2499397},
abstract = {Current approaches for defect prediction usually analyze files (or modules) and their development as work is done on a given release, to predict post-release defects. What is missing is an approach for predicting bugs to be detected in a more short-term interval, even within the development of a particular version. In this paper, we propose a defect predictor that looks into change bursts in a given file, analyzing the number of changes and their types, and then predict whether the file is likely to have a bug found within the next 3 months after that change burst. An analogy-based classifier is used for this task: the prediction is made based on comparisons with similar change bursts that occurred in other files. New metrics are described to capture the change type of a file (e.g., small local change, massive change all in one place, multiple changes scattered throughout the file).},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {5},
numpages = {4},
keywords = {analogy-based classifier, change burst, change type metrics, defect prediction, short-term prediction},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@inproceedings{10.1007/978-3-030-73128-1_5,
author = {EzzatiKarami, Mahtab and Madhavji, Nazim H.},
title = {Automatically Classifying Non-functional Requirements with Feature Extraction and Supervised Machine Learning Techniques: A Research Preview},
year = {2021},
isbn = {978-3-030-73127-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73128-1_5},
doi = {10.1007/978-3-030-73128-1_5},
abstract = {Context and Motivation: In large projects, extracting the relevant NFR-information as per the stakeholder’s responsibility and needs can be time-consuming and challenging. Question/Problem: Classification of NFRs is one way to mitigate this problem. However, because of the size and complexity of the SRS, the manual classification of NFRs is considered time-consuming, labour-intensive, and error-prone. An automated solution is needed that provides a reliable and efficient classification of NFRs. Principal ideas/results: Using natural language processing and supervised machine learning (SML) algorithms, we investigate feature extraction techniques (i.e., POS-tagging based, BoW, and TF-IDF) to assess their efficacy in automated classification, in conjunction with the SML algorithms (such as: SVM, SGD SVM, LR, DT, Bagging DT, Extra Tree, RF, GNB, MNB, and BNB). Contribution: The proposed combinations: (i) SVM with TF-IDF, (ii) LR with POS and BoW, and (iii) MNB with BoW, all achieve precision and recall values greater than 0.85, and process execution time of less than 0.1 s. Comparison with related work is favourable as is preliminary validation using an industry dataset.},
booktitle = {Requirements Engineering:  Foundation  for Software Quality: 27th International Working Conference, REFSQ 2021, Essen, Germany, April 12–15, 2021, Proceedings},
pages = {71–78},
numpages = {8},
keywords = {Non-functional requirements, Classification, Supervised Machine Learning, Feature extraction}
}

@inproceedings{10.1109/ASE.2013.6693126,
author = {Scanniello, Giuseppe and Gravino, Carmine and Marcus, Andrian and Menzies, Tim},
title = {Class level fault prediction using software clustering},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693126},
doi = {10.1109/ASE.2013.6693126},
abstract = {Defect prediction approaches use software metrics and fault data to learn which software properties associate with faults in classes. Existing techniques predict fault-prone classes in the same release (intra) or in a subsequent releases (inter) of a subject software system. We propose an intra-release fault prediction technique, which learns from clusters of related classes, rather than from the entire system. Classes are clustered using structural information and fault prediction models are built using the properties of the classes in each cluster. We present an empirical investigation on data from 29 releases of eight open source software systems from the PROMISE repository, with predictors built using multivariate linear regression. The results indicate that the prediction models built on clusters outperform those built on all the classes of the system.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {640–645},
numpages = {6},
keywords = {software clustering, fault prediction, empirical study},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1145/1985793.1985950,
author = {Nguyen, Tung Thanh and Nguyen, Tien N. and Phuong, Tu Minh},
title = {Topic-based defect prediction (NIER track)},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985950},
doi = {10.1145/1985793.1985950},
abstract = {Defects are unavoidable in software development and fixing them is costly and resource-intensive. To build defect prediction models, researchers have investigated a number of factors related to the defect-proneness of source code, such as code complexity, change complexity, or socio-technical factors. In this paper, we propose a new approach that emphasizes on technical concerns/functionality of a system. In our approach, a software system is viewed as a collection of software artifacts that describe different technical concerns/-aspects. Those concerns are assumed to have different levels of defect-proneness, thus, cause different levels of defectproneness to the relevant software artifacts. We use topic modeling to measure the concerns in source code, and use them as the input for machine learning-based defect prediction models. Preliminary result on Eclipse JDT shows that the topic-based metrics have high correlation to the number of bugs (defect-proneness), and our topic-based defect prediction has better predictive performance than existing state-of-the-art approaches.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {932–935},
numpages = {4},
keywords = {topic modeling, defect prediction},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@phdthesis{10.5555/2125736,
author = {Vivanco, Rodrigo},
title = {Improving predictive models of software quality using search-based metric selection and decision trees},
year = {2010},
isbn = {9780494711217},
publisher = {University of Manitoba},
address = {CAN},
abstract = {Software engineering is a human centric endeavour where the majority of the effort is spent understanding and modifying source code. The ability to automatically identify potentially problematic components would assist developers and project managers to make the best use of limited resources when taking mitigating actions such as detailed code inspections, more exhaustive testing, refactoring or reassignment to more experienced developers. Predictive models can be used to discover poor quality components via structural information from the design and/or source code. There exist many traditional source code metrics to capture the size, algorithmic complexity, cohesion and coupling of modules. Object-oriented systems have introduced additional structural concepts such as encapsulation and inheritance, providing even more ways to capture and measure different aspects of coupling, cohesion, complexity and size. An important question to answer is:  Which metrics should be used with a model for a particular predictive objective__ __ In machine learning, large dimensional feature spaces may contain inputs that are irrelevant or redundant. Feature selection is the process of identifying a subset of features that improve a classifier's discriminatory performance. In analysis of software system, the features used are source code metrics. In this work, an analysis tool has been developed that implements a parallel genetic algorithm (GA) as a search-based metric selection strategy. A comparative study has been carried out between GA, the Chidamber and Kemerer metrics suite (for an objected-oriented dataset), and principal component analysis (PCA) as metric selection strategies with different datasets. Program comprehension is important for programmers and the first dataset evaluated uses source code inspections as a subjective measure of cognitively complexity that degrade program understanding. Predicting the likely location of system failures is important in order to improve a system's reliability. The second dataset uses an objective measure of faults found in system modules in order to predict fault-prone components. The aim of this research has been to advance the current state of the art in predictive models of software quality by exploring the efficacy of a search-based approach in selecting appropriate metrics subsets for various predictive objectives. Results show that a search-based strategy, such as GA, performs well as a metric selection strategy when used with a linear discriminant analysis classifier. When predicting cognitive complex classes, GA achieved an F-value of 0.845 compared to an F-value of 0.740 using principal component analysis, and 0.750 when using only the CK metrics suite. By examining the GA chosen metrics with a white box predictive model (decision tree classifier) additional insights into the structural properties of a system that degrade product quality were observed. Source code metrics have been designed for human understanding and program comprehension and predictive models for cognitive complexity perform well with just source code metrics. Models for fault prone modules do not perform as well when using only source code metrics and need additional non-source code information, such module modification history or testing history.},
note = {AAINR71121}
}

@article{10.1007/s42979-021-00872-6,
author = {Sakhrawi, Zaineb and Sellami, Asma and Bouassida, Nadia},
title = {Software Enhancement Effort Prediction Using Machine-Learning Techniques: A Systematic Mapping Study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {6},
url = {https://doi.org/10.1007/s42979-021-00872-6},
doi = {10.1007/s42979-021-00872-6},
abstract = {Accurate prediction of software enhancement effort is a key success in software project management. To increase the accuracy of estimates, several proposals used machine-learning (ML) techniques for predicting the software project effort. However, there is no clear evidence for determining which techniques to select for predicting more accurate effort within the context of enhancement projects. This paper aims to present a systematic mapping study (SMS) related to the use of ML techniques for predicting software enhancement effort (SEME). A SMS was performed by reviewing relevant papers from 1995 through 2020. We followed well-known guidelines. We selected 30 relevant studies; 19 from journals and 11 conferences proceedings through 4 search engines. Some of the key findings indicate that (1) there is relatively little activity in the area of SEME, (2) most of the successful studies cited focused on regression problems for enhancement maintenance effort prediction, (3) SEME is the dependent variable the most commonly used in software enhancement project planning, and the enhancement size (or the functional change size) is the most used independent variables, (4) several private datasets were used in the selected studies, and there is a growing demand for the use of commonly published datasets, and (5) only single models were employed for SEME prediction. Results indicate that much more work is needed to develop repositories in all prediction models. Based on the findings obtained in this SMS, estimators should be aware that SEME using ML techniques as part of non-algorithmic models demonstrated increased accuracy prediction over the algorithmic models. The use of ML techniques generally provides a reasonable accuracy when using the enhancement functional size as independent variables.},
journal = {SN Comput. Sci.},
month = sep,
numpages = {15},
keywords = {Machine learning (ML), Software enhancement effort (SEME) prediction, Functional change (FC), Systematic mapping study (SMS)}
}

@article{10.1016/j.infsof.2013.02.009,
author = {Radjenovi\'{c}, Danijel and Heri\v{c}ko, Marjan and Torkar, Richard and \v{Z}ivkovi\v{c}, Ale\v{s}},
title = {Software fault prediction metrics},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.02.009},
doi = {10.1016/j.infsof.2013.02.009},
abstract = {ContextSoftware metrics may be used in fault prediction models to improve software quality by predicting fault location. ObjectiveThis paper aims to identify software metrics and to assess their applicability in software fault prediction. We investigated the influence of context on metrics' selection and performance. MethodThis systematic literature review includes 106 papers published between 1991 and 2011. The selected papers are classified according to metrics and context properties. ResultsObject-oriented metrics (49%) were used nearly twice as often compared to traditional source code metrics (27%) or process metrics (24%). Chidamber and Kemerer's (CK) object-oriented metrics were most frequently used. According to the selected studies there are significant differences between the metrics used in fault prediction performance. Object-oriented and process metrics have been reported to be more successful in finding faults compared to traditional size and complexity metrics. Process metrics seem to be better at predicting post-release faults compared to any static code metrics. ConclusionMore studies should be performed on large industrial software systems to find metrics more relevant for the industry and to answer the question as to which metrics should be used in a given context.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1397–1418},
numpages = {22},
keywords = {Systematic literature review, Software metric, Software fault prediction}
}

@inproceedings{10.1145/2393596.2393669,
author = {Rahman, Foyzur and Posnett, Daryl and Devanbu, Premkumar},
title = {Recalling the "imprecision" of cross-project defect prediction},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393669},
doi = {10.1145/2393596.2393669},
abstract = {There has been a great deal of interest in defect prediction: using prediction models trained on historical data to help focus quality-control resources in ongoing development. Since most new projects don't have historical data, there is interest in cross-project prediction: using data from one project to predict defects in another. Sadly, results in this area have largely been disheartening. Most experiments in cross-project defect prediction report poor performance, using the standard measures of precision, recall and F-score. We argue that these IR-based measures, while broadly applicable, are not as well suited for the quality-control settings in which defect prediction models are used. Specifically, these measures are taken at specific threshold settings (typically thresholds of the predicted probability of defectiveness returned by a logistic regression model). However, in practice, software quality control processes choose from a range of time-and-cost vs quality tradeoffs: how many files shall we test? how many shall we inspect? Thus, we argue that measures based on a variety of tradeoffs, viz., 5%, 10% or 20% of files tested/inspected would be more suitable. We study cross-project defect prediction from this perspective. We find that cross-project prediction performance is no worse than within-project performance, and substantially better than random prediction!},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {61},
numpages = {11},
keywords = {inspection, fault prediction, empirical software engineering},
location = {Cary, North Carolina},
series = {FSE '12}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00034,
author = {Lwakatare, Lucy Ellen and R\r{a}nge, Ellinor and Crnkovic, Ivica and Bosch, Jan},
title = {On the experiences of adopting automated data validation in an industrial machine learning project},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00034},
doi = {10.1109/ICSE-SEIP52600.2021.00034},
abstract = {Background: Data errors are a common challenge in machine learning (ML) projects and generally cause significant performance degradation in ML-enabled software systems. To ensure early detection of erroneous data and avoid training ML models using bad data, research and industrial practice suggest incorporating a data validation process and tool in ML system development process.Aim: The study investigates the adoption of a data validation process and tool in industrial ML projects. The data validation process demands significant engineering resources for tool development and maintenance. Thus, it is important to identify the best practices for their adoption especially by development teams that are in the early phases of deploying ML-enabled software systems.Method: Action research was conducted at a large-software intensive organization in telecommunications, specifically within the analytics R&amp;D organization for an ML use case of classifying faults from returned hardware telecommunication devices.Results: Based on the evaluation results and learning from our action research, we identified three best practices, three benefits, and two barriers to adopting the data validation process and tool in ML projects. We also propose a data validation framework (DVF) for systematizing the adoption of a data validation process.Conclusions: The results show that adopting a data validation process and tool in ML projects is an effective approach of testing ML-enabled software systems. It requires having an overview of the level of data (feature, dataset, cross-dataset, data stream) at which certain data quality tests can be applied.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {248–257},
numpages = {10},
keywords = {software engineering, machine learning, data validation, data quality, data errors},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3395363.3397366,
author = {Dutta, Saikat and Shi, August and Choudhary, Rutvik and Zhang, Zhekun and Jain, Aryaman and Misailovic, Sasa},
title = {Detecting flaky tests in probabilistic and machine learning applications},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397366},
doi = {10.1145/3395363.3397366},
abstract = {Probabilistic programming systems and machine learning frameworks like Pyro, PyMC3, TensorFlow, and PyTorch provide scalable and efficient primitives for inference and training. However, such operations are non-deterministic. Hence, it is challenging for developers to write tests for applications that depend on such frameworks, often resulting in flaky tests – tests which fail non-deterministically when run on the same version of code.  In this paper, we conduct the first extensive study of flaky tests in this domain. In particular, we study the projects that depend on four frameworks: Pyro, PyMC3, TensorFlow-Probability, and PyTorch. We identify 75 bug reports/commits that deal with flaky tests, and we categorize the common causes and fixes for them. This study provides developers with useful insights on dealing with flaky tests in this domain.  Motivated by our study, we develop a technique, FLASH, to systematically detect flaky tests due to assertions passing and failing in different runs on the same code. These assertions fail due to differences in the sequence of random numbers in different runs of the same test. FLASH exposes such failures, and our evaluation on 20 projects results in 11 previously-unknown flaky tests that we reported to developers.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {211–224},
numpages = {14},
keywords = {Randomness, Probabilistic Programming, Non-Determinism, Machine Learning, Flaky tests},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@article{10.1155/2021/9976306,
author = {Wang, Wei and Wu, Wenqing},
title = {Using Machine Learning Algorithms to Recognize Shuttlecock Movements},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9976306},
doi = {10.1155/2021/9976306},
abstract = {Shuttlecock is an excellent traditional national sport in China. Because of its simplicity, convenience, and fun, it is loved by the broad masses of people, especially teenagers and children. The development of shuttlecock sports into a confrontational event is not long, and it takes a period of research to master the tactics and strategies of shuttlecock sports. Based on this, this article proposes the use of machine learning algorithms to recognize the movement of shuttlecock movements, aiming to provide more theoretical and technical support for shuttlecock competitions by identifying features through actions with the assistance of technical algorithms. This paper uses literature research methods, model methods, comparative analysis methods, and other methods to deeply study the motion characteristics of shuttlecock motion, the key algorithms of machine learning algorithms, and other theories and construct the shuttlecock motion recognition based on multiview clustering algorithm. The model analyzes the robustness and accuracy of the machine learning algorithm and other algorithms, such as a variety of performance comparisons, and the results of the shuttlecock motion recognition image. For the key movements of shuttlecock movement, disk, stretch, hook, wipe, knock, and abduction, the algorithm proposed in this paper has a good movement recognition rate, which can reach 91.2%. Although several similar actions can be recognized well, the average recognition accuracy rate can exceed 75%, and even through continuous image capture, the number of occurrences of the action can be automatically analyzed, which is beneficial to athletes. And the coach can better analyze tactics and research strategies.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@inproceedings{10.5555/2820690.2820697,
author = {Cavezza, Davide G. and Pietrantuono, Roberto and Russo, Stefano},
title = {Performance of defect prediction in rapidly evolving software},
year = {2015},
publisher = {IEEE Press},
abstract = {Defect prediction techniques allow spotting modules (or commits) likely to contain (introduce) a defect by training models with product or process metrics -- thus supporting testing, code integration, and release decisions. When applied to processes where software changes rapidly, conventional techniques might fail, as trained models are not thought to evolve along with the software.In this study, we analyze the performance of defect prediction in rapidly evolving software. Framed in a high commit frequency context, we set up an approach to continuously refine prediction models by using new commit data, and predict whether or not an attempted commit is going to introduce a bug. An experiment is set up on the Eclipse JDT software to assess the prediction ability trend. Results enable to leverage defect prediction potentials in modern development paradigms with short release cycle and high code variability.},
booktitle = {Proceedings of the Third International Workshop on Release Engineering},
pages = {8–11},
numpages = {4},
location = {Florence, Italy},
series = {RELENG '15}
}

@article{10.1016/j.knosys.2017.02.020,
author = {Prez-Ortiz, M. and Gutirrez, P.A. and Aylln-Tern, M.D. and Heaton, N. and Ciria, R. and Briceo, J. and Hervs-Martnez, C.},
title = {Synthetic semi-supervised learning in imbalanced domains},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.020},
doi = {10.1016/j.knosys.2017.02.020},
abstract = {Liver transplantation is a promising and widely-accepted treatment for patients with terminal liver disease. However, transplantation is restricted by the lack of suitable donors, resulting in significant waiting list deaths. This paper proposes a novel donor-recipient allocation system that uses machine learning to predict graft survival after transplantation using a dataset comprised of donor-recipient pairs from the Kings College Hospital (United Kingdom). The main novelty of the system is that it tackles the imbalanced nature of the dataset by considering semi-supervised learning, analysing its potential for obtaining more robust and equitable models in liver transplantation. We propose two different sources of unsupervised data for this specific problem (recent transplants and virtual donor-recipient pairs) and two methods for using these data during model construction (a semi-supervised algorithm and a label propagation scheme). The virtual pairs and the label propagation method are shown to alleviate the imbalanced distribution. The results of our experiments show that the use of synthetic and real unsupervised information helps to improve and stabilise the performance of the model and leads to fairer decisions with respect to the use of only supervised data. Moreover, the best model is combined with the Model for End-stage Liver Disease score (MELD), which is at the moment the most popular assignation methodology worldwide. By doing this, our decision-support system considers both the compatibility of the donor and the recipient (by our prediction system) and the recipient severity (via the MELD score), supporting then the principles of fairness and benefit.},
journal = {Know.-Based Syst.},
month = may,
pages = {75–87},
numpages = {13},
keywords = {Transplant recipient, Survival analysis, Support vector machines, Semi-supervised learning, Machine learning, Liver transplantation, Imbalanced classification}
}

@inproceedings{10.1145/3324884.3415295,
author = {Khanan, Chaiyakarn and Luewichana, Worawit and Pruktharathikoon, Krissakorn and Jiarpakdee, Jirayus and Tantithamthavorn, Chakkrit and Choetkiertikul, Morakot and Ragkhitwetsagul, Chaiyong and Sunetnanta, Thanwadee},
title = {JITBot: an explainable just-in-time defect prediction bot},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3415295},
doi = {10.1145/3324884.3415295},
abstract = {Just-In-Time (JIT) defect prediction is a classification model that is trained using historical data to predict bug-introducing changes. However, recent studies raised concerns related to the explainability of the predictions of many software analytics applications (i.e., practitioners do not understand why commits are risky and how to improve them). In addition, the adoption of Just-In-Time defect prediction is still limited due to a lack of integration into CI/CD pipelines and modern software development platforms (e.g., GitHub). In this paper, we present an explainable Just-In-Time defect prediction framework to automatically generate feedback to developers by providing the riskiness of each commit, explaining why such commit is risky, and suggesting risk mitigation plans. The proposed framework is integrated into the GitHub CI/CD pipeline as a GitHub application to continuously monitor and analyse a stream of commits in many GitHub repositories. Finally, we discuss the usage scenarios and their implications to practitioners. The VDO demonstration is available at https://jitbot-tool.github.io/},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1336–1339},
numpages = {4},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1007/s11219-015-9297-z,
author = {Tosun, Ayse and Bener, Ayse Basar and Akbarinasaji, Shirin},
title = {A systematic literature review on the applications of Bayesian networks to predict software quality},
year = {2017},
issue_date = {March     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-015-9297-z},
doi = {10.1007/s11219-015-9297-z},
abstract = {Bayesian networks (BN) have been used for decision making in software engineering for many years. In other fields such as bioinformatics, BNs are rigorously evaluated in terms of the techniques that are used to build the network structure and to learn the parameters. We extend our prior mapping study to investigate the extent to which contextual and methodological details regarding BN construction are reported in the studies. We conduct a systematic literature review on the applications of BNs to predict software quality. We focus on more detailed questions regarding (1) dataset characteristics, (2) techniques used for parameter learning, (3) techniques used for structure learning, (4) use of tools, and (5) model validation techniques. Results on ten primary studies show that BNs are mostly built based on expert knowledge, i.e. structure and prior distributions are defined by experts, whereas authors benefit from BN tools and quantitative data to validate their models. In most of the papers, authors do not clearly explain their justification for choosing a specific technique, and they do not compare their proposed BNs with other machine learning approaches. There is also a lack of consensus on the performance measures to validate the proposed BNs. Compared to other domains, the use of BNs is still very limited and current publications do not report enough details to replicate the studies. We propose a framework that provides a set of guidelines for reporting the essential contextual and methodological details of BNs. We believe such a framework would be useful to replicate and extend the work on BNs.},
journal = {Software Quality Journal},
month = mar,
pages = {273–305},
numpages = {33},
keywords = {Systematic literature review, Software reliability prediction, Software quality, Bayesian network, Applications of AI}
}

@inproceedings{10.1145/3382494.3410681,
author = {Serban, Alex and van der Blom, Koen and Hoos, Holger and Visser, Joost},
title = {Adoption and Effects of Software Engineering Best Practices in Machine Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410681},
doi = {10.1145/3382494.3410681},
abstract = {Background. The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner.Aim. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components.Method. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models.Results. Our findings indicate, for example, that larger teams tend to adopt more practices, and that traditional software engineering practices tend to have lower adoption than ML specific practices. Also, the statistical models can accurately predict perceived effects such as agility, software quality and traceability, from the degree of adoption for specific sets of practices. Combining practice adoption rates with practice importance, as revealed by statistical models, we identify practices that are important but have low adoption, as well as practices that are widely adopted but are less important for the effects we studied.Conclusion. Overall, our survey and the analysis of responses received provide a quantitative basis for assessment and step-wise improvement of practice adoption by ML teams.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {3},
numpages = {12},
keywords = {survey, machine learning engineering, best practices},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/3297280.3297411,
author = {Leotta, Maurizio and Olianas, Dario and Ricca, Filippo and Noceti, Nicoletta},
title = {How do implementation bugs affect the results of machine learning algorithms?},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297411},
doi = {10.1145/3297280.3297411},
abstract = {Applications based on Machine learning (ML) are growing in popularity in a multitude of different contexts such as medicine, bioinformatics, and finance. However, there is a lack of established approaches and strategies able to assure the reliability of this category of software. This has a big impact since nowadays our society relies on (potentially) unreliable applications that could cause, in extreme cases, catastrophic events (e.g., loss of life due to a wrong diagnosis of an ML-based cancer classifier).In this paper, as a preliminary step towards providing a solution to this big problem, we used automatic mutations to mimic realistic bugs in the code of two machine learning algorithms, Multilayer Perceptron and Logistic Regression, with the goal of studying the impact of implementation bugs on their behaviours.Unexpectedly, our experiments show that about 2/3 of the injected bugs are silent since they does not influence the results of the algorithms, while the bugs emerge as runtime errors, exceptions, or modified accuracy of the predictions only in the remaining cases. Moreover, we also discovered that about 1% of the bugs are extremely dangerous since they drastically affect the quality of the prediction only in rare cases and with specific datasets increasing the possibility of going unnoticed.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1304–1313},
numpages = {10},
keywords = {Oracle problem, accuracy, bug, machine learning, software quality assurance, testing},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.4018/IJSSCI.2016070102,
author = {Rashid, Ekbal},
title = {R4 Model for Case-Based Reasoning and Its Application for Software Fault Prediction},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {8},
number = {3},
issn = {1942-9045},
url = {https://doi.org/10.4018/IJSSCI.2016070102},
doi = {10.4018/IJSSCI.2016070102},
abstract = {Making R4 model effective and efficient I have introduced some new features, i.e., renovation of knowledgebase KBS and reducing the maintenance cost by removing the duplicate record from the KBS. Renovation of knowledgebase is the process of removing duplicate record stored in knowledgebase and adding world new problems along with world new solutions. This paper explores case-based reasoning and its applications for software quality improvement through early prediction of error patterns. It summarizes a variety of techniques for software quality prediction in the domain of software engineering. The system predicts the error level with respect to LOC and with respect to development time, and both affects the quality level. This paper also reviews four existing models of case-based reasoning CBR. The paper presents a work in which I have expanded our previous work Rashid et al., 2012. I have used different similarity measures to find the best method that increases reliability. The present work is also credited through introduction of some new terms like coefficient of efficiency, i.e., developer's ability.},
journal = {Int. J. Softw. Sci. Comput. Intell.},
month = jul,
pages = {19–38},
numpages = {20},
keywords = {Software Fault Prediction, Similarity Function, Reliability, Machine Learning, LOC, Development Time}
}

@article{10.1007/s10009-020-00577-w,
author = {Usman, Muhammad and Wang, Wenxi and Wang, Kaiyuan and Yelen, Cagdas and Dini, Nima and Khurshid, Sarfraz},
title = {A study of learning likely data structure properties using machine learning models},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-020-00577-w},
doi = {10.1007/s10009-020-00577-w},
abstract = {Data structure properties are important for many testing and analysis tasks. For example, model checkers use these properties to find program faults. These properties are often written manually which can be error prone and lead to false alarms. This paper presents the results of controlled experiments performed using existing machine learning (ML) models on various data structures. These data structures are dynamic and reside on the program heap. We use ten data structure subjects and ten ML models to evaluate the learnability of data structure properties. The study reveals five key findings. One, most of the ML models perform well in learning data structure properties, but some of the ML models such as quadratic discriminant analysis and Gaussian naive Bayes are not suitable for learning data structure properties. Two, most of the ML models have high performance even when trained on just 1% of data samples. Three, certain data structure properties such as binary heap and red black tree are more learnable than others. Four, there are no significant differences between the learnability of varied-size (i.e., up to a certain size) and fixed-size data structures. Five, there can be significant differences in performance based on the encoding used. These findings show that using machine learning models to learn data structure properties is very promising. We believe that these properties, once learned, can be used to provide a run-time check to see whether a program state at a particular point satisfies the learned property. Learned properties can also be employed in the future to automate static and dynamic analysis, which would enhance software testing and verification techniques.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {601–615},
numpages = {15},
keywords = {Learnability, Korat, Machine learning, Data structure invariants}
}

@inproceedings{10.1145/2491411.2491418,
author = {Rahman, Foyzur and Posnett, Daryl and Herraiz, Israel and Devanbu, Premkumar},
title = {Sample size vs. bias in defect prediction},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491418},
doi = {10.1145/2491411.2491418},
abstract = {Most empirical disciplines promote the reuse and sharing of datasets, as it leads to greater possibility of replication. While this is increasingly the case in Empirical Software Engineering, some of the most popular bug-fix datasets are now known to be biased. This raises two significant concerns: first, that sample bias may lead to underperforming prediction models, and second, that the external validity of the studies based on biased datasets may be suspect. This issue has raised considerable consternation in the ESE literature in recent years. However, there is a confounding factor of these datasets that has not been examined carefully: size. Biased datasets are sampling only some of the data that could be sampled, and doing so in a biased fashion; but biased samples could be smaller, or larger. Smaller data sets in general provide less reliable bases for estimating models, and thus could lead to inferior model performance. In this setting, we ask the question, what affects performance more, bias, or size? We conduct a detailed, large-scale meta-analysis, using simulated datasets sampled with bias from a high-quality dataset which is relatively free of bias. Our results suggest that size always matters just as much bias direction, and in fact much more than bias direction when considering information-retrieval measures such as AUCROC and F-score. This indicates that at least for prediction models, even when dealing with sampling bias, simply finding larger samples can sometimes be sufficient. Our analysis also exposes the complexity of the bias issue, and raises further issues to be explored in the future.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {147–157},
numpages = {11},
keywords = {size, defect prediction, bias},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1504/ijbis.2021.115366,
author = {Cerqueira, Marcelo Gomes De and Silva, Paulo Caetano Da},
title = {A survey of XBRL adoption impact on financial software development processes and software quality},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {37},
number = {2},
issn = {1746-0972},
url = {https://doi.org/10.1504/ijbis.2021.115366},
doi = {10.1504/ijbis.2021.115366},
abstract = {XBRL technology is currently used by several government institutions and companies around the world. The literature includes many papers related to the uses and benefits of XBRL in the areas of finance and accounting. However, there is a research gap on its benefits to software engineering. This paper seeks to identify the XBRL impacts on financial software development processes and software quality factors. Identifying such impacts may help software development companies better understand the advantages related to XBRL use and increase the adoption in other companies. This, in turn, may contribute to software quality improvement, facilitating the implementation of better financial software development frameworks in organisations.},
journal = {Int. J. Bus. Inf. Syst.},
month = jan,
pages = {263–286},
numpages = {23},
keywords = {software engineering, software quality factors, software process, financial software development, XBRL, extensible business reporting language}
}

@article{10.1007/s11219-018-9406-x,
author = {Mendes, Emilia and Winkler, Dietmar},
title = {Special issue on "software quality in software-intensive systems"},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9406-x},
doi = {10.1007/s11219-018-9406-x},
journal = {Software Quality Journal},
month = jun,
pages = {657–660},
numpages = {4}
}

@inproceedings{10.1007/978-3-319-25945-1_9,
author = {Altinger, Harald and Herbold, Steffen and Grabowski, Jens and Wotawa, Franz},
title = {Novel Insights on Cross Project Fault Prediction Applied to Automotive Software},
year = {2015},
isbn = {9783319259444},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-25945-1_9},
doi = {10.1007/978-3-319-25945-1_9},
abstract = {Defect prediction is a powerful tool that greatly helps focusing quality assurance efforts during development. In the case of the availability of fault data from a particular context, there are different ways of using such fault predictions in practice. Companies like Google, Bell Labs and Cisco make use of fault prediction, whereas its use within automotive industry has not yet gained a lot of attraction, although, modern cars require a huge amount of software to operate. In this paper, we want to contribute the adoption of fault prediction techniques for automotive software projects. Hereby we rely on a publicly available data set comprising fault data from three automotive software projects. When learning a fault prediction model from the data of one particular project, we achieve a remarkably high and nearly perfect prediction performance for the same project. However, when applying a cross-project prediction we obtain rather poor results. These results are rather surprising, because of the fact that the underlying projects are as similar as two distinct projects can possibly be within a certain application context. Therefore we investigate the reasons behind this observation through correlation and factor analyses techniques. We further report the obtained findings and discuss the consequences for future applications of Cross-Project Fault Prediction CPFP in the domain of automotive software.},
booktitle = {Proceedings of the 27th IFIP WG 6.1 International Conference on Testing Software and Systems - Volume 9447},
pages = {141–157},
numpages = {17},
keywords = {Project fault prediction, Principal component analysis, Cross project fault prediction, Automotive},
location = {Sharjah and Dubai, United Arab Emirates},
series = {ICTSS 2015}
}

@inproceedings{10.1145/3338906.3342484,
author = {Moghadam, Mahshid Helali},
title = {Machine learning-assisted performance testing},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342484},
doi = {10.1145/3338906.3342484},
abstract = {Automated testing activities like automated test case generation imply a reduction in human effort and cost, with the potential to impact the test coverage positively. If the optimal policy, i.e., the course of actions adopted, for performing the intended test activity could be learnt by the testing system, i.e., a smart tester agent, then the learnt policy could be reused in analogous situations which leads to even more efficiency in terms of required efforts. Performance testing under stress execution conditions, i.e., stress testing, which involves providing extreme test conditions to find the performance breaking points, remains a challenge, particularly for complex software systems. Some common approaches for generating stress test conditions are based on source code or system model analysis, or use-case based design approaches. However, source code or precise system models might not be easily available for testing. Moreover, drawing a precise performance model is often difficult, particularly for complex systems. In this research, I have used model-free reinforcement learning to build a self-adaptive autonomous stress testing framework which is able to learn the optimal policy for stress test case generation without having a model of the system under test. The conducted experimental analysis shows that the proposed smart framework is able to generate the stress test conditions for different software systems efficiently and adaptively without access to performance models.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1187–1189},
numpages = {3},
keywords = {Test case generation, Stress testing, Reinforcement learning, Performance testing, Autonomous testing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3452383.3452400,
author = {Misra, Janardan and Podder, Sanjay},
title = {Association of Defect Log Suitability for Machine Learning with Performance: An Experience Report},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452400},
doi = {10.1145/3452383.3452400},
abstract = {Machine learning (ML) based solutions utilizing textual details in defect logs have been shown to enable automation of defect management process and make it cost effective. In this work, we assess effectiveness of apriori manual analysis of the suitability of applying ML to problems encountered during defect management process. We consider problems of mapping defects to service engineers and business processes for designing experiments. Experimental analysis on these problems using multiple defect logs from practice reveals that a systematic analysis of the defect log data by project experts can provide approximate indication of the eventual performance of the ML model even before they are actually built. We discuss practical significance of the conclusions for designing ML based solutions in-practice.},
booktitle = {Proceedings of the 14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {17},
numpages = {5},
keywords = {Assignee Recommendation, Business Process Mapping, Defect Management Life-Cycle, Machine Learning Suitability, Mining Defect Repositories, Text Analysis},
location = {Bhubaneswar, Odisha, India},
series = {ISEC '21}
}

@article{10.1155/2021/6627588,
author = {Xie, Yuan and Zhao, Jisheng and Qiang, Baohua and Mi, Luzhong and Tang, Chenghua and Li, Longge and Xue, Xingsi},
title = {Attention Mechanism-Based CNN-LSTM Model for Wind Turbine Fault Prediction Using SSN Ontology Annotation},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/6627588},
doi = {10.1155/2021/6627588},
abstract = {The traditional model for wind turbine fault prediction is not sensitive to the time sequence data and cannot mine the deep connection between the time series data, resulting in poor generalization ability of the model. To solve this problem, this paper proposes an attention mechanism-based CNN-LSTM model. The semantic sensor data annotated by SSN ontology is used as input data. Firstly, CNN extracts features to get high-level feature representation from input data. Then, the latent time sequence connection of features in different time periods is learned by LSTM. Finally, the output of LSTM is input into the attention mechanism module to obtain more fault-related target information, which improves the efficiency, accuracy, and generalization ability of the model. In addition, in the data preprocessing stage, the random forest algorithm analyzes the feature correlation degree of the data to get the features of high correlation degree with the wind turbine fault, which further improves the efficiency, accuracy, and generalization ability of the model. The model is validated on the icing fault dataset of No. 21 wind turbine and the yaw dataset of No. 4 wind turbine. The experimental results show that the proposed model has better efficiency, accuracy, and generalization ability than RNN, LSTM, and XGBoost.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {12}
}

@inproceedings{10.1145/3266003.3266004,
author = {de Santiago, Valdivino Alexandre and da Silva, Leoni Augusto Romain and de Andrade Neto, Pedro Ribeiro},
title = {Testing Environmental Models supported by Machine Learning},
year = {2018},
isbn = {9781450365550},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266003.3266004},
doi = {10.1145/3266003.3266004},
abstract = {In this paper we present a new methodology, DaOBML, to test environmental models whose outputs are complex artifacts such as images (maps) or plots. Our approach suggests several test data generation techniques (Combinatorial Interaction Testing, Model-Based Testing, Random Testing) and digital image processing methods to drive the creation of Knowledge Bases (KBs). Considering such KBs and Machine Learning (ML) algorithms, a test oracle assigns the verdicts of new test data. Our methodology is supported by a tool and we applied it to models developed via the TerraME product. A controlled experiment was carried out and we conclude that Random Testing is the most feasible test data generation approach for developing the KBs, Artificial Neural Networks present the best performance out of six ML algorithms, and the larger the KB, in terms of size, the better.},
booktitle = {Proceedings of the III Brazilian Symposium on Systematic and Automated Software Testing},
pages = {3–12},
numpages = {10},
keywords = {Random Testing, Model-Based Testing, Machine Learning, Environmental Modeling, Empirical Software Engineering, Digital Image Processing, Combinatorial Interaction Testing},
location = {SAO CARLOS, Brazil},
series = {SAST '18}
}

@article{10.1016/j.inffus.2018.10.005,
author = {Diez-Olivan, Alberto and Del Ser, Javier and Galar, Diego and Sierra, Basilio},
title = {Data fusion and machine learning for industrial prognosis: Trends and perspectives towards Industry 4.0},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {50},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2018.10.005},
doi = {10.1016/j.inffus.2018.10.005},
journal = {Inf. Fusion},
month = oct,
pages = {92–111},
numpages = {20},
keywords = {VCM, SVMs, SOM-MQE, SBM, SARMA, RNN, RBM, PoF, PCA, LOF, LAD, KDE, kNN, HMM, GRNN, GRBMs, GMM, FPCA, FFT, EWMA, EM, DWT, DBN, BPNN, ANNs, ANFIS, Industry 4.0, Machine learning, Data fusion, Data-driven prognosis}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00066,
author = {Wan, Chengcheng and Liu, Shicheng and Hoffmann, Henry and Maire, Michael and Lu, Shan},
title = {A replication of are machine learning cloud APIs used correctly},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00066},
doi = {10.1109/ICSE-Companion52605.2021.00066},
abstract = {This artifact aims to provide benchmark suite, data, and script used in our study "Are Machine Learning Cloud APIs Used Correctly?". We collected a suite of 360 non-trivial applications that use ML cloud APIs for manual study. We also developed checkers and tool to detect and fix API mis-uses. We hope this artifact can motivate and help future research to further tackle ML API mis-uses. All related data are available online.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {158–159},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@article{10.1007/s10515-011-0090-3,
author = {He, Zhimin and Shu, Fengdi and Yang, Ye and Li, Mingshu and Wang, Qing},
title = {An investigation on the feasibility of cross-project defect prediction},
year = {2012},
issue_date = {June      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0090-3},
doi = {10.1007/s10515-011-0090-3},
abstract = {Software defect prediction helps to optimize testing resources allocation by identifying defect-prone modules prior to testing. Most existing models build their prediction capability based on a set of historical data, presumably from the same or similar project settings as those under prediction. However, such historical data is not always available in practice. One potential way of predicting defects in projects without historical data is to learn predictors from data of other projects. This paper investigates defect predictions in the cross-project context focusing on the selection of training data. We conduct three large-scale experiments on 34 data sets obtained from 10 open source projects. Major conclusions from our experiments include: (1) in the best cases, training data from other projects can provide better prediction results than training data from the same project; (2) the prediction results obtained using training data from other projects meet our criteria for acceptance on the average level, defects in 18 out of 34 cases were predicted at a Recall greater than 70% and a Precision greater than 50%; (3) results of cross-project defect predictions are related with the distributional characteristics of data sets which are valuable for training data selection. We further propose an approach to automatically select suitable training data for projects without historical data. Prediction results provided by the training data selected by using our approach are comparable with those provided by training data from the same project.},
journal = {Automated Software Engg.},
month = jun,
pages = {167–199},
numpages = {33},
keywords = {Training data, Machine learning, Defect prediction, Data characteristics, Cross-project}
}

@inproceedings{10.5555/3291291.3291297,
author = {Nascimento, Nathalia and Alencar, Paulo and Lucena, Carlos and Cowan, Donald},
title = {A context-aware machine learning-based approach},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {It is known that training a general and versatile Machine Learning (ML)-based model is more cost-effective than training several specialized ML-models for different operating contexts. However, as the volume of training information grows, the higher the probability of producing biased results. Learning bias is a critical problem for many applications, such as those related to healthcare scenarios, environmental monitoring and air traffic control. In this paper, we compare the use of a general model that was trained using all contexts against a system that is composed of a set of specialized models that was trained for each particular operating context. For this purpose, we propose a local learning approach based on context-awareness, which involves: (i) anticipating, analyzing and representing context changes; (ii) training and finding machine learning models to maximize a given scoring function for each operating context; (iii) storing trained ML-based models and associating them with corresponding operating contexts; and (iv) deploying a system that is able to select the best-fit ML-based model at runtime based on the context. To illustrate our proposed approach, we reproduce two experiments: one that uses a neural network regression-based model to perform predictions and another one that uses an evolutionary neural network-based approach to make decisions. For each application, we compare the results of the general model, which was trained based on all contexts, against the results of our proposed approach. We show that our context-aware approach can improve results by alleviating bias with different ML tasks.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {40–47},
numpages = {8},
keywords = {neural network, machine learning, learning bias, contextual modeling, context-awareness},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1109/EUROMICRO.2006.56,
author = {Ceylan, Evren and Kutlubay, F. Onur and Bener, Ayse B.},
title = {Software Defect Identification Using Machine Learning Techniques},
year = {2006},
isbn = {0769525946},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/EUROMICRO.2006.56},
doi = {10.1109/EUROMICRO.2006.56},
abstract = {Software engineering is a tedious job that includes people, tight deadlines and limited budgets. Delivering what customer wants involves minimizing the defects in the programs. Hence, it is important to establish quality measures early on in the project life cycle. The main objective of this research is to analyze problems in software code and propose a model that will help catching those problems earlier in the project life cycle. Our proposed model uses machine learning methods. Principal Component Analysis is used for dimensionality reduction, and Decision Tree, Multi Layer Perceptron and Radial Basis Functions are used for defect prediction. The experiments in this research are carried out with different software metric datasets that are obtained from real-life projects of three big software companies in Turkey. We can say that, the improved method that we proposed brings out satisfactory results in terms of defect prediction.},
booktitle = {Proceedings of the 32nd EUROMICRO Conference on Software Engineering and Advanced Applications},
pages = {240–247},
numpages = {8},
series = {EUROMICRO '06}
}

@article{10.1007/s11042-017-4419-1,
author = {Singh, Sandip Kumar and Kumar, Sandeep and Dwivedi, J. P.},
title = {Compound fault prediction of rolling bearing using multimedia data},
year = {2017},
issue_date = {Sep 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {18},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-017-4419-1},
doi = {10.1007/s11042-017-4419-1},
abstract = {Catastrophic failure of mechanical systems due to faults occurring on rolling bearing is still a great challenge. These faults, which are of multiple type, are compounded in nature. Vibration analysis of multimedia signals is one of the most effective techniques for the health monitoring of these bearings. A compound fault signal usually consists of multiple characteristic signals and strong confusion noise, which makes it a tough task to separate weak fault signals from them. To resolve the compound fault diagnosis problem of rolling bearings byseparation of multimedia signals' (obtained from acoustic or acceleration sensors), ensemble empirical mode decomposition (EEMD) method along with some classifier (like independent component analysis (ICA) technique) has been used to some degree of success. But, they are not found capable of detecting difficult faults existing on small balls of the bearing. In order to solve this problem, we are going to propose a new method based on use of Combined Mode Functions (CMF) for selecting the intrinsic mode functions(IMFs) instead of the maximum cross correlation coefficient based EEMD technique, sandwiched with, Convolution Neural Networks (CNN), which are deep neural nets, used as fault classifiers. This composite CNN-CMF-EEMD methodovercomes the deficiencies of other approaches, such as the inability to learn the complex non-linear relationships in fault diagnosis issues and fine compound faults like those occurring on small balls of the bearing. The difficult compound faults can be separated effectively by executing CNN-CMF-EEMD method, which makes the fault features more easily extracted and more clearly identified. Experimental results reinforce the effectiveness of using CNN-CMF--EEMD technique for fine compound faults. A comparison of CNN-CMF-EEMD with Artificial Neural Networks (ANN) based ANN-CMF-EEMD shows the capability of CNN as a powerful classifier in the domain of compound fault features of rolling bearing.},
journal = {Multimedia Tools Appl.},
month = sep,
pages = {18771–18788},
numpages = {18},
keywords = {Multimedia signals, Intrinsic mode functions, Independent component analysis, Ensemble empirical mode distribution, Convolution neural network, Compound faults, Combined mode functions, Artificial neural networks}
}

@article{10.1109/TSE.2012.20,
author = {Dejaeger, Karel and Verbraken, Thomas and Baesens, Bart},
title = {Toward Comprehensible Software Fault Prediction Models Using Bayesian Network Classifiers},
year = {2013},
issue_date = {February 2013},
publisher = {IEEE Press},
volume = {39},
number = {2},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2012.20},
doi = {10.1109/TSE.2012.20},
abstract = {Software testing is a crucial activity during software development and fault prediction models assist practitioners herein by providing an upfront identification of faulty software code by drawing upon the machine learning literature. While especially the Naive Bayes classifier is often applied in this regard, citing predictive performance and comprehensibility as its major strengths, a number of alternative Bayesian algorithms that boost the possibility of constructing simpler networks with fewer nodes and arcs remain unexplored. This study contributes to the literature by considering 15 different Bayesian Network (BN) classifiers and comparing them to other popular machine learning techniques. Furthermore, the applicability of the Markov blanket principle for feature selection, which is a natural extension to BN theory, is investigated. The results, both in terms of the AUC and the recently introduced H-measure, are rigorously tested using the statistical framework of Dem\v{s}ar. It is concluded that simple and comprehensible networks with less nodes can be constructed using BN classifiers other than the Naive Bayes classifier. Furthermore, it is found that the aspects of comprehensibility and predictive performance need to be balanced out, and also the development context is an item which should be taken into account during model selection.},
journal = {IEEE Trans. Softw. Eng.},
month = feb,
pages = {237–257},
numpages = {21},
keywords = {comprehensibility, classification, Software fault prediction, Software, Probability distribution, Predictive models, Measurement, Machine learning, Capability maturity model, Bayesian networks, Bayesian methods}
}

@inproceedings{10.1007/978-3-030-86475-0_15,
author = {Azimi, Shelernaz and Pahl, Claus},
title = {The Effect of IoT Data Completeness and Correctness on Explainable Machine Learning Models},
year = {2021},
isbn = {978-3-030-86474-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86475-0_15},
doi = {10.1007/978-3-030-86475-0_15},
abstract = {Many systems in the Edge Cloud, the Internet-of-Things or Cyber-Physical Systems are built for processing data, which is delivered from sensors and devices, transported, processed and consumed locally by actuators. This, given the regularly high volume of data, permits Artificial Intelligence (AI) strategies like Machine Learning (ML) to be used to generate the application and management functions needed. The quality of both source data and machine learning model is here unavoidably of high significance, yet has not been explored sufficiently as an explicit connection of the ML model quality that are created through ML procedures to the quality of data that the model functions consume in their construction. Here, we investigated the link between input data quality for ML function construction and the quality of these functions in data-driven software systems towards explainable model construction through an experimental approach with IoT Data using decision trees. We have 3 objectives in this research: 1. Search for indicators that influence data quality such as correctness and completeness and model construction factors on accuracy, precision and recall. 2. Estimate the impact of variations in model construction and data quality. 3. Identify change patterns that can be attributed to specific input changes.},
booktitle = {Database and Expert Systems Applications: 32nd International Conference, DEXA 2021, Virtual Event, September 27–30, 2021, Proceedings, Part II},
pages = {151–160},
numpages = {10},
keywords = {Explainable AI, Data quality, IoT systems, Machine learning, Data correctness, Data completeness, Decision trees}
}

@article{10.1007/s11219-012-9180-0,
author = {\c{C}al\i{}kl\i{}, G\"{u}l and Bener, Ay\c{s}e Ba\c{s}ar},
title = {Influence of confirmation biases of developers on software quality: an empirical study},
year = {2013},
issue_date = {June      2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9180-0},
doi = {10.1007/s11219-012-9180-0},
abstract = {The thought processes of people have a significant impact on software quality, as software is designed, developed and tested by people. Cognitive biases, which are defined as patterned deviations of human thought from the laws of logic and mathematics, are a likely cause of software defects. However, there is little empirical evidence to date to substantiate this assertion. In this research, we focus on a specific cognitive bias, confirmation bias, which is defined as the tendency of people to seek evidence that verifies a hypothesis rather than seeking evidence to falsify a hypothesis. Due to this confirmation bias, developers tend to perform unit tests to make their program work rather than to break their code. Therefore, confirmation bias is believed to be one of the factors that lead to an increased software defect density. In this research, we present a metric scheme that explores the impact of developers' confirmation bias on software defect density. In order to estimate the effectiveness of our metric scheme in the quantification of confirmation bias within the context of software development, we performed an empirical study that addressed the prediction of the defective parts of software. In our empirical study, we used confirmation bias metrics on five datasets obtained from two companies. Our results provide empirical evidence that human thought processes and cognitive aspects deserve further investigation to improve decision making in software development for effective process management and resource allocation.},
journal = {Software Quality Journal},
month = jun,
pages = {377–416},
numpages = {40},
keywords = {Software psychology, Human factors, Defect prediction, Confirmation bias}
}

@inproceedings{10.1145/3409501.3409543,
author = {Yan, Ziyue and Zong, Lu},
title = {Spatial Prediction of Housing Prices in Beijing Using Machine Learning Algorithms},
year = {2020},
isbn = {9781450375603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409501.3409543},
doi = {10.1145/3409501.3409543},
abstract = {The real estate industry places key influence on almost every aspect of social economy given its great financing capacity and prolonged upstream and downstream industry chain. Therefore, predicting housing prices is regarded as an emerging topic in the recent decades. Hedonic Regression and Machine Learning Algorithms are two main methods in this field. This study aims to explore the important explanatory features and determine an accurate mechanism to implement spatial prediction of housing prices in Beijing by incorporating a list of machine learning techniques, including XGBoost, linear regression, Random Forest Regression, Ridge and Lasso Model, bagging and boosting, based on the housing price and features data in Beijing, China. Our result shows that compared to traditional hedonic method, machine learning methods demonstrate significant improvements on the accuracy of estimation despite that they are more time-costly. Moreover, it is found that XGBoost is the most accurate model in explaining and prediciting the spatial dynamics of housing prices in Beijing.},
booktitle = {Proceedings of the 2020 4th High Performance Computing and Cluster Technologies Conference &amp; 2020 3rd International Conference on Big Data and Artificial Intelligence},
pages = {64–71},
numpages = {8},
keywords = {Spatial Modeling, Prediction, Machine Learning Algorithms, Housing Price},
location = {Qingdao, China},
series = {HPCCT &amp; BDAI '20}
}

@inproceedings{10.1145/2020390.2020392,
author = {Bell, Robert M. and Ostrand, Thomas J. and Weyuker, Elaine J.},
title = {Does measuring code change improve fault prediction?},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020392},
doi = {10.1145/2020390.2020392},
abstract = {Background: Several studies have examined code churn as a variable for predicting faults in large software systems. High churn is usually associated with more faults appearing in code that has been changed frequently.Aims: We investigate the extent to which faults can be predicted by the degree of churn alone, whether other code characteristics occur together with churn, and which combinations of churn and other characteristics provide the best predictions. We also investigate different types of churn, including both additions to and deletions from code, as well as overall amount of change to code.Method: We have mined the version control database of a large software system to collect churn and other software measures from 18 successive releases of the system. We examine the frequency of faults plotted against various code characteristics, and evaluate a diverse set of prediction models based on many different combinations of independent variables, including both absolute and relative churn.Results: Churn measures based on counts of lines added, deleted, and modified are very effective for fault prediction. Individually, counts of adds and modifications outperform counts of deletes, while the sum of all three counts was most effective. However, these counts did not improve prediction accuracy relative to a model that included a simple count of the number of times that a file had been changed in the prior release.Conclusions: Including a measure of change in the prior release is an essential component of our fault prediction method. Various measures seem to work roughly equivalently.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {2},
numpages = {8},
keywords = {code churn, empirical study, fault prediction, fault-percentile average, software faults},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@inproceedings{10.1109/ICSE43902.2021.00033,
author = {Tang, Yiming and Khatchadourian, Raffi and Bagherzadeh, Mehdi and Singh, Rhia and Stewart, Ajani and Raja, Anita},
title = {An Empirical Study of Refactorings and Technical Debt in Machine Learning Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00033},
doi = {10.1109/ICSE43902.2021.00033},
abstract = {Machine Learning (ML), including Deep Learning (DL), systems, i.e., those with ML capabilities, are pervasive in today's data-driven society. Such systems are complex; they are comprised of ML models and many subsystems that support learning processes. As with other complex systems, ML systems are prone to classic technical debt issues, especially when such systems are long-lived, but they also exhibit debt specific to these systems. Unfortunately, there is a gap of knowledge in how ML systems actually evolve and are maintained. In this paper, we fill this gap by studying refactorings, i.e., source-to-source semantics-preserving program transformations, performed in real-world, open-source software, and the technical debt issues they alleviate. We analyzed 26 projects, consisting of 4.2 MLOC, along with 327 manually examined code patches. The results indicate that developers refactor these systems for a variety of reasons, both specific and tangential to ML, some refactorings correspond to established technical debt categories, while others do not, and code duplication is a major crosscutting theme that particularly involved ML configuration and model code, which was also the most refactored. We also introduce 14 and 7 new ML-specific refactorings and technical debt categories, respectively, and put forth several recommendations, best practices, and anti-patterns. The results can potentially assist practitioners, tool developers, and educators in facilitating long-term ML system usefulness.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {238–250},
numpages = {13},
keywords = {technical debt, software repository mining, refactoring, machine learning systems, empirical studies},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1007/s11219-019-09472-3,
author = {Li, J . Jenny and Ulrich, Andreas and Bai, Xiaoying and Bertolino, Antonia},
title = {Advances in test automation for software with special focus on artificial intelligence and machine learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09472-3},
doi = {10.1007/s11219-019-09472-3},
journal = {Software Quality Journal},
month = mar,
pages = {245–248},
numpages = {4}
}

@book{10.5555/2911053,
author = {Mistrik, Ivan and Soley, Richard M. and Ali, Nour and Grundy, John and Tekinerdogan, Bedir},
title = {Software Quality Assurance: In Large Scale and Complex Software-intensive Systems},
year = {2015},
isbn = {0128023015},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Quality Assurance in Large Scale and Complex Software-intensive Systems presents novel and high-quality research related approaches that relate the quality of software architecture to system requirements, system architecture and enterprise-architecture, or software testing. Modern software has become complex and adaptable due to the emergence of globalization and new software technologies, devices and networks. These changes challenge both traditional software quality assurance techniques and software engineers to ensure software quality when building today (and tomorrows) adaptive, context-sensitive, and highly diverse applications. This edited volume presents state of the art techniques, methodologies, tools, best practices and guidelines for software quality assurance and offers guidance for future software engineering research and practice. Each contributed chapter considers the practical application of the topic through case studies, experiments, empirical validation, or systematic comparisons with other approaches already in practice. Topics of interest include, but are not limited, to: quality attributes of system/software architectures; aligning enterprise, system, and software architecture from the point of view of total quality; design decisions and their influence on the quality of system/software architecture; methods and processes for evaluating architecture quality; quality assessment of legacy systems and third party applications; lessons learned and empirical validation of theories and frameworks on architectural quality; empirical validation and testing for assessing architecture quality.Focused on quality assurance at all levels of software design and developmentCovers domain-specific software quality assurance issues e.g. for cloud, mobile, security, context-sensitive, mash-up and autonomic systemsExplains likely trade-offs from design decisions in the context of complex software system engineering and quality assuranceIncludes practical case studies of software quality assurance for complex, adaptive and context-critical systems}
}

@article{10.1007/s10515-021-00285-y,
author = {Goyal, Somya},
title = {Predicting the Defects using Stacked Ensemble Learner with Filtered Dataset},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-021-00285-y},
doi = {10.1007/s10515-021-00285-y},
abstract = {Software defect prediction is a crucial software project management activity to enhance the software quality. It aids the development team to forecast about which modules need extra attention for testing; which part of software is more prone to errors and faults; before the commencement of testing phase. It helps to reduce the testing cost and hence the overall development cost of the software. Though, it ensures in-time delivery of good quality end-product, but there is one major hinderance in making this prediction. This is the class imbalance issue in the training data. Data imbalance in class distribution adversely affects the performance of classifiers. This paper proposes a K-nearest neighbour (KNN) filtering-based data pre-processing technique for stacked ensemble classifier to handle class imbalance issue. First, nearest neighbour-based filtering is applied to filter out the overlapped data-points to reduce Imbalanced Ratio, then, the processed data with static code metrics is supplied to stacked ensemble for prediction. The stacking is achieved with five base classifiers namely Artificial Neural Network, Decision Tree, Na\"{\i}ve Bayes, K-nearest neighbour (KNN) and Support Vector Machine. A comparative analysis among 30 classifiers (5 data pre-processing techniques * 6 prediction techniques) is made. In the experiments, five public datasets from NASA repository namely CM1, JM1, KC1, KC2 and PC1 are used. In total 150 prediction models (5 data pre-processing techniques * 6 classification techniques * 5 datasets) are proposed and their performances are assessed in terms of measures namely Receiver Operator Curve, Area under the Curve and accuracy. The statistical analysis shows that proposed stacked ensemble classifier with KNN filtering performs best among all the predictors independent of datasets.},
journal = {Automated Software Engg.},
month = nov,
numpages = {81},
keywords = {ROC and AUC, Support vector machine, Nearest neighbour, Decision trees, Stacked ensembles, Artificial neural networks (ANN), Class imbalance, Data pre-processing, Defect prediction, Software quality}
}

@article{10.1007/s10515-010-0069-5,
author = {Menzies, Tim and Milton, Zach and Turhan, Burak and Cukic, Bojan and Jiang, Yue and Bener, Ay\c{s}e},
title = {Defect prediction from static code features: current results, limitations, new approaches},
year = {2010},
issue_date = {December  2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0069-5},
doi = {10.1007/s10515-010-0069-5},
abstract = {Building quality software is expensive and software quality assurance (QA) budgets are limited. Data miners can learn defect predictors from static code features which can be used to control QA resources; e.g. to focus on the parts of the code predicted to be more defective.Recent results show that better data mining technology is not leading to better defect predictors. We hypothesize that we have reached the limits of the standard learning goal of maximizing area under the curve (AUC) of the probability of false alarms and probability of detection "AUC(pd, pf)"; i.e. the area under the curve of a probability of false alarm versus probability of detection.Accordingly, we explore changing the standard goal. Learners that maximize "AUC(effort, pd)" find the smallest set of modules that contain the most errors. WHICH is a meta-learner framework that can be quickly customized to different goals. When customized to AUC(effort, pd), WHICH out-performs all the data mining methods studied here. More importantly, measured in terms of this new goal, certain widely used learners perform much worse than simple manual methods.Hence, we advise against the indiscriminate use of learners. Learners must be chosen and customized to the goal at hand. With the right architecture (e.g. WHICH), tuning a learner to specific local business goals can be a simple task.},
journal = {Automated Software Engg.},
month = dec,
pages = {375–407},
numpages = {33},
keywords = {WHICH, Static code features, Defect prediction}
}

@article{10.1007/s11334-015-0258-2,
author = {Abdi, Yousef and Parsa, Saeed and Seyfari, Yousef},
title = {A hybrid one-class rule learning approach based on swarm intelligence for software fault prediction},
year = {2015},
issue_date = {December  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-015-0258-2},
doi = {10.1007/s11334-015-0258-2},
abstract = {Software testing is a fundamental activity in the software development process aimed to determine the quality of software. To reduce the effort and cost of this process, defect prediction methods can be used to determine fault-prone software modules through software metrics to focus testing activities on them. Because of model interpretation and easily used by programmers and testers some recent studies presented classification rules to make prediction models. This study presents a rule-based prediction approach based on kernel k-means clustering algorithm and Distance based Multi-objective Particle Swarm Optimization (DSMOPSO). Because of discrete search space, we modified this algorithm and named it DSMOPSO-D. We prevent best global rules to dominate local rules by dividing the search space with kernel k-means algorithm and by taking different approaches for imbalanced and balanced clusters, we solved imbalanced data set problem. The presented model performance was evaluated by four publicly available data sets from the PROMISE repository and compared with other machine learning and rule learning algorithms. The obtained results demonstrate that our model presents very good performance, especially in large data sets.},
journal = {Innov. Syst. Softw. Eng.},
month = dec,
pages = {289–301},
numpages = {13},
keywords = {Multi-objective particle swarm optimization, Kernel k-means, Imbalanced data sets, Fault prediction, DSMOPSO-D, Classification rules}
}

@inproceedings{10.1145/3482909.3482911,
author = {Santos, Sebasti\~{a}o and Silveira, Beatriz and Durelli, Vinicius and Durelli, Rafael and Souza, Simone and Delamaro, Marcio},
title = {On Using Decision Tree Coverage Criteria forTesting Machine Learning Models},
year = {2021},
isbn = {9781450385039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482909.3482911},
doi = {10.1145/3482909.3482911},
abstract = {Over the past decade, there has been a growing interest in applying machine learning (ML) to address a myriad of tasks. Owing to this interest, the adoption of ML-based systems has gone mainstream. However, this widespread adoption of ML-based systems poses new challenges for software testers that must improve the quality and reliability of these ML-based solutions. To cope with the challenges of testing ML-based systems, we propose novel test adequacy criteria based on decision tree models. Differently from the traditional approach to testing ML models, which relies on manual collection and labelling of data, our criteria leverage the internal structure of decision tree models to guide the selection of test inputs. Thus, we introduce decision tree coverage (DTC) and boundary value analysis (BVA) as approaches to systematically guide the creation of effective test data that exercises key structural elements of a given decision tree model. To evaluate these criteria, we carried out an experiment using 12 datasets. We measured the effectiveness of test inputs in terms of the difference in model’s behavior between the test input and the training data. The experiment results indicate that our testing criteria can be used to guide the generation of effective test data.},
booktitle = {Proceedings of the 6th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {1–9},
numpages = {9},
keywords = {Testing Criterion, Software Testing, Decision Tree},
location = {Joinville, Brazil},
series = {SAST '21}
}

@article{10.1007/s11219-007-9013-8,
author = {Seliya, Naeem and Khoshgoftaar, Taghi M.},
title = {Software quality estimation with limited fault data: a semi-supervised learning perspective},
year = {2007},
issue_date = {September 2007},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-007-9013-8},
doi = {10.1007/s11219-007-9013-8},
abstract = {We addresses the important problem of software quality analysis when there is limited software fault or fault-proneness data. A software quality model is typically trained using software measurement and fault data obtained from a previous release or similar project. Such an approach assumes that fault data is available for all the training modules. Various issues in software development may limit the availability of fault-proneness data for all the training modules. Consequently, the available labeled training dataset is such that the trained software quality model may not provide predictions. More specifically, the small set of modules with known fault-proneness labels is not sufficient for capturing the software quality trends of the project. We investigate semi-supervised learning with the Expectation Maximization (EM) algorithm for software quality estimation with limited fault-proneness data. The hypothesis is that knowledge stored in software attributes of the unlabeled program modules will aid in improving software quality estimation. Software data collected from a large NASA software project is used during the semi-supervised learning process. The software quality model is evaluated with multiple test datasets collected from other NASA software projects. Compared to software quality models trained only with the available set of labeled program modules, the EM-based semi-supervised learning scheme improves generalization performance of the software quality models.},
journal = {Software Quality Journal},
month = sep,
pages = {327–344},
numpages = {18},
keywords = {Unlabeled data, Software quality estimation, Software metrics, Semi-supervised learning, Expectation maximization}
}

@article{10.1007/s11390-020-0323-7,
author = {Mhawish, Mohammad Y. and Gupta, Manjari},
title = {Predicting Code Smells and Analysis of Predictions: Using Machine Learning Techniques and Software Metrics},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {6},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-0323-7},
doi = {10.1007/s11390-020-0323-7},
abstract = {Code smell detection is essential to improve software quality, enhancing software maintainability, and decrease the risk of faults and failures in the software system. In this paper, we proposed a code smell prediction approach based on machine learning techniques and software metrics. The local interpretable model-agnostic explanations (LIME) algorithm was further used to explain the machine learning model’s predictions and interpretability. The datasets obtained from Fontana et al. were reformed and used to build binary-label and multi-label datasets. The results of 10-fold cross-validation show that the performance of tree-based algorithms (mainly Random Forest) is higher compared with kernel-based and network-based algorithms. The genetic algorithm based feature selection methods enhance the accuracy of these machine learning algorithms by selecting the most relevant features in each dataset. Moreover, the parameter optimization techniques based on the grid search algorithm significantly enhance the accuracy of all these algorithms. Finally, machine learning techniques have high potential in predicting the code smells, which contribute to detect these smells and enhance the software’s quality.},
journal = {J. Comput. Sci. Technol.},
month = nov,
pages = {1428–1445},
numpages = {18},
keywords = {parameter optimization, prediction explanation, feature selection, code smell detection, code smell}
}

@inproceedings{10.5555/3507788.3507810,
author = {Korlepara, Piyush and Grigoriou, Marios and Kontogiannis, Kostas and Brealey, Chris and Giammaria, Alberto},
title = {Combining domain expert knowledge and machine learning for the identification of error prone files},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Identifying as early as possible fault prone modules in order to facilitate continuous delivery in large software systems, has been an area where significant attention has been paid over the past few years. Recent efforts consider source code metrics and process metrics for training machine learning models to predict whether a software source code file is fault prone or not. In such prediction frameworks the accuracy of the trained model relies heavily on the features selected and the profiles of the metrics used for training the model which are unique to each system. Furthermore, these models act as black-boxes, where the end-user does not know how a specific prediction was reached. In this paper, we propose an approach which allows for domain expert knowledge to be combined with machine learning in order to yield fault-proneness prediction models that both exhibit high levels of recall and at the same time are able to provide explanations to the developers as to how and why these predictions were reached. For this paper we apply two rule-based inferencing techniques namely, Fuzzy reasoning, and Markov Logic Networks. The main contribution of this work is that it allows for expert developers to identify in the form of if-then rules domain logic that pertains to the fault-proneness of a source code file in the specific system being analysed. Results obtained from 19 open source systens indicate that MLNs perform better than Fuzzy Logic models and that project-customized rules achieve better results than generic rules. Furthermore, results indicate that its possible to compile a common set of rules that yields consistently acceptable results across different projects.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {153–162},
numpages = {10},
keywords = {software repositories, process metrics, fault-proneness prediction, continuous software engineering},
location = {Toronto, Canada},
series = {CASCON '21}
}

@article{10.1016/j.knosys.2017.04.014,
author = {Arcelli Fontana, Francesca and Zanoni, Marco},
title = {Code smell severity classification using machine learning techniques},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {128},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.04.014},
doi = {10.1016/j.knosys.2017.04.014},
abstract = {Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Machine learning techniques have been used for different topics in software engineering, e.g., design pattern detection, code smell detection, bug prediction, recommending systems. In this paper, we focus our attention on the classification of code smell severity through the use of machine learning techniques in different experiments. The severity of code smells is an important factor to take into consideration when reporting code smell detection results, since it allows the prioritization of refactoring efforts. In fact, code smells with high severity can be particularly large and complex, and create larger issues to the maintainability of software a system. In our experiments, we apply several machine learning models, spanning from multinomial classification to regression, plus a method to apply binary classifiers for ordinal classification. In fact, we model code smell severity as an ordinal variable. We take the baseline models from previous work, where we applied binary classification models for code smell detection with good results. We report and compare the performance of the models according to their accuracy and four different performance measures used for the evaluation of ordinal classification techniques. From our results, while the accuracy of the classification of severity is not high as in the binary classification of absence or presence of code smells, the ranking correlation of the actual and predicted severity for the best models reaches 0.880.96, measured through Spearmans .},
journal = {Know.-Based Syst.},
month = jul,
pages = {43–58},
numpages = {16},
keywords = {Refactoring prioritization, Ordinal classification, Machine learning, Code smells detection, Code smell severity}
}

@inproceedings{10.1145/3185089.3185152,
author = {Li, Boshu and Wu, Wenjun and Hu, Zhenhui},
title = {Evaluation of Software Quality for Competition-based Software Crowdsourcing Projects},
year = {2018},
isbn = {9781450354141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185089.3185152},
doi = {10.1145/3185089.3185152},
abstract = {Crowdsourcing-based Software Development (CSSD) performs as: many software practitioners use their own experience and technology to participate software development related tasks, through the open platform such as TopCoder. Crowdsourcing software quality issue has caught some researchers' attention, but it is still far from enough, and no work has been done on evaluating crowdsourcing software projects from a macro point of view. In the paper, we apply traditional quality evaluation practice and theory into the evaluation of crowdsourcing-based software quality by proper modification. The main contributions of this paper are: evaluate TopCoder software quality from the perspective of Project Rating and Project Effort respectively, and explore their aggregation strategies. In order to explore the relationship between them, we introduce the definition of quality assurance effort. We believe the final project rating indicator and quality assurance effort can help a project manager to make reasonable decisions on crowdsourcing-based software development tasks.},
booktitle = {Proceedings of the 2018 7th International Conference on Software and Computer Applications},
pages = {102–109},
numpages = {8},
keywords = {TopCoder, Software Quality, Software Competition, Quality Assurance Effort, Project Rating, Project Effort, Crowdsourcing-based Software Development},
location = {Kuantan, Malaysia},
series = {ICSCA '18}
}

@article{10.1007/s10586-019-02917-1,
author = {Mohammed, Bashir and Awan, Irfan and Ugail, Hassan and Younas, Muhammad},
title = {Failure prediction using machine learning in a virtualised HPC system and application},
year = {2019},
issue_date = {Mar 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-019-02917-1},
doi = {10.1007/s10586-019-02917-1},
abstract = {Failure is an increasingly important issue in high performance computing and cloud systems. As large-scale systems continue to grow in scale and complexity, mitigating the impact of failure and providing accurate predictions with sufficient lead time remains a challenging research problem. Traditional existing fault-tolerance strategies such as regular check-pointing and replication are not adequate because of the emerging complexities of high performance computing systems. This necessitates the importance of having an effective as well as proactive failure management approach in place aimed at minimizing the effect of failure within the system. With the advent of machine learning techniques, the ability to learn from past information to predict future pattern of behaviours makes it possible to predict potential system failure more accurately. Thus, in this paper, we explore the predictive abilities of machine learning by applying a number of algorithms to improve the accuracy of failure prediction. We have developed a failure prediction model using time series and machine learning, and performed comparison based tests on the prediction accuracy. The primary algorithms we considered are the support vector machine (SVM), random forest (RF), k-nearest neighbors (KNN), classification and regression trees (CART) and linear discriminant analysis (LDA). Experimental results indicates that the average prediction accuracy of our model using SVM when predicting failure is 90% accurate and effective compared to other algorithms. This finding implies that our method can effectively predict all possible future system and application failures within the system.},
journal = {Cluster Computing},
month = jun,
pages = {471–485},
numpages = {15},
keywords = {Machine learning, High performance computing, Failure, Cloud computing}
}

@inproceedings{10.5555/2487085.2487161,
author = {Peters, Fayola and Menzies, Tim and Marcus, Andrian},
title = {Better cross company defect prediction},
year = {2013},
isbn = {9781467329361},
publisher = {IEEE Press},
abstract = {How can we find data for quality prediction? Early in the life cycle, projects may lack the data needed to build such predictors. Prior work assumed that relevant training data was found nearest to the local project. But is this the best approach?  This paper introduces the Peters filter which is based on the following conjecture: When local data is scarce, more information exists in other projects. Accordingly, this filter selects training data via the structure of other projects.  To assess the performance of the Peters filter, we compare it with two other approaches for quality prediction. Within- company learning and cross-company learning with the Burak filter (the state-of-the-art relevancy filter). This paper finds that: 1) within-company predictors are weak for small data-sets; 2) the Peters filter+cross-company builds better predictors than both within-company and the Burak filter+cross-company; and 3) the Peters filter builds 64% more useful predictors than both within- company and the Burak filter+cross-company approaches. Hence, we recommend the Peters filter for cross-company learning.},
booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
pages = {409–418},
numpages = {10},
location = {San Francisco, CA, USA},
series = {MSR '13}
}

@article{10.1007/s10664-019-09735-4,
author = {Brindescu, Caius and Ahmed, Iftekhar and Jensen, Carlos and Sarma, Anita},
title = {An empirical investigation into merge conflicts and their effect on software quality},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09735-4},
doi = {10.1007/s10664-019-09735-4},
abstract = {Merge conflicts are known to cause extra effort for developers, but little is known about their effect on software. While some research has been done, many questions remain. To better understand merge conflicts and their impact we performed an empirical study about the types, frequency, and impact of merge conflicts, where impact is measured in terms of bug fixing commits associated with conflicts. We analyzed 143 open source projects and found that almost 1 in 5 merges cause conflicts. In 75.23% of these cases, a developer needed to reflect on the program logic to resolve it. We also found that the code associated with a merge conflict is twice as likely to have a bug. When the code associated with merge conflicts require manual intervention, the code is 26\texttimes{} more likely to have a bug.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {562–590},
numpages = {29},
keywords = {Mining software repositories, Empirical study, Software quality, Merge conflicts, Software merging, Version control}
}

@article{10.1007/s10515-019-00266-2,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong},
title = {Using multi-objective search and machine learning to infer rules constraining product configurations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00266-2},
doi = {10.1007/s10515-019-00266-2},
abstract = {Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.},
journal = {Automated Software Engg.},
month = jun,
pages = {1–62},
numpages = {62},
keywords = {Interacting products, Machine learning, Multi-objective search, Rule mining, Configuration, Product line}
}

@inproceedings{10.1145/3387940.3391490,
author = {Liem, Cynthia C. S. and Panichella, Annibale},
title = {Oracle Issues in Machine Learning and Where to Find Them},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391490},
doi = {10.1145/3387940.3391490},
abstract = {The rise in popularity of machine learning (ML), and deep learning in particular, has both led to optimism about achievements of artificial intelligence, as well as concerns about possible weaknesses and vulnerabilities of ML pipelines. Within the software engineering community, this has led to a considerable body of work on ML testing techniques, including white- and black-box testing for ML models. This means the oracle problem needs to be addressed. For supervised ML applications, oracle information is indeed available in the form of dataset 'ground truth', that encodes input data with corresponding desired output labels. However, while ground truth forms a gold standard, there still is no guarantee it is truly correct. Indeed, syntactic, semantic, and conceptual framing issues in the oracle may negatively affect the ML system's integrity. While syntactic issues may automatically be verified and corrected, the higher-level issues traditionally need human judgment and manual analysis. In this paper, we employ two heuristics based on information entropy and semantic analysis on well-known computer vision models and benchmark data from ImageNet. The heuristics are used to semi-automatically uncover potential higher-level issues in (i) the label taxonomy used to define the ground truth oracle (labels), and (ii) data encoding and representation. In doing this, beyond existing ML testing efforts, we illustrate the need for software engineering strategies that especially target and assess the oracle.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {483–488},
numpages = {6},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.2478/acss-2021-0003,
author = {undefinedikiforova, Oksana and Zabiniako, Vitaly and Kornienko, Jurijs and Gasparovi\v{c}a-Asundefinedte, Madara and Siliundefineda, Amanda},
title = {Mapping of Source and Target Data for Application to Machine Learning Driven Discovery of IS Usability Problems},
year = {2021},
issue_date = {May 2021},
publisher = {Walter de Gruyter GmbH},
address = {Berlin, DEU},
volume = {26},
number = {1},
issn = {2255-8691},
url = {https://doi.org/10.2478/acss-2021-0003},
doi = {10.2478/acss-2021-0003},
abstract = {Improving IS (Information System) end-user experience is one of the most important tasks in the analysis of end-users behaviour, evaluation and identification of its improvement potential. However, the application of Machine Learning methods for the UX (User Experience) usability and effic iency improvement is not widely researched. In the context of the usability analysis, the information about behaviour of end-users could be used as an input, while in the output data the focus should be made on non-trivial or difficult attention-grabbing events and scenarios. The goal of this paper is to identify which data potentially can serve as an input for Machine Learning methods (and accordingly graph theory, transformation methods, etc.), to define dependency between these data and desired output, which can help to apply Machine Learning / graph algorithms to user activity records.},
journal = {Appl. Comput. Syst.},
month = may,
pages = {22–30},
numpages = {9},
keywords = {User Experience (UX), Machine Learning, IS usability, Data mapping}
}

@inproceedings{10.1145/3266237.3266273,
author = {Braga, Rony\'{e}rison and Neto, Pedro Santos and Rab\^{e}lo, Ricardo and Santiago, Jos\'{e} and Souza, Matheus},
title = {A machine learning approach to generate test oracles},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266273},
doi = {10.1145/3266237.3266273},
abstract = {One of the essential activities for quality assurance in software development is the software testing. Studies report that Software Testing is one of the most costly activities in the development process, can reach up to 50 percent of its total cost. One of the great challenges of conducting software testing is related to the automation of a mechanism known as "test oracle". This work presents an approach based on machine learning (ML) for automation of the test oracle mechanism in software. The approach uses historical usage data from an application captured by inserting a capture component into the application under test. These data go through a Knowledge Discovery in Database step and are then used for training to generate an oracle suitable for the application under test. Four experiments were executed with web applications to evaluate the proposed approach. The first and second experiments were performed with a fictitious application, with faults inserted randomly in the first experiment, inserted by a developer in the second one and inserted by mutation tests in third one. The fourth experiment was carried out with a large real application in order to assure the results of the preliminary experiments. The experiments presented indications of the suitability of the approach to the solution of the problem.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {142–151},
numpages = {10},
keywords = {testing automation, test oracle, machine learning},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@article{10.1016/j.ins.2010.04.019,
author = {Peng, Yi and Wang, Guoxun and Wang, Honggang},
title = {User preferences based software defect detection algorithms selection using MCDM},
year = {2012},
issue_date = {May, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {191},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2010.04.019},
doi = {10.1016/j.ins.2010.04.019},
abstract = {A variety of classification algorithms for software defect detection have been developed over the years. How to select an appropriate classifier for a given task is an important issue in Data mining and knowledge discovery (DMKD). Many studies have compared different types of classification algorithms and the performances of these algorithms may vary using different performance measures and under different circumstances. Since the algorithm selection task needs to examine several criteria, such as accuracy, computational time, and misclassification rate, it can be modeled as a multiple criteria decision making (MCDM) problem. The goal of this paper is to use a set of MCDM methods to rank classification algorithms, with empirical results based on the software defect detection datasets. Since the preferences of the decision maker (DM) play an important role in algorithm evaluation and selection, this paper involved the DM during the ranking procedure by assigning user weights to the performance measures. Four MCDM methods are examined using 38 classification algorithms and 13 evaluation criteria over 10 public-domain software defect datasets. The results indicate that the boosting of CART and the boosting of C4.5 decision tree are ranked as the most appropriate algorithms for software defect datasets. Though the MCDM methods provide some conflicting results for the selected software defect datasets, they agree on most top-ranked classification algorithms.},
journal = {Inf. Sci.},
month = may,
pages = {3–13},
numpages = {11},
keywords = {Software defect detection, Multi-criteria decision making (MCDM), Knowledge-driven data mining, Classification algorithm, Algorithm selection}
}

@inproceedings{10.1007/978-3-030-78361-7_26,
author = {Fujinuma, Ryota and Asahi, Yumi},
title = {Proposal of Credit Risk Model Using Machine Learning in Motorcycle Sales},
year = {2021},
isbn = {978-3-030-78360-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78361-7_26},
doi = {10.1007/978-3-030-78361-7_26},
abstract = {While the new BIS regulations are reviewing the way of thinking about loans all over the world, many people in Central and South America still have a vague way of thinking about loans. It is due to the global recession. As a result, companies have not been able to recover their manufacturing costs. Therefore, in this study, we create a classification model of customers who default and customers who do not default. Also, explore the characteristics of the default customers. This is because it is thought that it will be easier for companies to improve the loan problem and secure profits.In this study, we compare the accuracy of Random Forest and XG boost. Since the data handled in this study were unbalanced data, data expansion by Synthetic Minority Over-sampling Technique (SMOTE) was effective. Mainly the accuracy of Recall has increased by 30%. Feature selection is performed by correlation, which is one of the filter methods. This can be expected to have the effect of improving accuracy and the effect of improving the interpretability of the model. We were able to reduce it from 46 variables to 22 variables. Furthermore, the accuracy increased by 1% for Binary Accuracy and 1% for Recall. The accuracy decreased when the number of variables was reduced by 23 variables or more. This is probably because important features have been deleted. Shows the accuracy of the model. The accuracy of Random Forest is Binary Accuracy = 61.3%, Recall = 58.2%. The accuracy of XGboost is Binary Accuracy = 60.3%, Recall = 61.6%. Therefore, XG boost became the model that can identify the default of the customer than the random forest.Finally, SHApley Additive exPlanations (SHAP) analyzes what variables contribute to the model. From this analysis result, we will explore the characteristics of what kind of person is the default customer. The variables with the highest contribution were the type of vehicle purchased, the area where the customer lives, and credit information. It turns out that customers who have gone loan bankruptcy in the past tend to be loan bankruptcy again.},
booktitle = {Human Interface and the Management of Information. Information-Rich and Intelligent Environments: Thematic Area, HIMI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II},
pages = {353–363},
numpages = {11},
keywords = {Loan, Loan bankruptcy, Credit risk model, Machine learning}
}

@inproceedings{10.1145/1868328.1868342,
author = {Jureczko, Marian and Madeyski, Lech},
title = {Towards identifying software project clusters with regard to defect prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868342},
doi = {10.1145/1868328.1868342},
abstract = {Background: This paper describes an analysis that was conducted on newly collected repository with 92 versions of 38 proprietary, open-source and academic projects. A preliminary study perfomed before showed the need for a further in-depth analysis in order to identify project clusters.Aims: The goal of this research is to perform clustering on software projects in order to identify groups of software projects with similar characteristic from the defect prediction point of view. One defect prediction model should work well for all projects that belong to such group. The existence of those groups was investigated with statistical tests and by comparing the mean value of prediction efficiency.Method: Hierarchical and k-means clustering, as well as Kohonen's neural network was used to find groups of similar projects. The obtained clusters were investigated with the discriminant analysis. For each of the identified group a statistical analysis has been conducted in order to distinguish whether this group really exists. Two defect prediction models were created for each of the identified groups. The first one was based on the projects that belong to a given group, and the second one - on all the projects. Then, both models were applied to all versions of projects from the investigated group. If the predictions from the model based on projects that belong to the identified group are significantly better than the all-projects model (the mean values were compared and statistical tests were used), we conclude that the group really exists.Results: Six different clusters were identified and the existence of two of them was statistically proven: 1) cluster proprietary B -- T=19, p=0.035, r=0.40; 2) cluster proprietary/open - t(17)=3.18, p=0.05, r=0.59. The obtained effect sizes (r) represent large effects according to Cohen's benchmark, which is a substantial finding.Conclusions: The two identified clusters were described and compared with results obtained by other researchers. The results of this work makes next step towards defining formal methods of reuse defect prediction models by identifying groups of projects within which the same defect prediction model may be used. Furthermore, a method of clustering was suggested and applied.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {9},
numpages = {10},
keywords = {clustering, defect prediction, design metrics, size metrics},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@article{10.1504/IJDATS.2016.075971,
author = {Erturk, Ezgi and Sezer, Ebru Akcapinar},
title = {Software fault prediction using Mamdani type fuzzy inference system},
year = {2016},
issue_date = {April 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {8},
number = {1},
issn = {1755-8050},
url = {https://doi.org/10.1504/IJDATS.2016.075971},
doi = {10.1504/IJDATS.2016.075971},
abstract = {High quality software requires the occurrence of minimum number of failures while software runs. Software fault prediction is the determining whether software modules are prone to fault or not. Identification of the modules or code segments which need detailed testing, editing or, reorganising can be possible with the help of software fault prediction systems. In literature, many studies present models for software fault prediction using some soft computing methods which use training/testing phases. As a result, they require historical data to build models. In this study, to eliminate this drawback, Mamdani type fuzzy inference system FIS is applied for the software fault prediction problem. Several FIS models are produced and assessed with ROC-AUC as performance measure. The results achieved are ranging between 0.7138 and 0.7304; they are encouraging us to try FIS with the different software metrics and data to demonstrate general FIS performance on this problem.},
journal = {Int. J. Data Anal. Tech. Strateg.},
month = apr,
pages = {14–28},
numpages = {15}
}

@inproceedings{10.1109/ASE.2015.56,
author = {Nam, Jaechang and Kim, Sunghun},
title = {CLAMI: defect prediction on unlabeled datasets},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.56},
doi = {10.1109/ASE.2015.56},
abstract = {Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distributions among datasets. Approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation, the necessity for manual effort.In this study, we propose novel approaches, CLA and CLAMI, that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort. The key idea of the CLA and CLAMI approaches is to label an unlabeled dataset by using the magnitude of metric values. In our empirical study on seven open-source projects, the CLAMI approach led to the promising prediction performances, 0.636 and 0.723 in average f-measure and AUC, that are comparable to those of defect prediction based on supervised learning.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {452–463},
numpages = {12},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1155/2021/4767388,
author = {Soleymani, Ali and Arabgol, Fatemeh and Shojae Chaeikar, Saman},
title = {A Novel Approach for Detecting DGA-Based Botnets in DNS Queries Using Machine Learning Techniques},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {2090-7141},
url = {https://doi.org/10.1155/2021/4767388},
doi = {10.1155/2021/4767388},
abstract = {In today’s security landscape, advanced threats are becoming increasingly difficult to detect as the pattern of attacks expands. Classical approaches that rely heavily on static matching, such as blacklisting or regular expression patterns, may be limited in flexibility or uncertainty in detecting malicious data in system data. This is where machine learning techniques can show their value and provide new insights and higher detection rates. The behavior of botnets that use domain-flux techniques to hide command and control channels was investigated in this research. The machine learning algorithm and text mining used to analyze the network DNS protocol and identify botnets were also described. For this purpose, extracted and labeled domain name datasets containing healthy and infected DGA botnet data were used. Data preprocessing techniques based on a text-mining approach were applied to explore domain name strings with n-gram analysis and PCA. Its performance is improved by extracting statistical features by principal component analysis. The performance of the proposed model has been evaluated using different classifiers of machine learning algorithms such as decision tree, support vector machine, random forest, and logistic regression. Experimental results show that the random forest algorithm can be used effectively in botnet detection and has the best botnet detection accuracy.},
journal = {J. Comput. Netw. Commun.},
month = jan,
numpages = {13}
}

@article{10.1016/j.eswa.2020.114176,
author = {AlOmar, Eman Abdullah and Peruma, Anthony and Mkaouer, Mohamed Wiem and Newman, Christian and Ouni, Ali and Kessentini, Marouane},
title = {How we refactor and how we document it? On the use of supervised machine learning algorithms to classify refactoring documentation},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114176},
doi = {10.1016/j.eswa.2020.114176},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {26},
keywords = {Machine learning, Software engineering, Software quality, Refactoring}
}

@article{10.1007/s11334-017-0294-1,
author = {Pandey, Nitish and Sanyal, Debarshi Kumar and Hudait, Abir and Sen, Amitava},
title = {Automated classification of software issue reports using machine learning techniques: an empirical study},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-017-0294-1},
doi = {10.1007/s11334-017-0294-1},
abstract = {Software developers, testers and customers routinely submit issue reports to software issue trackers to record the problems they face in using a software. The issues are then directed to appropriate experts for analysis and fixing. However, submitters often misclassify an improvement request as a bug and vice versa. This costs valuable developer time. Hence automated classification of the submitted reports would be of great practical utility. In this paper, we analyze how machine learning techniques may be used to perform this task. We apply different classification algorithms, namely naive Bayes, linear discriminant analysis, k-nearest neighbors, support vector machine (SVM) with various kernels, decision tree and random forest separately to classify the reports from three open-source projects. We evaluate their performance in terms of F-measure, average accuracy and weighted average F-measure. Our experiments show that random forests perform best, while SVM with certain kernels also achieve high performance.},
journal = {Innov. Syst. Softw. Eng.},
month = dec,
pages = {279–297},
numpages = {19},
keywords = {Random forest, Machine learning, F-measure, Bug classification, Accuracy}
}

@inproceedings{10.1145/2811411.2811544,
author = {Siebra, Clauirton A. and Mello, Michael A. B.},
title = {The importance of replications in software engineering: a case study in defect prediction},
year = {2015},
isbn = {9781450337380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811411.2811544},
doi = {10.1145/2811411.2811544},
abstract = {Prediction of defects in software is an important investigation area in software engineering, since such technique is able to return indications of parts of the code that are prone to contain problems. Thus, test teams can optimize the allocation of their resources by directing them to modules that are more defect-prone. The use of supervised learning is one of the approaches to support the design of prediction models. However, the erroneous use of training datasets can lead to poor models and, consequently, false results regarding accuracy. This work replicates important experiments of the area and shows how they could provide reliable results via the use of simple techniques of pre-processing. Based on the results, we discuss the importance of replications as method to find problems in current results and how this method is being motivated inside the software engineering area.},
booktitle = {Proceedings of the 2015 Conference on Research in Adaptive and Convergent Systems},
pages = {376–381},
numpages = {6},
keywords = {supervised learning, replication, defect prediction},
location = {Prague, Czech Republic},
series = {RACS '15}
}

@article{10.1007/s00500-019-03923-6,
author = {Yue, Chuan},
title = {An intuitionistic fuzzy projection-based approach and application to software quality evaluation },
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {1},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-03923-6},
doi = {10.1007/s00500-019-03923-6},
abstract = {Projection is a very important measure in decision science. However, this research finds that the existing projection measures are not always reasonable in intuitionistic fuzzy settings. To solve this problem, this work provides a new normalized projection measure. And this work establishes a new group decision-making model based on new normalized projection measure and TOPSIS (technique for order preference by similarity to ideal solution) technique. This paper also introduces a practical application to the software quality evaluation. An experimental analysis shows the practicability, feasibility and validity of method introduced in this paper. In a word, this article contributes to knowledge domain a new decision-making technique and tool.},
journal = {Soft Comput.},
month = jan,
pages = {429–443},
numpages = {15},
keywords = {Software quality evaluation, Interval-valued intuitionistic fuzzy vector, Intuitionistic fuzzy vector, Group decision-making, Normalized projection measure}
}

@article{10.1023/A:1024424811345,
author = {Khoshgoftaar, Taghi M. and Seliya, Naeem},
title = {Fault Prediction Modeling for Software Quality Estimation: Comparing Commonly Used Techniques},
year = {2003},
issue_date = {September 2003},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1023/A:1024424811345},
doi = {10.1023/A:1024424811345},
abstract = {High-assurance and complex mission-critical software systems are heavily dependent on reliability of their underlying software applications. An early software fault prediction is a proven technique in achieving high software reliability. Prediction models based on software metrics can predict number of faults in software modules. Timely predictions of such models can be used to direct cost-effective quality enhancement efforts to modules that are likely to have a high number of faults. We evaluate the predictive performance of six commonly used fault prediction techniques: CART-LS (least squares), CART-LAD (least absolute deviation), S-PLUS, multiple linear regression, artificial neural networks, and case-based reasoning. The case study consists of software metrics collected over four releases of a very large telecommunications system. Performance metrics, average absolute and average relative errors, are utilized to gauge the accuracy of different prediction models. Models were built using both, original software metrics (RAW) and their principle components (PCA). Two-way ANOVA randomized-complete block design models with two blocking variables are designed with average absolute and average relative errors as response variables. System release and the model type (RAW or PCA) form the blocking variables and the prediction technique is treated as a factor. Using multiple-pairwise comparisons, the performance order of prediction models is determined. We observe that for both average absolute and average relative errors, the CART-LAD model performs the best while the S-PLUS model is ranked sixth.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {255–283},
numpages = {29},
keywords = {software metrics, neural networks, multiple linear regression, fault prediction, case-based reasoning, Software quality prediction, S-PLUS, CART}
}

@inproceedings{10.5555/2040660.2040688,
author = {Wahyudin, Dindin and Ramler, Rudolf and Biffl, Stefan},
title = {A framework for defect prediction in specific software project contexts},
year = {2008},
isbn = {9783642223853},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software defect prediction has drawn the attention of many researchers in empirical software engineering and software maintenance due to its importance in providing quality estimates and to identify the needs for improvement from project management perspective. However, most defect prediction studies seem valid primarily in a particular context and little concern is given on how to find out which prediction model is well suited for a given project context. In this paper we present a framework for conducting software defect prediction as aid for the project manager in the context of a particular project or organization. The framework has been aligned with practitioners' requirements and is supported by our findings from a systematical literature review on software defect prediction. We provide a guide to the body of existing studies on defect prediction by mapping the results of the systematic literature review to the framework.},
booktitle = {Proceedings of the Third IFIP TC 2 Central and East European Conference on Software Engineering Techniques},
pages = {261–274},
numpages = {14},
keywords = {systematical literature review, software defect prediction, metric-based defect prediction},
location = {Brno, Czech Republic},
series = {CEE-SET'08}
}

@article{10.1016/j.csi.2017.02.003,
title = {An empirical analysis of the effectiveness of software metrics and fault prediction model for identifying faulty classes},
year = {2017},
issue_date = {August 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {53},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2017.02.003},
doi = {10.1016/j.csi.2017.02.003},
abstract = {Software fault prediction models are used to predict faulty modules at the very early stage of software development life cycle. Predicting fault proneness using source code metrics is an area that has attracted several researchers' attention. The performance of a model to assess fault proneness depends on the source code metrics which are considered as the input for the model. In this work, we have proposed a framework to validate the source code metrics and identify a suitable set of source code metrics with the aim to reduce irrelevant features and improve the performance of the fault prediction model. Initially, we applied a t-test analysis and univariate logistic regression analysis to each source code metric to evaluate their potential for predicting fault proneness. Next, we performed a correlation analysis and multivariate linear regression stepwise forward selection to find the right set of source code metrics for fault prediction. The obtained set of source code metrics are considered as the input to develop a fault prediction model using a neural network with five different training algorithms and three different ensemble methods. The effectiveness of the developed fault prediction models are evaluated using a proposed cost evaluation framework. We performed experiments on fifty six Open Source Java projects. The experimental results reveal that the model developed by considering the selected set of source code metrics using the suggested source code metrics validation framework as the input achieves better results compared to all other metrics. The experimental results also demonstrate that the fault prediction model is best suitable for projects with faulty classes less than the threshold value depending on fault identification efficiency (low 48.89%, median- 39.26%, and high 27.86%). HighlightsFault prediction improve the effectiveness of software quality assurance activities.This paper focus on building an effective fault prediction tool.Fault prediction model using ANN and ensemble methods.We perform experiments on 56 Open Source Java projects.Fault prediction model is best suitable for projects with faulty classes less than the threshold value.},
journal = {Comput. Stand. Interfaces},
month = aug,
pages = {1–32},
numpages = {32}
}

@inproceedings{10.1145/3379177.3388905,
author = {Liu, Hanyan and Eksmo, Samuel and Risberg, Johan and Hebig, Regina},
title = {Emerging and Changing Tasks in the Development Process for Machine Learning Systems},
year = {2020},
isbn = {9781450375122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379177.3388905},
doi = {10.1145/3379177.3388905},
abstract = {Integrating machine learning components in software systems is a task more and more companies are confronted with. However, there is not much knowledge today on how the software development process needs to change, when such components are integrated into a software system. We performed an interview study with 16 participants, focusing on emerging and changing task. The results uncover a set of 25 tasks associated to different software development phases, such as requirements engineering or deployment. We are just starting to understand the implications of using machine-learning components on the software development process. This study allows some first insights into how widespread the required process changes are.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {125–134},
numpages = {10},
keywords = {Software process, Roles, Machine learning, Challenges},
location = {Seoul, Republic of Korea},
series = {ICSSP '20}
}

@inproceedings{10.1145/2786805.2786813,
author = {Jing, Xiaoyuan and Wu, Fei and Dong, Xiwei and Qi, Fumin and Xu, Baowen},
title = {Heterogeneous cross-company defect prediction by unified metric representation and CCA-based transfer learning},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786813},
doi = {10.1145/2786805.2786813},
abstract = {Cross-company defect prediction (CCDP) learns a prediction model by using training data from one or multiple projects of a source company and then applies the model to the target company data. Existing CCDP methods are based on the assumption that the data of source and target companies should have the same software metrics. However, for CCDP, the source and target company data is usually heterogeneous, namely the metrics used and the size of metric set are different in the data of two companies. We call CCDP in this scenario as heterogeneous CCDP (HCCDP) task. In this paper, we aim to provide an effective solution for HCCDP. We propose a unified metric representation (UMR) for the data of source and target companies. The UMR consists of three types of metrics, i.e., the common metrics of the source and target companies, source-company specific metrics and target-company specific metrics. To construct UMR for source company data, the target-company specific metrics are set as zeros, while for UMR of the target company data, the source-company specific metrics are set as zeros. Based on the unified metric representation, we for the first time introduce canonical correlation analysis (CCA), an effective transfer learning method, into CCDP to make the data distributions of source and target companies similar. Experiments on 14 public heterogeneous datasets from four companies indicate that: 1) for HCCDP with partially different metrics, our approach significantly outperforms state-of-the-art CCDP methods; 2) for HCCDP with totally different metrics, our approach obtains comparable prediction performances in contrast with within-project prediction results. The proposed approach is effective for HCCDP.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {496–507},
numpages = {12},
keywords = {unified metric representation, company-specific metrics, common metrics, canonical correlation analysis (CCA), Heterogeneous cross-company defect prediction (HCCDP)},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1016/j.infsof.2019.01.008,
author = {Meqdadi, Omar and Alhindawi, Nouh and Alsakran, Jamal and Saifan, Ahmad and Migdadi, Hatim},
title = {Mining software repositories for adaptive change commits using machine learning techniques},
year = {2019},
issue_date = {May 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {109},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.01.008},
doi = {10.1016/j.infsof.2019.01.008},
journal = {Inf. Softw. Technol.},
month = may,
pages = {80–91},
numpages = {12},
keywords = {Machine learning, Maintenance classification, Commit types, Adaptive maintenance, Code change metrics}
}

@inproceedings{10.1145/3330089.3330121,
author = {Mateen, Ahmed and Zhu, Qingsheng and Afsar, Salman},
title = {Comparitive Analysis of Manual vs Automotive Testing for Software Quality},
year = {2018},
isbn = {9781450361019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330089.3330121},
doi = {10.1145/3330089.3330121},
abstract = {Success and failure of Software depend upon the quality of a software selection of an appropriate model for development of the product. In previous research many techniques were used to check the quality of software. But it is still a challenge for developers to select which technique may be best suited for quality of software. Quality attribute requirements such as those for performance, security, modifiability, reliability, and usability have a considerable influence on the software architecture of a system. Architects need to understand their designs in terms of quality attributes. Software quality assurance (SQA) consists of a means of monitoring the software engineering processes and methods used to ensure quality. The methods by which this is accomplished are varied and may include ensuring conformance to one or more standards. Both manual and automated testing offer benefits and disadvantages. In manual testing (as the name suggests), test cases are executed manually (by a human, that is) without any support from tools or scripts. But with automated testing, test cases are executed with the assistance of tools, scripts, and software.},
booktitle = {Proceedings of the 7th International Conference on Software Engineering and New Technologies},
articleno = {21},
numpages = {7},
keywords = {Radio Access Network, Network Retrieval, Fog Computing, Cloud Computing, 5 G Network},
location = {Hammamet, Tunisia},
series = {ICSENT 2018}
}

@inproceedings{10.1109/ICTAI.2004.108,
author = {Seliya, Naeem and Khoshgoftaar, Taghi M. and Zhong, Shi},
title = {Semi-Supervised Learning for Software Quality Estimation},
year = {2004},
isbn = {076952236X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2004.108},
doi = {10.1109/ICTAI.2004.108},
abstract = {A software quality estimation model is often built using known software metrics and fault data obtained from program modules of previously developed releases or similar projects. Such a sup ervise dlearning approach to software quality estimation assumes that fault data is available for all the previously developed modules. Considering the various practical issues in software project development, fault data may not be available for all the software modules in the training data. More specifically, the available labeled training data is such that a supervised learning approach may not yield good software quality prediction. In contrast, a supervised classification scheme aided by unlabeled data. i.e., semi-supervised learning, may yield better results. This paper investigates semi-supervised learning with the Expectation Maximization (EM) algorithm for the software quality classification problem. Case studies of software measurement data obtained from two NASA software projects, JM1 and KC2, are used in our empirical investigation. A small portion of the JM1 dataset is randomly extracte dand used as the labeled data, while the remaining JM1 instances are used as unlabeled data. The performance of the semi-supervised classification models built using the EM algorithm is evaluated by using the KC2 project as a test dataset. It is shown that the EM-based semi-supervised learning scheme improves the predictive accuracy of the software quality classification models.},
booktitle = {Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence},
pages = {183–190},
numpages = {8},
keywords = {unlabeled data, software quality estimation, software metrics, semi-supervise d learning, expectation maximization},
series = {ICTAI '04}
}

@article{10.1145/3450288,
author = {Lo, Sin Kit and Lu, Qinghua and Wang, Chen and Paik, Hye-Young and Zhu, Liming},
title = {A Systematic Literature Review on Federated Machine Learning: From a Software Engineering Perspective},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3450288},
doi = {10.1145/3450288},
abstract = {Federated learning is an emerging machine learning paradigm where clients train models locally and formulate a global model based on the local model updates. To identify the state-of-the-art in federated learning and explore how to develop federated learning systems, we perform a systematic literature review from a software engineering perspective, based on 231 primary studies. Our data synthesis covers the lifecycle of federated learning system development that includes background understanding, requirement analysis, architecture design, implementation, and evaluation. We highlight and summarise the findings from the results and identify future trends to encourage researchers to advance their current work.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {95},
numpages = {39},
keywords = {systematic literature review, software engineering, privacy, edge learning, distributed learning, Federated learning}
}

@article{10.1007/s00500-016-2284-x,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {An empirical study of some software fault prediction techniques for the number of faults prediction},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {24},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-016-2284-x},
doi = {10.1007/s00500-016-2284-x},
abstract = {During the software development process, prediction of the number of faults in software modules can be more helpful instead of predicting the modules being faulty or non-faulty. Such an approach may help in more focused software testing process and may enhance the reliability of the software system. Most of the earlier works on software fault prediction have used classification techniques for classifying software modules into faulty or non-faulty categories. The techniques such as Poisson regression, negative binomial regression, genetic programming, decision tree regression, and multilayer perceptron can be used for the prediction of the number of faults. In this paper, we present an experimental study to evaluate and compare the capability of six fault prediction techniques such as genetic programming, multilayer perceptron, linear regression, decision tree regression, zero-inflated Poisson regression, and negative binomial regression for the prediction of number of faults. The experimental investigation is carried out for eighteen software project datasets collected from the PROMISE data repository. The results of the investigation are evaluated using average absolute error, average relative error, measure of completeness, and prediction at level l measures. We also perform Kruskal---Wallis test and Dunn's multiple comparison test to compare the relative performance of the considered fault prediction techniques.},
journal = {Soft Comput.},
month = dec,
pages = {7417–7434},
numpages = {18},
keywords = {Zero-inflated Poisson regression, Software fault prediction, Multilayer perceptron, Kruskal---Wallis test, Genetic programming, Dunn's multiple comparison test}
}

@article{10.1007/s10664-011-9180-x,
author = {Ekanayake, Jayalath and Tappolet, Jonas and Gall, Harald C. and Bernstein, Abraham},
title = {Time variance and defect prediction in software projects},
year = {2012},
issue_date = {August    2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4–5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-011-9180-x},
doi = {10.1007/s10664-011-9180-x},
abstract = {It is crucial for a software manager to know whether or not one can rely on a bug prediction model. A wrong prediction of the number or the location of future bugs can lead to problems in the achievement of a project's goals. In this paper we first verify the existence of variability in a bug prediction model's accuracy over time both visually and statistically. Furthermore, we explore the reasons for such a high variability over time, which includes periods of stability and variability of prediction quality, and formulate a decision procedure for evaluating prediction models before applying them. To exemplify our findings we use data from four open source projects and empirically identify various project features that influence the defect prediction quality. Specifically, we observed that a change in the number of authors editing a file and the number of defects fixed by them influence the prediction quality. Finally, we introduce an approach to estimate the accuracy of prediction models that helps a project manager decide when to rely on a prediction model. Our findings suggest that one should be aware of the periods of stability and variability of prediction quality and should use approaches such as ours to assess their models' accuracy in advance.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {348–389},
numpages = {42},
keywords = {Time variance, Mining software repository, Defect prediction, Decision tree learner, Concept drift}
}

@article{10.1007/s00607-019-00781-w,
author = {Renga, Daniela and Apiletti, Daniele and Giordano, Danilo and Nisi, Matteo and Huang, Tao and Zhang, Yang and Mellia, Marco and Baralis, Elena},
title = {Data-driven exploratory models of an electric distribution network for fault prediction and diagnosis},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {102},
number = {5},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-019-00781-w},
doi = {10.1007/s00607-019-00781-w},
abstract = {Data-driven models are becoming of fundamental importance in electric distribution networks to enable predictive maintenance, to perform effective diagnosis and to reduce related expenditures, with the final goal of improving the electric service efficiency and reliability to the benefit of both the citizens and the grid operators themselves. This paper considers a dataset collected over 6 years in a real-world medium-voltage distribution network by the Supervisory Control And Data Acquisition (SCADA) system. A transparent, exploratory, and exhaustive data-mining workflow, based on data characterisation, time-windowing, association rule mining, and associative classification is proposed and experimentally evaluated to automatically identify correlations and build a prognostic–diagnostic model from the SCADA events occurring before and after specific service interruptions, i.e., network faults. Our results, evaluated by both data-driven quality metrics and domain expert interpretations, highlight the capability to assess the limited predictive capability of the SCADA events for medium-voltage distribution networks, while their effective exploitation for diagnostic purposes is promising.},
journal = {Computing},
month = may,
pages = {1199–1211},
numpages = {13},
keywords = {68T04, Associative classification, Data mining, Medium Voltage distribution networks, Fault diagnosis, Predictive maintenance, Smart grid}
}

@article{10.1145/3343440,
author = {Kaur, Harsurinder and Pannu, Husanbir Singh and Malhi, Avleen Kaur},
title = {A Systematic Review on Imbalanced Data Challenges in Machine Learning: Applications and Solutions},
year = {2019},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3343440},
doi = {10.1145/3343440},
abstract = {In machine learning, the data imbalance imposes challenges to perform data analytics in almost all areas of real-world research. The raw primary data often suffers from the skewed perspective of data distribution of one class over the other as in the case of computer vision, information security, marketing, and medical science. The goal of this article is to present a comparative analysis of the approaches from the reference of data pre-processing, algorithmic and hybrid paradigms for contemporary imbalance data analysis techniques, and their comparative study in lieu of different data distribution and their application areas.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {79},
numpages = {36},
keywords = {sampling, machine learning, data analysis, Data imbalance}
}

@article{10.1007/s10664-008-9064-x,
author = {Hewett, Rattikorn and Kijsanayothin, Phongphun},
title = {On modeling software defect repair time},
year = {2009},
issue_date = {April     2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9064-x},
doi = {10.1007/s10664-008-9064-x},
abstract = {The ability to predict the time required to repair software defects is important for both software quality management and maintenance. Estimated repair times can be used to improve the reliability and time-to-market of software under development. This paper presents an empirical approach to predicting defect repair times by constructing models that use well-established machine learning algorithms and defect data from past software defect reports. We describe, as a case study, the analysis of defect reports collected during the development of a large medical software system. Our predictive models give accuracies as high as 93.44%, despite the limitations of the available data. We present the proposed methodology along with detailed experimental results, which include comparisons with other analytical modeling approaches.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {165–186},
numpages = {22},
keywords = {Testing management, Software testing, Quality assurance, Defect report analysis, Data mining}
}

@article{10.1111/exsy.12078,
author = {Malhotra, Ruchika and Bansal, Ankita Jain},
title = {Fault prediction considering threshold effects of object-oriented metrics},
year = {2015},
issue_date = {April 2015},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {32},
number = {2},
issn = {0266-4720},
url = {https://doi.org/10.1111/exsy.12078},
doi = {10.1111/exsy.12078},
abstract = {Software product quality can be enhanced significantly if we have a good knowledge and understanding of the potential faults therein. This paper describes a study to build predictive models to identify parts of the software that have high probability of occurrence of fault. We have considered the effect of thresholds of object-oriented metrics on fault proneness and built predictive models based on the threshold values of the metrics used. Prediction of fault prone classes in earlier phases of software development life cycle will help software developers in allocating the resources efficiently. In this paper, we have used a statistical model derived from logistic regression to calculate the threshold values of object oriented, Chidamber and Kemerer metrics. Thresholds help developers to alarm the classes that fall outside a specified risk level. In this way, using the threshold values, we can divide the classes into two levels of risk - low risk and high risk. We have shown threshold effects at various risk levels and validated the use of these thresholds on a public domain, proprietary dataset, KC1 obtained from NASA and two open source, Promise datasets, IVY and JEdit using various machine learning methods and data mining classifiers. Interproject validation has also been carried out on three different open source datasets, Ant and Tomcat and Sakura. This will provide practitioners and researchers with well formed theories and generalised results. The results concluded that the proposed threshold methodology works well for the projects of similar nature or having similar characteristics.},
journal = {Expert Sys: J. Knowl. Eng.},
month = apr,
pages = {203–219},
numpages = {17},
keywords = {software quality, receiver operating characteristics, object-oriented metrics, machine learning, logistic regression, empirical validation}
}

@article{10.1016/j.jss.2019.03.027,
author = {Xu, Zhou and Li, Shuai and Luo, Xiapu and Liu, Jin and Zhang, Tao and Tang, Yutian and Xu, Jun and Yuan, Peipei and Keung, Jacky},
title = {TSTSS: A two-stage training subset selection framework for cross version defect prediction},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.027},
doi = {10.1016/j.jss.2019.03.027},
journal = {J. Syst. Softw.},
month = aug,
pages = {59–78},
numpages = {20},
keywords = {99-00, 00–01, Weighted extreme learning machine, Training subset selection, Spare modeling, Cross version defect prediction}
}

@inproceedings{10.5220/0006694502410248,
author = {Tamai, Tetsuo and Anzai, Taichi},
title = {Quality Requirements Analysis with Machine Learning},
year = {2018},
isbn = {9789897583001},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0006694502410248},
doi = {10.5220/0006694502410248},
abstract = {The importance of software quality requirements (QR) is being widelyrecognized, which motivates studies that investigate softwarerequirements specifications (SRS) in practice and collect data on howmuch QR are written vs. functional requirements (FR) and what kindof QR are specified. It is useful to develop a tool that automatesthe process of filtering out QR statements from an SRS and classifyingthem into the quality characteristic attributes such as defined in theISO/IEC 25000 quality model.We propose an approach that uses a machine learning technique to mechanizethe process. With this mechanism, we can identify how each QRcharacteristic scatters over the document, i.e. how much in volume andin what way. A tool textit{QRMiner} is developed to support theprocess and case studies were conducted, taking thirteen SRS documentsthat were written for real use. We report our findings from these cases},
booktitle = {Proceedings of the 13th International Conference on Evaluation of Novel Approaches to Software Engineering},
pages = {241–248},
numpages = {8},
keywords = {Requirements Classification, Quality Requirements, Natural Language Processing., Machine Learning},
location = {Funchal, Madeira, Portugal},
series = {ENASE 2018}
}

@article{10.5555/3292849.3292858,
title = {A hybrid approach to improve the quality of software fault prediction using Na\"{\i}ve Bayes and k-NN classification algorithm with ensemble method},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {17},
number = {4},
issn = {1740-8865},
abstract = {This paper considers an improvisation in software fault prediction research area using supervised classification algorithms and it mainly focuses to increase the performance of fault prediction. In this paper, we propose a hybrid prediction model using Na\"{\i}ve Bayes and k-nearest neighbour classification algorithm with vote ensemble method; in short it called as hNK. The goal of this model is to predict the best classification algorithm for software fault prediction based on the metrics and attributes of datasets. In the work, we have applied training sets and testing sets in hNK model with ensemble vote and we proposed the model to identify a suitable classification algorithm for fault prediction based on the accuracy and precision. We have achieved better results using hNK model for classifying supervised algorithms with different dataset.},
journal = {Int. J. Intell. Syst. Technol. Appl.},
month = jan,
pages = {483–496},
numpages = {14}
}

@inproceedings{10.1145/3338906.3338937,
author = {Aggarwal, Aniya and Lohia, Pranay and Nagar, Seema and Dey, Kuntal and Saha, Diptikalyan},
title = {Black box fairness testing of machine learning models},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338937},
doi = {10.1145/3338906.3338937},
abstract = {Any given AI system cannot be accepted unless its trustworthiness is proven. An important characteristic of a trustworthy AI system is the absence of algorithmic bias. 'Individual discrimination' exists when a given individual different from another only in 'protected attributes' (e.g., age, gender, race, etc.) receives a different decision outcome from a given machine learning (ML) model as compared to the other individual. The current work addresses the problem of detecting the presence of individual discrimination in given ML models. Detection of individual discrimination is test-intensive in a black-box setting, which is not feasible for non-trivial systems. We propose a methodology for auto-generation of test inputs, for the task of detecting individual discrimination. Our approach combines two well-established techniques - symbolic execution and local explainability for effective test case generation. We empirically show that our approach to generate test cases is very effective as compared to the best-known benchmark systems that we examine.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {625–635},
numpages = {11},
keywords = {Symbolic Execution, Local Explainability, Individual Discrimination, Fairness Testing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1007/978-3-030-30949-7_28,
author = {Farooq, Basit and Bao, Jinsong},
title = {Machine Learning Method for Spinning Cyber-Physical Production System Subject to Condition Monitoring},
year = {2019},
isbn = {978-3-030-30948-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30949-7_28},
doi = {10.1007/978-3-030-30949-7_28},
abstract = {Digitalization encapsulates the importance of machine condition monitoring which is subjected to predictive analytics for realizing significant improvements in the performance and reliability of rotating equipment i.e., spinning. This paper presents a machine learning approach for condition monitoring, based on a regularized deep neural network using automated diagnostics for spinning manufacturing. This article contributes a solution to find disturbances in a running system through real-time data sensing and signal to process via industrial internet of things. Because this controlled sensor network may comprise on different critical components of the same type of machines, therefore back propagation neural network based multi-sensor performance assessment and prediction strategy were developed for our system which worked as intelligent maintenance and diagnostic system. It is completely automatic requiring no manual extraction of handcrafted features.},
booktitle = {Cooperative Design, Visualization, and Engineering: 16th International Conference, CDVE 2019, Mallorca, Spain, October 6–9, 2019, Proceedings},
pages = {244–253},
numpages = {10},
keywords = {Spinning, Prognostics and health management, Machine learning, Condition monitoring, Cyber-physical production system},
location = {Mallorca, Spain}
}

@inproceedings{10.1109/ACT.2009.212,
author = {Singh, Pradeep and Verma, Shirish},
title = {An Investigation of the Effect of Discretization on Defect Prediction Using Static Measures},
year = {2010},
isbn = {9780769539157},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ACT.2009.212},
doi = {10.1109/ACT.2009.212},
abstract = {Software repositories with defect logs are main resource for defect prediction. In recent years, researchers have used the vast amount of data that is contained by software repositories to predict the location of defect in the code that caused problems. In this paper we evaluate the effectiveness of software fault prediction with Naive-Bayes classifiers and J48 classifier by integrating with supervised discretization algorithm developed by Fayyad and Irani. Public datasets from the promise repository have been explored for this purpose. The repository contains software metric data and error data at the function/method level. Our experiment shows that integration of discretization method improves the software fault prediction accuracy when integrated with Naive-Bayes and J48 classifiers},
booktitle = {Proceedings of the 2009 International Conference on Advances in Computing, Control, and Telecommunication Technologies},
pages = {837–839},
numpages = {3},
keywords = {Software metrics, Machine learning, Defect prediction},
series = {ACT '09}
}

@article{10.1007/s10462-019-09760-1,
author = {Hassanien, Aboul Ella and Darwish, Ashraf and Abdelghafar, Sara},
title = {Machine learning in telemetry data mining of space mission: basics, challenging and future directions},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {5},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-019-09760-1},
doi = {10.1007/s10462-019-09760-1},
abstract = {The development of an intelligent artificial satellite health monitoring system is a key issue in aerospace engineering that determines satellite health status and failure using telemetry data. The modern design of data mining and machine learning technologies allows the use of satellite telemetry data and the mining of integrated information to produce an advanced health monitoring system. This paper reviews the current status and presents a framework of necessary processes on data mining to solving various problems in telemetry data such as error detection, prediction, summarization, and visualization of large quantities, and help them understand the health status of the satellite and detect the symptoms of anomalies. Machine learning technologies that include neural networks, fuzzy sets, rough sets, support vector machines, Naive Bayesian, swarm optimization, and deep learning are also presented. Also, this paper reviews a wide range of existing satellite health monitoring solutions and discusses them in the framework of remote data mining techniques. In addition, we are discussing the analysis of space debris flow analysis and the prediction of low earth orbit collision based on our orbital Petri nets model. Challenges to be addressed and future directions of research are identified and an extensive bibliography is also included.},
journal = {Artif. Intell. Rev.},
month = jun,
pages = {3201–3230},
numpages = {30},
keywords = {Aerospace engineering, Debris, Deep learning, Machine learning, Satellite ground control operations, Satellite health monitoring, Satellite telemetry data mining}
}

@inproceedings{10.1145/2590748.2590755,
author = {Rathore, Santosh Singh and Gupta, Atul},
title = {A comparative study of feature-ranking and feature-subset selection techniques for improved fault prediction},
year = {2014},
isbn = {9781450327763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590748.2590755},
doi = {10.1145/2590748.2590755},
abstract = {The quality of a fault prediction model depends on the software metrics that are used to build the prediction model. Feature selection represents a process of selecting a subset of relevant features that may lead to build improved prediction models. Feature selection techniques can be broadly categorized into two subcategories: feature-ranking and feature-subset selection. In this paper, we present a comparative investigation of seven feature-ranking techniques and eight feature-subset selection techniques for improved fault prediction. The performance of these feature selection techniques is evaluated using two popular machine-learning classifiers: Naive Bayes and Random Forest, over fourteen software project's fault-datasets obtained from the PROMISE data repository. The performances were measured using F-measure and AUC values. Our results demonstrated that feature-ranking techniques produced better results compared to feature-subset selection techniques. Among, the feature-ranking techniques used in the study, InfoGain and PCA techniques provided the best performance over all the datasets, while for feature-subset selection techniques ClassifierSubsetEval and Logistic Regression produced better results against their peers.},
booktitle = {Proceedings of the 7th India Software Engineering Conference},
articleno = {7},
numpages = {10},
keywords = {wrappers, software metrics, filters, feature-ranking, feature selection, fault prediction},
location = {Chennai, India},
series = {ISEC '14}
}

@inproceedings{10.1145/2499393.2499404,
author = {Lochmann, Klaus and Ramadani, Jasmin and Wagner, Stefan},
title = {Are comprehensive quality models necessary for evaluating software quality?},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499404},
doi = {10.1145/2499393.2499404},
abstract = {The concept of software quality is very complex and has many facets. Reflecting all these facets and at the same time measuring everything related to these facets results in comprehensive but large quality models and extensive measurements. In contrast, there are also many smaller, focused quality models claiming to evaluate quality with few measures.We investigate if and to what extent it is possible to build a focused quality model with similar evaluation results as a comprehensive quality model but with far less measures needed to be collected and, hence, reduced effort. We make quality evaluations with the comprehensive Quamoco base quality model and build focused quality models based on the same set of measures and data from over 2,000 open source systems. We analyse the ability of the focused model to predict the results of the Quamoco model by comparing them with a random predictor as a baseline. We calculate the standardised accuracy measure SA and effect sizes.We found that for the Quamoco model and its 378 automatically collected measures, we can build a focused model with only 10 measures but an accuracy of 61% and a medium to high effect size. We conclude that we can build focused quality models to get an impression of a system's quality similar to comprehensive models. However, when including manually collected measures, the accuracy of the models stayed below 50%. Hence, manual measures seem to have a high impact and should therefore not be ignored in a focused model.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {3},
numpages = {9},
keywords = {quality evaluation, quality model, software quality},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@article{10.1016/j.engappai.2017.09.008,
author = {Wang, Hong-Qiao and Cai, Yan-Ning and Fu, Guang-Yuan and Wu, Ming and Wei, Zhen-Hua},
title = {Data-driven fault prediction and anomaly measurement for complex systems using support vector probability density estimation},
year = {2018},
issue_date = {January 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {67},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2017.09.008},
doi = {10.1016/j.engappai.2017.09.008},
abstract = {To quantitatively monitor the state of complex system, a data-driven fault prediction and anomaly degree measurement method based on probability density estimation is studied in this paper. First, an anomaly index is introduced and defined to measure the anomaly degree of samples. Then By improving the form of constraint condition, a single slack factor multiple kernel support vector machine probability density estimation model is presented. As a result, the scale of object function and the solution number are all reduced, and the computational efficiency of the presented model is greatly enhanced. On the other hand, as the introduction of multiple kernel functions, a multiple kernel matrix with better data mapping performance is obtained, which can well solve the composite probability density estimation for uncoupled data. The simulation test shows that the presented model has higher estimation precision and speed. The experiments on complex system fault prediction also show that the systems anomaly degree can be quantitatively and accurately measured by the anomaly index gained from the prediction results, which can effectively improve the fault prediction precision and increase the prediction advances.},
journal = {Eng. Appl. Artif. Intell.},
month = jan,
pages = {1–13},
numpages = {13},
keywords = {Support vector machine, Probability density estimation, Fault prediction, Data-driven, Anomaly degree measurement}
}

@phdthesis{10.5555/AAI28022522,
author = {Islam, Md Johirul and Ciardo, Gianfranco and Prabhu, Gurpur and Tian, Jin and Sharma, Anuj},
advisor = {Hridesh, Rajan,},
title = {Towards Understanding the Challenges Faced by Machine Learning Software Developers and Enabling Automated Solutions},
year = {2020},
isbn = {9798672106496},
publisher = {Iowa State University},
address = {USA},
abstract = {Modern software systems are increasingly including machine learning (ML) as an integral component. However, we do not yet understand the difficulties faced by software developers when learning about ML libraries and using them within their systems. To fill that gap this thesis reports on a detailed (manual) examination of 3,243 highly-rated Q&amp;A posts related to ten ML libraries, namely Tensorflow, Keras, scikitlearn, Weka, Caffe, Theano, MLlib, Torch, Mahout, and H2O, on Stack Overflow, a popular online technical Q&amp;A forum. Our findings reveal the urgent need for software engineering (SE) research in this area.The second part of the thesis particularly focuses on understanding the Deep Neural Network (DNN) bug characteristics. We study 2,716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, their root causes and impacts, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software.While exploring the bug characteristics, our findings imply that repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. So, the third part of this thesis presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from Github for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns and the most common bug fix patterns are fixing data dimension and neural network connectivity.Finally, we propose an automatic technique to detect ML Application Programming Interface (API) misuses. We started with an empirical study to understand ML API misuses. Our study shows that ML API misuse is prevalent and distinct compared to non-ML API misuses. Inspired by these findings, we contributed Amimla (Api Misuse In Machine Learning Apis) an approach and a tool for ML API misuse detection. Amimla relies on several technical innovations. First, we proposed an abstract representation of ML pipelines to use in misuse detection. Second, we proposed an abstract representation of neural networks for deep learning related APIs. Third, we have developed a representation strategy for constraints on ML APIs. Finally, we have developed a misuse detection strategy for both single and multi-APIs. Our experimental evaluation shows that Amimla achieves a high average accuracy of ∼80% on two benchmarks of misuses from Stack Overflow and Github.},
note = {AAI28022522}
}

@article{10.1016/j.procs.2015.02.154,
author = {Mahajan, Rohit and Gupta, Sunil Kumar and Bedi, Rajeev Kumar},
title = {Design of Software Fault Prediction Model Using BR Technique},
year = {2015},
issue_date = {2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2015.02.154},
doi = {10.1016/j.procs.2015.02.154},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {849–858},
numpages = {10},
keywords = {public dataset ;, Neural network, Levenberg-Marquardt (LM)algorithm, Back Propagation (BPA) algorithm ;Bayesian Regularization(BR)algorithml}
}

@article{10.1016/j.cosrev.2021.100376,
author = {Amutha, J. and Sharma, Sandeep and Sharma, Sanjay Kumar},
title = {Strategies based on various aspects of clustering in wireless sensor networks using classical, optimization and machine learning techniques: Review, taxonomy, research findings, challenges and future directions},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2021.100376},
doi = {10.1016/j.cosrev.2021.100376},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {43},
keywords = {Reliability, Security, Routing, Machine learning, Optimization, Wireless Sensor Networks}
}

@article{10.1016/j.jksuci.2014.12.008,
author = {Goyal, Rinkaj and Chandra, Pravin and Singh, Yogesh},
title = {Fuzzy inferencing to identify degree of interaction in the development of fault prediction models},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {29},
number = {1},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2014.12.008},
doi = {10.1016/j.jksuci.2014.12.008},
abstract = {The software fault prediction models, based on different modeling techniques have been extensively researched to improve software quality for the last three decades. Out of the analytical techniques used by the researchers, fuzzy modeling and its variants are bringing out a major share of the attention of research communities. In this work, we demonstrate the models developed through data driven fuzzy inference system. A comprehensive set of rules induced by such an inference system, followed by a simplification process provides deeper insight into the linguistically identified level of interaction. This work makes use of a publicly available data repository for four software modules, advocating the consideration of compound effects in the model development, especially in the area of software measurement.One related objective is the identification of influential metrics in the development of fault prediction models. A fuzzy rule intrinsically represents a form of interaction between fuzzified inputs. Analysis of these rules establishes that Low and NOT (High) level of inheritance based metrics significantly contributes to the F-measure estimate of the model. Further, the Lack of Cohesion of Methods (LCOM) metric was found insignificant in this empirical study.},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = jan,
pages = {93–102},
numpages = {10},
keywords = {Software fault prediction, Object oriented metrics, Influential metrics, Fuzzy inference system}
}

@article{10.1016/j.imavis.2016.11.013,
author = {Yue, Zongsheng and Meng, Deyu and He, Juan and Zhang, Gemeng},
title = {Semi-supervised learning through adaptive Laplacian graph trimming},
year = {2017},
issue_date = {April 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {60},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.11.013},
doi = {10.1016/j.imavis.2016.11.013},
abstract = {Graph-based semi-supervised learning (GSSL) attracts considerable attention in recent years. The performance of a general GSSL method relies on the quality of Laplacian weighted graph (LWR) composed of the similarity imposed on input examples. A key for constructing an effective LWR is on the proper selection of the neighborhood size K or on the construction of KNN graph or -neighbor graph on training samples, which constitutes the fundamental elements in LWR. Specifically, too large K or will result in shortcut phenomenon while too small ones cannot guarantee to represent a complete manifold structure underlying data. To this issue, this study attempts to propose a method, called adaptive Laplacian graph trimming (ALGT), to make an automatic tuning to cut improper inter-cluster shortcut edges while enhance the connection between intra-cluster samples, so as to adaptively fit a proper LWR from data. The superiority of the proposed method is substantiated by experimental results implemented on synthetic and UCI data sets. A method which can adaptively fit a proper Laplacian weighted graph from data.A penalty helping cut inter-cluster shortcuts and enhance intra-cluster connections.A graph-based SSL model is less sensitive to neighborhood size by integrating ALGT.Superiority of ALGT is verified by experimental results on synthetic and UCI data.},
journal = {Image Vision Comput.},
month = apr,
pages = {38–47},
numpages = {10},
keywords = {Semi-supervised learning, Self-paced learning, Nearest neighborhood graph, Graph Laplacian}
}

@article{10.1155/2021/6612342,
author = {Li, Yao and Dourado, Ant\'{o}nio},
title = {A Fault Prediction and Cause Identification Approach in Complex Industrial Processes Based on Deep Learning},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/6612342},
doi = {10.1155/2021/6612342},
abstract = {Faults occurring in the production line can cause many losses. Predicting the fault events before they occur or identifying the causes can effectively reduce such losses. A modern production line can provide enough data to solve the problem. However, in the face of complex industrial processes, this problem will become very difficult depending on traditional methods. In this paper, we propose a new approach based on a deep learning (DL) algorithm to solve the problem. First, we regard these process data as a spatial sequence according to the production process, which is different from traditional time series data. Second, we improve the long short-term memory (LSTM) neural network in an encoder-decoder model to adapt to the branch structure, corresponding to the spatial sequence. Meanwhile, an attention mechanism (AM) algorithm is used in fault detection and cause identification. Third, instead of traditional biclassification, the output is defined as a sequence of fault types. The approach proposed in this article has two advantages. On the one hand, treating data as a spatial sequence rather than a time sequence can overcome multidimensional problems and improve prediction accuracy. On the other hand, in the trained neural network, the weight vectors generated by the AM algorithm can represent the correlation between faults and the input data. This correlation can help engineers identify the cause of faults. The proposed approach is compared with some well-developed fault diagnosing methods in the Tennessee Eastman process. Experimental results show that the approach has higher prediction accuracy, and the weight vector can accurately label the factors that cause faults.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {13}
}

@phdthesis{10.5555/AAI28216179,
author = {Li, Xia},
advisor = {Lingming, Zhang,},
title = {An Integrated Approach for Automated Software Debugging via Machine Learning and Big Code Mining},
year = {2020},
isbn = {9798678186591},
publisher = {The University of Texas at Dallas},
abstract = {Over the past decades, software systems have been widely adopted in almost all aspects of human lives, and are making our lives more and more convenient. However, software systems also inevitably suffer from different faults (a.k.a., bugs), which can incur great loss of properties and even lives. Due to the huge code volume, manual debugging can be always time-consuming and error-prone. This thesis is a novel integrated approach for automated debugging that can help localize and detect different software faults. Specifically, fault localization (FL) can help localize the potential faulty location(s) if some test cases fail in a program while API-misuse detection can help detect API related bugs due to API misuses without the execution of test cases. We seek to improve the effectiveness of fault localization and API misuses detection by applying knowledge from various fields such as static and dynamic program analysis, machine learning/deep learning techniques, as well as mining big code repositories. In this dissertation, we propose two fault localization techniques and one API-misuse detection technique. The first fault localization technique is called TraPT, which is a learning-to-rank-based technique to combine transformed impact information extracted from mutation-based fault localization (MBFL) and coverage information extracted from spectrum-based fault localization (SBFL). The second fault localization technique is called DeepFL which is the first deep-learning-based fault localization technique integrating various dynamic and static program features. The two fault localization techniques rely on high-quality test cases to capture necessary program features but not all software systems can provide such tests, making fault localization techniques not always available. To solve more comprehensive debugging problems, we also propose an API-misuse detection technique called BiD3 based on the analysis of a large-scale of bug-fixing commits (958,368 commits in total) in history, which doesn't require the execution of test cases. Various experiments on the three techniques show the promising effectiveness. For example, DeepFL can localize 213 faults within Top-1 out of 395 real faults, 53 more faults than state-of-the-art technique (33.1% improvement). BiD3 can detect 360 real misuses in the latest Apache projects and 57 misuses have been confirmed and fixed by developers.},
note = {AAI28216179}
}

@inproceedings{10.1145/3416505.3423563,
author = {Palma, Stefano Dalla and Mohammadi, Majid and Di Nucci, Dario and Tamburri, Damian A.},
title = {Singling the odd ones out: a novelty detection approach to find defects in infrastructure-as-code},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423563},
doi = {10.1145/3416505.3423563},
abstract = {Infrastructure-as-Code (IaC) is increasingly adopted. However, little is known about how to best maintain and evolve it. Previous studies focused on defining Machine-Learning models to predict defect-prone blueprints using supervised binary classification. This class of techniques uses both defective and non-defective instances in the training phase. Furthermore, the high imbalance between defective and non-defective samples makes the training more difficult and leads to unreliable classifiers. In this work, we tackle the defect-prediction problem from a different perspective using novelty detection and evaluate the performance of three techniques, namely OneClassSVM, LocalOutlierFactor, and IsolationForest, and compare their performance with a baseline RandomForest binary classifier. Such models are trained using only non-defective samples: defective data points are treated as novelty because the number of defective samples is too little compared to defective ones. We conduct an empirical study on an extremely-imbalanced dataset consisting of 85 real-world Ansible projects containing only small amounts of defective instances. We found that novelty detection techniques can recognize defects with a high level of precision and recall, an AUC-PR up to 0.86, and an MCC up to 0.31. We deem our results can influence the current trends in defect detection and put forward a new research path toward dealing with this problem.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {31–36},
numpages = {6},
keywords = {Novelty Detection, Infrastructure-as-Code, Defect Prediction},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@article{10.1002/spe.1043,
author = {Gao, Kehan and Khoshgoftaar, Taghi M. and Wang, Huanjing and Seliya, Naeem},
title = {Choosing software metrics for defect prediction: an investigation on feature selection techniques},
year = {2011},
issue_date = {April 2011},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {41},
number = {5},
issn = {0038-0644},
url = {https://doi.org/10.1002/spe.1043},
doi = {10.1002/spe.1043},
abstract = {The selection of software metrics for building software quality prediction models is a search-based software engineering problem. An exhaustive search for such metrics is usually not feasible due to limited project resources, especially if the number of available metrics is large. Defect prediction models are necessary in aiding project managers for better utilizing valuable project resources for software quality improvement. The efficacy and usefulness of a fault-proneness prediction model is only as good as the quality of the software measurement data. This study focuses on the problem of attribute selection in the context of software quality estimation. A comparative investigation is presented for evaluating our proposed hybrid attribute selection approach, in which feature ranking is first used to reduce the search space, followed by a feature subset selection. A total of seven different feature ranking techniques are evaluated, while four different feature subset selection approaches are considered. The models are trained using five commonly used classification algorithms. The case study is based on software metrics and defect data collected from multiple releases of a large real-world software system. The results demonstrate that while some feature ranking techniques performed similarly, the automatic hybrid search algorithm performed the best among the feature subset selection methods. Moreover, performances of the defect prediction models either improved or remained unchanged when over 85were eliminated. Copyright © 2011 John Wiley &amp; Sons, Ltd.},
journal = {Softw. Pract. Exper.},
month = apr,
pages = {579–606},
numpages = {28},
keywords = {software quality, software metric, search-based software engineering, feature subset selection, feature ranking, defect prediction, attribute selection}
}

@inproceedings{10.5555/1890580.1890595,
author = {Zhong, Shi and Khoshgoftaar, Taghi M. and Seliya, Naeem},
title = {Unsupervised learning for expert-based software quality estimation},
year = {2004},
isbn = {0769520944},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Current software quality estimation models often involve using supervised learning methods to train a software quality classifier or a software fault prediction model. In such models, the dependent variable is a software quality measurement indicating the quality of a software module by either a risk-based class membership (e.g., whether it is fault-prone or not fault-prone) or the number of faults. In reality, such a measurement may be inaccurate, or even unavailable. In such situations, this paper advocates the use of unsupervised learning (i.e., clustering) techniques to build a software quality estimation system, with the help of a software engineering human expert. The system first clusters hundreds of software modules into a small number of coherent groups and presents the representative of each group to a software quality expert, who labels each cluster as either fault-prone or not fault-prone based on his domain knowledge as well as some data statistics (without any knowledge of the dependent variable, i.e., the software quality measurement). Our preliminary empirical results show promising potentials of this methodology in both predicting software quality and detecting potential noise in a software measurement and quality dataset.},
booktitle = {Proceedings of the Eighth IEEE International Conference on High Assurance Systems Engineering},
pages = {149–155},
numpages = {7},
location = {Tampa, Florida},
series = {HASE'04}
}

@article{10.3233/KES-190421,
author = {Panigrahi, Rasmita and Kuanar, Sanjay K. and Kumar, Lov and Padhy, Neelamadhab and Satapathy, Suresh Chandra},
title = {Software reusability metrics prediction and cost estimation by using machine learning algorithms},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {23},
number = {4},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-190421},
doi = {10.3233/KES-190421},
abstract = {In this research, a highly robust and efficient software design optimization model has been proposed for object-oriented programming based software solutions while considering the importance of quality and reliability. Due to a piece of information that software component reusability has allowed cost and time-efficient software design. The software reusability metrics prediction and cost estimation play a vital role in the software industry. Software quality prediction is an important feature that can be achieved a novel machine learning approach. It is a process of gathering and analyzing recurring patterns in software metrics. Machine learning techniques play a crucial role in intelligent decision making and proactive forecasting. This paper focuses on analyzing software reusability and cost estimation metrics by providing the data set. In the present world software, cost estimation and reusability prediction problem has been resolved using various newly developed methods. This paper emphasizes to solve the novel machine learning algorithms as well as improved Output layer self-connection recurrent neural networks (OLSRNN) with kernel fuzzy c-means clustering (KFCM). The investigational results confirmed the competence of the proposed method for solving software reusability and cost estimation.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {317–328},
numpages = {12},
keywords = {software cost estimation, machine learning techniques, software reusability metrics, Object-Oriented Metrics}
}

@article{10.1007/s10270-020-00856-9,
author = {Pilarski, Sebastian and Staniszewski, Martin and Bryan, Matthew and Villeneuve, Frederic and Varr\'{o}, D\'{a}niel},
title = {Predictions-on-chip: model-based training and automated deployment of machine learning models at runtime: For multi-disciplinary design and operation of gas turbines},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00856-9},
doi = {10.1007/s10270-020-00856-9},
abstract = {The design of gas turbines is a challenging area of cyber-physical systems where complex model-based simulations across multiple disciplines (e.g., performance, aerothermal) drive the design process. As a result, a continuously increasing amount of data is derived during system design. Finding new insights in such data by exploiting various machine learning (ML) techniques is a promising industrial trend since better predictions based on real data result in substantial product quality improvements and cost reduction. This paper presents a method that generates data from multi-paradigm simulation tools, develops and trains ML models for prediction, and deploys such prediction models into an active control system operating at runtime with limited computational power. We explore the replacement of existing traditional prediction modules with ML counterparts with different architectures. We validate the effectiveness of various ML models in the context of three (real) gas turbine bearings using over 150,000 data points for training, validation, and testing. We introduce code generation techniques for automated deployment of neural network models to industrial off-the-shelf programmable logic controllers.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {685–709},
numpages = {25},
keywords = {Gas turbine engines, Code generation, Automated deployment, Neural networks, Machine learning, Prediction-at-runtime}
}

@inproceedings{10.1007/978-3-030-30244-3_10,
author = {Khoza, Sibusiso C. and Grobler, Jacomine},
title = {Comparing Machine Learning and Statistical Process Control for Predicting Manufacturing Performance},
year = {2019},
isbn = {978-3-030-30243-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30244-3_10},
doi = {10.1007/978-3-030-30244-3_10},
abstract = {Quality has become one of the most important factors in the success of manufacturing companies. In this paper, the use of machine learning algorithms in quality control is compared to the use of statistical process monitoring, a classical quality management technique. The test dataset has a large number of features, which requires the use of principal component analysis and clustering to isolate the data into potential process groups. A Random Forest, Support Vector Machine and Naive Bayes algorithms were used to predict when the manufacturing process is out of control. The Random Forest algorithm performed significantly better than both the Naive Bayes and SVM algorithms in all 3 clusters of the dataset. The results were benchmarked against Hotelling’s  control charts which were trained using 80% of each cluster dataset and tested on the remaining 20%. In comparison with Hotelling’s  multivariate statistical process monitoring charts, the Random Forest algorithm still emerges as the better quality control method.},
booktitle = {Progress in Artificial Intelligence: 19th EPIA Conference on Artificial Intelligence, EPIA 2019, Vila Real, Portugal, September 3–6, 2019, Proceedings, Part II},
pages = {108–119},
numpages = {12},
location = {Vila Real, Portugal}
}

@inproceedings{10.1145/1868328.1868357,
author = {Ostrand, Thomas J. and Weyuker, Elaine J. and Bell, Robert M.},
title = {Programmer-based fault prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868357},
doi = {10.1145/1868328.1868357},
abstract = {Background: Previous research has provided evidence that a combination of static code metrics and software history metrics can be used to predict with surprising success which files in the next release of a large system will have the largest numbers of defects. In contrast, very little research exists to indicate whether information about individual developers can profitably be used to improve predictions.Aims: We investigate whether files in a large system that are modified by an individual developer consistently contain either more or fewer faults than the average of all files in the system. The goal of the investigation is to determine whether information about which particular developer modified a file is able to improve defect predictions. We also continue an earlier study to evaluate the use of counts of the number of developers who modified a file as predictors of the file's future faultiness.Method: We analyzed change reports filed by 107 programmers for 16 releases of a system with 1,400,000 LOC and 3100 files. A "bug ratio" was defined for programmers, measuring the proportion of faulty files in release R out of all files modified by the programmer in release R-1. The study compares the bug ratios of individual programmers to the average bug ratio, and also assesses the consistency of the bug ratio across releases for individual programmers.Results: Bug ratios varied widely among all the programmers, as well as for many individual programmers across all the releases that they participated in. We found a statistically significant correlation between the bug ratios for programmers for the first half of changed files versus the ratios for the second half, indicating a measurable degree of persistence in the bug ratio. However, when the computation was repeated with the bug ratio controlled not only by release, but also by file size, the correlation disappeared. In addition to the bug ratios, we confirmed that counts of the cumulative number of different developers changing a file over its lifetime can help to improve predictions, while other developer counts are not helpful.Conclusions: The results from this preliminary study indicate that adding information to a model about which particular developer modified a file is not likely to improve defect predictions. The study is limited to a single large system, and its results may not hold more widely. The bug ratio is only one way of measuring the "fault-proneness" of an individual programmer's coding, and we intend to investigate other ways of evaluating bug introduction by individuals.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {19},
numpages = {10},
keywords = {bug ratio, empirical study, fault-prone, prediction, regression model, software faults},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@article{10.1145/3092566,
author = {Ghaffarian, Seyed Mohammad and Shahriari, Hamid Reza},
title = {Software Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques: A Survey},
year = {2017},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3092566},
doi = {10.1145/3092566},
abstract = {Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and data-mining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {56},
numpages = {36},
keywords = {survey, software vulnerability discovery, software security, review, machine-learning, data-mining, Software vulnerability analysis}
}

@article{10.1016/j.ins.2019.03.045,
author = {Li, Jin and Palmieri, Francesco and Xiang, Yang},
title = {Special Issue on Security and Privacy in Machine Learning},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {487},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.03.045},
doi = {10.1016/j.ins.2019.03.045},
journal = {Inf. Sci.},
month = jun,
pages = {208–209},
numpages = {2}
}

@inproceedings{10.5555/876882.879797,
title = {Combining Software Quality Predictive Models: An Evolutionary Approach},
year = {2002},
isbn = {0769518192},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {During the past ten years, a large number of qualitymodels have been proposed in the literature. In general,the goal of these models is to predict a quality factor startingfrom a set of direct measures. The lack of data behindthese models makes it hard to generalize, to cross-validate,and to reuse existing models. As a consequence, for a company,selecting an appropriate quality model is a difficult,non-trivial decision. In this paper, we propose a general approachand a particular solution to this problem. The mainidea is to combine and adapt existing models (experts) insuch way that the combined model works well on the particularsystem or in the particular type of organization. In ourparticular solution, the experts are assumed to be decisiontree or rule-based classifiers and the combination is doneby a genetic algorithm. The result is a white-box model: foreach software component, not only the model gives the predictionof the software quality factor, but it also provides theexpert that was used to obtain the prediction. Test results indicatethat the proposed model performs significantly betterthan individual experts in the pool.},
booktitle = {Proceedings of the International Conference on Software Maintenance (ICSM'02)},
pages = {385},
series = {ICSM '02}
}

@article{10.1016/j.procs.2021.09.086,
author = {Butryn, Beata and Chomiak-Orsa, Iwona and Hauke, Krzysztof and Pondel, Maciej and Siennicka, Agnieszka},
title = {Application of Machine Learning in medical data analysis illustrated with an example of association rules},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {192},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.09.086},
doi = {10.1016/j.procs.2021.09.086},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {3134–3143},
numpages = {10},
keywords = {association rules, risk identification, tailored intervention, machine learning in medicine, mart medicine, artificial intelligence}
}

@inproceedings{10.1145/3373477.3373486,
author = {Aggarwal, Simran},
title = {Software code analysis using ensemble learning techniques},
year = {2020},
isbn = {9781450372916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373477.3373486},
doi = {10.1145/3373477.3373486},
abstract = {Ensuing the advent of advancements in software systems, the probability of them containing high severity defects is exponentially on the rise. With each technological addition, the complexity of software is increasing. Reproduction and rectification of a defect requires time and effort. Current state of the art analysis tools cater to the investigation of static aspects of a production level code. However, it is imperative to assess the dynamic development process of a system so as to be able to timely detect erroneous components early on in the development life cycle of a software. A novel automated defect prediction feature enhancement is proposed that analyses the static structure of the current code and state of the software in past releases to extract relevant static and dynamic feature sets. Data generated is modelled for defect trends in the future release of the software by four ensemble classifiers. Results demonstrate the superiority of Voting algorithm for the problem of defect prediction.},
booktitle = {Proceedings of the 1st International Conference on Advanced Information Science and System},
articleno = {9},
numpages = {7},
keywords = {software quality, object-oriented metrics, machine learning, ensemble learning, empirical validation, defect prediction},
location = {Singapore, Singapore},
series = {AISS '19}
}

@article{10.1145/3212695,
author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
title = {A Survey of Machine Learning for Big Code and Naturalness},
year = {2018},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3212695},
doi = {10.1145/3212695},
abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit the abundance of patterns of code. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {81},
numpages = {37},
keywords = {software engineering tools, machine learning, code naturalness, Big code}
}

@article{10.1016/j.knosys.2018.04.006,
author = {Lee, Gichang and Jeong, Jaeyun and Seo, Seungwan and Kim, CzangYeob and Kang, Pilsung},
title = {Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network},
year = {2018},
issue_date = {July 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {152},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.04.006},
doi = {10.1016/j.knosys.2018.04.006},
abstract = {In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment (positive/negative) of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is a lack of information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network (CNN). In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. To verify the proposed methodology, we evaluated the classification accuracy and the rate of polarity words among high scoring words using two movie review datasets. Experimental results show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores.},
journal = {Know.-Based Syst.},
month = jul,
pages = {70–82},
numpages = {13},
keywords = {Word localization, Weakly supervised learning, Sentiment analysis, Convolutional neural network, Class activation mapping}
}

@article{10.1007/s00521-016-2437-y,
author = {Chatterjee, S. and Nigam, S. and Roy, A.},
title = {Software fault prediction using neuro-fuzzy network and evolutionary learning approach},
year = {2017},
issue_date = {January   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {1},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-016-2437-y},
doi = {10.1007/s00521-016-2437-y},
abstract = {In the real world, a great deal of information is provided by human experts that normally do not conform to the rules of physics, but describe the complicated systems by a set of incomplete or vague statements. The need of conducting uncertainty analysis in software reliability for the large and complex system is demanding. For large complex systems made up of many components, the uncertainty of each individual parameter amplifies the uncertainty of the total system reliability. In this paper, to overcome with the problem of uncertainty in software development process and environment, a neuro-fuzzy modeling has been proposed for software fault prediction. The training of the proposed neuro-fuzzy model has been done with genetic algorithm and back-propagation learning algorithm. The proposed model has been validated using some real software failure data. The efficiency of the two learning algorithms has been compared with various fuzzy and statistical time series-based forecasting algorithms on the basis of their prediction ability.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {1221–1231},
numpages = {11},
keywords = {Software reliability, Genetic algorithm, Fuzzy neural network, Faults}
}

@article{10.1007/s10664-019-09769-8,
author = {Ochodek, Miroslaw and Hebig, Regina and Meding, Wilhelm and Frost, Gert and Staron, Miroslaw},
title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09769-8},
doi = {10.1007/s10664-019-09769-8},
abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {220–265},
numpages = {46},
keywords = {Code reviews, Action research, Machine learning, Measurement}
}

@inproceedings{10.1145/1868328.1868356,
author = {Matsumoto, Shinsuke and Kamei, Yasutaka and Monden, Akito and Matsumoto, Ken-ichi and Nakamura, Masahide},
title = {An analysis of developer metrics for fault prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868356},
doi = {10.1145/1868328.1868356},
abstract = {Background: Software product metrics have been widely used as independent variables for constructing a fault prediction model. However, fault injection depends not only on characteristics of the products themselves, but also on characteristics of developers involved in the project. Aims: The goal of this paper is to study the effects of developer features on software reliability. Method: This paper proposes developer metrics such as the number of code churns made by each developer, the number of commitments made by each developer and the number of developers for each module. By using the eclipse project dataset, we experimentally analyzed the relationship between the number of faults and developer metrics. Second, the effective of developer metrics for performance improvements of fault prediction models were evaluated. Results: The result revealed that the modules touched by more developer contained more faults. Compared with conventional fault prediction models, developer metrics improved the prediction performance. Conclusions: We conclude that developer metrics are good predictor of faults and we must consider the human factors for improving the software reliability.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {18},
numpages = {9},
keywords = {developer metrics, fault prediction, human factor},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1145/3180155.3180197,
author = {Agrawal, Amritanshu and Menzies, Tim},
title = {Is "better data" better than "better data miners"? on the benefits of tuning SMOTE for defect prediction},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180197},
doi = {10.1145/3180155.3180197},
abstract = {We report and fix an important systematic error in prior studies that ranked classifiers for software analytics. Those studies did not (a) assess classifiers on multiple criteria and they did not (b) study how variations in the data affect the results. Hence, this paper applies (a) multi-performance criteria while (b) fixing the weaker regions of the training data (using SMOTUNED, which is an auto-tuning version of SMOTE). This approach leads to dramatically large increases in software defect predictions when applied in a 5*5 cross-validation study for 3,681 JAVA classes (containing over a million lines of code) from open source systems, SMOTUNED increased AUC and recall by 60% and 20% respectively. These improvements are independent of the classifier used to predict for defects. Same kind of pattern (improvement) was observed when a comparative analysis of SMOTE and SMOTUNED was done against the most recent class imbalance technique.In conclusion, for software analytic tasks like defect prediction, (1) data pre-processing can be more important than classifier choice, (2) ranking studies are incomplete without such pre-processing, and (3) SMOTUNED is a promising candidate for pre-processing.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1050–1061},
numpages = {12},
keywords = {SMOTE, classification, data analytics for software engineering, defect prediction, preprocessing, search based SE, unbalanced data},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2365324.2365338,
author = {Bowes, David and Hall, Tracy and Gray, David},
title = {Comparing the performance of fault prediction models which report multiple performance measures: recomputing the confusion matrix},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365338},
doi = {10.1145/2365324.2365338},
abstract = {There are many hundreds of fault prediction models published in the literature. The predictive performance of these models is often reported using a variety of different measures. Most performance measures are not directly comparable. This lack of comparability means that it is often difficult to evaluate the performance of one model against another. Our aim is to present an approach that allows other researchers and practitioners to transform many performance measures of categorical studies back into a confusion matrix. Once performance is expressed in a confusion matrix alternative preferred performance measures can then be derived. Our approach has enabled us to compare the performance of 600 models published in 42 studies. We demonstrate the application of our approach on several case studies, and discuss the advantages and implications of doing this.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {109–118},
numpages = {10},
keywords = {confusion matrix, fault, machine learning},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@article{10.1007/s00500-016-2316-6,
author = {Chinna Gounder Dhanajayan, Rajaganapathy and Appavu Pillai, Subramani},
title = {SLMBC: spiral life cycle model-based Bayesian classification technique for efficient software fault prediction and classification},
year = {2017},
issue_date = {January   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-016-2316-6},
doi = {10.1007/s00500-016-2316-6},
abstract = {Software fault prediction and classification plays a vital role in the software development process for assuring high quality and reliability of the software product. Earlier prediction of the fault-prone software modules enables timely correction of the faults and delivery of reliable product. Generally, the fuzzy logic, decision tree and neural networks are deployed for fault prediction. But these techniques suffer due to low accuracy and inconsistency. To overcome these issues, this paper proposes a spiral life cycle model-based Bayesian classification technique for efficient software fault prediction and classification. In this process, initially the dependent and independent software modules are identified. The spiral life cycle model is used for testing the software modules in each life cycle of the software development process. Bayesian classification is applied to classify the software modules as faulty module and non-faulty module, by using the probability distribution models. Robust similarity-aware clustering algorithm performs clustering of the faulty and non-faulty software modules based on the similarity measure of the features in the dataset. From the experimental results, it is observed that the proposed method enables accurate prediction and classification of the faulty modules. The proposed technique achieves higher accuracy, precision, recall, probability of detection, F-measure and lower error rate than the existing techniques. The misclassification rate of the proposed technique is found to be lower than the existing techniques. Hence, the reliability of the software development process can be improved.},
journal = {Soft Comput.},
month = jan,
pages = {403–415},
numpages = {13},
keywords = {Spiral life cycle model-based Bayesian classification technique (SLMBC), Spiral life cycle model, Software development, Segregate fault prediction algorithm, Robust similarity-aware clustering (RSC) algorithm, Bayesian classification}
}

@inproceedings{10.5555/2042243.2042277,
author = {Rodr\'{\i}guez, Daniel and Ruiz, R. and Riquelme, J. C. and Harrison, Rachel},
title = {Subgroup discovery for defect prediction},
year = {2011},
isbn = {9783642237157},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the Third International Conference on Search Based Software Engineering},
pages = {269–270},
numpages = {2},
location = {Szeged, Hungary},
series = {SSBSE'11}
}

@inproceedings{10.1145/3395363.3404540,
author = {Tizpaz-Niari, Saeid and \v{C}ern\'{y}, Pavol and Trivedi, Ashutosh},
title = {Detecting and understanding real-world differential performance bugs in machine learning libraries},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3404540},
doi = {10.1145/3395363.3404540},
abstract = {Programming errors that degrade the performance of systems are widespread, yet there is very little tool support for finding and diagnosing these bugs. We present a method and a tool based on differential performance analysis---we find inputs for which the performance varies widely, despite having the same size. To ensure that the differences in the performance are robust (i.e. hold also for large inputs), we compare the performance of not only single inputs, but of classes of inputs, where each class has similar inputs parameterized by their size. Thus, each class is represented by a performance function from the input size to performance. Importantly, we also provide an explanation for why the performance differs in a form that can be readily used to fix a performance bug. The two main phases in our method are discovery with fuzzing and explanation with decision tree classifiers, each of which is supported by clustering. First, we propose an evolutionary fuzzing algorithm to generate inputs that characterize different performance functions. For this fuzzing task, the unique challenge is that we not only need the input class with the worst performance, but rather a set of classes exhibiting differential performance. We use clustering to merge similar input classes which significantly improves the efficiency of our fuzzer. Second, we explain the differential performance in terms of program inputs and internals (e.g., methods and conditions). We adapt discriminant learning approaches with clustering and decision trees to localize suspicious code regions. We applied our techniques on a set of micro-benchmarks and real-world machine learning libraries. On a set of micro-benchmarks, we show that our approach outperforms state-of-the-art fuzzers in finding inputs to characterize differential performance. On a set of case-studies, we discover and explain multiple performance bugs in popular machine learning frameworks, for instance in implementations of logistic regression in scikit-learn. Four of these bugs, reported first in this paper, have since been fixed by the developers.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {189–199},
numpages = {11},
keywords = {Testing, ML Libraries, Differential Performance Bugs, Debugging},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@article{10.1016/j.jksuci.2018.04.012,
author = {Atoum, Issa},
title = {A novel framework for measuring software quality-in-use based on semantic similarity and sentiment analysis of software reviews},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {32},
number = {1},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2018.04.012},
doi = {10.1016/j.jksuci.2018.04.012},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = jan,
pages = {113–125},
numpages = {13},
keywords = {Text similarity, Software quality, Sentiment analysis, Quality in use, ISO25010}
}

@article{10.1016/j.infsof.2010.06.006,
author = {Tosun, Ay\c{s}e and Bener, Ay\c{s}e and Turhan, Burak and Menzies, Tim},
title = {Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry},
year = {2010},
issue_date = {November, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.06.006},
doi = {10.1016/j.infsof.2010.06.006},
abstract = {Context: Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results. Objective: In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software. Method: We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naive Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naive Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost-benefit analysis to show that our approach can be efficiently put into practice. Results: Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naive Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold. Conclusion: Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naive Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1242–1257},
numpages = {16},
keywords = {Static code attributes, Software defect prediction, Na\"{\i}ve Bayes, Experience report}
}

@inproceedings{10.1109/ICMLA.2013.13,
author = {Gao, Kehan and Khoshgoftaar, Taghi and Napolitano, Amri},
title = {Improving Software Quality Estimation by Combining Boosting and Feature Selection},
year = {2013},
isbn = {9780769551449},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2013.13},
doi = {10.1109/ICMLA.2013.13},
abstract = {The predictive accuracy of a classification modelis often affected by the quality of training data. However, there are two problems which may affect the quality of the training data: high dimensionality (too many independent attributes in a dataset) and class imbalance (many more instances of one class than the other class in a binary-classification problem). In this study, we present an iterative feature selection approach working with an ensemble learning method to solve both of these problems. The iterative feature selection approach samples the dataset k times and applies feature ranking to each sampled dataset, the k different rankings are then aggregated to create a single feature ranking. The ensemble learning method used is RUSBoost, in which random under sampling(RUS) is integrated into a boosting algorithm. The main purpose of this paper is to investigate the impact of feature selection as well as the RUSBoost approach on the classification performance in the context of software quality prediction. In the experiment, we explore six rankers, each used along with RUS in the iterative feature selection process. Following feature selection, models are built either using a plain learner or byusing the RUSBoost algorithm. We also examine the case of no feature selection and use this as the baseline for comparisons. The experimental results demonstrate that with the exception of one learner, feature selection combined with boosting provides better classification performance than when either is applied alone or when neither are applied.},
booktitle = {Proceedings of the 2013 12th International Conference on Machine Learning and Applications - Volume 01},
pages = {27–33},
numpages = {7},
keywords = {Software Quality Classification, RUSBoost, Performance Metric, Feature Selection, Data Sampling},
series = {ICMLA '13}
}

@inproceedings{10.1145/3411564.3411610,
author = {Prado, Fernando F. and Digiampietri, Luciano A.},
title = {A systematic review of automated feature engineering solutions in machine learning problems},
year = {2020},
isbn = {9781450388733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411564.3411610},
doi = {10.1145/3411564.3411610},
abstract = {During the last decades, machine learning has played an important role in building data-driven experiments. Based on information extracted from a variety of sources, new patterns can be identified, predictions can be made more easily and decisions made faster and more effective. The specialized application of machine learning solutions requires specific knowledge in areas such as math, computation, and statistics, as well as being extremely costly in time and having a high chance of incurring any kind of human error during the process. Automated Machine Learning Techniques (AutoML) seek to automate parts of the process of building machine learning applications, allowing non-experts to perform this process. An important part of this kind of problem is the feature engineering part which creates a transformation in the data, making it more representative for the final model. This paper presents a systematic review of automated feature engineering solutions in machine learning problems. With the main objective of identifying and analyzing the existing methods and techniques for performing the automated feature engineering step within the framework of machine learning problems.},
booktitle = {Proceedings of the XVI Brazilian Symposium on Information Systems},
articleno = {12},
numpages = {7},
keywords = {Feature Selection, Feature Generation, Feature Engineering, Auto Learning},
location = {S\~{a}o Bernardo do Campo, Brazil},
series = {SBSI '20}
}

@inproceedings{10.1145/3385412.3386015,
author = {Usman, Muhammad and Wang, Wenxi and Vasic, Marko and Wang, Kaiyuan and Vikalo, Haris and Khurshid, Sarfraz},
title = {A study of the learnability of relational properties: model counting meets machine learning (MCML)},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386015},
doi = {10.1145/3385412.3386015},
abstract = {This paper introduces the MCML approach for empirically studying the learnability of relational properties that can be expressed in the well-known software design language Alloy. A key novelty of MCML is quantification of the performance of and semantic differences among trained machine learning (ML) models, specifically decision trees, with respect to entire (bounded) input spaces, and not just for given training and test datasets (as is the common practice). MCML reduces the quantification problems to the classic complexity theory problem of model counting, and employs state-of-the-art model counters. The results show that relatively simple ML models can achieve surprisingly high performance (accuracy and F1-score) when evaluated in the common setting of using training and test datasets -- even when the training dataset is much smaller than the test dataset -- indicating the seeming simplicity of learning relational properties. However, MCML metrics based on model counting show that the performance can degrade substantially when tested against the entire (bounded) input space, indicating the high complexity of precisely learning these properties, and the usefulness of model counting in quantifying the true performance.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1098–1111},
numpages = {14},
keywords = {Alloy, ApproxMC, ProjMC, Relational properties, SAT solving, machine learning, model counting},
location = {London, UK},
series = {PLDI 2020}
}

@article{10.4018/ijsi.2014100105,
author = {Abaei, Golnoush and Selamat, Ali},
title = {Increasing the Accuracy of Software Fault Prediction using Majority Ranking Fuzzy Clustering},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {2},
number = {4},
issn = {2166-7160},
url = {https://doi.org/10.4018/ijsi.2014100105},
doi = {10.4018/ijsi.2014100105},
abstract = {Despite proposing many software fault prediction models, this area has yet to be explored as still there is a room for stable and consistent model with better performance. In this paper, a new method is proposed to increase the accuracy of fault prediction based on the notion of fuzzy clustering and majority ranking. The authors investigated the effect of irrelevant and inconsistent modules on software fault prediction and tried to decrease it by designing a new framework, in which the entire project modules are clustered. The obtained results showed that fuzzy clustering could decrease the negative effect of irrelevant modules on prediction performance. Eight data sets from NASA and Turkish white-goods software is employed to evaluate our model. Performance evaluation in terms of false positive rate, false negative rate, and overall error showed the superiority of our model compared to other predicting models. The authors proposed majority ranking fuzzy clustering approach showed between 3% to 18% and 1% to 4% improvement in false negative rate and overall error, respectively, compared with other available proposed models (ACF and ACN) in more than half of the testing cases. According to the results, our systems can be used to guide testing effort by identifying fault prone modules to improve the quality of software development and software testing in a limited time and budget.},
journal = {Int. J. Softw. Innov.},
month = oct,
pages = {60–71},
numpages = {12},
keywords = {Software Fault Prediction, NASA, Majority Ranking, Fuzzy Clustering, Available Proposed Models}
}

@inproceedings{10.1145/3371158.3371233,
author = {Mannarswamy, Sandya and Roy, Shourya and Chidambaram, Saravanan},
title = {Tutorial on Software Testing &amp; Quality Assurance for Machine Learning Applications from research bench to real world},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371233},
doi = {10.1145/3371158.3371233},
abstract = {Rapid progress in Machine Learning (ML) has seen a swift translation to real world commercial deployment. While research and development of ML applications have progressed at an exponential pace, the required software engineering process for ML applications and the corresponding eco-system of testing and quality assurance tools which enable software reliable, trustworthy and safe and easy to deploy, have sadly lagged behind. Specifically, the challenges and gaps in quality assurance (QA) and testing of AI applications have largely remained unaddressed contributing to a poor translation rate of ML applications from research to real world. Unlike traditional software, which has a well-defined software testing methodology, ML applications have largely taken an ad-hoc approach to testing. ML researchers and practitioners either fall back to traditional software testing approaches, which are inadequate for this domain, due to its inherent probabilistic and data dependent nature, or rely largely on non-rigorous self-defined QA methodologies. These issues have driven the ML and Software Engineering research communities to develop of newer tools and techniques designed specifically for ML. These research advances need to be publicized and practiced in real world in ML development and deployment for enabling successful translation of ML from research prototypes to real world. This tutorial intends to address this need.This tutorial aims to:[1] Provide a comprehensive overview of testing of ML applications[2] Provide practical insights and share community best practices for testing ML softwareBesides scientific literature, we derive our insights from our conversations with industry experts in ML.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {373–374},
numpages = {2},
keywords = {Software Testing, Quality Assurance, Machine Learning},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@inproceedings{10.1007/978-3-030-58817-5_66,
author = {Kumari, Madhu and Singh, Ujjawal Kumar and Sharma, Meera},
title = {Entropy Based Machine Learning Models for Software Bug Severity Assessment in Cross Project Context},
year = {2020},
isbn = {978-3-030-58816-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58817-5_66},
doi = {10.1007/978-3-030-58817-5_66},
abstract = {There can be noise and uncertainty in the bug reports data as the bugs are reported by a heterogeneous group of users working across different countries. Bug description is an essential attribute that helps to predict other bug attributes, such as severity, priority, and time fixes. We need to consider the noise and confusion present in the text of the bug report, as it can impact the output of different machine learning techniques. Shannon entropy has been used in this paper to calculate summary uncertainty about the bug. Bug severity attribute tells about the type of impact the bug has on the functionality of the software. Correct bug severity estimation allows scheduling and repair bugs and hence help in resource and effort utilization. To predict the severity of the bug we need software project historical data to train the classifier. These training data are not always available in particular for new software projects. The solution which is called cross project prediction is to use the training data from other projects. Using bug priority, summary weight and summary entropy, we have proposed cross project bug severity assessment models. Results for proposed summary entropy based approach for bug severity prediction in cross project context show improved performance of the Accuracy and F-measure up to 70.23% and 93.72% respectively across all the machine learning techniques over existing work.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part VI},
pages = {939–953},
numpages = {15},
location = {Cagliari, Italy}
}

@inproceedings{10.1109/QSIC.2010.40,
author = {He, Zhimin and Shu, Fengdi and Yang, Ye and Zhang, Wen and Wang, Qing},
title = {Data Unpredictability in Software Defect-Fixing Effort Prediction},
year = {2010},
isbn = {9780769541310},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/QSIC.2010.40},
doi = {10.1109/QSIC.2010.40},
abstract = {The prediction of software defect-fixing effort is important for strategic resource allocation and software quality management. Machine learning techniques have become very popular in addressing this problem and many related prediction models have been proposed. However, almost every model today faces a challenging issue of demonstrating satisfactory prediction accuracy and meaningful prediction results. In this paper, we investigate what makes high-precision prediction of defect-fixing effort so hard from the perspective of the characteristics of defect dataset. We develop a method using a metric to quantitatively analyze the unpredictability of a defect dataset and carry out case studies on two defect datasets. The results show that data unpredictability is a key factor for unsatisfactory prediction accuracy and our approach can explain why high-precision prediction for some defect datasets is hard to achieve inherently. We also provide some suggestions on how to collect highly predictable defect data.},
booktitle = {Proceedings of the 2010 10th International Conference on Quality Software},
pages = {220–226},
numpages = {7},
keywords = {software defect-fixing effort prediction, machine learning, data unpredictability, MAE},
series = {QSIC '10}
}

@inproceedings{10.1145/2811681.2811699,
author = {Hussain, Shahid and Keung, Jacky and Khan, Arif Ali and Bennin, Kwabena Ebo},
title = {Performance Evaluation of Ensemble Methods For Software Fault Prediction: An Experiment},
year = {2015},
isbn = {9781450337960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811681.2811699},
doi = {10.1145/2811681.2811699},
abstract = {In object-oriented software development, a plethora of studies have been carried out to present the application of machine learning algorithms for fault prediction. Furthermore, it has been empirically validated that an ensemble method can improve classification performance as compared to a single classifier. But, due to the inherent differences among machine learning and data mining approaches, the classification performance of ensemble methods will be varied. In this study, we investigated and evaluated the performance of different ensemble methods with itself and base-level classifiers, in predicting the faults proneness classes. Subsequently, we used three ensemble methods AdaboostM1, Vote and StackingC with five base-level classifiers namely Naivebayes, Logistic, J48, VotedPerceptron and SMO in Weka tool. In order to evaluate the performance of ensemble methods, we retrieved twelve datasets of open source projects from PROMISE repository. In this experiment, we used k-fold (k=10) cross-validation and ROC analysis for validation. Besides, we used recall, precision, accuracy, F-value measures to evaluate the performance of ensemble methods and base-level Classifiers. Finally, we observed significant performance improvement of applying ensemble methods as compared to its base-level classifier, and among ensemble methods we observed StackingC outperformed other selected ensemble methods for software fault prediction.},
booktitle = {Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference},
pages = {91–95},
numpages = {5},
keywords = {Weka, Performance, Measures, Ensemble Methods, Classifiers, Chidamber and Kemerer (CK) Metrics},
location = {Adelaide, SA, Australia},
series = {ASWEC ' 15 Vol. II}
}

@inproceedings{10.1109/ICPC.2019.00023,
author = {Pecorelli, Fabiano and Palomba, Fabio and Di Nucci, Dario and De Lucia, Andrea},
title = {Comparing heuristic and machine learning approaches for metric-based code smell detection},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00023},
doi = {10.1109/ICPC.2019.00023},
abstract = {Code smells represent poor implementation choices performed by developers when enhancing source code. Their negative impact on source code maintainability and comprehensibility has been widely shown in the past and several techniques to automatically detect them have been devised. Most of these techniques are based on heuristics, namely they compute a set of code metrics and combine them by creating detection rules; while they have a reasonable accuracy, a recent trend is represented by the use of machine learning where code metrics are used as predictors of the smelliness of code artefacts. Despite the recent advances in the field, there is still a noticeable lack of knowledge of whether machine learning can actually be more accurate than traditional heuristic-based approaches. To fill this gap, in this paper we propose a large-scale study to empirically compare the performance of heuristic-based and machine-learning-based techniques for metric-based code smell detection. We consider five code smell types and compare machine learning models with Decor, a state-of-the-art heuristic-based approach. Key findings emphasize the need of further research aimed at improving the effectiveness of both machine learning and heuristic approaches for code smell detection: while Decor generally achieves better performance than a machine learning baseline, its precision is still too low to make it usable in practice.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {93–104},
numpages = {12},
keywords = {machine learning, heuristics, empirical study, code smells detection},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@article{10.1007/s11334-017-0308-z,
author = {Gosain, Anjana and Singh, Jaspreeti},
title = {Investigating structural metrics for understandability prediction of data warehouse multidimensional schemas using machine learning techniques},
year = {2018},
issue_date = {March     2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {1},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-017-0308-z},
doi = {10.1007/s11334-017-0308-z},
abstract = {Data warehouse (DW) quality metrics help in evaluating quality attributes and building classification models for predicting multidimensional (MD) schemas as understandable/non-understandable, thereby assisting in DW maintenance. To evaluate DW MD schema quality, we have earlier proposed a set of metrics based on some important aspects of dimension hierarchies and its sharing (like sharing of few hierarchy levels within a dimension; sharing of few hierarchy levels between dimensions, within and across facts) which may lead to structural complexity of MD schemas, thereby affecting its quality. The preliminary empirical validation of these metrics using classical statistical techniques (correlation and linear regression) indicated some of them as possible understandability indicators. However, machine learning (ML) techniques can model the complex associations between DW structural metrics and their quality attributes in a better way. Therefore, this work employs five ML classifiers [J48, partial decision trees (PART), Na ve Bayes, support vector machines (SVM) and logistic regression] to empirically investigate whether accurate prediction models can be built, based on our structural metrics, to be used as understandability predictors. The obtained results reveal that four of our metrics are good predictors of understandability of DW MD schemas. The experimentation further involved comparing the classifiers using mainly five performance measures: accuracy, precision, sensitivity, specificity and area under the receiver operating characteristic curve. The study confirmed the predictive capability of ML techniques for understandability prediction of DW MD schemas. The results also suggest that the SVM and Na ve Bayes classifiers perform better than other classifiers included in the study. Further, the typically used logistic regression technique gave results that were reasonably competitive with the more sophisticated techniques. However, the tree-based (J48) and rule-based (PART) techniques performed significantly worse than the best performing techniques.},
journal = {Innov. Syst. Softw. Eng.},
month = mar,
pages = {59–80},
numpages = {22},
keywords = {Understandability, Structural metrics, Multidimensional schemas, Machine learning, Empirical validation, Data warehouse quality}
}

@inproceedings{10.1145/1595696.1595713,
author = {Zimmermann, Thomas and Nagappan, Nachiappan and Gall, Harald and Giger, Emanuel and Murphy, Brendan},
title = {Cross-project defect prediction: a large scale experiment on data vs. domain vs. process},
year = {2009},
isbn = {9781605580012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595696.1595713},
doi = {10.1145/1595696.1595713},
abstract = {Prediction of software defects works well within projects as long as there is a sufficient amount of data available to train any models. However, this is rarely the case for new software projects and for many companies. So far, only a few have studies focused on transferring prediction models from one project to another. In this paper, we study cross-project defect prediction models on a large scale. For 12 real-world applications, we ran 622 cross-project predictions. Our results indicate that cross-project prediction is a serious challenge, i.e., simply using models from projects in the same domain or with the same process does not lead to accurate predictions. To help software engineers choose models wisely, we identified factors that do influence the success of cross-project predictions. We also derived decision trees that can provide early estimates for precision, recall, and accuracy before a prediction is attempted.},
booktitle = {Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {91–100},
numpages = {10},
keywords = {prediction quality, logistic regression, defect prediction, decision trees, cross-project, churn},
location = {Amsterdam, The Netherlands},
series = {ESEC/FSE '09}
}

@inproceedings{10.1145/3368926.3369679,
author = {Kikuma, Kazuhiro and Yamada, Takeshi and Sato, Koki and Ueda, Kiyoshi},
title = {Preparation Method in Automated Test Case Generation using Machine Learning},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369679},
doi = {10.1145/3368926.3369679},
abstract = {The role of telecom carrier networks has shifted from the provision of telephone services to that of Internet infrastructures, making it inevitable to invest in equipment capable of handling the rapidly increasing traffic due to the distribution of video and other bandwidth-heavy contents. This has given rise to management needs for reducing the cost of communication systems as a whole. Likewise, software used in communication systems is increasingly becoming more complex and larger in scale to accommodate various service requirements, thereby making it more difficult to reduce the cost and period for software development. Further, since telecom carrier networks serve as basic social infrastructures, it is important to maintain their reliability and safety as a critical lifeline. Hence, ill-considered reduction of development costs and shortening of development period that could lead to decline in service quality must be avoided. In telecom carrier networks, communications softwares incorporate numerous quality improvement measures to prevent service interruptions during operation of public networks. The implementation of numerous quality improvement measures, however, has resulted in prolonged development periods and higher costs. To address these issues, we have been working on the automation of software development processes. Among the different processes in software development, testing yields the most influence in software quality. Test cases are therefore written by skilled engineers and are decided after multiple reviews, requiring a large amount of manpower in preparing them. Thoughtless reduction of these steps, however, could diminish software quality. We therefore used the knowhow of skilled engineers in writing test cases as training data to automate the generation of homogeneous test cases through machine learning. Our method automatically extracts homogeneous test cases that are not dependent on skills and knowhow of the engineer writing the test cases from requirements specification documents, which are the products of the basic design process in the past development. However, the required accuracy cannot be obtained by applying simple machine learning. The expansion of training data increases the cost and the quantity of requirement specification documents is limited, so it is necessary to improve the learning efficiency per unit training data. In this paper, we propose a method to increase accuracy by preparation of training data inputted into the machine learning process, and report the results of evaluation of effectiveness of the method.},
booktitle = {Proceedings of the 10th International Symposium on Information and Communication Technology},
pages = {393–398},
numpages = {6},
keywords = {Training data, Test case automatic generation, CRF},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT '19}
}

@inproceedings{10.1145/3385032.3385042,
author = {Tummalapalli, Sahithi and Kumar, Lov and Murthy, N. L. Bhanu},
title = {Prediction of Web Service Anti-patterns Using Aggregate Software Metrics and Machine Learning Techniques},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385042},
doi = {10.1145/3385032.3385042},
abstract = {Service-Oriented Architecture(SOA) can be characterized as an approximately coupled engineering intended to meet the business needs of an association/organization. Service-Based Systems (SBSs) are inclined to continually change to enjoy new client necessities and adjust the execution settings, similar to some other huge and complex frameworks. These changes may lead to the evolution of designs/products with poor Quality of Service (QoS), resulting in the bad practiced solutions, commonly known as Anti-patterns. Anti-patterns makes the evolution and maintenance of the software systems hard and complex. Early identification of modules, classes, or source code regions where anti-patterns are more likely to occur can help in amending and maneuvering testing efforts leading to the improvement of software quality. In this work, we investigate the application of three sampling techniques, three feature selection techniques, and sixteen different classification techniques to develop the models for web service anti-pattern detection. We report the results of an empirical study by evaluating the approach proposed, on a data set of 226 Web Service Description Language(i.e., WSDL)files, a variety of five types of web-service anti-patterns. Experimental results demonstrated that SMOTE is the best performing data sampling techniques. The experimental results also reveal that the model developed by considering Uncorrelated Significant Predictors(SUCP) as the input obtained better performance compared to the model developed by other metrics. Experimental results also show that the Least Square Support Vector Machine with Linear(LSLIN) function has outperformed all other classifier techniques.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {8},
numpages = {11},
keywords = {Aggregation measures, Anti-pattern, Class imbalance distribution, Classifiers, Feature Selection, Machine Learning, Service-Based Systems(SBS), Source Code Metrics, WSDL, Web-Services},
location = {Jabalpur, India},
series = {ISEC '20}
}

@article{10.1016/j.eswa.2008.10.027,
author = {Catal, Cagatay and Diri, Banu},
title = {A systematic review of software fault prediction studies},
year = {2009},
issue_date = {May, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {36},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2008.10.027},
doi = {10.1016/j.eswa.2008.10.027},
abstract = {This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle.},
journal = {Expert Syst. Appl.},
month = may,
pages = {7346–7354},
numpages = {9},
keywords = {Public datasets, Method-level metrics, Machine learning, Expert systems, Automated fault prediction models}
}

@article{10.5555/2938006.2938019,
author = {Arcelli Fontana, Francesca and M\"{a}ntyl\"{a}, Mika V. and Zanoni, Marco and Marino, Alessandro},
title = {Comparing and experimenting machine learning techniques for code smell detection},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {3},
issn = {1382-3256},
abstract = {Several code smell detection tools have been developed providing different results, because smells can be subjectively interpreted, and hence detected, in different ways. In this paper, we perform the largest experiment of applying machine learning algorithms to code smells to the best of our knowledge. We experiment 16 different machine-learning algorithms on four code smells (Data Class, Large Class, Feature Envy, Long Method) and 74 software systems, with 1986 manually validated code smell samples. We found that all algorithms achieved high performances in the cross-validation data set, yet the highest performances were obtained by J48 and Random Forest, while the worst performance were achieved by support vector machines. However, the lower prevalence of code smells, i.e., imbalanced data, in the entire data set caused varying performances that need to be addressed in the future studies. We conclude that the application of machine learning to the detection of these code smells can provide high accuracy (&gt;96 %), and only a hundred training examples are needed to reach at least 95 % accuracy.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1143–1191},
numpages = {49},
keywords = {Machine learning techniques, Code smells detection, Benchmark for code smell detection}
}

@inproceedings{10.1145/1540438.1540448,
author = {Mende, Thilo and Koschke, Rainer},
title = {Revisiting the evaluation of defect prediction models},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540448},
doi = {10.1145/1540438.1540448},
abstract = {Defect Prediction Models aim at identifying error-prone parts of a software system as early as possible. Many such models have been proposed, their evaluation, however, is still an open question, as recent publications show.An important aspect often ignored during evaluation is the effort reduction gained by using such models. Models are usually evaluated per module by performance measures used in information retrieval, such as recall, precision, or the area under the ROC curve (AUC). These measures assume that the costs associated with additional quality assurance activities are the same for each module, which is not reasonable in practice. For example, costs for unit testing and code reviews are roughly proportional to the size of a module.In this paper, we investigate this discrepancy using optimal and trivial models. We describe a trivial model that takes only the module size measured in lines of code into account, and compare it to five classification methods. The trivial model performs surprisingly well when evaluated using AUC. However, when an effort-sensitive performance measure is used, it becomes apparent that the trivial model is in fact the worst.},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {7},
numpages = {10},
keywords = {cost-sensitive performance measures, defect prediction},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@article{10.1016/j.future.2019.06.022,
author = {Raza, Muhammad and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Zhao, Ming and Rehman, Zia ur},
title = {A comparative analysis of machine learning models for quality pillar assessment of SaaS services by multi-class text classification of users’ reviews},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.06.022},
doi = {10.1016/j.future.2019.06.022},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {341–371},
numpages = {31},
keywords = {Machine learning approaches, Text classification, User reviews, Quality pillars, SaaS}
}

@inproceedings{10.1145/2393216.2393332,
author = {Kaur, Inderpreet and Kaur, Arvinder},
title = {Empirical study of Software Quality estimation},
year = {2012},
isbn = {9781450313100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393216.2393332},
doi = {10.1145/2393216.2393332},
abstract = {Software Quality is an important nonfunctional requirement which is not satisfied by many software products. Prediction models using object oriented metrics can be used to identify the faulty classes. In this paper, we will empirically study the relationship between object oriented metrics and fault proneness of an open source project Emma. Twelve machine Learning classifiers have been used. Univariate and Multivariate analysis of Emma shows that Random Forest provides optimum values for accuracy, precision, sensitivity and specificity.},
booktitle = {Proceedings of the Second International Conference on Computational Science, Engineering and Information Technology},
pages = {694–700},
numpages = {7},
keywords = {specificity, software quality, software maintenance, sensitivity, recall, precision, object-orientedsoftware metrics, fault proneness, f-measure, confusion matrix, classifiers, bug, accuracy, ROC, Quality metrics},
location = {Coimbatore UNK, India},
series = {CCSEIT '12}
}

@article{10.1504/IJIIDS.2015.070825,
author = {Abaei, Golnoush and Mashinchi, M. Reza and Selamat, Ali},
title = {Software fault prediction using BP-based crisp artificial neural networks},
year = {2015},
issue_date = {July 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {1},
issn = {1751-5858},
url = {https://doi.org/10.1504/IJIIDS.2015.070825},
doi = {10.1504/IJIIDS.2015.070825},
abstract = {Early fault detection for software reduces the cost of developments. Fault level can be predicted through learning mechanisms. Conventionally, precise metrics measure the fault level and crisp artificial neural networks CANNs perform the learning. However, the performance of CANNs depends on complexities of data and learning algorithm. This paper considers these two complexities to predict the fault level of software. We apply the principle component analysis PCA to reduce the dimensionality of data, and employ the correlation-based feature selection CFS to select the best features. CANNs, then, predict the fault level of software using back propagation BP algorithm as a learning mechanism. To investigate the performance of BP-based CANNs, we analyse varieties of dimensionality reduction. The results reveal the superiority of PCA to CFS in terms of accuracy.},
journal = {Int. J. Intell. Inf. Database Syst.},
month = jul,
pages = {15–31},
numpages = {17}
}

@inproceedings{10.1145/3106237.3106291,
author = {Ma, Shiqing and Aafer, Yousra and Xu, Zhaogui and Lee, Wen-Chuan and Zhai, Juan and Liu, Yingqi and Zhang, Xiangyu},
title = {LAMP: data provenance for graph based machine learning algorithms through derivative computation},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106291},
doi = {10.1145/3106237.3106291},
abstract = {Data provenance tracking determines the set of inputs related to a given output. It enables quality control and problem diagnosis in data engineering. Most existing techniques work by tracking program dependencies. They cannot quantitatively assess the importance of related inputs, which is critical to machine learning algorithms, in which an output tends to depend on a huge set of inputs while only some of them are of importance. In this paper, we propose LAMP, a provenance computation system for machine learning algorithms. Inspired by automatic differentiation (AD), LAMP quantifies the importance of an input for an output by computing the partial derivative. LAMP separates the original data processing and the more expensive derivative computation to different processes to achieve cost-effectiveness. In addition, it allows quantifying importance for inputs related to discrete behavior, such as control flow selection. The evaluation on a set of real world programs and data sets illustrates that LAMP produces more precise and succinct provenance than program dependence based techniques, with much less overhead. Our case studies demonstrate the potential of LAMP in problem diagnosis in data engineering.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {786–797},
numpages = {12},
keywords = {Machine Learning, Debugging, Data Provenance},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/1414004.1414066,
author = {Tosun, Ayse and Turhan, Burak and Bener, Ayse},
title = {Ensemble of software defect predictors: a case study},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414066},
doi = {10.1145/1414004.1414066},
abstract = {In this paper, we present a defect prediction model based on ensemble of classifiers, which has not been fully explored so far in this type of research. We have conducted several experiments on public datasets. Our results reveal that ensemble of classifiers considerably improve the defect detection capability compared to Naive Bayes algorithm. We also conduct a cost-benefit analysis for our ensemble, where it turns out that it is enough to inspect 32% of the code on the average, for detecting 76% of the defects.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {318–320},
numpages = {3},
keywords = {static code attributes, ensemble of classifiers, defect prediction},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@inproceedings{10.1109/MSR.2019.00016,
author = {Hoang, Thong and Dam, Hoa Khanh and Kamei, Yasutaka and Lo, David and Ubayashi, Naoyasu},
title = {DeepJIT: an end-to-end deep learning framework for just-in-time defect prediction},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00016},
doi = {10.1109/MSR.2019.00016},
abstract = {Software quality assurance efforts often focus on identifying defective code. To find likely defective code early, change-level defect prediction - aka. Just-In-Time (JIT) defect prediction - has been proposed. JIT defect prediction models identify likely defective changes and they are trained using machine learning techniques with the assumption that historical changes are similar to future ones. Most existing JIT defect prediction approaches make use of manually engineered features. Unlike those approaches, in this paper, we propose an end-to-end deep learning framework, named DeepJIT, that automatically extracts features from commit messages and code changes and use them to identify defects. Experiments on two popular software projects (i.e., QT and OPENSTACK) on three evaluation settings (i.e., cross-validation, short-period, and long-period) show that the best variant of DeepJIT (DeepJIT-Combined), compared with the best performing state-of-the-art approach, achieves improvements of 10.36--11.02% for the project QT and 9.51--13.69% for the project OPENSTACK in terms of the Area Under the Curve (AUC).},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {34–45},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1007/978-3-030-26250-1_33,
author = {Matsuno, Yutaka and Ishikawa, Fuyuki and Tokumoto, Susumu},
title = {Tackling Uncertainty in Safety Assurance for Machine Learning: Continuous Argument Engineering with Attributed Tests},
year = {2019},
isbn = {978-3-030-26249-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26250-1_33},
doi = {10.1007/978-3-030-26250-1_33},
abstract = {There are unique kinds of uncertainty in implementations constructed by machine learning from training data. This uncertainty affects the strategy and activities for safety assurance. In this paper, we investigate this point in terms of continuous argument engineering with a granular performance evaluation over the expected operational domain. We employ an attribute testing method for evaluating an implemented model in terms of explicit (partial) specification. We then show experimental results that demonstrate how safety arguments are affected by the uncertainty of machine learning. As an example, we show the weakness of a model, which cannot be predicted beforehand. We show our tool for continuous argument engineering to track the latest state of assurance.},
booktitle = {Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings},
pages = {398–404},
numpages = {7},
location = {Turku, Finland}
}

@article{10.1155/2019/4368036,
author = {Deli\'{c}, Vlado and Peri\'{c}, Zoran and Se\v{c}ujski, Milan and Jakovljevi\'{c}, Nik\v{s}a and Nikoli\'{c}, Jelena and Mi\v{s}kovi\'{c}, Dragi\v{s}a and Simi\'{c}, Nikola and Suzi\'{c}, Sini\v{s}a and Deli\'{c}, Tijana and Gastaldo, Paolo},
title = {Speech Technology Progress Based on New Machine Learning Paradigm},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1687-5265},
url = {https://doi.org/10.1155/2019/4368036},
doi = {10.1155/2019/4368036},
abstract = {Speech technologies have been developed for decades as a typical signal processing area, while the last decade has brought a huge progress based on new machine learning paradigms. Owing not only to their intrinsic complexity but also to their relation with cognitive sciences, speech technologies are now viewed as a prime example of interdisciplinary knowledge area. This review article on speech signal analysis and processing, corresponding machine learning algorithms, and applied computational intelligence aims to give an insight into several fields, covering speech production and auditory perception, cognitive aspects of speech communication and language understanding, both speech recognition and text-to-speech synthesis in more details, and consequently the main directions in development of spoken dialogue systems. Additionally, the article discusses the concepts and recent advances in speech signal compression, coding, and transmission, including cognitive speech coding. To conclude, the main intention of this article is to highlight recent achievements and challenges based on new machine learning paradigms that, over the last decade, had an immense impact in the field of speech signal processing.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@article{10.1016/j.knosys.2021.107541,
author = {Pandey, Sushant Kumar and Tripathi, Anil Kumar},
title = {DNNAttention: A deep neural network and attention based architecture for cross project defect number prediction},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {233},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107541},
doi = {10.1016/j.knosys.2021.107541},
journal = {Know.-Based Syst.},
month = dec,
numpages = {30},
keywords = {Software defect number prediction, Long short term memory (LSTM), Attention layer, Deep neural network, Cross project defect prediction}
}

@inproceedings{10.1109/ARSO.2016.7736285,
author = {Tao, Yong and Zheng, Jiaqi and Wang, Tianmiao and Hu, Yaoguang},
title = {A state and fault prediction method based on RBF neural networks},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ARSO.2016.7736285},
doi = {10.1109/ARSO.2016.7736285},
abstract = {A state and fault prediction method based on RBF neural networks is proposed. The agricultural machinery is chosen as the experimental object of the method. There are 4 health level, such as failure, hazardous, sub-healthy and healthy. Some data of different provinces have been obtained, the health level can be acquired by RBF neural networks. The mathematical model of agricultural machinery is difficult to be proposed in this paper, so the traditional control algorithm can't be used in agricultural machinery. However, the RBF neural networks can solve this problem. At the same time, some vital factors should be considered, such as mileages, rotational speed, stubble height, water temperature, oil pressure of agricultural machinery. The rotational speed and stubble height have a big effect on fault prediction of agriculture. The experimental results verify the effectiveness of the proposed method.},
booktitle = {2016 IEEE Workshop on Advanced Robotics and Its Social Impacts (ARSO)},
pages = {221–225},
numpages = {5},
location = {Shanghai, China}
}

@inproceedings{10.1007/978-3-030-31280-0_4,
author = {Nakajima, Shin and Chen, Tsong Yueh},
title = {Generating Biased Dataset for Metamorphic Testing of Machine Learning Programs},
year = {2019},
isbn = {978-3-030-31279-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-31280-0_4},
doi = {10.1007/978-3-030-31280-0_4},
abstract = {Although both positive and negative testing are important for assuring quality of programs, generating a variety of test inputs for such testing purposes is difficult for machine learning software. This paper studies why it is difficult, and then proposes a new method of generating datasets that are test inputs to machine learning programs. The proposed idea is demonstrated with a case study of classifying hand-written numbers.},
booktitle = {Testing Software and Systems: 31st IFIP WG 6.1 International Conference, ICTSS 2019, Paris, France, October 15–17, 2019, Proceedings},
pages = {56–64},
numpages = {9},
location = {Paris, France}
}

@article{10.1049/iet-sen.2019.0291,
author = {Silva‐Rodr\'{\i}guez, Viridiana and Nava‐Mu\~{n}oz, Sandra Edith and Castro, Luis A. and Mart\'{\i}nez‐P\'{e}rez, Francisco E. and P\'{e}rez‐Gonz\'{a}lez, H\'{e}ctor G. and Torres‐Reyes, Francisco},
title = {Classifying design‐level requirements using machine learning for a recommender of interaction design patterns},
year = {2020},
issue_date = {October 2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {14},
number = {5},
url = {https://doi.org/10.1049/iet-sen.2019.0291},
doi = {10.1049/iet-sen.2019.0291},
abstract = {Software Engineering is a discipline that encompasses processes associated with the development of interactive systems. The perceived quality of an interactive system is heavily influenced by the user interface design, which may result in many challenges. One such challenge is design‐level requirements analysis. The success of the software system is mostly dependent on how well users’ requirements have been understood and translated into appropriate functionalities. During the interactive system design process, it is common to find recurring problems in human–computer interactions, for which reusing solutions is highly feasible. Interaction design patterns seek to support designers in decision making during the design of interactive systems. Due to the design task tends to be subjective and prone to errors. This work aims at presenting and evaluating an interaction design patterns recommendation model based on design‐level requirements classification, through the application of supervised machine learning algorithms. To compare the performance of four classification algorithms, a study was carried out, in which the linear support vector machine was the most suitable to this problem. The results of this work can be used for implementing frameworks that can better support designers’ decision making when designing user interfaces.},
journal = {IET Software},
month = oct,
pages = {544–552},
numpages = {9},
keywords = {supervised machine learning algorithms, design‐level requirements classification, interaction design patterns recommendation model, design task, human–computer interactions, interactive system design process, software system, design‐level requirements analysis, user interface design, software quality, decision making, interactive systems, object‐oriented methods, pattern classification, human computer interaction, user interfaces, learning (artificial intelligence)}
}

@article{10.1007/s10664-008-9079-3,
author = {Jiang, Yue and Cukic, Bojan and Ma, Yan},
title = {Techniques for evaluating fault prediction models},
year = {2008},
issue_date = {October   2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9079-3},
doi = {10.1007/s10664-008-9079-3},
abstract = {Many statistical techniques have been proposed to predict fault-proneness of program modules in software engineering. Choosing the "best" candidate among many available models involves performance assessment and detailed comparison, but these comparisons are not simple due to the applicability of varying performance measures. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Methodologies for precise evaluation of fault prediction models should be at the core of empirical software engineering research, but have attracted sporadic attention. In this paper, we overview model evaluation techniques. In addition to many techniques that have been used in software engineering studies before, we introduce and discuss the merits of cost curves. Using the data from a public repository, our study demonstrates the strengths and weaknesses of performance evaluation techniques and points to a conclusion that the selection of the "best" model cannot be made without considering project cost characteristics, which are specific in each development environment.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {561–595},
numpages = {35},
keywords = {Predictive models in software engineering, Model evaluation, Fault-prediction models, Empirical studies}
}

@article{10.1007/s11334-017-0295-0,
author = {Shatnawi, Raed},
title = {The application of ROC analysis in threshold identification, data imbalance and metrics selection for software fault prediction},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {2–3},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-017-0295-0},
doi = {10.1007/s11334-017-0295-0},
abstract = {Software engineers have limited resources and need metrics analysis tools to investigate software quality such as fault-proneness of modules. There are a large number of software metrics available to investigate quality. However, not all metrics are strongly correlated with faults. In addition, software fault data are imbalanced and affect quality assessment tools such as fault prediction or threshold values that are used to identify risky modules. Software quality is investigated for three purposes. First, the receiver operating characteristics (ROC) analysis is used to identify threshold values to identify risky modules. Second, the ROC analysis is investigated for imbalanced data. Third, the ROC analysis is considered for feature selection. This work validated the use of ROC to identify thresholds for four metrics (WMC, CBO, RFC and LCOM). The ROC results after sampling the data are not significantly different from before sampling. The ROC analysis selects the same metrics (WMC, CBO and RFC) in most datasets, while other techniques have a large variation in selecting metrics.},
journal = {Innov. Syst. Softw. Eng.},
month = sep,
pages = {201–217},
numpages = {17},
keywords = {Software metrics, ROC analysis, Imbalanced data, Feature selection, Fault-proneness models}
}

@inproceedings{10.1145/3236024.3236055,
author = {Hu, Gang and Zhu, Linjie and Yang, Junfeng},
title = {AppFlow: using machine learning to synthesize robust, reusable UI tests},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236055},
doi = {10.1145/3236024.3236055},
abstract = {UI testing is known to be difficult, especially as today’s development cycles become faster. Manual UI testing is tedious, costly and error- prone. Automated UI tests are costly to write and maintain. This paper presents AppFlow, a system for synthesizing highly robust, highly reusable UI tests. It leverages machine learning to automatically recognize common screens and widgets, relieving developers from writing ad hoc, fragile logic to use them in tests. It enables developers to write a library of modular tests for the main functionality of an app category (e.g., an “add to cart” test for shopping apps). It can then quickly test a new app in the same category by synthesizing full tests from the modular ones in the library. By focusing on the main functionality, AppFlow provides “smoke testing” requiring little manual work. Optionally, developers can customize AppFlow by adding app-specific tests for completeness. We evaluated AppFlow on 60 popular apps in the shopping and the news category, two case studies on the BBC news app and the JackThreads shopping app, and a user-study of 15 subjects on the Wish shopping app. Results show that AppFlow accurately recognizes screens and widgets, synthesizes highly robust and reusable tests, covers 46.6% of all automatable tests for Jackthreads with the tests it synthesizes, and reduces the effort to test a new app by up to 90%. Interestingly, it found eight bugs in the evaluated apps, including seven functionality bugs, despite that they were publicly released and supposedly went through thorough testing.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {269–282},
numpages = {14},
keywords = {test synthesis, test reuse, mobile testing, machine learning, UI testing, UI recognition},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.5555/3045390.3045466,
author = {Patrini, Giorgio and Nielsen, Frank and Nock, Richard and Carioni, Marcello},
title = {Loss factorization, weakly supervised learning and label noise robustness},
year = {2016},
publisher = {JMLR.org},
abstract = {We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the same loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator -- the focal quantity of this work -- which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation.Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like SGD and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {708–717},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@article{10.1007/s10664-020-09880-1,
author = {Trautsch, Alexander and Herbold, Steffen and Grabowski, Jens},
title = {A longitudinal study of static analysis warning evolution and the effects of PMD on software quality in Apache open source projects},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09880-1},
doi = {10.1007/s10664-020-09880-1},
abstract = {Automated static analysis tools (ASATs) have become a major part of the software development workflow. Acting on the generated warnings, i.e., changing the code indicated in the warning, should be part of, at latest, the code review phase. Despite this being a best practice in software development, there is still a lack of empirical research regarding the usage of ASATs in the wild. In this work, we want to study ASAT warning trends in software via the example of PMD as an ASAT and its usage in open source projects. We analyzed the commit history of 54 projects (with 112,266 commits in total), taking into account 193 PMD rules and 61 PMD releases. We investigate trends of ASAT warnings over up to 17 years for the selected study subjects regarding changes of warning types, short and long term impact of ASAT use, and changes in warning severities. We found that large global changes in ASAT warnings are mostly due to coding style changes regarding braces and naming conventions. We also found that, surprisingly, the influence of the presence of PMD in the build process of the project on warning removal trends for the number of warnings per lines of code is small and not statistically significant. Regardless, if we consider defect density as a proxy for external quality, we see a positive effect if PMD is present in the build configuration of our study subjects.},
journal = {Empirical Softw. Engg.},
month = nov,
pages = {5137–5192},
numpages = {56},
keywords = {Software quality, Software metrics, Quality evolution, Static code analysis}
}

@article{10.4018/JITR.2020040107,
author = {Agrawal, Anushree and Singh, R. K.},
title = {Identification of Co-Changed Classes in Software Applications Using Software Quality Attributes},
year = {2020},
issue_date = {Apr 2020},
publisher = {IGI Global},
address = {USA},
volume = {13},
number = {2},
issn = {1938-7857},
url = {https://doi.org/10.4018/JITR.2020040107},
doi = {10.4018/JITR.2020040107},
abstract = {When changes are made to software applications often, defects can occur in software applications, and eventually leads to expensive operational faults. Comprehensive testing is not feasible with the limited time and resources available. There is a need for test case selection and prioritization so that testing can be completed with maximum confidence in a minimum time. Advance knowledge of co-changed classes in software applications can be very useful during the software maintenance phase. In this article, the authors have proposed a co-change prediction model based upon the combination of structural code measures and dynamic revision history from change repository. Univariate analysis is applied to identify the useful measures in co-change identification. The proposed model is validated using eight open source software applications. The results are promising and indicate that they can be very beneficial in maintenance of software applications.},
journal = {J. Inf. Technol. Res.},
month = apr,
pages = {110–128},
numpages = {19},
keywords = {Software Metrics, Regression Analysis, Open Source Software, Co-change Prediction}
}

@article{10.1016/j.procs.2015.02.041,
author = {Dhanalaxmi, B. and Naidu, G. Apparao and Anuradha, K.},
title = {Adaptive PSO Based Association Rule Mining Technique for Software Defect Classification Using ANN},
year = {2015},
issue_date = {2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2015.02.041},
doi = {10.1016/j.procs.2015.02.041},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {432–442},
numpages = {11},
keywords = {Defect prevention., Software testing, Artificial neural network, Adaptive particle swarm optimization algorithm, Association rule mining}
}

@inproceedings{10.1145/1540438.1540466,
author = {Jiang, Yue and Cukic, Bojan},
title = {Misclassification cost-sensitive fault prediction models},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540466},
doi = {10.1145/1540438.1540466},
abstract = {Traditionally, software fault prediction models are built by assuming a uniform misclassification cost. In other words, cost implications of misclassifying a faulty module as fault free are assumed to be the same as the cost implications of misclassifying a fault free module as faulty. In reality, these two types of misclassification costs are rarely equal. They are project-specific, reflecting the characteristics of the domain in which the program operates. In this paper, using project information from a public repository, we analyze the benefits of techniques which incorporate misclassification costs in the development of software fault prediction models. We find that cost-sensitive learning does not provide operational points which outperform cost-insensitive classifiers. However, an advantage of cost-sensitive modeling is the explicit choice of the operational threshold appropriate for the cost differential.},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {20},
numpages = {10},
keywords = {cost-sensitive, fault prediction, machine learning, misclassification cost},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@article{10.1007/s10664-015-9381-9,
author = {Mcintosh, Shane and Kamei, Yasutaka and Adams, Bram and Hassan, Ahmed E.},
title = {An empirical study of the impact of modern code review practices on software quality},
year = {2016},
issue_date = {October   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9381-9},
doi = {10.1007/s10664-015-9381-9},
abstract = {Software code review, i.e., the practice of having other team members critique changes to a software system, is a well-established best practice in both open source and proprietary software domains. Prior work has shown that formal code inspections tend to improve the quality of delivered software. However, the formal code inspection process mandates strict review criteria (e.g., in-person meetings and reviewer checklists) to ensure a base level of review quality, while the modern, lightweight code reviewing process does not. Although recent work explores the modern code review process, little is known about the relationship between modern code review practices and long-term software quality. Hence, in this paper, we study the relationship between post-release defects (a popular proxy for long-term software quality) and: (1) code review coverage, i.e., the proportion of changes that have been code reviewed, (2) code review participation, i.e., the degree of reviewer involvement in the code review process, and (3) code reviewer expertise, i.e., the level of domain-specific expertise of the code reviewers. Through a case study of the Qt, VTK, and ITK projects, we find that code review coverage, participation, and expertise share a significant link with software quality. Hence, our results empirically confirm the intuition that poorly-reviewed code has a negative impact on software quality in large systems using modern reviewing tools.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {2146–2189},
numpages = {44},
keywords = {Software quality, Code review}
}

@article{10.1007/s10664-010-9151-7,
author = {Kpodjedo, Segla and Ricca, Filippo and Galinier, Philippe and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Antoniol, Giuliano},
title = {Design evolution metrics for defect prediction in object oriented systems},
year = {2011},
issue_date = {February  2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-010-9151-7},
doi = {10.1007/s10664-010-9151-7},
abstract = {Testing is the most widely adopted practice to ensure software quality. However, this activity is often a compromise between the available resources and software quality. In object-oriented development, testing effort should be focused on defective classes. Unfortunately, identifying those classes is a challenging and difficult activity on which many metrics, techniques, and models have been tried. In this paper, we investigate the usefulness of elementary design evolution metrics to identify defective classes. The metrics include the numbers of added, deleted, and modified attributes, methods, and relations. The metrics are used to recommend a ranked list of classes likely to contain defects for a system. They are compared to Chidamber and Kemerer's metrics on several versions of Rhino and of ArgoUML. Further comparison is conducted with the complexity metrics computed by Zimmermann et al. on several releases of Eclipse. The comparisons are made according to three criteria: presence of defects, number of defects, and defect density in the top-ranked classes. They show that the design evolution metrics, when used in conjunction with known metrics, improve the identification of defective classes. In addition, they show that the design evolution metrics make significantly better predictions of defect density than other metrics and, thus, can help in reducing the testing effort by focusing test activity on a reduced volume of code.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {141–175},
numpages = {35},
keywords = {Error tolerant graph matching, Design evolution metrics, Defect prediction}
}

@article{10.1016/j.jss.2017.07.012,
author = {Huang, Jianglin and Keung, Jacky Wai and Sarro, Federica and Li, Yan-Fu and Yu, Y.T. and Chan, W.K. and Sun, Hongyi},
title = {Cross-validation based K nearest neighbor imputation for software quality datasets},
year = {2017},
issue_date = {October 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {132},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.07.012},
doi = {10.1016/j.jss.2017.07.012},
abstract = {Proposing a new KNN imputation technique in SEE context.The new KNN imputation outperforms the other approaches.Evaluating the impact of MM and MP on imputation accuracy.Finding a good parameter setting of KNN imputation for software quality datasets. Being able to predict software quality is essential, but also it pose significant challenges in software engineering. Historical software project datasets are often being utilized together with various machine learning algorithms for fault-proneness classification. Unfortunately, the missing values in datasets have negative impacts on the estimation accuracy and therefore, could lead to inconsistent results. As a method handling missing data, K nearest neighbor (KNN) imputation gradually gains acceptance in empirical studies by its exemplary performance and simplicity. To date, researchers still call for optimized parameter setting for KNN imputation to further improve its performance. In the work, we develop a novel incomplete-instance based KNN imputation technique, which utilizes a cross-validation scheme to optimize the parameters for each missing value. An experimental assessment is conducted on eight quality datasets under various missingness scenarios. The study also compared the proposed imputation approach with mean imputation and other three KNN imputation approaches. The results show that our proposed approach is superior to others in general. The relatively optimal fixed parameter settings for KNN imputation for software quality data is also determined. It is observed that the classification accuracy is improved or at least maintained by using our approach for missing data imputation.},
journal = {J. Syst. Softw.},
month = oct,
pages = {226–252},
numpages = {27},
keywords = {SVMSupport Vector Machine, SEESoftware Engineering Estimation, RQResearch Question, RMSERoot Mean Square Error, PROMISEPRedictOr Models In Software Engineering, NINon-ignorable, Missing data, MRMissingness Ratio, MPMissingness Pattern, MMMissingness Mechanism, MIMutual Information, MEIMean Imputation, MDTMissing Data Treatment, MCARMissing Completely At Random, MARMissing At Random, LOCLines of Code, KNNK Nearest Neighbor, KNN, K=3 with Dudani adaptation based imputation, Imputation, IRWMInverse Rank Weighted Mean, IDWMInverse Distance Weighted Mean, ICkNNIIncomplete-case based KNN Imputation, GRGGrey Relational Grade, GRCGrey Relational Coefficient, GRAGrey Relational Analysis, G3DGRA-based distance, FWGkNNIFeature Weighted Grey based KNN Imputation, FPFault-proneness, Empirical software engineering estimation, DkNNIDefault KNN Imputation, Cross-validation, CVBkNNICross-validation based KNN Imputation, CKChidamber and Kemerer object-oriented metric, CCkNNIComplete-case based KNN Imputation, CAClassification Accuracy, BMIBayes Multiple Imputation}
}

@inproceedings{10.1145/2938503.2938553,
author = {Soltanifar, Behjat and Akbarinasaji, Shirin and Caglayan, Bora and Bener, Ayse Basar and Filiz, Asli and Kramer, Bryan M.},
title = {Software Analytics in Practice: A Defect Prediction Model Using Code Smells},
year = {2016},
isbn = {9781450341189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2938503.2938553},
doi = {10.1145/2938503.2938553},
abstract = {In software engineering, maintainability is related to investigating the defects and their causes, correcting the defects and modifying the system to meet customer requirements. Maintenance is a time consuming activity within the software life cycle. Therefore, there is a need for efficiently organizing the software resources in terms of time, cost and personnel for maintenance activity. One way of efficiently managing maintenance resources is to predict defects that may occur after the deployment. Many researchers so far have built defect prediction models using different sets of metrics such as churn and static code metrics. However, hidden causes of defects such as code smells have not been investigated thoroughly. In this study we propose using data science and analytics techniques on software data to build defect prediction models. In order to build the prediction model we used code smells metrics, churn metrics and combination of churn and code smells metrics. The results of our experiments on two different software companies show that code smells is a good indicator of defect proneness of the software product. Therefore, we recommend that code smells metrics should be used to train a defect prediction model to guide the software maintenance team.},
booktitle = {Proceedings of the 20th International Database Engineering &amp; Applications Symposium},
pages = {148–155},
numpages = {8},
keywords = {Mining software repositories, Defect Prediction Model, Code Smells},
location = {Montreal, QC, Canada},
series = {IDEAS '16}
}

@article{10.1007/s10836-018-5716-y,
author = {Mandouh, Eman and Wassal, Amr G.},
title = {Application of Machine Learning Techniques in Post-Silicon Debugging and Bug Localization},
year = {2018},
issue_date = {April     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {34},
number = {2},
issn = {0923-8174},
url = {https://doi.org/10.1007/s10836-018-5716-y},
doi = {10.1007/s10836-018-5716-y},
abstract = {As the size of hardware (HW) design increases significantly, a huge amount of data is generated during the design simulation, emulation or prototyping. Debugging large HW designs becomes a tedious, time consuming and a bottleneck task within the function verification activities. This paper proposes the utilization of machine learning techniques to automate the diagnosis of design trace dump as well as helping in bug localization during post-silicon validation. Our framework starts by signal selection algorithm that identifies which signals to monitor during design execution. Signal selection depends on signal types as well as their connectivity network. The design is then executed and the trace dump is saved for offline analysis. Big-Data processing technique, namely, Map-Reduce is used to overcome the challenge of processing huge trace dump resulted from design running on FPGA prototype. K-means Clustering method is applied to group trace segments that are very similar and to identify the ones with a rare occurrence during the design execution. Additionally, we propose a bug localization framework in which X-means clustering is used to group the passing regression tests in clusters such that buggy tests can be detected when they fail to be assigned to any of the trained clusters. Our experimental results demonstrate the feasibility of the proposed approach in guiding the debugging effort using a group of industrial HW designs and its ability to detect multiple design injected defects using mutation-based-testing method.},
journal = {J. Electron. Test.},
month = apr,
pages = {163–181},
numpages = {19},
keywords = {Post-silicon validation, HW debugging and machine learning, Big-data}
}

@inproceedings{10.1145/2393596.2393619,
author = {Caglayan, Bora and Misirli, Ayse Tosun and Calikli, Gul and Bener, Ayse and Aytac, Turgay and Turhan, Burak},
title = {Dione: an integrated measurement and defect prediction solution},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393619},
doi = {10.1145/2393596.2393619},
abstract = {We present an integrated measurement and defect prediction tool: Dione. Our tool enables organizations to measure, monitor, and control product quality through learning based defect prediction. Similar existing tools either provide data collection and analytics, or work just as a prediction engine. Therefore, companies need to deal with multiple tools with incompatible interfaces in order to deploy a complete measurement and prediction solution. Dione provides a fully integrated solution where data extraction, defect prediction and reporting steps fit seamlessly. In this paper, we present the major functionality and architectural elements of Dione followed by an overview of our demonstration.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {20},
numpages = {2},
keywords = {software tool, software defect prediction, measurement},
location = {Cary, North Carolina},
series = {FSE '12}
}

@article{10.1016/j.infsof.2010.11.013,
author = {Azar, D. and Vybihal, J.},
title = {An ant colony optimization algorithm to improve software quality prediction models},
year = {2011},
issue_date = {April, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.11.013},
doi = {10.1016/j.infsof.2010.11.013},
abstract = {ContextAssessing software quality at the early stages of the design and development process is very difficult since most of the software quality characteristics are not directly measurable. Nonetheless, they can be derived from other measurable attributes. For this purpose, software quality prediction models have been extensively used. However, building accurate prediction models is hard due to the lack of data in the domain of software engineering. As a result, the prediction models built on one data set show a significant deterioration of their accuracy when they are used to classify new, unseen data. ObjectiveThe objective of this paper is to present an approach that optimizes the accuracy of software quality predictive models when used to classify new data. MethodThis paper presents an adaptive approach that takes already built predictive models and adapts them (one at a time) to new data. We use an ant colony optimization algorithm in the adaptation process. The approach is validated on stability of classes in object-oriented software systems and can easily be used for any other software quality characteristic. It can also be easily extended to work with software quality predictive problems involving more than two classification labels. ResultsResults show that our approach out-performs the machine learning algorithm C4.5 as well as random guessing. It also preserves the expressiveness of the models which provide not only the classification label but also guidelines to attain it. ConclusionOur approach is an adaptive one that can be seen as taking predictive models that have already been built from common domain data and adapting them to context-specific data. This is suitable for the domain of software quality since the data is very scarce and hence predictive models built from one data set is hard to generalize and reuse on new data.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {388–393},
numpages = {6},
keywords = {Software quality, Search-based software engineering, Metric, Ant colony optimization}
}

@article{10.1007/s10664-008-9103-7,
author = {Turhan, Burak and Menzies, Tim and Bener, Ay\c{s}e B. and Di Stefano, Justin},
title = {On the relative value of cross-company and within-company data for defect prediction},
year = {2009},
issue_date = {October   2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9103-7},
doi = {10.1007/s10664-008-9103-7},
abstract = {We propose a practical defect prediction approach for companies that do not track defect related data. Specifically, we investigate the applicability of cross-company (CC) data for building localized defect predictors using static code features. Firstly, we analyze the conditions, where CC data can be used as is. These conditions turn out to be quite few. Then we apply principles of analogy-based learning (i.e. nearest neighbor (NN) filtering) to CC data, in order to fine tune these models for localization. We compare the performance of these models with that of defect predictors learned from within-company (WC) data. As expected, we observe that defect predictors learned from WC data outperform the ones learned from CC data. However, our analyses also yield defect predictors learned from NN-filtered CC data, with performance close to, but still not better than, WC data. Therefore, we perform a final analysis for determining the minimum number of local defect reports in order to learn WC defect predictors. We demonstrate in this paper that the minimum number of data samples required to build effective defect predictors can be quite small and can be collected quickly within a few months. Hence, for companies with no local defect data, we recommend a two-phase approach that allows them to employ the defect prediction process instantaneously. In phase one, companies should use NN-filtered CC data to initiate the defect prediction process and simultaneously start collecting WC (local) data. Once enough WC data is collected (i.e. after a few months), organizations should switch to phase two and use predictors learned from WC data.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {540–578},
numpages = {39},
keywords = {Within-company, Nearest-neighbor filtering, Metrics (product metrics), Learning, Defect prediction, Cross-company}
}

@article{10.1007/s10664-009-9115-y,
author = {Gokhale, Swapna S. and Mullen, Robert E.},
title = {A multiplicative model of software defect repair times},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-009-9115-y},
doi = {10.1007/s10664-009-9115-y},
abstract = {We hypothesize that software defect repair times can be characterized by the Laplace Transform of the Lognormal (LTLN) distribution. This hypothesis is rooted in the observation that software defect repair times are influenced by the multiplicative interplay of several factors, and the lognormal distribution is a natural choice to model rates of occurrence of such phenomenon. Conversion of the lognormal rate distribution to an occurrence time distribution yields the LTLN. We analyzed a total of more than 10,000 software defect repair times collected over nine products at Cisco Systems to confirm our LTLN hypothesis. Our results also demonstrate that the LTLN distribution provides a statistically better fit to the observed repair times than either of the two most widely used repair time distributions, namely, the lognormal and the exponential. Moreover, we show that the repair times of subsets of defects, partitioned according to the Orthogonal Defect Classification (ODC) scheme also follow the LTLN distribution. Finally, we describe how the insights that lead to the LTLN repair time model allow us to consider and evaluate alternative process improvement strategies.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {296–319},
numpages = {24},
keywords = {Software defect repair, Multiplicative effect, Lognormal}
}

@inproceedings{10.1145/3194095.3194096,
author = {Srisopha, Kamonphop and Alfayez, Reem},
title = {Software quality through the eyes of the end-user and static analysis tools: a study on Android OSS applications},
year = {2018},
isbn = {9781450357371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194095.3194096},
doi = {10.1145/3194095.3194096},
abstract = {Source code analysis tools have been the vehicle for measuring and assessing the quality of a software product for decades. However, recently many studies have shown that post-deployment end-user reviews provide a wealth of insight into the quality of a software product and how it should evolve and be maintained. For example, end-user reviews help to identify missing features or inform developers about incorrect or unexpected software behavior. We believe that analyzing end-user reviews and utilizing analysis tools are a crucial step towards understanding the complete picture of the quality of a software product, as well as towards reasoning about the evolution history of it. In this paper, we investigate whether both methods correlate with one another. In other words, we explore if there exists a relationship between user satisfaction and the application's internal quality characteristics. To conduct our research, we analyze a total of 46 actual releases of three Android open source software (OSS) applications on the Google Play Store. For each release, we employ multiple static analysis tools to assess several aspects of the application's software quality. We retrieve and manually analyze the complete reviews after each release of each application from its store page, totaling 1004 reviews. Our initial results suggest that having high or low code quality does not necessary ensure user overall satisfaction.},
booktitle = {Proceedings of the 1st International Workshop on Software Qualities and Their Dependencies},
pages = {1–4},
numpages = {4},
keywords = {user satisfaction, static analysis, software quality, software evolution, software engineering},
location = {Gothenburg, Sweden},
series = {SQUADE '18}
}

@article{10.1109/TSE.2016.2553030,
author = {Tantithamthavorn, Chakkrit and McIntosh, Shane and Hassan, Ahmed E. and Matsumoto, Kenichi},
title = {Comments on \'{z}Researcher Bias: The Use of Machine Learning in Software Defect Prediction\'{z}},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {42},
number = {11},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2016.2553030},
doi = {10.1109/TSE.2016.2553030},
abstract = {Shepperd&nbsp;et&nbsp;al.&nbsp;find that the reported performance of a defect prediction model shares a strong relationship with the group of researchers who construct the models. In this paper, we perform an alternative investigation of Shepperd&nbsp;et&nbsp;al.'s data. We observe that (a) research group shares a strong association with other explanatory variables (i.e., the dataset and metric families that are used to build a model); (b) the strong association among these explanatory variables makes it difficult to discern the impact of the research group on model performance; and (c) after mitigating the impact of this strong association, we find that the research group has a smaller impact than the metric family. These observations lead us to conclude that the relationship between the research group and the performance of a defect prediction model are more likely due to the tendency of researchers to reuse experimental components (e.g., datasets and metrics). We recommend that researchers experiment with a broader selection of datasets and metrics to combat any potential bias in their results.},
journal = {IEEE Trans. Softw. Eng.},
month = nov,
pages = {1092–1094},
numpages = {3}
}

@inproceedings{10.1145/1987875.1987888,
author = {Bi\c{c}er, Serdar and Bener, Ay\c{s}e Ba\c{s}ar and \c{C}a\u{g}layan, Bora},
title = {Defect prediction using social network analysis on issue repositories},
year = {2011},
isbn = {9781450307307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987875.1987888},
doi = {10.1145/1987875.1987888},
abstract = {People are the most important pillar of software development process. It is critical to understand how they interact with each other and how these interactions affect the quality of the end product in terms of defects. In this research we propose to include a new set of metrics, a.k.a. social network metrics on issue repositories in predicting defects. Social network metrics on issue repositories has not been used before to predict defect proneness of a software product. To validate our hypotheses we used two datasets, development data of IBM1 Rational ® Team Concert™ (RTC) and Drupal, to conduct our experiments. The results of the experiments revealed that compared to other set of metrics such as churn metrics using social network metrics on issue repositories either considerably decreases high false alarm rates without compromising the detection rates or considerably increases low prediction rates without compromising low false alarm rates. Therefore we recommend practitioners to collect social network metrics on issue repositories since people related information is a strong indicator of past patterns in a given team.},
booktitle = {Proceedings of the 2011 International Conference on Software and Systems Process},
pages = {63–71},
numpages = {9},
keywords = {social networks, network metrics, developer communication, defect prediction},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSSP '11}
}

@inproceedings{10.1145/3213846.3213858,
author = {Dwarakanath, Anurag and Ahuja, Manish and Sikand, Samarth and Rao, Raghotham M. and Bose, R. P. Jagadeesh Chandra and Dubash, Neville and Podder, Sanjay},
title = {Identifying implementation bugs in machine learning based image classifiers using metamorphic testing},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213858},
doi = {10.1145/3213846.3213858},
abstract = {We have recently witnessed tremendous success of Machine Learning (ML) in practical applications. Computer vision, speech recognition and language translation have all seen a near human level performance. We expect, in the near future, most business applications will have some form of ML. However, testing such applications is extremely challenging and would be very expensive if we follow today's methodologies. In this work, we present an articulation of the challenges in testing ML based applications. We then present our solution approach, based on the concept of Metamorphic Testing, which aims to identify implementation bugs in ML based image classifiers. We have developed metamorphic relations for an application based on Support Vector Machine and a Deep Learning based application. Empirical validation showed that our approach was able to catch 71% of the implementation bugs in the ML applications.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {118–128},
numpages = {11},
keywords = {Testing Machine Learning based applications, Metamorphic Testing},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/3299771.3299777,
author = {Kumar, Lov and Satapathy, Shashank Mouli and Murthy, Lalita Bhanu},
title = {Method Level Refactoring Prediction on Five Open Source Java Projects using Machine Learning Techniques},
year = {2019},
isbn = {9781450362153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299771.3299777},
doi = {10.1145/3299771.3299777},
abstract = {Introduction: Identifying code segments in large and complex systems in need of refactoring is non-trivial for software developers. Our research aim is to develop recommendation systems for suggesting methods which require refactoring. Materials and Methods: Previous research shows that source code metrics for object-oriented software systems are indicators of complexity of a software system. We compute 25 different source code metrics at the method level and use it as features in a machine learning framework to predict the need of refactoring. We conduct a series of experiments on a publicly available annotated dataset of five software systems to investigate the performance of our proposed approach. In this proposed solution, ten different machine learning classifiers have been considered. In order to handle issues related to class imbalance, three different data sampling methods are also considered during implementation. Conclusion: Our analysis reveals that the mean accuracy for the SMOTE and RUSBoost data sampling technique is 98.47% respectively. The mean accuracy for the classifier AdaBoost is 98.16% and the mean accuracy for the classifier ANN+GD is 98.17% respectively. Hypothesis testing results reveals that the performance of different classifiers and data sampling techniques are statistically significant in nature.},
booktitle = {Proceedings of the 12th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {7},
numpages = {10},
keywords = {Empirical Software Engineering and Measurements (ESEM), Machine Learning, Software Maintenance, Software Refactoring, Source Code Analysis and Measurement},
location = {Pune, India},
series = {ISEC '19}
}

@article{10.1016/j.eswa.2007.02.012,
author = {Bibi, S. and Tsoumakas, G. and Stamelos, I. and Vlahavas, I.},
title = {Regression via Classification applied on software defect estimation},
year = {2008},
issue_date = {April, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {34},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.02.012},
doi = {10.1016/j.eswa.2007.02.012},
abstract = {In this paper we apply Regression via Classification (RvC) to the problem of estimating the number of software defects. This approach apart from a certain number of faults, it also outputs an associated interval of values, within which this estimate lies with a certain confidence. RvC also allows the production of comprehensible models of software defects exploiting symbolic learning algorithms. To evaluate this approach we perform an extensive comparative experimental study of the effectiveness of several machine learning algorithms in two software data sets. RvC manages to get better regression error than the standard regression approaches on both datasets.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {2091–2101},
numpages = {11},
keywords = {Software quality, Software metrics, Software fault estimation, Regression via Classification, Machine learning, ISBSG data set}
}

@inproceedings{10.1145/1540438.1540453,
author = {Tosun, Ay\c{s}e and Turhan, Burak and Bener, Ay\c{s}e},
title = {Practical considerations in deploying AI for defect prediction: a case study within the Turkish telecommunication industry},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540453},
doi = {10.1145/1540438.1540453},
abstract = {We have conducted a study in a large telecommunication company in Turkey to employ a software measurement program and to predict pre-release defects. We have previously built such predictors using AI techniques. This project is a transfer of our research experience into a real life setting to solve a specific problem for the company: to improve code quality by predicting pre-release defects and efficiently allocating testing resources. Our results in this project have many practical implications that managers have started benefiting: code analysis, bug tracking, effective use of version management system and defect prediction. Using version history information, developers can find around 88% of the defects with 28% false alarms, compared to same detection rate with 50% false alarms without using historical data. In this paper we also shared in detail our experience in terms of the project steps (i.e. challenges and opportunities).},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {11},
numpages = {9},
keywords = {AI methods, experience report, prediction, software defect prediction, static code attributes},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@article{10.1016/j.jss.2009.12.023,
author = {de Carvalho, Andr\'{e} B. and Pozo, Aurora and Vergilio, Silvia Regina},
title = {A symbolic fault-prediction model based on multiobjective particle swarm optimization},
year = {2010},
issue_date = {May, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.12.023},
doi = {10.1016/j.jss.2009.12.023},
abstract = {In the literature the fault-proneness of classes or methods has been used to devise strategies for reducing testing costs and efforts. In general, fault-proneness is predicted through a set of design metrics and, most recently, by using Machine Learning (ML) techniques. However, some ML techniques cannot deal with unbalanced data, characteristic very common of the fault datasets and, their produced results are not easily interpreted by most programmers and testers. Considering these facts, this paper introduces a novel fault-prediction approach based on Multiobjective Particle Swarm Optimization (MOPSO). Exploring Pareto dominance concepts, the approach generates a model composed by rules with specific properties. These rules can be used as an unordered classifier, and because of this, they are more intuitive and comprehensible. Two experiments were accomplished, considering, respectively, fault-proneness of classes and methods. The results show interesting relationships between the studied metrics and fault prediction. In addition to this, the performance of the introduced MOPSO approach is compared with other ML algorithms by using several measures including the area under the ROC curve, which is a relevant criterion to deal with unbalanced data.},
journal = {J. Syst. Softw.},
month = may,
pages = {868–882},
numpages = {15},
keywords = {Rule learning algorithm, Particle swarm optimization, Multiobjective, Fault prediction}
}

@inproceedings{10.1145/2532443.2532461,
author = {Chen, Jiaqiang and Liu, Shulong and Chen, Xiang and Gu, Qing and Chen, Daoxu},
title = {Empirical studies on feature selection for software fault prediction},
year = {2013},
isbn = {9781450323697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2532443.2532461},
doi = {10.1145/2532443.2532461},
abstract = {Classification based software fault prediction methods aim to classify the modules into either fault-prone or non-fault-prone. Feature selection is a preprocess step used to improve the data quality. However most of previous research mainly focus on feature relevance analysis, there is little work focusing on feature redundancy analysis. Therefore we propose a two-stage framework for feature selection to solve this issue. In particular, during the feature relevance phase, we adopt three different relevance measures to obtain the relevant feature subset. Then during the feature redundancy analysis phase, we use a cluster-based method to eliminate redundant features. To verify the effectiveness of our proposed framework, we choose typical real-world software projects, including Eclipse projects and NASA software project KC1. Final empirical result shows the effectiveness of our proposed framework.},
booktitle = {Proceedings of the 5th Asia-Pacific Symposium on Internetware},
articleno = {26},
numpages = {4},
keywords = {software fault prediction, relevance analysis, redundancy analysis, feature selection},
location = {Changsha, China},
series = {Internetware '13}
}

@inproceedings{10.5555/1961414.1961436,
author = {Al-Badareen, Anas Bassam and Muda, Zaiton and Jabar, Marzanah A. and Din, Jamilah and Turaev, Sherzod},
title = {Software quality evaluation through maintenance processes},
year = {2010},
isbn = {9789604742509},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {the software is always required to be developed and maintained a quality to the rapid progresses in industry, technology, economy, and other fields. Thus, software maintainability is the major activity in the software life cycle. This process bypass the cost required to develop new software product. However, software maintenance costs often total twice the original development cost in the lifetime of application software. Therefore, software maintainability is considered as one of the most important quality factor in software quality models. This study introduces a software maintenance process model that emphasizes the impact of the software quality on the maintenance and considers the quality of the modified software.},
booktitle = {Proceedings of the European Conference of Systems, and European Conference of Circuits Technology and Devices, and European Conference of Communications, and European Conference on Computer Science},
pages = {131–134},
numpages = {4},
keywords = {software quality, maintenance process, maintenance framework, maintenance evaluation},
location = {Tenerife, Spain},
series = {ECS'10/ECCTD'10/ECCOM'10/ECCS'10}
}

@article{10.1504/IJCAT.2016.080493,
author = {Kayarvizhy, N. and Kanmani, S. and Uthariaraj, V. Rhymend},
title = {Enhancing the fault prediction accuracy of CK metrics using high precision cohesion metric},
year = {2016},
issue_date = {January 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {54},
number = {4},
issn = {0952-8091},
url = {https://doi.org/10.1504/IJCAT.2016.080493},
doi = {10.1504/IJCAT.2016.080493},
abstract = {Object-oriented programs can be viewed as a collection of objects communicating with each other to perform a unique task. Many complex commercial applications have taken the object-oriented approach because of the benefits that it offers. The need for a reliable software resulted in the study and analysis of object-oriented metrics. The Chidamber and Kemerer CK metric suite has been considered as a pioneering work on object-oriented metrics and is the default standard for any new metric to be compared against. In this paper we evaluate the fault prediction capability of CK metric suite and validate it empirically. To further improve the accuracy of fault prediction we explore replacing the cohesion metric LCOM in CK suite with the proposed cohesion metric high precision cohesion metric. We have considered data from 500 classes spread across 12 projects for the study. The results show that there is a considerable improvement in the prediction accuracy.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {290–296},
numpages = {7},
keywords = {software reliability, object-oriented programming, object-oriented metrics, high precision cohesion metric, fault prediction accuracy, OOP, HPCM, Chidamber and Kemerer, CK metrics}
}

@article{10.1007/s11219-008-9058-3,
author = {Khoshgoftaar, Taghi M. and Rebours, Pierre and Seliya, Naeem},
title = {Software quality analysis by combining multiple projects and learners},
year = {2009},
issue_date = {March     2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-008-9058-3},
doi = {10.1007/s11219-008-9058-3},
abstract = {When building software quality models, the approach often consists of training data mining learners on a single fit dataset. Typically, this fit dataset contains software metrics collected during a past release of the software project that we want to predict the quality of. In order to improve the predictive accuracy of such quality models, it is common practice to combine the predictive results of multiple learners to take advantage of their respective biases. Although multi-learner classifiers have been proven to be successful in some cases, the improvement is not always significant because the information in the fit dataset sometimes can be insufficient. We present an innovative method to build software quality models using majority voting to combine the predictions of multiple learners induced on multiple training datasets. To our knowledge, no previous study in software quality has attempted to take advantage of multiple software project data repositories which are generally spread across the organization. In a large scale empirical study involving seven real-world datasets and seventeen learners, we show that, on average, combining the predictions of one learner trained on multiple datasets significantly improves the predictive performance compared to one learner induced on a single fit dataset. We also demonstrate empirically that combining multiple learners trained on a single training dataset does not significantly improve the average predictive accuracy compared to the use of a single learner induced on a single fit dataset.},
journal = {Software Quality Journal},
month = mar,
pages = {25–49},
numpages = {25},
keywords = {Software quality classification model, Multiple software metrics repositories, Multiple learners, Majority voting, Data mining, Cost of misclassification}
}

@inproceedings{10.1109/ACCT.2013.83,
author = {Gupta, Deepak and Goyal, Vinay Kr. and Mittal, Harish},
title = {Estimating of Software Quality with Clustering Techniques},
year = {2013},
isbn = {9780769549415},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ACCT.2013.83},
doi = {10.1109/ACCT.2013.83},
abstract = {Software faults are one of major criteria to estimate the software quality or the software reliability. There is number of matrices defined that uses the software faults to estimate the software quality. When we have a large software system with thousands of class modules, then it is not easy to apply the software matrices on each module of software system. The present work is the solution of the defined problem. This paper aims at comparing different models based on clustering techniques: k-means (KM), fuzzy c-means (FCM) and hierarchical agglomerative clustering (HAC) for building software quality estimation system. We propose quality measure of partition clustering technique (KM, FCM) in order to evaluate the results and we comparatively analyze the obtained results on two case studies. This paper focuses on clustering with very large datasets and very many attributes of different types.},
booktitle = {Proceedings of the 2013 Third International Conference on Advanced Computing &amp; Communication Technologies},
pages = {20–27},
numpages = {8},
keywords = {K-means, Hierarchical agglomerative., Fuzzy c-means, Clustering},
series = {ACCT '13}
}

@article{10.3233/JIFS-169703,
author = {Vig, Vidhi and Kaur, Arvinder and Tiwari, Shailesh and Trivedi, Munesh and Kohle, Mohan L.},
title = {Test effort estimation and prediction of traditional and rapid release models using machine learning algorithms},
year = {2018},
issue_date = {2018},
publisher = {IOS Press},
address = {NLD},
volume = {35},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169703},
doi = {10.3233/JIFS-169703},
abstract = {Recently, many software companies have shifted to shorter release cycles from the traditional multi-month release cycle. Evolution and transition of release cycles may affect the test effort in the system. This paper analyses 25 traditional releases containing 1210 classes and 69 rapid releases containing 2616 classes of four Open Source Java systems. Correlations between 48 Object Oriented metrics and 2 test metrics were evaluated to identify the best indicators of test effort. The results show that (i) correlation between OO and test metrics remain irrespective of release models, (ii) test effort required in Rapid Release (RR) models (shorter release cycles) is slightly more as compared to Traditional Release (TR) models, (iii) Out of 18 machine learning algorithms instance based machine learning algorithms IBK and K star followed by Multi-Layer Perceptron (MLP) and additive regression are able to predict the test effort accurately in classes.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1657–1669},
numpages = {13},
keywords = {test effort, software metrics, prediction, machine learning, Release cycles}
}

@article{10.1016/j.eswa.2010.09.136,
author = {Chiu, Nan-Hsing},
title = {Combining techniques for software quality classification: An integrated decision network approach},
year = {2011},
issue_date = {April, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.09.136},
doi = {10.1016/j.eswa.2010.09.136},
abstract = {Accurately predicting fault-prone modules is a major problem in quality control of a software system during software development. Selecting an appropriate suggestion from various software quality classification models is a difficult decision for software project managers. In this paper, an integrated decision network is proposed to combine the well-known software quality classification models in providing the summarized suggestion. A particle swarm optimization algorithm is used to search for suitable combinations among the software quality classification models in the integrated decision network. The experimental results show that the proposed integrated decision network outperforms the independent software quality classification models. It also provides an appropriate summary for decision makers.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {4618–4625},
numpages = {8},
keywords = {Software quality classification, Software project management, Particle swarm optimization, Integrated decision network, Decision support systems}
}

@inproceedings{10.1109/ICMA.2017.8015822,
author = {Gao, Qiang and Liu, Wenjie and Zhao, Xuewen and Li, Junfang and Yu, Xiao},
title = {Research and application of the distillation column process fault prediction based on the improved KPCA},
year = {2017},
isbn = {978-1-5090-6758-9},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICMA.2017.8015822},
doi = {10.1109/ICMA.2017.8015822},
abstract = {With the development of modern intelligent, automation and integration of the increasingly complex industrial process control system, the traditional prediction methods of faults perform not well, so it has faced a huge challenge. In this paper, a new improved kernel principal component analysis method is presented which uses the concept of indiscernibility and eigenvector applied to distillation column process fault prediction. Compared with traditional statistical techniques, improved KPCA not only can remove variables with little or no correlation with the fault, but also can reduce the amount of datas calculated by K. Applying this new method to distillation column process fault prediction, the simulation results show that the proposed methods have great advantages. Compared with the traditional KPCA, the improved KPCA improves the ability to predict the process failure caused by small disturbance and becomes more effective.},
booktitle = {2017 IEEE International Conference on Mechatronics and Automation (ICMA)},
pages = {247–251},
numpages = {5},
location = {Takamatsu, Japan}
}

@inproceedings{10.1145/3134600.3134620,
author = {Yan, Hua and Sui, Yulei and Chen, Shiping and Xue, Jingling},
title = {Machine-Learning-Guided Typestate Analysis for Static Use-After-Free Detection},
year = {2017},
isbn = {9781450353458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134600.3134620},
doi = {10.1145/3134600.3134620},
abstract = {Typestate analysis relies on pointer analysis for detecting temporal memory safety errors, such as use-after-free (UAF). For large programs, scalable pointer analysis is usually imprecise in analyzing their hard "corner cases", such as infeasible paths, recursion cycles, loops, arrays, and linked lists. Due to a sound over-approximation of the points-to information, a large number of spurious aliases will be reported conservatively, causing the corresponding typestate analysis to report a large number of false alarms. Thus, the usefulness of typestate analysis for heap-intensive clients, like UAF detection, becomes rather limited, in practice.We introduce Tac, a static UAF detector that bridges the gap between typestate and pointer analyses by machine learning. Tac learns the correlations between program features and UAF-related aliases by using a Support Vector Machine (SVM) and applies this knowledge to further disambiguate the UAF-related aliases reported imprecisely by the pointer analysis so that only the ones validated by its SVM classifier are further investigated by the typestate analysis. Despite its unsoundness, Tac represents a practical typestate analysis approach for UAF detection. We have implemented Tac in LLVM-3.8.0 and evaluated it using a set of eight open-source C/C++ programs. The results show that Tac is effective (in terms of finding 5 known CVE vulnerabilities, 1 known bug, and 8 new bugs with a low false alarm rate) and scalable (in terms of analyzing a large codebase with 2,098 KLOC in just over 4 hours).},
booktitle = {Proceedings of the 33rd Annual Computer Security Applications Conference},
pages = {42–54},
numpages = {13},
keywords = {vulnerability detection, use-after-free, static analysis, machine learning},
location = {Orlando, FL, USA},
series = {ACSAC '17}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {rule mining, product line, multi-objective search, machine learning, configuration},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1145/3330204.3330275,
author = {Luiz, Frederico Caram and de Oliveira Rodrigues, Bruno Rafael and Parreiras, Fernando Silva},
title = {Machine learning techniques for code smells detection: an empirical experiment on a highly imbalanced setup},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330275},
doi = {10.1145/3330204.3330275},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {65},
numpages = {8},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@article{10.1109/TKDE.2011.163,
author = {Bishnu, Partha S. and Bhattacherjee, Vandana},
title = {Software Fault Prediction Using Quad Tree-Based K-Means Clustering Algorithm},
year = {2012},
issue_date = {June 2012},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {24},
number = {6},
issn = {1041-4347},
url = {https://doi.org/10.1109/TKDE.2011.163},
doi = {10.1109/TKDE.2011.163},
abstract = {Unsupervised techniques like clustering may be used for fault prediction in software modules, more so in those cases where fault labels are not available. In this paper a Quad Tree-based K-Means algorithm has been applied for predicting faults in program modules. The aims of this paper are twofold. First, Quad Trees are applied for finding the initial cluster centers to be input to the K-Means Algorithm. An input threshold parameter delta governs the number of initial cluster centers and by varying delta the user can generate desired initial cluster centers. The concept of clustering gain has been used to determine the quality of clusters for evaluation of the Quad Tree-based initialization algorithm as compared to other initialization techniques. The clusters obtained by Quad Tree-based algorithm were found to have maximum gain values. Second, the Quad Tree-based algorithm is applied for predicting faults in program modules. The overall error rates of this prediction approach are compared to other existing algorithms and are found to be better in most of the cases.},
journal = {IEEE Trans. on Knowl. and Data Eng.},
month = jun,
pages = {1146–1150},
numpages = {5},
keywords = {software fault prediction., Quad Tree, K-Means clustering}
}

@inproceedings{10.1145/3472674.3473981,
author = {Pontillo, Valeria and Palomba, Fabio and Ferrucci, Filomena},
title = {Toward static test flakiness prediction: a feasibility study},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473981},
doi = {10.1145/3472674.3473981},
abstract = {Flaky tests are tests that exhibit both a passing and failing behavior when run against the same code. While the research community has attempted to define automated approaches for detecting and addressing test flakiness, most of them suffer from scalability issues and uncertainty as they require test cases to be run multiple times. This limitation has been recently targeted by means of machine learning solutions that could predict the flakiness of tests using a set of both static and dynamic metrics that would avoid the re-execution of tests. Recognizing the effort spent so far, this paper poses the first steps toward an orthogonal view of the problem, namely the classification of flaky tests using only statically computable software metrics. We propose a feasibility study on 72 projects of the iDFlakies dataset, and investigate the differences between flaky and non-flaky tests in terms of 25 test and production code metrics and smells. First, we statistically assess those differences. Second, we build a logistic regression model to verify the extent to which the differences observed are still significant when the metrics are considered together. The results show a relation between test flakiness and a number of test and production code factors, indicating the possibility to build classification approaches that exploit those factors to predict test flakiness.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {19–24},
numpages = {6},
keywords = {Software Quality Evaluation, Flaky Tests, Empirical Studies},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@inproceedings{10.1145/2593833.2593842,
author = {Malhotra, Ruchika},
title = {Search based techniques for software fault prediction: current trends and future directions},
year = {2014},
isbn = {9781450328524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593833.2593842},
doi = {10.1145/2593833.2593842},
abstract = {The effective allocation of the resources is crucial and essential in the testing phase of the software development life cycle so that the weak areas in the software can be verified and validated efficiently. The prediction of fault prone classes in the early phases of software development can help software developers to focus the limited available resources on those portions of software, which are more prone to fault. Recently, the search based techniques have been successfully applied in the software engineering domain. In this study, we analyze the position of search based techniques for use in software fault prediction by collecting relevant studies from the literature which were conducted during the period January 1991 to October 2013. We further summarize current trends by assessing the performance capability of the search based techniques in the existing research and suggest future directions.},
booktitle = {Proceedings of the 7th International Workshop on Search-Based Software Testing},
pages = {35–36},
numpages = {2},
keywords = {Software Quality, Software Fault proneness, Search Based Techniques},
location = {Hyderabad, India},
series = {SBST 2014}
}

@inproceedings{10.5555/1802408.1802423,
author = {Jiang, Yue and Lin, Jie and Cukic, Bojan and Menzies, Tim},
title = {Variance analysis in software fault prediction models},
year = {2009},
isbn = {9781424453757},
publisher = {IEEE Press},
abstract = {Software fault prediction models play an important role in software quality assurance. They identify software subsystems (modules, components, classes, or files) which are likely to contain faults. These subsystems, in turn, receive additional resources for verification and validation activities. Fault prediction models are binary classifiers typically developed using one of the supervised learning techniques from either a subset of the fault data from the current project or from a similar past project. In practice, it is critical that such models provide a reliable prediction performance on the data not used in training. Variance is an important reliability indicator of software fault prediction models. However, variance is often ignored or barely mentioned in many published studies. In this paper, through the analysis of twelve data sets from a public software engineering repository from the perspective of variance, we explore the following five questions regarding fault prediction models: (1) Do different types of classification performance measures exhibit different variance? (2) Does the size of the data set imply a more (or less) accurate prediction performance? (3) Does the size of training subset impact model's stability? (4) Do different classifiers consistently exhibit different performance in terms of model's variance? (5) Are there differences between variance from 1000 runs and 10 runs of 10-fold cross validation experiments? Our results indicate that variance is a very important factor in understanding fault prediction models and we recommend the best practice for reporting variance in empirical software engineering studies.},
booktitle = {Proceedings of the 20th IEEE International Conference on Software Reliability Engineering},
pages = {99–108},
numpages = {10},
location = {Bengaluru-Mysuru, India},
series = {ISSRE'09}
}

@inproceedings{10.1145/3036290.3036323,
author = {Badri, Mourad and Badri, Linda and Flageol, William and Toure, Fadel},
title = {Investigating the Accuracy of Test Code Size Prediction using Use Case Metrics and Machine Learning Algorithms: An Empirical Study},
year = {2017},
isbn = {9781450348287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3036290.3036323},
doi = {10.1145/3036290.3036323},
abstract = {Software testing plays a crucial role in software quality assurance. It is, however, a time and resource consuming process. It is, therefore, important to predict as soon as possible the effort required to test software, so that activities can be planned and resources can be optimally allocated. Test code size, in terms of Test Lines Of Code (TLOC), is an important testing effort indicator used in many empirical studies. In this paper, we investigate empirically the early prediction of TLOC for object-oriented software using use case metrics. We used different machine learning algorithms (linear regression, k-NN, Na\"{\i}ve Bayes, C4.5, Random Forest, and Multilayer Perceptron) to build the prediction models. We performed an empirical study using data collected from five Java projects. The use case metrics have been compared to the well-known Use Case Points (UCP) method. Results show that the use case metrics-based approach gives a more accurate prediction of TLOC than the UCP method.},
booktitle = {Proceedings of the 2017 International Conference on Machine Learning and Soft Computing},
pages = {25–33},
numpages = {9},
keywords = {Use Cases, Use Case Points, Testing Effort, Test Code Size, Software Testing, Prediction, Metrics, Machine Learning Algorithms, Cross Validation},
location = {Ho Chi Minh City, Vietnam},
series = {ICMLSC '17}
}

@inproceedings{10.1007/978-3-319-91238-7_10,
author = {Katsanos, Christos and Xenos, Michalis and Tselios, Nikolaos},
title = {Tool-Mediated HCI Modeling Instruction in a Campus-Based Software Quality Course},
year = {2018},
isbn = {978-3-319-91237-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91238-7_10},
doi = {10.1007/978-3-319-91238-7_10},
abstract = {The Keystroke Level Model (KLM) and Fitts’ Law constitute core teaching subjects in most HCI courses, as well as many courses on software design and evaluation. The KLM Form Analyzer (KLM-FA) has been introduced as a practitioner’s tool to facilitate web form design and evaluation, based on these established HCI predictive models. It was also hypothesized that KLM-FA can also be used for educational purposes, since it provides step-by-step tracing of the KLM modeling for any web form filling task, according to various interaction strategies or users’ characteristics. In our previous work, we found that KLM-FA supports teaching and learning of HCI modeling in the context of distance education. This paper reports a study investigating the learning effectiveness of KLM-FA in the context of campus-based higher education. Students of a software quality course completed a knowledge test after the lecture-based instruction (pre-test condition) and after being involved in a KLM-FA mediated learning activity (post-test condition). They also provided post-test ratings for their educational experience and the tool’s usability. Results showed that KLM-FA can significantly improve learning of the HCI modeling. In addition, participating students rated their perceived educational experience as very satisfactory and the perceived usability of KLM-FA as good to excellent.},
booktitle = {Human-Computer Interaction. Theories, Methods, and Human Issues: 20th International Conference, HCI International 2018, Las Vegas, NV, USA, July 15–20, 2018, Proceedings, Part I},
pages = {114–125},
numpages = {12},
keywords = {Fitts’ law, Keystroke level model, Web form design, Learning activity, Educational tool, Project-based learning},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1145/2365324.2365337,
author = {Hampp, Tilmann},
title = {A cost-benefit model for software quality assurance activities},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365337},
doi = {10.1145/2365324.2365337},
abstract = {Software project managers must schedule quality assurance activities. This is difficult because not enough information is available. Therefore, we developed and validated the quantitative model CoBe. It is based on detailed relationships and is quantified with historical data. It allows to decide which reviews and tests have to be conducted, how they are conducted, and how corrected defects are retested. The results are costs and benefits for quality assurance activities during development and after delivery. Results are given in terms of effort, time, and staff. They are summed up and weighted financially so that an optimal trade-off between costs and benefits can be found. The model is validated with real-world data: Detailed relationships and the complete model are validated with data from 21 student projects. A sensitivity analysis was conducted. CoBe was also validated with data of two industry projects. Overall, the model results are sufficiently accurate. But a calibration is necessary for applying the model in a specific environment. For this, only a few parameters must be set. Their values can be obtained from data that is available frequently from past projects.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {99–108},
numpages = {10},
keywords = {cost-benefit model, software quality assurance},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@inproceedings{10.1109/ISSRE.2009.13,
author = {Jiang, Yue and Lin, Jie and Cukic, Bojan and Menzies, Tim},
title = {Variance Analysis in Software Fault Prediction Models},
year = {2009},
isbn = {9780769538785},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISSRE.2009.13},
doi = {10.1109/ISSRE.2009.13},
abstract = {Software fault prediction models play an important role in softwarequality assurance. They identify software subsystems (modules,components, classes, or files) which are likely to contain faults.These subsystems, in turn, receive additional resources forverification and validation activities.Fault prediction models arebinary classifiers typically developed using one of the supervisedlearning techniques from either a subset of the fault data from thecurrent project or from a similar past project.In practice, itis critical that such models provide a reliable predictionperformance on the data not used in training.Variance is animportant reliability indicator of software fault prediction models.However, variance is often ignored or barely mentioned in manypublished studies. In this paper, through the analysis of twelvedata sets from a public software engineering repository from theperspective of variance, we explore the following five questionsregarding fault prediction models:(1) Do different types ofclassification performance measures exhibit different variance? (2)Does the size of the data set imply a more (or less) accurateprediction performance? (3) Does the size of training subset impactmodel's stability? (4) Do different classifiers consistently exhibitdifferent performance in terms of model's variance? (5) Are theredifferences between variance from 1000 runs and 10 runs of 10-fold crossvalidation experiments?Our results indicate that variance is avery important factor in understanding fault prediction models andwe recommend the best practice for reporting variance in empiricalsoftware engineering studies.},
booktitle = {Proceedings of the 2009 20th International Symposium on Software Reliability Engineering},
pages = {99–108},
numpages = {10},
keywords = {variance, machine learning, fault prediction models},
series = {ISSRE '09}
}

@inproceedings{10.1007/978-3-030-38085-4_19,
author = {Christodoulopoulos, Konstantinos and Sartzetakis, Ippokratis and Soumplis, Polizois and Varvarigos, Emmanouel (Manos)},
title = {Machine Learning Assisted Quality of Transmission Estimation and Planning with Reduced Margins},
year = {2019},
isbn = {978-3-030-38084-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38085-4_19},
doi = {10.1007/978-3-030-38085-4_19},
abstract = {In optical transport networks, the Quality of Transmission (QoT) using a physical layer model (PLM) is estimated before establishing new or reconfiguring established optical connections. Traditionally, high margins are added to account for the model’s inaccuracy and the uncertainty in the current and evolving physical layer conditions, targeting uninterrupted operation for several years, until the end-of-life (EOL). Reducing the margins increases network efficiency but requires accurate QoT estimation. We present two machine learning (ML) assisted QoT estimators that leverage monitoring data of existing connections to understand the actual physical layer conditions and achieve high estimation accuracy. We then quantify the benefits of planning/upgrading a network over multiple periods with accurate QoT estimation as opposed to planning with EOL margins.},
booktitle = {Optical Network Design and Modeling: 23rd IFIP WG 6.10 International Conference, ONDM 2019, Athens, Greece, May 13–16, 2019, Proceedings},
pages = {211–222},
numpages = {12},
keywords = {Overprovisioning, Static network planning, End-of-life margins, Physical layer impairments, Monitoring, Cross-layer optimization, Incremental multi-period planning, Marginless},
location = {Athens, Greece}
}

@article{10.1016/j.ins.2013.04.027,
author = {Pizzi, Nick J.},
title = {A fuzzy classifier approach to estimating software quality},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {241},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2013.04.027},
doi = {10.1016/j.ins.2013.04.027},
abstract = {With the increasing sophistication of today's software systems, it is often difficult to estimate the overall quality of underlying software components with respect to attributes such as complexity, utility, and extensibility. Many metrics exist in the software engineering literature that attempt to quantify, with varying levels of accuracy, a large swath of qualitative attributes. However, the overall quality of a software object may manifest itself in ways that the simple interpretation of metrics fails to identify. A better strategy is to determine the best, possibly non-linear, subset of many software metrics for accurately estimating software quality. This strategy may be couched in terms of a problem of classification, that is, determine a mapping from a set of software metrics to a set of class labels representing software quality. We implement this strategy using a fuzzy classification approach. The software metrics are automatically computed and presented as features (input) to a classifier, while the class labels (output) are assigned via an expert's (software architect) thorough assessment of the quality of individual software objects. A large collection of classifiers is presented with subsets of the software metric features. Subsets are selected stochastically using a fuzzy logic based sampling method. The classifiers then predict the quality, specifically the class label, of each software object. Fuzzy integration is applied to the results from the most accurate individual classifiers. We empirically evaluate this approach using software objects from a sophisticated algorithm development framework used to develop biomedical data analysis systems. We demonstrate that the sampling method attenuates the effects of confounding features, and the aggregated classification results using fuzzy integration are superior to the predictions from the respective best individual classifiers.},
journal = {Inf. Sci.},
month = aug,
pages = {1–11},
numpages = {11},
keywords = {Software metric, Software engineering, Pattern classification, Fuzzy logic, Computational intelligence}
}

@inproceedings{10.1007/978-3-540-73451-2_66,
author = {Ramanna, Sheela and Bhatt, Rajen and Biernot, Piotr},
title = {Software Defect Classification: A Comparative Study with Rough Hybrid Approaches},
year = {2007},
isbn = {9783540734505},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-73451-2_66},
doi = {10.1007/978-3-540-73451-2_66},
abstract = {This paper is an extension of our earlier work in combining strengths of rough set theory and neuro-fuzzy decision trees in classifying software defect data. The extension includes the application of a rough-fuzzy classification trees to classifying defects. We compare classification results for five methods: rough sets, neuro-fuzzy decision trees, partial decision trees, rough-neuro-fuzzy decision trees and rough-fuzzy classification trees. The analysis of the results include a paired t-test for accuracy and number of rules. The results demonstrate that there is improvement in classification accuracy with the rough fuzzy classification trees with a minimal set of rules. The contribution of this paper is a comparative study of several hybrid approaches in classifying software defect data.},
booktitle = {Proceedings of the International Conference on Rough Sets and Intelligent Systems Paradigms},
pages = {630–638},
numpages = {9},
keywords = {software defects, rough-fuzzy classification trees, rough sets, neuro-fuzzy decision trees, Classification},
location = {Warsaw, Poland},
series = {RSEISP '07}
}

@article{10.1109/TSE.2011.103,
author = {Hall, Tracy and Beecham, Sarah and Bowes, David and Gray, David and Counsell, Steve},
title = {A Systematic Literature Review on Fault Prediction Performance in Software Engineering},
year = {2012},
issue_date = {November 2012},
publisher = {IEEE Press},
volume = {38},
number = {6},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2011.103},
doi = {10.1109/TSE.2011.103},
abstract = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.},
journal = {IEEE Trans. Softw. Eng.},
month = nov,
pages = {1276–1304},
numpages = {29},
keywords = {software fault prediction, Systematics, Systematic literature review, Software testing, Predictive models, Fault diagnosis, Data models, Context modeling, Analytical models}
}

@inproceedings{10.1145/2979779.2979811,
author = {Kaur, Ishleen and Bajpai, Neha},
title = {An Empirical Study on Fault Prediction using Token-Based Approach},
year = {2016},
isbn = {9781450342131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2979779.2979811},
doi = {10.1145/2979779.2979811},
abstract = {Since exhaustive testing is not possible, prediction of fault prone modules can be used for prioritizing the components of a software system. Various approaches have been proposed for the prediction of fault prone modules. Most of them uses module metrics as quality estimators. In this study, we proposed a tokenbased approach and combine the metric evaluated from our approach with the module metrics to further improve the prediction results. We conducted the experiment on an open source project for evaluating the approach. The proposed approach is further compared with the existing fault prone filtering technique. The results show that the proposed approach is an improvement over fault prone filtering technique.},
booktitle = {Proceedings of the International Conference on Advances in Information Communication Technology &amp; Computing},
articleno = {32},
numpages = {7},
keywords = {software testing, software metrics, logistic regression, fault prone modules, fault, Classification},
location = {Bikaner, India},
series = {AICTC '16}
}

@inproceedings{10.1145/2365324.2365325,
author = {Kim, Sunghun},
title = {Defect, defect, defect: defect prediction 2.0},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365325},
doi = {10.1145/2365324.2365325},
abstract = {Defect prediction has been a very active research area in software engineering [6--8, 11, 13, 16, 19, 20].In 1971, Akiyama proposed one of the earliest defect prediction models using Lines of Code (LOC) [1]: "Defect = 4.86 + 0.018LOC."Since then, many effective new defect prediction models and metrics have been proposed. For the prediction models, typical machine learners and regression algorithms such as Naive Bayes, Decision Tree, and Linear Regression are widely used. On the other hand, Kim et al. proposed a cache-based prediction model using bug occurrence properties [9]. Hassan proposed a change entropy model to effectively predict defects [6]. Recently, Bettenburg et al. proposed Multivariate Adaptive Regression Splines to improve defect prediction models by learning from local and global properties together [4].Besides LOC, many new effective metrics for defect prediction have been proposed. Among them, source code metrics and change history metrics are widely used and yield reasonable defect prediction accuracy. For example, Basili et al. [3] used Chidamber and Kemerer metrics, and Ohlsson et al. [14] used McCabe's cyclomatic complexity for defect prediction. Moser et al. [12] used the number of revisions, authors, and past fixes, and age of a file as defect predictors. Recently, micro interaction metrics (MIMs) [10] and source code quality measures [15] for effective defect prediction are proposed.However, there is much room to improve for defect prediction 2.0. First of all, understanding the actual causes of defects is necessary. Without understanding them, we may reach to nonsensical conclusions from defect prediction results [18]. Many effective prediction models have been proposed, but successful application cases in practice are scarcely reported. To be more attractive for developers in practice, it is desirable to predict defects in finer granularity levels such as the code line or even keyword level. Note that static bug finders such as FindBugs [2] can identify potential bugs in the line level, and many developers find them useful in practice. Dealing with noise in defect data has become an important issue. Bird et al. identified there is non-neglectable noise in defect data [5]. This noise may yield poor and/or meaningless defect prediction results. Cross-prediction is highly desirable: for new projects or projects with limited training data, it is necessary to learn a prediction model using sufficient training data from other projects, and to apply the model to those projects. However, Zimmermann et al. [21] identified cross-project prediction is a challenging problem. Turhan et al. [17] analyzed Cross-Company (CC) and Within-Company (WC) data for defect prediction, and confirmed that it is challenging to reuse CC data directly to predict defects in other companies' software.Overall, defect prediction is a very interesting and promising research area. However, there are still many research challenges and problems to be addressed. Hopefully, this discussion calls new solutions and ideas to address these challenges.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {1–2},
numpages = {2},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@inproceedings{10.1145/3377713.3377753,
author = {Lu, Qiwei and Cheng, Jinpei and Guo, Dianlin and Su, Mengmeng and Wu, Xuewei and Ru, Tao},
title = {Binary Classification Model Based on Machine Learning Algorithm for the Short-Circuit Detection in Power System},
year = {2020},
isbn = {9781450372619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377713.3377753},
doi = {10.1145/3377713.3377753},
abstract = {Short circuit faults usually occur in the damaged insulation lines or line connections, which will cause serious accidents such as fires and explosions. As the power supply distance increases, accuracy of short-circuit fault detection is insufficient and the process is tedious with the traditional analysis method. In order to solve the problems above, the short-circuit fault detection is classified into the two classification problems while the machine learning method is used. The data of the normal state and short circuit fault state are obtained by the short-circuit simulation experiment. Extract four features from time domain, including the average current and so on. By training support vector machine (SVM) using the different combinations of extraction features above, the model is obtained. The accuracy of classification of the test data set by the model is high. The results show that the short-circuit fault detection method based on machine learning is more accurate and robust than traditional analysis methods.},
booktitle = {Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {271–275},
numpages = {5},
keywords = {normalization, feature extraction, feature analysis, Short-circuit detection},
location = {Sanya, China},
series = {ACAI '19}
}

@inproceedings{10.5555/1193212.1193789,
author = {Mertik, Matej and Lenic, Mitja and Stiglic, Gregor and Kokol, Peter},
title = {Estimating Software Quality with Advanced Data Mining Techniques},
year = {2006},
isbn = {0769527035},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Current software quality estimation models often involve the use of supervised learning methods for building a software fault prediction models. In such models, dependent variable usually represents a software quality measurement indicating the quality of a module by risk-basked class membership, or the number of faults. Independent variables include various software metrics as McCabe, Error Count, Halstead, Line of Code, etc... In this paper we present the use of advanced tool for data mining called Multimethod on the case of building software fault prediction model. Multimethod combines different aspects of supervised learning methods in dynamical environment and therefore can improve accuracy of generated prediction model. We demonstrate the use Multimethod tool on the real data from the Metrics Data Project Data (MDP) Repository. Our preliminary empirical results show promising potentials of this approach in predicting software quality in a software measurement and quality dataset.},
booktitle = {Proceedings of the International Conference on Software Engineering Advances},
pages = {19},
keywords = {Supervised learning, Software quality, Software fault prediction models, Multimethod data mining},
series = {ICSEA '06}
}

@article{10.1145/2020976.2020991,
author = {Malhotra, Ruchika and Jain, Ankita},
title = {Software fault prediction for object oriented systems: a literature review},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2020976.2020991},
doi = {10.1145/2020976.2020991},
abstract = {There always has been a demand to produce efficient and high quality software. There are various object oriented metrics that measure various properties of the software like coupling, cohesion, inheritance etc. which affect the software to a large extent. These metrics can be used in predicting important quality attributes such as fault proneness, maintainability, effort, productivity and reliability. Early prediction of fault proneness will help us to focus on testing resources and use them only on the classes which are predicted to be fault-prone. Thus, this will help in early phases of software development to give a measurement of quality assessment.This paper provides the review of the previous studies which are related to software metrics and the fault proneness. In other words, it reviews several journals and conference papers on software fault prediction. There is large number of software metrics proposed in the literature. Each study uses a different subset of these metrics and performs the analysis using different datasets. Also, the researchers have used different approaches such as Support vector machines, naive bayes network, random forest, artificial neural network, decision tree, logistic regression etc. Thus, this study focuses on the metrics used, dataset used and the evaluation or analysis method used by various authors. This review will be beneficial for the future studies as various researchers and practitioners can use it for comparative analysis.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–6},
numpages = {6},
keywords = {software quality, object oriented, metrics, fault proneness, empirical validation}
}

@inproceedings{10.1109/ICSE43902.2021.00109,
author = {Bui, Nghi D. Q. and Yu, Yijun and Jiang, Lingxiao},
title = {InferCode: Self-Supervised Learning of Code Representations by Predicting Subtrees},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00109},
doi = {10.1109/ICSE43902.2021.00109},
abstract = {Learning code representations has found many uses in software engineering, such as code classification, code search, comment generation, and bug prediction, etc. Although representations of code in tokens, syntax trees, dependency graphs, paths in trees, or the combinations of their variants have been proposed, existing learning techniques have a major limitation that these models are often trained on datasets labeled for specific downstream tasks, and as such the code representations may not be suitable for other tasks. Even though some techniques generate representations from unlabeled code, they are far from being satisfactory when applied to the downstream tasks. To overcome the limitation, this paper proposes InferCode, which adapts the self-supervised learning idea from natural language processing to the abstract syntax trees (ASTs) of code. The novelty lies in the training of code representations by predicting subtrees automatically identified from the contexts of ASTs. With InferCode, subtrees in ASTs are treated as the labels for training the code representations without any human labelling effort or the overhead of expensive graph construction, and the trained representations are no longer tied to any specific downstream tasks or code units.We have trained an instance of InferCode model using Tree-Based Convolutional Neural Network (TBCNN) as the encoder of a large set of Java code. This pre-trained model can then be applied to downstream unsupervised tasks such as code clustering, code clone detection, cross-language code search, or be reused under a transfer learning scheme to continue training the model weights for supervised tasks such as code classification and method name prediction. Compared to prior techniques applied to the same downstream tasks, such as code2vec, code2seq, ASTNN, using our pre-trained InferCode model higher performance is achieved with a significant margin for most of the tasks, including those involving different programming languages. The implementation of InferCode and the trained embeddings are available at the link: https://github.com/bdqnghi/infercode.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1186–1197},
numpages = {12},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3293882.3330580,
author = {Cordy, Maxime and Muller, Steve and Papadakis, Mike and Le Traon, Yves},
title = {Search-based test and improvement of machine-learning-based anomaly detection systems},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330580},
doi = {10.1145/3293882.3330580},
abstract = {Machine-learning-based anomaly detection systems can be vulnerable to new kinds of deceptions, known as training attacks, which exploit the live learning mechanism of these systems by progressively injecting small portions of abnormal data. The injected data seamlessly swift the learned states to a point where harmful data can pass unnoticed. We focus on the systematic testing of these attacks in the context of intrusion detection systems (IDS). We propose a search-based approach to test IDS by making training attacks. Going a step further, we also propose searching for countermeasures, learning from the successful attacks and thereby increasing the resilience of the tested IDS. We evaluate our approach on a denial-of-service attack detection scenario and a dataset recording the network traffic of a real-world system. Our experiments show that our search-based attack scheme generates successful attacks bypassing the current state-of-the-art defences. We also show that our approach is capable of generating attack patterns for all configuration states of the studied IDS and that it is capable of providing appropriate countermeasures. By co-evolving our attack and defence mechanisms we succeeded at improving the defence of the IDS under test by making it resilient to 49 out of 50 independently generated attacks.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {158–168},
numpages = {11},
keywords = {software testing, intrusion detection systems, clustering},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1145/3302333.3302338,
author = {Amand, Benoit and Cordy, Maxime and Heymans, Patrick and Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards Learning-Aided Configuration in 3D Printing: Feasibility Study and Application to Defect Prediction},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302338},
doi = {10.1145/3302333.3302338},
abstract = {Configurators rely on logical constraints over parameters to aid users and determine the validity of a configuration. However, for some domains, capturing such configuration knowledge is hard, if not infeasible. This is the case in the 3D printing industry, where parametric 3D object models contain the list of parameters and their value domains, but no explicit constraints. This calls for a complementary approach that learns what configurations are valid based on previous experiences. In this paper, we report on preliminary experiments showing the capability of state-of-the-art classification algorithms to assist the configuration process. While machine learning holds its promises when it comes to evaluation scores, an in-depth analysis reveals the opportunity to combine the classifiers with constraint solvers.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {7},
numpages = {9},
keywords = {Sampling, Machine Learning, Configuration, 3D printing},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1007/s10664-014-9308-x,
author = {Khomh, Foutse and Adams, Bram and Dhaliwal, Tejinder and Zou, Ying},
title = {Understanding the impact of rapid releases on software quality},
year = {2015},
issue_date = {April     2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9308-x},
doi = {10.1007/s10664-014-9308-x},
abstract = {Many software companies are shifting from the traditional multi-month release cycle to shorter release cycles. For example, Google Chrome and Mozilla Firefox release new versions every 6 weeks. These shorter release cycles reduce the users' waiting time for a new release and offer better feedback and marketing opportunities to companies, but it is unclear if the quality of the software product improves as well, since developers and testers are under more pressure. In this paper, we extend our previous empirical study of Mozilla Firefox on the impact of rapid releases on quality assurance with feedback by Mozilla project members. The study compares crash rates, median uptime, and the proportion of pre- and post-release bugs in traditional releases with those in rapid releases, and we also analyze the source code changes made by developers to identify potential changes in the development process. We found that (1) with shorter release cycles, users do not experience significantly more pre- or post-release bugs (percentage-wise) and (2) bugs are fixed faster, yet (3) users experience these bugs earlier during software execution (the program crashes earlier). Increased integration activity and propagation of harder bugs to later versions account for some of these findings. Overall, our case study suggests that a clear release engineering process with thorough automation is one of the major challenges when switching to rapid releases.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {336–373},
numpages = {38},
keywords = {Testing, Software release, Software quality, Release cycle, Bugs}
}

@inproceedings{10.1145/1083165.1083173,
author = {Boetticher, Gary D.},
title = {Nearest neighbor sampling for better defect prediction},
year = {2005},
isbn = {1595931252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083165.1083173},
doi = {10.1145/1083165.1083173},
abstract = {An important step in building effective predictive models applies one or more sampling techniques. Traditional sampling techniques include random, stratified, systemic, and clustered. The problem with these techniques is that they focus on the class attribute, rather than the non-class attributes. For example, if a test instance's nearest neighbor is from the opposite class of the training set, then it seems doomed to misclassification. To illustrate this problem, this paper conducts 20 experiments on five different NASA defect datasets (CM1, JM1, KC1, KC2, PC1) using two different learners (J48 and Na\"{\i}ve Bayes). Each data set is divided into 3 groups, a training set, and "nice/nasty" neighbor test sets. Using a nearest neighbor approach, "Nice neighbors" consist of those test instances closest to class training instances. "Nasty neighbors" are closest to opposite class training instances. The "Nice" experiments average 94 percent accuracy and the "Nasty" experiments average 20 percent accuracy. Based on these results a new nearest neighbor sampling technique is proposed.},
booktitle = {Proceedings of the 2005 Workshop on Predictor Models in Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {NASA data repository, decision trees, defect prediction, empirical software engineering, nearest neighbor analysis},
location = {St. Louis, Missouri},
series = {PROMISE '05}
}

@article{10.1016/j.infsof.2009.10.003,
author = {Hassouna, Alaa and Tahvildari, Ladan},
title = {An effort prediction framework for software defect correction},
year = {2010},
issue_date = {February, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.10.003},
doi = {10.1016/j.infsof.2009.10.003},
abstract = {This article tackles the problem of predicting effort (in person-hours) required to fix a software defect posted on an Issue Tracking System. The proposed method is inspired by the Nearest Neighbour Approach presented by the pioneering work of Weiss et al. (2007) [1]. We propose four enhancements to Weiss et al. (2007) [1]: Data Enrichment, Majority Voting, Adaptive Threshold and Binary Clustering. Data Enrichment infuses additional issue information into the similarity-scoring procedure, aiming to increase the accuracy of similarity scores. Majority Voting exploits the fact that many of the similar historical issues have repeating effort values, which are close to the actual. Adaptive Threshold automatically adjusts the similarity threshold to ensure that we obtain only the most similar matches. We use Binary Clustering if the similarity scores are very low, which might result in misleading predictions. This uses common properties of issues to form clusters (independent of the similarity scores) which are then used to produce the predictions. Numerical results are presented showing a noticeable improvement over the method proposed in Weiss et al. (2007) [1].},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {197–209},
numpages = {13},
keywords = {Software effort prediction, Software defect correction, Issue tracking system, Clustering, Case-based reasoning}
}

@inproceedings{10.5555/1888258.1888293,
author = {Eichinger, Frank and Krogmann, Klaus and Klug, Roland and B\"{o}hm, Klemens},
title = {Software-defect localisation by mining dataflow-enabled call graphs},
year = {2010},
isbn = {364215879X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Defect localisation is essential in software engineering and is an important task in domain-specific data mining. Existing techniques building on call-graph mining can localise different kinds of defects. However, these techniques focus on defects that affect the controlflow and are agnostic regarding the dataflow. In this paper, we introduce dataflowenabled call graphs that incorporate abstractions of the dataflow. Building on these graphs, we present an approach for defect localisation. The creation of the graphs and the defect localisation are essentially data mining problems, making use of discretisation, frequent subgraph mining and feature selection. We demonstrate the defect-localisation qualities of our approach with a study on defects introduced into Weka. As a result, defect localisation now works much better, and a developer has to investigate on average only 1.5 out of 30 methods to fix a defect.},
booktitle = {Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: Part I},
pages = {425–441},
numpages = {17},
location = {Barcelona, Spain},
series = {ECML PKDD'10}
}

@inproceedings{10.1007/978-3-540-72530-5_9,
author = {Ramanna, Sheela and Bhatt, Rajen and Biernot, Piotr},
title = {A Rough-Hybrid Approach to Software Defect Classification},
year = {2009},
isbn = {9783540725299},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-72530-5_9},
doi = {10.1007/978-3-540-72530-5_9},
abstract = {Knowledge discovery methods used to find relationships among software engineering data and the extraction of rules have gained increasing importance in recent years. These methods have become necessary for improvements in the quality of the software product and the process. The focus of this paper is a first attempt towards combining strengths of rough set theory and neuro-fuzzy decision trees in classifying software defect data. We compare classification results for four methods: rough sets, neuro-fuzzy decision trees, partial decision trees, rough-neuro-fuzzy decision trees. The analysis of the results include a family-wise 10 fold paired t-test for accuracy and number of rules. The contribution of this paper is the application of a hybrid rough-neuro-fuzzy decision tree method in classifying software defect data.},
booktitle = {Proceedings of the 11th International Conference on Rough Sets, Fuzzy Sets, Data Mining and Granular Computing},
pages = {79–86},
numpages = {8},
keywords = {Software Defects, Rough Sets, Neuro-Fuzzy-Decision Trees, Classification},
location = {Toronto, Ontario, Canada},
series = {RSFDGrC '07}
}

@inproceedings{10.1145/2499393.2499402,
author = {Walkinshaw, Neil},
title = {Using evidential reasoning to make qualified predictions of software quality},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499402},
doi = {10.1145/2499393.2499402},
abstract = {Software quality is commonly characterised in a top-down manner. High-level notions such as quality are decomposed into hierarchies of sub-factors, ranging from abstract notions such as maintainability and reliability to lower-level notions such as test coverage or team-size. Assessments of abstract factors are derived from relevant sources of information about their respective lower-level sub-factors, by surveying sources such as metrics data and inspection reports. This can be difficult because (1) evidence might not be available, (2) interpretations of the data with respect to certain quality factors may be subject to doubt and intuition, and (3) there is no straightforward means of blending hierarchies of heterogeneous data into a single coherent and quantitative prediction of quality. This paper shows how Evidential Reasoning (ER) - a mathematical technique for reasoning about uncertainty and evidence - can address this problem. It enables the quality assessment to proceed in a bottom-up manner, by the provision of low-level assessments that make any uncertainty explicit, and automatically propagating these up to higher-level 'belief-functions' that accurately summarise the developer's opinion and make explicit any doubt or ignorance.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {2},
numpages = {10},
keywords = {estimation, evidential reasoning, quality models},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@inproceedings{10.1145/2463676.2465338,
author = {Condie, Tyson and Mineiro, Paul and Polyzotis, Neoklis and Weimer, Markus},
title = {Machine learning for big data},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465338},
doi = {10.1145/2463676.2465338},
abstract = {Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities.The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {939–942},
numpages = {4},
keywords = {machine learning, databases, big data},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@article{10.1109/MS.2005.149,
author = {Koru, A. Gunes and Liu, Hongfang},
title = {Building Defect Prediction Models in Practice},
year = {2005},
issue_date = {November 2005},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {22},
number = {6},
issn = {0740-7459},
url = {https://doi.org/10.1109/MS.2005.149},
doi = {10.1109/MS.2005.149},
abstract = {Predicting defect-prone modules successfully can help software developers improve product quality by focusing quality assurance activities on those modules. We built several machine-learning models to predict the defective modules in five software products developed by NASA, named, CM1, JM1, KC1, KC2, and PC1. Using a set of static measures as predictor variables, the models failed to predict performance satisfactorily on the products' original data sets. However, these data sets used the smallest unit of functionality--that is, a function or method as a module. This meant the defect prediction was performed at a fine granularity level. Stratifying the original data sets according to module size showed the prediction performance to be better in subsets that included larger modules. Aggregating the method-level KC1 data to class level improved prediction performance for the top defect classes. Guidelines based on these results help software developers build effective defect-prediction models for focused quality assurance activities.This article is part of a special issue on predictor modeling.},
journal = {IEEE Softw.},
month = nov,
pages = {23–29},
numpages = {7},
keywords = {software quality, software metrics}
}

@article{10.1007/s00500-019-04047-7,
author = {Sudharson, D. and Prabha, D.},
title = {RETRACTED ARTICLE: A novel machine learning
approach for software reliability growth modelling with pareto distribution
function},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {18},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-04047-7},
doi = {10.1007/s00500-019-04047-7},
abstract = {Software reliability is the important quantifiable attribute in gaining
reliability by assessing faults at the time of testing in the software products.
Time-based software reliability models used to identify the defects in the product,
and it is not suitable for dynamic situations. Instead of time, test effect is used
in few explorations through effort function and it is not realistic for infinite
testing time. Identifying number of defects is essential in software reliability
models, and this research work presents a Pareto distribution (PD) to predict the
fault distribution of software under homogenous and nonhomogeneous conditions along
with artificial neural network (ANN). This methodology enables the parallel
evolution of a product through NN models which exhibit estimated Pareto optimality
with respect to multiple error measures. The proposed PD-ANN-based SRGM describes
types of failure data and also improves the accuracy of parameter estimation more
than existing growth models such as homogeneous poison process and two fuzzy time
series-based software reliability models. Experimental evidence is presented for
general application and the proposed framework by generating solutions for different
product and developer indexes.},
journal = {Soft Comput.},
month = sep,
pages = {8379–8387},
numpages = {9},
keywords = {Software reliability, Artificial neural networks, Pareto distribution, Distribution parameter estimation}
}

@phdthesis{10.5555/2519129,
author = {Xu, Jie},
title = {Empirical analysis of the procedure for deriving software defect estimation models},
year = {2011},
isbn = {9780494895252},
publisher = {University of Western Ontario},
address = {CAN},
abstract = {Software has become ubiquitous in our daily lives, and software with trusted quality has been critical in many domains. As a result, software quality assurance has been a heated topic for several decades. More precise quality management can be accomplished if we can allocate resources based on accurate quality estimation in the early stages of a project. Many software metrics and software quality estimation models have been proposed to enhance software quality and they have shown promise, but their usefulness was controversial and none of them was widely accepted. Moreover, there is no practical guideline for building software quality estimation models. In this dissertation, a general procedure is proposed to derive software defect estimation models and various techniques are presented to accomplish the tasks in individual steps. The description of the six-step procedure is clearly stated and thorough. The first step is to review the literature review in order to identify the estimation target and related problems. Next, data preparation is done to collect suitable data for the modeling. Then statistical techniques together with machine learning methods are used to validate the effectiveness of software metrics. After that, statistical modeling can be performed to obtain the regression formula. Moreover, a neuro-fuzzy recalibration approach is adopted to improve the accuracy of the statistical model. Lastly, the evaluation of the derived model can be made. This six-step procedure is carried out based on data from both the ISBSG repository and OSS projects to present its practical value. The results demonstrate the significance of the procedure in building software quality estimation models. The proposed procedure is applicable to build defect estimation models from scratch or improve performance of current algorithmic estimation models. Moreover, it can be used in other areas of software estimation such as software cost estimation.   Keywords:  software quality, quality estimation, statistical techniques, soft computing, neuro-fuzzy approach},
note = {AAINR89525}
}

@article{10.1016/j.eswa.2010.09.144,
author = {Si, Xiao-Sheng and Hu, Chang-Hua and Yang, Jian-Bo and Zhang, Qi},
title = {On the dynamic evidential reasoning algorithm for fault prediction},
year = {2011},
issue_date = {May, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {5},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.09.144},
doi = {10.1016/j.eswa.2010.09.144},
abstract = {Research highlights A dynamic evidential reasoning algorithm is presented for dynamic fusion. A fault prognosis model is established based on the dynamic evidential reasoning algorithm. The optimization models are presented for estimating the parameters of the prognosis model. The developed model has been validated by case studies. In this paper, a new fault prediction model is presented to deal with the fault prediction problems in the presence of both quantitative and qualitative data based on the dynamic evidential reasoning (DER) approach. In engineering practice, system performance is constantly changed with time. As such, there is a need to develop a supporting mechanism that can be used to conduct dynamic fusion with time, and establish a prediction model to trace and predict system performance. In this paper, a DER approach is first developed to realize dynamic fusion. The new approach takes account of time effect by introducing belief decaying factor, which reflects the nature that evidence credibility is decreasing over time. Theoretically, it is show that the new DER aggregation schemes also satisfy the synthesis theorems. Then a fault prediction model based on the DER approach is established and several optimization models are developed for locally training the DER prediction model. The main feature of these optimization models is that only partial input and output information is required, which can be either incomplete or vague, either numerical or judgmental, or mixed. The models can be used to fine tune the DER prediction model whose initial parameters are decided by expert's knowledge or common sense. Finally, two numerical examples are provided to illustrate the detailed implementation procedures of the proposed approach and demonstrate its potential applications in fault prediction.},
journal = {Expert Syst. Appl.},
month = may,
pages = {5061–5080},
numpages = {20},
keywords = {Utility, Nonlinear programming, Fault prediction, Dynamic evidential reasoning approach, Artificial intelligence}
}

@article{10.1016/j.eswa.2010.08.022,
author = {Catal, Cagatay and Sevim, Ugur and Diri, Banu},
title = {Practical development of an Eclipse-based software fault prediction tool using Naive Bayes algorithm},
year = {2011},
issue_date = {March, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.08.022},
doi = {10.1016/j.eswa.2010.08.022},
abstract = {Despite the amount of effort software engineers have been putting into developing fault prediction models, software fault prediction still poses great challenges. This research using machine learning and statistical techniques has been ongoing for 15years, and yet we still have not had a breakthrough. Unfortunately, none of these prediction models have achieved widespread applicability in the software industry due to a lack of software tools to automate this prediction process. Historical project data, including software faults and a robust software fault prediction tool, can enable quality managers to focus on fault-prone modules. Thus, they can improve the testing process. We developed an Eclipse-based software fault prediction tool for Java programs to simplify the fault prediction process. We also integrated a machine learning algorithm called Naive Bayes into the plug-in because of its proven high-performance for this problem. This article presents a practical view to software fault prediction problem, and it shows how we managed to combine software metrics with software fault data to apply Naive Bayes technique inside an open source platform.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {2347–2353},
numpages = {7},
keywords = {Software fault prediction, Naive Bayes, Machine learning, Eclipse technology}
}

@article{10.1145/1943371.1943381,
author = {Bishnu, P. S. and Bhattacherjee, V.},
title = {Application of K-Medoids with Kd-Tree for Software Fault Prediction},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1943371.1943381},
doi = {10.1145/1943371.1943381},
abstract = {Software fault prediction area is subject to problems like non availability of fault data which makes the application of supervised techniques difficult. In such cases unsupervised approaches like clustering are helpful. In this paper, K-Medoids clustering approach has been applied for software fault prediction. To overcome the inherent computational complexity of KMedoids algorithm a data structure called Kd-Tree has been used to identify data agents in the datasets. Partitioning Around Medoids is applied on these data agents and this results in a set of medoids. All the remaining data points are assigned to the nearest medoids thus obtained to get the final clusters. Software fault prediction error analysis results show that our approach outperforms all unsupervised approaches in the case of one given real dataset and gives best values for the evaluation parameters. For other real datasets, our results are comparable to other techniques. Performance evaluation of our technique with other techniques has been done. Results show that our technique reduces the total number of distance calculations drastically since the number of data agents is much less than the number of data points.},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {1–6},
numpages = {6},
keywords = {Software fault prediction, Kd-Tree, K-Medoids}
}

@inproceedings{10.1145/3475716.3475790,
author = {Wang, Song and Wang, Junjie and Nam, Jaechang and Nagappan, Nachiappan},
title = {Continuous Software Bug Prediction},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475790},
doi = {10.1145/3475716.3475790},
abstract = {Background: Many software bug prediction models have been proposed and evaluated on a set of well-known benchmark datasets. We conducted pilot studies on the widely used benchmark datasets and observed common issues among them. Specifically, most of existing benchmark datasets consist of randomly selected historical versions of software projects, which poses non-trivial threats to the validity of existing bug prediction studies since the real-world software projects often evolve continuously. Yet how to conduct software bug prediction in the real-world continuous software development scenarios is not well studied.Aims: In this paper, to bridge the gap between current software bug prediction practice and real-world continuous software development, we propose new approaches to conduct bug prediction in real-world continuous software development regarding model building, updating, and evaluation.Method: For model building, we propose ConBuild, which leverages distributional characteristics of bug prediction data to guide the training version selection. For model updating, we propose ConUpdate, which leverages the evolution of distributional characteristics of bug prediction data between versions to guide the reuse or update of bug prediction models in continuous software development. For model evaluation, we propose ConEA, which leverages the evolution of buggy probability of files between versions to conduct effort-aware evaluation.Results: Experiments on 120 continuously release versions that span across six large-scale open-source software systems show the practical value of our approaches.Conclusions: This paper provides new insights and guidelines for conducting software bug prediction in the context of continuous software development.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {14},
numpages = {12},
keywords = {software quality, software defect prediction, continuous software development, Empirical software engineering},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/2961111.2962617,
author = {Honsel, Daniel and Honsel, Verena and Welter, Marlon and Waack, Stephan and Grabowski, Jens},
title = {Monitoring Software Quality by Means of Simulation Methods},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962617},
doi = {10.1145/2961111.2962617},
abstract = {The evolution of software projects is driven by developers who are in control of the developed artifacts and the quality of software projects depends on the work of participating developers. Thus, a simulation tool requires a suitable model of the commit behavior of different developer types. In this paper, we present an agent-based model for software processes containing the commit behavior for different developer types. The description of these types results from mining software repositories. Since relationships between software entities, e.g., files, classes, modules, axe represented as dependency graphs, simulation results can be assessed automatically by Conditional Random Fields (CRFs). By adjusting simulation parameters for one project we are able to give a quality trend of other projects similar in size and duration only by changing the effort and the size of other projects to simulate.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {11},
numpages = {6},
keywords = {Software Dependency Analysis, Mining Software Repositories, Conditional Random Fields, Agent-Based Simulation},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@article{10.1007/s00521-019-04175-z,
author = {Kaur, Amandeep and Jain, Sushma and Goel, Shivani},
title = {SP-J48: a novel optimization and machine-learning-based approach for solving complex problems: special application in software engineering for detecting code smells},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04175-z},
doi = {10.1007/s00521-019-04175-z},
abstract = {This paper presents a novel hybrid algorithm based on optimization and machine-learning approaches for solving real-life complex problems. The optimization algorithm is inspired from the searching and attacking behaviors of sandpipers, called as Sandpiper Optimization Algorithm (SPOA). These two behaviors are modeled and implemented computationally to emphasize intensification and diversification in the search space. A comparison of the proposed SPOA algorithm is performed with nine competing optimization algorithms over 23 benchmark test functions. The proposed SPOA is further hybridized with B-J48 pruned machine-learning approach for efficiently detecting the code smells from the data set. The results reveal that the proposed technique is able to solve challenging problems and outperforms the other well-known approaches.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {7009–7027},
numpages = {19},
keywords = {Code smells, Machine-learning, Bio-inspired metaheuristic techniques, Optimization}
}

@inproceedings{10.1109/iCECE.2010.253,
author = {Luhui, Lin and Jie, Ma},
title = {Fault Prediction Based on Data-Driven Technique},
year = {2010},
isbn = {9780769540313},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/iCECE.2010.253},
doi = {10.1109/iCECE.2010.253},
abstract = {This paper presents principal component analysis (PCA), some improvement of PCA and the development of PCA. PCA does not depend on the accurate mathematical model, is able to implement the feature extraction of the complex process data, and establishes a principal component model of the corresponding process. It can achieve the extraction of the system information and eliminate the interference the system. So there is the existence of a good applications prospect in the complex process of fault diagnosis and prediction maintain.},
booktitle = {Proceedings of the 2010 International Conference on Electrical and Control Engineering},
pages = {997–1001},
numpages = {5},
keywords = {principal component analysis (PCA), improvement, fault prediction, data-driven},
series = {ICECE '10}
}

@inproceedings{10.1109/ACCT.2012.27,
author = {Gupta, Deepak and Goyal, Vinay Kr. and Mittal, Harish},
title = {Analysis of Clustering Techniques for Software Quality Prediction},
year = {2012},
isbn = {9780769546407},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ACCT.2012.27},
doi = {10.1109/ACCT.2012.27},
abstract = {Clustering is the unsupervised classification of patterns into groups. A clustering algorithm partitions a data set into several groups such that similarity within a group is larger than among groups The clustering problem has been addressed in many contexts and by researchers in many disciplines, this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. There is need to develop some methods to build the software fault prediction model based on unsupervised learning which can help to predict the fault-- proneness of a program modules when fault labels for modules are not present. One of the such method is use of clustering techniques. This paper presents a case study of different clustering techniques and analyzes their performance.},
booktitle = {Proceedings of the 2012 Second International Conference on Advanced Computing &amp; Communication Technologies},
pages = {6–9},
numpages = {4},
keywords = {Subtractive clustering, SOM, Mountain, K-means, Hierarchical, Fuzzy C-means},
series = {ACCT '12}
}

@inproceedings{10.1145/3053600.3053618,
author = {Janes, Andrea and Lenarduzzi, Valentina and Stan, Alexandru Cristian},
title = {A Continuous Software Quality Monitoring Approach for Small and Medium Enterprises},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053618},
doi = {10.1145/3053600.3053618},
abstract = {Context: SMEs cannot always afford the effort required for software quality assurance, and therefore there is the need of easy and affordable practices to prevent issues in the software they develop.Object: In this paper we propose an approach to allow SMEs to access SQA practices, using an SQA approach based on a continuous issue and error monitoring and a recommendation system that will suggest quality practices, recommending a set of quality actions based on the issues that previously created errors, so as to help SMEs to maintain quality above a minimum threshold. Method: First, we aim to identify a set of SQA practices applicable in SMEs, based on the main constraints of SMEs and a set of tools and practices to fulfill a complete DevOps pipeline. Second, we aim to define a recommendation system to provide software quality feedback to micro-teams, suggesting which action(s) they should take to maintain a certain quality level and allowing them to remove the most severe issues with the lowest possible effort. Our approach will be validated by a set of local SMEs. Moreover, the tools developed will be published as Open Source.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {97–100},
numpages = {4},
keywords = {software monitoring, software maintenance, continuous quality assurance, code smells, anti-patterns},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@article{10.1007/s10489-009-0193-8,
author = {Hewett, Rattikorn},
title = {Mining software defect data to support software testing management},
year = {2011},
issue_date = {April     2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {34},
number = {2},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-009-0193-8},
doi = {10.1007/s10489-009-0193-8},
abstract = {Achieving high quality software would be easier if effective software development practices were known and deployed in appropriate contexts. Because our theoretical knowledge of the underlying principles of software development is far from complete, empirical analysis of past experience in software projects is essential for acquiring useful software practices. As advances in software technology continue to facilitate automated tracking and data collection, more software data become available. Our research aims to develop methods to exploit such data for improving software development practices.This paper proposes an empirical approach, based on the analysis of defect data, that provides support for software testing management in two ways: (1) construction of a predictive model for defect repair times, and (2) a method for assessing testing quality across multiple releases. The approach employs data mining techniques including statistical methods and machine learning. To illustrate the proposed approach, we present a case study using the defect reports created during the development of three releases of a large medical software system, produced by a large well-established software company. We validate our proposed testing quality assessment using a statistical test at a significance level of 0.1. Despite the limitations of the available data, our predictive models give accuracies as high as 93%.},
journal = {Applied Intelligence},
month = apr,
pages = {245–257},
numpages = {13},
keywords = {Software testing management, Quality assurance, Defect report, Data mining}
}

@inproceedings{10.1145/2245276.2231967,
author = {Sarro, F. and Di Martino, S. and Ferrucci, F. and Gravino, C.},
title = {A further analysis on the use of Genetic Algorithm to configure Support Vector Machines for inter-release fault prediction},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231967},
doi = {10.1145/2245276.2231967},
abstract = {Some studies have reported promising results on the use of Support Vector Machines (SVMs) for predicting fault-prone software components. Nevertheless, the performance of the method heavily depends on the setting of some parameters. To address this issue, we investigated the use of a Genetic Algorithm (GA) to search for a suitable configuration of SVMs to be used for inter-release fault prediction. In particular, we report on an assessment of the method on five software systems. As benchmarks we exploited SVMs with random and Grid-search configuration strategies and several other machine learning techniques. The results show that the combined use of GA and SVMs is effective for inter-release fault prediction.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1215–1220},
numpages = {6},
keywords = {support vector machines, genetic algorithm, fault prediction},
location = {Trento, Italy},
series = {SAC '12}
}

@article{10.1016/j.jss.2010.11.920,
author = {Xie, Xiaoyuan and Ho, Joshua W. K. and Murphy, Christian and Kaiser, Gail and Xu, Baowen and Chen, Tsong Yueh},
title = {Testing and validating machine learning classifiers by metamorphic testing},
year = {2011},
issue_date = {April, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.11.920},
doi = {10.1016/j.jss.2010.11.920},
abstract = {Abstract: Machine learning algorithms have provided core functionality to many application domains - such as bioinformatics, computational linguistics, etc. However, it is difficult to detect faults in such applications because often there is no ''test oracle'' to verify the correctness of the computed outputs. To help address the software quality, in this paper we present a technique for testing the implementations of machine learning classification algorithms which support such applications. Our approach is based on the technique ''metamorphic testing'', which has been shown to be effective to alleviate the oracle problem. Also presented include a case study on a real-world machine learning application framework, and a discussion of how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also conduct mutation analysis and cross-validation, which reveal that our method has high effectiveness in killing mutants, and that observing expected cross-validation result alone is not sufficiently effective to detect faults in a supervised classification program. The effectiveness of metamorphic testing is further confirmed by the detection of real faults in a popular open-source classification program.},
journal = {J. Syst. Softw.},
month = apr,
pages = {544–558},
numpages = {15},
keywords = {Verification, Validation, Test oracle, Oracle problem, Metamorphic testing, Machine learning}
}

@article{10.1016/j.infsof.2019.05.009,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh and Marston, Chad},
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.009},
doi = {10.1016/j.infsof.2019.05.009},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {131–145},
numpages = {15},
keywords = {Case study, User trust, Business-to-business, Machine learning, Big data}
}

@inproceedings{10.1007/11497455_10,
author = {Leszak, Marek},
title = {Software defect analysis of a multi-release telecommunications system},
year = {2005},
isbn = {3540262008},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11497455_10},
doi = {10.1007/11497455_10},
abstract = {This paper provides a study of several process metrics of an industrial large-scale embedded software system, the Lucent product Lambda-UniteTM MSS. This product is an evolutionary hardware/software system for the metropolitan and wide-area transmission and switching market. An analysis of defect data is performed, including and comparing all major (i.e. feature) releases till end of 2004. Several defect metrics on file-level are defined and analyzed, as basis for a defect prediction model. Main analysis results include the following. Faults and code size per file show only a weak correlation. Portion of faulty files per release tend to decrease across releases. Size and error-proneness in previous release alone is not a good predictor of a file's faults per release. Customer-found defects are strongly correlated with pre-delivery defects found per subsystem. These results are being compared to a recent similar study of fault distributions; the differences are significant.},
booktitle = {Proceedings of the 6th International Conference on Product Focused Software Process Improvement},
pages = {98–114},
numpages = {17},
keywords = {software process metrics, errorproneness, defect prediction, defect density, case study},
location = {Oulu, Finland},
series = {PROFES'05}
}

@inproceedings{10.5555/2337223.2337246,
author = {Peters, Fayola and Menzies, Tim},
title = {Privacy and utility for defect prediction: experiments with MORPH},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Ideally, we can learn lessons from software projects across multiple organizations. However, a major impediment to such knowledge sharing are the privacy concerns of software development organizations. This paper aims to provide defect data-set owners with an effective means of privatizing their data prior to release. We explore MORPH which understands how to maintain class boundaries in a data-set. MORPH is a data mutator that moves the data a random distance, taking care not to cross class boundaries. The value of training on this MORPHed data is tested via a 10-way within learning study and a cross learning study using Random Forests, Naive Bayes, and Logistic Regression for ten object-oriented defect data-sets from the PROMISE data repository. Measured in terms of exposure of sensitive attributes, the MORPHed data was four times more private than the unMORPHed data. Also, in terms of the f-measures, there was little difference between the MORPHed and unMORPHed data (original data and data privatized by data-swapping) for both the cross and within study. We conclude that at least for the kinds of OO defect data studied in this project, data can be privatized without concerns for inference efficacy.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {189–199},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@article{10.1016/j.ins.2010.11.028,
author = {Khoshgoftaar, Taghi M. and Xiao, Yudong and Gao, Kehan},
title = {Software quality assessment using a multi-strategy classifier},
year = {2014},
issue_date = {February, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {259},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2010.11.028},
doi = {10.1016/j.ins.2010.11.028},
abstract = {Classifying program modules as fault-prone or not fault-prone is a valuable technique for guiding the software development process, so that resources can be allocated to components most likely to have faults. The rule-based classification and the case-based learning techniques are commonly used in software quality classification problems. However, studies show that these two techniques share some complementary strengths and weaknesses. Therefore, in this paper we propose a new multi-strategy classification model, RB2CBL, which integrates a rule-based (RB) model with two case-based learning (CBL) models. RB2CBL possesses the merits of both the RB model and CBL model and restrains their drawbacks. In the RB2CBL model, the parameter optimization of the CBL models is critical and an embedded genetic algorithm optimizer is used. Two case studies were carried out to validate the proposed method. The results show that, by suitably choosing the accuracy of the RB model, the RB2CBL model outperforms the RB model alone without overfitting.},
journal = {Inf. Sci.},
month = feb,
pages = {555–570},
numpages = {16},
keywords = {Software quality classification, Rule-based model, Multi-strategy classifier, Genetic algorithm, Case-based learning}
}

@inproceedings{10.1109/ISSRE.2008.54,
author = {Jiang, Yue and Cukic, Bojan and Menzies, Tim},
title = {Cost Curve Evaluation of Fault Prediction Models},
year = {2008},
isbn = {9780769534053},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISSRE.2008.54},
doi = {10.1109/ISSRE.2008.54},
abstract = {Prediction of fault prone software components is one of the most researched problems in software engineering. Many statistical techniques have been proposed but there is no consensus on the methodology to select the "best model" for the specific project. In this paper, we introduce and discuss the merits of cost curve analysis of fault prediction models. Cost curves allow software quality engineers to introduce project-specific cost of module misclassification into model evaluation. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Through the analysis of sixteen projects from public repositories, we observe that software quality does not necessarily benefit from the prediction of fault prone components. The inclusion of misclassification cost in model evaluation may indicate that even the "best" models achieve performance no better than trivial classification. Our results support a recommendation to adopt cost curves as one of the standard methods for software quality model performance evaluation.},
booktitle = {Proceedings of the 2008 19th International Symposium on Software Reliability Engineering},
pages = {197–206},
numpages = {10},
keywords = {software quality, verification and validation, machine learning, classification},
series = {ISSRE '08}
}

@inproceedings{10.1109/ARTCom.2009.12,
author = {Gayatri, N. and S., Nickolas and Reddy, A. V. and Chitra, R.},
title = {Performance Analysis of Datamining Algorithms for Software Quality Prediction},
year = {2009},
isbn = {9780769538457},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ARTCom.2009.12},
doi = {10.1109/ARTCom.2009.12},
abstract = {Data mining techniques are applied in building software fault prediction models for improving the software quality. Early identification of high-risk modules can assist in quality enhancement efforts to modules that are likely to have a high number of faults. Classification tree models are simple and effective as software quality prediction models, and timely predictions of defects from such models can be used to achieve high software reliability. In this paper, the performance of five data mining classifier algorithms named J48, CART, Random Forest, BFTree and Na\"{\i}ve Bayesian classifier(NBC) are evaluated based on 10 fold cross validation test. Experimental results using KC2 NASA software metrics dataset demonstrates that decision trees are much useful for fault predictions and based on rules generated only some measurement attributes in the given set of the metrics play an important role in establishing final rules and for improving the software quality by giving correct predictions. Thus we can suggest that these attributes are sufficient for future classification process. To evaluate the performance of the above algorithms Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Receiver Operating Characteristic (ROC) and Accuracy measures are applied.},
booktitle = {Proceedings of the 2009 International Conference on Advances in Recent Technologies in Communication and Computing},
pages = {393–395},
numpages = {3},
keywords = {Software Quality, Random forest, Naive Bayesian, J48, Cross-Validation, Classification, CART, BFTree},
series = {ARTCOM '09}
}

@article{10.1016/j.eswa.2010.10.024,
author = {Catal, Cagatay},
title = {Review: Software fault prediction: A literature review and current trends},
year = {2011},
issue_date = {April, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.10.024},
doi = {10.1016/j.eswa.2010.10.024},
abstract = {Software engineering discipline contains several prediction approaches such as test effort prediction, correction cost prediction, fault prediction, reusability prediction, security prediction, effort prediction, and quality prediction. However, most of these prediction approaches are still in preliminary phase and more research should be conducted to reach robust models. Software fault prediction is the most popular research area in these prediction approaches and recently several research centers started new projects on this area. In this study, we investigated 90 software fault prediction papers published between year 1990 and year 2009 and then we categorized these papers according to the publication year. This paper surveys the software engineering literature on software fault prediction and both machine learning based and statistical based approaches are included in this survey. Papers explained in this article reflect the outline of what was published so far, but naturally this is not a complete review of all the papers published so far. This paper will help researchers to investigate the previous studies from metrics, methods, datasets, performance evaluation metrics, and experimental results perspectives in an easy and effective manner. Furthermore, current trends are introduced and discussed.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {4626–4636},
numpages = {11},
keywords = {Statistical methods, Software quality engineering, Software engineering, Machine learning, Expert systems, Automated fault prediction models}
}

@inproceedings{10.1007/978-3-642-28714-5_21,
author = {Radli\'{n}ski, \L{}ukasz},
title = {Empirical analysis of the impact of requirements engineering on software quality},
year = {2012},
isbn = {9783642287138},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-28714-5_21},
doi = {10.1007/978-3-642-28714-5_21},
abstract = {[Context &amp; motivation] The process of requirements engineering affects software quality. However, stronger empirical evaluation of this impact is required. [Question/problem] This paper aims to answer the following questions: (1) which factors related to requirements engineering affect software quality, (2) what is the nature of these relationships, and (3) how are soft quality features related to each other? [Principal ideas/results] To answer these questions we performed a quantitative and visual analysis using the extended ISBSG dataset. Obtained results cover a discussion on identified and unconfirmed relationships. [Contribution] The main contribution is an investigation of the relationships between factors of requirements engineering and software quality. Provided results can be used in further research and to guide industrial decision makers. The main limitation in generalizing the results is related to the high number of missing values in the dataset.},
booktitle = {Proceedings of the 18th International Conference on Requirements Engineering: Foundation for Software Quality},
pages = {232–238},
numpages = {7},
keywords = {software quality, requirements engineering, process factors, empirical analysis},
location = {Essen, Germany},
series = {REFSQ'12}
}

@inproceedings{10.5555/786769.787023,
author = {Azar, Danielle and Precup, Doina and Bouktif, Salah and K\'{e}gl, Bal\'{a}zs and Sahraoui, Houari},
title = {Combining and Adapting Software Quality Predictive Models by Genetic Algorithms},
year = {2002},
isbn = {0769517366},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The goal of quality models is to predict a quality factor starting from a set of direct measures. Selecting an appropriate quality model for a particular software is a difficult, non-trivial decision. In this paper, we propose an approach to combine and/or adapt existing models (experts) in such way that the combined/adapted model works well on the particular system. Test results indicate that the models perform significantly better than individual experts in the pool.},
booktitle = {Proceedings of the 17th IEEE International Conference on Automated Software Engineering},
pages = {285},
series = {ASE '02}
}

@inproceedings{10.1145/2837060.2837066,
author = {Han, Ji-Hyeong and Kim, Rockwon and Chi, Su-Young},
title = {Applications of Machine Learning Algorithms to Predictive Manufacturing: Trends and Application of Tool Wear Compensation Parameter Recommendation},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837066},
doi = {10.1145/2837060.2837066},
abstract = {The manufacturing industry has become more competitive because of globalization and fast change in the industry. To survive from the global market, manufacturing enterprises should reduce the product cost and increase the productivity. The most promising way is applying the information communication technology especially machine learning algorithms to the traditional manufacturing system. This paper presents recent trends of applying machine learning techniques to manufacturing system and briefly explains each kind of applications. As a representative application of machine learning algorithms to manufacturing system, a generalized tool wear compensation parameter recommendation framework using regression algorithms and preliminary results using real data gathered from local and small manufacturing are also presented.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {51–57},
numpages = {7},
keywords = {tool wear compensation parameter recommendation, machine learning, Predictive manufacturing},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@article{10.1016/j.jss.2007.07.034,
author = {Vandecruys, Olivier and Martens, David and Baesens, Bart and Mues, Christophe and De Backer, Manu and Haesen, Raf},
title = {Mining software repositories for comprehensible software fault prediction models},
year = {2008},
issue_date = {May, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.07.034},
doi = {10.1016/j.jss.2007.07.034},
abstract = {Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization (ACO)-based classification technique AntMiner+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by AntMiner+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the AntMiner+ models can be considered superior to the latter models.},
journal = {J. Syst. Softw.},
month = may,
pages = {823–839},
numpages = {17},
keywords = {Software mining, Fault prediction, Comprehensibility, Classification, Ant Colony Optimization}
}

@inproceedings{10.1109/QRS.2015.37,
author = {Chuang, Hsin-Yu and Wang, Sheng-De},
title = {Machine Learning Based Hybrid Behavior Models for Android Malware Analysis},
year = {2015},
isbn = {9781467379892},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/QRS.2015.37},
doi = {10.1109/QRS.2015.37},
abstract = {Malware analysis on the Android platform has been an important issue as the platform became prevalent. The paper proposes a malware detection approach based on static analysis and machine learning techniques. By conducting SVM training on two different feature sets, malicious-preferred features and normal-preferred features, we built a hybrid-model classifier to improve the detection accuracy. With the consideration of normal behavior features, the ability of detecting unknown malwares can be improved. The experiments show that the accuracy is as high as 96.69% in predicting unknown applications. Further, the proposed approach can be applied to make confident decisions on labeling unknown applications. The experiment results show that the proposed hybrid model classifier can label 79.4% applications without false positive and false negative occurred in the labeling process.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Software Quality, Reliability and Security},
pages = {201–206},
numpages = {6},
keywords = {static analysis, malware detection, machine learning, classification, Android},
series = {QRS '15}
}

@inproceedings{10.5555/2669379.2669380,
author = {Di Penta, Massimiliano},
title = {Mining developers' communication to assess software quality: promises, challenges, perils},
year = {2012},
isbn = {9781467317627},
publisher = {IEEE Press},
abstract = {In recent years, researchers are building models relying on a wide variety of data that can be extracted from software repositories, concerning for example characteristics of source code changes, or be related to bug introduction and fixing. Software repositories also contain a huge amount of non-structured information, often expressed in natural language, concerning communication between developers, as well as tags, commit notes, or comments developers produce during their activities.This keynote illustrates, on the one hand, how explanatory or predictive models build upon software repositories could be enhanced by integrating them with the analysis of communication among developers. On the other hand, the keynote warns agains perils in doing that, due to the intrinsic imprecision and incompleteness of such a textual information, and explains how such problems could, at least, be mitigated.},
booktitle = {Proceedings of the 3rd International Workshop on Emerging Trends in Software Metrics},
pages = {1},
numpages = {1},
keywords = {software quality, software metrics, social network analysis, natural language processing},
location = {Zurich, Switzerland},
series = {WETSoM '12}
}

@inproceedings{10.1007/978-3-642-39068-5_56,
author = {Nawaz, Javeria Muhammad and Arshad, Muhammad Zeeshan and Hong, Sang Jeen},
title = {Time series fault prediction in semiconductor equipment using recurrent neural network},
year = {2013},
isbn = {9783642390678},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39068-5_56},
doi = {10.1007/978-3-642-39068-5_56},
abstract = {This paper presents a model of Elman recurrent neural network (ERNN) for time series fault prediction in semiconductor etch equipment. ERNN maintains a copy of previous state of the input in its context units, as well as the current state of the input. Derivative dynamic time warping (DDTW) method is also discussed for the synchronization of time series data set acquired from plasma etcher. For each parameter of the data, the best ERNN structure was selected and trained using Levenberg Marquardt to generate one-step-ahead prediction for 10 experimental runs. The faulty experimental runs were successfully distinguished from healthy experimental runs with one missed alarm out of ten experimental runs.},
booktitle = {Proceedings of the 10th International Conference on Advances in Neural Networks - Volume Part II},
pages = {463–472},
numpages = {10},
keywords = {time series prediction, recurrent neural network, derivative dynamic time warping},
location = {Dalian, China},
series = {ISNN'13}
}

@inproceedings{10.1007/978-3-642-37343-5_8,
author = {do Prado, H\'{e}rcules Antonio and Bianchi Campos, F\'{a}bio and Ferneda, Edilson and Nunes Cornelio, Nildo and Haendchen Filho, Aluizio},
title = {Prediction of software quality based on variables from the development process},
year = {2012},
isbn = {9783642373428},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-37343-5_8},
doi = {10.1007/978-3-642-37343-5_8},
abstract = {Since the arising of software engineering many efforts have been devoted to improve the software development process. More recently, software quality has received attention from researchers due to the importance that software has gained in supporting all levels of the organizations. New methods, techniques, and tools were created to increase the quality and productivity of the software development process. Approaches based on the practitioners' experience, for example, or on the analysis of the data generated during the development process, have been adopted. This paper follows the second path by applying data mining procedures to figure out variables from the development process that most affect the software quality. The premise is that the quality of decision making in management of software projects is closely related to information gathered during the development process. A case study is presented in which some regression models were built to explore this idea during the phases of testing, approval, and production. The results can be applied, mainly, to help the development managers in focusing those variables to improve the quality of the software as a final product.},
booktitle = {Proceedings of the 16th International Conference on Knowledge Engineering, Machine Learning and Lattice Computing with Applications},
pages = {71–77},
numpages = {7},
keywords = {software quality, regression models, data mining},
location = {San Sebastian, Spain},
series = {KES'12}
}

@article{10.1007/s00521-021-06158-5,
author = {Nevendra, Meetesh and Singh, Pradeep},
title = {Defect count prediction via metric-based convolutional neural network},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {22},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06158-5},
doi = {10.1007/s00521-021-06158-5},
abstract = {With the increasing complexity and volume of the software, the number of defects in software modules is also increasing consistently, which affects the quality and delivery of software in time and budget. To improve the software quality and timely allocation of resources, defects should be detected at the initial phases of the software development life cycle. However, the existing defect prediction methodology based on high-dimensional and limited data only focuses on predicting defective modules. In contrast, the number of defects present in the software module has not been explored so far, especially using deep neural network. Also, whether deep learning could enhance the performance of defect count prediction is still uninvestigated. To fill this gap, we proposed an improved Convolutional Neural Network model, called metrics-based convolutional neural network (MB-CNN), which combines the advantages of appropriate metrics and an improved CNN method by introducing dropout for regularization between convolutions and dense layer. The proposed method predicts the presented defect count in the software module for homogeneous scenarios as within-version and cross-version. The experimental results show that, on average, across the fourteen real-world defect datasets, the proposed approach improves Li’s CNN architecture by 31% in within-version prediction and 28% in cross-version prediction. Moreover, the Friedman ranking test and Wilcoxon nonparametric test reveal the usefulness of our proposed approach over ten benchmark learning algorithms to predict defect count.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {15319–15344},
numpages = {26},
keywords = {Cross-project, CNN, Deep learning, Software defect count prediction}
}

@inproceedings{10.1145/1868328.1868333,
author = {Brady, Adam and Menzies, Tim},
title = {Case-based reasoning vs parametric models for software quality optimization},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868333},
doi = {10.1145/1868328.1868333},
abstract = {Background: There are many data mining methods but few comparisons between them. For example, there are at least two ways to build quality optimizers, programs that find project options that change quality measures like defects, development effort (total staff hours), and time (elapsed calendar months). In the first way, we construct a parametric model to represent prior software projects. In the second way, we just apply case-based reasoning to reason directly from historical cases.Aim: To assess case-based reasoning vs parametric modeling for quality optimization.Method: We compared the W case-based reasoner against the SEEWAW parametric modeling tool.Results: W is easy to explain and fast to build. It makes no parametric assumptions and hence can be rapidly applied to project data in many formats. SEESAW is an elaborate tool that can only process project data expressed in a particular ontology (i.e. just the COCOMO attributes). It is also slower to execute than W. In 24 different tests comparing W and SEESAW, W always performs at least as well as SEESAW. In 6 of those tests W performed statistically better (all tests used Mann-Whitney, 95% confidence). Lastly, like any CBR method, it comes with a built-in maintenance strategy (just add more cases).Conclusion: The W case-based reasoning tool is recommended over the SEESAW parametric modeling tool for purposes of quality optimization (except in the case where there is no local data).},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {3},
numpages = {10},
keywords = {COCOMO, analogy, case based reasoning, effort estimation, optimization, parametric modeling, software quality},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@article{10.1155/2019/8391425,
author = {Ren, Jiadong and Zheng, Zhangqi and Liu, Qian and Wei, Zhiyao and Yan, Huaizhi and Chen, Jiageng},
title = {A Buffer Overflow Prediction Approach Based on Software Metrics and Machine Learning},
year = {2019},
issue_date = {2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2019},
issn = {1939-0114},
url = {https://doi.org/10.1155/2019/8391425},
doi = {10.1155/2019/8391425},
abstract = {Buffer overflow vulnerability is the most common and serious type of vulnerability in software today, as network security issues have become increasingly critical. To alleviate the security threat, many vulnerability mining methods based on static and dynamic analysis have been developed. However, the current analysis methods have problems regarding high computational time, low test efficiency, low accuracy, and low versatility. This paper proposed a software buffer overflow vulnerability prediction method by using software metrics and a decision tree algorithm. First, the software metrics were extracted from the software source code, and data from the dynamic data stream at the functional level was extracted by a data mining method. Second, a model based on a decision tree algorithm was constructed to measure multiple types of buffer overflow vulnerabilities at the functional level. Finally, the experimental results showed that our method ran in less time than SVM, Bayes, adaboost, and random forest algorithms and achieved 82.53% and 87.51% accuracy in two different data sets. The method presented in this paper achieved the effect of accurately predicting software buffer overflow vulnerabilities in C/C++ and Java programs.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/2591062.2591068,
author = {Heinemann, Lars and Hummel, Benjamin and Steidl, Daniela},
title = {Teamscale: software quality control in real-time},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591068},
doi = {10.1145/2591062.2591068},
abstract = {When large software systems evolve, the quality of source code is essential for successful maintenance. Controlling code quality continuously requires adequate tool support. Current quality analysis tools operate in batch-mode and run up to several hours for large systems, which hampers the integration of quality control into daily development. In this paper, we present the incremental quality analysis tool Teamscale, providing feedback to developers within seconds after a commit and thus enabling real-time software quality control. We evaluated the tool within a development team of a German insurance company. A video demonstrates our tool: http://www.youtube.com/watch?v=nnuqplu75Cg.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {592–595},
numpages = {4},
keywords = {static analysis, real-time, incremental, Quality control},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/2970276.2970364,
author = {Li, Xin and Liang, Yongjuan and Qian, Hong and Hu, Yi-Qi and Bu, Lei and Yu, Yang and Chen, Xin and Li, Xuandong},
title = {Symbolic execution of complex program driven by machine learning based constraint solving},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970364},
doi = {10.1145/2970276.2970364},
abstract = {Symbolic execution is a widely-used program analysis technique. It collects and solves path conditions to guide the program traversing. However, due to the limitation of the current constraint solvers, it is difficult to apply symbolic execution on programs with complex path conditions, like nonlinear constraints and function calls. In this paper, we propose a new symbolic execution tool MLB to handle such problem. Instead of relying on the classical constraint solving, in MLB, the feasibility problems of the path conditions are transformed into optimization problems, by minimizing some dissatisfaction degree. The optimization problems are then handled by the underlying optimization solver through machine learning guided sampling and validation. MLB is implemented on the basis of Symbolic PathFinder and encodes not only the simple linear path conditions, but also nonlinear arithmetic operations, and even black-box function calls of library methods, into symbolic path conditions. Experiment results show that MLB can achieve much better coverage on complex real-world programs.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {554–559},
numpages = {6},
keywords = {Symbolic Execution, Machine Learning, Constraint Solving, Complicated Path Condition},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1109/IVS.2018.8500421,
author = {Tuncali, Cumhur Erkan and Fainekos, Georgios and Ito, Hisahiro and Kapinski, James},
title = {Simulation-based Adversarial Test Generation for Autonomous Vehicles with Machine Learning Components},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IVS.2018.8500421},
doi = {10.1109/IVS.2018.8500421},
abstract = {Many organizations are developing autonomous driving systems, which are expected to be deployed at a large scale in the near future. Despite this, there is a lack of agreement on appropriate methods to test, debug, and certify the performance of these systems. One of the main challenges is that many autonomous driving systems have machine learning (ML) components, such as deep neural networks, for which formal properties are difficult to characterize. We present a testing framework that is compatible with test case generation and automatic falsification methods, which are used to evaluate cyber-physical systems. We demonstrate how the framework can be used to evaluate closed-loop properties of an autonomous driving system model that includes the ML components, all within a virtual environment. We demonstrate how to use test case generation methods, such as covering arrays, as well as requirement falsification methods to automatically identify problematic test scenarios. The resulting framework can be used to increase the reliability of autonomous driving systems.},
booktitle = {2018 IEEE Intelligent Vehicles Symposium (IV)},
pages = {1555–1562},
numpages = {8},
location = {Changshu, Suzhou, China}
}

@article{10.1016/j.ins.2008.12.001,
author = {Catal, Cagatay and Diri, Banu},
title = {Investigating the effect of dataset size, metrics sets, and feature selection techniques on software fault prediction problem},
year = {2009},
issue_date = {March, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {179},
number = {8},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2008.12.001},
doi = {10.1016/j.ins.2008.12.001},
abstract = {Software quality engineering comprises of several quality assurance activities such as testing, formal verification, inspection, fault tolerance, and software fault prediction. Until now, many researchers developed and validated several fault prediction models by using machine learning and statistical techniques. There have been used different kinds of software metrics and diverse feature reduction techniques in order to improve the models' performance. However, these studies did not investigate the effect of dataset size, metrics set, and feature selection techniques for software fault prediction. This study is focused on the high-performance fault predictors based on machine learning such as Random Forests and the algorithms based on a new computational intelligence approach called Artificial Immune Systems. We used public NASA datasets from the PROMISE repository to make our predictive models repeatable, refutable, and verifiable. The research questions were based on the effects of dataset size, metrics set, and feature selection techniques. In order to answer these questions, there were defined seven test groups. Additionally, nine classifiers were examined for each of the five public NASA datasets. According to this study, Random Forests provides the best prediction performance for large datasets and Naive Bayes is the best prediction algorithm for small datasets in terms of the Area Under Receiver Operating Characteristics Curve (AUC) evaluation parameter. The parallel implementation of Artificial Immune Recognition Systems (AIRS2Parallel) algorithm is the best Artificial Immune Systems paradigm-based algorithm when the method-level metrics are used.},
journal = {Inf. Sci.},
month = mar,
pages = {1040–1058},
numpages = {19},
keywords = {Software fault prediction, Random Forests, Naive Bayes, Machine learning, J48, Artificial Immune Systems}
}

@article{10.1155/2021/2668761,
author = {Zhang, Xiangquan and Ma, Zhili and Wang, Anmin and Mi, Haifeng and Hang, Junjun and Xiong, Jinbo},
title = {LstFcFedLear: A LSTM-FC with Vertical Federated Learning Network for Fault Prediction},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/2668761},
doi = {10.1155/2021/2668761},
abstract = {The firefighting IoT platform links multiple firefighting subsystems. The data of each subsystem belongs to the sensitive data of the profession. Failure prediction is a crucial topic for firefighting IoT platforms, because failures may cause equipment injuries. Currently, in the maintenance of fire IoT terminal equipment, fault prediction based on equipment time series has not been included. The use of intelligent technology to continuously predict the failure of firefighting IoT equipment can not only eliminate the intervention of regular maintenance but also provide early warning of upcoming failures. In order to solve this problem, we propose a vertical federated learning framework based on LSTM fault classification network (LstFcFedLear). The advantage of this framework is that it can encrypt and integrate the data on the entire firefighting IoT platform to form a new dataset. After the synthesized data is trained through each model, the optimal model parameters can be finally updated. At the same time, it can ensure that the data of each business system is not leaked. The framework can predict when IoT equipment will fail in the future and then provide what measures should be used. The experimental results show that the LstFcFedLear model provides an effective method for fault prediction, and its results are comparable to the baseline.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {10}
}

@article{10.1016/j.jpdc.2018.03.006,
author = {Kozik, Rafa\l{} and Chora\'{s}, Micha\l{} and Ficco, Massimo and Palmieri, Francesco},
title = {A scalable distributed machine learning approach for attack detection in edge computing environments},
year = {2018},
issue_date = {Sep 2018},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {119},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2018.03.006},
doi = {10.1016/j.jpdc.2018.03.006},
journal = {J. Parallel Distrib. Comput.},
month = sep,
pages = {18–26},
numpages = {9},
keywords = {Attack detection, Distributed machine learning, Extreme learning machines, Edge computing, IoT}
}

@article{10.1007/s42979-021-00585-w,
author = {Rezaei, Amirhossein},
title = {Using Ensemble Learning Technique for Detecting Botnet on IoT},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {3},
url = {https://doi.org/10.1007/s42979-021-00585-w},
doi = {10.1007/s42979-021-00585-w},
abstract = {Despite the growing reputation and the ubiquitous nature of the IoT, it poses significant challenges as it is also considered a convenient platform for cyberattacks. The connection of various devices without fixed security help attackers in allowing botnet to run high crash DDoS attacks against a range of internet services. The botnet is one of the main security challenges that have the most impact on IoT for several reasons. It allows the private network devices to be infected by malicious software and controlled as a group without the owners’ awareness. Botnets are often utilized for sending spam, stealing data, and performing DDoS attacks. As there are many areas of security that utilize machine learning (ML) technology, utilizing ML to detect botnet appears to be realistic and workable. While several studies have been conducted using ML for detecting botnet, they are either not highly accurate or only compatible with specific types of botnet or devices. This study aims to create an ensemble learning model using the best ML methods among supervised learning, unsupervised learning, and regression learning to optimize the accuracy of botnet detection on IoT and minimize the number of features requested. After several examinations in different combinations of ML algorithms, this study managed to achieve 100% accuracy of botnet detection as well as minimizing the number of features to only 20.},
journal = {SN Comput. Sci.},
month = mar,
numpages = {14},
keywords = {Internet of things, Botnet, Ensemble learning, Regression learning, Unsupervised learning, Supervised learning, Machine learning}
}

@inproceedings{10.1007/978-3-030-00374-6_13,
author = {Villalobos, Ignacio and Ferrer, Javier and Alba, Enrique},
title = {Measuring the Quality of Machine Learning and Optimization Frameworks},
year = {2018},
isbn = {978-3-030-00373-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00374-6_13},
doi = {10.1007/978-3-030-00374-6_13},
abstract = {Software frameworks are daily and extensively used in research, both for fundamental studies and applications. Researchers usually trust in the quality of these frameworks without any evidence that they are correctly build, indeed they could contain some defects that potentially could affect to thousands of already published and future papers. Considering the important role of these frameworks in the current state-of-the-art in research, their quality should be quantified to show the weaknesses and strengths of each software package.In this paper we study the main static quality properties, defined in the product quality model proposed by the ISO 25010 standard, of ten well-known frameworks. We provide a quality rating for each characteristic depending on the severity of the issues detected in the analysis. In addition, we propose an overall quality rating of 12 levels (ranging from A+ to D−) considering the ratings of all characteristics. As a result, we have data evidence to claim that the analysed frameworks are not in a good shape, because the best overall rating is just a C+ for Mahout framework, i.e., all packages need to go for a revision in the analysed features. Focusing on the characteristics individually, maintainability is by far the one which needs the biggest effort to fix the found defects. On the other hand, performance obtains the best average rating, a result which conforms to our expectations because frameworks’ authors used to take care about how fast their software runs.},
booktitle = {Advances in Artificial Intelligence: 18th Conference of the Spanish Association for Artificial Intelligence, CAEPIA 2018, Granada, Spain, October 23–26, 2018, Proceedings},
pages = {128–139},
numpages = {12},
keywords = {Maintainability, Reliability, Performance, Security, Quality},
location = {Granada, Spain}
}

@inproceedings{10.1109/ICGCIoT.2015.7380664,
author = {Shrivastava, Ritu},
title = {Ranking software quality factors using intuitionistic preference relations and group decision making},
year = {2015},
isbn = {9781467379106},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICGCIoT.2015.7380664},
doi = {10.1109/ICGCIoT.2015.7380664},
abstract = {It is well known that developers and professionals do not agree on common factors that derive quality of software. Therefore it is appropriate to apply group decision taking to rank quality factors of software. The present research considers three decision takers, viz., users, developers and professionals to decide ranking of quality factors. In this process, each decision maker provides his/her intuitionistic fuzzy preference values rather than exact values for each factor. These preference values are aggregated using averaging operator, which is further aggregated using weighted arithmetic averaging operator. Finally eight considered factors are ranked.},
booktitle = {Proceedings of the 2015 International Conference on Green Computing and Internet of Things (ICGCIoT)},
pages = {1288–1291},
numpages = {4},
series = {ICGCIOT '15}
}

@inproceedings{10.1145/3375627.3375858,
author = {Zucker, Julian and d'Leeuwen, Myraeka},
title = {Arbiter: A Domain-Specific Language for Ethical Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375858},
doi = {10.1145/3375627.3375858},
abstract = {The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {421–425},
numpages = {5},
keywords = {domain-specific languages, ethical machine learning},
location = {New York, NY, USA},
series = {AIES '20}
}

@article{10.1016/j.ins.2018.05.035,
author = {Siers, Michael J. and Islam, Md Zahidul},
title = {Novel algorithms for cost-sensitive classification and knowledge discovery in class imbalanced datasets with an application to NASA software defects},
year = {2018},
issue_date = {Aug 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {459},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.05.035},
doi = {10.1016/j.ins.2018.05.035},
journal = {Inf. Sci.},
month = aug,
pages = {53–70},
numpages = {18},
keywords = {Knowledge discovery, Decision forest, Cost-sensitive, Class imbalance, Software defect prediction}
}

@article{10.1016/j.infsof.2010.07.003,
author = {Bouktif, Salah and Ahmed, Faheem and Khalil, Issa and Antoniol, Giuliano},
title = {A novel composite model approach to improve software quality prediction},
year = {2010},
issue_date = {December, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {12},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.07.003},
doi = {10.1016/j.infsof.2010.07.003},
abstract = {Context:: How can quality of software systems be predicted before deployment? In attempting to answer this question, prediction models are advocated in several studies. The performance of such models drops dramatically, with very low accuracy, when they are used in new software development environments or in new circumstances. Objective: The main objective of this work is to circumvent the model generalizability problem. We propose a new approach that substitutes traditional ways of building prediction models which use historical data and machine learning techniques. Method: In this paper, existing models are decision trees built to predict module fault-proneness within the NASA Critical Mission Software. A genetic algorithm is developed to combine and adapt expertise extracted from existing models in order to derive a ''composite'' model that performs accurately in a given context of software development. Experimental evaluation of the approach is carried out in three different software development circumstances. Results: The results show that derived prediction models work more accurately not only for a particular state of a software organization but also for evolving and modified ones. Conclusion: Our approach is considered suitable for software data nature and at the same time superior to model selection and data combination approaches. It is then concluded that learning from existing software models (i.e., software expertise) has two immediate advantages; circumventing model generalizability and alleviating the lack of data in software-engineering.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {1298–1311},
numpages = {14},
keywords = {Software quality prediction, Genetic algorithm, Fault-proneness, Decision trees}
}

@inproceedings{10.1145/3340482.3342742,
author = {Borg, Markus and Svensson, Oscar and Berg, Kristian and Hansson, Daniel},
title = {SZZ unleashed: an open implementation of the SZZ algorithm - featuring example usage in a study of just-in-time bug prediction for the Jenkins project},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342742},
doi = {10.1145/3340482.3342742},
abstract = {Machine learning applications in software engineering often rely on detailed information about bugs. While issue trackers often contain information about when bugs were fixed, details about when they were introduced to the system are often absent. As a remedy, researchers often rely on the SZZ algorithm as a heuristic approach to identify bug-introducing software changes. Unfortunately, as reported in a recent systematic literature review, few researchers have made their SZZ implementations publicly available. Consequently, there is a risk that research effort is wasted as new projects based on SZZ output need to initially reimplement the approach. Furthermore, there is a risk that newly developed (closed source) SZZ implementations have not been properly tested, thus conducting research based on their output might introduce threats to validity. We present SZZ Unleashed, an open implementation of the SZZ algorithm for git repositories. This paper describes our implementation along with a usage example for the Jenkins project, and conclude with an illustrative study on just-in-time bug prediction. We hope to continue evolving SZZ Unleashed on GitHub, and warmly invite the community to contribute.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {mining software repositories, issue tracking, defect prediction, SZZ},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@article{10.1016/j.engappai.2013.01.008,
author = {Rafael Lenz, Alexandre and Pozo, Aurora and Regina Vergilio, Silvia},
title = {Linking software testing results with a machine learning approach},
year = {2013},
issue_date = {May, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {26},
number = {5–6},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2013.01.008},
doi = {10.1016/j.engappai.2013.01.008},
abstract = {Software testing techniques and criteria are considered complementary since they can reveal different kinds of faults and test distinct aspects of the program. The functional criteria, such as Category Partition, are difficult to be automated and are usually manually applied. Structural and fault-based criteria generally provide measures to evaluate test sets. The existing supporting tools produce a lot of information including: input and produced output, structural coverage, mutation score, faults revealed, etc. However, such information is not linked to functional aspects of the software. In this work, we present an approach based on machine learning techniques to link test results from the application of different testing techniques. The approach groups test data into similar functional clusters. After this, according to the tester's goals, it generates classifiers (rules) that have different uses, including selection and prioritization of test cases. The paper also presents results from experimental evaluations and illustrates such uses.},
journal = {Eng. Appl. Artif. Intell.},
month = may,
pages = {1631–1640},
numpages = {10},
keywords = {Test coverage criteria, Software testing, Machine learning}
}

@article{10.1145/2693208.2693219,
author = {Li, Ming and Zhang, Hongyu and Lo, David and Lucia},
title = {Improving Software Quality and Productivity Leveraging Mining Techniques: [Summary of the Second Workshop on Software Mining, at ASE 2013]},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2693208.2693219},
doi = {10.1145/2693208.2693219},
abstract = {The second International Workshop on Software Mining (Soft-mine) was held on the 11th of November 2013. The workshop was held in conjunction with the 28th IEEE/ACM International Conference on Automated Software Engineering (ASE) in Silicon Valley, California, USA. The workshop has facilitated researchers who are interested in mining various types of software-related data and in applying data mining techniques to support software engineering tasks. During the workshop, seven papers on software mining and behavior models, execution trace mining, and bug localization and fixing were presented. One of the papers received the best paper award. Furthermore, there were two invited talk sessions presented by two active researchers from software engineering and data mining community.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–2},
numpages = {2},
keywords = {workshop report, developer productivity, data mining, Software quality}
}

@inproceedings{10.1145/3178212.3178221,
author = {Rizwan, Syed and Tiantian, Wang and Xiaohong, Su and Salahuddin},
title = {Empirical Study on Software Bug Prediction},
year = {2017},
isbn = {9781450354882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178212.3178221},
doi = {10.1145/3178212.3178221},
abstract = {Software defect prediction is a vital research direction in software engineering field. Software defect prediction predicts whether software errors are present in the software by using machine learning analysis on software metrics. It can help software developers to improve the quality of the software. Software defect prediction is usually a binary classification problem, which relies on software metrics and the use of classifiers. There have been many research efforts to improve accuracy in software defect prediction using a variety of classifiers and data preprocessing techniques. However, the "classic classifier validity" and "data preprocessing techniques can enhance the functionality of software defect prediction" has not yet been answered explicitly. Therefore, it is necessary to conduct an empirical analysis to compare these studies. In software defect prediction, the category of interest is a defective module, and the number of defective modules is much less than that of a non-defective module in data. This leads to a category of imbalance problem that reduces the accuracy of the prediction. Therefore, the problem of imbalance is a key problem that needs to be solved in software defect prediction. In this paper, we proposed an experimental model and used the NASA MDP data set to analyze the software defect prediction. Five research questions were defined and analyzed experimentally. In addition to experimental analysis, this paper focuses on the improvement of SMOTE. SMOTE ASMO algorithm has been proposed to overcome the shortcomings of SMOTE.},
booktitle = {Proceedings of the 2017 International Conference on Software and E-Business},
pages = {55–59},
numpages = {5},
keywords = {SMOTE, Defect prediction, Data preprocessing, Classification},
location = {Hong Kong, Hong Kong},
series = {ICSEB '17}
}

@article{10.5555/2595223.2595232,
author = {Li, Qingxiang and Zhao, Xiaofei and Lin, Rui and Chen, Benyan},
title = {Relative entropy method for fuzzy multiple attribute decision making and its application to software quality evaluation},
year = {2014},
issue_date = {July 2014},
publisher = {IOS Press},
address = {NLD},
volume = {26},
number = {4},
issn = {1064-1246},
abstract = {To study the problem of multiple attribute decision making in which the decision making information values are triangular fuzzy number, a relative entropy decision making method for software quality evaluation is proposed. Then, according to the concept of the relative entropy, the relative closeness degree is defined to determine the ranking order of all alternatives by calculating the relative entropy to both the fuzzy positive-ideal solution FPIS and fuzzy negative-ideal solution FNIS simultaneously. At last, a numerical example for software quality evaluation is provided to illustrate the proposed method. The result shows the approach is simple, effective and easy to calculate.},
journal = {J. Intell. Fuzzy Syst.},
month = jul,
pages = {1687–1693},
numpages = {7},
keywords = {Triangular Fuzzy Number, Software Quality Evaluation, Relative Entropy, Multiple Attribute Decision Making, Information Measure}
}

@inproceedings{10.1109/ICSE-SEIP.2019.00032,
author = {Parthy, Abhaya and Silberstein, Leo and Kowalczyk, Emily and High, John-Paul and Nagarajan, Adithya and Memon, Atif},
title = {Using machine learning to recommend correctness checks for geographic map data},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2019.00032},
doi = {10.1109/ICSE-SEIP.2019.00032},
abstract = {Developing an industry application that serves geographic map data to users across the world presents the significant challenge of checking the data using "data correctness checks." The size of data that needs to be checked---the entire world---and data churn rate---thousands per day---makes executing the full set of candidate checks cost prohibitive. Current techniques rely on hand-curated static subsets of checks to be run at different stages of the data production pipeline, These hard-coded subsets are uninformed of data changes, and cause bug detection to be delayed to downstream quality assurance activities. To address these problems, we have developed new representations of map data changes and checks, formally defined "check safety," and built a recommender system that dynamically and automatically selects and ranks a relevant subset of checks using signals from latest data changes. Empirical evaluation shows that it improves (1) efficiency by eliminating 65% of checks unrelated to changes, (2) coverage by recommending and ranking change-related checks from the full set of candidate checks, previously excluded by the manual process, and (3) overall visibility into the data editing process by quickly and automatically identifying latest fault prone parts of the data.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice},
pages = {223–232},
numpages = {10},
keywords = {safe test/check selection, data check recommender, automated data validation},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIP '19}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00026,
author = {Li, Paul Luo and Chai, Xiaoyu and Campbell, Frederick and Liao, Jilong and Abburu, Neeraja and Kang, Minsuk and Niculescu, Irina and Brake, Greg and Patil, Siddharth and Dooley, James and Paddock, Brandon},
title = {Evolving software to be ML-driven utilizing real-world A/B testing: experiences, insights, challenges},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00026},
doi = {10.1109/ICSE-SEIP52600.2021.00026},
abstract = {ML-driven software is heralded as the next major advancement in software engineering; existing software today can benefit from being evolved to be ML-driven. In this paper, we contribute practical knowledge about evolving software to be ML-driven, utilizing real-world A/B testing. We draw on experiences evolving two software features from the Windows operating system to be ML-driven, with more than ten realworld A/B tests on millions of PCs over more than two years. We discuss practical reasons for using A/B testing to engineer ML-driven software, insights for success, as well as on-going realworld challenges. This knowledge may help practitioners, as well as help direct future research and innovations.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {170–179},
numpages = {10},
keywords = {software quality, software engineering, software development management, predictive models, machine learning algorithms, machine learning, learning (artificial intelligence), data analysis, big data applications},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@article{10.1007/s10664-014-9316-x,
author = {Yang, Jiachen and Hotta, Keisuke and Higo, Yoshiki and Igaki, Hiroshi and Kusumoto, Shinji},
title = {Classification model for code clones based on machine learning},
year = {2015},
issue_date = {August    2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9316-x},
doi = {10.1007/s10664-014-9316-x},
abstract = {Results from code clone detectors may contain plentiful useless code clones, but judging whether each code clone is useful varies from user to user based on a user's purpose for the clone. In this research, we propose a classification model that applies machine learning to the judgments of each individual user regarding the code clones. To evaluate the proposed model, 32 participants completed an online survey to test its usability and accuracy. The result showed several important observations on the characteristics of the true positives of code clones for the users. Our classification model showed more than 70 % accuracy on average and more than 90 % accuracy for some particular users and projects.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1095–1125},
numpages = {31},
keywords = {Machine learning, Filtering, Code clone detector, Classify}
}

@inproceedings{10.1145/1143997.1144313,
author = {Bouktif, Salah and Sahraoui, Houari and Antoniol, Giuliano},
title = {Simulated annealing for improving software quality prediction},
year = {2006},
isbn = {1595931864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143997.1144313},
doi = {10.1145/1143997.1144313},
abstract = {In this paper, we propose an approach for the combination and adaptation of software quality predictive models. Quality models are decomposed into sets of expertise. The approach can be seen as a search for a valuable set of expertise that when combined form a model with an optimal predictive accuracy. Since, in general, there will be several experts available and each expert will provide his expertise, the problem can be reformulated as an optimization and search problem in a large space of solutions.We present how the general problem of combining quality experts, modeled as Bayesian classifiers, can be tackled via a simulated annealing algorithm customization. The general approach was applied to build an expert predicting object-oriented software stability, a facet of software quality. Our findings demonstrate that, on available data, composed expert predictive accuracy outperforms the best available expert and it compares favorably with the expert build via a customized genetic algorithm.},
booktitle = {Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation},
pages = {1893–1900},
numpages = {8},
keywords = {software quality, simulated annealing, predictive models, expertise reuse, Bayesian classifiers},
location = {Seattle, Washington, USA},
series = {GECCO '06}
}

@inproceedings{10.1145/2641580.2641625,
title = {Impact of Collaboration on Structural Software Quality},
year = {2014},
isbn = {9781450330169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2641580.2641625},
doi = {10.1145/2641580.2641625},
abstract = {The structural quality of a codebase is a key determining factor in the software's total cost of ownership, yet it is notoriously difficult to measure or predict. In this doctoral research we leverage the power of open source repositories to understand the factors that influence structural quality (and by extension fault-proneness) in the context of the patterns of collaborative behaviour exhibited by contributors. The objective is to further our understanding of how such behaviour impacts structural quality with the end goal being to inform management decision making across the industry in the pursuit of better software engineering practices.},
booktitle = {Proceedings of The International Symposium on Open Collaboration},
pages = {1–3},
numpages = {3},
location = {Berlin, Germany},
series = {OpenSym '14}
}

@inproceedings{10.1145/3422392.3422507,
author = {Cunha, Warteruzannan Soyer and Armijo, Guisella Angulo and de Camargo, Valter Vieira},
title = {InSet: A Tool to Identify Architecture Smells Using Machine Learning},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422507},
doi = {10.1145/3422392.3422507},
abstract = {Architectural smells (ASs) are architectural decisions that negatively affect the maintenance and evolution of software. Most of the existing tools able to identify AS rely on few metrics with fixed thresholds. However, it is not possible to define specific metrics and thresholds that meet all the cases, i.e., the classification of a piece of code in smell or not can depend on the domain, the experience of developers, organization patterns or even from a vast set of features - so there is a subjective ingredient in this decision. Machine Learning (ML) can help to make these decisions/classifications more precise by taking into consideration a vast set of features and also feedback from experts. This paper presents a machine learning-based tool to detect the architectural smells Unstable Dependency(UD) and God Component(GC). Our tool is able to take into consideration users' feedback to retrain the algorithms and constantly improve their performance. Our tool got good result in terms of accuracy, precision, recall, F-measure and Kappa's coefficient.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {760–765},
numpages = {6},
keywords = {Architecture Anomalies, Architecture Smells, Automatic Approach, Machine Learning, Predictive Model, Software Smells},
location = {Natal, Brazil},
series = {SBES '20}
}

@article{10.1109/TSE.2010.51,
author = {Liu, Yi and Khoshgoftaar, Taghi M. and Seliya, Naeem},
title = {Evolutionary Optimization of Software Quality Modeling with Multiple Repositories},
year = {2010},
issue_date = {November 2010},
publisher = {IEEE Press},
volume = {36},
number = {6},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2010.51},
doi = {10.1109/TSE.2010.51},
abstract = {A novel search-based approach to software quality modeling with multiple software project repositories is presented. Training a software quality model with only one software measurement and defect data set may not effectively encapsulate quality trends of the development organization. The inclusion of additional software projects during the training process can provide a cross-project perspective on software quality modeling and prediction. The genetic-programming-based approach includes three strategies for modeling with multiple software projects: Baseline Classifier, Validation Classifier, and Validation-and-Voting Classifier. The latter is shown to provide better generalization and more robust software quality models. This is based on a case study of software metrics and defect data from seven real-world systems. A second case study considers 17 different (nonevolutionary) machine learners for modeling with multiple software data sets. Both case studies use a similar majority-voting approach for predicting fault-proneness class of program modules. It is shown that the total cost of misclassification of the search-based software quality models is consistently lower than those of the non-search-based models. This study provides clear guidance to practitioners interested in exploiting their organization's software measurement data repositories for improved software quality modeling.},
journal = {IEEE Trans. Softw. Eng.},
month = nov,
pages = {852–864},
numpages = {13},
keywords = {software quality, software measurement., optimization, machine learning, defects, Genetic programming}
}

@inproceedings{10.1109/ESEM.2011.29,
author = {Li, Lianfa and Leung, Hareton},
title = {Mining Static Code Metrics for a Robust Prediction of Software Defect-Proneness},
year = {2011},
isbn = {9780769546049},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ESEM.2011.29},
doi = {10.1109/ESEM.2011.29},
abstract = {Defect-proneness prediction is affected by multiple aspects including sampling bias, non-metric factors, uncertainty of models etc. These aspects often contribute to prediction uncertainty and result in variance of prediction. This paper proposes two methods of data mining static code metrics to enhance defect-proneness prediction. Given little non-metric or qualitative information extracted from software codes, we first suggest to use a robust unsupervised learning method, shared nearest neighbors (SNN) to extract the similarity patterns of the code metrics. These patterns indicate similar characteristics of the components of the same cluster that may result in introduction of similar defects. Using the similarity patterns with code metrics as predictors, defect-proneness prediction may be improved. The second method uses the Occam's windows and Bayesian model averaging to deal with model uncertainty: first, the datasets are used to train and cross-validate multiple learners and then highly qualified models are selected and integrated into a robust prediction. From a study based on 12 datasets from NASA, we conclude that our proposed solutions can contribute to a better defect-proneness prediction.},
booktitle = {Proceedings of the 2011 International Symposium on Empirical Software Engineering and Measurement},
pages = {207–214},
numpages = {8},
keywords = {uncertainty, software quality, robust prediction, defect-proneness, data mining},
series = {ESEM '11}
}

@article{10.1016/j.jss.2009.06.055,
author = {Arisholm, Erik and Briand, Lionel C. and Johannessen, Eivind B.},
title = {A systematic and comprehensive investigation of methods to build and evaluate fault prediction models},
year = {2010},
issue_date = {January, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.055},
doi = {10.1016/j.jss.2009.06.055},
abstract = {This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets such as source code structural measures and change/fault history (process measures), and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area (ROC), and (iii) our proposed cost-effectiveness measure (CE). The results of the study indicate that the choice of fault-proneness modeling technique has limited impact on the resulting classification accuracy or cost-effectiveness. There is however large differences between the individual metric sets in terms of cost-effectiveness, and although the process measures are among the most expensive ones to collect, including them as candidate measures significantly improves the prediction models compared with models that only include structural measures and/or their deltas between releases - both in terms of ROC area and in terms of CE. Further, we observe that what is considered the best model is highly dependent on the criteria that are used to evaluate and compare the models. And the regular confusion matrix criteria, although popular, are not clearly related to the problem at hand, namely the cost-effectiveness of using fault-proneness prediction models to focus verification efforts to deliver software with less faults at less cost.},
journal = {J. Syst. Softw.},
month = jan,
pages = {2–17},
numpages = {16},
keywords = {Verification, Fault prediction models, Cost-effectiveness}
}

@inproceedings{10.1007/978-3-642-02397-2_5,
author = {Cottrell, Marie and Gaubert, Patrice and Eloy, C\'{e}dric and Fran\c{c}ois, Damien and Hallaux, Geoffroy and Lacaille, J\'{e}r\^{o}me and Verleysen, Michel},
title = {Fault Prediction in Aircraft Engines Using Self-Organizing Maps},
year = {2009},
isbn = {9783642023965},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-02397-2_5},
doi = {10.1007/978-3-642-02397-2_5},
abstract = {Aircraft engines are designed to be used during several tens of years. Their maintenance is a challenging and costly task, for obvious security reasons. The goal is to ensure a proper operation of the engines, in all conditions, with a zero probability of failure, while taking into account aging. The fact that the same engine is sometimes used on several aircrafts has to be taken into account too.The maintenance can be improved if an efficient procedure for the prediction of failures is implemented. The primary source of information on the health of the engines comes from measurement during flights. Several variables such as the core speed, the oil pressure and quantity, the fan speed, etc. are measured, together with environmental variables such as the outside temperature, altitude, aircraft speed, etc.In this paper, we describe the design of a procedure aiming at visualizing successive data measured on aircraft engines. The data are multi-dimensional measurements on the engines, which are projected on a self-organizing map in order to allow us to follow the trajectories of these data over time. The trajectories consist in a succession of points on the map, each of them corresponding to the two-dimensional projection of the multi-dimensional vector of engine measurements. Analyzing the trajectories aims at visualizing any deviation from a normal behavior, making it possible to anticipate an operation failure.However rough engine measurements are inappropriate for such an analysis; they are indeed influenced by external conditions, and may in addition vary between engines. In this work, we first process the data by a General Linear Model (GLM), to eliminate the effect of engines and of measured environmental conditions. The residuals are then used as inputs to a Self-Organizing Map for the easy visualization of trajectories.},
booktitle = {Proceedings of the 7th International Workshop on Advances in Self-Organizing Maps},
pages = {37–44},
numpages = {8},
keywords = {self-organizing maps, general linear models, fault detection, aircraft engine maintenance},
location = {St. Augustine, FL, USA},
series = {WSOM '09}
}

@inproceedings{10.1145/3368089.3417062,
author = {Suh, Alexander},
title = {Adapting bug prediction models to predict reverted commits at Wayfair},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417062},
doi = {10.1145/3368089.3417062},
abstract = {Researchers have proposed many algorithms to predict software bugs. Given a software entity (e.g., a file or method), these algorithms predict whether the entity is bug-prone. However, since these algorithms cannot identify specific bugs, this does not tend to be particularly useful in practice. In this work, we adapt this prior work to the related problem of predicting whether a commit is likely to be reverted. Given the batch nature of continuous integration deployment at scale, this allows developers to find time-sensitive bugs in production more quickly. The models in this paper are based on features extracted from the revision history of a codebase that are typically used in bug prediction. Our experiments, performed on the three main repositories for the Wayfair website, show that our models can rank reverted commits above 80% of non-reverted commits on average. Moreover, when given to Wayfair developers, our models reduce the amount of time needed to find certain kinds of bugs by 55%. Wayfair continues to use our findings and models today to help find bugs during software deployments.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1251–1262},
numpages = {12},
keywords = {software deployment, software defect prediction, reverted commits},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3472674.3473983,
author = {Lomio, Francesco and Jurvansuu, Sampsa and Taibi, Davide},
title = {Metrics selection for load monitoring of service-oriented system},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473983},
doi = {10.1145/3472674.3473983},
abstract = {Background. Complex software systems produce a large amount of data depicting their internal state and activities. The data can be monitored to make estimations and predictions of the status of the system, helping taking preventative actions in case of impending malfunctions and failures. However, a complex system may reveal thousands of internal metrics, which makes it a non-trivial task to decide which metrics are the most important to monitor. Objective. In this work we aim at finding a subset of metrics to collect and analyse for the monitoring of the load in a Service-oriented system. Method. We use a performance test bench tool to generate load of different intensities on the target system, which is a specific service-oriented application platform. The numeric metrics data collected from the system is combined with the load intensity at each moment. The combined data is used to analyse which metrics are best at estimating the load of the system. By using a regression analysis it was possible to rank the metrics by their ability to measure the load of the system. Results. The results show that (1) the use of machine learning regressor allows to correctly measure the load of a service-oriented system, and (2) the most important metrics are related to network traffic and request counts, as well as memory usage and disk activity. Conclusion. The results help with the designs of efficient monitoring tool. In addition, further investigation should be focused on exploring more precise machine learning model to further improve the metric selection process.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {31–36},
numpages = {6},
keywords = {Service-Oriented System, Monitoring, Metrics Selection, Machine Learning},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@inproceedings{10.1145/3474624.3477070,
author = {Martins, Luana and Bezerra, Carla and Costa, Heitor and Machado, Ivan},
title = {Smart prediction for refactorings in the software test code},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3477070},
doi = {10.1145/3474624.3477070},
abstract = {Test smells are bad practices to either design or implement a test code. Their presence may reduce the test code quality, harming the software testing activities, primarily from a maintenance perspective. Therefore, defining strategies and tools to handle test smells and improve the test code quality is necessary. State-of-the-art strategies encompass automated support mainly based on hard thresholds of rules, static and dynamic metrics to identify the test smells. Such thresholds are subjective to interpretation and may not consider the complexity of the software projects. Moreover, they are limited as they do not automate test refactoring but only count on developers’ expertise and intuition. In this context, a technique that uses historical implicit or tacit data to generate knowledge could assist the identification and refactoring of test smells. This study aims to establish a novel approach based on machine learning techniques to suggest developers refactoring strategies for test smells. As an expected result, we could understand the applicability of the machine learning techniques to handle test smells and a framework proposal that helps developers in decision-making regarding the refactoring of test smells.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {115–120},
numpages = {6},
keywords = {Test Smells, Software Quality, Machine Learning},
location = {Joinville, Brazil},
series = {SBES '21}
}

@phdthesis{10.5555/AAI29173689,
author = {\"{O}zt\"{u}rk, Muhammed Maruf and Tuncay, Yi̇\u{g}i̇t, and brahim, \c{C}undefinedl, and Ahmet, \"{O}zmen, and Arif, Koyun,},
advisor = {Ahmet, Zengin,},
title = {Tekrar Eden Veri Analizini Kullanarak yaz\i{}l\i{}m geli\c{s}tirme i\c{c}in iyile\c{s}tirilmi\c{s} Hata Tahmini / A New Improved Defect Prediction Framework for Software Development Using Repeated Data Analysis},
year = {2016},
isbn = {9798837582400},
publisher = {Sakarya Universitesi (Turkey)},
abstract = {Yaz\i{}l\i{}m hata tahmini, yo\u{g}un \c{c}aba gerektiren ve yaz\i{}l\i{}m geli\c{s}tirme maliyetlerini azaltmaya odaklanm\i{}\c{s} metotlar\i{}n geli\c{s}tirildi\u{g}i i\c{s}lemleri i\c{c}erir. Veri analizleri i\c{c}in istatistiksel ve makine \"{o}\u{g}renmesi metotlar\i{}n\i{}n s\i{}kl\i{}kla kullan\i{}ld\i{}\u{g}\i{} bu i\c{s}lemler hata veri setlerindeki bozukluk veya eksikliklerden kaynaklanan yanl\i{}\c{s} sonu\c{c}lar\i{} \"{u}retebilmektedir. Bunlara ek olarak s\i{}n\i{}f dengesizli\u{g}i (class imbalance) olarak adland\i{}r\i{}lan hatal\i{} verilerin sistemin belirli b\"{o}lgelerinde yo\u{g}unla\c{s}mas\i{}ndan kaynaklanan sorunlar da ortaya \c{c}\i{}kmaktad\i{}r. Tahmin edici modellerin do\u{g}ruluklar\i{}n\i{} artt\i{}rabilmek i\c{c}in hata veri setlerinin bir \"{o}n i\c{s}lemeden ge\c{c}mesi gereklidir. Ancak bu \c{s}ekilde g\"{u}venilir veriler \"{u}zerinde \c{c}al\i{}\c{s}\i{}labilir. SMOTE, rastgele \"{o}rnekleme gibi y\"{o}ntemlerle hata veri setleri \"{u}zerinde i\c{s}lem yap\i{}lmaktad\i{}r. Bununla beraber hata veri setlerine y\"{o}nelik \"{o}zel bir \"{o}rnekleme tekni\u{g}i bulunmamaktad\i{}r.Tezin amac\i{} b\"{u}y\"{u}k veri setlerinde h\i{}zl\i{} \c{c}al\i{}\c{s}abilen ve tekrar eden verileri y\"{u}ksek do\u{g}rulukla tespit edebilen bir \"{o}n-i\c{s}leme algoritmas\i{} geli\c{s}tirmektir. Algoritma, metrik t\"{u}retimi ile ilgili i\c{s}lemleri kapsar. Bir di\u{g}er ama\c{c} ise algoritman\i{}n makine \"{o}\u{g}renmesi alan\i{}nda kullan\i{}labilecek hata veri setlerine y\"{o}nelik \"{o}zel bir y\"{o}ntem geli\c{s}tirmektir. Deney sonu\c{c}lar\i{}nda g\"{o}zlenen kesinlik de\u{g}erleri literart\"{u}deki di\u{g}er \"{o}n-i\c{s}leme y\"{o}ntemlerinden \"{u}st\"{u}n oldu\u{g}u takdirde makine \"{o}\u{g}renmesi ikili s\i{}n\i{}fland\i{}rma veri setlerinde de kullan\i{}labilir. Hata kesinli\u{g}i y\"{o}ntemle birlikte artt\i{}\u{g}\i{} i\c{c}in yaz\i{}l\i{}m geli\c{s}tirme maliyetlerinin azalt\i{}lmas\i{}na katk\i{} sa\u{g}lamaktad\i{}r. Veri madencili\u{g}i verilerinin temizlenmesi i\c{c}in geli\c{s}tirilecek y\"{o}ntemlere algoritman\i{}n y\"{o}n vermesi beklenmektedir. Y\"{o}ntem, ANOVA, t-test, ki-kare gibi istatistik tabanl\i{} metotlar kullanarak ikili s\i{}n\i{}fland\i{}rma verilerinde tekrar eden verilerin ortadan kald\i{}r\i{}lmas\i{}n\i{} sa\u{g}layan bir veri temizleme algoritmas\i{}n\i{} kapsar. Buna ek olarak d\"{u}\c{s}\"{u}k seviyeli metrik t\"{u}retiminin \"{o}\u{g}renme algoritmalar\i{}n\i{}n ba\c{s}ar\i{}s\i{}na etkisi g\"{o}zlemlenmi\c{s}tir.Kullan\i{}lan makine \"{o}\u{g}renmesi y\"{o}ntemleri ve istatistiksel i\c{s}lemler ile \"{o}nerilen \"{o}n-i\c{s}leme algoritmas\i{}n\i{} da i\c{c}eren bir yaz\i{}l\i{}m \c{c}er\c{c}evesi C# programlama dili kullan\i{}larak geli\c{s}tirilmi\c{s}tir. Bu \c{c}er\c{c}eve \c{c}e\c{s}itli formattaki hata veri setleri \"{u}zerinde ikili s\i{}n\i{}fland\i{}rma performans analizlerini ve temel istatistiksel i\c{s}lemleri de yapabilmektedir. \"{O}nerilen y\"{o}ntem 15 end\"{u}striyel ve 5 a\c{c}\i{}k kaynak olmak \"{u}zere toplamda 20 adet yaz\i{}l\i{}m proje veri seti \"{u}zerinde denenmi\c{s} e\u{g}rinin alt\i{}nda kalan alan (auc) ve kesinlik (precision) performans parametrelerinde algoritman\i{}n etkisi d\"{o}rt farkl\i{} s\i{}n\i{}land\i{}r\i{}c\i{} kullan\i{}larak \"{o}l\c{c}\"{u}lm\"{u}\c{s}t\"{u}r. Geli\c{s}tirilen algoritma hata veri setlerinde tekrar eden veri setlerinin elenmesine y\"{o}nelik geli\c{s}tirilen ilk algoritmad\i{}r. Anahtar kelimeler: Hata tahmini, yaz\i{}l\i{}m metrikleri, yaz\i{}l\i{}m kalitesi, makine \"{o}\u{g}renmesi.},
note = {AAI29173689}
}

@inproceedings{10.1145/1295074.1295080,
author = {Ostrand, Thomas J. and Weyuker, Elaine J.},
title = {How to measure success of fault prediction models},
year = {2007},
isbn = {9781595937247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1295074.1295080},
doi = {10.1145/1295074.1295080},
abstract = {Many fault prediction models have been proposed in the software engineering literature, and their success evaluated according to various metrics that are widely used in the statistics community. To be able to make meaningful comparisons among the proposed models, it is important that the metrics assess meaningful properties of the predictions. We examine several of the more common metrics, discuss the advantages and disadvantages of each, and illustrate their application to predictions made on a large industrial system. We conclude that the most useful metrics are the percentage of faults that occur in the predicted most fault-prone files, and the Type II misclassification rate.},
booktitle = {Fourth International Workshop on Software Quality Assurance: In Conjunction with the 6th ESEC/FSE Joint Meeting},
pages = {25–30},
numpages = {6},
location = {Dubrovnik, Croatia},
series = {SOQUA '07}
}

@article{10.1049/iet-sen.2013.0046,
author = {Ahmed, Moataz A. and Al-Jamimi, Hamdi A.},
title = {Machine learning approaches for predicting software maintainability: a fuzzy‐based transparent model},
year = {2013},
issue_date = {December 2013},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {7},
number = {6},
url = {https://doi.org/10.1049/iet-sen.2013.0046},
doi = {10.1049/iet-sen.2013.0046},
abstract = {Software quality is one of the most important factors for assessing the global competitive position of any software company. Thus, the quantification of the quality parameters and integrating them into the quality models is very essential.Many attempts have been made to precisely quantify the software quality parameters using various models such as Boehm's Model, McCall's Model and ISO/IEC 9126 Quality Model. A major challenge, although, is that effective quality models should consider two types of knowledge: imprecise linguistic knowledge from the experts and precise numerical knowledge from historical data.Incorporating the experts’ knowledge poses a constraint on the quality model; the model has to be transparent.In this study, the authorspropose a process for developing fuzzy logic‐based transparent quality prediction models.They applied the process to a case study where Mamdani fuzzy inference engine is used to predict software maintainability.Theycompared the Mamdani‐based model with other machine learning approaches.The resultsshow that the Mamdani‐based model is superior to all.},
journal = {IET Software},
month = dec,
pages = {317–326},
numpages = {10},
keywords = {Mamdani-based model, Mamdani fuzzy inference engine, fuzzy logic-based transparent quality prediction model, imprecise linguistic knowledge, software company, software quality parameter quantification, software maintainability prediction, machine learning approach, software quality, software maintenance, learning (artificial intelligence), fuzzy reasoning, fuzzy logic, DP industry, computational linguistics}
}

@inproceedings{10.1145/1294948.1294953,
author = {Bernstein, Abraham and Ekanayake, Jayalath and Pinzger, Martin},
title = {Improving defect prediction using temporal features and non linear models},
year = {2007},
isbn = {9781595937223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294948.1294953},
doi = {10.1145/1294948.1294953},
abstract = {Predicting the defects in the next release of a large software system is a very valuable asset for the project manger to plan her resources. In this paper we argue that temporal features (or aspects) of the data are central to prediction performance. We also argue that the use of non-linear models, as opposed to traditional regression, is necessary to uncover some of the hidden interrelationships between the features and the defects and maintain the accuracy of the prediction in some cases.Using data obtained from the CVS and Bugzilla repositories of the Eclipse project, we extract a number of temporal features, such as the number of revisions and number of reported issues within the last three months. We then use these data to predict both the location of defects (i.e., the classes in which defects will occur) as well as the number of reported bugs in the next month of the project. To that end we use standard tree-based induction algorithms in comparison with the traditional regression.Our non-linear models uncover the hidden relationships between features and defects, and present them in easy to understand form. Results also show that using the temporal features our prediction model can predict whether a source file will have a defect with an accuracy of 99% (area under ROC curve 0.9251) and the number of defects with a mean absolute error of 0.019 (Spearman's correlation of 0.96).},
booktitle = {Ninth International Workshop on Principles of Software Evolution: In Conjunction with the 6th ESEC/FSE Joint Meeting},
pages = {11–18},
numpages = {8},
keywords = {mining software repository, defect prediction, decision tree learner},
location = {Dubrovnik, Croatia},
series = {IWPSE '07}
}

@inproceedings{10.1109/ITNG.2009.12,
author = {Catal, Cagatay and Sevim, Ugur and Diri, Banu},
title = {Clustering and Metrics Thresholds Based Software Fault Prediction of Unlabeled Program Modules},
year = {2009},
isbn = {9780769535968},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ITNG.2009.12},
doi = {10.1109/ITNG.2009.12},
abstract = {Predicting the fault-proneness of program modules when the fault labels for modules are unavailable is a practical problem frequently encountered in the software industry. Because fault data belonging to previous software version is not available, supervised learning approaches can not be applied, leading to the need for new methods, tools, or techniques. In this study, we propose a clustering and metrics thresholds based software fault prediction approach for this challenging problem and explore it on three datasets, collected from a Turkish white-goods manufacturer developing embedded controller software. Experiments reveal that unsupervised software fault prediction can be automated and reasonable results can be produced with techniques based on metrics thresholds and clustering. The results of this study demonstrate the effectiveness of metrics thresholds and show that the standalone application of metrics thresholds (one-stage) is currently easier than the clustering and metrics thresholds based (two-stage) approach because the selection of cluster number is performed heuristically in this clustering based method.},
booktitle = {Proceedings of the 2009 Sixth International Conference on Information Technology: New Generations},
pages = {199–204},
numpages = {6},
keywords = {unsupervised learning, unlabeled program modules, software fault prediction, metrics thresholds, clustering},
series = {ITNG '09}
}

@inproceedings{10.5555/1566864.1566872,
author = {Hietaniemi, Manu and Elsil\"{a}, Ulla and Laurinen, Perttu and R\"{o}ning, Juha},
title = {Defect Prediction in Hot Strip Rolling Using ANN and SVM},
year = {2008},
isbn = {9781586038670},
publisher = {IOS Press},
address = {NLD},
abstract = {One of the largest factors affecting the loss for steel manufacturing are defects in the steel strips produced. Therefore the prediction of these defects forehand would be very important. In this study we used classifiers --feedforward neural networks and a support vector machine --to solve this problem. We also used different kinds of feature selection methods such as a preprocessing step for the classifiers. As a result, these two classifiers confirmed the same grade of classification error in this study.},
booktitle = {Proceedings of the 2008 Conference on Tenth Scandinavian Conference on Artificial Intelligence: SCAI 2008},
pages = {44–51},
numpages = {8},
keywords = {Support Vector Machine, Neural Networks, Hot Steel Rolling, Feature Selection, Classification}
}

@inproceedings{10.1109/SERA.2007.41,
author = {Lee, Soon-Bok and Kim, Jin-Woo and Song, Chee-Yang and Baik, Doo-Kwon},
title = {An Approach to Analyzing Commonality and Variability of Features using Ontology in a Software Product Line Engineering},
year = {2007},
isbn = {0769528678},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SERA.2007.41},
doi = {10.1109/SERA.2007.41},
abstract = {In a product line engineering, several studies have been made on analysis of feature which determines commonality and variability of product. Fundamentally, because the studies are based on developer's intuition and domain expert's experience, stakeholders lack common understanding of feature and a feature analysis is informal and subjective. Moreover, the reusability of software products, which were developed, is insufficient. This paper proposes an approach to analyzing commonality and variability of features using semantic-based analysis criteria which is able to change feature model of specific domain to featureontology. For the purpose, first feature attributes were made, create a feature model following the Meta model, transform it into feature-ontology, and save it to Meta feature-ontology repository. Henceforth, when we construct a feature model of the same product line, commonality and variability of the features can be extracted, comparing it with Meta feature ontology through a semantic similarity analysis method, which is proposed. Furthermore, a tool for a semantic similarity-comparing algorithm was implemented and an experiment with an electronic approval system domain in order to show the efficiency of the approach Was conducted. A Meta feature model can definitely be created through this approach, to construct a high-quality feature model based on common understanding of a feature. The main contributions are a formulating a method of extracting commonality and variability from features using ontology based on semantic similarity mapping and a enhancement of reusability of feature model.},
booktitle = {Proceedings of the 5th ACIS International Conference on Software Engineering Research, Management &amp; Applications},
pages = {727–734},
numpages = {8},
series = {SERA '07}
}

@inproceedings{10.1145/3410352.3410730,
author = {BenIdris, Mrwan and Ammar, Hany and Dzielski, Dale and Benamer, Wisam H.},
title = {Prioritizing Software Components Risk: Towards a Machine Learning-based Approach},
year = {2020},
isbn = {9781450377362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410352.3410730},
doi = {10.1145/3410352.3410730},
abstract = {Technical Debt (TD) can be detected using different methods. TD is a metaphor that refers to short-term solutions in software development, which may affect the cost of the software development life-cycle. Several tools have been developed to detect, estimate, or manage TD. TD can be indicated through smells, code comments, and software metrics. Machine learning Techniques (MLTs) are used in many software engineering topics such as fault-proneness, bug severity, and code smell. In this paper we use four internal structure metrics to identify and classify Architecture Technical Debt (ATD) risk by using MLTs. We show that MLTs can identify and classify the risk of ATD on software components to help the decision-makers to prioritizing the refactoring decisions based on the level of the risk.},
booktitle = {Proceedings of the 6th International Conference on Engineering &amp; MIS 2020},
articleno = {1},
numpages = {11},
keywords = {Architecture Smells, Architecture Technical Debt, Machine Learning, Software Risk},
location = {Almaty, Kazakhstan},
series = {ICEMIS'20}
}

@inproceedings{10.1109/APSEC.2012.14,
author = {Zhou, Hongbo and Jin, Dahai and Gong, Yunzhan},
title = {An Interval-Based Model for Detecting Software Defect Using Alias Analysis},
year = {2012},
isbn = {9780769549224},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2012.14},
doi = {10.1109/APSEC.2012.14},
abstract = {Alias analysis is a branch of static program analysis aiming at computing variables which are alias of each other. It is a basis research for many analyses and optimizations in software engineering and compiler construction. Precise modeling of alias analysis is fundamental for software analysis. This paper presents two practical approximation models for representing and computing alias: memory-sensitive model (MSM) and value-sensitive model (VSM). Based on defect-oriented detecting, we present a method to detect software defect using VSM and MSM, which realizes inter-procedure detecting by procedure summary. According to whether type of analysis object coming from defect is value-sensitive or memory-sensitive, we propose two detecting algorithms based on two alias models respectively. One is for memory leak (ML) based on MSM, and the other is for invalid arithmetic operation (IAO) based on VSM. We apply a defect testing system (DTS) to detect six C++ open source projects for proving our models effectiveness. Experimental results show that applying our technique to detect IAO and ML defect can improve detecting efficiency, at the same time reduce potential false positives and false negatives.},
booktitle = {Proceedings of the 2012 19th Asia-Pacific Software Engineering Conference - Volume 02},
pages = {136–144},
numpages = {9},
keywords = {static analysis, procedure summary, interval computation, defect detection, alias analysis},
series = {APSEC '12}
}

@article{10.1016/j.cie.2011.12.023,
author = {Alzghoul, Ahmad and L\"{o}fstrand, Magnus and Backe, Bj\"{o}rn},
title = {Data stream forecasting for system fault prediction},
year = {2012},
issue_date = {May, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {62},
number = {4},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2011.12.023},
doi = {10.1016/j.cie.2011.12.023},
abstract = {Competition among today's industrial companies is very high. Therefore, system availability plays an important role and is a critical point for most companies. Detecting failures at an early stage or foreseeing them before they occur is crucial for machinery availability. Data analysis is the most common method for machine health condition monitoring. In this paper we propose a fault-detection system based on data stream prediction, data stream mining, and data stream management system (DSMS). Companies that are able to predict and avoid the occurrence of failures have an advantage over their competitors. The literature has shown that data prediction can also reduce the consumption of communication resources in distributed data stream processing. In this paper different data-stream-based linear regression prediction methods have been tested and compared within a newly developed fault detection system. Based on the fault detection system, three DSM algorithms outputs are compared to each other and to real data. The three applied and evaluated data stream mining algorithms were: Grid-based classifier, polygon-based method, and one-class support vector machines (OCSVM). The results showed that the linear regression method generally achieved good performance in predicting short-term data. (The best achieved performance was with a Mean Absolute Error (MAE) around 0.4, representing prediction accuracy of 87.5%). Not surprisingly, results showed that the classification accuracy was reduced when using the predicted data. However, the fault-detection system was able to attain an acceptable performance of around 89% classification accuracy when using predicted data.},
journal = {Comput. Ind. Eng.},
month = may,
pages = {972–978},
numpages = {7},
keywords = {Fault detection system, Fault detection forecasting, Data stream prediction, Data stream mining, Data stream management system, Availability}
}

@article{10.1016/j.future.2018.09.053,
author = {Cecchinel, Cyril and Fouquet, Fran\c{c}ois and Mosser, S\'{e}bastien and Collet, Philippe},
title = {Leveraging live machine learning and deep sleep to support a self-adaptive efficient configuration of battery powered sensors},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.053},
doi = {10.1016/j.future.2018.09.053},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {225–240},
numpages = {16}
}

@article{10.1016/j.ins.2008.10.005,
author = {Quah, Tong-Seng},
title = {Estimating software readiness using predictive models},
year = {2009},
issue_date = {February, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {179},
number = {4},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2008.10.005},
doi = {10.1016/j.ins.2008.10.005},
abstract = {In this study, defect tracking is used as a proxy method to predict software readiness. The number of remaining defects in an application under development is one of the most important factors that allow one to decide if a piece of software is ready to be released. By comparing predicted number of faults and number of faults discovered in testing, software manager can decide whether the software is likely ready to be released or not. The predictive model developed in this research can predict: (i) the number of faults (defects) likely to exist, (ii) the estimated number of code changes required to correct a fault and (iii) the estimated amount of time (in minutes) needed to make the changes in respective classes of the application. The model uses product metrics as independent variables to do predictions. These metrics are selected depending on the nature of source code with regards to architecture layers, types of faults and contribution factors of these metrics. The use of neural network model with genetic training strategy is introduced to improve prediction results for estimating software readiness in this study. This genetic-net combines a genetic algorithm with a statistical estimator to produce a model which also shows the usefulness of inputs. The model is divided into three parts: (1) prediction model for presentation logic tier (2) prediction model for business tier and (3) prediction model for data access tier. Existing object-oriented metrics and complexity software metrics are used in the business tier prediction model. New sets of metrics have been proposed for the presentation logic tier and data access tier. These metrics are validated using data extracted from real world applications. The trained models can be used as tools to assist software mangers in making software release decisions.},
journal = {Inf. Sci.},
month = feb,
pages = {430–445},
numpages = {16},
keywords = {Software readiness, Software metrics, Presentation logic tier, Prediction, Neural network, Genetic net, Defect tracking, Data access tier, Business tier}
}

@inproceedings{10.1145/2693561.2693564,
author = {Perez, Juan F. and Wang, Weikun and Casale, Giuliano},
title = {Towards a DevOps Approach for Software Quality Engineering},
year = {2015},
isbn = {9781450333405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2693561.2693564},
doi = {10.1145/2693561.2693564},
abstract = {DevOps is a novel trend in software engineering that aims at bridging the gap between development and operations, putting in particular the developer in greater control of deployment and application runtime. Here we consider the problem of designing a tool capable of providing feedback to the developer on the performance, reliability, and in general quality characteristics of the application at runtime. This raises a number of questions related to what measurement information should be carried back from runtime to design-time and what degrees of freedom should be provided to the developer in the evaluation of performance data. To answer these questions, we describe the design of a filling-the-gap (FG) tool, a software system capable of automatically analyzing performance data either directly or through statistical inference. A natural application of the FG tool is the continuous training of stochastic performance models, such as layered queueing networks, that can inform developers on how to refactor the software architecture.},
booktitle = {Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development},
pages = {5–10},
numpages = {6},
keywords = {software performance engineering, quality of service, monitoring, design-time application models},
location = {Austin, Texas, USA},
series = {WOSP '15}
}

@inproceedings{10.1145/1083165.1083172,
author = {Koru, A. G\"{u}nes and Liu, Hongfang},
title = {An investigation of the effect of module size on defect prediction using static measures},
year = {2005},
isbn = {1595931252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083165.1083172},
doi = {10.1145/1083165.1083172},
abstract = {We used several machine learning algorithms to predict the defective modules in five NASA products, namely, CM1, JM1, KC1, KC2, and PC1. A set of static measures were employed as predictor variables. While doing so, we observed that a large portion of the modules were small, as measured by lines of code (LOC). When we experimented on the data subsets created by partitioning according to module size, we obtained higher prediction performance for the subsets that include larger modules. We also performed defect prediction using class-level data for KC1 rather than the method-level data. In this case, the use of class-level data resulted in improved prediction performance compared to using method-level data. These findings suggest that quality assurance activities can be guided even better if defect prediction is performed by using data that belong to larger modules.},
booktitle = {Proceedings of the 2005 Workshop on Predictor Models in Software Engineering},
pages = {1–5},
numpages = {5},
keywords = {defect prediction, prediction models, software metrics, software quality management, static measures},
location = {St. Louis, Missouri},
series = {PROMISE '05}
}

@inproceedings{10.1145/3416505.3423559,
author = {Lujan, Savanna and Pecorelli, Fabiano and Palomba, Fabio and De Lucia, Andrea and Lenarduzzi, Valentina},
title = {A preliminary study on the adequacy of static analysis warnings with respect to code smell prediction},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423559},
doi = {10.1145/3416505.3423559},
abstract = {Code smells are poor implementation choices applied during software evolution that can affect source code maintainability. While several heuristic-based approaches have been proposed in the past, machine learning solutions have recently gained attention since they may potentially address some limitations of state-of-the-art approaches. Unfortunately, however, machine learning-based code smell detectors still suffer from low accuracy. In this paper, we aim at advancing the knowledge in the field by investigating the role of static analysis warnings as features of machine learning models for the detection of three code smell types. We first verify the potential contribution given by these features. Then, we build code smell prediction models exploiting the most relevant features coming from the first analysis. The main finding of the study reports that the warnings given by the considered tools lead the performance of code smell prediction models to drastically increase with respect to what reported by previous research in the field.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {1–6},
numpages = {6},
keywords = {Static Analysis Tools, Machine Learning, Code Smells},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@article{10.1145/2632434.2659853,
author = {Goyal, Rinkaj and Chandra, Pravin and Singh, Yogesh},
title = {Why interaction between metrics should be considered in the development of software quality models: a preliminary study},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/2632434.2659853},
doi = {10.1145/2632434.2659853},
abstract = {This study examines the need to consider interactions between the measurements (metrics) of different quality factors in the development of software quality models. Though the correlation between metrics has been explored to a considerable depth in the development of these models, consideration of interactions between predictors is comparatively new in software engineering. This preliminary study is supported by statistically-proven results, differentiating interactions with correlation analysis.The issues raised here will assist analysts to improve empirical analyses by incorporating interactions in software quality model development, where amalgamating effects between different characteristics or subcharacteristics are observed.},
journal = {SIGSOFT Softw. Eng. Notes},
month = aug,
pages = {1–4},
numpages = {4},
keywords = {software fault prediction models, regression analysis, quality models, metrics, interaction, empirical software engineering}
}

@article{10.1016/j.infsof.2010.03.013,
author = {Sahraoui, Houari and Briand, Lionel C. and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Beaurepaire, Olivier},
title = {Investigating the impact of a measurement program on software quality},
year = {2010},
issue_date = {September, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.03.013},
doi = {10.1016/j.infsof.2010.03.013},
abstract = {Context: Measurement programs have been around for several decades but have been often misused or misunderstood by managers and developers. This misunderstanding prevented their adoption despite their many advantages. Objective: In this paper, we present the results of an empirical study on the impact of a measurement program, MQL (''Mise en Qualite du Logiciel'', French for ''Quality Software Development''), in an industrial context. Method: We analyzed data collected on 44 industrial systems of different sizes: 22 systems were developed using MQL while the other 22 used ad-hoc approaches to assess and control quality (control group, referred to as ''ad-hoc systems''). We studied the impact of MQL on a set of nine variables: six quality factors (maintainability, evolvability, reusability, robustness, testability, and architecture quality), corrective-maintenance effort, code complexity, and the presence of comments. Results: Our results show that MQL had a clear positive impact on all the studied indicators. This impact is statistically significant for all the indicators but corrective-maintenance effort. Conclusion: We bring concrete evidence that a measurement program can have a significant, positive impact on the quality of software systems if combined with appropriate decision making procedures and corrective actions.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {923–933},
numpages = {11},
keywords = {Software quality, Measurement program, Empirical study}
}

@article{10.1504/ijaip.2019.101983,
author = {Kumar, Reddi Kiran and Rao, S.V. Achuta},
title = {Severity of defect: an optimised prediction},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {3–4},
issn = {1755-0386},
url = {https://doi.org/10.1504/ijaip.2019.101983},
doi = {10.1504/ijaip.2019.101983},
abstract = {To assure the quality of software an important activity is performed namely software defect prediction (SDP). Historical databases are used to detect software defects using different machine learning techniques. Conversely, there are disadvantages like testing becomes expensive, poor quality and so the product is unreliable for use. This paper classifies the severity of defects by using a method based on optimised neural network (NN). In full search space, a solution is found by many meta-heuristic optimisations and global search ability has been used. Hence, high-quality solutions are finding within a reasonable period of time. SDP performance is improved by the combination of meta-heuristic optimisation methods. For class imbalance problem, meta-heuristic optimisation methods such as genetic algorithm (GA) and shuffled frog leaping algorithm (SFLA) are applied. The above method is based on SFLA and the experimental outputs show that it can do better than Leven berg Marquardt based NN system (LM-NN).},
journal = {Int. J. Adv. Intell. Paradigms},
month = jan,
pages = {334–345},
numpages = {11},
keywords = {shuffled frog and fuzzy classifier, LM, Levenberg Marquardt, neural network, severity, SDP, software defect prediction}
}

@inproceedings{10.1007/978-3-540-69566-0_21,
author = {Catal, Cagatay and Diri, Banu},
title = {A Fault Prediction Model with Limited Fault Data to Improve Test Process},
year = {2008},
isbn = {9783540695646},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-69566-0_21},
doi = {10.1007/978-3-540-69566-0_21},
abstract = {Software fault prediction models are used to identify the fault-prone software modules and produce reliable software. Performance of a software fault prediction model is correlated with available software metrics and fault data. In some occasions, there may be few software modules having fault data and therefore, prediction models using only labeled data can not provide accurate results. Semi-supervised learning approaches which benefit from unlabeled and labeled data may be applied in this case. In this paper, we propose an artificial immune system based semi-supervised learning approach. Proposed approach uses a recent semi-supervised algorithm called YATSI (Yet Another Two Stage Idea) and in the first stage of YATSI, AIRS (Artificial Immune Recognition Systems) is applied. In addition, AIRS, RF (Random Forests) classifier, AIRS based YATSI, and RF based YATSI are benchmarked. Experimental results showed that while YATSI algorithm improved the performance of AIRS, it diminished the performance of RF for unbalanced datasets. Furthermore, performance of AIRS based YATSI is comparable with RF which is the best machine learning classifier according to some researches.},
booktitle = {Proceedings of the 9th International Conference on Product-Focused Software Process Improvement},
pages = {244–257},
numpages = {14},
keywords = {software fault prediction, artificial immune systems, YATSI, Semi-supervised learning, AIRS},
location = {Monte Porzio Catone, Italy},
series = {PROFES '08}
}

@inproceedings{10.1145/2915970.2915979,
author = {Petri\'{c}, Jean},
title = {Using different characteristics of machine learners to identify different defect families},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2915979},
doi = {10.1145/2915970.2915979},
abstract = {Background: Software defect prediction has been an active area of research for the last few decades. Many models have been developed with aim to find locations in code likely to contain defects. As of yet, these prediction models are of limited use and rarely used in the software industry.Problem: Current modelling techniques are too coarse grained and fail in finding some defects. Most of the prediction models do not look for targeted defect characteristics, but rather treat them as a black box and homogeneous. No study has investigated in greater detail how well certain defect characteristics work with different prediction modelling techniques.Methodology: This PhD will address three major tasks. First, the relation among software defects, prediction models and static code metrics will be analysed. Second, the possibility of a mapping function between prediction models and defect characteristics shall be investigated. Third, an optimised ensemble model that searches for targeted defects will be developed.Contribution: A few contributions will yield from this work. Characteristics of defects will be identified, allowing other researchers to build on this work to produce more efficient prediction models in future. New modelling techniques that better suit state-of-the-art knowledge in defect prediction shall be designed. Such prediction models should be transformed in a tool that can be used by our industrial collaborator in the real industry environment.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {5},
numpages = {4},
keywords = {software defect prediction, prediction modeling, machine learning},
location = {Limerick, Ireland},
series = {EASE '16}
}

@article{10.1016/j.cie.2021.107580,
author = {Ma, Qiuping and Li, Hongyan and Thorstenson, Anders},
title = {A big data-driven root cause analysis system: Application of Machine Learning in quality problem solving},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {160},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107580},
doi = {10.1016/j.cie.2021.107580},
journal = {Comput. Ind. Eng.},
month = oct,
numpages = {16},
keywords = {Neural Network, Multi-class classification, Machine Learning, Data mining, Quality management}
}

@inproceedings{10.1145/3416505.3423564,
author = {Borovits, Nemania and Kumara, Indika and Krishnan, Parvathy and Palma, Stefano Dalla and Di Nucci, Dario and Palomba, Fabio and Tamburri, Damian A. and van den Heuvel, Willem-Jan},
title = {DeepIaC: deep learning-based linguistic anti-pattern detection in IaC},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423564},
doi = {10.1145/3416505.3423564},
abstract = {Linguistic anti-patterns are recurring poor practices concerning inconsistencies among the naming, documentation, and implementation of an entity. They impede readability, understandability, and maintainability of source code. This paper attempts to detect linguistic anti-patterns in infrastructure as code (IaC) scripts used to provision and manage computing environments. In particular, we consider inconsistencies between the logic/body of IaC code units and their names. To this end, we propose a novel automated approach that employs word embeddings and deep learning techniques. We build and use the abstract syntax tree of IaC code units to create their code embedments. Our experiments with a dataset systematically extracted from open source repositories show that our approach yields an accuracy between 0.785 and 0.915 in detecting inconsistencies.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {Word2Vec, Linguistic Anti-patterns, Infrastructure Code, IaC, Defects, Deep Learning, Code Embedding},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@article{10.1007/s11219-020-09546-7,
author = {Oyetoyan, Tosin Daniel and Morrison, Patrick},
title = {An improved text classification modelling approach to identify security messages in heterogeneous projects},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09546-7},
doi = {10.1007/s11219-020-09546-7},
abstract = {Security remains under-addressed in many organisations, illustrated by the number of large-scale software security breaches. Preventing breaches can begin during software development if attention is paid to security during the software’s design and implementation. One approach to security assurance during software development is to examine communications between developers as a means of studying the security concerns of the project. Prior research has investigated models for classifying project communication messages (e.g., issues or commits) as security related or not. A known problem is that these models are project-specific, limiting their use by other projects or organisations. We investigate whether we can build a generic classification model that can generalise across projects. We define a set of security keywords by extracting them from relevant security sources, dividing them into four categories: asset, attack/threat, control/mitigation, and implicit. Using different combinations of these categories and including them in the training dataset, we built a classification model and evaluated it on industrial, open-source, and research-based datasets containing over 45 different products. Our model based on harvested security keywords as a feature set shows average recall from 55 to 86%, minimum recall from 43 to 71% and maximum recall from 60 to 100%. An average f-score between 3.4 and 88%, an average g-measure of at least 66% across all the dataset, and an average AUC of ROC from 69 to 89%. In addition, models that use externally sourced features outperformed models that use project-specific features on average by a margin of 26–44% in recall, 22–50% in g-measure, 0.4–28% in f-score, and 15–19% in AUC of ROC. Further, our results outperform a state-of-the-art prediction model for security bug reports in all cases. We find using sound statistical and effect size tests that (1) using harvested security keywords as features to train a text classification model improve classification models and generalise to other projects significantly. (2) Including features in the training dataset before model construction improve classification models significantly. (3) Different security categories represent predictors for different projects. Finally, we introduce new and promising approaches to construct models that can generalise across different independent projects.},
journal = {Software Quality Journal},
month = jun,
pages = {509–553},
numpages = {45},
keywords = {Machine learning, Software repository, Text classification, Classification model, Security}
}

@inproceedings{10.5555/1627368.1627435,
author = {Podgorelec, Vili},
title = {On software fault prediction by mining software complexity data with dynamically filtered training sets},
year = {2009},
isbn = {9789604741137},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Software fault prediction methods are very appropriate for improving the software reliability. With the creation of large empirical databases of software projects, as a result of stimulated research on estimation models, metrics and methods for measuring and improving processes and products, intelligent mining of these datasets can largely add to the improvement of software reliability. In the paper we present a study on using decision tree classifiers for predicting software faults. A new training set filtering method is presented that should improve the classification performance when mining the software complexity measures data. The classification improvement should be achieved by removing the identified outliers from a training set. We argue that a classifier trained by a filtered dataset captures a more general knowledge model and should therefore perform better also on unseen cases. The proposed method is applied on a real-world software reliability analysis dataset and the obtained results are discussed.},
booktitle = {Proceedings of the 9th WSEAS International Conference on Simulation, Modelling and Optimization},
pages = {332–337},
numpages = {6},
keywords = {software fault prediction, search-based software engineering, filtering training set, complexity metrics, classification},
location = {Budapest, Hungary},
series = {SMO'09}
}

@inproceedings{10.1007/978-3-642-15880-3_33,
author = {Eichinger, Frank and Krogmann, Klaus and Klug, Roland and B\"{o}hm, Klemens},
title = {Software-defect localisation by mining dataflow-enabled call graphs},
year = {2010},
isbn = {364215879X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-15880-3_33},
doi = {10.1007/978-3-642-15880-3_33},
abstract = {Defect localisation is essential in software engineering and is an important task in domain-specific data mining. Existing techniques building on call-graph mining can localise different kinds of defects. However, these techniques focus on defects that affect the controlflow and are agnostic regarding the dataflow. In this paper, we introduce dataflow-enabled call graphs that incorporate abstractions of the dataflow. Building on these graphs, we present an approach for defect localisation. The creation of the graphs and the defect localisation are essentially data mining problems, making use of discretisation, frequent subgraph mining and feature selection. We demonstrate the defect-localisation qualities of our approach with a study on defects introduced into Weka. As a result, defect localisation now works much better, and a developer has to investigate on average only 1.5 out of 30 methods to fix a defect.},
booktitle = {Proceedings of the 2010th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part I},
pages = {425–441},
numpages = {17},
location = {Barcelona, Spain},
series = {ECMLPKDD'10}
}

@article{10.1016/j.ins.2013.04.025,
author = {Yan, Aijun and Wang, Weixian and Zhang, Chunxiao and Zhao, Hui},
title = {A fault prediction method that uses improved case-based reasoning to continuously predict the status of a shaft furnace},
year = {2014},
issue_date = {February, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {259},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2013.04.025},
doi = {10.1016/j.ins.2013.04.025},
abstract = {For the problem of predicting faults in the status of a shaft furnace, the missed alarm rate and false alarm rate have not been improved significantly by the traditional case-based reasoning (CBR) method. To predict faults more accurately, an improved CBR-based fault prediction method (ICBRP) is proposed in this paper. This ICBRP is composed of a water-filling theory-based weight allocation (WFA) model and a group decision-making-based revision (GDMR) model. According to the optimal allocation mechanism of channel power, a Lagrange function is designed to calculate the weights. Moreover, the credibility of historical results is used to revise the predicted results via the definition of a group utility function. Then, the proposed reasoning strategy can obtain more reasonable weights and take full advantage of comprehensive information from the retrieval results. Finally, the application results indicate that the proposed method is superior to traditional CBR and other methods. This proposed ICBRP significantly reduces the missed alarm rate and the false alarm rate of failure in the furnace status.},
journal = {Inf. Sci.},
month = feb,
pages = {269–281},
numpages = {13},
keywords = {Shaft furnace status, Group decision-making, Fault prediction, Case-based reasoning}
}

@article{10.1049/iet-sen.2013.0008,
author = {Shatnawi, Raed},
title = {Empirical study of fault prediction for open‐source systems using the Chidamber and Kemerer metrics},
year = {2014},
issue_date = {June 2014},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1049/iet-sen.2013.0008},
doi = {10.1049/iet-sen.2013.0008},
abstract = {Software testers are usually provoked with projects that have faults. Predicting a class's fault‐proneness is vital for minimising cost and improving the effectiveness of the software testing. Previous research on software metrics has shown strong relationships between software metrics and faults in object‐oriented systems using a binary variable. However, these models do not consider the history of faults in classes. In this work, a dependent variable is proposed that uses fault history to rate classes into four categories (none, low risk, medium risk and high risk) and to improve the predictive capability of fault models. The study is conducted on many releases of four open‐source systems. The study tests the statistical differences in seven machine learning algorithms to find whether the proposed variable can be used to build better prediction models. The performance of the classifiers using the four categories is significantly better than the binary variable. In addition, the results show improvements on the reliability of the prediction models as the software matures. Therefore the fault history improves the prediction of fault‐proneness of classes in open‐source systems.},
journal = {IET Software},
month = jun,
pages = {113–119},
numpages = {7},
keywords = {machine learning algorithms, statistical differences, binary variable, object-oriented systems, software metrics, software testing, software testers, Chidamber metrics, Kemerer metrics, open-source systems, fault prediction, software metrics, public domain software, program testing, object-oriented methods, learning (artificial intelligence)}
}

@inproceedings{10.1145/2568225.2568307,
author = {Lee, Sangho and Jung, Changhee and Pande, Santosh},
title = {Detecting memory leaks through introspective dynamic behavior modelling using machine learning},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568307},
doi = {10.1145/2568225.2568307},
abstract = {This paper expands staleness-based memory leak detection by presenting a machine learning-based framework. The proposed framework is based on an idea that object staleness can be better leveraged in regard to similarity of objects; i.e., an object is more likely to have leaked if it shows significantly high staleness not observed from other similar objects with the same allocation context.  A central part of the proposed framework is the modeling of heap objects. To this end, the framework observes the staleness of objects during a representative run of an application. From the observed data, the framework generates training examples, which also contain instances of hypothetical leaks. Via machine learning, the proposed framework replaces the error-prone user-definable staleness predicates used in previous research with a model-based prediction.  The framework was tested using both synthetic and real-world examples. Evaluation with synthetic leakage workloads of SPEC2006 benchmarks shows that the proposed method achieves the optimal accuracy permitted by staleness-based leak detection. Moreover, by incorporating allocation context into the model, the proposed method achieves higher accuracy than is possible with object staleness alone. Evaluation with real-world memory leaks demonstrates that the proposed method is effective for detecting previously reported bugs with high accuracy.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {814–824},
numpages = {11},
keywords = {Runtime analysis, Memory leak detection, Machine learning},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/1368088.1368114,
author = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
title = {A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368114},
doi = {10.1145/1368088.1368114},
abstract = {In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, Na\"{\i}ve Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: &gt;75% percentage of correctly classified files, a recall of &gt;80%, and a false positive rate &lt;30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {181–190},
numpages = {10},
keywords = {software metrics, defect prediction, cost-sensitive classification},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@article{10.1007/s10664-011-9165-9,
author = {Shin, Yonghee and Bell, Robert M. and Ostrand, Thomas J. and Weyuker, Elaine J.},
title = {On the use of calling structure information to improve fault prediction},
year = {2012},
issue_date = {August    2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4–5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-011-9165-9},
doi = {10.1007/s10664-011-9165-9},
abstract = {Previous studies have shown that software code attributes, such as lines of source code, and history information, such as the number of code changes and the number of faults in prior releases of software, are useful for predicting where faults will occur. In this study of two large industrial software systems, we investigate the effectiveness of adding information about calling structure to fault prediction models. Adding calling structure information to a model based solely on non-calling structure code attributes modestly improved prediction accuracy. However, the addition of calling structure information to a model that included both history and non-calling structure code attributes produced no improvement.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {390–423},
numpages = {34},
keywords = {Software faults, Negative binomial model, Empirical study, Calling structure attributes}
}

@inproceedings{10.1109/HASE.2007.60,
author = {Shin, Miyoung and Ratanothayanon, Sunida and Goel, Amrit L. and Paul, Raymond A.},
title = {Parsimonious Classifiers for Software Quality Assessment},
year = {2007},
isbn = {0769530435},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HASE.2007.60},
doi = {10.1109/HASE.2007.60},
abstract = {Modeling to predict faultproneness of software modules is an important area of research in software engineering. Most such models employ a large number of basic and derived metrics as predictors. This paper presents modeling results based on only two metrics, lines of code and cyclomatic complexity, using radial basis functions with Gaussian kernels as classifiers. Results from two NASA systems are presented and analyzed.},
booktitle = {Proceedings of the 10th IEEE High Assurance Systems Engineering Symposium},
pages = {411–412},
numpages = {2},
keywords = {Software quality, Software metrics, Parsimonious classifiers, Classification},
series = {HASE '07}
}

@inproceedings{10.1145/3493244.3493252,
author = {Passos, Leticia Carvalho and Viana, Lucas and Oliveira, Edson and Conte, Tayana},
title = {LabelUX! Guidelines to support software engineers to design data labeling systems},
year = {2021},
isbn = {9781450395533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493244.3493252},
doi = {10.1145/3493244.3493252},
abstract = {The demand for systems using artificial intelligence has substantially boosted in recent times, especially with Machine Learning (ML) techniques. Systems that use ML supervision techniques need representative and correctly categorized data to ensure its quality. In this context, a data labeling step plays a fundamental role during the development of such systems. The labeling is performed by users specialized in the data domain and aims to generate a database to enable a supervised ML model. However, labeling is exhausting for users, which can compromise the quality of the ML system, especially if the labeling is being done on systems that were not designed to assist the user in this activity. On the one hand, it can be difficult for a software engineer to design these kinds of systems. Depending on the type of data to be labeled, the interface needs different graphics and strategies to present and request user feedback. Aiming to help software engineers develop these kinds of systems, this work proposes the LabelUX guidelines. These guidelines aim to support software engineers in designing data labeling systems, defining a design with quality that provides a better user experience during the labeling task. We developed these guidelines from studies carried out in the literature and industry. We selected software engineers working on ML projects to participate in a feasibility study to evaluate the use of guidelines. The qualitative results obtained through the interview improved that the LabelUX guidelines supported a better design of textual type data labeling systems.},
booktitle = {Proceedings of the XX Brazilian Symposium on Software Quality},
articleno = {9},
numpages = {10},
keywords = {software design, interactive machine learning, human in the loop, data labeling},
location = {Virtual Event, Brazil},
series = {SBQS '21}
}

@inproceedings{10.1145/3387904.3389295,
author = {Lenarduzzi, Valentina and Palomba, Fabio and Taibi, Davide and Tamburri, Damian Andrew},
title = {OpenSZZ: A Free, Open-Source, Web-Accessible Implementation of the SZZ Algorithm},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389295},
doi = {10.1145/3387904.3389295},
abstract = {The accurate identification of defect-inducing commits represents a key problem for researchers interested in studying the naturalness of defects and defining defect prediction models. To tackle this problem, software engineering researchers have relied on and proposed several implementations of the well-known Sliwerski-Zimmermann-Zeller (SZZ) algorithm. Despite its popularity and wide usage, no open-source, publicly available, and web-accessible implementation of the algorithm has been proposed so far. In this paper, we prototype and make available one such implementation for further use by practitioners and researchers alike. The evaluation of the proposed prototype showed competitive results and lays the foundation for future work. This paper outlines our prototype, illustrating its usage and reporting on its evaluation in action.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {446–450},
numpages = {5},
keywords = {Web APIs, Software Defect Proneness, Software Defect Prediction, Open-Source Tools},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@article{10.3233/JIFS-169571,
author = {Kostopoulos, G. and Livieris, I.E. and Kotsiantis, S. and Tampakas, V. and Patnaik, Srikanta},
title = {CST-Voting: A semi-supervised ensemble method for classification problems},
year = {2018},
issue_date = {2018},
publisher = {IOS Press},
address = {NLD},
volume = {35},
number = {1},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169571},
doi = {10.3233/JIFS-169571},
abstract = {Semi-supervised learning is an emerging subfield of machine learning, with a view to building efficient classifiers exploiting a limited pool of labeled data together with a large pool of unlabeled ones. Most of the studies regarding semi-supervised learning deal with classification problems, whose goal is to learn a function that maps an unlabeled instance into a finite number of classes. In this paper, a new semi-supervised classification algorithm, which is based on a voting methodology, is proposed. The term attributed to this ensemble method is called CST-Voting. Ensemble methods have been effectively applied in various scientific fields and often perform better than the individual classifiers from which they are originated. The efficiency of the proposed algorithm is compared to three familiar semi-supervised learning methods on a plethora of benchmark datasets using three representative supervised classifiers as base learners. Experimental results demonstrate the predominance of the proposed method, outperforming classical semi-supervised classification algorithms as illustrated from the accuracy measurements and confirmed by the Friedman Aligned Ranks nonparametric test.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {99–109},
numpages = {11},
keywords = {accuracy, ensemble methods, voting, classification, Semi-supervised learning}
}

@article{10.1007/s11219-020-09525-y,
author = {Malhotra, Ruchika and Lata, Kusum},
title = {An empirical study on predictability of software maintainability using imbalanced data},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09525-y},
doi = {10.1007/s11219-020-09525-y},
abstract = {In software engineering predictive modeling, early prediction of software modules or classes that possess high maintainability effort is a challenging task. Many prediction models are constructed to predict the maintainability of software classes or modules by applying various machine learning (ML) techniques. If the software modules or classes need&nbsp;high maintainability, effort would be reduced&nbsp;in a dataset, and&nbsp;there would be imbalanced data to train the model. The imbalanced datasets make&nbsp;ML techniques bias their predictions towards low maintainability effort or majority classes, and minority class instances get discarded as noise by the machine learning (ML) techniques. In this direction, this paper presents empirical work to improve the performance of software maintainability prediction (SMP) models developed with ML techniques using imbalanced data. For developing the models, the imbalanced data is pre-processed by applying data resampling methods. Fourteen data resampling methods, including oversampling, undersampling, and hybrid resampling, are used in the study. The study results recommend that the safe-level synthetic minority oversampling technique (Safe-Level-SMOTE) is a useful method to deal with the imbalanced datasets and to develop competent prediction models to forecast software maintainability.},
journal = {Software Quality Journal},
month = dec,
pages = {1581–1614},
numpages = {34},
keywords = {Imbalanced learning, Data resampling, Machine learning, Software maintainability prediction}
}

@inproceedings{10.5555/2666527.2666533,
author = {Morgado, In\^{e}s Coimbra and Paiva, Ana C. R. and Faria, Jo\~{a}o Pascoal and Camacho, Rui},
title = {GUI reverse engineering with machine learning},
year = {2012},
isbn = {9781467317535},
publisher = {IEEE Press},
abstract = {This paper proposes a new approach to reduce the effort of building formal models representative of the structure and behaviour of Graphical User Interfaces (GUI). The main goal is to automatically extract the GUI model with a dynamic reverse engineering process, consisting in an exploration phase, that extracts information by interacting with the GUI, and in a model generation phase that, making use of machine learning techniques, uses the extracted information of the first step to generate a state-machine model of the GUI, including guard conditions to remove ambiguity in transitions.},
booktitle = {Proceedings of the First International Workshop on Realizing AI Synergies in Software Engineering},
pages = {27–31},
numpages = {5},
keywords = {reverse engineering, model-based testing, machine learning, inductive logic programming},
location = {Zurich, Switzerland},
series = {RAISE '12}
}

@inproceedings{10.1007/978-3-030-79463-7_35,
author = {Kawalerowicz, Marcin and Madeyski, Lech},
title = {Continuous Build Outcome Prediction: A Small-N Experiment in Settings of a Real Software Project},
year = {2021},
isbn = {978-3-030-79462-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79463-7_35},
doi = {10.1007/978-3-030-79463-7_35},
abstract = {We explain the idea of Continuous Build Outcome Prediction (CBOP) practice that uses classification to label the possible build results (success or failure) based on historical data and metrics (features) derived from the software repository. Additionally, we present a preliminary empirical evaluation of CBOP in a real live software project. In a small-n repeated-measure with two conditions and replicates experiment, we study whether CBOP will reduce the Failed Build Ratio (FBR). Surprisingly, the result of the study indicates a slight increase in FBR while using the CBOP, although the effect size is very small. A plausible explanation of the revealed phenomenon may come from the authority principle, which is rarely discussed in the software engineering context in general, and AI-supported software development practices in particular.},
booktitle = {Advances and Trends in Artificial Intelligence. From Theory to Practice: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part II},
pages = {412–425},
numpages = {14},
keywords = {Machine learning, Continuous integration, Agile experimentation, Software defect prediction},
location = {Kuala Lumpur, Malaysia}
}

@inproceedings{10.1145/1882362.1882409,
author = {Lowry, Michael R.},
title = {Towards predictive models of technology impact on software design productivity},
year = {2010},
isbn = {9781450304276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882362.1882409},
doi = {10.1145/1882362.1882409},
abstract = {In order to advance software engineering research, agencies should fund pilot studies for calibrating software design productivity impacts of potential technology advances. We need a predictive model of technology impacts in order to advocate technology programs and to select individual projects that provide most benefit to society. Current software cost estimation models can provide a starting point, but in the long run are inadequate because they are based on current methods and technologies for software development. Ultimately, the predictive models need to be rooted in fundamental factors affecting productivity, ranging from cognitive facility of different programming language paradigms, mathematical underpinnings for reuse and compositional approaches, and organizational psychology for large development projects. Such a productivity model would enable development of metrics for individual facets of software design productivity, and an understanding of how even narrow technology advances contribute to overall software design productivity.},
booktitle = {Proceedings of the FSE/SDP Workshop on Future of Software Engineering Research},
pages = {223–228},
numpages = {6},
keywords = {cost estimation},
location = {Santa Fe, New Mexico, USA},
series = {FoSER '10}
}

@inproceedings{10.1109/COMPSAC.2015.66,
author = {Liu, Wangshu and Liu, Shulong and Gu, Qing and Chen, Xiang and Chen, Daoxu},
title = {FECS: A Cluster Based Feature Selection Method for Software Fault Prediction with Noises},
year = {2015},
isbn = {9781467365642},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2015.66},
doi = {10.1109/COMPSAC.2015.66},
abstract = {Noises are inevitable when mining software archives for software fault prediction. Although some researchers have investigated the noise tolerance of existing feature selection methods, few studies focus on proposing new feature selection methods with a certain noise tolerance. To solve this issue, we propose a novel method FECS (FEature Clustering with Selection strategies). This method includes two phases: a feature clustering phase and a feature selection phase with three different heuristic search strategies. During empirical studies, we choose real-world software projects, such as Eclipse and NASA and inject class level and feature level noises simultaneously to imitate noisy datasets. After using classical feature selection methods as the baseline, we confirm the effectiveness of FECS and provide a guideline of using FECS after analyzing the effects of varying either the percentage of selected features or the noise rate.},
booktitle = {Proceedings of the 2015 IEEE 39th Annual Computer Software and Applications Conference - Volume 02},
pages = {276–281},
numpages = {6},
keywords = {Software Quality Assurance, Software Fault Prediction, Noise Tolerance, Feature Selection, Classification Model},
series = {COMPSAC '15}
}

@article{10.5555/2734212.2734215,
author = {Ronchieri, E. and Canaparo, M. and Salomoni, D.},
title = {A Software Quality Model by Using Discriminant Analysis Predictive Technique},
year = {2014},
issue_date = {October 2014},
publisher = {IOS Press},
address = {NLD},
volume = {18},
number = {4},
issn = {1092-0617},
abstract = {Scientific computing infrastructures need reliable software within pressing deadlines due to communities¡¯ requirements they support. Concerning the Grid, computing researchers have been developing software projects without exploiting solutions for discovering defects early enough in the implementation process. This has led to spending energy maintaining and correcting software once released. Achieving high reliability is therefore one of the most important challenges to be faced in the Grid context during the software development life cycle. Although developers perceive quality improvement solutions as limiting factors to their productivity, in our opinion enhancing quality enables us to eliminate mistakes and, as a consequence, reduce costs and delays; the software quality models and metrics represent the mainstream to reach high reliability by balancing both effort and results. In this paper, we aim to provide an extension of a mathematical model that connects software best practices with a set of metrics to periodically predict the quality at any stage of code development and to determine its problems at any early phase. For data statistical properties, we used a risk-threshold-based discriminant analysis technique to analyze the defined model and to detect fault-prone and non fault-prone components. We gathered input data for this model from several European Middleware Initiative packages having different scopes and characteristics, whilst outputs were derived from measures of all specified metrics. At the end we attempted to understand if the model is a true picture of the software under evaluation.},
journal = {J. Integr. Des. Process Sci.},
month = oct,
pages = {25–29},
numpages = {5},
keywords = {Software Metrics, Predictive Methods, Fault-Prone Modules, Discriminant Analysis}
}

@article{10.5555/3546258.3546276,
author = {Jiang, Gaoxia and Wang, Wenjian and Qian, Yuhua and Liang, Jiye},
title = {A unified sample selection framework for output noise filtering: an error-bound perspective},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {The existence of output noise will bring difficulties to supervised learning. Noise filtering, aiming to detect and remove polluted samples, is one of the main ways to deal with the noise on outputs. However, most of the filters are heuristic and could not explain the filtering in uence on the generalization error (GE) bound. The hyper-parameters in various filters are specified manually or empirically, and they are usually unable to adapt to the data environment. The filter with an improper hyper-parameter may overclean, leading to a weak generalization ability. This paper proposes a unified framework of optimal sample selection (OSS) for the output noise filtering from the perspective of error bound. The covering distance filter (CDF) under the framework is presented to deal with noisy outputs in regression and ordinal classification problems. Firstly, two necessary and sufficient conditions for a fixed goodness of fit in regression are deduced from the perspective of GE bound. They provide the unified theoretical framework for determining the filtering effectiveness and optimizing the size of removed samples. The optimal sample size has the adaptability to the environmental changes in the sample size, the noise ratio, and noise variance. It offers a choice of tuning the hyper-parameter and could prevent filters from overcleansing. Meanwhile, the OSS framework can be integrated with any noise estimator and produces a new filter. Then the covering interval is proposed to separate low-noise and high-noise samples, and the effectiveness is proved in regression. The covering distance is introduced as an unbiased estimator of high noises. Further, the CDF algorithm is designed by integrating the cover distance with the OSS framework. Finally, it is verified that the CDF not only recognizes noise labels correctly but also brings down the prediction errors on real apparent age data set. Experimental results on benchmark regression and ordinal classification data sets demonstrate that the CDF outperforms the state-of-the-art filters in terms of prediction ability, noise recognition, and efficiency.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {18},
numpages = {66},
keywords = {supervised learning, covering distance filtering, optimal sample selection, generalization error bound, output noise}
}

@inproceedings{10.5555/3290281.3290313,
author = {Griscom, Matt},
title = {MetaAutomation: a pattern language to apply automation to software quality},
year = {2017},
isbn = {9781941652060},
publisher = {The Hillside Group},
address = {USA},
abstract = {MetaAutomation is a pattern language for automated measurements and communication of functional software quality and performance within a team or company that is developing software. The "whole" of MetaAutomation addresses the quality automation problem space: automation to measure and communicate quality, bound on the technology side by operations on and measurements of software under development or maintenance for quality purposes, and bound on the business side by human customers of the quality information, and other automated processes that depend on quality, for example, operations.The focus of MetaAutomation is on answering the question for the business "Does the system do what we need it to do?" quickly and reliably, with highly trustworthy and structured detail that supports unprecedented visibility into and communication around the larger team - including dev, QA, and every team member concerned with quality - of what the automation drives the product to do, and how the product responds.Each of the patterns is based at least in part on existing patterns of human behavior and/or software development.MetaAutomation clarifies the value of what an intentional, designed approach to measuring and reporting software quality with automation can achieve, as opposed to common patterns of doing this which make poor use of automation's capabilities.The target audience is more than just the quality assurance role; it is anybody doing, managing, or leading quality work with automated verifications and communication of functional and performance requirements on a software development project, including software developers who would like to create software faster and with higher confidence.MetaAutomation has 9 patterns currently: Hierarchical Steps, Atomic Check, Event-Driven Check, Extension Check, Precondition Pool, Parallel Run, Smart Retry, Automated Triage, and Queryable Quality. It is open to extension with more patterns that address the quality automation problem space, and a community to make it the living pattern language that it deserves to be.},
booktitle = {Proceedings of the 24th Conference on Pattern Languages of Programs},
articleno = {27},
numpages = {34},
keywords = {test, software quality, quality automation, automation, MetaAutomation},
location = {Vancouver, British Columbia, Canada},
series = {PLoP '17}
}

@inproceedings{10.1109/WOSQ.2007.11,
author = {Stroggylos, Konstantinos and Spinellis, Diomidis},
title = {Refactoring--Does It Improve Software Quality?},
year = {2007},
isbn = {0769529593},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WOSQ.2007.11},
doi = {10.1109/WOSQ.2007.11},
abstract = {Software systems undergo modifications, improvements and enhancements to cope with evolving requirements. This maintenance can cause their quality to decrease. Various metrics can be used to evaluate the way the quality is affected. Refactoring is one of the most important and commonly used techniques of transforming a piece of software in order to improve its quality. However, although it would be expected that the increase in quality achieved via refactoring is reflected in the various metrics, measurements on real life systems indicate the opposite. We analyzed source code version control system logs of popular open source software systems to detect changes marked as refactorings and examine how the software metrics are affected by this process, in order to evaluate whether refactoring is effectively used as a means to improve software quality within the open source community.},
booktitle = {Proceedings of the 5th International Workshop on Software Quality},
pages = {10},
series = {WoSQ '07}
}

@article{10.1016/j.ins.2010.12.016,
author = {Seiffert, Chris and Khoshgoftaar, Taghi M. and Van Hulse, Jason and Folleco, Andres},
title = {An empirical study of the classification performance of learners on imbalanced and noisy software quality data},
year = {2014},
issue_date = {February, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {259},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2010.12.016},
doi = {10.1016/j.ins.2010.12.016},
abstract = {Data mining techniques are commonly used to construct models for identifying software modules that are most likely to contain faults. In doing so, an organization's limited resources can be intelligently allocated with the goal of detecting and correcting the greatest number of faults. However, there are two characteristics of software quality datasets that can negatively impact the effectiveness of these models: class imbalance and class noise. Software quality datasets are, by their nature, imbalanced. That is, most of a software system's faults can be found in a small percentage of software modules. Therefore, the number of fault-prone, fp, examples (program modules) in a software project dataset is much smaller than the number of not fault-prone, nfp, examples. Data sampling techniques attempt to alleviate the problem of class imbalance by altering a training dataset's distribution. A program module contains class noise if it is incorrectly labeled. While several studies have been performed to evaluate data sampling methods, the impact of class noise on these techniques has not been adequately addressed. This work presents a systematic set of experiments designed to investigate the impact of both class noise and class imbalance on classification models constructed to identify fault-prone program modules. We analyze the impact of class noise and class imbalance on 11 different learning algorithms (learners) as well as 7 different data sampling techniques. We identify which learners and which data sampling techniques are most robust when confronted with noisy and imbalanced data.},
journal = {Inf. Sci.},
month = feb,
pages = {571–595},
numpages = {25},
keywords = {Sampling, Imbalance, Class noise, Binary classification}
}

@article{10.1145/2556777,
author = {Zhou, Yuming and Xu, Baowen and Leung, Hareton and Chen, Lin},
title = {An in-depth study of the potentially confounding effect of class size in fault prediction},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/2556777},
doi = {10.1145/2556777},
abstract = {Background. The extent of the potentially confounding effect of class size in the fault prediction context is not clear, nor is the method to remove the potentially confounding effect, or the influence of this removal on the performance of fault-proneness prediction models. Objective. We aim to provide an in-depth understanding of the effect of class size on the true associations between object-oriented metrics and fault-proneness. Method. We first employ statistical methods to examine the extent of the potentially confounding effect of class size in the fault prediction context. After that, we propose a linear regression-based method to remove the potentially confounding effect. Finally, we empirically investigate whether this removal could improve the prediction performance of fault-proneness prediction models. Results. Based on open-source software systems, we found: (a) the confounding effect of class size on the associations between object-oriented metrics and fault-proneness in general exists; (b) the proposed linear regression-based method can effectively remove the confounding effect; and (c) after removing the confounding effect, the prediction performance of fault prediction models with respect to both ranking and classification can in general be significantly improved. Conclusion. We should remove the confounding effect of class size when building fault prediction models.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {10},
numpages = {51},
keywords = {prediction, fault, confounding effect, class size, Metrics}
}

@article{10.1007/s11219-020-09498-y,
author = {Guggulothu, Thirupathi and Moiz, Salman Abdul},
title = {Code smell detection using multi-label classification approach},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09498-y},
doi = {10.1007/s11219-020-09498-y},
abstract = {Code smells are characteristics of the software that indicates a code or design problem which can make software hard to understand, evolve, and maintain. There are several code smell detection tools proposed in the literature, but they produce different results. This is because smells are informally defined or subjective in nature. Machine learning techniques help in addressing the issues of subjectivity, which can learn and distinguish the characteristics of smelly and non-smelly source code elements (classes or methods). However, the existing machine learning techniques can only detect a single type of smell in the code element that does not correspond to a real-world scenario as a single element can have multiple design problems (smells). Further, the mechanisms proposed in the literature could not detect code smells by considering the correlation (co-occurrence) among them. To address these shortcomings, we propose and investigate the use of multi-label classification (MLC) methods to detect whether the given code element is affected by multiple smells or not. In this proposal, two code smell datasets available in the literature are converted into a multi-label dataset (MLD). In the MLD, we found that there is a positive correlation between the two smells (long method and feature envy). In the classification phase, the two methods of MLC considered the correlation among the smells and enhanced the performance (on average more than 95% accuracy) for the 10-fold cross-validation with the ten iterations. The findings reported help the researchers and developers in prioritizing the critical code elements for refactoring based on the number of code smells detected.},
journal = {Software Quality Journal},
month = sep,
pages = {1063–1086},
numpages = {24},
keywords = {Refactoring, Machine learning techniques, Code smells detection, Multi-label classification, Code smell correlation, Software quality, Code smells}
}

@inproceedings{10.1109/SMC.2013.282,
author = {Chandrashekar, Girish and Sahin, Ferat and Cinar, Eyup and Radomski, Aaron and Sarosky, Dan},
title = {In-Vivo Fault Analysis and Real-Time Fault Prediction for RF Generators Using State-of-the-Art Classifiers},
year = {2013},
isbn = {9781479906529},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SMC.2013.282},
doi = {10.1109/SMC.2013.282},
abstract = {In this paper we apply various machine learning techniques for fault detection of RF (Radio Frequency) Power Generators. Fast Fourier Transform features are used in our analysis for all experiments. Radial Basis Function Networks (RBF) is used to build a two class classifier to differentiate between normal and one fault condition. We apply three one class classifiers to model the normal operating conditions. The data is obtained from five different generators of the same model type.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Systems, Man, and Cybernetics},
pages = {1634–1639},
numpages = {6},
keywords = {Radial Basis Functions, RF generators, One class classification, Novelty detection, Fault analysis},
series = {SMC '13}
}

@article{10.1109/TSMCA.2006.889473,
author = {Seliya, Naeem and Khoshgoftaar, Taghi M.},
title = {Software Quality Analysis of Unlabeled Program Modules With Semisupervised Clustering},
year = {2007},
issue_date = {March 2007},
publisher = {IEEE Press},
volume = {37},
number = {2},
issn = {1083-4427},
url = {https://doi.org/10.1109/TSMCA.2006.889473},
doi = {10.1109/TSMCA.2006.889473},
abstract = {Software quality assurance is a vital component of software project development. A software quality estimation model is trained using software measurement and defect (software quality) data of a previously developed release or similar project. Such an approach assumes that the development organization has experience with systems similar to the current project and that defect data are available for all modules in the training data. In software engineering practice, however, various practical issues limit the availability of defect data for modules in the training data. In addition, the organization may not have experience developing a similar system. In such cases, the task of software quality estimation or labeling modules as fault prone or not fault prone falls on the expert. We propose a semisupervised clustering scheme for software quality analysis of program modules with no defect data or quality-based class labels. It is a constraint-based semisupervised clustering scheme that uses k-means as the underlying clustering algorithm. Software measurement data sets obtained from multiple National Aeronautics and Space Administration software projects are used in our empirical investigation. The proposed technique is shown to aid the expert in making better estimations as compared to predictions made when the expert labels the clusters formed by an unsupervised learning algorithm. In addition, the software quality knowledge learnt during the semisupervised process provided good generalization performance for multiple test data sets. An analysis of program modules that remain unlabeled subsequent to our semisupervised clustering scheme provided useful insight into the characteristics of their software attributes},
journal = {Trans. Sys. Man Cyber. Part A},
month = mar,
pages = {201–211},
numpages = {11},
keywords = {unlabeled data, software quality, software measurements, semisupervised learning, Constraint-based clustering}
}

@inproceedings{10.1109/ICTAI.2009.24,
author = {Gao, Kehan and Khoshgoftaar, Taghi M. and Napolitano, Amri},
title = {Exploring Software Quality Classification with a Wrapper-Based Feature Ranking Technique},
year = {2009},
isbn = {9780769539201},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2009.24},
doi = {10.1109/ICTAI.2009.24},
abstract = {Feature selection is a process of selecting a subset of relevant features for building learning models. It is an important activity for data preprocessing used in software quality modeling and other data mining problems. Feature selection algorithms can be divided into two categories, feature ranking and feature subset selection. Feature ranking orders the features by a criterion and a user selects some of the features that are appropriate for a given scenario. Feature subset selection techniques search the space of possible feature subsets and evaluate the suitability of each. This paper investigates performance metric based feature ranking techniques by using the multilayer perceptron (MLP) learner with nine different performance metrics. The nine performance metrics include Overall Accuracy (OA), Default F-Measure (DFM), Default Geometric Mean (DGM), Default Arithmetic Mean (DAM), Area Under ROC (AUC), Area Under PRC (PRC), Best F-Measure (BFM), Best Geometric Mean (BGM) and Best Arithmetic Mean (BAM). The goal of the paper is to study the effect of the different performance metrics on the feature ranking results, which in turn influences the classification performance. We assessed the performance of the classification models constructed on those selected feature subsets through an empirical case study that was carried out on six data sets of real-world software systems. The results demonstrate that AUC, PRC, BFM, BGM and BAM as performance metrics for feature ranking outperformed the other performance metrics, OA, DFM, DGMand DAM, unanimously across all the data sets and therefore are recommended based on this study. In addition, the performances of the classification models were maintained or even improved when over 85 percent of the features were eliminated from the original data sets.},
booktitle = {Proceedings of the 2009 21st IEEE International Conference on Tools with Artificial Intelligence},
pages = {67–74},
numpages = {8},
keywords = {software quality modeling, performance metric, feature ranking technique},
series = {ICTAI '09}
}

@article{10.1016/j.jss.2016.06.006,
author = {Okutan, Ahmet and Taner Yildiz, Olcay},
title = {A novel kernel to predict software defectiveness},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {119},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.006},
doi = {10.1016/j.jss.2016.06.006},
abstract = {We propose new kernels for defect prediction that are based on the source code similarity.We model the relationship between source code similarity and defectiveness.The precomputed kernels are used with SVM and KNN classifiers.The proposed technique performs better than the SVM with linear kernel.It also achieves comparable performance when compared to the KNN classifier. Although the software defect prediction problem has been researched for a long time, the results achieved are not so bright. In this paper, we propose to use novel kernels for defect prediction that are based on the plagiarized source code, software clones and textual similarity. We generate precomputed kernel matrices and compare their performance on different data sets to model the relationship between source code similarity and defectiveness. Each value in a kernel matrix shows how much parallelism exists between the corresponding files of a software system chosen. Our experiments on 10 real world datasets indicate that support vector machines (SVM) with a precomputed kernel matrix performs better than the SVM with the usual linear kernel in terms of F-measure. Similarly, when used with a precomputed kernel, the k-nearest neighbor classifier (KNN) achieves comparable performance with respect to KNN classifier. The results from this preliminary study indicate that source code similarity can be used to predict defect proneness.},
journal = {J. Syst. Softw.},
month = sep,
pages = {109–121},
numpages = {13},
keywords = {SVM, Kernel methods, Defect prediction}
}

@inproceedings{10.5555/2818754.2818851,
author = {Peters, Fayola and Menzies, Tim and Layman, Lucas},
title = {LACE2: better privacy-preserving data sharing for cross project defect prediction},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Before a community can learn general principles, it must share individual experiences. Data sharing is the fundamental step of cross project defect prediction, i.e. the process of using data from one project to predict for defects in another. Prior work on secure data sharing allowed data owners to share their data on a single-party basis for defect prediction via data minimization and obfuscation. However the studied method did not consider that bigger data required the data owner to share more of their data.In this paper, we extend previous work with LACE2 which reduces the amount of data shared by using multi-party data sharing. Here data owners incrementally add data to a cache passed among them and contribute "interesting" data that are not similar to the current content of the cache. Also, before data owner i passes the cache to data owner j, privacy is preserved by applying obfuscation algorithms to hide project details. The experiments of this paper show that (a) LACE2 is comparatively less expensive than the single-party approach and (b) the multi-party approach of LACE2 yields higher privacy than the prior approach without damaging predictive efficacy (indeed, in some cases, LACE2 leads to better defect predictors).},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {801–811},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.1016/j.infsof.2009.04.002,
author = {Alshayeb, Mohammad},
title = {Empirical investigation of refactoring effect on software quality},
year = {2009},
issue_date = {September, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.04.002},
doi = {10.1016/j.infsof.2009.04.002},
abstract = {Developers and designers always strive for quality software. Quality software tends to be robust, reliable and easy to maintain, and thus reduces the cost of software development and maintenance. Several methods have been applied to improve software quality. Refactoring is one of those methods. The goal of this paper is to validate/invalidate the claims that refactoring improves software quality. We focused this study on different external quality attributes, which are adaptability, maintainability, understandability, reusability, and testability. We found that refactoring does not necessarily improve these quality attributes.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {1319–1326},
numpages = {8},
keywords = {Software metrics, Refactoring, Quality improvement, Empirical study}
}

@inproceedings{10.1145/1370788.1370792,
author = {Weyuker, Elaine J. and Ostrand, Thomas J. and Bell, Robert M.},
title = {Comparing negative binomial and recursive partitioning models for fault prediction},
year = {2008},
isbn = {9781605580364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370788.1370792},
doi = {10.1145/1370788.1370792},
abstract = {Two different software fault prediction models have been used to predict the N% of the files of a large software system that are likely to contain the largest numbers of faults. We used the same predictor variables in a negative binomial regression model and a recursive partitioning model, and compared their effectiveness on three large industrial software systems. The negative binomial model identified files that contain 76 to 93 percent of the faults, and recursive partitioning identified files that contain 68 to 85 percent.},
booktitle = {Proceedings of the 4th International Workshop on Predictor Models in Software Engineering},
pages = {3–10},
numpages = {8},
keywords = {empirical study, fault prediction, negative binomial, recursive partition, software testing},
location = {Leipzig, Germany},
series = {PROMISE '08}
}

@article{10.1007/s10489-011-0316-x,
author = {Chatterjee, S. and Nigam, S. and Singh, J. B. and Upadhyaya, L. N.},
title = {Software fault prediction using Nonlinear Autoregressive with eXogenous Inputs (NARX) network},
year = {2012},
issue_date = {July      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {37},
number = {1},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-011-0316-x},
doi = {10.1007/s10489-011-0316-x},
abstract = {This paper explores a new approach for predicting software faults by means of NARX neural network. Also, a careful analysis has been carried out to determine the applicability of NARX network in software reliability. The validation of the proposed approach has been performed using two real software failure data sets. Comparison has been made with some existing parametric software reliability models as well as some neural network (Elman net and TDNN) based SRGM. The results computed shows that the proposed approach outperformed the other existing parametric and neural network based software reliability models with a reasonably good predictive accuracy.},
journal = {Applied Intelligence},
month = jul,
pages = {121–129},
numpages = {9},
keywords = {Time between failures, Software reliability, NARX neural network, Faults}
}

@article{10.1007/s00500-021-06048-x,
author = {Rathore, Santosh S.},
title = {An exploratory analysis of regression methods for predicting faults in software systems},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {23},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06048-x},
doi = {10.1007/s00500-021-06048-x},
abstract = {The use of regression methods, for instance, linear regression, decision tree regression, etc., has been used earlier to build software fault prediction (SFP) models. However, these methods showed limited SFP performance with higher misclassification errors. In previous works, issues such as multicollinearity, feature scaling, and imbalance distribution of faulty and non-faulty modules in the dataset have not been considered reasonably, which might be a potential cause behind the poor prediction performance of these regression methods. Motivated from it, in this paper, we investigate the impact of 15 different regression methods for the faults count prediction in the software system and report their interpretation for fault models. We consider different fault data quality issues, and a comprehensive assessment of the regression methods is presented to handle these issues. We believe that many used regression methods have not been explored before for the SFP by considering different data quality issues. In the presented study, 44 fault datasets and their versions are used that are collected from the PROMISE software data repository are used to validate the performance of the regression methods, and absolute relative error (ARE), root mean square error (RSME), and fault-percentile-average (FPA) are used as the performance measures. For the model building, five different scenarios are considered, (1) original dataset without preprocessing; (2) standardized processed dataset; (3) balanced dataset; (4) non-multicollinearity processed dataset; (5) balanced+non-multicollinearity processed dataset. Experimental results showed that overall kernel-based regression methods, KernelRidge and SVR (Support vector regression, both linear and nonlinear kernels), yielded the best performance for predicting the fault counts compared to other methods. Other regression methods, in particular NNR (Nearest neighbor regression), RFR (Random forest regression), and GBR (Gradient boosting regression), are performed significantly accurately. Further, results showed that applying standardization and handling multicollinearity in the fault dataset helped improve regression methods’ performance. It is concluded that regression methods are promising for building software fault prediction models.},
journal = {Soft Comput.},
month = dec,
pages = {14841–14872},
numpages = {32},
keywords = {Empirical study, PROMISE data repository, Regression methods, Software fault prediction}
}

@inproceedings{10.1145/1414004.1414049,
author = {Vivanco, Rodrigo and Jin, Dean},
title = {Enhancing predictive models using principal component analysis and search based metric selection: a comparative study},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414049},
doi = {10.1145/1414004.1414049},
abstract = {Predictive models are used for the detection of potentially problematic component that decrease product quality. Source code metrics can be used as input features in predictive models; however, there exist numerous structural measures that capture different aspects of size, coupling, cohesion, inheritance and complexity. An important question to answer is which metrics should be used with a predictor. A comparative analysis of metric selection strategies (principal component analysis, a genetic algorithm and the CK metrics set) has been carried out. Initial results indicate that search-based metric selection gives the best predictive performance in identifying Java classes with high cognitive complexity that degrades product maintenance.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {273–275},
numpages = {3},
keywords = {predictive models, pca, metric selection, genetic algorithm},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@article{10.1007/s10664-009-9111-2,
author = {Weyuker, Elaine J. and Ostrand, Thomas J. and Bell, Robert M.},
title = {Comparing the effectiveness of several modeling methods for fault prediction},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-009-9111-2},
doi = {10.1007/s10664-009-9111-2},
abstract = {We compare the effectiveness of four modeling methods--negative binomial regression, recursive partitioning, random forests and Bayesian additive regression trees--for predicting the files likely to contain the most faults for 28 to 35 releases of three large industrial software systems. Predictor variables included lines of code, file age, faults in the previous release, changes in the previous two releases, and programming language. To compare the effectiveness of the different models, we use two metrics--the percent of faults contained in the top 20% of files identified by the model, and a new, more general metric, the fault-percentile-average. The negative binomial regression and random forests models performed significantly better than recursive partitioning and Bayesian additive regression trees, as assessed by either of the metrics. For each of the three systems, the negative binomial and random forests models identified 20% of the files in each release that contained an average of 76% to 94% of the faults.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {277–295},
numpages = {19},
keywords = {Recursive partitioning, Random forests, Negative binomial, Fault-percentile-average, Fault prediction, Empirical study, Bayesian trees}
}

@inproceedings{10.5555/1792786.1792790,
author = {Weyuker, Elaine J. and Ostrand, Thomas J.},
title = {What can fault prediction do for you?},
year = {2008},
isbn = {354079123X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {It would obviously be very valuable to know in advance which files in the next release of a large software system are most likely to contain the largest numbers of faults. This is true whether the goal is to validate the system by testing or formally verifying it, or by using some hybrid approach. To accomplish this, we developed negative binomial regression models and used them to predict the expected number of faults in each file of the next release of a system. The predictions are based on code characteristics and fault and modification history data. This paper discusses what we have learned from applying the model to several large industrial systems, each with multiple years of field exposure. It also discusses our success in making accurate predictions and some of the issues that had to be considered.},
booktitle = {Proceedings of the 2nd International Conference on Tests and Proofs},
pages = {18–29},
numpages = {12},
location = {Prato, Italy},
series = {TAP'08}
}

@inproceedings{10.1145/3323503.3360643,
author = {Kieling, Eduardo J. and Rodrigues, Felipe C. and Filippetto, Alexsandro and Barbosa, Jorge},
title = {Smartalloc: a model based on machine learning for human resource allocation in projects},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360643},
doi = {10.1145/3323503.3360643},
abstract = {This article presents the Smartalloc model for human resource allocations in projects based on machine learning. The model learns about the allocation strategies used by the organization over time and makes recommendations based on this information. The model has two scientific contributions, based on the study of related works: (1) allows the choice of the strategic objective of the organization (cost, time or quality) in the definition of the resource allocation strategy; (2) uses the historical allocations of previous projects. A prototype was implemented and applied in an evaluation involving 2 project managers of 2 organizations who answered structured research in the Technology Acceptance Model (TAM) methodology, confirming the usability of Smartalloc. Then, the Accuracy calculation of the machine learning algorithm was measured, whose ideal value should be 1. In 6 projects in the first company, the average was 0.77. In the second company, the average was 0.70 in 3 projects. Both project managers considered the Smartalloc model to be useful in allocating human resources to projects.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {365–368},
numpages = {4},
keywords = {strategic objective, project management, machine learning, human resources allocation},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/3425269.3425281,
author = {Cunha, Warteruzannan Soyer and Armijo, Guisella Angulo and de Camargo, Valter Vieira},
title = {Investigating Non-Usually Employed Features in the Identification of Architectural Smells: A Machine Learning-Based Approach},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425281},
doi = {10.1145/3425269.3425281},
abstract = {Architectural smells (ASs) negatively affect the maintenance and evolution of software at the architectural level. Most of the current approaches for ASs identification rely on the same small and well-known set of usually employed metrics (UE-Ms) with fixed thresholds. Machine learning (ML) is a promising technique for smell identification as algorithms can learn from a rich set of metrics/features, covering several characteristics of the software and incorporating a certain degree of subjectivity. This has been explored by building datasets with a robust and rich set of features, including not only the UE-Ms but also other non-usually employed metrics (NUE-Ms). However, usually the UE-Ms determine the output of the algorithms, obfuscating other metrics that have the potential to improve the classification. This also leads to inflated and difficult to maintain datasets. In this paper, we investigate the accuracy of some ML algorithms employing only NUE-Ms. We scoped our study in the classification of two smells: God Component and Unstable Dependency. This investigation revealed a set of NUE-Ms that can be also used to identify these smells and the contribution of each one for the classification. Based on this information, software engineers can then build a final dataset just with the potential features. We also briefly present our tool, called InSet, that was used by academics and practitioners to identify smells in their systems. The feedback of them was used as the oracle to compare our tool to other approaches and good results were reached.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {21–30},
numpages = {10},
keywords = {Software Smells, Predictive Model, Machine Learning, Automatic Approach, Architecture Smells, Architecture Anomalies},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.5555/2029427.2029455,
author = {Radli\'{n}ski, \L{}ukasz},
title = {A framework for integrated software quality prediction using Bayesian nets},
year = {2011},
isbn = {9783642219337},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The aim of this study is to develop a framework for integrated software quality prediction. This integration is reflected by a range of quality attributes incorporated in the model as well as relationships between these attributes. The model is formulated as a Bayesian net, a technique that has already been used in various software engineering studies. The framework enables to incorporate expert knowledge about the domain as well as related empirical data and encode them in the Bayesian net model. Such model may be used in decision support for software analysts and managers.},
booktitle = {Proceedings of the 2011 International Conference on Computational Science and Its Applications - Volume Part V},
pages = {310–325},
numpages = {16},
keywords = {software quality prediction, quality model, quality features, framework, Bayesian net},
location = {Santander, Spain},
series = {ICCSA'11}
}

@article{10.1016/j.infsof.2020.106269,
author = {Mahdieh, Mostafa and Mirian-Hosseinabadi, Seyed-Hassan and Etemadi, Khashayar and Nosrati, Ali and Jalali, Sajad},
title = {Incorporating fault-proneness estimations into coverage-based test case prioritization methods},
year = {2020},
issue_date = {May 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {121},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106269},
doi = {10.1016/j.infsof.2020.106269},
journal = {Inf. Softw. Technol.},
month = may,
numpages = {12},
keywords = {Bug history, Machine learning, Defect prediction, Test case prioritization, Regression testing}
}

@inproceedings{10.1109/MICAI.2008.38,
author = {Virk, Shafqat M. and Muhammad, Aslam and Martinez-Enriquez, A. M.},
title = {Fault Prediction Using Artificial Neural Network and Fuzzy Logic},
year = {2008},
isbn = {9780769534411},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MICAI.2008.38},
doi = {10.1109/MICAI.2008.38},
abstract = {This paper studies different vehicle fault prediction techniques, using artificial neural network and fuzzy logic based model. With increasing demands for efficiency and product quality as well as progressing integration of automatic control systems in high-cost mechatronics and safety-critical processes, monitoring is necessary to detect and diagnose faults using symptoms and related data. However, beyond protective maintenance services, it is viable to integrate fault prediction services. Thus, we studied different parameters to model a fault prediction service. This service not only helps to predict faults but is also useful to take precautionary measures to avoid tangible and intangible losses.},
booktitle = {Proceedings of the 2008 Seventh Mexican International Conference on Artificial Intelligence},
pages = {149–154},
numpages = {6},
keywords = {Faults, Artificial Neural Network, Fuzzy Logic, Neuro-Fuzzy, Neuro-Neuro, Recurrent Neural Network, Back-propagation},
series = {MICAI '08}
}

@inproceedings{10.5555/646358.689931,
author = {Almeida, Mauricio Amaral de and Matwin, Stan},
title = {Machine Learning Method for Software Quality Model Building},
year = {1999},
isbn = {354065965X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the 11th International Symposium on Foundations of Intelligent Systems},
pages = {565–573},
numpages = {9},
series = {ISMIS '99}
}

@inproceedings{10.1007/978-3-030-27455-9_1,
author = {Sarro, Federica},
title = {Search-Based Predictive Modelling for Software Engineering: How Far Have We Gone?},
year = {2019},
isbn = {978-3-030-27454-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27455-9_1},
doi = {10.1007/978-3-030-27455-9_1},
abstract = {In this keynote I introduce the use of Predictive Analytics for Software Engineering (SE) and then focus on the use of search-based heuristics to tackle long-standing SE prediction problems including (but not limited to) software development effort estimation and software defect prediction. I review recent research in Search-Based Predictive Modelling for SE in order to assess the maturity of the field and point out promising research directions. I conclude my keynote by discussing best practices for a rigorous and realistic empirical evaluation of search-based predictive models, a condicio sine qua non to facilitate the adoption of prediction models in software industry practices.},
booktitle = {Search-Based Software Engineering: 11th International Symposium, SSBSE 2019, Tallinn, Estonia, August 31 – September 1, 2019, Proceedings},
pages = {3–7},
numpages = {5},
keywords = {Software analytics, Machine learning, Search-based software engineering, Predictive modelling, Predictive analytics},
location = {Tallinn, Estonia}
}

@inproceedings{10.1109/FLOSS.2009.5071357,
author = {Caglayan, Bora and Bener, Ayse and Koch, Stefan},
title = {Merits of using repository metrics in defect prediction for open source projects},
year = {2009},
isbn = {9781424437207},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FLOSS.2009.5071357},
doi = {10.1109/FLOSS.2009.5071357},
abstract = {Many corporate code developers are the beta testers of open source software.They continue testing until they are sure that they have a stable version to build their code on. In this respect defect predictors play a critical role to identify defective parts of the software. Performance of a defect predictor is determined by correctly finding defective parts of the software without giving any false alarms. Having high false alarms means testers/ developers would inspect bug free code unnecessarily. Therefore in this research we focused on decreasing the false alarm rates by using repository metrics. We conducted experiments on the data sets of Eclipse project. Our results showed that repository metrics decreased the false alarm rates on the average to 23% from 32% corresponding up to 907 less files to inspect.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Emerging Trends in Free/Libre/Open Source Software Research and Development},
pages = {31–36},
numpages = {6},
series = {FLOSS '09}
}

@inproceedings{10.1145/1810295.1810313,
author = {Kl\"{a}s, Michael and Elberzhager, Frank and M\"{u}nch, J\"{u}rgen and Hartjes, Klaus and von Graevemeyer, Olaf},
title = {Transparent combination of expert and measurement data for defect prediction: an industrial case study},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810295.1810313},
doi = {10.1145/1810295.1810313},
abstract = {Defining strategies on how to perform quality assurance (QA) and how to control such activities is a challenging task for organizations developing or maintaining software and software-intensive systems. Planning and adjusting QA activities could benefit from accurate estimations of the expected defect content of relevant artifacts and the effectiveness of important quality assurance activities. Combining expert opinion with commonly available measurement data in a hybrid way promises to overcome the weaknesses of purely data-driven or purely expert-based estimation methods. This article presents a case study of the hybrid estimation method HyDEEP for estimating defect content and QA effectiveness in the telecommunication domain. The specific focus of this case study is the use of the method for gaining quantitative predictions. This aspect has not been empirically analyzed in previous work. Among other things, the results show that for defect content estimation, the method performs significantly better statistically than purely data-based methods, with a relative error of 0.3 on average (MMRE).},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
pages = {119–128},
numpages = {10},
keywords = {hybrid estimation, effectiveness, defect content, HyDEEP},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/3362789.3362923,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco J. and Ther\'{o}n, Roberto},
title = {Automatic generation of software interfaces for supporting decision-making processes. An application of domain engineering and machine learning},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362923},
doi = {10.1145/3362789.3362923},
abstract = {Information dashboards are sophisticated tools. Although they enable users to reach useful insights and support their decision-making challenges, a good design process is essential to obtain powerful tools. Users need to be part of these design processes, as they will be the consumers of the information displayed. But users are very diverse and can have different goals, beliefs, preferences, etc., and creating a new dashboard for each potential user is not viable. There exist several tools that allow users to configure their displays without requiring programming skills. However, users might not exactly know what they want to visualize or explore, also becoming the configuration process a tedious task. This research project aims to explore the automatic generation of user interfaces for supporting these decision-making processes. To tackle these challenges, a domain engineering, and machine learning approach is taken. The main goal is to automatize the design process of dashboards by learning from the context, including the end-users and the target data to be displayed.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1007–1011},
numpages = {5},
keywords = {Meta-modeling, Information Dashboards, High-level requirements, Domain engineering, Automatic generation},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@inproceedings{10.1109/ICICIC.2007.308,
author = {Li, Bin and Zhang, Wei-guo and Ning, Dong-fang and Yin, Wei},
title = {Fault Prediction System Based on Neural Network Model},
year = {2007},
isbn = {0769528821},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICICIC.2007.308},
doi = {10.1109/ICICIC.2007.308},
abstract = {In the fault diagnosis of the plane steering surface, exact fault prediction is very important for the security of the aircraft. According to design requirement of the plane steering surface fault prediction system, the application of neural network technique is plane fault prediction is presented, and the algorithm based on the neural network model in the prediction system is given. Considering the advantage of neural network technique, the neural network and the fault prediction technique with expert are combined to form a fault prediction system. Finally, taking the steering surface of plane as an example to realize fault prediction, the result proves that the forecast model and algorithm based on the neural network are feasible.},
booktitle = {Proceedings of the Second International Conference on Innovative Computing, Informatio and Control},
pages = {496},
series = {ICICIC '07}
}

@article{10.1016/j.compeleceng.2014.05.013,
author = {Verma, Alok and Sarangi, Somnath and Kolekar, M.H.},
title = {Stator winding fault prediction of induction motors using multiscale entropy and grey fuzzy optimization methods},
year = {2014},
issue_date = {October 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {7},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2014.05.013},
doi = {10.1016/j.compeleceng.2014.05.013},
abstract = {The prediction of stator winding faults using multiscale entropy is performed for the first time.Real-time vibration and current are used as diagnostics to identify faults.The system complexity associated with motors is investigated using multiscale entropy.GFRG is used to predict fault and also to suggest optimal settings for motor operation.The motor condition has a maximum contribution of 54.21%, as determined from the ANOVA analysis. In the present work, stator winding fault prediction is studied using a multiscale entropy (MSE) algorithm combined with a grey-based fuzzy algorithm. Experiments were performed with a normal motor and a motor with faulty stator winding. Real time, motor current and vibration signals were acquired at different operating speeds and were used for the diagnosis of faults. The obtained signals were denoised by wavelet transform. Grey relational analysis (GRA) coupled with fuzzy logic was used to model the stator winding fault and to predict the optimal setting for running the induction motor within its parameters range. Analysis of variance (ANOVA) was performed to determine the effect of each individual parameter on the response. The results indicate that the proposed novel approach is very effective in predicting the stator winding fault. Furthermore, the best running parameters for the induction motor are also reported.},
journal = {Comput. Electr. Eng.},
month = oct,
pages = {2246–2258},
numpages = {13}
}

@article{10.1504/ijahuc.2020.105461,
author = {Sandhu, Jasminder Kaur and Verma, Anil Kumar and Rana, Prashant Singh},
title = {Enhancing dependability of wireless sensor network under flooding attack: a machine learning perspective},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {33},
number = {2},
issn = {1743-8225},
url = {https://doi.org/10.1504/ijahuc.2020.105461},
doi = {10.1504/ijahuc.2020.105461},
abstract = {The wireless sensor network (WSN) is gaining paramount importance due to its application in real-time monitoring of vast geographical regions. The deployment paradigm shift is taking place from mobile computing to data science. Bridging the two technologies results in the development of dependable network in which security plays a pivotal role. This work considers the flooding attack which causes the communication failure. To detect this attack, an intrusion detection system (IDS) based on the randomised and the normalised deployment of nodes is proposed. Furthermore, machine learning techniques are implemented to enhance the dependability of network under flooding attack. The data flow is a significant parameter for governing the flooding effect on the network. It is found that machine learning models play a significant role in the prediction of the data flow. The experiments on simulated dataset underline the role of machine learning model for data flow prediction on the normalised dataset.},
journal = {Int. J. Ad Hoc Ubiquitous Comput.},
month = jan,
pages = {73–89},
numpages = {16},
keywords = {normalised deployment, randomised deployment, PDR, packet delivery ratio, watchdog, IDS, intrusion detection system, data flow, flooding attack, security, dependability, WSNs, wireless sensor networks, machine learning}
}

@inproceedings{10.1145/1370788.1370793,
author = {Jiang, Yue and Cuki, Bojan and Menzies, Tim and Bartlow, Nick},
title = {Comparing design and code metrics for software quality prediction},
year = {2008},
isbn = {9781605580364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370788.1370793},
doi = {10.1145/1370788.1370793},
abstract = {The prediction of fault-prone modules continues to attract interest due to the significant impact it has on software quality assurance. One of the most important goals of such techniques is to accurately predict the modules where faults are likely to hide as early as possible in the development lifecycle. Design, code, and most recently, requirements metrics have been successfully used for predicting fault-prone modules. The goal of this paper is to compare the performance of predictive models which use design-level metrics with those that use code-level metrics and those that use both. We analyze thirteen datasets from NASA Metrics Data Program which offer design as well as code metrics. Using a range of modeling techniques and statistical significance tests, we confirmed that models built from code metrics typically outperform design metrics based models. However, both types of models prove to be useful as they can be constructed in different project phases. Code-based models can be used to increase the performance of design-level models and, thus, increase the efficiency of assigning verification and validation activities late in the development lifecycle. We also conclude that models that utilize a combination of design and code level metrics outperform models which use either one or the other metric set.},
booktitle = {Proceedings of the 4th International Workshop on Predictor Models in Software Engineering},
pages = {11–18},
numpages = {8},
keywords = {code metrics, design metrics, fault-proneness prediction, machine learning},
location = {Leipzig, Germany},
series = {PROMISE '08}
}

@article{10.1007/s11219-021-09550-5,
author = {Alkharabsheh, Khalid and Crespo, Yania and Fern\'{a}ndez-Delgado, Manuel and Viqueira, Jos\'{e} R. and Taboada, Jos\'{e} A.},
title = {Exploratory study of the impact of project domain and size category on the detection of the God class design smell},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09550-5},
doi = {10.1007/s11219-021-09550-5},
abstract = {Design smell detection has proven to be an efficient strategy to improve software quality and consequently decrease maintainability expenses. This work explores the influence of the&nbsp;information  about&nbsp;project context expressed as project domain and size category information, on the automatic detection of the god class design smell by machine learning techniques. A set of experiments using eight classifiers to detect god classes was conducted on a dataset containing 12, 587 classes from 24 Java projects. The results show that classifiers change their behavior when they are used on datasets that differ in these kinds of project information. The results show that god class design smell detection can be improved by feeding machine learning classifiers with this project context information.},
journal = {Software Quality Journal},
month = jun,
pages = {197–237},
numpages = {41},
keywords = {God class, Project context information, Software metrics, Machine learning, Design smell detection}
}

@article{10.5555/1534462.1534465,
author = {Lazic, Ljubomir and Kola\v{s};inac, Amel and Avdic, D\v{z}enan},
title = {The software quality economics model for software project optimization},
year = {2009},
issue_date = {January 2009},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
volume = {8},
number = {1},
issn = {1109-2750},
abstract = {There are many definitions of quality being given by experts that explains quality for manufacturing industry but still unable to define it with absolute clarity for software engineering. To enable software designers to achieve a higher quality for their design, a better insight into quality predictions for their design choices is given. In this paper we propose a model which traces design decisions and the possible alternatives. With this model it is possible to minimize the cost of switching between design alternatives, when the current choice cannot fulfill the quality constraints. With this model we do not aim to automate the software design process or the identification of design alternatives. Much rather we aim to define a method with which it is possible to assist the software engineer in evaluating design alternatives and adjusting design decisions in a systematic manner. As of today there is very little knowledge is available about the economics of software quality. The costs incurred and benefits of implementing different quality practices over the software development life cycle are not well understood. There are some prepositions, which are not being tested comprehensively, but some useful Economic Model of Software Quality Costs (CoSQ) and data from industry are described in this article. Significant research is needed to understand the economics of implementing quality practices and its behaviour. Such research must evaluate the cost benefit trade-offs in investing in quality practices where the returns are maximized over the software development life cycle. From a developer's perspective, there are two types of benefits that can accrue from the implementation of good software quality practices and tools: money and time. A financial ROI calculation of cost savings and the schedule ROI calculation of schedule savings are given.},
journal = {W. Trans. on Comp.},
month = jan,
pages = {21–47},
numpages = {27},
keywords = {software quality, quality cost model, cost optimization, TQM, ROI calculation}
}

@article{10.1007/s11219-019-09490-1,
author = {Kudjo, Patrick Kwaku and Chen, Jinfu and Mensah, Solomon and Amankwah, Richard and Kudjo, Christopher},
title = {The effect of Bellwether analysis on software vulnerability severity prediction models},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09490-1},
doi = {10.1007/s11219-019-09490-1},
abstract = {Vulnerability severity prediction (VSP) models provide useful insight for vulnerability prioritization and software maintenance. Previous studies have proposed a variety of machine learning algorithms as an important paradigm for VSP. However, to the best of our knowledge, there are no other existing research studies focusing on investigating how a subset of features can be used to improve VSP. To address this deficiency, this paper presents a general framework for VSP using the Bellwether analysis (i.e., exemplary data). First, we apply the natural language processing techniques to the textual descriptions of software vulnerability. Next, we developed an algorithm termed Bellvul to identify and select an exemplary subset of data (referred to as Bellwether) to be considered as the training set to yield improved prediction accuracy against the growing portfolio, within-project cases, and the k-fold cross-validation subset. Finally, we assessed the performance of four machine learning algorithms, namely, deep neural network, logistic regression, k-nearest neighbor, and random forest using the sampled instances. The prediction results of the suggested models and the benchmark techniques were assessed based on the standard classification evaluation metrics such as precision, recall, and F-measure. The experimental result shows that the Bellwether approach achieves F-measure ranging from 14.3% to 97.8%, which is an improvement over the benchmark techniques. In conclusion, the proposed approach is a promising research direction for assisting software engineers when seeking to predict instances of vulnerability records that demand much attention prior to software release.},
journal = {Software Quality Journal},
month = dec,
pages = {1413–1446},
numpages = {34},
keywords = {Severity, Machine learning algorithms, Feature selection, Software vulnerability, Bellwether}
}

@article{10.5555/2698256.2698490,
author = {Pizzi, Nick J.},
title = {Software quality prediction using fuzzy integration: a case study},
year = {2008},
issue_date = {January   2008},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {1},
issn = {1432-7643},
abstract = {Given the complexity of many contemporary software systems, it is often difficult to gauge the overall quality of their underlying software components. A potential technique to automatically evaluate such qualitative attributes is to use software metrics as quantitative predictors. In this case study, an aggregation technique based on fuzzy integration is presented that combines the predicted qualitative assessments from multiple classifiers. Multiple linear classifiers are presented with randomly selected subsets of automatically generated software metrics describing components from a sophisticated biomedical data analysis system. The external reference test is a software developer's thorough assessment of complexity, maintainability, and usability, which is used to assign corresponding quality class labels to each system component. The aggregated qualitative predictions using fuzzy integration are shown to be superior to the predictions from the respective best single classifiers.},
journal = {Soft Comput.},
month = jan,
pages = {67–76},
numpages = {10}
}

@article{10.1016/j.infsof.2006.07.005,
author = {Kanmani, S. and Uthariaraj, V. Rhymend and Sankaranarayanan, V. and Thambidurai, P.},
title = {Object-oriented software fault prediction using neural networks},
year = {2007},
issue_date = {May, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {5},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.07.005},
doi = {10.1016/j.infsof.2006.07.005},
abstract = {This paper introduces two neural network based software fault prediction models using Object-Oriented metrics. They are empirically validated using a data set collected from the software modules developed by the graduate students of our academic institution. The results are compared with two statistical models using five quality attributes and found that neural networks do better. Among the two neural networks, Probabilistic Neural Networks outperform in predicting the fault proneness of the Object-Oriented modules developed.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {483–492},
numpages = {10},
keywords = {Probabilistic neural network, Object-Oriented metrics, Logistic regression, Fault proneness, Discriminant analysis, Back propagation neural network}
}

@inproceedings{10.5555/1337691.1338390,
author = {Jiang, Yue and Cukic, Bojan and Menzies, Tim},
title = {Fault Prediction using Early Lifecycle Data},
year = {2007},
isbn = {0769530249},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The prediction of fault-prone modules in a software project has been the topic of many studies. In this paper, we investigate whether metrics available early in the development lifecycle can be used to identify fault-prone software modules. More precisely, we build predictive models using the metrics that characterize textual requirements. We compare the performance of requirements-based models against the performance of code-based models and models that combine requirement and code metrics. Using a range of modeling techniques and the data from three NASA projects, our study indicates that the early lifecycle metrics can play an important role in project management, either by pointing to the need for increased quality monitoring during the development or by using the models to assign verification and validation activities.},
booktitle = {Proceedings of the The 18th IEEE International Symposium on Software Reliability},
pages = {237–246},
numpages = {10},
series = {ISSRE '07}
}

@article{10.1016/j.infsof.2009.05.003,
author = {Azar, D. and Harmanani, H. and Korkmaz, R.},
title = {A hybrid heuristic approach to optimize rule-based software quality estimation models},
year = {2009},
issue_date = {September, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.05.003},
doi = {10.1016/j.infsof.2009.05.003},
abstract = {Software quality is defined as the degree to which a software component or system meets specified requirements and specifications. Assessing software quality in the early stages of design and development is crucial as it helps reduce effort, time and money. However, the task is difficult since most software quality characteristics (such as maintainability, reliability and reusability) cannot be directly and objectively measured before the software product is deployed and used for a certain period of time. Nonetheless, these software quality characteristics can be predicted from other measurable software quality attributes such as complexity and inheritance. Many metrics have been proposed for this purpose. In this context, we speak of estimating software quality characteristics from measurable attributes. For this purpose, software quality estimation models have been widely used. These take different forms: statistical models, rule-based models and decision trees. However, data used to build such models is scarce in the domain of software quality. As a result, the accuracy of the built estimation models deteriorates when they are used to predict the quality of new software components. In this paper, we propose a search-based software engineering approach to improve the prediction accuracy of software quality estimation models by adapting them to new unseen software products. The method has been implemented and favorable result comparisons are reported in this work.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {1365–1376},
numpages = {12},
keywords = {Software quality, Soft computing, Search-based software engineering}
}

@inproceedings{10.1145/3460945.3464954,
author = {Hasabnis, Niranjan and Gottschlich, Justin},
title = {ControlFlag: a self-supervised idiosyncratic pattern detection system for software control structures},
year = {2021},
isbn = {9781450384674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460945.3464954},
doi = {10.1145/3460945.3464954},
abstract = {Software debugging has been shown to utilize upwards of half of developers’ time. Yet, machine programming (MP), the field concerned with the automation of software (and hardware) development, has recently made strides in both research and production-quality automated debugging systems. In this paper we present ControlFlag, a self-supervised MP system that aims to improve debugging by attempting to detect idiosyncratic pattern violations in software control structures. ControlFlag also suggests possible corrections in the event an anomalous pattern is detected. We present ControlFlag’s design and provide an experimental evaluation and analysis of its efficacy in identifying potential programming errors in production-quality software. As a first concrete evidence towards improving software quality, ControlFlag has already found an anomaly in CURL that has been acknowledged and fixed by its developers. We also discuss future extensions of ControlFlag.},
booktitle = {Proceedings of the 5th ACM SIGPLAN International Symposium on Machine Programming},
pages = {32–42},
numpages = {11},
keywords = {self-supervised learning, Source-code mining},
location = {Virtual, Canada},
series = {MAPS 2021}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00061,
author = {Liu, Changlin and Xiao, Xusheng},
title = {ProMal: precise window transition graphs for Android via synergy of program analysis and machine learning},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00061},
doi = {10.1109/ICSE-Companion52605.2021.00061},
abstract = {Mobile apps have been an integral part in our daily life. As these apps become more complex, it is critical to provide automated analysis techniques to ensure the correctness, security, and performance of these apps. A key component for these automated analysis techniques is to create a graphical user interface (GUI) model of an app, i.e., a window transition graph (WTG), that models windows and transitions among the windows. While existing work has provided both static and dynamic analysis to build the WTG for an app, the constructed WTG misses many transitions or contains many infeasible transitions due to the coverage issues of dynamic analysis and over-approximation of the static analysis. We propose ProMal, a "tribrid" analysis that synergistically combines static analysis, dynamic analysis, and machine learning to construct a precise WTG. Specifically, ProMal first applies static analysis to build a static WTG, and then applies dynamic analysis to verify the transitions in the static WTG. For the unverified transitions, ProMal further provides machine learning techniques that leverage runtime information (i.e., screenshots, UI layouts, and text information) to predict whether they are feasible transitions. Our evaluations on 40 real-world apps demonstrate the superiority of ProMal in building WTGs over static analysis, dynamic analysis, and machine learning techniques when they are applied separately.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {144–146},
numpages = {3},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@article{10.1016/j.jss.2004.06.018,
author = {Pedrycz, Witold and Succi, Giancarlo},
title = {Genetic granular classifiers in modeling software quality},
year = {2005},
issue_date = {June 2005},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2004.06.018},
doi = {10.1016/j.jss.2004.06.018},
abstract = {Hyperbox classifiers are one of the most appealing and intuitively transparent classification schemes. As the name itself stipulates, these classifiers are based on a collection of hyperboxes--generic and highly interpretable geometric descriptors of data belonging to a given class. The hyperboxes translate into conditional statements (rules) of the form ''if feature"1 is in [a,b] and feature"2 is in [d,f] and ... and feature"n is in [w,z] then class @w'' where the intervals ([a,b],...,[w,z]) are the respective edges of the hyperbox. The proposed design process of hyperboxes comprises of two main phases. In the first phase, a collection of ''seeds'' of the hyperboxes is formed through data clustering (realized by means of the Fuzzy C-Means algorithm, FCM). In the second phase, the hyperboxes are ''grown'' (expanded) by applying mechanisms of genetic optimization (and genetic algorithm, in particular). We reveal how the underlying geometry of the hyperboxes supports an immediate interpretation of software data concerning software maintenance and dealing with rules describing a number of changes made to software modules and their linkages with various software measures (such as size of code, McCabe cyclomatic complexity, number of comments, number of characters, etc.).},
journal = {J. Syst. Softw.},
month = jun,
pages = {277–285},
numpages = {9},
keywords = {Software quality, Software measures, Hyperbox geometry of classifiers, Genetic algorithms, Fuzzy clustering, Fuzzy C-Means (FCM)}
}

@article{10.1016/j.eswa.2009.04.047,
author = {Hou, Shumin and Li, Yourong},
title = {Short-term fault prediction based on support vector machines with parameter optimization by evolution strategy},
year = {2009},
issue_date = {December, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {36},
number = {10},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2009.04.047},
doi = {10.1016/j.eswa.2009.04.047},
abstract = {Support vector machines (SVMs) are the effective machine-learning methods based on the structural risk minimization (SRM) principle, which is an approach to minimize the upper bound risk functional related to the generalization performance. The parameter selection is an important factor that impacts the performance of SVMs. Evolution Strategy with Covariance Matrix Adaptation (CMA-ES) is an evolutionary optimization strategy, which is used to optimize the parameters of SVMs in this paper. Compared with the traditional SVMs, the optimal SVMs using CMA-ES have more accuracy in predicting the Lorenz signal. The industry case illustrates that the proposed method is very successfully in forecasting the short-term fault of large machinery.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {12383–12391},
numpages = {9},
keywords = {Support vector machines, Fault prediction, Evolutionary algorithms}
}

@article{10.1023/A:1023760326768,
author = {Zhang, Du and Tsai, Jeffrey J. P.},
title = {Machine Learning and Software Engineering},
year = {2003},
issue_date = {June 2003},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {11},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1023/A:1023760326768},
doi = {10.1023/A:1023760326768},
abstract = {Machine learning deals with the issue of how to build programs that improve their performance at some task through experience. Machine learning algorithms have proven to be of great practical value in a variety of application domains. They are particularly useful for (a) poorly understood problem domains where little knowledge exists for the humans to develop effective algorithms; (b) domains where there are large databases containing valuable implicit regularities to be discovered; or (c) domains where programs must adapt to changing conditions. Not surprisingly, the field of software engineering turns out to be a fertile ground where many software development and maintenance tasks could be formulated as learning problems and approached in terms of learning algorithms. This paper deals with the subject of applying machine learning in software engineering. In the paper, we first provide the characteristics and applicability of some frequently utilized machine learning algorithms. We then summarize and analyze the existing work and discuss some general issues in this niche area. Finally we offer some guidelines on applying machine learning methods to software engineering tasks and use some software development and maintenance tasks as examples to show how they can be formulated as learning problems and approached in terms of learning algorithms.},
journal = {Software Quality Journal},
month = jun,
pages = {87–119},
numpages = {33},
keywords = {software engineering, machine learning, learning algorithms}
}

@article{10.1016/j.asoc.2014.03.030,
author = {Chatterjee, Subhashish and Roy, Arunava},
title = {Web software fault prediction under fuzzy environment using MODULO-M multivariate overlapping fuzzy clustering algorithm and newly proposed revised prediction algorithm},
year = {2014},
issue_date = {September, 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {22},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2014.03.030},
doi = {10.1016/j.asoc.2014.03.030},
abstract = {In recent years some research works have been carried out on web software error analysis and reliability predictions. In all these works the web environment has been considered as crisp one, which is not a very realistic assumption. Moreover, web error forecasting remains unworthy for the researchers for quite a long time. Furthermore, among various well known forecasting techniques, fuzzy time series based methods are extensively used, though they are suffering from some serious drawbacks, viz., fixed sized intervals, using some fixed membership values (0, 0.5, and 1) and moreover, the defuzzification process only deals with the factor that is to be predicted. Prompted by these facts, the present authors have proposed a novel multivariate fuzzy forecasting algorithm that is able to remove all the aforementioned drawbacks as also can predict the future occurrences of different web failures (considering the web environment as fuzzy) with better predictive accuracy. Also, the complexity analysis of the proposed algorithm is done to unveil its better run time complexity. Moreover, the comparisons with the other existing frequently used forecasting algorithms were performed to demonstrate its better efficiency and predictive accuracy. Additionally, at the very end, the developed algorithm was applied on the real web failure data of http://www.ismdhanbad.ac.in/, the official website of ISM Dhanbad, India, collected from the corresponding HTTP log files.},
journal = {Appl. Soft Comput.},
month = sep,
pages = {372–396},
numpages = {25},
keywords = {Web software reliability, Web errors, Server logs, Fuzzy time series, Fuzzy clustering, Algorithm}
}

@inproceedings{10.1145/3416505.3423561,
author = {Vasiliev, Roman and Koznov, Dmitrij and Chernishev, George and Khvorov, Aleksandr and Luciv, Dmitry and Povarov, Nikita},
title = {TraceSim: a method for calculating stack trace similarity},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423561},
doi = {10.1145/3416505.3423561},
abstract = {Many contemporary software products have subsystems for automatic crash reporting. However, it is well-known that the same bug can produce slightly different reports. To manage this problem, reports are usually grouped, often manually by developers. Manual triaging, however, becomes infeasible for products that have large userbases, which is the reason for many different approaches to automating this task. Moreover, it is important to improve quality of triaging due to a large volume of reports that needs to be processed properly. Therefore, even a relatively small improvement could play a significant role in the overall accuracy of report bucketing. The majority of existing studies use some kind of a stack trace similarity metric, either based on information retrieval techniques or string matching methods. However, it should be stressed that the quality of triaging is still insufficient.  In this paper, we describe TraceSim — a novel approach to this problem which combines TF-IDF, Levenshtein distance, and machine learning to construct a similarity metric. Our metric has been implemented inside an industrial-grade report triaging system. The evaluation on a manually labeled dataset shows significantly better results compared to baseline approaches.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {25–30},
numpages = {6},
keywords = {Stack Trace, Software Repositories, Software Engineering, Information Retrieval, Duplicate Crash Report, Duplicate Bug Report, Deduplication, Crash Stack, Crash Reports, Crash Report Deduplication, Automatic Problem Reporting Tools, Automatic Crash Reporting},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@inproceedings{10.5555/2486788.2487006,
author = {Jonsson, Leif},
title = {Increasing anomaly handling efficiency in large organizations using applied machine learning},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Maintenance costs can be substantial for large organizations (several hundreds of programmers) with very large and complex software systems. By large we mean lines of code in the range of hundreds of thousands or millions. Our research objective is to improve the process of handling anomaly reports for large organizations. Specifically, we are addressing the problem of the manual, laborious and time consuming process of assigning anomaly reports to the correct design teams and the related issue of localizing faults in the system architecture. In large organizations, with complex systems, this is particularly problematic because the receiver of an anomaly report may not have detailed knowledge of the whole system. As a consequence, anomaly reports may be assigned to the wrong team in the organization, causing delays and unnecessary work. We have so far developed two machine learning prototypes to validate our approach. The latest, a re-implementation and extension, of the first is being evaluated on four large systems at Ericsson AB. Our main goal is to investigate how large software development organizations can significantly improve development efficiency by replacing manual anomaly report assignment and fault localization with machine learning techniques. Our approach focuses on training machine learning systems on anomaly report databases; this is in contrast to many other approaches that are based on test case execution combined with program sampling and/or source code analysis.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1361–1364},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.5555/1563296.1563307,
author = {Denis, Kozlov},
title = {Approaches to Qualitative Evaluation of the Software Quality Attributes: Overview},
year = {2005},
isbn = {1586035568},
publisher = {IOS Press},
address = {NLD},
abstract = {An overview of three most frequently used methods for qualitative evaluation of entities is presented in the paper, namely Overall Integral Index, Fuzzy Analytic Hierarchy Process and Consensus Relation. The implementation of these approaches to the measurement of the software quality attributes is discussed.},
booktitle = {Proceedings of the 2005 Conference on New Trends in Software Methodologies, Tools and Techniques: Proceedings of the Fourth SoMeT_W05},
pages = {103–110},
numpages = {8},
keywords = {Software Quality Attributes, Evaluation}
}

@phdthesis{10.5555/2519464,
author = {Pelayo Ramirez, Lourdes},
advisor = {Dick, Scott},
title = {Developing and evaluating methods for mitigating sample selection bias in machine learning},
year = {2011},
isbn = {9780494892763},
publisher = {University of Alberta},
address = {CAN},
abstract = {The imbalanced learning problem occurs in a large number of economic and health domains of great importance; consequently, it has drawn a significant amount of interest from academia, industry, and government funding agencies. Several researchers have used stratification to alleviate this problem; however, it is not clear what stratification strategy is in general more effective: under-sampling, over-sampling or the combination of both. Our first topic evaluates the contribution of stratification strategies in the software defect prediction area. We study the statistical contribution of stratification in the new Mozilla dataset, a new large-scale software defect prediction dataset which includes both object-oriented metrics and a count of defects per module. Our second topic responds to the debate about the contribution of over-sampling, under-sampling and the combination of both with the employment of a full-factorial design experiment using the Analysis of Variance (ANOVA) over six software defect prediction datasets. We extend our research to develop a stratification method to mitigate sample selection bias in function approximation problems. The sample selection bias is present when the training and test instances are drawn from a different distribution, with the imbalance dataset problem considered a particular case of sample selection bias. We extend the well-known SMOTE over-sampling technique to continuous-valued response variables. Our new algorithm proves to be a valuable algorithm helping to increase the performance on function approximation problems and effectively reducing the impact of sample selection bias.},
note = {AAINR89276}
}

@inproceedings{10.1145/2597073.2597135,
author = {Farah, Gabriel and Tejada, Juan Sebastian and Correal, Dario},
title = {OpenHub: a scalable architecture for the analysis of software quality attributes},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597135},
doi = {10.1145/2597073.2597135},
abstract = {There is currently a vast array of open source projects available on the web, and although they are searchable by name or description in the search engines, there is no way to search for projects by how well they perform on a given set of quality attributes (e.g. usability or maintainability). With OpenHub, we present a scalable and extensible architecture for the static and runtime analysis of open source repositories written in Python, presenting the architecture and pinpointing future possibilities with it.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {420–423},
numpages = {4},
keywords = {Quality Attributes, Python, GitHub, Architecture},
location = {Hyderabad, India},
series = {MSR 2014}
}

@article{10.1504/IJCAT.2006.011996,
author = {Khoshgoftaar, Taghi M. and Rebours, Pierre},
title = {Noise elimination with partitioning filter for software quality estimation},
year = {2006},
issue_date = {October 2006},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {27},
number = {4},
issn = {0952-8091},
url = {https://doi.org/10.1504/IJCAT.2006.011996},
doi = {10.1504/IJCAT.2006.011996},
abstract = {We present two new noise filtering techniques which improve the quality of training datasets by removing data points that are likely to be noisy. In addition, a new measure called 'efficiency paired comparison' is introduced for simplifying the comparison between two filters. The filtering techniques are based on the partitioning approach the training dataset is first split into subsets, and base learners are induced on each of these subsets. The predictions are then combined in such a way that an instance in the training data is identified as noisy if it is misclassified by a certain number of base learners. The first technique, multiple partitioning filter combines several classifiers induced on each subset. The second technique, iterative-partitioning filter uses only one base learner but goes through multiple filtering iterations. The amount of noise removed by the techniques is varied by tuning either the filtering level or the number of iterations. Empirical studies using software measurement data from a high assurance software project assess the efficiencies of our two noise filtering approaches. The empirical results suggest that using several base classifiers as well as performing several iterations with a conservative filtering scheme can improve the efficiency of the filtering technique.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {246–258},
numpages = {13},
keywords = {software quality, partitioning filters, noise elimination, noise detection, iterative filtering, filtering level, data quality, data mining}
}

@inproceedings{10.1109/ICMLA.2007.14,
author = {Khoshgoftaar, Taghi M. and Bullard, Lofton A. and Gao, Kehan},
title = {An Application of a Rule-Based Model in Software Quality Classification},
year = {2007},
isbn = {0769530699},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2007.14},
doi = {10.1109/ICMLA.2007.14},
abstract = {A new rule-based classification model (RBCM) and rulebased model selection technique are presented. The RBCM utilizes rough set theory to significantly reduce the number of attributes, discretation to partition the domain of attribute values, and Boolean predicates to generate the decision rules that comprise the model. When the domain values of an attribute are continuous and relatively large, rough set theory requires that they be discretized. The subsequent discretized domain must have the same characteristics as the original domain values. However, this can lead to a large number of partitions of the attribute's domain space, which in turn leads to large rule sets. These rule sets tend to form models that over-fit. To address this issue, the proposed rule-based model adopts a new model selection strategy that minimizes over-fitting for the RBCM. Empirical validation of the RBCM is accomplished through a case study on a large legacy telecommunications system. The results demonstrate that the proposed RBCM and the model selection strategy are effective in identifying the classification model that minimizes over-fitting and high cost classification errors. Keywords: rule-based classification model, rough set, reducts, discretization, software quality classification},
booktitle = {Proceedings of the Sixth International Conference on Machine Learning and Applications},
pages = {204–210},
numpages = {7},
series = {ICMLA '07}
}

@article{10.1007/s11219-005-6215-9,
author = {Stavrinoudis, Dimitris and Xenos, Michalis and Peppas, Pavlos and Christodoulakis, Dimitris},
title = {Early Estimation of Users' Perception of Software Quality},
year = {2005},
issue_date = {June 2005},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-005-6215-9},
doi = {10.1007/s11219-005-6215-9},
abstract = {This paper presents a methodology for estimating users' opinion of the quality of a software product. Users' opinion changes with time as they progressively become more acquainted with the software product. In this paper, we study the dynamics of users' opinion and offer a method for assessing users' final perception, based on measurements in the early stages of product release. The paper also presents methods for collecting users' opinion and from the derived data, shows how their initial belief state for the quality of the product is formed. It adapts aspects of Belief Revision theory in order to present a way of estimating users' opinion, subsequently formed after their opinion revisions. This estimation is achieved by using the initial measurements and without having to conduct surveys frequently. It reports the correlation that users tend to infer among quality characteristics and represents this correlation through a determination of a set of constraints between the scores of each quality characteristic. Finally, this paper presents a fast and automated way of forming users' new belief state for the quality of a product after examining their opinion revisions.},
journal = {Software Quality Journal},
month = jun,
pages = {155–175},
numpages = {21},
keywords = {users' opinion, software quality measurements, quality characteristics, belief revision, ISO9126, Grove's System of Spheres}
}

@inproceedings{10.1109/VLHCC.2008.4639052,
author = {Langelier, Guillaume and Sahraoui, Houari and Poulin, Pierre},
title = {Exploring the evolution of software quality with animated visualization},
year = {2008},
isbn = {9781424425280},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/VLHCC.2008.4639052},
doi = {10.1109/VLHCC.2008.4639052},
abstract = {Assessing software quality and understanding how events in its evolution have lead to anomalies are two important steps toward reducing costs in software maintenance. Unfortunately, evaluation of large quantities of code over several versions is a task too time-consuming, if not overwhelming, to be applicable in general.},
booktitle = {Proceedings of the 2008 IEEE Symposium on Visual Languages and Human-Centric Computing},
pages = {13–20},
numpages = {8},
series = {VLHCC '08}
}

@inproceedings{10.5555/2394450.2394484,
author = {Catal, Cagatay and Diri, Banu},
title = {Software fault prediction with object-oriented metrics based artificial immune recognition system},
year = {2007},
isbn = {3540734597},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software testing is a time-consuming and expensive process. Software fault prediction models are used to identify fault-prone classes automatically before system testing. These models can reduce the testing duration, project risks, resource and infrastructure costs. In this study, we propose a novel fault prediction model to improve the testing process. Chidamber-Kemerer Object-Oriented metrics and method-level metrics such as Halstead and McCabe are used as independent metrics in our Artificial Immune Recognition System based model. According to this study, class-level metrics based model which applies AIRS algorithm can be used successfully for fault prediction and its performance is higher than J48 based approach. A fault prediction tool which uses this model can be easily integrated into the testing process.},
booktitle = {Proceedings of the 8th International Conference on Product-Focused Software Process Improvement},
pages = {300–314},
numpages = {15},
location = {Riga, Latvia},
series = {PROFES'07}
}

@article{10.4018/jaec.2010070104,
author = {Mohanty, Ramakanta and Ravi, V. and Patra, M. R.},
title = {Application of Machine Learning Techniques to Predict Software Reliability},
year = {2010},
issue_date = {July 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {3},
issn = {1942-3594},
url = {https://doi.org/10.4018/jaec.2010070104},
doi = {10.4018/jaec.2010070104},
abstract = {In this paper, the authors employed machine learning techniques, specifically, Back propagation trained neural network (BPNN), Group method of data handling (GMDH), Counter propagation neural network (CPNN), Dynamic evolving neuro-fuzzy inference system (DENFIS), Genetic Programming (GP), TreeNet, statistical multiple linear regression (MLR), and multivariate adaptive regression splines (MARS), to accurately forecast software reliability. Their effectiveness is demonstrated on three datasets taken from literature, where performance is compared in terms of normalized root mean square error (NRMSE) obtained in the test set. From rigorous experiments conducted, it was observed that GP outperformed all techniques in all datasets, with GMDH coming a close second.},
journal = {Int. J. Appl. Evol. Comput.},
month = jul,
pages = {70–86},
numpages = {17},
keywords = {Software Reliability, Machine Learning Techniques, Group Method of Data Handling, Genetic Programming, Dynamic Evolving Neuro-Fuzzy Inference System, Counter Propagation Neural Network, Back Propagation Trained Neural Network}
}

@inproceedings{10.1007/11427469_146,
author = {Wang, Qi and Zhu, Jie and Yu, Bo},
title = {Combining classifiers in software quality prediction: a neural network approach},
year = {2005},
isbn = {3540259147},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11427469_146},
doi = {10.1007/11427469_146},
abstract = {Software quality prediction models seek to predict quality factors such as whether a component is fault prone or not. This can be treated as a kind of pattern recognition problem. In pattern recognition, there is a growing use of multiple classifier combinations with the goal to increase recognition performance. In this paper, we propose a neural network approach to combine multiple classifiers. The combination network consists of two neural networks: a Kohonen self-organization network and a multilayer perceptron network. The multilayer perceptron network is used as Dynamic Selection Network (DSN) and Kohonen self-organization network is served as the final combiner. A case study illustrates our approach and provides the evidence that the combination network with DSN performs better than some other popular combining schemes and the DSN can efficiently improve the performance of the combination network.},
booktitle = {Proceedings of the Second International Conference on Advances in Neural Networks - Volume Part III},
pages = {921–926},
numpages = {6},
location = {Chongqing, China},
series = {ISNN'05}
}

@article{10.1007/s00521-016-2327-3,
author = {Nassif, Ali Bou and Azzeh, Mohammad and Banitaan, Shadi and Neagu, Daniel},
title = {Guest editorial: special issue on predictive analytics using machine learning},
year = {2016},
issue_date = {November  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {8},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-016-2327-3},
doi = {10.1007/s00521-016-2327-3},
journal = {Neural Comput. Appl.},
month = nov,
pages = {2153–2155},
numpages = {3}
}

@inproceedings{10.1109/QSIC.2008.29,
author = {Briand, Lionel C.},
title = {Novel Applications of Machine Learning in Software Testing},
year = {2008},
isbn = {9780769533124},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/QSIC.2008.29},
doi = {10.1109/QSIC.2008.29},
abstract = {Machine learning techniques have long been used for various purposes in software engineering. This paper provides a brief overview of the state of the art and reports on a number of novel applications I was involved with in the area of software testing. Reflecting on this personal experience, I draw lessons learned and argue that more research should be performed in that direction as machine learning has the potential to significantly help in addressing some of the long-standing software testing problems.},
booktitle = {Proceedings of the 2008 The Eighth International Conference on Quality Software},
pages = {3–10},
numpages = {8},
keywords = {software testing, Machine learning},
series = {QSIC '08}
}

@article{10.1109/TSE.2013.11,
author = {Shepperd, Martin and Song, Qinbao and Sun, Zhongbin and Mair, Carolyn},
title = {Data Quality: Some Comments on the NASA Software Defect Datasets},
year = {2013},
issue_date = {September 2013},
publisher = {IEEE Press},
volume = {39},
number = {9},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2013.11},
doi = {10.1109/TSE.2013.11},
abstract = {Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.},
journal = {IEEE Trans. Softw. Eng.},
month = sep,
pages = {1208–1215},
numpages = {8},
keywords = {machine learning, defect prediction, data quality, Sun, Software, PROM, NASA, Empirical software engineering, Educational institutions, Communities, Abstracts}
}

@article{10.1016/j.infsof.2015.07.004,
author = {Huang, Jianglin and Li, Yan-Fu and Xie, Min},
title = {An empirical analysis of data preprocessing for machine learning-based software cost estimation},
year = {2015},
issue_date = {November 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {67},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.07.004},
doi = {10.1016/j.infsof.2015.07.004},
abstract = {ContextDue to the complex nature of software development process, traditional parametric models and statistical methods often appear to be inadequate to model the increasingly complicated relationship between project development cost and the project features (or cost drivers). Machine learning (ML) methods, with several reported successful applications, have gained popularity for software cost estimation in recent years. Data preprocessing has been claimed by many researchers as a fundamental stage of ML methods; however, very few works have been focused on the effects of data preprocessing techniques. ObjectiveThis study aims for an empirical assessment of the effectiveness of data preprocessing techniques on ML methods in the context of software cost estimation. MethodIn this work, we first conduct a literature survey of the recent publications using data preprocessing techniques, followed by a systematic empirical study to analyze the strengths and weaknesses of individual data preprocessing techniques as well as their combinations. ResultsOur results indicate that data preprocessing techniques may significantly influence the final prediction. They sometimes might have negative impacts on prediction performance of ML methods. ConclusionIn order to reduce prediction errors and improve efficiency, a careful selection is necessary according to the characteristics of machine learning methods, as well as the datasets used for software cost estimation.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {108–127},
numpages = {20},
keywords = {Software cost estimation, Scaling, Missing-data treatments, Feature selection, Data preprocessing, Case selection}
}

@inproceedings{10.1145/3416506.3423578,
author = {Fan, Ming and Jia, Ang and Liu, Jingwen and Liu, Ting and Chen, Wei},
title = {When representation learning meets software analysis},
year = {2020},
isbn = {9781450381253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416506.3423578},
doi = {10.1145/3416506.3423578},
abstract = {In recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). Especially, representation learning, which can learn vectors from the syntactic and semantics of the code, offers much convenience and promotion for the downstream tasks such as code search and vulnerability detection. In this work, we introduce our two applications of leveraging representation learning for software analysis, including defect prediction and vulnerability detection.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages},
pages = {17–18},
numpages = {2},
keywords = {vulnerability detection, representation learning, defect prediction},
location = {Virtual, USA},
series = {RL+SE&amp;PL 2020}
}

@inproceedings{10.5555/2664398.2664412,
author = {Yang, Jiachen and Hotta, Keisuke and Higo, Yoshiki and Igaki, Hiroshi and Kusumoto, Shinji},
title = {Filtering clones for individual user based on machine learning analysis},
year = {2012},
isbn = {9781467317955},
publisher = {IEEE Press},
abstract = {Results from code clone detectors may contain plentiful useless code clones, and judging whether a code clone is useful varies from user to user based on different purposes of them. We are planing a system to study the judgment of each individual user by applying machine learning algorithms on code clones. We describe the reason why individual judgment should be respected and how in this paper.},
booktitle = {Proceedings of the 6th International Workshop on Software Clones},
pages = {76–77},
numpages = {2},
keywords = {token-based, machine learning, judgment of user, filtering, code clone detector, classify},
location = {Zurich, Switzerland},
series = {IWSC '12}
}

@article{10.1016/j.jss.2004.04.027,
author = {Khoshgoftaar, Taghi M. and Seliya, Naeem and Herzberg, Angela},
title = {Resource-oriented software quality classification models},
year = {2005},
issue_date = {May 2005},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {2},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2004.04.027},
doi = {10.1016/j.jss.2004.04.027},
abstract = {Developing high-quality software within the allotted time and budget is a key element for a productive and successful software project. Software quality classification models that provide a risk-based quality estimation, such as fault-prone (fp) and not fault-prone (nfp), have proven their usefulness as software quality assurance techniques. However, their usefulness is largely dependent on the availability of resources for deploying quality improvements to modules predicted as fp. Since every project has its own special needs and specifications, we feel a classification modeling approach based on resource availability is greatly warranted.We propose and demonstrate the use of a resource-based measure, i.e., "Modified Expected Cost of Misclassification" (MECM), for selecting and evaluating classification models. It is an extension of the "Expected Cost of Misclassification" (ECM) measure, which we have previously applied for model-evaluation purposes. The proposed measure facilitates building resource-oriented classification models and overcomes the limitation of ECM, which assumes that enough resources are available to enhance all modules predicted as fp. The primary aspect of MECM is that it penalizes a model, in terms of costs of misclassifications, if the model predicts more number of fp modules than the number that can be enhanced with the available resources. Based on the resources available for improving quality of software modules, a practitioner can use the proposed methodology to select a model that bestsuits the projects goals. Hence, the best possible and practical usage of the available resources can be achieved. The application, analysis, and benefits of MECM is shown by developing models using Logistic Regression. It is concluded that the use of MECM is a promising approach for practical software quality improvement.},
journal = {J. Syst. Softw.},
month = may,
pages = {111–126},
numpages = {16}
}

@article{10.2478/v10065-011-0032-5,
author = {Radli\'{n}Ski, \L{}Ukasz},
title = {A conceptual Bayesian net model for integrated software quality prediction},
year = {2011},
issue_date = {Number 4 / 2011},
publisher = {Versita},
address = {Warsaw, POL},
volume = {11},
number = {4},
issn = {1732-1360},
url = {https://doi.org/10.2478/v10065-011-0032-5},
doi = {10.2478/v10065-011-0032-5},
abstract = {Software quality can be described by a set of features, such as functionality, reliability, usability, efficiency, maintainability, portability and others. There are various models for software quality prediction developed in the past. Unfortunately, they typically focus on a single quality feature. The main goal of this study is to develop a predictive model that integrates several features of software quality, including relationships between them. This model is an expert-driven Bayesian net, which can be used in diverse analyses and simulations. The paper discusses model structure, behaviour, calibration and enhancement options as well as possible use in fields other than software engineering.},
journal = {Ann. UMCS, Inf.},
month = jan,
pages = {49–60},
numpages = {12}
}

@inproceedings{10.1109/CSMR.2008.4493325,
author = {Khomh, Foutse and Gueheneuce, Yann-Gael},
title = {Do Design Patterns Impact Software Quality Positively?},
year = {2008},
isbn = {9781424421572},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CSMR.2008.4493325},
doi = {10.1109/CSMR.2008.4493325},
abstract = {We study the impact of design patterns on quality attributes in the context of software maintenance and evolution. We show that, contrary to popular beliefs, design patterns in practice impact negatively several quality attributes, thus providing concrete evidence against common lore. We then study design patterns and object-oriented best practices by formulating a second hypothesis on the impact of these principles on quality. We show that results for some design patterns cannot be explained and conclude on the need for further studies. Thus, we bring further evidence that design patterns should be used with caution during development because they may actually impede maintenance and evolution.},
booktitle = {Proceedings of the 2008 12th European Conference on Software Maintenance and Reengineering},
pages = {274–278},
numpages = {5},
series = {CSMR '08}
}

@inproceedings{10.5555/1689250.1689300,
author = {Gao, Kehan and Khoshgoftaar, Taghi M. and Wang, Huanjing},
title = {An empirical investigation of filter attribute selection techniques for software quality classification},
year = {2009},
isbn = {9781424441143},
publisher = {IEEE Press},
abstract = {Attribute selection is an important activity in data preprocessing for software quality modeling and other data mining problems. The software quality models have been used to improve the fault detection process. Finding faulty components in a software system during early stages of software development process can lead to a more reliable final product and can reduce development and maintenance costs. It has been shown in some studies that prediction accuracy of the models improves when irrelevant and redundant features are removed from the original data set. In this study, we investigated four filter attribute selection techniques, Automatic Hybrid Search (AHS), Rough Sets (RS), Kolmogorov-Smirnov (KS) and Probabilistic Search (PS) and conducted the experiments by using them on a very large telecommunications software system. In order to evaluate their classification performance on the smaller subsets of attributes selected using different approaches, we built several classification models using five different classifiers. The empirical results demonstrated that by applying an attribution selection approach we can build classification models with an accuracy comparable to that built with a complete set of attributes. Moreover, the smaller subset of attributes has less than 15 percent of the complete set of attributes. Therefore, the metrics collection, model calibration, model validation, and model evaluation times of future software development efforts of similar systems can be significantly reduced. In addition, we demonstrated that our recently proposed attribute selection technique, KS, outperformed the other three attribute selection techniques.},
booktitle = {Proceedings of the 10th IEEE International Conference on Information Reuse &amp; Integration},
pages = {272–277},
numpages = {6},
location = {Las Vegas, Nevada, USA},
series = {IRI'09}
}

@article{10.1007/s10270-017-0600-2,
author = {Hartmann, Thomas and Moawad, Assaad and Fouquet, Francois and Le Traon, Yves},
title = {The next evolution of MDE: a seamless integration of machine learning into domain modeling},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0600-2},
doi = {10.1007/s10270-017-0600-2},
abstract = {Machine learning algorithms are designed to resolve unknown behaviors by extracting commonalities over massive datasets. Unfortunately, learning such global behaviors can be inaccurate and slow for systems composed of heterogeneous elements, which behave very differently, for instance as it is the case for cyber-physical systems and Internet of Things applications. Instead, to make smart decisions, such systems have to continuously refine the behavior on a per-element basis and compose these small learning units together. However, combining and composing learned behaviors from different elements is challenging and requires domain knowledge. Therefore, there is a need to structure and combine the learned behaviors and domain knowledge together in a flexible way. In this paper we propose to weave machine learning into domain modeling. More specifically, we suggest to decompose machine learning into reusable, chainable, and independently computable small learning units, which we refer to as microlearning units. These microlearning units are modeled together with and at the same level as the domain data. We show, based on a smart grid case study, that our approach can be significantly more accurate than learning a global behavior, while the performance is fast enough to be used for live learning.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1285–1304},
numpages = {20},
keywords = {Smart grids, Model-driven engineering, Metamodeling, Live learning, Domain modeling, Cyber-physical systems}
}

@article{10.1016/j.engappai.2012.09.017,
author = {Galitsky, Boris},
title = {Machine learning of syntactic parse trees for search and classification of text},
year = {2013},
issue_date = {March, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {26},
number = {3},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2012.09.017},
doi = {10.1016/j.engappai.2012.09.017},
abstract = {We build an open-source toolkit which implements deterministic learning to support search and text classification tasks. We extend the mechanism of logical generalization towards syntactic parse trees and attempt to detect weak semantic signals from them. Generalization of syntactic parse tree as a syntactic similarity measure is defined as the set of maximum common sub-trees and performed at a level of paragraphs, sentences, phrases and individual words. We analyze semantic features of such similarity measure and compare it with semantics of traditional anti-unification of terms. Nearest-neighbor machine learning is then applied to relate a sentence to a semantic class. Using syntactic parse tree-based similarity measure instead of bag-of-words and keyword frequency approach, we expect to detect a weak semantic signal otherwise unobservable. The proposed approach is evaluated in a four distinct domains where a lack of semantic information makes classification of sentences rather difficult. We describe a toolkit which is a part of Apache Software Foun-dation project OpenNLP, designed to aid search engineers in tasks requiring text relevance assessment.},
journal = {Eng. Appl. Artif. Intell.},
month = mar,
pages = {1072–1091},
numpages = {20},
keywords = {Text search, Text classification, Parse trees, Machine learning}
}

@article{10.1016/j.jss.2007.05.035,
author = {Gondra, Iker},
title = {Applying machine learning to software fault-proneness prediction},
year = {2008},
issue_date = {February, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {2},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.05.035},
doi = {10.1016/j.jss.2007.05.035},
abstract = {The importance of software testing to quality assurance cannot be overemphasized. The estimation of a module's fault-proneness is important for minimizing cost and improving the effectiveness of the software testing process. Unfortunately, no general technique for estimating software fault-proneness is available. The observed correlation between some software metrics and fault-proneness has resulted in a variety of predictive models based on multiple metrics. Much work has concentrated on how to select the software metrics that are most likely to indicate fault-proneness. In this paper, we propose the use of machine learning for this purpose. Specifically, given historical data on software metric values and number of reported errors, an Artificial Neural Network (ANN) is trained. Then, in order to determine the importance of each software metric in predicting fault-proneness, a sensitivity analysis is performed on the trained ANN. The software metrics that are deemed to be the most critical are then used as the basis of an ANN-based predictive model of a continuous measure of fault-proneness. We also view fault-proneness prediction as a binary classification task (i.e., a module can either contain errors or be error-free) and use Support Vector Machines (SVM) as a state-of-the-art classification method. We perform a comparative experimental study of the effectiveness of ANNs and SVMs on a data set obtained from NASA's Metrics Data Program data repository.},
journal = {J. Syst. Softw.},
month = feb,
pages = {186–195},
numpages = {10},
keywords = {Support vector machine, Software testing, Software metrics, Sensitivity analysis, Neural network, Machine learning, Fault-proneness}
}

@inproceedings{10.1109/ICMV.2009.54,
author = {Kaur, Arashdeep and Sandhu, Parvinder S. and Bra, Amanpreet Singh},
title = {Early Software Fault Prediction Using Real Time Defect Data},
year = {2010},
isbn = {9780769539447},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMV.2009.54},
doi = {10.1109/ICMV.2009.54},
abstract = {Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict faulty modules in software data different techniques have been proposed which includes statistical method, machine learning methods, neural network techniques and clustering techniques. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules by using clustering techniques. This approach has been tested with three real time defect datasets of NASA software projects, JM1, PC1 and CM1. Predicting faults early in the software life cycle can be used to improve software process control and achieve high software reliability. The results show that when all the prediction techniques are evaluated, the best prediction model is found to be the fusion of requirement and code metric model.},
booktitle = {Proceedings of the 2009 Second International Conference on Machine Vision},
pages = {242–245},
numpages = {4},
series = {ICMV '09}
}

@article{10.5555/1239098.1239104,
author = {Khoshgoftaar, Taghi M. and Rebours, Pierre},
title = {Evaluating noise elimination techniques for software quality estimation},
year = {2005},
issue_date = {September 2005},
publisher = {IOS Press},
address = {NLD},
volume = {9},
number = {5},
issn = {1088-467X},
abstract = {The poor quality of a training dataset can have untoward consequences in software quality estimation problems. The presence of noise in software measurement data may hinder the prediction accuracy of a given learner. A filter improves the quality of training datasets by removing data that is likely noise. We evaluate the Ensemble Filter against the Partitioning Filter and the Classification Filter. These filtering techniques combine the predictions of base classifiers in such a way that an instance is identified as noisy if it is misclassified by a given number of these learners. The Partitioning Filter first splits the training dataset into subsets, and different base learners are induced on each subset. Two different implementations of the Partitioning Filter are presented: the Multiple-Partitioning Filter and the Iterative-Partitioning Filter. In contrast, the Ensemble Filter uses base classifiers induced on the entire training dataset. The filtering level and/or the number of iterations modify the filtering conservativeness: a conservative filter is less likely to remove good data at the expense of retaining noisy instances. A unique measure for comparing the relative efficiencies of two filters is also presented. Empirical studies on a high assurance software project evaluate the relative performances of the Ensemble Filter, Multiple-Partitioning Filter, Iterative-Partitioning Filter, and Classification Filter. Our study demonstrates that with a conservative filtering approach, using several different base learners can improve the efficiency of the filtering schemes.},
journal = {Intell. Data Anal.},
month = sep,
pages = {487–508},
numpages = {22}
}

@inproceedings{10.5555/795685.797692,
author = {Khoshgoftaar, Taghi M. and Geleyn, Erik and Nguyen, Laurent and Bullard, Lofton},
title = {Cost-Sensitive Boosting In Software Quality Modeling},
year = {2002},
isbn = {0769517692},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Early prediction of the quality of software modules prior to software testing and operations can yield great benefits to the software development teams, especially those of high-assurance and mission-critical systems. Such an estimation allows effective use of the testing resources to improve the modules of the software system that need it most and achieve high reliability. To achieve high reliability, by the means of predictive methods, several tools are available.Software classification models provide a prediction of the class of a module, i.e., fault-prone or not fault-prone. Recent advances in the data mining field allow to improve individual classifiers (models) by using the combined decision from multiple classifiers.This paper presents a couple of algorithms using the concept of combined classification. The algorithms provided useful models for software quality modeling.A comprehensive comparative evaluation of the Boosting and Cost-Boosting algorithms is presented. We demonstrate how the use of boosting algorithms (original and cost-sensitive) meets many of the specific requir ements for software quality modeling. C4.5 decision trees and Decision Stumps were used to evaluate these algorithms with two large-scale case studies of industrial software systems.},
booktitle = {Proceedings of the 7th IEEE International Symposium on High Assurance Systems Engineering},
pages = {51},
keywords = {software quality modeling, cost-sensitivity, Decision Stumps, Cost-Boosting, C4.5, Boosting},
series = {HASE '02}
}

@article{10.1007/s10664-008-9082-8,
author = {Weyuker, Elaine J. and Ostrand, Thomas J. and Bell, Robert M.},
title = {Do too many cooks spoil the broth? Using the number of developers to enhance defect prediction models},
year = {2008},
issue_date = {October   2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9082-8},
doi = {10.1007/s10664-008-9082-8},
abstract = {Fault prediction by negative binomial regression models is shown to be effective for four large production software systems from industry. A model developed originally with data from systems with regularly scheduled releases was successfully adapted to a system without releases to identify 20% of that system's files that contained 75% of the faults. A model with a pre-specified set of variables derived from earlier research was applied to three additional systems, and proved capable of identifying averages of 81, 94 and 76% of the faults in those systems. A primary focus of this paper is to investigate the impact on predictive accuracy of using data about the number of developers who access individual code units. For each system, including the cumulative number of developers who had previously modified a file yielded no more than a modest improvement in predictive accuracy. We conclude that while many factors can "spoil the broth" (lead to the release of software with too many defects), the number of developers is not a major influence.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {539–559},
numpages = {21},
keywords = {Software faults, Negative binomial model, Empirical study, Developer counts}
}

@inproceedings{10.1145/3416508.3417121,
author = {Villalobos-Arias, Leonardo and Quesada-L\'{o}pez, Christian and Guevara-Coto, Jose and Mart\'{\i}nez, Alexandra and Jenkins, Marcelo},
title = {Evaluating hyper-parameter tuning using random search in support vector machines for software effort estimation},
year = {2020},
isbn = {9781450381277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416508.3417121},
doi = {10.1145/3416508.3417121},
abstract = {Studies in software effort estimation&nbsp;(SEE) have explored the use of hyper-parameter tuning for machine learning algorithms&nbsp;(MLA) to improve the accuracy of effort estimates. In other contexts random search&nbsp;(RS) has shown similar results to grid search, while being less computationally-expensive. In this paper, we investigate to what extent the random search hyper-parameter tuning approach affects the accuracy and stability of support vector regression&nbsp;(SVR) in SEE. Results were compared to those obtained from ridge regression models and grid search-tuned models. A case study with four data sets extracted from the ISBSG 2018 repository shows that random search exhibits similar performance to grid search, rendering it an attractive alternative technique for hyper-parameter tuning. RS-tuned SVR achieved an increase of 0.227 standardized accuracy&nbsp;(SA) with respect to default hyper-parameters. In addition, random search improved prediction stability of SVR models to a minimum ratio of 0.840. The analysis showed that RS-tuned SVR attained performance equivalent to GS-tuned SVR. Future work includes extending this research to cover other hyper-parameter tuning approaches and machine learning algorithms, as well as using additional data sets.},
booktitle = {Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {31–40},
numpages = {10},
keywords = {Software effort estimation, empirical study, grid search, hyper-parameter tuning, random search, support vector machines},
location = {Virtual, USA},
series = {PROMISE 2020}
}

@inproceedings{10.1145/3493244.3493262,
author = {e Silva, Jonathan Messias and de Lima J\'{u}nior, Manoel Limeira},
title = {Prediction of Pull Requests Review Time in Open Source Projects},
year = {2021},
isbn = {9781450395533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493244.3493262},
doi = {10.1145/3493244.3493262},
abstract = {In open-source projects that receive large amounts of pull requests, the tasks of maintaining quality and prioritizing code review have become a complex task. In this sense, several works explored data on pull requests in order to provide useful information. Although, the review time was treated as the interval between the submission and the integration of the pull requests, that is, the lifetime itself. Since December 2016, a feature allows requesting reviews to one or more specific reviewers, which, together with the review status, allowed to establish the period closest to the effective code review time, the interval between the review request and the last review with approval status. In this context, the main objective of this work is to predict the review time of pull requests. Furthermore, the lifetime and acceptance of pull requests with and without review time were compared and the CFS (Correlation-based Feature Selection) attribute selection strategy was used to identify those most relevant to the forecast. The results of the experiments indicate that the SMO (Sequential Minimal Optimization) algorithm had the smallest error, averaging 8,504 minutes (approximately 5,9 days) and that the presence of approvals in the review requests has a positive influence on both the acceptance and in the pull request lifetime.},
booktitle = {Proceedings of the XX Brazilian Symposium on Software Quality},
articleno = {11},
numpages = {10},
keywords = {review time, pull request, lifetime, Distributed software development},
location = {Virtual Event, Brazil},
series = {SBQS '21}
}

@article{10.5555/1239046.1239048,
author = {Khoshgoftaar, Taghi M. and Zhong, Shi and Joshi, Vedang},
title = {Enhancing software quality estimation using ensemble-classifier based noise filtering},
year = {2005},
issue_date = {January 2005},
publisher = {IOS Press},
address = {NLD},
volume = {9},
number = {1},
issn = {1088-467X},
abstract = {This paper presents a technique that improves the accuracy of classification models by enhancing the quality of training data. The idea is to eliminate instances that are likely to be noisy, and train classification models on "clean" data. Our approach uses 25 different classification techniques to create an ensemble classifier to filter noise. Using a relatively large number of base-level classifiers in the ensemble filter helps achieve different levels of desired noise removal conservativeness with several possible levels of filtering. It also provides a high degree of confidence in the noise elimination procedure as the results are less likely to get influenced by (possible) inappropriate learning bias of a few algorithms with 25 base-level classifiers than with a relatively smaller number of base-level classifiers. An empirical case study with software measurement data of a high assurance software project demonstrates the effectiveness of our noise elimination approach in improving classification accuracies. The similarities among predictions from the 25 classifiers are also investigated, and preliminary results suggest that the 25 classifiers may be effectively reduced to 13.},
journal = {Intell. Data Anal.},
month = jan,
pages = {3–27},
numpages = {25}
}

@article{10.1007/s10664-021-09944-w,
author = {Riom, Timoth\'{e} and Sawadogo, Arthur and Allix, Kevin and Bissyand\'{e}, Tegawend\'{e} F. and Moha, Naouel and Klein, Jacques},
title = {Revisiting the VCCFinder approach for the identification of vulnerability-contributing commits},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09944-w},
doi = {10.1007/s10664-021-09944-w},
abstract = {Detecting vulnerabilities in software is a constant race between development teams and potential attackers. While many static and dynamic approaches have focused on regularly analyzing the software in its entirety, a recent research direction has focused on the analysis of changes that are applied to the code. VCCFinder is a seminal approach in the literature that builds on machine learning to automatically detect whether an incoming commit will introduce some vulnerabilities. Given the influence of VCCFinder in the literature, we undertake an investigation into its performance as a state-of-the-art system. To that end, we propose to attempt a replication study on the VCCFinder supervised learning approach. The insights of our failure to replicate the results reported in the original publication informed the design of a new approach to identify vulnerability-contributing commits based on a semi-supervised learning technique with an alternate feature set. We provide all artefacts and a clear description of this approach as a new reproducible baseline for advancing research on machine learning-based identification of vulnerability-introducing commits.},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {30},
keywords = {Software engineering, Replication, Machine learning, Vulnerability detection}
}

@article{10.1007/s10586-018-1923-7,
author = {Viji, C. and Rajkumar, N. and Duraisamy, S.},
title = {Prediction of software fault-prone classes using an unsupervised hybrid SOM algorithm},
year = {2019},
issue_date = {Jan 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-018-1923-7},
doi = {10.1007/s10586-018-1923-7},
abstract = {In software engineering fault proneness prediction is one of the important fields for quality measurement using multiple code metrics. The metrics thresholds are very practical in measuring the code quality for fault proneness prediction. It helps to improvise the software quality in short time with very low cost. Many researchers are in the race to develop a measuring attribute for the software quality using various methodologies. Currently so many fault proneness prediction models are available. Among that most of the methods are used to identify the faults either by data history or by special supervising algorithms. In most of the real time cases the fault data bases may not be available so that the process becomes tedious. This article proposes a hybrid model for identifying the faults in the software models and also we proposed coupling model along with the algorithm so that the metrics are used to identify the faults and the coupling model couples the metrics and the faults for the developed system software.},
journal = {Cluster Computing},
month = jan,
pages = {133–143},
numpages = {11},
keywords = {ANN, Fault proneness, Coupling, Fault prediction, Software metrics}
}

@inproceedings{10.1109/MMIT.2010.11,
author = {Jin, Cong and Dong, En-Mei and Qin, Li-Na},
title = {Software Fault Prediction Model Based on Adaptive Dynamical and Median Particle Swarm Optimization},
year = {2010},
isbn = {9780769540085},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MMIT.2010.11},
doi = {10.1109/MMIT.2010.11},
abstract = {Software quality prediction can play a role of importance in software management, and thus in improve the quality of software systems. By mining software with data mining technique, predictive models can be induced that software managers the insights they need to tackle these quality problems in an efficient way. This paper deals with the adaptive dynamic and median particle swarm optimization (ADMPSO) based on the PSO classification technique. ADMPSO can act as a valid data mining technique to predict erroneous software modules. The predictive model in this paper extracts the relationship rules of software quality and metrics. Information entropy approach is applied to simplify the extraction rule set. The empirical result shows that this method set of rules can be streamlined and the forecast accuracy can be improved.},
booktitle = {Proceedings of the 2010 Second International Conference on MultiMedia and Information Technology - Volume 01},
pages = {44–47},
numpages = {4},
series = {MMIT '10}
}

@inproceedings{10.5555/3507788.3507833,
author = {M\"{u}ller, Hausi A. and Rivera, Luis F. and Jim\'{e}nez, Miguel and Villegas, Norha M. and Tamura, Gabriel and Akkiraju, Rama and Watts, Ian and Erpenbach, Eric},
title = {Proactive AIOps through digital twins},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {The rise of advanced IT environments (IT ·Envs) that meet ever increasing user expectations on software quality necessitates innovative practices in the development and operation of software-intensive systems. DevOps teams find themselves searching for ways to deliver value by attacking operational challenges that tend to overwhelm human capabilities. Most of these challenges relate to the structural and behavioural complexities of modern IT·Envs. While the former concerns the orchestration of multiple technologies, the latter involves the exploitation of the huge data streams produced that are integral to DevOps activities. As automation, autonomy, and artificial intelligence technologies are maturing and permeating various activities in the software development lifecycle, opportunities arise from their integration with DevOps practices to improve risk mitigation, root cause analysis, problem resolution, and operational optimization in IT·Envs. This CASCON x EVOKE 2021 workshop discussed challenges and opportunities in developing proactive AIOps through digital twin technologies.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {275–276},
numpages = {2},
keywords = {machine learning, fault prediction, digital twins, cloud, IT operations, DevOps, AIOps, AI},
location = {Toronto, Canada},
series = {CASCON '21}
}

@article{10.1016/j.jss.2009.06.036,
author = {Binkley, David and Feild, Henry and Lawrie, Dawn and Pighin, Maurizio},
title = {Increasing diversity: Natural language measures for software fault prediction},
year = {2009},
issue_date = {November, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {11},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.036},
doi = {10.1016/j.jss.2009.06.036},
abstract = {While challenging, the ability to predict faulty modules of a program is valuable to a software project because it can reduce the cost of software development, as well as software maintenance and evolution. Three language-processing based measures are introduced and applied to the problem of fault prediction. The first measure is based on the usage of natural language in a program's identifiers. The second measure concerns the conciseness and consistency of identifiers. The third measure, referred to as the QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgments of software quality. Two case studies consider the language processing measures applicability to fault prediction using two programs (one open source, one proprietary). Linear mixed-effects regression models are used to identify relationships between defects and the measures. Results, while complex, show that language processing measures improve fault prediction, especially when used in combination. Overall, the models explain one-third and two-thirds of the faults in the two case studies. Consistent with other uses of language processing, the value of the three measures increases with the size of the program module considered.},
journal = {J. Syst. Softw.},
month = nov,
pages = {1793–1803},
numpages = {11},
keywords = {Linear regression models, Information retrieval, Fault prediction, Empirical software engineering, Code comprehension}
}

@inproceedings{10.1109/CSSS.2012.121,
author = {Miao, Xudong and Lu, Yansheng and Dai, Yao},
title = {Method of Military Software Quality Evaluation Based on Multi-agent Fuzzy Neural Network},
year = {2012},
isbn = {9780769547190},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CSSS.2012.121},
doi = {10.1109/CSSS.2012.121},
abstract = {In allusion to the characteristic of military software, constructs multi-Agent fuzzy neural network military software evaluation system which based on Web environment. Discusses multi-Agent communication and message mechanism, designs military software evaluation metrics system and studies self-adjustability fuzzy neural network quality evaluation model. This method can obviously improve the synchronism, efficiency, scientific and objective.},
booktitle = {Proceedings of the 2012  International Conference on Computer Science and Service System},
pages = {459–462},
numpages = {4},
keywords = {quality evaluation, multi-Agent, military software, fuzzy neural network, Metrics system},
series = {CSSS '12}
}

@article{10.1007/s11219-006-7598-y,
author = {Liu, Frank and Noguchi, Kunio and Dhungana, Anuj and A., V. V. and Inuganti, Praveen},
title = {A quantitative approach for setting technical targets based on impact analysis in software quality function deployment (SQFD)},
year = {2006},
issue_date = {June      2006},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-006-7598-y},
doi = {10.1007/s11219-006-7598-y},
abstract = {Target setting in software quality function deployment (SQFD) is very important since it is directly related to development of high quality products with high customer satisfaction. However target setting is usually done subjectively in practice, which is not scientific. Two quantitative approaches for setting target values: benchmarking and primitive linear regression have been developed and applied in the past to overcome this problem (Akao and Yoji, 1990). But these approaches cannot be used to assess the impact of unachieved targets on satisfaction of customers for customer requirements. In addition, both of them are based on linear regression and not very practical in many applications.In this paper, we present an innovative quantitative method of setting technical targets in SQFD to enable analysis of impact of unachieved target values on customer satisfaction. It is based on assessment of impact of technical attributes on satisfaction of customer requirements. In addition both linear and non linear regression techniques are utilized in our method, which certainly improves the existing quantitative methods which are based on only linear regression.},
journal = {Software Quality Journal},
month = jun,
pages = {113–134},
numpages = {22},
keywords = {Software quality function deployment (SQFD), Quality planning, Linear regression, Impact analysis, Benchmarking}
}

@article{10.1016/j.infsof.2017.03.007,
author = {Yang, Xinli and Lo, David and Xia, Xin and Sun, Jianling},
title = {TLEL},
year = {2017},
issue_date = {July 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {87},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.03.007},
doi = {10.1016/j.infsof.2017.03.007},
abstract = {We propose a novel approach TLEL, which can be seen as a two-layer ensemble learning technique, to achieve a better performance for just-in-time defect prediction problem.We compare TLEL with three baselines, i.e., Deeper, DNC and MKEL, on six large software projects.The experiment results show that our approach can achieve a substantial improvement over all of them. Moreover, TLEL could discover over 70% reviewing only 20% of the lines of code. Display Omitted ContextDefect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time [1]. ObjectiveEnsemble learning becomes a hot topic in recent years. There have been several studies about applying ensemble learning to defect prediction [25]. Traditional ensemble learning approaches only have one layer, i.e., they use ensemble learning once. There are few studies that leverages ensemble learning twice or more. To bridge this research gap, we try to hybridize various ensemble learning methods to see if it will improve the performance of just-in-time defect prediction. In particular, we focus on one way to do this by hybridizing bagging and stacking together and leave other possibly hybridization strategies for future work. MethodIn this paper, we propose a two-layer ensemble learning approach TLEL which leverages decision tree and ensemble learning to improve the performance of just-in-time defect prediction. In the inner layer, we combine decision tree and bagging to build a Random Forest model. In the outer layer, we use random under-sampling to train many different Random Forest models and use stacking to ensemble them once more. ResultsTo evaluate the performance of TLEL, we use two metrics, i.e., cost effectiveness and F1-score. We perform experiments on the datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. Also, we compare our approach with three baselines, i.e., Deeper, the approach proposed by us [6], DNC, the approach proposed by Wang etal. [2], and MKEL, the approach proposed by Wang etal. [3]. The experimental results show that on average across the six datasets, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code, as compared with about 50% for the baselines. In addition, the F1-scores TLEL can achieve are substantially and statistically significantly higher than those of three baselines across the six datasets. ConclusionTLEL can achieve a substantial and statistically significant improvement over the state-of-the-art methods, i.e., Deeper, DNC and MKEL. Moreover, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {206–220},
numpages = {15},
keywords = {Just-in-time defect prediction, Ensemble learning, Cost effectiveness}
}

@article{10.1016/j.jss.2021.111060,
author = {Xu, Liming and Towey, Dave and French, Andrew P. and Benford, Steve and Zhou, Zhi Quan and Chen, Tsong Yueh},
title = {Using metamorphic relations to verify and enhance Artcode classification},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111060},
doi = {10.1016/j.jss.2021.111060},
journal = {J. Syst. Softw.},
month = dec,
numpages = {14},
keywords = {Artcode, Machine learning, Software verification, Classification, Metamorphic relation, Metamorphic testing}
}

@inproceedings{10.1145/3143434.3143451,
author = {Kupi\'{n}ski, Szymon and Walter, Bartosz and Wolski, Marcin and Chojnacki, Jakub},
title = {Filling the gaps: imputation of missing metrics' values in a software quality model},
year = {2017},
isbn = {9781450348539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143434.3143451},
doi = {10.1145/3143434.3143451},
abstract = {Hierarchical software quality models usually rely on a number of metrics, which, once aggregated, provide an overview of selected perspectives of a system's quality. Missing values of some metrics, that usually result from data unavailability, can seriously affect the final score. In the paper we empirically validate a few imputation methods in context of a custom G\'{e}ant-QM framework, used for evaluation of several open source systems. Early results indicate imputing a missing value based on its close neighbors as data donors introduces less noise that using a wider set of donors.},
booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
pages = {82–87},
numpages = {6},
keywords = {software quality model, predictive mean matching, multiple imputation, missing data, distance based donor selection},
location = {Gothenburg, Sweden},
series = {IWSM Mensura '17}
}

@inproceedings{10.1145/1370788.1370794,
author = {Watanabe, Shinya and Kaiya, Haruhiko and Kaijiri, Kenji},
title = {Adapting a fault prediction model to allow inter languagereuse},
year = {2008},
isbn = {9781605580364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370788.1370794},
doi = {10.1145/1370788.1370794},
abstract = {An important step in predicting error prone modules in a project is to construct the prediction model by using training data of that project, but the resulting prediction model depends on the training data. Therefore it is difficult to apply the model to other projects. The training data consists of metrics data and bug data, and these data should be prepared for each project. Metrics data can be computed by using metric tools, but it is not so easy to collect bug data. In this paper, we try to reuse the generated prediction model. By using the metrics and bug data which are computed from C++ and Java projects, we have evaluated the possibility of applying the prediction model, which is generated based on one project, to other projects, and have proposed compensation techniques for applying to other projects. We showed the evaluation result based on open source projects.},
booktitle = {Proceedings of the 4th International Workshop on Predictor Models in Software Engineering},
pages = {19–24},
numpages = {6},
keywords = {error prone, inter language prediction, metrics, open source},
location = {Leipzig, Germany},
series = {PROMISE '08}
}

@inproceedings{10.1145/3472674.3473980,
author = {Fortz, Sophie and Temple, Paul and Devroey, Xavier and Heymans, Patrick and Perrouin, Gilles},
title = {VaryMinions: leveraging RNNs to identify variants in event logs},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473980},
doi = {10.1145/3472674.3473980},
abstract = {Business processes have to manage variability in their execution, e.g., to deliver the correct building permit in different municipalities. This variability is visible in event logs, where sequences of events are shared by the core process (building permit authorisation) but may also be specific to each municipality. To rationalise resources (e.g., derive a configurable business process capturing all municipalities’ permit variants) or to debug anomalous behaviour, it is mandatory to identify to which variant a given trace belongs. This paper supports this task by training Long Short Term Memory (LSTMs) and Gated Recurrent Units (GRUs) algorithms on two datasets: a configurable municipality and a travel expenses workflow. We demonstrate that variability can be identified accurately (&gt;87%) and discuss the challenges of learning highly entangled variants.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {13–18},
numpages = {6},
keywords = {Variability Mining, Recurrent Neural Networks, Configurable processes},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@article{10.1016/j.neucom.2015.10.137,
author = {Shafiabady, Niusha and Lee, L.H. and Rajkumar, R. and Kallimani, V.P. and Akram, Nik Ahmad and Isa, Dino},
title = {Using unsupervised clustering approach to train the Support Vector Machine for text classification},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {211},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.10.137},
doi = {10.1016/j.neucom.2015.10.137},
abstract = {The use of learning algorithms for text classification assumes the availability of a large amount of documents which have been organized and labeled correctly by human experts for use in the training phase. Unless the text documents in question have been in existence for some time, using an expert system is inevitable because manual organizing and labeling of thousands of groups of text documents can be a very labor intensive and intellectually challenging activity. Also, in some new domains, the knowledge to organize and label different classes might not be unavailable. Therefore unsupervised learning schemes for automatically clustering data in the training phase are needed. Furthermore, even when knowledge exists, variation is high when the subject under classification depends on personal opinions and is open to different interpretations. This paper describes a methodology which uses Self Organizing Maps (SOM) and alternatively does the automatic clustering by using the Correlation Coefficient (CorrCoef). Consequently the clusters are used as the labels to train the Support Vector Machine (SVM). Experiments and results are presented based on applying the methodology to some standard text datasets in order to verify the accuracy of the proposed scheme. We will also present results which are used to evaluate the effect that dimensionality reduction and changes in the clustering schemes have on the accuracy of the SVM. Results show that the proposed combination has better accuracy compared to training the learning machine using the expert knowledge.},
journal = {Neurocomput.},
month = oct,
pages = {4–10},
numpages = {7},
keywords = {Unsupervised learning, Support Vector Machines, Classification}
}

@inproceedings{10.1007/11751632_89,
author = {Choi, Wan-Kyoo and Lee, Sung-Joo and Chung, Il-Yong and Bae, Yong-Geun},
title = {The classification of the software quality by employing the tolerence class},
year = {2006},
isbn = {3540340777},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11751632_89},
doi = {10.1007/11751632_89},
abstract = {When we decide the software quality on the basis of the software measurement, the transitive property which is a requirement for an equivalence relation is not always satisfied. Therefore, we propose a scheme for classifing the software quality that employs a tolerance relation instead of an equivalence relation. Given the experimental data set, the proposed scheme generates the tolerant classes for elements in the experiment data set, and generates the tolerant ranges for classfing the software quality by clustering the means of the tolerance classes. Through the experiment, we showed that the proposed scheme could product very useful and valid results. That is, it has no problems that we use as the criteria for classifing the software quality the tolerant ranges generated by the proposed scheme.},
booktitle = {Proceedings of the 2006 International Conference on Computational Science and Its Applications - Volume Part IV},
pages = {828–834},
numpages = {7},
location = {Glasgow, UK},
series = {ICCSA'06}
}

@article{10.1109/TSMCA.2009.2020804,
author = {Drown, Dennis J. and Khoshgoftaar, Taghi M. and Seliya, Naeem},
title = {Evolutionary sampling and software quality modeling of high-assurance systems},
year = {2009},
issue_date = {September 2009},
publisher = {IEEE Press},
volume = {39},
number = {5},
issn = {1083-4427},
url = {https://doi.org/10.1109/TSMCA.2009.2020804},
doi = {10.1109/TSMCA.2009.2020804},
abstract = {Software quality modeling for high-assurance systems, such as safety-critical systems, is adversely affected by the skewed distribution of fault-prone program modules. This sparsity of defect occurrence within the software system impedes training and performance of software quality estimation models. Data sampling approaches presented in data mining and machine learning literature can be used to address the imbalance problem. We present a novel genetic algorithm-based data sampling method, named Evolutionary Sampling, as a solution to improving software quality modeling for high-assurance systems. The proposed solution is compared with multiple existing data sampling techniques, including random undersampling, one-sided selection, Wilson's editing, random oversampling, cluster-based oversampling, Synthetic Minority Oversampling Technique (SMOTE), and Borderline-SMOTE. This paper involves case studies of two real-world software systems and builds C4.5- and RIPPER-based software quality models both before and after applying a given data sampling technique. It is empirically shown that Evolutionary Sampling improves performance of software quality models for high-assurance systems and is significantly better than most existing data sampling techniques.},
journal = {Trans. Sys. Man Cyber. Part A},
month = sep,
pages = {1097–1107},
numpages = {11},
keywords = {software metrics, imbalanced data, high-assurance system, evolutionary computing, data sampling, Data sampling}
}

@article{10.1007/s11219-014-9230-x,
author = {Caglayan, Bora and Tosun Misirli, Ayse and Bener, Ayse Basar and Miranskyy, Andriy},
title = {Predicting defective modules in different test phases},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-014-9230-x},
doi = {10.1007/s11219-014-9230-x},
abstract = {Defect prediction is a well-established research area in software engineering . Prediction models in the literature do not predict defect-prone modules in different test phases. We investigate the relationships between defects and test phases in order to build defect prediction models for different test phases. We mined the version history of a large-scale enterprise software product to extract churn and static code metrics. We used three testing phases that have been employed by our industry partner, namely function, system and field, to build a learning-based model for each testing phase. We examined the relation of different defect symptoms with the testing phases. We compared the performance of our proposed model with a benchmark model that has been constructed for the entire test phase (benchmark model). Our results show that building a model to predict defect-prone modules for each test phase significantly improves defect prediction performance and shortens defect detection time. The benefit analysis shows that using the proposed model, the defects are detected on the average 7 months earlier than the actual. The outcome of prediction models should lead to an action in a software development organization. Our proposed model gives a more granular outcome in terms of predicting defect-prone modules in each testing phase so that managers may better organize the testing teams and effort.},
journal = {Software Quality Journal},
month = jun,
pages = {205–227},
numpages = {23},
keywords = {Testing phase, Software testing, Defect prediction}
}

@inproceedings{10.1145/3472674.3473982,
author = {Norambuena, Ignacio Nu\~{n}ez and Bergel, Alexandre},
title = {Building a bot for automatic expert retrieval on discord},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473982},
doi = {10.1145/3472674.3473982},
abstract = {It is common for software practitioners to look for experts on online chat platforms, such as Discord. However, finding them is a complex activity that requires a deep knowledge of the open source community. As a consequence, newcomers and casual participants may not be able to adequately find experts willing to discuss a particular topic.  Our paper describes a bot that provides a ranked list of Discord users that are experts in a particular set of topics. Our bot uses simple heuristics to model expertise, such as a word occurrence table and word embeddings. Our bot shows that at least half of the retrieved users are indeed experts.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {25–30},
numpages = {6},
keywords = {Word Embeddings, Software, Expert Retrieval Systems, Discord, Bot},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@article{10.1007/s11390-007-9054-2,
author = {Khoshgoftaar, Taghi M. and Rebours, Pierre},
title = {Improving software quality prediction by noise filtering techniques},
year = {2007},
issue_date = {May 2007},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {3},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-007-9054-2},
doi = {10.1007/s11390-007-9054-2},
abstract = {Accuracy of machine learners is affected by quality of the data the learners are induced on. In this paper, quality of the training dataset is improved by removing instances detected as noisy by the Partitioning Filter. The fit dataset is first split into subsets, and different base learners are induced on each of these splits. The predictions are combined in such a way that an instance is identified as noisy if it is misclassified by a certain number of base learners. Two versions of the Partitioning Filter are used: Multiple-Partitioning Filter and Iterative-Partitioning Filter. The number of instances removed by the filters is tuned by the voting scheme of the filter and the number of iterations. The primary aim of this study is to compare the predictive performances of the final models built on the filtered and the un-filtered training datasets. A case study of software measurement data of a high assurance software project is performed. It is shown that predictive performances of models built on the filtered fit datasets and evaluated on a noisy test dataset are generally better than those built on the noisy (un-filtered) fit dataset. However, predictive performance based on certain aggressive filters is affected by presence of noise in the evaluation dataset.},
journal = {J. Comput. Sci. Technol.},
month = may,
pages = {387–396},
numpages = {10},
keywords = {voting expert, software quality classification, noise filtering, expected cost of misclassification, data quality}
}

@article{10.1007/s10664-007-9058-0,
author = {Moses, John and Farrow, Malcolm},
title = {Tests for consistent measurement of external subjective software quality attributes},
year = {2008},
issue_date = {June      2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-007-9058-0},
doi = {10.1007/s10664-007-9058-0},
abstract = {One reason that researchers may wish to demonstrate that an external software quality attribute can be measured consistently is so that they can validate a prediction system for the attribute. However, attempts at validating prediction systems for external subjective quality attributes have tended to rely on experts indicating that the values provided by the prediction systems informally agree with the experts' intuition about the attribute. These attempts are undertaken without a pre-defined scale on which it is known that the attribute can be measured consistently. Consequently, a valid unbiased estimate of the predictive capability of the prediction system cannot be given because the experts'  measurement process  is not independent of the prediction system's values. Usually, no justification is given for not checking to see if the experts can measure the attribute consistently. It seems to be assumed that:  subjective measurement isn't proper measurement or subjective measurement cannot be quantified or no one knows the true values of the attributes anyway and they cannot be estimated . However, even though the classification of software systems' or software artefacts' quality attributes is subjective, it is possible to quantify experts' measurements in terms of conditional probabilities. It is then possible, using a statistical approach, to assess formally whether the experts' measurements can be considered consistent. If the measurements are consistent, it is also possible to identify estimates of the true values, which are independent of the prediction system. These values can then be used to assess the predictive capability of the prediction system. In this paper we use Bayesian inference, Markov chain Monte Carlo simulation and missing data imputation to develop statistical tests for consistent measurement of subjective ordinal scale attributes.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {261–287},
numpages = {27},
keywords = {Subjective software quality attributes, Multinomial distribution, Minimum rejection principle, Error rates, Distribution principle, Data imputation, Consistent measurement, Conditional probability, Bayesian inference}
}

@article{10.4018/jossp.2012040103,
author = {Chaturvedi, K. K. and Singh, V.B.},
title = {An Empirical Comparison of Machine Learning Techniques in Predicting the Bug Severity of Open and Closed Source Projects},
year = {2012},
issue_date = {April 2012},
publisher = {IGI Global},
address = {USA},
volume = {4},
number = {2},
issn = {1942-3926},
url = {https://doi.org/10.4018/jossp.2012040103},
doi = {10.4018/jossp.2012040103},
abstract = {Bug severity is the degree of impact that a defect has on the development or operation of a component or system, and can be classified into different levels based on their impact on the system. Identification of severity level can be useful for bug triager in allocating the bug to the concerned bug fixer. Various researchers have attempted text mining techniques in predicting the severity of bugs, detection of duplicate bug reports and assignment of bugs to suitable fixer for its fix. In this paper, an attempt has been made to compare the performance of different machine learning techniques namely Support vector machine SVM, probability based Na\"{\i}ve Bayes NB, Decision Tree based J48 A Java implementation of C4.5, rule based Repeated Incremental Pruning to Produce Error Reduction RIPPER and Random Forests RF learners in predicting the severity level 1 to 5 of a reported bug by analyzing the summary or short description of the bug reports. The bug report data has been taken from NASA's PITS Projects and Issue Tracking System datasets as closed source and components of Eclipse, Mozilla &amp; GNOME datasets as open source projects. The analysis has been carried out in RapidMiner and STATISTICA data mining tools. The authors measured the performance of different machine learning techniques by considering i the value of accuracy and F-Measure for all severity level and ii number of best cases at different threshold level of accuracy and F-Measure.},
journal = {Int. J. Open Source Softw. Process.},
month = apr,
pages = {32–59},
numpages = {28},
keywords = {Text Mining, Supervised Classification, Multiclass Classification, Bug Severity, Bug Repositories, 10-fold Cross Validation}
}

@article{10.1145/1022494.1022515,
author = {Kanmani, S. and Uthariaraj, V. Rhymend and Sankaranarayanan, V. and Thambidurai, P.},
title = {Object oriented software quality prediction using general regression neural networks},
year = {2004},
issue_date = {September 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1022494.1022515},
doi = {10.1145/1022494.1022515},
abstract = {This paper discusses the application of General Regression Neural Network (GRNN) for predicting the software quality attribute -- fault ratio. This study is carried out using static Object-Oriented (OO) measures (64 in total) as the independent variables and fault ratio as the dependent variable. Software metrics used include those concerning inheritance, size, cohesion and coupling. Prediction models are designed using 15 possible combinations of the four categories of the measures. We also tested the goodness of fit of the neural network model with the standard parameters. Our study is conducted in an academic institution with the software developed by students of Undergraduate/Graduate courses.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–6},
numpages = {6},
keywords = {software quality, object oriented, neural network, metrics}
}

@article{10.1007/s10664-004-6191-x,
author = {Khoshgoftaar, Taghi M. and Seliya, Naeem and Gao, Kehan},
title = {Assessment of a New Three-Group Software Quality Classification Technique: An Empirical Case Study},
year = {2005},
issue_date = {April     2005},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {10},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-004-6191-x},
doi = {10.1007/s10664-004-6191-x},
abstract = {The primary aim of risk-based software quality classification models is to detect, prior to testing or operations, components that are most-likely to be of high-risk. Their practical usage as quality assurance tools is gauged by the prediction-accuracy and cost-effective aspects of the models. Classifying modules into two risk groups is the more commonly practiced trend. Such models assume that all modules predicted as high-risk will be subjected to quality improvements. Due to the always-limited reliability improvement resources and the variability of the quality risk-factor, a more focused classification model may be desired to achieve cost-effective software quality assurance goals. In such cases, calibrating a three-group (high-risk, medium-risk, and low-risk) classification model is more rewarding. We present an innovative method that circumvents the complexities, computational overhead, and difficulties involved in calibrating pure or direct three-group classification models. With the application of the proposed method, practitioners can utilize an existing two-group classification algorithm thrice in order to yield the three risk-based classes. An empirical approach is taken to investigate the effectiveness and validity of the proposed technique. Some commonly used classification techniques are studied to demonstrate the proposed methodology. They include, the C4.5 decision tree algorithm, discriminant analysis, and case-based reasoning. For the first two, we compare the three-group model calibrated using the respective techniques with the one built by applying the proposed method. Any two-group classification technique can be employed by the proposed method, including those that do not provide a direct three-group classification model, e.x., logistic regression and certain binary classification trees, such as  CART . Based on a case study of a large-scale industrial software system, it is observed that the proposed method yielded promising results. For a given classification technique, the expected cost of misclassification of the proposed three-group models were significantly better (generally) when compared to the technique s direct three-group model. In addition, the proposed method is also evaluated against an alternate indirect three-group classification method.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {183–218},
numpages = {36},
keywords = {three-group classification, expected cost of misclassification, discriminant analysis, decision trees, case-based reasoning, Software quality prediction}
}

@inproceedings{10.5555/951951.952241,
author = {Khoshgoftaar, Taghi M. and Liu, Yi and Seliya, Naeem},
title = {Genetic Programming-Based Decision Trees for Software Quality Classification},
year = {2003},
isbn = {0769520383},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The knowledge of the likely problematic areas of a software system is very useful for improving its overall quality. Based on such information, a more focussed software testing and inspection plan can be devised. Decision trees are attractive for a software quality classification problem which predicts the quality of program modules in terms of risk-based classes.They provide a comprehensible classification model which can be directly interpreted by observing thetree-structure. A simultaneous optimization of the classification accuracy and the size of the decision tree is a difficult problem, and very few studies have addressed the issue. This paper presents an automated and simplified genetic programming (gp) based decision tree modeling technique for the software quality classification problem. Genetic programming is ideally suited for problems that require optimization of multiple criteria. The proposed technique is based on multi-objective optimization using strongly typed GP.In the context of an industrial high-assurance software system, two fitness functions are used for the optimization problem: one for minimizing the average weighted cost of misclassification, and one for controlling the size of the decision tree. The classification performances of the GP-based decision trees are compared with those based on standard GP, i.e., S-expression tree. It is shown that the GP-based decision tree technique yielded better classification models. As compared to other decision tree-based methods, such as C4.5, GP-based decision trees are more flexible and can allow optimization of performance objectives other than accuracy. Moreover, it provides a practical solution for building models in the presence of conflicting objectives, which is commonly observed in software development practice},
booktitle = {Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence},
pages = {374},
keywords = {software quality classification, software metrics, multi-objective optimization, genetic programming, decision tree, cost of misclassification},
series = {ICTAI '03}
}

@inproceedings{10.1007/978-3-030-91265-9_11,
author = {Wei, Shaozhi and Mo, Ran and Xiong, Pu and Zhang, Siyuan and Zhao, Yang and Li, Zengyang},
title = {Predicting and Monitoring Bug-Proneness at the Feature Level},
year = {2021},
isbn = {978-3-030-91264-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91265-9_11},
doi = {10.1007/978-3-030-91265-9_11},
abstract = {Enabling quick feature modification and delivery is important for a project’s success. Obtaining early estimates of software features’ bug-proneness is helpful for effectively allocating resources to the bug-prone features requiring further fixes. Researchers have proposed various studies on bug prediction at different granularity levels, such as class level, package level, method level, etc. However, there exists little work building predictive models at the feature level. In this paper, we investigated how to predict bug-prone features and monitor their evolution. More specifically, we first identified a project’s features and their involved files. Next, we collected a suite of code metrics and selected a relevant set of metrics as attributes to be used for six machine learning algorithms to predict bug-prone features. Through our evaluation, we have presented that using the machine learning algorithms with an appropriate set of code metrics, we can build effective models of bug prediction at the feature level. Furthermore, we build regression models to monitor growth trends of bug-prone features, which shows how these features accumulate bug-proneness over time.},
booktitle = {Dependable Software Engineering. Theories, Tools, and Applications: 7th International Symposium, SETTA 2021, Beijing, China, November 25–27, 2021, Proceedings},
pages = {201–218},
numpages = {18},
keywords = {Feature bug prediction, Machine learning, Code metrics},
location = {Beijing, China}
}

@article{10.1016/j.infsof.2009.06.006,
author = {Briand, Lionel C. and Labiche, Yvan and Bawar, Zaheer and Spido, Nadia Traldi},
title = {Using machine learning to refine Category-Partition test specifications and test suites},
year = {2009},
issue_date = {November, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.06.006},
doi = {10.1016/j.infsof.2009.06.006},
abstract = {In the context of open source development or software evolution, developers often face test suites which have been developed with no apparent rationale and which may need to be augmented or refined to ensure sufficient dependability, or even reduced to meet tight deadlines. We refer to this process as the re-engineering of test suites. It is important to provide both methodological and tool support to help people understand the limitations of test suites and their possible redundancies, so as to be able to refine them in a cost effective manner. To address this problem in the case of black-box, Category-Partition testing, we propose a methodology and a tool based on machine learning that has shown promising results on a case study involving students as testers.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1551–1564},
numpages = {14},
keywords = {Test improvement, Machine learning, Category-Partition, Black box testing}
}

@article{10.5555/3455716.3455827,
author = {Alcoba\c{c}a, Edesio and Siqueira, Felipe and Rivolli, Adriano and Garcia, Lu\'{\i}s P. F. and Oliva, Jefferson T. and De Carvalho, Andr\'{e} C. P. L. F.},
title = {MFE: towards reproducible meta-feature extraction},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Automated recommendation of machine learning algorithms is receiving a large deal of attention, not only because they can recommend the most suitable algorithms for a new task, but also because they can support efficient hyper-parameter tuning, leading to better machine learning solutions. The automated recommendation can be implemented using meta-learning, learning from previous learning experiences, to create a meta-model able to associate a data set to the predictive performance of machine learning algorithms. Although a large number of publications report the use of meta-learning, reproduction and comparison of meta-learning experiments is a difficult task. The literature lacks extensive and comprehensive public tools that enable the reproducible investigation of the different meta-learning approaches. An alternative to deal with this difficulty is to develop a meta-feature extractor package with the main characterization measures, following uniform guidelines that facilitate the use and inclusion of new meta-features. In this paper, we propose two Meta-Feature Extractor (MFE) packages, written in both Python and R, to fill this lack. The packages follow recent frameworks for meta-feature extraction, aiming to facilitate the reproducibility of meta-learning experiments.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {111},
numpages = {5},
keywords = {meta-features, meta-learning, AutoML, machine learning}
}

@article{10.1007/s11219-019-09483-0,
author = {Nguyen, Phuong T. and Di Rocco, Juri and Rubei, Riccardo and Di Ruscio, Davide},
title = {An automated approach to assess the similarity of GitHub repositories},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09483-0},
doi = {10.1007/s11219-019-09483-0},
abstract = {Open source software (OSS) allows developers to study, change, and improve the code free of charge. There are several high-quality software projects which deliver stable and well-documented products. Most OSS forges typically sustain active user and expert communities which in turn provide decent levels of support both with respect to answering user questions as well as to repairing reported software bugs. Code reuse is an intrinsic feature of OSS, and developing a new system by leveraging existing open source components can reduce development effort, and thus it can be beneficial to at least two phases of the software life cycle, i.e., implementation and maintenance. However, to improve software quality, it is essential to develop a system by learning from well-defined, mature projects. In this sense, the ability to find similar projects that facilitate the undergoing development activities is of high importance. In this paper, we address the issue of mining open source software repositories to detect similar projects, which can be eventually reused by developers. We propose CrossSim as a novel approach to model the OSS ecosystem and to compute similarities among software projects. An evaluation on a dataset collected from GitHub shows that our proposed approach outperforms three well-established baselines.},
journal = {Software Quality Journal},
month = jun,
pages = {595–631},
numpages = {37},
keywords = {SimRank, Software quality, Software similarity, Mining software repositories}
}

@inproceedings{10.1145/2600428.2609601,
author = {Cormack, Gordon V. and Grossman, Maura R.},
title = {Evaluation of machine-learning protocols for technology-assisted review in electronic discovery},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609601},
doi = {10.1145/2600428.2609601},
abstract = {Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P&lt;0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P&lt;0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of "stabilization" -- determining when training is adequate, and therefore may stop.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {153–162},
numpages = {10},
keywords = {technology-assisted review, predictive coding, electronic discovery, e-discovery},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@article{10.1016/j.jss.2019.110493,
author = {Pascarella, Luca and Palomba, Fabio and Bacchelli, Alberto},
title = {On the performance of method-level bug prediction: A negative result},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {161},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110493},
doi = {10.1016/j.jss.2019.110493},
journal = {J. Syst. Softw.},
month = mar,
numpages = {15},
keywords = {Mining software repositories, Empirical software engineering, Defect prediction}
}

@inproceedings{10.1145/3422392.3422420,
author = {Lima, Rodrigo and Souza, Jairo and Fonseca, Baldoino and Teixeira, Leopoldo and Gheyi, Rohit and Ribeiro, M\'{a}rcio and Garcia, Alessandro and de Mello, Rafael},
title = {Understanding and Detecting Harmful Code},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422420},
doi = {10.1145/3422392.3422420},
abstract = {Code smells typically indicate poor design implementation and choices that may degrade software quality. Hence, they need to be carefully detected to avoid such poor design. In this context, some studies try to understand the impact of code smells on the software quality, while others propose rules or machine learning-based techniques to detect code smells. However, none of those studies or techniques focus on analyzing code snippets that are really harmful to software quality. This paper presents a study to understand and classify code harmfulness. We analyze harmfulness in terms of CLEAN, SMELLY, BUGGY, and HARMFUL code. By HARMFUL CODE, we define a SMELLY code element having one or more bugs reported. These bugs may have been fixed or not. Thus, the incidence of HARMFUL CODE may represent a increased risk of introducing new defects and/or design problems during its fixing. We perform our study with 22 smell types, 803 versions of 13 open-source projects, 40,340 bugs and 132,219 code smells. The results show that even though we have a high number of code smells, only 0.07% of those smells are harmful. The Abstract Function Call From Constructor is the smell type more related to HARMFUL CODE. To cross-validate our results, we also perform a survey with 60 developers. Most of them (98%) consider code smells harmful to the software, and 85% of those developers believe that code smells detection tools are important. But, those developers are not concerned about selecting tools that are able to detect HARMFUL CODE. We also evaluate machine learning techniques to classify code harmfulness: they reach the effectiveness of at least 97% to classify HARMFUL CODE. While the Random Forest is effective in classifying both SMELLY and HARMFUL CODE, the Gaussian Naive Bayes is the less effective technique. Our results also suggest that both software and developers' metrics are important to classify HARMFUL CODE.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {223–232},
numpages = {10},
keywords = {Code Smells, Machine Learning, Software Quality},
location = {Natal, Brazil},
series = {SBES '20}
}

@article{10.1023/A:1025316301168,
author = {Khoshgoftaar, Taghi M. and Seliya, Naeem},
title = {Analogy-Based Practical Classification Rules for Software Quality Estimation},
year = {2003},
issue_date = {December 2003},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1023/A:1025316301168},
doi = {10.1023/A:1025316301168},
abstract = {Software metrics-based quality estimation models can be effective tools for identifying which modules are likely to be fault-prone or not fault-prone. The use of such models prior to system deployment can considerably reduce the likelihood of faults discovered during operations, hence improving system reliability. A software quality classification model is calibrated using metrics from a past release or similar project, and is then applied to modules currently under development. Subsequently, a timely prediction of which modules are likely to have faults can be obtained. However, software quality classification models used in practice may not provide a useful balance between the two misclassification rates, especially when there are very few faulty modules in the system being modeled.This paper presents, in the context of case-based reasoning, two practical classification rules that allow appropriate emphasis on each type of misclassification as per the project requirements. The suggested techniques are especially useful for high-assurance systems where faulty modules are rare. The proposed generalized classification methods emphasize on the costs of misclassifications, and the unbalanced distribution of the faulty program modules. We illustrate the proposed techniques with a case study that consists of software measurements and fault data collected over multiple releases of a large-scale legacy telecommunication system. In addition to investigating the two classification methods, a brief relative comparison of the techniques is also presented. It is indicated that the level of classification accuracy and model-robustness observed for the case study would be beneficial in achieving high software reliability of its subsequent system releases. Similar observations are made from our empirical studies with other case studies.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {325–350},
numpages = {26},
keywords = {software metrics, multiple releases, majority voting, data clustering, classification models, case-based reasoning, Software reliability estimation}
}

@inproceedings{10.1145/2499393.2499398,
author = {Calikli, Gul and Bener, Ayse},
title = {An algorithmic approach to missing data problem in modeling human aspects in software development},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499398},
doi = {10.1145/2499393.2499398},
abstract = {Background: In our previous research, we built defect prediction models by using confirmation bias metrics. Due to confirmation bias developers tend to perform unit tests to make their programs run rather than breaking their code. This, in turn, leads to an increase in defect density. The performance of prediction model that is built using confirmation bias was as good as the models that were built with static code or churn metrics.Aims: Collection of confirmation bias metrics may result in partially "missing data" due to developers' tight schedules, evaluation apprehension and lack of motivation as well as staff turnover. In this paper, we employ Expectation-Maximization (EM) algorithm to impute missing confirmation bias data.Method: We used four datasets from two large-scale companies. For each dataset, we generated all possible missing data configurations and then employed Roweis' EM algorithm to impute missing data. We built defect prediction models using the imputed data. We compared the performances of our proposed models with the ones that used complete data.Results: In all datasets, when missing data percentage is less than or equal to 50% on average, our proposed model that used imputed data yielded performance results that are comparable with the performance results of the models that used complete data.Conclusions: We may encounter the "missing data" problem in building defect prediction models. Our results in this study showed that instead of discarding missing or noisy data, in our case confirmation bias metrics, we can use effective techniques such as EM based imputation to overcome this problem.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {10},
numpages = {10},
keywords = {confirmation bias, expectation maximisation (EM) algorithm, handling missing data, software defect prediction},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@inproceedings{10.1145/3383219.3383268,
author = {Lenz, Luca and Felderer, Michael and Schwedes, Sascha and M\"{u}ller, Kai},
title = {Explainable Priority Assessment of Software-Defects using Categorical Features at SAP HANA},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383268},
doi = {10.1145/3383219.3383268},
abstract = {We want to automate priority assessment of software defects. To do so we provide a tool which uses an explainability-driven framework and classical machine learning algorithms to keep the decisions transparent. Differing from other approaches we only use objective and categorical fields from the bug tracking system as features. This makes our approach lightweight and extremely fast. We perform binary classification with priority labels corresponding to deadlines. Additionally, we evaluate the tool on real data to ensure good performance in the practical use case.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {366–367},
numpages = {2},
keywords = {software quality, machine learning, defect assessment, bug priority},
location = {Trondheim, Norway},
series = {EASE '20}
}

@article{10.5555/1283720.1283725,
author = {Pizzi, Nick J.},
title = {Software quality prediction using fuzzy integration: a case study},
year = {2007},
issue_date = {August 2007},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {1},
issn = {1432-7643},
abstract = {Given the complexity of many contemporary software systems, it is often difficult to gauge the overall quality of their underlying software components. A potential technique to automatically evaluate such qualitative attributes is to use software metrics as quantitative predictors. In this case study, an aggregation technique based on fuzzy integration is presented that combines the predicted qualitative assessments from multiple classifiers. Multiple linear classifiers are presented with randomly selected subsets of automatically generated software metrics describing components from a sophisticated biomedical data analysis system. The external reference test is a software developer’s thorough assessment of complexity, maintainability, and usability, which is used to assign corresponding quality class labels to each system component. The aggregated qualitative predictions using fuzzy integration are shown to be superior to the predictions from the respective best single classifiers.},
journal = {Soft Comput.},
month = aug,
pages = {67–76},
numpages = {10}
}

@article{10.1145/3384517,
author = {Kapur, Ritu and Sodhi, Balwinder},
title = {A Defect Estimator for Source Code: Linking Defect Reports with Programming Constructs Usage Metrics},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3384517},
doi = {10.1145/3384517},
abstract = {An important issue faced during software development is to identify defects and the properties of those defects, if found, in a given source file. Determining defectiveness of source code assumes significance due to its implications on software development and maintenance cost.We present a novel system to estimate the presence of defects in source code and detect attributes of the possible defects, such as the severity of defects. The salient elements of our system are: (i) a dataset of newly introduced source code metrics, called PROgramming CONstruct (PROCON) metrics, and (ii) a novel Machine-Learning (ML)-based system, called Defect Estimator for Source Code (DESCo), that makes use of PROCON dataset for predicting defectiveness in a given scenario. The dataset was created by processing 30,400+ source files written in four popular programming languages, viz., C, C++, Java, and Python.The results of our experiments show that DESCo system outperforms one of the state-of-the-art methods with an improvement of 44.9%. To verify the correctness of our system, we compared the performance of 12 different ML algorithms with 50+ different combinations of their key parameters. Our system achieves the best results with SVM technique with a mean accuracy measure of 80.8%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {12},
numpages = {35},
keywords = {source code mining, software metrics, software faults and failures, software defect prediction, automated software engineering, Maintaining software, AI in software engineering}
}

@inproceedings{10.1145/3273934.3273941,
author = {Dey, Tapajit and Mockus, Audris},
title = {Modeling Relationship between Post-Release Faults and Usage in Mobile Software},
year = {2018},
isbn = {9781450365932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3273934.3273941},
doi = {10.1145/3273934.3273941},
abstract = {Background: The way post-release usage of a software affects the number of faults experienced by users is scarcely explored due to the proprietary nature of such data. The commonly used quality measure of post-release faults may, therefore, reflect usage instead of the quality of the software development process. Aim: To determine how software faults and software use are related in a post-deployment scenario and, based on that, derive post-deployment quality measure that reflects developers' performance more accurately. Method: We analyze Google Analytics data counting daily new users, visits, time-on-site, visits per user, and release start date and duration for 169 releases of a complex communication application for Android OS. We utilize Linear Regression, Bayesian Network, and Random Forest models to explain the interrelationships and to derive release quality measure that is relatively stable with respect to variations in software usage. Results: We found the number of new users and release start date to be the determining factors for the number of exceptions, and found no direct link between the intensity and frequency of software usage and software faults. Furthermore, the relative increase in the number of crashes was found to be stably associated with a power of 1.3 relative increase in the number of new users. Based on the findings we propose a release quality measure: number of crashes per user for a release of the software, which was seen to be independent of any other usage variables, providing us with a usage independent measure of software quality. Conclusions: We expect our result and our proposed quality measure will help gauge release quality of a software more accurately and inspire further research in this area.},
booktitle = {Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {56–65},
numpages = {10},
keywords = {Bayesian Networks, Linear Regression, Random Forest, Software Faults, Software Quality, Software Usage},
location = {Oulu, Finland},
series = {PROMISE'18}
}

@inproceedings{10.1145/3364641.3364657,
author = {J\'{u}nior, Luiz Cavamura and Morimoto, Ricardo and Fabbri, Sandra and Vincenzi, Auri Marcelo Rizzo},
title = {Software Operational Profile vs. Test Profile},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364657},
doi = {10.1145/3364641.3364657},
abstract = {Software Operational Profile (SOP) is a software specification based on how users use the software. This specification corresponds to a quantitative representation of software that identifies the most used software parts. As software reliability depends on the context in which users operate the software, ones use SOP in software reliability engineering. However, there are evidence of a misalignment between the software tested parts and SOP. Therefore, this paper investigates a possible misalignment between SOP and the tested software parts to obtain, based on experimental data, more evidence of this misalignment. We performed an exploratory study composed of three activities to verify: a) whether there are significant variations in how users operate the software; b) whether there is a misalignment between SOP and the tested software parts; c) if failures occur in untested SOP parts in case of misalignment. To perform these verifications, we defined the term "test profile" and presented it in this paper. We instrumented three software to collect data from them while the users were operating this software. Posteriorly, we analyzed these collected data in an attempt to reach the goals of this paper. To evaluate the originality of this research, we performed a Literature Systematic Review (SLR) and presented its conclusions. The obtained results evidence that there are significant variations in how users operate the software and also that there was a misalignment between SOP and the tested software parts when we evaluated the three software mentioned above. There were also indications of the occurrence of failures in the untested SOP parts. These results indicate that SOP becomes relevant not only to software reliability engineering but also to contribute to testing activities, regardless of the adopted strategy.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {139–148},
numpages = {10},
keywords = {Test Profile, Software Testing, Software Quality, Operational Profile},
location = {Fortaleza, Brazil},
series = {SBQS '19}
}

@inproceedings{10.1145/2905055.2905123,
author = {Singh, Satwinder and Singla, Rozy},
title = {Comparative Performance of Fault-Prone Prediction Classes with K-means Clustering and MLP},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905123},
doi = {10.1145/2905055.2905123},
abstract = {Software defect in today's era is most important in the field of software engineering. Most of the organizations used various techniques to predict defects in their products before they are delivered. Defect prediction techniques help the organizations to use their resources effectively which results in lower cost and time requirements. There are various techniques that are used for predicting defects in software before it has to be delivered. For example clustering, neural networks, support vector machine (SVM) etc. In this paper two defect prediction techniques: - K-means Clustering and Multilayer Perceptron model (MLP), are compared. Both the techniques are implemented on different platforms. K-means clustering is implemented using WEKA tool and MLP is implemented using SPSS. The results are compared to find which algorithm produces better results. In this paper Object-Oriented metrics are used for predicting defects in the software.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {65},
numpages = {7},
keywords = {Weka, Object-Oriented Metrics, Neural Network, K-means Clustering, Defect prediction},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1007/978-3-030-57321-8_28,
author = {Saranti, Anna and Taraghi, Behnam and Ebner, Martin and Holzinger, Andreas},
title = {Property-Based Testing for Parameter Learning of Probabilistic Graphical Models},
year = {2020},
isbn = {978-3-030-57320-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-57321-8_28},
doi = {10.1007/978-3-030-57321-8_28},
abstract = {Code quality is a requirement for successful and sustainable software development. The emergence of Artificial Intelligence and data driven Machine Learning in current applications makes customized solutions for both data as well as code quality a requirement. The diversity and the stochastic nature of Machine Learning algorithms require different test methods, each of which is suitable for a particular method. Conventional unit tests in test-automation environments provide the common, well-studied approach to tackle code quality issues, but Machine Learning applications pose new challenges and have different requirements, mostly as far the numerical computations are concerned. In this research work, a concrete use of property-based testing for quality assurance in the parameter learning algorithm of a probabilistic graphical model is described. The necessity and effectiveness of this method in comparison to unit tests is analyzed with concrete code examples for enhanced retraceability and interpretability, thus highly relevant for what is called explainable AI.},
booktitle = {Machine Learning and Knowledge Extraction: 4th IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 International Cross-Domain Conference, CD-MAKE 2020, Dublin, Ireland, August 25–28, 2020, Proceedings},
pages = {499–515},
numpages = {17},
keywords = {Machine learning, Probabilistic graphical models, Property-based testing},
location = {Dublin, Ireland}
}

@article{10.1016/j.infsof.2013.03.006,
author = {Menzies, Tim},
title = {Guest editorial for the Special Section on BEST PAPERS from the 2011 conference on Predictive Models in Software Engineering (PROMISE)},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.03.006},
doi = {10.1016/j.infsof.2013.03.006},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1477–1478},
numpages = {2}
}

@inproceedings{10.1109/PROMISE.2007.14,
author = {Weyuker, Elaine J. and Ostrand, Thomas J. and Bell, Robert M.},
title = {Using Developer Information as a Factor for Fault Prediction},
year = {2007},
isbn = {0769529542},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PROMISE.2007.14},
doi = {10.1109/PROMISE.2007.14},
abstract = {We have been investigating different prediction models to identify which files of a large multi-release industrial software system are most likely to contain the largest numbers of faults in the next release. To make predictions we considered a number of different file characteristics and change information about the files, and have built fully-automatable models that do not require that the user have any statistical expertise. We now consider the effect of adding developer information as a prediction factor and assess the extent to which this affects the quality of the predictions.},
booktitle = {Proceedings of the Third International Workshop on Predictor Models in Software Engineering},
pages = {8},
series = {PROMISE '07}
}

@inproceedings{10.1145/1868328.1868341,
author = {Caglayan, Bora and Tosun, Ayse and Miranskyy, Andriy and Bener, Ayse and Ruffolo, Nuzio},
title = {Usage of multiple prediction models based on defect categories},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868341},
doi = {10.1145/1868328.1868341},
abstract = {Background: Most of the defect prediction models are built for two purposes: 1) to detect defective and defect-free modules (binary classification), and 2) to estimate the number of defects (regression analysis). It would also be useful to give more information on the nature of defects so that software managers can plan their testing resources more effectively.Aims: In this paper, we propose a defect prediction model that is based on defect categories.Method: We mined the version history of a large-scale enterprise software product to extract churn and static code metrics. and grouped them into three defect categories according to different testing phases. We built a learning-based model for each defect category. We compared the performance of our proposed model with a general one. We conducted statistical techniques to evaluate the relationship between defect categories and software metrics. We also tested our hypothesis by replicating the empirical work on Eclipse data.Results: Our results show that building models that are sensitive to defect categories is cost-effective in the sense that it reveals more information and increases detection rates (pd) by 10% keeping the false alarms (pf) constant.Conclusions: We conclude that slicing defect data and categorizing it for use in a defect prediction model would enable practitioners to take immediate actions. Our results on Eclipse replication showed that haphazard categorization of defects is not worth the effort.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {8},
numpages = {9},
keywords = {defect categories, defect prediction, software quality},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1145/3340482.3342747,
author = {Lenarduzzi, Valentina and Martini, Antonio and Taibi, Davide and Tamburri, Damian Andrew},
title = {Towards surgically-precise technical debt estimation: early results and research roadmap},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342747},
doi = {10.1145/3340482.3342747},
abstract = {The concept of technical debt has been explored from many perspectives but its precise estimation is still under heavy empirical and experimental inquiry. We aim to understand whether, by harnessing approximate, data-driven, machine-learning approaches it is possible to improve the current techniques for technical debt estimation, as represented by a top industry quality analysis tool such as SonarQube. For the sake of simplicity, we focus on relatively simple regression modelling techniques and apply them to modelling the additional project cost connected to the sub-optimal conditions existing in the projects under study. Our results shows that current techniques can be improved towards a more precise estimation of technical debt and the case study shows promising results towards the identification of more accurate estimation of technical debt.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {37–42},
numpages = {6},
keywords = {Technical Debt, Machine Learning, Empirical Study},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@article{10.1007/s11219-020-09502-5,
author = {Raibulet, Claudia and Arcelli Fontana, Francesca and Carettoni, Simone},
title = {A preliminary analysis of self-adaptive systems according to different issues},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09502-5},
doi = {10.1007/s11219-020-09502-5},
abstract = {Self-adaptive systems dynamically change their structure and behavior in response to changes in their execution environment to ensure the quality of the services they provide. Self-adaptive systems are usually built of a managed part, which implements their functionality, and a managing part, which implements the self-adaptive mechanisms. Hence, the complexity of self-adaptive systems results also from the existence of the managing part and the interaction between the managed and the managing parts. The available evaluation approaches of self-adaptive systems focus on their performances, i.e., on the benefits (e.g., degree of autonomy, support for detecting anomalous behavior, adaptivity time, quality of response) achieved through the self-adaptive mechanisms of the managing part. In this paper, we evaluate the quality of the design of self-adaptive systems (including the managed and the managing parts) as it is done in traditional software engineering. We are interested in the internal software quality of self-adaptive systems, as the existence of the managing part and its interaction with the managed part leads to a tightly coupled system. We analyze the self-adaptive systems through the detection of different issues such as architectural and code smells and the detection of design patterns. The smells provide some hints on possible design and implementation problems, and help software engineers to improve the quality of the systems. While, design patterns are usually indicators of the application of good practices in the software development and allow to capture part of the design rationale. In this way, they can help software engineers to understand, reuse, and extend self-adaptive systems. In this paper, we have considered the detection of 3 architectural smells, 18 code smells, and 15 design patterns in 11 self-adaptive systems written in the Java programming language. The results indicate that the 3 architectural smells, 9 out of the 18 code smells, and the 15 design patterns have been detected in all the analyzed self-adaptive systems. We also discuss the possible reasons behind the presence of these quality issues, and provide our lessons learned.},
journal = {Software Quality Journal},
month = sep,
pages = {1213–1243},
numpages = {31},
keywords = {Design patterns, Code smells, Architectural smells, Software quality, Self-adaptive systems}
}

@inproceedings{10.5555/951951.952272,
author = {Khoshgoftaar, Taghi M. and Nguyen, Laurent and Gao, Kehan and Rajeevalochanam, Jayanth},
title = {Application of an Attribute Selection Method to CBR-Based Software Quality Classification},
year = {2003},
isbn = {0769520383},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This study investigates the attribute selection problem for reducing the number of software metrics (program attributes) used by a case-based reasoning (CBR) software quality classification model. The metrics are selected using the Kolmogorov-Smirnov (K-S) two sample test. The "Modified Expected Cost of Misclassification" measure, recently proposed by ourresearch team, is used as a performance measure to select, evaluate, and compare classification models. The attribute selection procedure presented in this paper can assist a software development organization in determining the software metrics that are better indicators of software quality. By reducing the number of software metrics to be collected during the development process, the metrics data collection task can be simplified. Moreover, reducing the number of metrics would result in reducing the computation time of a cbr model. Using an empirical case study of a real-world software system, it is shown that with a reduced number of metrics the cbr technique is capable of yielding useful software quality classification models. Moreover, their performances were better than or similar to cbr models calibrated without attribute selection.},
booktitle = {Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence},
pages = {47},
keywords = {software quality, software metrics selection, modified expected cost of misclassification, case-based reasoning, Kolmogorov-Smirnov two-sample test},
series = {ICTAI '03}
}

@article{10.4018/ijismd.2014100102,
author = {Deneck\`{e}re, Rebecca and Hug, Charlotte and Khodabandelou, Ghazaleh and Salinesi, Camille},
title = {Intentional Process Mining: Discovering and Modeling the Goals Behind Processes using Supervised Learning},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/ijismd.2014100102},
doi = {10.4018/ijismd.2014100102},
abstract = {Understanding people's goals is a challenging issue that is met in many different areas such as security, sales, information retrieval, etc. Intention Mining aims at uncovering intentions from observations of actual activities. While most Intention Mining techniques proposed so far focus on mining individual intentions to analyze web engine queries, this paper proposes a generic technique to mine intentions from activity traces. The proposed technique relies on supervised learning and generates intentional models specified with the Map formalism. The originality of the contribution lies in the demonstration that it is actually possible to reverse engineer the underlying intentional plans built by people when in action, and specify them in models e.g. with intentions at different levels, dependencies, links with other concepts, etc. After an introduction on intention mining, the paper presents the Supervised Map Miner Method and reports two controlled experiments that were undertaken to evaluate precision, recall and F-Score. The results are promising since the authors were able to find the intentions underlying the activities as well as the corresponding map process model with satisfying accuracy, efficiency and performance.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {22–47},
numpages = {26},
keywords = {Trace, Supervised Learning, Intention Mining, Hidden Markov Model, Goal Modeling, Event Log}
}

@article{10.1016/j.infsof.2008.08.006,
author = {Acu\~{n}a, Silvia T. and G\'{o}mez, Marta and Juristo, Natalia},
title = {How do personality, team processes and task characteristics relate to job satisfaction and software quality?},
year = {2009},
issue_date = {March, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2008.08.006},
doi = {10.1016/j.infsof.2008.08.006},
abstract = {This article analyses the relationships between personality, team processes, task characteristics, product quality and satisfaction in software development teams. The data analysed here were gathered from a sample of 35 teams of students (105 participants). These teams applied an adaptation of an agile methodology, eXtreme Programming (XP), to develop a software product. We found that the teams with the highest job satisfaction are precisely the ones whose members score highest for the personality factors agreeableness and conscientiousness. The satisfaction levels are also higher when the members can decide how to develop and organize their work. On the other hand, the level of satisfaction and cohesion drops the more conflict there is between the team members. Finally, the teams exhibit a significant positive correlation between the personality factor extraversion and software product quality.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {627–639},
numpages = {13},
keywords = {Team processes, Task characteristics, Software quality, Personality factors, Job satisfaction}
}

@article{10.1007/s11219-017-9397-z,
author = {Sultana, Kazi Zakia and Williams, Byron J. and Bhowmik, Tanmay},
title = {A study examining relationships between micro patterns and security vulnerabilities},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9397-z},
doi = {10.1007/s11219-017-9397-z},
abstract = {Software security is an integral part of software quality and reliability. Software vulnerabilities make the software susceptible to attacks which violates software security. Metric-based software vulnerability prediction is one way to evaluate vulnerabilities beforehand so that developers can take preventative measures against attacks. In this study, we explore the correlation between software vulnerabilities and code-level constructs called micro patterns. These code patterns characterize class-level object-oriented program features. Existing research addressed micro pattern correlation with software defects. We analyzed the correlation between vulnerabilities and micro patterns from different viewpoints and explored whether they are related. We studied the distribution of micro patterns and their associations with vulnerable classes in 42 versions of the Apache Tomcat and three Java web applications. This study shows that certain micro patterns are frequently present in vulnerable classes. We also show that there is a high correlation between certain patterns that coexist in a vulnerable class.},
journal = {Software Quality Journal},
month = mar,
pages = {5–41},
numpages = {37},
keywords = {Software vulnerabilities, Software security, Software quality, Micro patterns}
}

@article{10.1007/s11334-015-0256-4,
author = {Valles-Barajas, Fernando},
title = {A comparative analysis between two techniques for the prediction of software defects: fuzzy and statistical linear regression},
year = {2015},
issue_date = {December  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-015-0256-4},
doi = {10.1007/s11334-015-0256-4},
abstract = {Software engineers should estimate the necessary resources (time, people, software tools among others) to satisfy software project requirements; this activity is carried out in the planning phase. The estimated time for developing software projects is a necessary element to establish the cost of software projects and to assign human resources to every phase of software projects. Most companies fail to finish software projects on time because of a poor estimation technique or the lack of the same. The estimated time must consider the time spent eliminating software defects injected during each of the software phases. A comparative analysis between two techniques (fuzzy linear regression and statistical linear regression) to perform software defect estimation is presented. These two techniques model uncertainty in a different way; statistical linear regression models uncertainty as randomness, whereas fuzzy linear regression models uncertainty as fuzziness. The main objective of this paper was to establish the kind of uncertainty associated with software defect prediction and to contrast these two prediction techniques. The KC1 NASA data set was used to do this analysis. Only six of the metrics included in KC1 data set and lines of code metric were used in this comparative analysis. Descriptive statistics was first used to have an overview of the main characteristics of the data set used in this research. Linearity property between predictor variables and the variable of interest number of defects was checked using scatter plots and Pearson's correlation coefficient. Then the problem of multicollinearity was verified using inter-correlations among metrics and the variance inflation factor. Best subset regression was applied to detect the most influencing subset of predictor variables; this subset was later used to build fuzzy and statistical regression models. Linearity property between metrics and number of defects was confirmed. The problem of multicollinearity was not detected in the predictor variables. Best subset regression found that the subset composed of 5 variables was the most influencing subset. The analysis showed that the statistical regression model in general outperformed the fuzzy regression model. Techniques for making software defect prediction should be carefully employed in order to have quality plans. Software engineers should consider and understand a set of prediction techniques and know their weaknesses and strengths. At least, in the KC1 data set, the uncertainty in the software defect prediction model is due to randomness so it is reasonable to use statistical linear regression instead of fuzzy linear regression to build a prediction model.},
journal = {Innov. Syst. Softw. Eng.},
month = dec,
pages = {277–287},
numpages = {11},
keywords = {Statistical linear regression, Software defect prediction, Fuzzy linear regression}
}

@inproceedings{10.1145/3416505.3423560,
author = {Lomio, Francesco and Baselga, Diego Mart\'{\i}nez and Moreschini, Sergio and Huttunen, Heikki and Taibi, Davide},
title = {RARE: a labeled dataset for cloud-native memory anomalies},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423560},
doi = {10.1145/3416505.3423560},
abstract = {Anomaly detection has been attracting interest from both the industry and the research community for many years, as the number of published papers and services adopted grew exponentially over the last decade. One of the reasons behind this is the wide adoption of cloud systems from the majority of players in multiple industries, such as online shopping, advertisement or remote computing. In this work we propose a Dataset foR cloud-nAtive memoRy anomaliEs: RARE. It includes labelled anomaly time-series data, comprising of over 900 unique metrics. This dataset has been generated using a microservice for injecting artificial byte stream in order to overload the nodes, provoking memory anomalies, which in some cases resulted in a crash. The system was built using a Kafka server deployed on a Kubernetes system. Moreover, in order to get access and download the metrics related to the server, we utilised Prometheus. In this paper we present a dataset that can be used coupled with machine learning algorithms for detecting anomalies in a cloud based system. The dataset will be available in the form of CSV file through an online repository. Moreover, we also included an example of application using a Random Forest algorithm for classifying the data as anomalous or not. The goal of the RARE dataset is to help in the development of more accurate and reliable machine learning methods for anomaly detection in cloud based systems.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {19–24},
numpages = {6},
keywords = {self healing, machine learnin, kubernetes, anomaly detection, Dataset},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@article{10.2753/MIS0742-1222240206,
author = {Choudhary, Vidyanand},
title = {Comparison of Software Quality Under Perpetual Licensing and Software as a Service},
year = {2007},
issue_date = {Number 2 / Fall 2007},
publisher = {M. E. Sharpe, Inc.},
address = {USA},
volume = {24},
number = {2},
issn = {0742-1222},
url = {https://doi.org/10.2753/MIS0742-1222240206},
doi = {10.2753/MIS0742-1222240206},
abstract = {Software is available through a number of different licensing models such as the commonly used perpetual licensing model and a relatively new licensing model called software as a service (SaaS). There are several differences between SaaS and perpetual licensing. SaaS licensing offers software using a subscription model, whereas perpetual licensing involves a one-time payment for a perpetual use license and optional additional payments for future upgrades. Prior literature has not considered the impact of these licensing schemes on the publisher's incentive to invest in software quality. We model differences in how new software features are disseminated in SaaS and perpetual licensing. We show that these differences affect the publisher's incentive to invest in product development. We find that the SaaS licensing model leads to greater investment in product development under most conditions. This increased investment leads to higher software quality in equilibrium under SaaS as compared to perpetual licensing. The software publisher earns greater profits and social welfare is higher under SaaS under these conditions.},
journal = {J. Manage. Inf. Syst.},
month = oct,
pages = {141–165},
numpages = {25},
keywords = {Software Quality, Software Licensing, Software As A Service, Product Development, Pricing, On-Demand Computing, Monopoly, Information Goods, Application Service Providers}
}

@article{10.1504/IJDATS.2017.10003991,
title = {Software fault proneness prediction: a comparative study between bagging, boosting, and stacking ensemble and base learner methods},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {1},
issn = {1755-8050},
url = {https://doi.org/10.1504/IJDATS.2017.10003991},
doi = {10.1504/IJDATS.2017.10003991},
abstract = {Modules with defects might be the prime reason for decreasing the software quality and increasing the cost of maintenance. Therefore, the prediction of faulty modules of systems under test at early stages contributes to the overall quality of software products. In this research three symmetric ensemble methods: bagging, boosting and stacking are used to predict faulty modules based on evaluating the performance of 11 base learners. The results reveal that the defect prediction performance of the base learner classifier and ensemble learner classifiers is the same for na\"{\i}ve Bayes, Bayes net, PART, random forest, IB1, VFI, decision table, and NB tree base learners, the case was different for boosted SMO, bagged J48 and boosted and bagged random tree. In addition the results showed that the random forest classifier is one of the most significant classifiers that should be stacked with other classifiers to gain the better fault prediction.},
journal = {Int. J. Data Anal. Tech. Strateg.},
month = jan,
pages = {1–16},
numpages = {16}
}

@article{10.1016/j.ins.2021.05.041,
author = {Kluska, Jacek and Madera, Michal},
title = {Extremely simple classifier based on fuzzy logic and gene expression programming},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {571},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.05.041},
doi = {10.1016/j.ins.2021.05.041},
journal = {Inf. Sci.},
month = sep,
pages = {560–579},
numpages = {20},
keywords = {Interpretability, Gene expression programming, Fuzzy rule-based classifier, Data mining, Machine learning}
}

@article{10.1016/j.eswa.2017.04.014,
author = {Rathore, Santosh Singh and Kumar, Sandeep},
title = {Towards an ensemble based system for predicting the number of software faults},
year = {2017},
issue_date = {October 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {82},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.04.014},
doi = {10.1016/j.eswa.2017.04.014},
abstract = {Paper presents ensemble based system for the prediction of number of software faults.System is based on the heterogeneous ensemble method.System uses three fault prediction techniques as base learners for the ensemble.Results are verified on Eclipse datasets. Software fault prediction using different techniques has been done by various researchers previously. It is observed that the performance of these techniques varied from dataset to dataset, which make them inconsistent for fault prediction in the unknown software project. On the other hand, use of ensemble method for software fault prediction can be very effective, as it takes the advantage of different techniques for the given dataset to come up with better prediction results compared to individual technique. Many works are available on binary class software fault prediction (faulty or non-faulty prediction) using ensemble methods, but the use of ensemble methods for the prediction of number of faults has not been explored so far. The objective of this work is to present a system using the ensemble of various learning techniques for predicting the number of faults in given software modules. We present a heterogeneous ensemble method for the prediction of number of faults and use a linear combination rule and a non-linear combination rule based approaches for the ensemble. The study is designed and conducted for different software fault datasets accumulated from the publicly available data repositories. The results indicate that the presented system predicted number of faults with higher accuracy. The results are consistent across all the datasets. We also use prediction at level l (Pred(l)), and measure of completeness to evaluate the results. Pred(l) shows the number of modules in a dataset for which average relative error value is less than or equal to a threshold value l. The results of prediction at level l analysis and measure of completeness analysis have also confirmed the effectiveness of the presented system for the prediction of number of faults. Compared to the single fault prediction technique, ensemble methods produced improved performance for the prediction of number of software faults. Main impact of this work is to allow better utilization of testing resources helping in early and quick identification of most of the faults in the software system.},
journal = {Expert Syst. Appl.},
month = oct,
pages = {357–382},
numpages = {26},
keywords = {Software fault prediction techniques, Promise repository, Linear regression, Gradient boosting, Genetic programming, Empirical study}
}

@article{10.1023/B:SQJO.0000042059.16470.f0,
author = {Ohlsson, Niclas and Zhao, Ming and Helander, Mary},
title = {Application of multivariate analysis for software fault prediction},
year = {1998},
issue_date = {1998},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {7},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1023/B:SQJO.0000042059.16470.f0},
doi = {10.1023/B:SQJO.0000042059.16470.f0},
abstract = {Prediction of fault-prone modules provides one way to support software quality engineering through improved scheduling and project control. The primary goal of our research was to develop and refine techniques for early prediction of fault-prone modules. The objective of this paper is to review and improve an approach previously examined in the literature for building prediction models, i.e. principal component analysis (PCA) and discriminant analysis (DA). We present findings of an empirical study at Ericsson Telecom AB for which the previous approach was found inadequate for predicting the most fault-prone modules using software design metrics. Instead of dividing modules into fault-prone and not-fault-prone, modules are categorized into several groups according to the ordered number of faults. It is shown that the first discriminant coordinates (DC) statistically increase with the ordering of modules, thus improving prediction and prioritization efforts. The authors also experienced problems with the smoothing parameter as used previously for DA. To correct this problem and further improve predictability, separate estimation of the smoothing parameter is shown to be required.},
journal = {Software Quality Journal},
month = may,
pages = {51–66},
numpages = {16},
keywords = {software design metrics, prediction, multivariate analysis, fault-prone modules, empirical study}
}

@phdthesis{10.5555/1124033,
author = {Azar, Danielle},
title = {Using genetic algorithms to optimize software quality estimation models},
year = {2004},
isbn = {0494062703},
publisher = {McGill University},
address = {CAN},
abstract = {Assessing software quality is fundamental in the software developing field. Most software quality characteristics cannot be measured before a certain period of use of the software product. However, they can be predicted or estimated based on other measurable quality attributes. Software quality estimation models are built and used extensively for this purpose. Most such models are constructed using statistical or machine learning techniques. However, in this domain it is very hard to obtain data sets on which to train such models; often such data sets are proprietary, and the publicly available data sets are too small, or not representative. Hence, the accuracy of the models often deteriorates significantly when they are used to classify new data. This thesis explores the use of genetic algorithms for the problem of optimizing existing rule-based software quality estimation models. The main contributions of this work are two evolutionary approaches to this optimization problem. In the first approach, we assume the existence of several models, and we use a genetic algorithm to combine them, and adapt them to a given data set. The second approach optimizes a single model. The core concept of this thesis is to consider existing models that have been constructed on one data set and adapt them to new data. In real applications, this can be seen as adapting already existing software quality estimation models that have been constructed on data extracted from common domain knowledge to context-specific data. Our technique maintains the white-box nature of the models which can be used as guidelines in future software development processes.},
note = {AAINR06270}
}

@inproceedings{10.1145/1985374.1985391,
author = {Turnu, Ivana and Concas, Giulio and Marchesi, Michele and Tonelli, Roberto},
title = {The fractal dimension metric and its use to assess object-oriented software quality},
year = {2011},
isbn = {9781450305938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985374.1985391},
doi = {10.1145/1985374.1985391},
abstract = {We present a study were software systems are considered as complex networks which have a self-similar structure under a length-scale transformation. On such complex software networks we computed a self-similar coefficient, also known as fractal dimension, using "the box counting method".We analyzed various releases of the publically available Eclipse software systems, calculating the fractal dimension for twenty sub-projects, randomly chosen, for every release, as well as for each release as a whole. Our results display an overall consistency among the sub-projects and among all the analyzed releases.We found a very good correlation between the fractal dimension and the number of bugs for Eclipse and for twenty sub-projects. Since the fractal dimension is just a scalar number that characterizes a whole system, while complexity and quality metrics are in general computed on every system module, this result suggests that the fractal dimension could be considered as a global quality metric for large software systems. Our results need however to be confirmed for other large software systems.},
booktitle = {Proceedings of the 2nd International Workshop on Emerging Trends in Software Metrics},
pages = {69–74},
numpages = {6},
keywords = {software metrics, fractal dimension, complex networks},
location = {Waikiki, Honolulu, HI, USA},
series = {WETSoM '11}
}

@article{10.1504/IJCAT.2009.026595,
author = {Singh, Yogesh and Kaur, Arvinder and Malhotra, Ruchika},
title = {Comparative analysis of regression and machine learning methods for predicting fault proneness models},
year = {2009},
issue_date = {June 2009},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {35},
number = {2/3/4},
issn = {0952-8091},
url = {https://doi.org/10.1504/IJCAT.2009.026595},
doi = {10.1504/IJCAT.2009.026595},
abstract = {Demand for quality software has undergone rapid growth during the last few years. This is leading to increase in development of machine learning techniques for exploring datasets which can be used in constructing models for predicting quality attributes such as Decision Tree (DT), Support Vector Machine (SVM) and Artificial Neural Network (ANN). This paper examines and compares Logistic Regression (LR), ANN (model predicted in an analogous study using the same dataset), SVM and DT methods. These two methods are explored empirically to find the effect of object-oriented metrics given by Chidamber and Kemerer on the fault proneness of object-oriented system classes. Data collected from Java applications is used in the study. The performance of the methods was compared by Receiver Operating Characteristic (ROC) analysis. DT modelling showed 84.7% of correct classifications of faulty classes and is a better model than the model predicted using LR, SVM and ANN method. The area under the ROC curve of LR, ANN, SVM and DT model is 0.826, 0.85, 0.85 and 0.87, respectively. The paper shows that machine learning methods are useful in constructing software quality models.},
journal = {Int. J. Comput. Appl. Technol.},
month = jun,
pages = {183–193},
numpages = {11},
keywords = {support vector machine, software quality models, receiver operating characteristics curve, object-oriented systems, metrics, machine learning, logistic regression, fault-prone software, decision trees, artificial neural networks, SVM, ANNs}
}

@article{10.1023/B:EMSE.0000027781.18360.9b,
author = {Khoshgoftaar, Taghi M. and Seliya, Naeem},
title = {Comparative Assessment of Software Quality Classification Techniques: An Empirical Case Study},
year = {2004},
issue_date = {September 2004},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {9},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1023/B:EMSE.0000027781.18360.9b},
doi = {10.1023/B:EMSE.0000027781.18360.9b},
abstract = {Software metrics-based quality classification models predict a software module as either fault-prone (fp) or not fault-prone (nfp). Timely application of such models can assist in directing quality improvement efforts to modules that are likely to be fp during operations, thereby cost-effectively utilizing the software quality testing and enhancement resources. Since several classification techniques are available, a relative comparative study of some commonly used classification techniques can be useful to practitioners. We present a comprehensive evaluation of the relative performances of seven classification techniques and/or tools. These include logistic regression, case-based reasoning, classification and regression trees (CART), tree-based classification with S-PLUS, and the Sprint-Sliq, C4.5, and Treedisc algorithms. The use of expected cost of misclassification (ECM), is introduced as a singular unified measure to compare the performances of different software quality classification models. A function of the costs of the Type I (a nfp module misclassified as fp) and Type II (a fp module misclassified as nfp) misclassifications, ECM is computed for different cost ratios. Evaluating software quality classification models in the presence of varying cost ratios is important, because the usefulness of a model is dependent on the system-specific costs of misclassifications. Moreover, models should be compared and preferred for cost ratios that fall within the range of interest for the given system and project domain. Software metrics were collected from four successive releases of a large legacy telecommunications system. A two-way ANOVA randomized-complete block design modeling approach is used, in which the system release is treated as a block, while the modeling method is treated as a factor. It is observed that predictive performances of the models is significantly different across the system releases, implying that in the software engineering domain prediction models are influenced by the characteristics of the data and the system being modeled. Multiple-pairwise comparisons are performed to evaluate the relative performances of the seven models for the cost ratios of interest to the case study. In addition, the performance of the seven classification techniques is also compared with a classification based on lines of code. The comparative approach presented in this paper can also be applied to other software systems.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {229–257},
numpages = {29},
keywords = {logistic regression, expected cost of misclassification, decision trees, case-based reasoning, analysis of variance, Software quality classification}
}

@inproceedings{10.1145/2365324.2365326,
author = {Shepperd, Martin},
title = {The scientific basis for prediction research},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365326},
doi = {10.1145/2365324.2365326},
abstract = {In recent years there has been a huge growth in using statistical and machine learning methods to find useful prediction systems for software engineers. Of particular interest is predicting project effort and duration and defect behaviour. Unfortunately though results are often promising no single technique dominates and there are clearly complex interactions between technique, training methods and the problem domain. Since we lack deep theory our research is of necessity experimental. Minimally, as scientists, we need reproducible studies. We also need comparable studies. I will show through a meta-analysis of many primary studies that we are not presently in that situation and so the scientific basis for our collective research remains in doubt. By way of remedy I will argue that we need to address these issues of reporting protocols and expertise plus ensure blind analysis is routine.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {1–2},
numpages = {2},
keywords = {defect prediction, empirical research, machine learning, software metrics},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@article{10.1007/s11219-011-9132-0,
author = {Gao, Kehan and Khoshgoftaar, Taghi M. and Seliya, Naeem},
title = {Predicting high-risk program modules by selecting the right software measurements},
year = {2012},
issue_date = {March     2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9132-0},
doi = {10.1007/s11219-011-9132-0},
abstract = {A timely detection of high-risk program modules in high-assurance software is critical for avoiding the high consequences of operational failures. While software risk can initiate from external sources, such as management or outsourcing, software quality is adversely affected when internal software risks are realized, such as improper practice of standard software processes or lack of a defined software quality infrastructure. Practitioners employ various techniques to identify and rectify high-risk or low-quality program modules. Effectiveness of detecting such modules is affected by the software measurements used, making feature selection an important step during software quality prediction. We use a wrapper-based feature ranking technique to select the optimal set of software metrics to build defect prediction models. We also address the adverse effects of class imbalance (very few low-quality modules compared to high-quality modules), a practical problem observed in high-assurance systems. Applying a data sampling technique followed by feature selection is a relatively unique contribution of our work. We present a comprehensive investigation on the impact of data sampling followed by attribute selection on the defect predictors built with imbalanced data. The case study data are obtained from several real-world high-assurance software projects. The key results are that attribute selection is more efficient when applied after data sampling, and defect prediction performance generally improves after applying data sampling and feature selection.},
journal = {Software Quality Journal},
month = mar,
pages = {3–42},
numpages = {40},
keywords = {Wrapper-based feature ranking, Software quality classification, Performance metrics, Imbalanced data, Feature selection, Data sampling}
}

@phdthesis{10.5555/1369140,
author = {Challagulla, Venkata Udaya Bhaskar},
advisor = {Bastani, Farokh B.},
title = {A machine learning-based approach for dynamic reliability assessment of mission critical software systems},
year = {2007},
isbn = {9780549270720},
publisher = {University of Texas at Dallas},
address = {USA},
abstract = {Software continues to become more complex and difficult to certify to a high degree of confidence due to the increasing scope and sophistication of the requirements. Consequently, traditional development techniques face growing challenges in satisfying these requirements. Future distributed real-time systems, such as robotic swarm systems, telecontrol systems, and industrial automation systems, may need to dynamically adapt themselves based on the run-time mission-specific requirements and operating conditions. This further compounds the problems of developing highly dependable systems. This is also the case with emerging Service Oriented Architecture (SOA) based systems that perform dynamic discovery of services and reconfiguration and composition of services at run-time. These dynamic features combined with the abstractions provided by the services necessitate the need for high-confidence run-time software reliability assessment techniques. This Dissertation investigates machine learning-based software defect prediction techniques to monitor and assess the services in the synthesized code. Experimental assessment of various prediction algorithms using real-world data shows that memory-based reasoning (MBR) techniques perform relatively better than other methods. Based on these results, a framework is developed to automatically derive the optimal configuration of an MBR classifier for software defect data by logical variations of its configuration parameters. This adaptive MBR technique provides a flexible and effective environment for accurate prediction of mission-critical software defect data. In practice, since these systems are dynamically assembled from existing services, a dearth of sufficient sample data regarding the actual operational environment can reduce the level of confidence in the reliability estimate. The Dissertation investigates the combination of Bayesian Belief Network (BBN) and MBR methodologies to integrate multiple evidences from all the services to obtain high-confidence estimates in the reliability of dynamically assembled mission-critical SOA-based systems. Latent defects in more frequently executed domains affect the reliability of the component much more than the domains tested using random testing strategies. A dynamic monitoring and diagnosis framework is developed to accurately estimate the reliability of the system as it executes. The framework incorporates a Markov model to determine the service reliability from its component reliabilities. This systematic assessment method is evaluated using a simulated system and a real-world case study involving an Enterprise Content Management System. An Intelligent Software Defect Analysis Tool (ISDAT) that implements the above framework is developed, to realize the framework objectives of providing a unified framework for dynamically assessing the reliability of mission-critical SOA-based systems to a high-degree of confidence by using AI-based prediction analysis on the defect metrics data collected from real-time system monitoring.},
note = {AAI3285271}
}

@article{10.1007/s11219-016-9339-1,
author = {Almaghairbe, Rafig and Roper, Marc},
title = {Separating passing and failing test executions by clustering anomalies},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9339-1},
doi = {10.1007/s11219-016-9339-1},
abstract = {Developments in the automation of test data generation have greatly improved efficiency of the software testing process, but the so-called oracle problem (deciding the pass or fail outcome of a test execution) is still primarily an expensive and error-prone manual activity. We present an approach to automatically detect passing and failing executions using cluster-based anomaly detection on dynamic execution data based on firstly, just a system's input/output pairs and secondly, amalgamations of input/output pairs and execution traces. The key hypothesis is that failures will group into small clusters, whereas passing executions will group into larger ones. Evaluation on three systems with a range of faults demonstrates this hypothesis to be valid--in many cases small clusters were composed of at least 60 % failures (and often more). Concentrating the failures in these small clusters substantially reduces the numbers of outputs that a developer would need to manually examine following a test run and illustrates that the approach has the potential to improve the effectiveness and efficiency of the testing process.},
journal = {Software Quality Journal},
month = sep,
pages = {803–840},
numpages = {38},
keywords = {Test oracles, Software testing, Clustering, Anomaly detection}
}

@article{10.1016/j.procs.2021.08.013,
author = {Tomescu, Vlad-Ioan and Czibula, Gabriela and Ni\c{t}ic\u{a}, \c{S}tefan},
title = {A study on using deep autoencoders for imbalanced binary classification},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {192},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.08.013},
doi = {10.1016/j.procs.2021.08.013},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {119–128},
numpages = {10},
keywords = {68T10, 2000 MSC: 68T07, Breast cancer detection, Autoencoders, Deep learning, Imbalanced classification}
}

@article{10.1007/s11219-020-09515-0,
author = {Ferenc, Rudolf and T\'{o}th, Zolt\'{a}n and Lad\'{a}nyi, Gergely and Siket, Istv\'{a}n and Gyim\'{o}thy, Tibor},
title = {A public unified bug dataset for java and its assessment regarding metrics and bug prediction},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09515-0},
doi = {10.1007/s11219-020-09515-0},
abstract = {Bug datasets have been created and used by many researchers to build and validate novel bug prediction models. In this work, our aim is to collect existing public source code metric-based bug datasets and unify their contents. Furthermore, we wish to assess the plethora of collected metrics and the capabilities of the unified bug dataset in bug prediction. We considered 5 public datasets and we downloaded the corresponding source code for each system in the datasets and performed source code analysis to obtain a common set of source code metrics. This way, we produced a unified bug dataset at class and file level as well. We investigated the diversion of metric definitions and values of the different bug datasets. Finally, we used a decision tree algorithm to show the capabilities of the dataset in bug prediction. We found that there are statistically significant differences in the values of the original and the newly calculated metrics; furthermore, notations and definitions can severely differ. We compared the bug prediction capabilities of the original and the extended metric suites (within-project learning). Afterwards, we merged all classes (and files) into one large dataset which consists of 47,618 elements (43,744 for files) and we evaluated the bug prediction model build on this large dataset as well. Finally, we also investigated cross-project capabilities of the bug prediction models and datasets. We made the unified dataset publicly available for everyone. By using a public unified dataset as an input for different bug prediction related investigations, researchers can make their studies reproducible, thus able to be validated and verified.},
journal = {Software Quality Journal},
month = dec,
pages = {1447–1506},
numpages = {60},
keywords = {Bug prediction, Static code analysis, Code metrics, Bug dataset}
}

@article{10.1016/j.eswa.2019.113085,
author = {Pandey, Sushant Kumar and Mishra, Ravi Bhushan and Tripathi, Anil Kumar},
title = {BPDET: An effective software bug prediction model using deep representation and ensemble learning techniques},
year = {2020},
issue_date = {Apr 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113085},
doi = {10.1016/j.eswa.2019.113085},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {22},
keywords = {Heterogeneous Ensemble learning technique, Staked denoising auto-encoder, Boosting, Deep representation, Software metrics, Classification technique, Software bug prediction}
}

@article{10.1007/s10115-021-01560-w,
author = {Brzezinski, Dariusz and Minku, Leandro L. and Pewinski, Tomasz and Stefanowski, Jerzy and Szumaczuk, Artur},
title = {The impact of data difficulty factors on classification of imbalanced and concept drifting data streams},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {63},
number = {6},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-021-01560-w},
doi = {10.1007/s10115-021-01560-w},
abstract = {Class imbalance introduces additional challenges when learning classifiers from concept drifting data streams. Most existing work focuses on designing new algorithms for dealing with the global imbalance ratio and does not consider other data complexities. Independent research on static imbalanced data has highlighted the influential role of local data difficulty factors such as minority class decomposition and presence of unsafe types of examples. Despite often being present in real-world data, the interactions between concept drifts and local data difficulty factors have not been investigated in concept drifting data streams yet. We thoroughly study the impact of such interactions on drifting imbalanced streams. For this purpose, we put forward a new categorization of concept drifts for class imbalanced problems. Through comprehensive experiments with synthetic and real data streams, we study the influence of concept drifts, global class imbalance, local data difficulty factors, and their combinations, on predictions of representative online classifiers. Experimental results reveal the high influence of new considered factors and their local drifts, as well as differences in existing classifiers’ reactions to such factors. Combinations of multiple factors are the most challenging for classifiers. Although existing classifiers are partially capable of coping with global class imbalance, new approaches are needed to address challenges posed by imbalanced data streams.},
journal = {Knowl. Inf. Syst.},
month = jun,
pages = {1429–1469},
numpages = {41},
keywords = {Stream classification, Drift categorization, Data difficulty factors, Concept drift, Class imbalance}
}

@inproceedings{10.5555/850952.853812,
author = {Khoshgoftaar, Taghi M. and Seliya, Naeem},
title = {Software Quality Classification Modeling Using The SPRINT Decision Tree Algorithm},
year = {2002},
isbn = {0769518494},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Predicting the quality of system modules prior to software testing and operations can benefit the software development team. Such a timely reliability estimation can be used to direct cost-effective quality improvement efforts to the high-risk modules. Tree-based softwarequality classification models based on software metrics are used to predict whether a software module is fauIt-prone or not fault-prone. They are white box quality estimation models with good accuracy, and are simpIe and easy to interpret.This paper presents an in-depth study of calibrating classification trees for software quality estimation using the SPRINT decision tree algorithm. Many classification algorithms have memory limitations including the requirement that data sets be memory resident. SPRINT removes all of these limitations and provides a fast and scalable analysis. It is an extension of a commonly used decision tree algorithm, CART, and provides a unique tree-pruning technique based on the Minimum Description Length (MDL) principle. Combining the MDL pruning technique and the modified classification algorithm, SPRINT yields classification trees with useful prediction accuracy. The case study used comprises of software metrics and fault data collected over four releases from a very large telecommunications system. It is observed that classification trees built by SPRINT are more balanced and demonstrate better stability incomparison to those built by CART.},
booktitle = {Proceedings of the 14th IEEE International Conference on Tools with Artificial Intelligence},
pages = {365},
keywords = {software quality estimation, software metrics, minimum description lengthprinciple, classification trees, IBM Intelligent Data Miner},
series = {ICTAI '02}
}

@inproceedings{10.1109/ICST.2010.46,
author = {Silva, Daniel G. e. and Jino, Mario and Abreu, Bruno T. de},
title = {Machine Learning Methods and Asymmetric Cost Function to Estimate Execution Effort of Software Testing},
year = {2010},
isbn = {9780769539904},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICST.2010.46},
doi = {10.1109/ICST.2010.46},
abstract = {Planning and scheduling of testing activities play an important role for any independent test team that performs tests for different software systems, developed by different development teams. This work studies the application of machine learning tools and variable selection tools to solve the problem of estimating the execution effort of functional tests. An analysis of the test execution process is developed and experiments are performed on two real databases. The main contributions of this paper are the approach of selecting the significant variables for database synthesis and the use of an artificial neural network trained with an asymmetric cost function.},
booktitle = {Proceedings of the 2010 Third International Conference on Software Testing, Verification and Validation},
pages = {275–284},
numpages = {10},
keywords = {software testing, prediction, neural networks, estimate, effort, asymmetric function},
series = {ICST '10}
}

@inproceedings{10.1007/978-3-030-58545-7_45,
author = {Li, Junbing and Zhang, Changqing and Zhu, Pengfei and Wu, Baoyuan and Chen, Lei and Hu, Qinghua},
title = {SPL-MLL: Selecting Predictable Landmarks for Multi-label Learning},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_45},
doi = {10.1007/978-3-030-58545-7_45},
abstract = {Although significant progress achieved, multi-label classification is still challenging due to the complexity of correlations among different labels. Furthermore, modeling the relationships between input and some (dull) classes further increases the difficulty of accurately predicting all possible labels. In this work, we propose to select a small subset of labels as landmarks which are easy to predict according to input (predictable) and can well recover the other possible labels (representative). Different from existing methods which separate the landmark selection and landmark prediction in the 2-step manner, the proposed algorithm, termed Selecting Predictable Landmarks for Multi-Label Learning (SPL-MLL), jointly conducts landmark selection, landmark prediction, and label recovery in a unified framework, to ensure both the representativeness and predictableness for selected landmarks. We employ the Alternating Direction Method (ADM) to solve our problem. Empirical studies on real-world datasets show that our method achieves superior classification performance over other state-of-the-art methods.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {783–799},
numpages = {17},
keywords = {Multi-label learning, Predictable landmarks, A unified framework},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s11219-018-9430-x,
author = {Gergely, Tam\'{a}s and Balogh, Gergo? and Horv\'{a}th, Ferenc and Vancsics, B\'{e}la and Besz\'{e}des, \'{A}rp\'{a}d and Gyim\'{o}thy, Tibor},
title = {Differences between a static and a dynamic test-to-code traceability recovery method},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9430-x},
doi = {10.1007/s11219-018-9430-x},
abstract = {Recovering test-to-code traceability links may be required in virtually every phase of development. This task might seem simple for unit tests thanks to two fundamental unit testing guidelines: isolation (unit tests should exercise only a single unit) and separation (they should be placed next to this unit). However, practice shows that recovery may be challenging because the guidelines typically cannot be fully followed. Furthermore, previous works have already demonstrated that fully automatic test-to-code traceability recovery for unit tests is virtually impossible in a general case. In this work, we propose a semi-automatic method for this task, which is based on computing traceability links using static and dynamic approaches, comparing their results and presenting the discrepancies to the user, who will determine the final traceability links based on the differences and contextual information. We define a set of discrepancy patterns, which can help the user in this task. Additional outcomes of analyzing the discrepancies are structural unit testing issues and related refactoring suggestions. For the static test-to-code traceability, we rely on the physical code structure, while for the dynamic, we use code coverage information. In both cases, we compute combined test and code clusters which represent sets of mutually traceable elements. We also present an empirical study of the method involving 8 non-trivial open source Java systems.},
journal = {Software Quality Journal},
month = jun,
pages = {797–822},
numpages = {26},
keywords = {Unit testing, Traceability link recovery, Test-to-code traceability, Structural test smells, Refactoring, Code coverage}
}

@article{10.1016/j.procs.2019.09.156,
author = {Czibula, Gabriela and Mihai, Andrei and Crivei, Liana Maria},
title = {S PRAR: A novel relational association rule mining classification model applied for academic performance prediction},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {159},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.09.156},
doi = {10.1016/j.procs.2019.09.156},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {20–29},
numpages = {10},
keywords = {68P15, Relational association rules 2000 MSC: 68T05, Supervised learning, Students’ performance prediction, Educational data mining}
}

@article{10.1155/2016/7658207,
author = {Tomar, Divya and Agarwal, Sonali},
title = {Prediction of defective software modules using class imbalance learning},
year = {2016},
issue_date = {January 2016},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2016},
issn = {1687-9724},
url = {https://doi.org/10.1155/2016/7658207},
doi = {10.1155/2016/7658207},
abstract = {Software defect predictors are useful to maintain the high quality of software products effectively. The early prediction of defective software modules can help the software developers to allocate the available resources to deliver high quality software products. The objective of software defect prediction system is to find as many defective software modules as possible without affecting the overall performance. The learning process of a software defect predictor is difficult due to the imbalanced distribution of software modules between defective and nondefective classes. Misclassification cost of defective software modules generally incurs much higher cost than the misclassification of nondefective one. Therefore, on considering the misclassification cost issue, we have developed a software defect prediction system using Weighted Least Squares Twin Support Vector Machine (WLSTSVM). This system assigns higher misclassification cost to the data samples of defective classes and lower cost to the data samples of nondefective classes. The experiments on eight software defect prediction datasets have proved the validity of the proposed defect prediction system. The significance of the results has been tested via statistical analysis performed by using nonparametric Wilcoxon signed rank test.},
journal = {Appl. Comp. Intell. Soft Comput.},
month = jan,
articleno = {6},
numpages = {1}
}

@article{10.1007/s11219-021-09555-0,
author = {Siavvas, Miltiadis and Kehagias, Dionysios and Tzovaras, Dimitrios and Gelenbe, Erol},
title = {A hierarchical model for quantifying software security based on static analysis alerts and software metrics},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09555-0},
doi = {10.1007/s11219-021-09555-0},
abstract = {Despite the acknowledged importance of quantitative security assessment in secure software development, current literature still lacks an efficient model for measuring internal software security risk. To this end, in this paper, we introduce a hierarchical security assessment model (SAM), able to assess the internal security level of software products based on low-level indicators, i.e., security-relevant static analysis alerts and software metrics. The model, following the guidelines of ISO/IEC 25010, and based on a set of thresholds and weights, systematically aggregates these low-level indicators in order to produce a high-level security score that reflects the internal security level of the analyzed software. The proposed model is practical, since it is fully automated and operationalized in the form of a standalone tool and as part of a broader Computer-Aided Software Engineering (CASE) platform. In order to enhance its reliability, the thresholds of the model were calibrated based on a repository of 100 popular software applications retrieved from Maven Repository. Furthermore, its weights were elicited in a way to chiefly reflect the knowledge expressed by the Common Weakness Enumeration (CWE), through a novel weights elicitation approach grounded on popular decision-making techniques. The proposed model was evaluated on a large repository of 150 open-source software applications retrieved from GitHub and 1200 classes retrieved from the OWASP Benchmark. The results of the experiments revealed the capacity of the proposed model to reliably assess internal security at both product level and class level of granularity, with sufficient discretion power. They also provide preliminary evidence for the ability of the model to be used as the basis for vulnerability prediction. To the best of our knowledge, this is the first fully automated, operationalized and sufficiently evaluated security assessment model in the modern literature.},
journal = {Software Quality Journal},
month = jun,
pages = {431–507},
numpages = {77},
keywords = {Security Assessment, Software Quality Evaluation, Software Security}
}

@inproceedings{10.1109/ICNC.2007.347,
author = {Yang, Bo and Yao, Lan and Huang, Hong-Zhong},
title = {Early Software Quality Prediction Based on a Fuzzy Neural Network Model},
year = {2007},
isbn = {0769528759},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICNC.2007.347},
doi = {10.1109/ICNC.2007.347},
abstract = {For the management of a software development project, a software quality prediction model is very helpful since it can provide the management with useful information needed for decision-makings. Many software quality prediction models and techniques have been proposed and studied in the literature. Nevertheless, the complicated situations of software development process call for a more flexible model that can cater for all factors that have impact on the quality of the target software, including the characteristics of the software product, the characteristics of the development process and the operation conditions. In this paper, a software quality prediction model based on a fuzzy neural network is presented. The proposed model is a hybrid model of Artificial Neural Network (ANN) and Fuzzy Logic (FL), which exploits the advantages of ANN and FL while eliminating their limitations. The model exhibits some favorable features such as being able to deal with objective data collected in the software development process as well as knowledge/experiences obtained from experts or from similar projects, which is the main information that is available in the early phases of a software development process. Using this model, early prediction of software quality becomes feasible and the management can have knowledge of the quality of target software product as early as possible, which helps to identify design errors and avoid expensive rework. Experimental results show that for problems of small-to-medium scale, the proposed model can be easily trained and thus can be of practical use.},
booktitle = {Proceedings of the Third International Conference on Natural Computation - Volume 01},
pages = {760–764},
numpages = {5},
series = {ICNC '07}
}

@inproceedings{10.1109/ICSE.2019.00069,
author = {Cui, Di and Liu, Ting and Cai, Yuanfang and Zheng, Qinghua and Feng, Qiong and Jin, Wuxia and Guo, Jiaqi and Qu, Yu},
title = {Investigating the impact of multiple dependency structures on software defects},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00069},
doi = {10.1109/ICSE.2019.00069},
abstract = {Over the past decades, numerous approaches were proposed to help practitioner to predict or locate defective files. These techniques often use syntactic dependency, history co-change relation, or semantic similarity. The problem is that, it remains unclear whether these different dependency relations will present similar accuracy in terms of defect prediction and localization. In this paper, we present our systematic investigation of this question from the perspective of software architecture. Considering files involved in each dependency type as an individual design space, we model such a design space using one DRSpace. We derived 3 DRSpaces for each of the 117 Apache open source projects, with 643,079 revision commits and 101,364 bug reports in total, and calculated their interactions with defective files. The experiment results are surprising: the three dependency types present significantly different architectural views, and their interactions with defective files are also drastically different. Intuitively, they play completely different roles when used for defect prediction/localization. The good news is that the combination of these structures has the potential to improve the accuracy of defect prediction/localization. In summary, our work provides a new perspective regarding to which type(s) of relations should be used for the task of defect prediction/localization. These quantitative and qualitative results also advance our knowledge of the relationship between software quality and architectural views formed using different dependency types.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {584–595},
numpages = {12},
keywords = {software structure, software quality, software maintenance},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.5555/3455716.3455773,
author = {Ma, Fan and Meng, Deyu and Dong, Xuanyi and Yang, Yi},
title = {Self-paced multi-view co-training},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a "draw without replacement" strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {57},
numpages = {38},
keywords = {probably approximately correct learnable, ε-expansion theory, semi-supervised learning, multi-view learning, self-paced learning, co-training}
}

@inproceedings{10.1145/3172871.3172872,
author = {Kumar, Lov and Sureka, Ashish},
title = {Feature Selection Techniques to Counter Class Imbalance Problem for Aging Related Bug Prediction: Aging Related Bug Prediction},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172872},
doi = {10.1145/3172871.3172872},
abstract = {Aging-Related Bugs (ARBs) occur in long running systems due to error conditions caused because of accumulation of problems such as memory leakage or unreleased files and locks. Aging-Related Bugs are hard to discover during software testing and also challenging to replicate. Automatic identification and prediction of aging related fault-prone files and classes in an object oriented system can help the software quality assurance team to optimize their testing efforts. In this paper, we present a study on the application of static source code metrics and machine learning techniques to predict aging related bugs. We conduct a series of experiments on publicly available dataset from two large open-source software systems: Linux and MySQL. Class imbalance and high dimensionality are the two main technical challenges in building effective predictors for aging related bugs.We investigate the application of five different feature selection techniques (OneR, Information Gain, Gain Ratio, RELEIF and Symmetric Uncertainty) for dimensionality reduction and five different strategies (Random Under-sampling, Random Oversampling, SMOTE, SMOTEBoost and RUSBoost) to counter the effect of class imbalance in our proposed machine learning based solution approach. Experimental results reveal that the random under-sampling approach performs best followed by RUSBoost in-terms of the mean AUC metric. Statistical significance test demonstrates that there is a significant difference between the performance of the various feature selection techniques. Experimental results shows that Gain Ratio and RELEIF performs best in comparison to other strategies to address the class imbalance problem. We infer from the statistical significance test that there is no difference between the performances of the five different learning algorithms.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {2},
numpages = {11},
keywords = {Source Code Metrics, Software Maintenance, Predictive Modeling, Machine Learning, Imbalance Learning, Feature Selection Techniques, Empirical Software Engineering, Aging Related Bugs},
location = {Hyderabad, India},
series = {ISEC '18}
}

@inproceedings{10.1007/978-3-030-87589-3_19,
author = {Yan, Yutong and Conze, Pierre-Henri and Lamard, Mathieu and Zhang, Heng and Quellec, Gwenol\'{e} and Cochener, B\'{e}atrice and Coatrieux, Gouenou},
title = {Deep Active Learning for Dual-View Mammogram Analysis},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_19},
doi = {10.1007/978-3-030-87589-3_19},
abstract = {Supervised deep learning on medical imaging requires massive manual annotations, which are expertise-needed and time-consuming to perform. Active learning aims at reducing annotation efforts by adaptively selecting the most informative samples for labeling. We propose in this paper a novel deep active learning approach for dual-view mammogram analysis, especially for breast mass segmentation and detection, where the necessity of labeling is estimated by exploiting the consistency of predictions arising from craniocaudal (CC) and mediolateral-oblique (MLO) views. Intuitively, if mass segmentation or detection is robustly performed, prediction results achieved on CC and MLO views should be consistent. Exploiting the inter-view consistency is hence a good way to guide the sampling mechanism which iteratively selects the next image pairs to be labeled by an oracle. Experiments on public DDSM-CBIS and INbreast datasets demonstrate that comparable performance with respect to fully-supervised models can be reached using only 6.83% (9.56%) of labeled data for segmentation (detection). This suggests that combining dual-view mammogram analysis and active learning can strongly contribute to the development of computer-aided diagnosis systems.},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {180–189},
numpages = {10},
keywords = {Computer-aided diagnosis, Active learning, Dual-view mammogram analysis, Mass detection, Mass segmentation, Breast cancer},
location = {Strasbourg, France}
}

@article{10.1016/j.infsof.2010.05.009,
author = {Oliveira, Adriano L. I. and Braga, Petronio L. and Lima, Ricardo M. F. and Corn\'{e}lio, M\'{a}rcio L.},
title = {GA-based method for feature selection and parameters optimization for machine learning regression applied to software effort estimation},
year = {2010},
issue_date = {November, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.05.009},
doi = {10.1016/j.infsof.2010.05.009},
abstract = {Context: In software industry, project managers usually rely on their previous experience to estimate the number men/hours required for each software project. The accuracy of such estimates is a key factor for the efficient application of human resources. Machine learning techniques such as radial basis function (RBF) neural networks, multi-layer perceptron (MLP) neural networks, support vector regression (SVR), bagging predictors and regression-based trees have recently been applied for estimating software development effort. Some works have demonstrated that the level of accuracy in software effort estimates strongly depends on the values of the parameters of these methods. In addition, it has been shown that the selection of the input features may also have an important influence on estimation accuracy. Objective: This paper proposes and investigates the use of a genetic algorithm method for simultaneously (1) select an optimal input feature subset and (2) optimize the parameters of machine learning methods, aiming at a higher accuracy level for the software effort estimates. Method: Simulations are carried out using six benchmark data sets of software projects, namely, Desharnais, NASA, COCOMO, Albrecht, Kemerer and Koten and Gray. The results are compared to those obtained by methods proposed in the literature using neural networks, support vector machines, multiple additive regression trees, bagging, and Bayesian statistical models. Results: In all data sets, the simulations have shown that the proposed GA-based method was able to improve the performance of the machine learning methods. The simulations have also demonstrated that the proposed method outperforms some recent methods reported in the recent literature for software effort estimation. Furthermore, the use of GA for feature selection considerably reduced the number of input features for five of the data sets used in our analysis. Conclusions: The combination of input features selection and parameters optimization of machine learning methods improves the accuracy of software development effort. In addition, this reduces model complexity, which may help understanding the relevance of each input feature. Therefore, some input parameters can be ignored without loss of accuracy in the estimations.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1155–1166},
numpages = {12},
keywords = {Support vector regression, Software effort estimation, Regression, Genetic algorithms, Feature selection}
}

@phdthesis{10.5555/1087226,
author = {Guo, Lan and Cukic, Bojan},
title = {Software quality and reliability prediction using dempster-shafer theory},
year = {2004},
isbn = {0496171690},
publisher = {West Virginia University},
address = {USA},
abstract = {As software systems are increasingly deployed in mission critical applications, accurate quality and reliability predictions are becoming a necessity. Most accurate prediction models require extensive testing effort, implying increased cost and slowing down the development life cycle. We developed two novel statistical models based on Dempster-Shafer theory, which provide accurate predictions from relatively small data sets of direct and indirect software reliability and quality predictors. The models are flexible enough to incorporate information generated throughout the development life-cycle to improve the prediction accuracy. Our first contribution is an original algorithm for building Dempster-Shafer Belief Networks using prediction logic. This model has been applied to software quality prediction. We demonstrated that the prediction accuracy of Dempster-Shafer Belief Networks is higher than that achieved by logistic regression, discriminant analysis, random forests, as well as the algorithms in two machine learning software packages, See5 and WEKA. The difference in the performance of the Dempster-Shafer Belief Networks over the other methods is statistically significant. Our second contribution is also based on a practical extension of Dempster-Shafer theory. The major limitation of the Dempsters rule and other known rules of evidence combination is the inability to handle information coming from correlated sources. Motivated by inherently high correlations between early life-cycle predictors of software reliability, we extended Murphy's rule of combination to account for these correlations. When used as a part of the methodology that fuses various software reliability prediction systems, this rule provided more accurate predictions than previously reported methods. In addition, we proposed an algorithm, which defines the upper and lower bounds of the belief function of the combination results. To demonstrate its generality, we successfully applied it in the design of the Online Safety Monitor, which fuses multiple correlated time varying estimations of convergence of neural network learning in an intelligent flight control system.},
note = {AAI3156448}
}

@article{10.1007/s10515-013-0129-8,
author = {Bowes, David and Hall, Tracy and Gray, David},
title = {DConfusion: a technique to allow cross study performance evaluation of fault prediction studies},
year = {2014},
issue_date = {April     2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-013-0129-8},
doi = {10.1007/s10515-013-0129-8},
abstract = {There are many hundreds of fault prediction models published in the literature. The predictive performance of these models is often reported using a variety of different measures. Most performance measures are not directly comparable. This lack of comparability means that it is often difficult to evaluate the performance of one model against another. Our aim is to present an approach that allows other researchers and practitioners to transform many performance measures back into a confusion matrix. Once performance is expressed in a confusion matrix alternative preferred performance measures can then be derived. Our approach has enabled us to compare the performance of 600 models published in 42 studies. We demonstrate the application of our approach on 8 case studies, and discuss the advantages and implications of doing this.},
journal = {Automated Software Engg.},
month = apr,
pages = {287–313},
numpages = {27},
keywords = {Machine learning, Fault, Confusion matrix}
}

@inproceedings{10.1145/3345629.3345635,
author = {Wang, Song and Bansal, Chetan and Nagappan, Nachiappan and Philip, Adithya Abraham},
title = {Leveraging Change Intents for Characterizing and Identifying Large-Review-Effort Changes},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345635},
doi = {10.1145/3345629.3345635},
abstract = {Code changes to software occur due to various reasons such as bug fixing, new feature addition, and code refactoring. In most existing studies, the intent of the change is rarely leveraged to provide more specific, context aware analysis.In this paper, we present the first study to leverage change intent to characterize and identify Large-Review-Effort (LRE) changes regarding review effort---changes with large review effort. Specifically, we first propose a feedback-driven and heuristics-based approach to obtain change intents. We then characterize the changes regarding review effort by using various features extracted from change metadata and the change intents. We further explore the feasibility of automatically classifying LRE changes. We conduct our study on a large-scale project from Microsoft and three large-scale open source projects, i.e., Qt, Android, and OpenStack. Our results show that, (i) code changes with some intents are more likely to be LRE changes, (ii) machine learning based prediction models can efficiently help identify LRE changes, and (iii) prediction models built for code changes with some intents achieve better performance than prediction models without considering the change intent, the improvement in AUC can be up to 19 percentage points and is 7.4 percentage points on average. The tool developed in this study has already been used in Microsoft to provide the review effort and intent information of changes for reviewers to accelerate the review process.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {46–55},
numpages = {10},
keywords = {Code review, change intent, machine learning, review effort},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@article{10.1007/s11219-020-09538-7,
author = {Eken, Beyza and Palma, Francis and Ay\c{s}e, Ba\c{s}ar and Ay\c{s}e, Tosun},
title = {An empirical study on the effect of community smells on bug prediction},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09538-7},
doi = {10.1007/s11219-020-09538-7},
abstract = {Community-aware metrics through socio-technical developer networks or organizational structures have already been studied in the&nbsp;software bug prediction field. Community smells are also proposed to identify communication and collaboration patterns in developer communities. Prior work reports a statistical association between community smells and code smells identified in software modules. We investigate the contribution of community smells on predicting bug-prone classes and compare their contribution with that of code smell-related information and state-of-the-art process metrics. We conduct our empirical analysis on ten open-source projects with varying sizes, buggy and smelly class ratios. We build seven different bug prediction models to answer three RQs: a baseline model including a state-of-the-art metric set used, three models incorporating a particular metric set, namely community smells, code smells, code smell intensity, into the baseline, and three models incorporating a combination of smell-related metrics into the baseline. The performance of these models is reported in terms of recall, false positive rates, F-measure and AUC and statistically compared using Scott–Knott ESD tests. Community smells improve the prediction performance of a baseline model by up to 3% in terms of AUC, while code smell intensity improves the baseline models by up to 40% in terms of F-measure and up to 17% in terms of AUC. The conclusions are significantly influenced by the validation strategies used, algorithms and the selected projects’ data characteristics. While the code smell intensity metric captures the most information about technical flaws in predicting bug-prone classes, the community smells also contribute to bug prediction models by revealing communication and collaboration flaws in software development teams. Future research is needed to capture the communication patterns through multiple channels and to understand whether socio-technical flaws could be used in a cross-project bug prediction setting.},
journal = {Software Quality Journal},
month = mar,
pages = {159–194},
numpages = {36},
keywords = {Mining software repositories, Bug prediction, Community smells}
}

@article{10.1007/s00521-020-05016-0,
author = {Naz, Samina and Majeed, Hammad and Khan, Farrukh Aslam},
title = {Memory augmented hyper-heuristic framework to solve multi-disciplinary problems inspired by cognitive problem solving skills},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {4},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05016-0},
doi = {10.1007/s00521-020-05016-0},
abstract = {This paper proposes a new framework, named Deja-Vu+, which is an extension of Deja Vu framework, a classic study on hyper-heuristic framework with 2R (Record and Recall) modules. Deja-Vu+ has the ability to handle two other domains, namely regression and unsupervised learning. The extension examines the strength of Deja-Vu+ for solving regression and unsupervised learning tasks. The regression problems are treated here as multiclass classification tasks, and unsupervised learning tasks are considered as clustering problems. The proposed framework is tested on a number of regression and unsupervised learning benchmark problems and has shown promising results to handle regression as classification. The framework attains an overall average accuracy of 70% for regression and clustering data sets. Deja-Vu+ is knowledge-rich hyper-heuristic framework, which is capable enough to transfer knowledge successfully. This knowledge transfer improves the performance of learning by avoiding the extensive heuristic search process. Our experimental results show that using previously attained knowledge to reduce the&nbsp;computational effort is beneficial in solving multi-disciplinary machine learning problems.},
journal = {Neural Comput. Appl.},
month = feb,
pages = {1367–1378},
numpages = {12},
keywords = {Deja Vu, Hyper-heuristic framework, K-means clustering, Discretization, Unsupervised learning, Regression, Transfer learning}
}

@inproceedings{10.1109/ICTAI.2006.77,
author = {Zhang, Du},
title = {Machine Learning in Value-Based Software Test Data Generation},
year = {2006},
isbn = {0769527280},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2006.77},
doi = {10.1109/ICTAI.2006.77},
abstract = {Software engineering research and practice thus far are primarily conducted in a value-neutral setting where each artifact in software development such as requirement, use case, test case, and defect, is treated as equally important during a software system development process. There are a number of shortcomings of such value-neutral software engineering. Value-based software engineering is to integrate value considerations into the full range of existing and emerging software engineering principles and practices. Machine learning has been playing an increasingly important role in helping develop and maintain large and complex software systems. However, machine learning applications to software engineering have been largely confined to the value-neutral software engineering setting. In this paper, we advocate a shift to applying machine learning methods to value-based software engineering. We propose a framework for value-based software test data generation. The proposed framework incorporates some general principles in value-based software testing and can help improve return on investment.},
booktitle = {Proceedings of the 18th IEEE International Conference on Tools with Artificial Intelligence},
pages = {732–736},
numpages = {5},
keywords = {value-based software test data generation, value-based software engineering, genetic algorithms.},
series = {ICTAI '06}
}

@article{10.1016/j.knosys.2016.12.017,
author = {Rathore, Santosh Singh and Kumar, Sandeep},
title = {Linear and non-linear heterogeneous ensemble methods to predict the number of faults in software systems},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {119},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.12.017},
doi = {10.1016/j.knosys.2016.12.017},
abstract = {This paper expands the use of ensemble methods for the prediction of number of faults unlikely the earlier works on ensemble methods that focused on predicting software modules as faulty or non-faulty.This paper investigates the usage of both heterogeneous ensemble methods as well as homogeneous ensemble methods for the prediction of number of faults.We present two linear combination rules and two non-linear combination rules for combining the outputs of the base learners in the ensemble.In addition, we assess the performance of ensemble methods under two different scenarios, intra-release prediction and inter-releases prediction.The experiments are performed over five open-source software systems with their fifteen releases, collected from the PROMISE data repository. Several classification techniques have been investigated and evaluated earlier for the software fault prediction. These techniques have produced different prediction accuracy for the different software systems and none of the technique has always performed consistently better across different domains. On the other hand, software fault prediction using ensemble methods can be very effective, as they take the advantage of each participating technique for the given dataset and try to come up with better prediction results compared to the individual techniques. Many works are available for classifying software modules being faulty or non-faulty using the ensemble methods. These works are only specifying that whether a given software module is faulty or not, but number of faults in that module are not predicted by them. The use of ensemble methods for the prediction of number of faults has not been explored so far. To fulfill this gap, this paper presents ensemble methods for the prediction of number of faults in the given software modules. The experimental study is designed and conducted for five open-source software projects with their fifteen releases, collected from the PROMISE data repository. The results are evaluated under two different scenarios, intra-release prediction and inter-releases prediction. The prediction accuracy of ensemble methods is evaluated using absolute error, relative error, prediction at level l, and measure of completeness performance measures. Results show that the presented ensemble methods yield improved prediction accuracy over the individual fault prediction techniques under consideration. Further, the results are consistent for all the used datasets. The evidences obtained from the prediction at level l and measure of completeness analysis have also confirmed the effectiveness of the proposed ensemble methods for predicting the number of faults.},
journal = {Know.-Based Syst.},
month = mar,
pages = {232–256},
numpages = {25},
keywords = {Software fault prediction, Prediction of number of faults, Heterogeneous ensemble, Ensemble methods}
}

@inproceedings{10.5555/3495724.3496169,
author = {Parvaneh, Amin and Abbasnejad, Ehsan and Teney, Damien and Shi, Javen Qinfeng and van den Hengel, Anton},
title = {Counterfactual vision-and-language navigation: unravelling the unseen},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The task of vision-and-language navigation (VLN) requires an agent to follow text instructions to find its way through simulated household environments. A prominent challenge is to train an agent capable of generalising to new environments at test time, rather than one that simply memorises trajectories and visual details observed during training. We propose a new learning strategy that learns both from observations and generated counterfactual environments. We describe an effective algorithm to generate counterfactual observations on the fly for VLN, as linear combinations of existing environments. Simultaneously, we encourage the agent's actions to remain stable between original and counterfactual environments through our novel training objective – effectively removing spurious features that would otherwise bias the agent. Our experiments show that this technique provides significant improvements in generalisation on benchmarks for Room-to-Room navigation and Embodied Question Answering.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {445},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.1145/3460319.3464840,
author = {Pan, Cong and Pradel, Michael},
title = {Continuous test suite failure prediction},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464840},
doi = {10.1145/3460319.3464840},
abstract = {Continuous integration advocates to run the test suite of a project frequently, e.g., for every code change committed to a shared repository. This process imposes a high computational cost and sometimes also a high human cost, e.g., when developers must wait for the test suite to pass before a change appears in the main branch of the shared repository. However, only 4% of all test suite invocations turn a previously passing test suite into a failing test suite. The question arises whether running the test suite for each code change is really necessary. This paper presents continuous test suite failure prediction, which reduces the cost of continuous integration by predicting whether a particular code change should trigger the test suite at all. The core of the approach is a machine learning model based on features of the code change, the test suite, and the development history. We also present a theoretical cost model that describes when continuous test suite failure prediction is worthwhile. Evaluating the idea with 15k test suite runs from 242 open-source projects shows that the approach is effective at predicting whether running the test suite is likely to reveal a test failure. Moreover, we find that our approach improves the AUC over baselines that use features proposed for just-in-time defect prediction and test case failure prediction by 13.9% and 2.9%, respectively. Overall, continuous test suite failure prediction can significantly reduce the cost of continuous integration.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {553–565},
numpages = {13},
keywords = {machine learning, cost model, continuous test suite failure prediction, continuous integration},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1007/s11219-019-09467-0,
author = {Du, Xiaoting and Zhou, Zenghui and Yin, Beibei and Xiao, Guanping},
title = {Cross-project bug type prediction based on transfer learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09467-0},
doi = {10.1007/s11219-019-09467-0},
abstract = {The prediction of bug types provides useful insights into the software maintenance process. It can improve the efficiency of software testing and help developers adopt corresponding strategies to fix bugs before releasing software projects. Typically, the prediction tasks are performed through machine learning classifiers, which rely heavily on labeled data. However, for a software project that has insufficient labeled data, it is difficult to train the classification model for predicting bug types. Although labeled data of other projects can be used as training data, the results of the cross-project prediction are often poor. To solve this problem, this paper proposes a cross-project bug type prediction framework based on transfer learning. Transfer learning breaks the assumption of traditional machine learning methods that the training set and the test set should follow the same distribution. Our experiments show that the results of cross-project bug type prediction have significant improvement by adopting transfer learning. In addition, we have studied the factors that influence the prediction results, including different pairs of source and target projects, and the number of bug reports in the source project.},
journal = {Software Quality Journal},
month = mar,
pages = {39–57},
numpages = {19},
keywords = {Transfer learning, Bug report, Cross-project, Bug prediction}
}

@article{10.1007/s11219-018-9439-1,
author = {K\i{}ra\c{c}, M. Furkan and Aktemur, Bar\i{}\c{s} and S\"{o}zer, Hasan and Gebizli, Ceren \c{S}ahin},
title = {Automatically learning usage behavior and generating event sequences for black-box testing of reactive systems},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9439-1},
doi = {10.1007/s11219-018-9439-1},
abstract = {We propose a novel technique based on recurrent artificial neural networks to generate test cases for black-box testing of reactive systems. We combine functional testing inputs that are automatically generated from a model together with manually-applied test cases for robustness testing. We use this combination to train a long short-term memory (LSTM) network. As a result, the network learns an implicit representation of the usage behavior that is liable to failures. We use this network to generate new event sequences as test cases. We applied our approach in the context of an industrial case study for the black-box testing of a digital TV system. LSTM-generated test cases were able to reveal several faults, including critical ones, that were not detected with existing automated or manual testing activities. Our approach is complementary to model-based and exploratory testing, and the combined approach outperforms random testing in terms of both fault coverage and execution time.},
journal = {Software Quality Journal},
month = jun,
pages = {861–883},
numpages = {23},
keywords = {Test case generation, Recurrent neural networks, Long short-term memory networks, Learning usage behavior, Black-box testing}
}

@inproceedings{10.1145/1540438.1540443,
author = {Fu, Yu and Koru, A. G\"{u}ne\c{s} and Chen, Zhiyuan and El Emam, Khaled},
title = {A tree-based approach to preserve the privacy of software engineering data and predictive models},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540443},
doi = {10.1145/1540438.1540443},
abstract = {In empirical disciplines, data sharing leads to verifiable research and facilitates future research studies. Recent efforts of the PROMISE community contributed to data sharing and reproducible research in software engineering. However, an important portion of data used in empirical software engineering research still remains classified. This situation is unlikely to change because many companies, governments, and defense organizations will be always hesitant to share their project data such as, effort and defect data, due to various confidentiality, privacy, and security concerns. In this paper, we present, demonstrate, and evaluate a novel tree-based data perturbation approach. This approach does not only preserve privacy effectively, but it also preserves the predictive patterns in the original data set. Consequently, the empirical software engineering researchers will have access to another category of data sets, transformed data sets, which will increase the verifiability of research results and facilitate the future research studies in this area. Our approach can be immediately useful to many researchers and organizations who are willing to share their software engineering data but cannot do so due to privacy concerns.},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {3},
numpages = {12},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@inproceedings{10.1145/3468264.3478690,
author = {Nagappan, Nachiappan},
title = {The 4ps: product, process, people, and productivity: a data-driven approach to improve software engineering (keynote)},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3478690},
doi = {10.1145/3468264.3478690},
abstract = {In this talk I will provide a broad overview on developer productivity and dive deep into specific analysis related to how product, process and the people impact productivity. I will use examples from industry on effort estimation and defect prediction in product, distributed development in process and the ramp up of new employees in the people category. The talk will also cover interventions via tools and process changes and their impact and discuss future challenges. This talk will be based on previously published work.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {3},
numpages = {1},
keywords = {Empirical Software Engineering, Effort Estimation, Distributed Development, Developer Productivity, Defect Prediction},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1007/s11219-021-09547-0,
author = {Azzeh, Mohammad and Nassif, Ali Bou and Mart\'{\i}n, Cuauht\'{e}moc L\'{o}pez},
title = {Empirical analysis on productivity prediction and locality for use case points method},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09547-0},
doi = {10.1007/s11219-021-09547-0},
abstract = {Use case points (UCP) method has been around for over two decades. Although there was a substantial criticism concerning the algebraic construction and factor assessment of UCP, it remains an efficient early size estimation method. Predicting software effort from UCP is still an ever-present challenge. The earlier version of UCP method suggested using productivity as a cost driver, where fixed or a few pre-defined productivity ratios have been widely agreed. While this approach was successful when not enough historical data is available, it is no longer acceptable because software projects are different in terms of development aspects. Therefore, it is better to understand the relationship between productivity and other UCP variables. This paper examines the impact of data locality approaches on productivity and effort prediction from multiple UCP variables. The environmental factors are used as partitioning factors to produce local homogeneous data either based on their influential levels or using clustering algorithms. Different machine learning methods, including solo and ensemble methods, are used to construct productivity and effort prediction models based on the local data. The results demonstrate that the prediction models that are created based on local data surpass models that use entire data. Also, the results show that conforming to&nbsp;the hypothetical assumption between productivity and environmental factors is not necessarily a requirement for the&nbsp;success of locality.},
journal = {Software Quality Journal},
month = jun,
pages = {309–336},
numpages = {28},
keywords = {Data locality, Effort estimation, Productivity, Use case points}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {variability management, technologies and concepts, product families, microservice platforms, configuration management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3439961.3439987,
author = {J\'{u}nior, Luiz Cavamura and Belgamo, Anderson and Mendon\c{c}a, Vin\'{\i}cius Rafael Lobo de and Vincenzi, Auri Marcelo Rizzo},
title = {WarningsFIX: a Recommendation System for Prioritizing Warnings Generated by Automated Static Analyzers},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439987},
doi = {10.1145/3439961.3439987},
abstract = {Recommendation systems try to guide the users in carrying out a task providing them with useful information about it. Considering the context of software development, programs are ever-increasing, making it difficult to conduct a detailed verification and validation. Automated static analyzers help to detect possible faults on software products earlier and quickly but, in general, the issue maybe a false-positive warning. In this sense, this work presents and evaluates a recommendation system, called WarningsFIX (WFX), which combines several static analyzers aim at: i) Expand the possible fault domain approached by each static analysis tool increasing the range of warnings types covered, allowing the concentration of a higher number of true-positive warnings. ii) Establish different prioritization strategies of warnings aiming at suggesting for reviewers first analyze the ones with a higher chance of being true-positive. WFX organizes the warnings information via treemaps considering four levels of abstraction: program, package, class, and line. The nodes of the treemap on each level may be classified by three different prioritization strategies based on the number of warnings, the number of tools, and the suspicions rate. The use of these strategies enables the reviewer to handle the set of warnings in a coordinated way depending on the cost and time constraint available. We perform a feasibility study to evaluate the WFX effectiveness whose results shown that: i) WFX was able to improve the results obtained from combined static analyzers to 44% of the analyzed programs, concentrating for them a greater number of true-positives. ii) WFX, depending on the adopted prioritization strategy, improved from 67.5% to 55% the ranking of lines with real bugs when compared with the list of warnings provided by the automated static analyzers without the WFX support.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {26},
numpages = {10},
keywords = {warnings prioritization, static analysis, software visualization, recommendation system, WarningsFix},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@inproceedings{10.5555/3524938.3525939,
author = {Yasunaga, Michihiro and Liang, Percy},
title = {Graph-based, self-supervised program repair from diagnostic feedback},
year = {2020},
publisher = {JMLR.org},
abstract = {We consider the problem of learning to repair programs from diagnostic feedback (e.g., compiler error messages). Program repair is challenging for two reasons: First, it requires reasoning and tracking symbols across source code and diagnostic feedback. Second, labeled datasets available for program repair are relatively small. In this work, we propose novel solutions to these two challenges. First, we introduce a program-feedback graph, which connects symbols relevant to program repair in source code and diagnostic feedback, and then apply a graph neural network on top to model the reasoning process. Second, we present a self-supervised learning paradigm for program repair that leverages unlabeled programs available online to create a large amount of extra program repair examples, which we use to pre-train our models. We evaluate our proposed approach on two applications: correcting introductory programming assignments (DeepFix dataset) and correcting the outputs of program synthesis (SPoC dataset). Our final system, DrRepair, significantly outperforms prior work, achieving 68.2% full repair rate on DeepFix (+22.9% over the prior best), and 48.4% synthesis success rate on SPoC (+3.7% over the prior best).},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {1001},
numpages = {10},
series = {ICML'20}
}

@article{10.1016/j.infsof.2021.106653,
author = {Xiao, Xi and Pan, Yuqing and Zhang, Bin and Hu, Guangwu and Li, Qing and Lu, Runiu},
title = {ALBFL: A novel neural ranking model for software fault localization via combining static and dynamic features},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106653},
doi = {10.1016/j.infsof.2021.106653},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {11},
keywords = {Software quality, Learning to rank, Fault localization, Attention mechanism}
}

@article{10.1007/s11219-016-9334-6,
author = {S\"{o}ylemez, Mehmet and Tarhan, Ayca},
title = {Challenges of software process and product quality improvement: catalyzing defect root-cause investigation by process enactment data analysis},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9334-6},
doi = {10.1007/s11219-016-9334-6},
abstract = {It is claimed by software quality management that the quality of a software product is highly influenced by the quality of the software process followed to develop it. Since measurement of the software process is a challenging task, it is frequently the defects in the software product that are used to measure development quality. By extracting semantic information from defect records, practitioners can investigate and address root causes of software defects to improve development process and product quality. Investigating root causes requires effort for a detailed analysis into the components of the development process that originated the software defects, and is therefore encouraged only at higher maturity levels by most known process improvement models such as Capability Maturity Model Integration (CMMI). This practice, however, postpones the benefits that root-cause analysis would bring in gaining process awareness to improve the software development process and product quality in emergent organizations or organizations residing at lower maturity levels (MLs). In this article, we present a method for and results from applying root-cause analysis for software defects recorded in a software-intensive project of a CMMI ML3 certified institute. The suggested method combines process enactment data collection and analysis with Orthogonal Defect Classification which is a known technique in defect root-cause analysis. Prior to and after implementing the method in the study, defect attributes were analyzed and compared in order to understand any improvements in development performance and product quality. The results of the comparison indicate that the suggested method was efficient in the effort it required and effective in improving development performance and product quality. Defect triggers have become more active in identifying software defects in the earlier phases of software development, and the cost of quality due to software defects has decreased in consequence.},
journal = {Software Quality Journal},
month = jun,
pages = {779–807},
numpages = {29},
keywords = {Software process, Software defect, Product quality, Process improvement, Process enactment, Orthogonal defect classification, Causal analysis}
}

@article{10.1016/j.micpro.2020.103538,
author = {Li, Liya},
title = {RETRACTED: Software Reliability Growth Fault Correction Model Based on Machine Learning and Neural Network Algorithm},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {80},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2020.103538},
doi = {10.1016/j.micpro.2020.103538},
journal = {Microprocess. Microsyst.},
month = feb,
numpages = {5}
}

@article{10.1155/2021/6662932,
author = {Gupta, Mansi and Rajnish, Kumar and Bhattacharjee, Vandana and Gou, Jianping},
title = {Impact of Parameter Tuning for Optimizing Deep Neural Network Models for Predicting Software Faults},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/6662932},
doi = {10.1155/2021/6662932},
abstract = {Deep neural network models built by the appropriate design decisions are crucial to obtain the desired classifier performance. This is especially desired when predicting fault proneness of software modules. When correctly identified, this could help in reducing the testing cost by directing the efforts more towards the modules identified to be fault prone. To be able to build an efficient deep neural network model, it is important that the parameters such as number of hidden layers, number of nodes in each layer, and training details such as learning rate and regularization methods be investigated in detail. The objective of this paper is to show the importance of hyperparameter tuning in developing efficient deep neural network models for predicting fault proneness of software modules and to compare the results with other machine learning algorithms. It is shown that the proposed model outperforms the other algorithms in most cases.},
journal = {Sci. Program.},
month = jan,
numpages = {17}
}

@article{10.1007/s11277-019-06238-9,
author = {Padmakumari, P. and Umamakeswari, A.},
title = {Task Failure Prediction using Combine Bagging Ensemble (CBE) Classification in Cloud Workflow},
year = {2019},
issue_date = {Jul 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {107},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-019-06238-9},
doi = {10.1007/s11277-019-06238-9},
abstract = {Scientific applications adopt cloud environment for executing its workflows as tasks. When a task fails, dependency nature of the workflows affects the overall performance of the execution. An efficient failure prediction mechanism is needed to execute the workflow efficiently. This paper proposes a failure prediction method which is implemented using various machine learning classifiers. Among different classifiers, Na\"{\i}ve Bayes predicts the failure with the highest accuracy of 94.4%. Further, to improve the accuracy of prediction, a novel ensemble method called combine bagging ensemble is introduced and acquires overall accuracy as 95.8%. The validation of proposed method is carried out by comparing simulation and real-time cloud testbed.},
journal = {Wirel. Pers. Commun.},
month = jul,
pages = {23–40},
numpages = {18},
keywords = {Task failure, Scientific workflow, Machine learning, Fault prediction, Ensemble, Cloud computing}
}

@article{10.1002/widm.38,
author = {Seliya, Naeem and Khoshgoftaar, Taghi M.},
title = {The use of decision trees for cost-sensitive classification: an empirical study in software quality prediction},
year = {2011},
issue_date = {September 2011},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {1},
number = {5},
url = {https://doi.org/10.1002/widm.38},
doi = {10.1002/widm.38},
abstract = {This empirical study investigates two commonly used decision tree classification algorithms in the context of cost-sensitive learning. A review of the literature shows that the cost-based performance of a software quality prediction model is usually determined after the model-training process has been completed. In contrast, we incorporate cost-sensitive learning during the model-training process. The C4.5 and Random Forest decision tree algorithms are used to build defect predictors either with, or without, any cost-sensitive learning technique. The paper investigates six different cost-sensitive learning techniques: AdaCost, Adc2, Csb2, MetaCost, Weighting, and Random Undersampling RUS. The data come from case study include 15 software measurement datasets obtained from several high-assurance systems. In addition, to a unique insight into the cost-based performance of defection prediction models, this study is one of the first to use misclassification cost as a parameter during the model-training process. The practical appeal of this research is that it provides a software quality practitioner with a clear process for how to consider during model training and analyze during model evaluation the cost-based performance of a defect prediction model. RUS is ranked as the best cost-sensitive technique among those considered in this study. © 2011 John Wiley &amp; Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 448-459 DOI: 10.1002/widm.38},
journal = {Wiley Int. Rev. Data Min. and Knowl. Disc.},
month = sep,
pages = {448–459},
numpages = {12}
}

@article{10.1155/2021/5549300,
author = {Museba, Tinofirei and Nelwamondo, Fulufhelo and Ouahada, Khmaies and Yi, Yugen},
title = {ADES: A New Ensemble Diversity-Based Approach for Handling Concept Drift},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/5549300},
doi = {10.1155/2021/5549300},
abstract = {Beyond applying machine learning predictive models to static tasks, a significant corpus of research exists that applies machine learning predictive models to streaming environments that incur concept drift. With the prevalence of streaming real-world applications that are associated with changes in the underlying data distribution, the need for applications that are capable of adapting to evolving and time-varying dynamic environments can be hardly overstated. Dynamic environments are nonstationary and change with time and the target variables to be predicted by the learning algorithm and often evolve with time, a phenomenon known as concept drift. Most work in handling concept drift focuses on updating the prediction model so that it can recover from concept drift while little effort has been dedicated to the formulation of a learning system that is capable of learning different types of drifting concepts at any time with minimum overheads. This work proposes a novel and evolving data stream classifier called Adaptive Diversified Ensemble Selection Classifier (ADES) that significantly optimizes adaptation to different types of concept drifts at any time and improves convergence to new concepts by exploiting different amounts of ensemble diversity. The ADES algorithm generates diverse base classifiers, thereby optimizing the margin distribution to exploit ensemble diversity to formulate an ensemble classifier that generalizes well to unseen instances and provides fast recovery from different types of concept drift. Empirical experiments conducted on both artificial and real-world data streams demonstrate that ADES can adapt to different types of drifts at any given time. The prediction performance of ADES is compared to three other ensemble classifiers designed to handle concept drift using both artificial and real-world data streams. The comparative evaluation performed demonstrated the ability of ADES to handle different types of concept drifts. The experimental results, including statistical test results, indicate comparable performances with other algorithms designed to handle concept drift and prove their significance and effectiveness.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {17}
}

@inproceedings{10.1145/3345629.3345631,
author = {Amit, Idan and Feitelson, Dror G.},
title = {Which Refactoring Reduces Bug Rate?},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345631},
doi = {10.1145/3345629.3345631},
abstract = {We present a methodology to identify refactoring operations that reduce the bug rate in the code. The methodology is based on comparing the bug fixing rate in certain time windows before and after the refactoring. We analyzed 61,331 refactor commits from 1,531 large active GitHub projects. When comparing three-month windows, the bug rate is substantially reduced in 17% of the files of analyzed refactors, compared to 12% of the files in random commits. Within this group, implementing 'todo's provides the most benefits. Certain operations like reuse, upgrade, and using enum and namespaces are also especially beneficial.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {12–15},
numpages = {4},
keywords = {Code quality, machine learning, refactoring},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@inproceedings{10.1145/2184751.2184798,
author = {Rao, G. Subrahmanya Vrk and Diwanji, Vivek and Parthasarathi, Jinka},
title = {Application case study of machine learning techniques towards a fault diagnosis system for a manufacturing plant environment},
year = {2012},
isbn = {9781450311724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2184751.2184798},
doi = {10.1145/2184751.2184798},
abstract = {Fault diagnosis is a vital problem in process engineering. It is the fundamental component of anomalous event management (AEM) which has attracted a lot of attention over recent years. AEM deals with the timely detection, diagnosis and correction of abnormal conditions of faults in a process. Early detection and diagnosis of process faults while the plant is still operating in a controllable region can help avoid anomalous event evolution, improve uptime and reduce efficiency loss. There is a great quantity of literature on process fault diagnosis ranging from analytical methods to artificial intelligence and statistical approaches. From a modeling perspective, there are methods that require accurate process models, semi-quantitative models, or qualitative models. At the other end of the gamut, there are methods that do not assume any form of model information and rely only on historical process data. In this paper we present the performance of few approaches of data driven modeling/machine learning techniques on the simulated data from a distillation column.},
booktitle = {Proceedings of the 6th International Conference on Ubiquitous Information Management and Communication},
articleno = {38},
numpages = {4},
keywords = {support vector machine, process industries, pattern discovery, na\"{\i}ve Bayesian, fault diagnosis, data mining, artificial intelligence},
location = {Kuala Lumpur, Malaysia},
series = {ICUIMC '12}
}

@inproceedings{10.5555/823456.823989,
author = {Schneidewind, Norman F.},
title = {Investigation of Logistic Regression as a Discriminant of Software Quality},
year = {2001},
isbn = {0769510434},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We investigated the possibility that Logistic Regression Functions (LRFs), when used in combination with Boolean Discriminant Functions (BDFs), which we had previously developed, would improve the quality classification ability of BDFs when used alone. This was the case; when the union of a BDF and LRF was used to classify quality, the predicative accuracy of quality and inspection cost was improved over that of using either function alone for the Space Shuttle. Also, the LRFs proved useful for ranking the quality of modules in a build. The significance of these results is that very high quality classification accuracy (1.25% error) can be obtained while reducing the inspection cost incurred in achieving high quality. This is particularly important for safety critical systems. Because the methods are general and not particular to the Shuttle, they could be applied to other domains. A key part of the LRF development was a method for identifying the critical value (i.e. threshold) that could discriminate between high and low quality and at the same time constrain the cost of inspection to a reasonable value.},
booktitle = {Proceedings of the 7th International Symposium on Software Metrics},
pages = {328},
keywords = {Software quality prediction, Logistic Regression Functions, Boolean Discriminant Functions.},
series = {METRICS '01}
}

@article{10.1007/s11219-021-09549-y,
author = {Magalh\~{a}es, Claudio and Mota, Alexandre and Momente, Luis},
title = {UI Test case prioritization on an industrial setting: A search for the best criteria},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09549-y},
doi = {10.1007/s11219-021-09549-y},
abstract = {This work was developed in an industrial setting towards UI regression testing, where we do not have access to source code and the majority of test cases are manually executed (and only part of the regression-based test cases can be executed due to limited resources). Test case prioritization (TCP) is indicated for such a scenario. But characteristic of many TCP techniques is that they rely on source code coverage information, whereas we just have access to test cases, change requests, and their features. Thus, our goal is to investigate which criteria is the most relevant for prioritization. Thus, according to the&nbsp;literature we create an optimization model based on historical data. This model is embedded in a constraint solver designed for optimization. Our optimization function is based on the APFD (Average of the Percentage of Faults Detected) metric, but other metrics can be used as well. We have found that our partner already uses an appropriate criterion to identify failures which is statistically equivalent to other criteria used in experiments using our optimization model.},
journal = {Software Quality Journal},
month = jun,
pages = {381–403},
numpages = {23},
keywords = {Constraint satisfaction, Historical data, UI Test case prioritization}
}

@inproceedings{10.1145/2639490.2639504,
author = {Russo, Barbara},
title = {A proposed method to evaluate and compare fault predictions across studies},
year = {2014},
isbn = {9781450328982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639490.2639504},
doi = {10.1145/2639490.2639504},
abstract = {Studies on fault prediction often pay little attention to empirical rigor and presentation. Researchers might not have full command over the statistical method they use, full understanding of the data they have, or tend not to report key details about their work. What does it happen when we want to compare such studies for building a theory on fault prediction? There are two issues that if not addressed, we believe, prevent building such theory. The first concerns how to compare and report prediction performance across studies on different data sets. The second regards fitting performance of prediction models. Studies tend not to control and report the performance of predictors on historical data underestimating the risk that good predictors may poorly perform on past data. The degree of both fitting and prediction performance determines the risk managers are requested to take when they use such predictors. In this work, we propose a framework to compare studies on categorical fault prediction that aims at addressing the two issues. We propose three algorithms that automate our framework. We finally review baseline studies on fault prediction to discuss the application of the framework.},
booktitle = {Proceedings of the 10th International Conference on Predictive Models in Software Engineering},
pages = {2–11},
numpages = {10},
keywords = {confusion matrix, fault, machine learning, model comparison},
location = {Turin, Italy},
series = {PROMISE '14}
}

@inproceedings{10.1145/3493244.3493245,
author = {Nakamura, Walter Takashi and de Souza, Jos\'{e} Cezar and Teixeira, L\'{\i}gia M\'{a}rcia and Silva, Afonso and da Silva, Rogenis and Gadelha, Bruno and Conte, Tayana},
title = {Requirements Behind Reviews: How do Software Practitioners See App User Reviews to Think of Requirements?},
year = {2021},
isbn = {9781450395533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493244.3493245},
doi = {10.1145/3493244.3493245},
abstract = {The popularity of mobile computing has led to the development of several mobile apps to solve end-users everyday problems. Nowadays, app stores allow users to write reviews reporting their experiences using the apps. Negative experiences can reveal the reasons why users are moving from one app to another and drive app adoption by future users. Thus, software companies should listen to users’ voices to meet their needs and improve the quality of their apps. In this context, Data-Driven Requirements Engineering (DDRE) emerges as an alternative approach to develop and manage requirements from large volumes of data such as user reviews. Most research on DDRE has focused on assessing different machine learning, information retrieval, and natural language processing techniques for automatic requirements mining and classification. However, we need to move forward to understand how to support the DDRE adoption by software practitioners who will apply the paradigm in practice. In this paper, we present a tool-supported approach for data-driven requirements elicitation. It applies text mining techniques to obtain user reviews from app stores and presents the most frequent terms with their associated reviews through a visual representation. To advance the understanding of DDRE adoption, we conducted a feasibility study with industry practitioners to investigate how our approach supports the requirements elicitation process. Our main findings indicated a positive acceptance by practitioners. They perceived it as easy to use and useful to think of requirements while demonstrating an intention to adopt it in the requirements elicitation process.},
booktitle = {Proceedings of the XX Brazilian Symposium on Software Quality},
articleno = {12},
numpages = {9},
keywords = {requirements elicitation, data-driven requirements engineering, app user reviews},
location = {Virtual Event, Brazil},
series = {SBQS '21}
}

@inproceedings{10.1145/3340482.3342745,
author = {Rahman, Md. Abdur and Haque, Md. Ariful and Tawhid, Md. Nurul Ahad and Siddik, Md. Saeed},
title = {Classifying non-functional requirements using RNN variants for quality software development},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342745},
doi = {10.1145/3340482.3342745},
abstract = {Non-Functional Requirements (NFR), a set of quality attributes, required for software architectural design. Which are usually scattered in SRS and must be extracted for quality software development to meet user expectations. Researchers show that functional and non-functional requirements are mixed together within the same SRS, which requires a mammoth effort for distinguishing them. Automatic NFR classification would be a feasible way to characterize those requirements, where several techniques have been recommended e.g. IR, linguistic knowledge, etc. However, conventional supervised machine learning methods suffered for word representation problem and usually required hand-crafted features, which will be overcome by proposed research using RNN variants to categories NFR. The NFR are interrelated and one task happens after another, which is the ideal situation for RNN. In this approach, requirements are processed to eliminate unnecessary contents, which are used to extract features using word2vec to fed as input of RNN variants LSTM and GRU. Performance has been evaluated using PROMISE dataset considering several statistical analyses. Among those models, precision, recall, and f1-score of LSTM validation are 0.973, 0.967 and 0.966 respectively, which is higher over CNN and GRU models. LSTM also correctly classified minimum 60% and maximum 80% unseen requirements. In addition, classification accuracy of LSTM is 6.1% better than GRU, which concluded that RNN variants can lead to better classification results, and LSTM is more suitable for NFR classification from textual requirements.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {25–30},
numpages = {6},
keywords = {RNN, Non Functional Requirements, NLP, Deep Learning},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@inproceedings{10.1145/3318299.3318337,
author = {Zhang, Zongtang and Chen, Zhe and Dai, Weiguo and Cheng, Yusheng},
title = {An Over-sampling Method Based on Margin Theory},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318337},
doi = {10.1145/3318299.3318337},
abstract = {Imbalanced data widely exists in real life, while the traditional classification method usually takes accuracy as the classification criterion, which is not suitable for the classification of imbalanced data. Resampling is an important method to deal with imbalanced data classification. In this paper, a margin based random over-sampling (MRO) method is proposed, and then MROBoost algorithm is proposed by combining the AdaBoost algorithm. Experimental results on the UCI dataset show that the MROBoost algorithm is superior to AdaBoost for imbalanced data classification problem.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {506–510},
numpages = {5},
keywords = {over-sampling, imbalanced data, Machine learning, AdaBoost},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@article{10.1016/j.ins.2021.05.008,
author = {Zhang, Nana and Ying, Shi and Ding, Weiping and Zhu, Kun and Zhu, Dandan},
title = {WGNCS: A robust hybrid cross-version defect model via multi-objective optimization and deep enhanced feature representation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {570},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.05.008},
doi = {10.1016/j.ins.2021.05.008},
journal = {Inf. Sci.},
month = sep,
pages = {545–576},
numpages = {32},
keywords = {Convolutional neural network, Wasserstein GAN with Gradient Penalty, Deep learning techniques, Multi-objective feature selection, Cross-version defect prediction}
}

@article{10.1007/s11219-019-09480-3,
author = {Tjortjis, Christos},
title = {Mining Association Rules from Code (MARC) to support legacy software management},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09480-3},
doi = {10.1007/s11219-019-09480-3},
abstract = {This paper presents a methodology for Mining Association Rules from Code (MARC), aiming at capturing program structure, facilitating system understanding and supporting software management. MARC groups program entities (paragraphs or statements) based on similarities, such as variable use, data types and procedure calls. It comprises three stages: code parsing/analysis, association rule mining and rule grouping. Code is parsed to populate a database with records and respective attributes. Association rules are then extracted from this database and subsequently processed to abstract programs into groups containing interrelated entities. Entities are then grouped together if their attributes participate to common rules. This abstraction is performed at the program level or even the paragraph level, in contrast to other approaches that work at the system level. Groups can then be visualised as collections of interrelated entities. The methodology was evaluated using real-life COBOL programs. Results showed that the methodology facilitates program comprehension by using source code only, where domain knowledge and documentation are either unavailable or unreliable.},
journal = {Software Quality Journal},
month = jun,
pages = {633–662},
numpages = {30},
keywords = {Association rules, Data mining, Software analytics, Program comprehension, Software quality, Software management}
}

@inproceedings{10.1007/978-3-030-92708-0_22,
author = {Youssef, Ayman and Abdelrazek, Mohamed and Karmakar, Chandan and Baig, Zubair},
title = {Tracing Software Exploitation},
year = {2021},
isbn = {978-3-030-92707-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-92708-0_22},
doi = {10.1007/978-3-030-92708-0_22},
abstract = {Current exploit detection techniques are designed based on expert observations, manual analysis, and heuristic-like techniques. Because of the manual process for creating such defences, they are usually limited in the number of exploit techniques that can be detected. Machine Learning-based techniques offer greater promise to detect zero-day exploits. Current research in the use of machine learning for unknown attack detection is limited to intrusion detection and malware analysis, limited research is available for the detection of exploits targeting zero-day vulnerabilities using machine learning methods. These limitations stem from the lack of extensive datasets that are tailored for the problem of software exploitation. In this paper, we introduce a method and toolset for creating exploit traces datasets. Our approach allows capturing full traces of benign software under exploitation and recording of the vulnerable threads within an application, providing a comprehensive view of program execution. We evaluated our method and tools on 13 unique and distinct applications and recorded their traces while they were under attack. Our approach was able to successfully trace 53% of the applications and was able to detect the exploit payloads in 71% of the applications that were successfully traced.},
booktitle = {Network and System Security: 15th International Conference, NSS 2021, Tianjin, China, October 23, 2021, Proceedings},
pages = {340–352},
numpages = {13},
keywords = {Machine learning, Dataset, Zero-day, Trace collection, Exploit},
location = {Tianjin, China}
}

@inproceedings{10.1145/2810146.2810149,
author = {Bowes, David and Hall, Tracy and Petri\'{c}, Jean},
title = {Different Classifiers Find Different Defects Although With Different Level of Consistency},
year = {2015},
isbn = {9781450337151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810146.2810149},
doi = {10.1145/2810146.2810149},
abstract = {BACKGROUND -- During the last 10 years hundreds of different defect prediction models have been published. The performance of the classifiers used in these models is reported to be similar with models rarely performing above the predictive performance ceiling of about 80% recall.OBJECTIVE -- We investigate the individual defects that four classifiers predict and analyse the level of prediction uncertainty produced by these classifiers.METHOD -- We perform a sensitivity analysis to compare the performance of Random Forest, Na\"{\i}ve Bayes, RPart and SVM classifiers when predicting defects in 12 NASA data sets. The defect predictions that each classifier makes is captured in a confusion matrix and the prediction uncertainty is compared against different classifiers.RESULTS -- Despite similar predictive performance values for these four classifiers, each detects different sets of defects. Some classifiers are more consistent in predicting defects than others.CONCLUSIONS -- Our results confirm that a unique sub-set of defects can be detected by specific classifiers. However, while some classifiers are consistent in the predictions they make, other classifiers vary in their predictions. Classifier ensembles with decision making strategies not based on majority voting are likely to perform best.},
booktitle = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {3},
numpages = {10},
location = {Beijing, China},
series = {PROMISE '15}
}

@inproceedings{10.5555/1337691.1338380,
author = {Briand, Lionel C. and Labiche, Yvan and Liu, Xuetao},
title = {Using Machine Learning to Support Debugging with Tarantula},
year = {2007},
isbn = {0769530249},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Using a specific machine learning technique, this paper proposes a way to identify suspicious statements during debugging. The technique is based on principles similar to Tarantula but addresses its main flaw: its difficulty to deal with the presence of multiple faults as it assumes that failing test cases execute the same fault(s). The improvement we present in this paper results from the use of C4.5 decision trees to identify various failure conditions based on information regarding the test cases' inputs and outputs. Failing test cases executing under similar conditions are then assumed to fail due to the same fault(s). Statements are then considered suspicious if they are covered by a large proportion of failing test cases that execute under similar conditions. We report on a case study that demonstrates improvement over the original Tarantula technique in terms of statement ranking. Another contribution of this paper is to show that failure conditions as modeled by a C4.5 decision tree accurately predict failures and can therefore be used as well to help debugging.},
booktitle = {Proceedings of the The 18th IEEE International Symposium on Software Reliability},
pages = {137–146},
numpages = {10},
series = {ISSRE '07}
}

@inproceedings{10.1145/2568225.2568269,
author = {Rahman, Foyzur and Khatri, Sameer and Barr, Earl T. and Devanbu, Premkumar},
title = {Comparing static bug finders and statistical prediction},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568269},
doi = {10.1145/2568225.2568269},
abstract = {The all-important goal of delivering better software at lower cost has led to a vital, enduring quest for ways to find and remove defects efficiently and accurately. To this end, two parallel lines of research have emerged over the last years. Static analysis seeks to find defects using algorithms that process well-defined semantic abstractions of code. Statistical defect prediction uses historical data to estimate parameters of statistical formulae modeling the phenomena thought to govern defect occurrence and predict where defects are likely to occur. These two approaches have emerged from distinct intellectual traditions and have largely evolved independently, in “splendid isolation”. In this paper, we evaluate these two (largely) disparate approaches on a similar footing. We use historical defect data to apprise the two approaches, compare them, and seek synergies. We find that under some accounting principles, they provide comparable benefits; we also find that in some settings, the performance of certain static bug-finders can be enhanced using information provided by statistical defect prediction.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {424–434},
numpages = {11},
keywords = {Software Quality, Inspection, Fault Prediction, Empirical Software Engineering, Empirical Research},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3177457.3191709,
author = {Ren, Yidan and Zhu, Zhengzhou and Chen, Xiangzhou and Ding, Huixia and Zhang, Geng},
title = {Research on Defect Detection Technology of Trusted Behavior Decision Tree Based on Intelligent Data Semantic Analysis of Massive Data},
year = {2018},
isbn = {9781450363396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177457.3191709},
doi = {10.1145/3177457.3191709},
abstract = {With the rapid development of information technology, software systems' scales and complexity are showing a trend of expansion. The users' needs for the software security, software security reliability and software stability are growing increasingly. At present, the industry has applied machine learning methods to the fields of defect detection to repair and improve software defects through the massive data intelligent semantic analysis or code scanning. The model in machine learning is faced with big difficulty of model building, understanding, and the poor visualization in the field of traditional software defect detection. In view of the above problems, we present a point of view that intelligent semantic analysis technology based on massive data, and using the trusted behavior decision tree model to analyze the soft behavior by layered detection technology. At the same time, it is equipped related test environment to compare the tested software. The result shows that the defect detection technology based on intelligent semantic analysis of massive data is superior to other techniques at the cost of building time and error reported ratio.},
booktitle = {Proceedings of the 10th International Conference on Computer Modeling and Simulation},
pages = {168–175},
numpages = {8},
keywords = {software defect detection, intelligent semantic analysis, decision tree, Massive data},
location = {Sydney, Australia},
series = {ICCMS '18}
}

@inproceedings{10.1007/978-3-030-73128-1_9,
author = {Camilli, Matteo and Felderer, Michael and Giusti, Andrea and Matt, Dominik Tobias and Perini, Anna and Russo, Barbara and Susi, Angelo},
title = {Risk-Driven Compliance Assurance for Collaborative AI Systems: A Vision Paper},
year = {2021},
isbn = {978-3-030-73127-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73128-1_9},
doi = {10.1007/978-3-030-73128-1_9},
abstract = {Context and motivation. Collaborative AI systems aim at working together with humans in a shared space. Building these systems, which comply with quality requirements, domain specific standards and regulations is a challenging research direction. This challenge is even more exacerbated for new generation of systems that leverage on machine learning components rather than deductive (top-down programmed) AI.Question/problem. How can requirements engineering, together with software and systems engineering, contribute towards the objective of building flexible and compliant collaborative AI with strong assurances?Principal idea/results. In this paper, we identify three main research directions: automated specification and management of compliance requirements, and their alignment with assurance cases; risk management; and risk-driven assurance methods. Each one tackles challenges that currently hinder engineering processes in this context.Contributions. This vision paper aims at fostering further discussion on the challenges and research directions towards appropriate methods and tools to engineer collaborative AI systems in compliance with existing standards, norms, and regulations.},
booktitle = {Requirements Engineering:  Foundation  for Software Quality: 27th International Working Conference, REFSQ 2021, Essen, Germany, April 12–15, 2021, Proceedings},
pages = {123–130},
numpages = {8},
keywords = {Compliance requirements, Compliance cases, Collaborative AI systems, Machine Learning, Risk management}
}

@inproceedings{10.1145/3439961.3439964,
author = {Moreira, Jaziel S. and Alves, Everton L. G. and Andrade, Wilkerson L.},
title = {A Systematic Mapping on Energy Efficiency Testing in Android Applications},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439964},
doi = {10.1145/3439961.3439964},
abstract = {Android devices include a wide range of features and functionalities. However, they are limited by their battery capacity. Energy efficiency has become a critical non-functional requirement for Android applications. Most applications use multiple hardware elements that may consume a great amount of energy. Moreover, energy faults and bad resource management may aggravate this issue. Several works have proposed solutions to help developers deal with energy consumption issues. In this work, we present a systematic mapping study on energy efficiency testing for Android applications. From a starting set of 1525 papers, we narrowed our investigation to 32 relevant ones. The most common research topics were Fine-grained Estimation with nine studies, followed by Test Generation and Classification, both with six studies. We also found that most apply only dynamic solutions and use software-based strategies to estimate energy consumption. Finally, we discuss a series of open problems that should be addressed by future research.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {3},
numpages = {10},
keywords = {Testing, Energy Efficiency, Android Application},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@article{10.1007/s11219-010-9128-1,
author = {M\i{}s\i{}rl\i{}, Ay\c{s}e Tosun and Bener, Ay\c{s}e Ba\c{s}ar and Turhan, Burak},
title = {An industrial case study of classifier ensembles for locating software defects},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9128-1},
doi = {10.1007/s11219-010-9128-1},
abstract = {As the application layer in embedded systems dominates over the hardware, ensuring software quality becomes a real challenge. Software testing is the most time-consuming and costly project phase, specifically in the embedded software domain. Misclassifying a safe code as defective increases the cost of projects, and hence leads to low margins. In this research, we present a defect prediction model based on an ensemble of classifiers. We have collaborated with an industrial partner from the embedded systems domain. We use our generic defect prediction models with data coming from embedded projects. The embedded systems domain is similar to mission critical software so that the goal is to catch as many defects as possible. Therefore, the expectation from a predictor is to get very high probability of detection (pd). On the other hand, most embedded systems in practice are commercial products, and companies would like to lower their costs to remain competitive in their market by keeping their false alarm (pf) rates as low as possible and improving their precision rates. In our experiments, we used data collected from our industry partners as well as publicly available data. Our results reveal that ensemble of classifiers significantly decreases pf down to 15% while increasing precision by 43% and hence, keeping balance rates at 74%. The cost-benefit analysis of the proposed model shows that it is enough to inspect 23% of the code on local datasets to detect around 70% of defects.},
journal = {Software Quality Journal},
month = sep,
pages = {515–536},
numpages = {22},
keywords = {Static code attributes, Ensemble of classifiers, Embedded software, Defect prediction}
}

@inproceedings{10.1007/978-3-642-27549-4_49,
author = {Ramler, Rudolf and Natschl\"{a}ger, Thomas},
title = {Applying heuristic approaches for predicting defect-prone software components},
year = {2011},
isbn = {9783642275487},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-27549-4_49},
doi = {10.1007/978-3-642-27549-4_49},
abstract = {Effective and efficient quality assurance has to focus on those parts of a software system that are most likely to fail. Defect prediction promises to indicate the defect-prone components of a software system. In this paper we investigate the viability of predicting defect-prone components in upcoming releases of a large industrial software system. Prediction models constructed with heuristic machine learning are used to classify the components of future versions of the software system as defective or defect-free. It could be shown that the accuracy of the predictions made for the next version is significantly higher (around 74%) than guessing even when taking only new or modified components into account. Furthermore, the results reveal that, depending on the specific prediction model, acceptable accuracy can be achieved for up to three versions in the future.},
booktitle = {Proceedings of the 13th International Conference on Computer Aided Systems Theory - Volume Part I},
pages = {384–391},
numpages = {8},
keywords = {software defect prediction, machine learning},
location = {Las Palmas de Gran Canaria, Spain},
series = {EUROCAST'11}
}

@inproceedings{10.1007/978-3-030-86230-5_43,
author = {Malaguti, Roney and Louren\c{c}o, Nuno and Silva, Cristov\~{a}o},
title = {A Well Lubricated Machine: A Data Driven Model for Lubricant Oil Conditions},
year = {2021},
isbn = {978-3-030-86229-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86230-5_43},
doi = {10.1007/978-3-030-86230-5_43},
abstract = {Diagnostic and predictive failure processes based on intelligent lubricant oil analysis are a important of the condition-based maintenance (CBM) approaches for diesel vehicle fleets. Companies are equipping each vehicle in the fleet with a large number of sensors, which allows the collection of vast amounts of data about the current state of each asset. With all this information now allows for the research and development of predictive models to help a fleet manager make informed decisions about the operating condition of the vehicles. This allows companies to accurately identify the state of wear and tear of a piece of equipment or system, making CBM more effective and reliable.In this paper we present a supervised machine learning framework based on the Random Forest Classifier (RF) to determine the operating condition of lubricant oil in diesel engines based on data from 5 different vehicles. We describe the how practitioners should collect and process data, and which features can be engineered to help describe the state of the lubrication system. This data will then be used by a RF model to determine the operational condition of the lubricating oil.The results presented show that the proposed approach is able to successfully identify the oil operating conditions, with the predictive model obtaining a Recall of 97.9%, a Precision of 99.5% and a F1-score of 98.7%. In addition, we evaluate the importance is the inclusion of new engineered features projected from raw data for better determination of the operating condition.},
booktitle = {Progress in Artificial Intelligence: 20th EPIA Conference on Artificial Intelligence, EPIA 2021, Virtual Event, September 7–9, 2021, Proceedings},
pages = {549–560},
numpages = {12},
keywords = {Random forest classifier, Diesel vehicle, Lubricating oils, Condition-based maintenance (CBM)}
}

@article{10.1016/j.ins.2021.07.053,
author = {Zheng, Ming and Li, Tong and Zheng, Xiaoyao and Yu, Qingying and Chen, Chuanming and Zhou, Ding and Lv, Changlong and Yang, Weiyi},
title = {UFFDFR: Undersampling framework with denoising, fuzzy c-means clustering, and representative sample selection for imbalanced data classification},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {576},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.07.053},
doi = {10.1016/j.ins.2021.07.053},
journal = {Inf. Sci.},
month = oct,
pages = {658–680},
numpages = {23},
keywords = {Denoising, Clustering-based undersampling methods, Undersampling methods, Imbalanced data}
}

@inproceedings{10.1109/MILCOM47813.2019.9021013,
author = {Venkatesan, Sridhar and Newcomb, E. Allison and Hoffman, Blaine and Buchler, Norbou and Youzwak, Jason A. and Sugrim, Shridatt and Chiang, Cho-Yu J. and Poylisher, Alexander and Witkowski, Matthew and Walther, Gary and Wolberg, Michelle and Chadha, Ritu},
title = {VulnerVAN: A Vulnerable Network Generation Tool},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MILCOM47813.2019.9021013},
doi = {10.1109/MILCOM47813.2019.9021013},
abstract = {Cyber training, security testing, and research and development activities are vital to improve the security posture of a network. Currently, many institutions use cyber security testbeds to conduct these activities in an isolated virtual environment. One of the important requirements for such an environment is to provide organizers (or experimenters) with a library of vulnerable network scenarios and capabilities to mount attacks against them. However, the task of preparing a vulnerable network scenario in current testbed environments is costly in time and labor, requires significant support from the testbed staff. To this end, we present a toolset called VulnerVAN that creates a vulnerable network scenario to realize an attack sequence. In this paper, we discuss the design of VulnerVAn - our proof-of-concept implementation on CyberVAN - and present a new high-level attack specification language that enables users to chain attack steps into an attack sequence. For a given attack sequence and network scenario, VulnerVAN identifies all possible attack paths through the network that can realize the attack sequence, and provides instructions to configure machines on an attack path selected by the user. VulnerVAn also provides an attack blueprint that can guide a Red team or an automated attacker to execute the attack sequence. To demonstrate VulnerVAN's capability, we consider the use-case of a typical data exfiltration attack sequence conducted by APTs and study the performance of VulnerVAn in mapping the attack sequence to different networks.},
booktitle = {MILCOM 2019 - 2019 IEEE Military Communications Conference (MILCOM)},
pages = {1–6},
numpages = {6},
location = {Norfolk, VA, USA}
}

@article{10.1016/j.compeleceng.2019.04.011,
author = {G., Geetharamani and J., Arun Pandian},
title = {Identification of plant leaf diseases using a nine-layer deep convolutional neural network},
year = {2019},
issue_date = {Jun 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2019.04.011},
doi = {10.1016/j.compeleceng.2019.04.011},
journal = {Comput. Electr. Eng.},
month = jun,
pages = {323–338},
numpages = {16},
keywords = {Transfer learning, Training epoch, Mini batch, Machine learning, Leaf diseases identification, Image augmentation, Dropout, Deep learning, Deep convolutional neural networks, Artificial intelligence}
}

@inproceedings{10.1109/ICTAI.2004.62,
author = {Wang, Qi and Yu, Bo and Zhu, Jie},
title = {Extract Rules from Software Quality Prediction Model Based on Neural Network},
year = {2004},
isbn = {076952236X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2004.62},
doi = {10.1109/ICTAI.2004.62},
abstract = {To get a highly reliable software product to the market on schedule, software engineers must allocate resources on the fault-prone software modules across the development effort. Software quality models based upon data mining from past projects can identify fault-prone modules in current similar development efforts. So that resources can be focused on fault-prone modules to improve quality prior to release. Many researchers have applied the neural networks approach to predict software quality. Although neural networks have shown their strengths in solving complex problems, their shortcoming of being \'{y}black boxes\'{y} models has prevented them from being accepted as a common practice for fault-prone software modules prediction. That is a significant weakness, for without the ability to produce comprehensible decisions, it is hard to trust the reliability of neural networks that address real-world problems. In this paper, we introduce an interpretable neural network model for software quality prediction. First, a three-layer feed-forward neural network with the sigmoid function in hidden units and the identity function in output unit was trained. The data used to train the neural network is collected from an earlier release of a telecommunications software system. Then use clustering genetic algorithm (CGA) to extract comprehensible rules from the trained neural network. We use the rule set extracted from the trained neural network to detect the fault-prone software modules of the later release and compare the predicting results with the neural network predicting results. The comparison shows that although the rule set\'{y}s predicting accuracy is a little less than the trained neural network, it is more comprehensible.},
booktitle = {Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence},
pages = {191–195},
numpages = {5},
series = {ICTAI '04}
}

@inproceedings{10.1007/11758549_105,
author = {Jin, Xin and Liu, Zhaodong and Bie, Rongfang and Zhao, Guoxing and Ma, Jixin},
title = {Support vector machines for regression and applications to software quality prediction},
year = {2006},
isbn = {3540343857},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11758549_105},
doi = {10.1007/11758549_105},
abstract = {Software metrics are the key tool in software quality management. In this paper, we propose to use support vector machines for regression applied to software metrics to predict software quality. In experiments we compare this method with other regression techniques such as Multivariate Linear Regression, Conjunctive Rule and Locally Weighted Regression. Results on benchmark dataset MIS, using mean absolute error, and correlation coefficient as regression performance measures, indicate that support vector machines regression is a promising technique for software quality prediction. In addition, our investigation of PCA based metrics extraction shows that using the first few Principal Components (PC) we can still get relatively good performance.},
booktitle = {Proceedings of the 6th International Conference on Computational Science - Volume Part IV},
pages = {781–788},
numpages = {8},
location = {Reading, UK},
series = {ICCS'06}
}

@article{10.1007/s10664-021-10004-6,
author = {Quach, Sophia and Lamothe, Maxime and Adams, Bram and Kamei, Yasutaka and Shang, Weiyi},
title = {Evaluating the impact of falsely detected performance bug-inducing changes in JIT models},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10004-6},
doi = {10.1007/s10664-021-10004-6},
abstract = {Performance bugs bear a heavy cost on both software developers and end-users. Tools to reduce the occurrence, impact, and repair time of performance bugs, can therefore provide key assistance for software developers racing to fix these bugs. Classification models that focus on identifying defect-prone commits, referred to as Just-In-Time (JIT) Quality Assurance are known to be useful in allowing developers to review risky commits. These commits can be reviewed while they are still fresh in developers’ minds, reducing the costs of developing high-quality software. JIT models, however, leverage the SZZ approach to identify whether or not a change is bug-inducing. The fixes to performance bugs may be scattered across the source code, separated from their bug-inducing locations. The nature of performance bugs may make SZZ a sub-optimal approach for identifying their bug-inducing commits. Yet, prior studies that leverage or evaluate the SZZ approach do not distinguish performance bugs from other bugs, leading to potential bias in the results. In this paper, we conduct an empirical study on the JIT defect prediction for performance bugs. We concentrate on SZZ’s ability to identify the bug-inducing commits of performance bugs in two open-source projects, Cassandra, and Hadoop. We verify whether the bug-inducing commits found by SZZ are truly bug-inducing commits by manually examining these identified commits. Our manual examination includes cross referencing fix commits and JIRA bug reports. We evaluate model performance for JIT models by using them to identify bug-inducing code commits for performance related bugs. Our findings show that JIT defect prediction classifies non-performance bug-inducing commits better than performance bug-inducing commits, i.e., the SZZ approach does introduce errors when identifying bug-inducing commits. However, we find that manually correcting these errors in the training data only slightly improves the models. In the absence of a large number of correctly labelled performance bug-inducing commits, our findings show that combining all available training data (i.e., truly performance bug-inducing commits, non-performance bug-inducing commits, and non-bug-inducing commits) yields the best classification results.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {32},
keywords = {Performance, Just-in-time, Defect prediction bugs, Software engineering}
}

@inproceedings{10.1007/978-3-030-41299-9_14,
author = {Ji, Xiaotong and Zheng, Yuchen and Suehiro, Daiki and Uchida, Seiichi},
title = {Optimal Rejection Function Meets Character Recognition Tasks},
year = {2019},
isbn = {978-3-030-41298-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-41299-9_14},
doi = {10.1007/978-3-030-41299-9_14},
abstract = {In this paper, we propose an optimal rejection method for rejecting ambiguous samples by a rejection function. This rejection function is trained together with a classification function under the framework of Learning-with-Rejection (LwR). The highlights of LwR are: (1) the rejection strategy is not heuristic but has a strong background from a machine learning theory, and (2) the rejection function can be trained on an arbitrary feature space which is different from the feature space for classification. The latter suggests we can choose a feature space which is more suitable for rejection. Although the past research on LwR focused only its theoretical aspect, we propose to utilize LwR for practical pattern classification tasks. Moreover, we propose to use features from different CNN layers for classification and rejection. Our extensive experiments of notMNIST classification and character/non-character classification demonstrate that the proposed method achieves better performance than traditional rejection strategies.},
booktitle = {Pattern Recognition: 5th Asian Conference, ACPR 2019, Auckland, New Zealand, November 26–29, 2019, Revised Selected Papers, Part II},
pages = {169–183},
numpages = {15},
keywords = {Theoretical machine learning, Optimal rejection function, Learning with Rejection},
location = {Auckland, New Zealand}
}

@inproceedings{10.1109/QSIC.2006.13,
author = {Chan, Victor K. Y. and Wong, W. Eric and Xie, T. F.},
title = {Application of a Statistical Methodology to Simplify Software Quality Metric Models Constructed Using Incomplete Data Samples},
year = {2006},
isbn = {0769527183},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/QSIC.2006.13},
doi = {10.1109/QSIC.2006.13},
abstract = {During the construction of a software metric model, incomplete data often appear in the data sample used for the construction. Moreover, the decision on whether a particular predictor metric should be included is most likely based on an intuitive or experience-based assumption that the predictor metric has an impact on the target metric with a statistical significance. However, this assumption is usually not verifiable "retrospectively" after the model is constructed, leading to redundant predictor metric(s) and/or unnecessary predictor metric complexity. To solve all these problems, the authors have earlier derived a methodology consisting of the k-nearest neighbors (k-NN) imputation method, statistical hypothesis testing, and a "goodness-of-fit" criterion. Whilst the methodology has been applied successfully to software effort metric models, it is applied only recently to software quality metric models which usually suffer from far more serious incomplete data. This paper documents the latter application based on a successful case study.},
booktitle = {Proceedings of the Sixth International Conference on Quality Software},
pages = {15–21},
numpages = {7},
series = {QSIC '06}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Video testing, Variability modeling, Software product line engineering, Feature modeling, Domain-specific languages, Configuration, Automated reasoning}
}

@inproceedings{10.5555/1625275.1625642,
author = {Baskiotis, Nicolas and Sebag, Mich\`{e}le and Gaudel, Marie-Claude and Gouraud, Sandrine},
title = {A machine learning approach for statistical software testing},
year = {2007},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Some Statistical Software Testing approaches rely on sampling the feasible paths in the control flow graph of the program; the difficulty comes from the tiny ratio of feasible paths. This paper presents an adaptive sampling mechanismcalled EXIST for Exploration/ eXploitation Inference for Software Testing, able to retrieve distinct feasible paths with high probability. EXIST proceeds by alternatively exploiting and updating a distribution on the set of program paths. An original representation of paths, accommodating long-range dependencies and data sparsity and based on extended Parikh maps, is proposed. Experimental validation on real-world and artificial problems demonstrates dramatic improvements compared to the state of the art.},
booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
pages = {2274–2279},
numpages = {6},
location = {Hyderabad, India},
series = {IJCAI'07}
}

@article{10.1016/j.dsp.2021.103205,
author = {Pourebrahim, Yousef and Razzazi, Farbod and Sameti, Hossein},
title = {Semi-supervised parallel shared encoders for speech emotion recognition},
year = {2021},
issue_date = {Nov 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {118},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103205},
doi = {10.1016/j.dsp.2021.103205},
journal = {Digit. Signal Process.},
month = nov,
numpages = {11},
keywords = {Deep neural networks, Domain adaptation, Speech emotion recognition, Semi-supervised learning}
}

@article{10.1007/s11219-019-09460-7,
author = {Qin, Fangyun and Wan, Xiaohui and Yin, Beibei},
title = {An empirical study of factors affecting cross-project aging-related bug prediction with TLAP},
year = {2020},
issue_date = {Mar 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09460-7},
doi = {10.1007/s11219-019-09460-7},
abstract = {Software aging is a phenomenon in which long-running software systems show an increasing failure rate and/or progressive performance degradation. Due to their nature, Aging-Related Bugs (ARBs) are hard to discover during software testing and are also challenging to reproduce. Therefore, automatically predicting ARBs before software release can help developers reduce ARB impact or avoid ARBs. Many bug prediction approaches have been proposed, and most of them show effectiveness in within-project prediction settings. However, due to the low presence and reproducing difficulty of ARBs, it is usually hard to collect sufficient training data to build an accurate prediction model. A recent work proposed a method named Transfer Learning based Aging-related bug Prediction (TLAP) for performing cross-project ARB prediction. Although this method considerably improves cross-project ARB prediction performance, it has been observed that its prediction result is affected by several key factors, such as the normalization methods, kernel functions, and machine learning classifiers. Therefore, this paper presents the first empirical study to examine the impact of these factors on the effectiveness of cross-project ARB prediction in terms of single-factor pattern, bigram pattern, and triplet pattern and validates the results with the Scott-Knott test technique. We find that kernel functions and classifiers are key factors affecting the effectiveness of cross-project ARB prediction, while normalization methods do not show statistical influence. In addition, the order of values in three single-factor patterns is maintained in three bigram patterns and one triplet pattern to a large extent. Similarly, the order of values in the three bigram patterns is also maintained in the triplet pattern.},
journal = {Software Quality Journal},
month = mar,
pages = {107–134},
numpages = {28},
keywords = {Empirical study, Cross-project, Software aging, Aging-related bugs}
}

@inproceedings{10.1007/978-3-030-88942-5_2,
author = {Soomlek, Chitsutha and van Rijn, Jan N. and Bonsangue, Marcello M.},
title = {Automatic Human-Like Detection of Code Smells},
year = {2021},
isbn = {978-3-030-88941-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88942-5_2},
doi = {10.1007/978-3-030-88942-5_2},
abstract = {Many code smell detection techniques and tools have been proposed, mainly aiming to eliminate design flaws and improve software quality. Most of them are based on heuristics which rely on a set of software metrics and corresponding threshold values. Those techniques and tools suffer from subjectivity issues, discordant results among the tools, and the reliability of the thresholds. To mitigate these problems, we used machine learning to automate developers’ perception in code smells detection. Different from other existing machine learning used in code smell detection we trained our models with an extensive dataset based on more than 3000 professional reviews on 518 open source projects. We conclude by an empirical evaluation of the performance of the machine learning approach against PMD, a widely used metric-based code smell detection tool for Java. The experimental results show that the machine learning approach outperforms the PMD classifier in all evaluations.},
booktitle = {Discovery Science: 24th International Conference, DS 2021, Halifax, NS, Canada, October 11–13, 2021, Proceedings},
pages = {19–28},
numpages = {10},
keywords = {Software engineering, Machine learning, Code smells},
location = {Halifax, NS, Canada}
}

@article{10.1007/s00521-021-05995-8,
author = {Tyralis, Hristos and Papacharalampous, Georgia},
title = {Boosting algorithms in energy research: a systematic review},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {21},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05995-8},
doi = {10.1007/s00521-021-05995-8},
abstract = {Machine learning algorithms have been extensively exploited in energy research, due to their flexibility, automation and ability to handle big data. Among the most prominent machine learning algorithms are the boosting ones, which are known to be “garnering wisdom from a council of fools”, thereby transforming weak learners to strong learners. Boosting algorithms are characterized by both high flexibility and high interpretability. The latter property is the result of recent developments by the statistical community. In this work, we provide understanding on the properties of boosting algorithms to facilitate a better exploitation of their strengths in energy research. In this respect, (a) we summarize recent advances on boosting algorithms, (b) we review relevant applications in energy research with those focusing on renewable energy (in particular those focusing on wind energy and solar energy) consisting a significant portion of the total ones, and (c) we describe how boosting algorithms are implemented and how their use is related to their properties. We show that boosting has been underexploited so far, while great advances in the energy field are possible both in terms of explanation and interpretation, and in terms of predictive performance.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {14101–14117},
numpages = {17},
keywords = {Renewable energy, Machine learning, Energy forecasting, Artificial intelligence}
}

@article{10.1002/stvr.1594,
author = {Kanewala, Upulee and Bieman, James M. and Ben-Hur, Asa},
title = {Predicting metamorphic relations for testing scientific software: a machine learning approach using graph kernels},
year = {2016},
issue_date = {May 2016},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {26},
number = {3},
issn = {0960-0833},
url = {https://doi.org/10.1002/stvr.1594},
doi = {10.1002/stvr.1594},
abstract = {Comprehensive, automated software testing requires an oracle to check whether the output produced by a test case matches the expected behaviour of the programme. But the challenges in creating suitable oracles limit the ability to perform automated testing in some programmes, and especially in scientific software. Metamorphic testing is a method for automating the testing process for programmes without test oracles. This technique operates by checking whether the programme behaves according to properties called metamorphic relations. A metamorphic relation describes the change in output when the input is changed in a prescribed way. Unfortunately, finding the metamorphic relations satisfied by a programme or function remains a labour-intensive task, which is generally performed by a domain expert or a programmer. In this work, we propose a machine learning approach for predicting metamorphic relations that uses a graph-based representation of a programme to represent control flow and data dependency information. In earlier work, we found that simple features derived from such graphs provide good performance. An analysis of the features used in this earlier work led us to explore the effectiveness of several representations of those graphs using the machine learning framework of graph kernels, which provide various ways of measuring similarity between graphs. Our results show that a graph kernel that evaluates the contribution of all paths in the graph has the best accuracy and that control flow information is more useful than data dependency information. The data used in this study are available for download at http://www.cs.colostate.edu/saxs/MRpred/functions.tar.gz to help researchers in further development of metamorphic relation prediction methods. Copyright © 2015 John Wiley &amp; Sons, Ltd.},
journal = {Softw. Test. Verif. Reliab.},
month = may,
pages = {245–269},
numpages = {25},
keywords = {support vector machines, metamorphic testing, metamorphic relations, graph kernels}
}

@article{10.1007/s10489-014-0610-5,
author = {Zhang, Xueying and Song, Qinbao and Wang, Guangtao and Zhang, Kaiyuan and He, Liang and Jia, Xiaolin},
title = {A dissimilarity-based imbalance data classification algorithm},
year = {2015},
issue_date = {April     2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {42},
number = {3},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-014-0610-5},
doi = {10.1007/s10489-014-0610-5},
abstract = {Class imbalances have been reported to compromise the performance of most standard classifiers, such as Naive Bayes, Decision Trees and Neural Networks. Aiming to solve this problem, various solutions have been explored mainly via balancing the skewed class distribution or improving the existing classification algorithms. However, these methods pay more attention on the imbalance distribution, ignoring the discriminative ability of features in the context of class imbalance data. In this perspective, a dissimilarity-based method is proposed to deal with the classification of imbalanced data. Our proposed method first removes the useless and redundant features by feature selection from the given data set; and then, extracts representative instances from the reduced data as prototypes; finally, projects the reduced data into a dissimilarity space by constructing new features, and builds the classification model with data in the dissimilarity space. Extensive experiments over 24 benchmark class imbalance data sets show that, compared with seven other imbalance data tackling solutions, our proposed method greatly improves the performance of imbalance learning, and outperforms the other solutions with all given classification algorithms.},
journal = {Applied Intelligence},
month = apr,
pages = {544–565},
numpages = {22},
keywords = {Software defect prediction, Prototype selection, Feature selection, Dissimilarity-based classification, Class imbalance}
}

@inproceedings{10.1145/3273934.3273944,
author = {Palma, Francis and Abdou, Tamer and Bener, Ayse and Maidens, John and Liu, Stella},
title = {An Improvement to Test Case Failure Prediction in the Context of Test Case Prioritization},
year = {2018},
isbn = {9781450365932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3273934.3273944},
doi = {10.1145/3273934.3273944},
abstract = {Aim: In this study, we aim to re-evaluate research questions on the ability of a logistic regression model proposed in a previous work to predict and prioritize the failing test cases based on some test quality metrics. Background: The process of prioritizing test cases aims to come up with a ranked test suite where test cases meeting certain criteria are prioritized. One criterion may be the ability of test cases to find faults that can be predicted a priori. Ranking test cases and executing the top-ranked test cases is particularly beneficial when projects have tight schedules and budgets. Method: We performed the comparison by first rebuilding the predictive models using the features from the original study and then we extended the original work to improve the predictive models using new features by combining with the existing ones. Results: The results of our study, using a dataset of five open-source systems, confirm that the findings from the original study hold and that our predictive models with new features outperform the original models in predicting and prioritizing the failing test cases. Conclusions: We plan to apply this method to a large-scale dataset from a large commercial enterprise project, to better demonstrate the improvement that our modified features provide and to explore the model's performance at scale.},
booktitle = {Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {80–89},
numpages = {10},
keywords = {Logistic Regression Model, Machine Learning, Prediction, Test Case Prioritization, Test Quality Metrics},
location = {Oulu, Finland},
series = {PROMISE'18}
}

@inproceedings{10.1145/3330204.3330230,
author = {de Macedo, Charles Mendes and Ruela, Andr\'{e} Siqueira and Delgado, Karina Valdivia},
title = {Application of Clustering Algorithms for Discovering Bug Patterns in JavaScript Software},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330230},
doi = {10.1145/3330204.3330230},
abstract = {Applications developed with JavaScript language are increasing every day, not only for client-side, but also for server-side and for mobile devices. In this context, the existence of tools to identify faults is fundamental in order to assist developers during the evolution of their applications. Different tools and approaches have been proposed over the years, however they have limitations to evolve over time, becoming obsolete quickly. The reason for this is the use of a fixed list of pre-defined faults that are searched in the code. The BugAID tool implements a semiautomatic strategy for discovering bug patterns by grouping the changes made during the project development. The objective of this work is to contribute to the BugAID tool, extending this tool with improvements in the extraction of characteristics to be used by the clustering algorithm. The extended module of the BugAID extraction module (BE) that extracts the characteristics is called BE+. Additionally, an evaluation of the clustering algorithms used for discovering fault patterns in JavaScript software is performed. The results show that the DBScan and Optics algorithms with BE+ presented the best results for the Rand, Jaccard and Adjusted Rand indexes, while HDBScan with BE and BE+ presented the worst result.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {21},
numpages = {8},
keywords = {Software Quality, Pattern Recognition, Machine Learning, Data Mining, Bug Discovery},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@article{10.1007/s00354-021-00124-4,
author = {Karadeniz, Talha and Tokdemir, G\"{u}l and Mara\c{s}, Hadi Hakan},
title = {Ensemble Methods for Heart Disease Prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Ohmsha},
address = {JPN},
volume = {39},
number = {3–4},
issn = {0288-3635},
url = {https://doi.org/10.1007/s00354-021-00124-4},
doi = {10.1007/s00354-021-00124-4},
abstract = {Heart disease prediction is a critical task regarding human health. It is based on deriving an Machine Learning model from medical parameters to predict risk levels. In this work, we propose and test novel ensemble methods for heart disease prediction. Randomness analysis of distance sequences is utilized to derive a classifier, which is served as a base estimator of a bagging scheme. Method is successfully tested on medical Spectf dataset. Additionally, a Graph Lasso and Ledoit–Wolf shrinkage-based classifier is developed for Statlog dataset which is a UCI data. These two algorithms yield comparatively good accuracy results: 88.7 and 88.8 for Spectf and Statlog, respectively. These proposed algorithms provide promising results and novel classification methods that can be utilized in various domains to improve performance of ensemble methods.},
journal = {New Gen. Comput.},
month = nov,
pages = {569–581},
numpages = {13},
keywords = {Weak classifier, Bagging classifier, Mahalanobis distance, Covariance estimator, Heart disease prediction, Ensemble methods, Randomness test}
}

@article{10.1016/j.knosys.2019.03.013,
author = {Xu, Peng and Du, Rui and Zhang, Zhongbao},
title = {Predicting pipeline leakage in petrochemical system through GAN and LSTM},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {175},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.03.013},
doi = {10.1016/j.knosys.2019.03.013},
journal = {Know.-Based Syst.},
month = jul,
pages = {50–61},
numpages = {12},
keywords = {LSTM, GAN, Pipeline leakage, Fault prediction}
}

@article{10.1007/s11219-021-09564-z,
author = {Amit, Idan and Feitelson, Dror G.},
title = {Corrective commit probability: a measure of the effort invested in bug fixing},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09564-z},
doi = {10.1007/s11219-021-09564-z},
abstract = {The effort invested in software development should ideally be devoted to the implementation of new features. But some of the effort is invariably also invested in corrective maintenance, that is in fixing bugs. Not much is known about what fraction of software development work is devoted to bug fixing, and what factors affect this fraction. We suggest the Corrective Commit Probability (CCP), which measures the probability that a commit reflects corrective maintenance, as an estimate of the relative effort invested in fixing bugs. We identify corrective commits by applying a linguistic model to the commit messages, achieving an accuracy of 93%, higher than any previously reported model. We compute the CCP of all large active GitHub projects (7,557 projects with 200+ commits in 2019). This leads to the creation of an investment scale, suggesting that the bottom 10% of projects spend less than 6% of their total effort on bug fixing, while the top 10% of projects spend at least 39% of their effort on bug fixing — more than 6 times more. Being a process metric, CCP is conditionally independent of source code metrics, enabling their evaluation and investigation. Analysis of project attributes shows that lower CCP (that is, lower relative investment in bug fixing) is associated with smaller files, lower coupling, use of languages like JavaScript and C# as opposed to PHP and C++, fewer code smells, lower project age, better perceived quality, fewer developers, lower developer churn, better onboarding, and better productivity.},
journal = {Software Quality Journal},
month = dec,
pages = {817–861},
numpages = {45},
keywords = {Process metric, Effort estimate, Corrective commits, Corrective maintenance}
}

@article{10.1504/IJISTA.2017.081311,
author = {Singh, Satwinder and Singla, Rozy},
title = {Classification of defective modules using object-oriented metrics},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {1},
issn = {1740-8865},
url = {https://doi.org/10.1504/IJISTA.2017.081311},
doi = {10.1504/IJISTA.2017.081311},
abstract = {Software defect in today's era is crucial in the field of software engineering. Most of the organisations use various techniques to predict defects in their products before they are delivered. Defect prediction techniques help the organisations to use their resources effectively which results in lower cost and time requirements. There are various techniques that are used for predicting defects in software before it has to be delivered, e.g., clustering, neural networks, support vector machine SVM. In this paper two defect prediction techniques: K-means clustering and multi-layer perceptron model MLP are compared. Both the techniques are implemented on different platforms. K-means clustering is implemented using WEKA tool and MLP is implemented using SPSS. The results are compared to find which algorithm produces better results. In this paper object-oriented metrics are used for predicting defects in the software.},
journal = {Int. J. Intell. Syst. Technol. Appl.},
month = jan,
pages = {1–13},
numpages = {13},
keywords = {software engineering, software development, software defects, object-oriented metrics, multi-layer perceptron, defective modules, defect prediction, classification, artificial neural networks, WEKA, SPSS, MLP, K-means clustering, ANNs}
}

@inproceedings{10.1145/3412841.3441894,
author = {Gartziandia, Aitor and Arrieta, Aitor and Agirre, Aitor and Sagardui, Goiuria and Arratibel, Maite},
title = {Using regression learners to predict performance problems on software updates: a case study on elevators dispatching algorithms},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441894},
doi = {10.1145/3412841.3441894},
abstract = {Remote software deployment and updating has long been commonplace in many different fields, but now, the increasing expansion of IoT and CPSoS (Cyber-Physcal System of Systems) has highlighted the need for additional mechanisms in these systems, to ensure the correct behaviour of the deployed software version after deployment. In this sense, this paper investigates the use of Machine Learning algorithms to predict acceptable behaviour in system performance of a new software release. By monitoring the real performance, eventual unexpected problems can be identified. Based on previous knowledge and actual run-time information, the proposed approach predicts the response time that can be considered acceptable for the new software release, and this information is used to identify problematic releases. The mechanism has been applied to the post-deployment monitoring of traffic algorithms in elevator systems. To evaluate the approach, we have used performance mutation testing, obtaining good results. This paper makes two contributions. First, it proposes several regression learners that have been trained with different types of traffic profiles to efficiently predict response time of the traffic dispatching algorithm. This prediction is then compared with the actual response time of the new algorithm release, and provides a verdict about its performance. Secondly, a comparison of the different learners is performed.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {135–144},
numpages = {10},
keywords = {cyber-physical systems, machine learning, performance bugs},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@article{10.1016/j.neucom.2011.08.040,
author = {Wang, Huanjing and Khoshgoftaar, Taghi M. and Napolitano, Amri},
title = {Software measurement data reduction using ensemble techniques},
year = {2012},
issue_date = {September, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2011.08.040},
doi = {10.1016/j.neucom.2011.08.040},
abstract = {Software defect prediction models are used to identify program modules that are high-risk, or likely to have a high number of faults. These models are built using software metrics which are collected during the software development process. Various techniques and approaches have been created for improving fault predictions. One of these is feature (metric) selection. Choosing the most important features is important to improve the effectiveness of defect predictors. However, using a single feature subset selection method may generate local optima. Ensembles of feature selection methods attempt to combine multiple feature selection methods instead of using a single one. In this paper, we present a comprehensive empirical study examining 17 different ensembles of feature ranking techniques (rankers) including six commonly used feature ranking techniques, the signal-to-noise filter technique, and 11 threshold-based feature ranking techniques. This study utilized 16 real-world software measurement data sets of different sizes and built 54,400 classification models using four well known classifiers. The main conclusion is that ensembles of very few rankers are very effective and even better than ensembles of many or all rankers.},
journal = {Neurocomput.},
month = sep,
pages = {124–132},
numpages = {9},
keywords = {Feature selection, Ensembles of feature ranking techniques, Defect prediction}
}

@inproceedings{10.5555/3304889.3305050,
author = {Wang, Nan and Zhao, Xibin and Jiang, Yu and Gao, Yue},
title = {Iterative metric learning for imbalance data classification},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {In many classification applications, the amount of data from different categories usually vary significantly, such as software defect predication and medical diagnosis. Under such circumstances, it is essential to propose a proper method to solve the imbalance issue among the data. However, most of the existing methods mainly focus on improving the performance of classifiers rather than searching for an appropriate way to find an effective data space for classification. In this paper, we propose a method named Iterative Metric Learning (IML) to explore the correlations among the imbalance data and construct an effective data space for classification. Given the imbalance training data, it is important to select a subset of training samples for each testing data. Thus, we aim to find a more stable neighborhood for the testing data using the iterative metric learning strategy. To evaluate the effectiveness of the proposed method, we have conducted experiments on two groups of dataset, i.e., the NASA Metrics Data Program (NASA) dataset and UCI Machine Learning Repository (UCI) dataset. Experimental results and comparisons with state-of-the-art methods have exhibited better performance of our proposed method.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {2805–2811},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@article{10.1016/j.procs.2019.09.155,
author = {Miholca, Diana-Lucia and Czibula, Gabriela},
title = {DynGRAR: A dynamic approach to mining gradual relational association rules},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {159},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.09.155},
doi = {10.1016/j.procs.2019.09.155},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {10–19},
numpages = {10},
keywords = {68T35, 03B52, gradual relational association rule 2000 MSC: 68T05, data mining, Unuspervised learning}
}

@inproceedings{10.1109/TAIC-PART.2006.15,
author = {Li, Keqin and Groz, Roland and Shahbaz, Muzammil},
title = {Integration Testing of Components Guided by Incremental State Machine Learning},
year = {2006},
isbn = {0769526721},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/TAIC-PART.2006.15},
doi = {10.1109/TAIC-PART.2006.15},
abstract = {The design of complex systems, e.g., telecom services, is nowadays usually based on the integration of components (COTS), loosely coupled in distributed architectures. When components come from third party sources, their internal structure is usually unknown and the documentation is insufficient. Therefore, the system integrator faces the problem of providing a required system assembling COTS whose behaviour is barely specified and for which no model is usually available. In this paper, we address the problem of integration testing of COTS. It combines test generation techniques with machine learning algorithms. Statebased models of components are built from observed behaviours. The models are alternatively used to generate tests and extended to take into account observed behaviour. This process is iterated until a satisfactory level of confidence in testing is achieved.},
booktitle = {Proceedings of the Testing: Academic &amp; Industrial Conference on Practice And Research Techniques},
pages = {59–70},
numpages = {12},
series = {TAIC-PART '06}
}

@article{10.1016/j.infsof.2009.10.010,
author = {Casamayor, Agustin and Godoy, Daniela and Campo, Marcelo},
title = {Identification of non-functional requirements in textual specifications: A semi-supervised learning approach},
year = {2010},
issue_date = {April, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.10.010},
doi = {10.1016/j.infsof.2009.10.010},
abstract = {Context: Early detection of non-functional requirements (NFRs) is crucial in the evaluation of architectural alternatives starting from initial design decisions. The application of supervised text categorization strategies for requirements expressed in natural language has been proposed in several works as a method to help analysts in the detection and classification of NFRs concerning different aspects of software. However, a significant number of pre-categorized requirements are needed to train supervised text classifiers, which implies that analysts have to manually assign categories to numerous requirements before being able of accurately classifying the remaining ones. Objective: We propose a semi-supervised text categorization approach for the automatic identification and classification of non-functional requirements. Therefore, a small number of requirements, possibly identified by the requirement team during the elicitation process, enable learning an initial classifier for NFRs, which could successively identify the type of further requirements in an iterative process. The goal of the approach is the integration into a recommender system to assist requirement analysts and software designers in the architectural design process. Method: Detection and classification of NFRs is performed using semi-supervised learning techniques. Classification is based on a reduced number of categorized requirements by taking advantage of the knowledge provided by uncategorized ones, as well as certain properties of text. The learning method also exploits feedback from users to enhance classification performance. Results: The semi-supervised approach resulted in accuracy rates above 70%, considerably higher than the results obtained with supervised methods using standard collections of documents. Conclusion: Empirical evidence showed that semi-supervision requires less human effort in labeling requirements than fully supervised methods, and can be further improved based on feedback provided by analysts. Our approach outperforms previous supervised classification proposals and can be further enhanced by exploiting feedback provided by analysts.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {436–445},
numpages = {10},
keywords = {Semi-supervised text learning, Requirement classification, Non-functional requirements}
}

@inproceedings{10.1145/3493244.3493259,
author = {da Silva, Maike Bezerra and Bezerra, Carla and Coutinho, Emanuel and Maia, Paulo Henrique},
title = {A Catalog of Performance Measures for Self-Adaptive Systems},
year = {2021},
isbn = {9781450395533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493244.3493259},
doi = {10.1145/3493244.3493259},
abstract = {[Context] Self-adaptive systems (SAS) can evaluate their own behavior at runtime and change it when necessary to avoid failures. Many of those systems require a high processing power to carry out their dynamic and complex operations successfully. In this realm, performance arises as an important quality characteristic that has been widely used in the literature to drive the necessary adaptation strategies in SAS to achieve or maintain the QoS requirements. However, few studies evaluate the SAS performance efficiently due to the lack of knowledge on all possible performance measures. Still, there is a variety of measures scattered throughout the body of work in this area. [Objective] Our objective is to create a catalog of performance measures for assessing self-adaptive systems. [Method] To do that, we identified and analyzed the performance measures used in 32 primary studies found through a systematic literature mapping. [Results] As a result, 18 measures have been inserted in the catalog, having most of them addressed the sub-characteristics of time behavior and resource utilization. To validate the catalog, we implemented a subset of measures in a healthcare SAS.},
booktitle = {Proceedings of the XX Brazilian Symposium on Software Quality},
articleno = {1},
numpages = {10},
keywords = {Self-adaptive systems, Performance, Measures},
location = {Virtual Event, Brazil},
series = {SBQS '21}
}

@article{10.3103/S1060992X19020048,
author = {Yakovenko, A. A.},
title = {A Hybrid Learning Approach for Adaptive Classification of Acoustic Signals Using the Simulated Responses of Auditory Nerve Fibers},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {2},
issn = {1060-992X},
url = {https://doi.org/10.3103/S1060992X19020048},
doi = {10.3103/S1060992X19020048},
journal = {Opt. Mem. Neural Netw.},
month = apr,
pages = {118–128},
numpages = {11},
keywords = {unsupervised learning, self-organizing maps, radial basis functions, neural responses, machine perception, auditory periphery model, adaptive pattern classification}
}

@inproceedings{10.1007/978-3-030-78609-0_23,
author = {Peng, Yu and Li, Xiaoyu and Lu, Chao and Tang, Xiaolan and Lin, Bin},
title = {A Light-Weight Prediction Model for Aero-Engine Surge Based on Seq2Seq},
year = {2021},
isbn = {978-3-030-78608-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78609-0_23},
doi = {10.1007/978-3-030-78609-0_23},
abstract = {Surge is an abnormal fault caused by compressor in the process of aero-engine flight, which will reduce the performance of aero-engines. When the surge gets serious, it will even cause engine damage, endanger flight safety, and cause a huge loss of personnel and property. Therefore, it is of great significance to predict the aero-engine surge timely and accurately. At present, little research have been done on the prediction task of aero-engine surge, and there are problems of low prediction accuracy and long calculation time. In order to solve these problems, a light-weight prediction model for aero-engine surge based on Seq2Seq (sequence to sequence) is proposed, which is called Ligh4S. Ligh4S uses the one-dimensional convolution neural network instead of the LSTM (long short-term memory network) structure in tradition-al Seq2Seq, which allows the model to compute in parallel, thus greatly improving computational efficiency and reducing prediction time. Experiments on the experimental data of an aero-engine show that the model achieves the performances of 94.3%, 92.1%, 93.2% respectively on precision rate, recall rate and F1 score under the condition of significantly reducing the size of model and the amount of calculation. What’s more, the model takes only 2ms to make a single prediction, which increases the prediction speed by about 98% compared with the LSTM-based Seq2Seq model.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part I},
pages = {265–277},
numpages = {13},
keywords = {seq2seq, Light-weight, Fault prediction, Aero-engine surge},
location = {Dublin, Ireland}
}

@inproceedings{10.1145/3475960.3475985,
author = {Bhandari, Guru and Naseer, Amara and Moonen, Leon},
title = {CVEfixes: automated collection of vulnerabilities and their fixes from open-source software},
year = {2021},
isbn = {9781450386807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475960.3475985},
doi = {10.1145/3475960.3475985},
abstract = {Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes. The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits. CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.},
booktitle = {Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {30–39},
numpages = {10},
keywords = {Security vulnerabilities, dataset, software repository mining, source code repair, vulnerability classification, vulnerability prediction},
location = {Athens, Greece},
series = {PROMISE 2021}
}

@inproceedings{10.1007/978-3-030-58811-3_67,
author = {ElGhondakly, Roaa and Moussa, Sherin and Badr, Nagwa},
title = {Handling Faults in Service Oriented Computing: A Comprehensive Study},
year = {2020},
isbn = {978-3-030-58810-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58811-3_67},
doi = {10.1007/978-3-030-58811-3_67},
abstract = {Recently, service-oriented computing paradigms have become a trending development direction, in which software systems are built using a set of loosely coupled services distributed over multiple locations through a service-oriented architecture. Such systems encounter different challenges, as integration, performance, reliability, availability, etc., which made all associated testing activities to be another major challenge to avoid their faults and system failures. Services are considered the substantial element in service-oriented computing. Thus, the quality of services and the service dependability in a web service composition have become essential to manage faults within these software systems. Many studies addressed web service faults from diverse perspectives. In this paper, a comprehensive study is conducted to investigate the different perspectives to manipulate web service faults, including fault tolerance, fault injection, fault prediction and fault localization. An extensive comparison is provided, highlighting the main research gaps, challenges and limitations of each perspective for web services. An analytical discussion is then followed to suggest future research directions that can be adopted to face such obstacles by improving fault handling capabilities for an efficient testing in service-oriented computing systems.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part IV},
pages = {947–959},
numpages = {13},
keywords = {Service oriented computing, Service testing, Quality of Service, Fault injection, Fault prediction, Fault tolerance},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/1753196.1753199,
author = {Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {DEQUALITE: building design-based software quality models},
year = {2008},
isbn = {9781605581514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753196.1753199},
doi = {10.1145/1753196.1753199},
abstract = {Object-oriented software quality models usually use metrics of classes or of relationships between classes to measure internal attributes of systems. However, the quality of these systems does not depend on classes solely: It also depends on the organisation of classes, i.e., their design. We propose DEQUALITE, a method to build models to measure the quality of systems taking into account both their internal attributes and their designs. Our method uses a machine learning approach and also allows combining different models to improve the quality prediction. In this paper, we justify the use of patterns to build quality models, we illustrate our method on a set of systems implementing design patterns and on the quality model QMOOD from Bansiya et al. We discuss the advantages and limitations of this method, we then present a validation of a resulting quality model on a set of systems. We conclude on the advantages of using patterns to build models and the difficulty of doing so.},
booktitle = {Proceedings of the 15th Conference on Pattern Languages of Programs},
articleno = {2},
numpages = {7},
keywords = {quality, patterns, models, machine learning},
location = {Nashville, Tennessee, USA},
series = {PLoP '08}
}

@article{10.1007/s11277-018-5916-x,
author = {Guo, Jun and Tan, Zheng-Hua and Cho, Sung Ho and Zhang, Guoqiang},
title = {Wireless Personal Communications: Machine Learning for Big Data Processing in Mobile Internet},
year = {2018},
issue_date = {October   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {102},
number = {3},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-018-5916-x},
doi = {10.1007/s11277-018-5916-x},
journal = {Wirel. Pers. Commun.},
month = oct,
pages = {2093–2098},
numpages = {6}
}

@inproceedings{10.1109/ISSRE.2005.6,
author = {Xing, Fei and Guo, Ping and Lyu, Michael R.},
title = {A Novel Method for Early Software Quality Prediction Based on Support Vector Machine},
year = {2005},
isbn = {0769524826},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISSRE.2005.6},
doi = {10.1109/ISSRE.2005.6},
abstract = {The software development process imposes major impacts on the quality of software at every development stage; therefore, a common goal of each software development phase concerns how to improve software quality. Software quality prediction thus aims to evaluate software quality level periodically and to indicate software quality problems early. In this paper, we propose a novel technique to predict software quality by adopting Support Vector Machine (SVM) in the classification of software modules based on complexity metrics. Because only limited information of software complexity metrics is available in early software life cycle, ordinary software quality models cannot make good predictions generally. It is well known that SVM generalizes well even in high dimensional spaces under small training sample conditions. We consequently propose a SVM-based software classification model, whose characteristic is appropriate for early software quality predictions when only a small number of sample data are available. Experimental results with a Medical Imaging System software metrics data show that our SVM prediction model achieves better software quality prediction than some commonly used software quality prediction models.},
booktitle = {Proceedings of the 16th IEEE International Symposium on Software Reliability Engineering},
pages = {213–222},
numpages = {10},
series = {ISSRE '05}
}

@inproceedings{10.1145/3345629.3345630,
author = {Lenarduzzi, Valentina and Saarim\"{a}ki, Nyyti and Taibi, Davide},
title = {The Technical Debt Dataset},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345630},
doi = {10.1145/3345629.3345630},
abstract = {Technical Debt analysis is increasing in popularity as nowadays researchers and industry are adopting various tools for static code analysis to evaluate the quality of their code. Despite this, empirical studies on software projects are expensive because of the time needed to analyze the projects. In addition, the results are difficult to compare as studies commonly consider different projects. In this work, we propose the Technical Debt Dataset, a curated set of project measurement data from 33 Java projects from the Apache Software Foundation. In the Technical Debt Dataset, we analyzed all commits from separately defined time frames with SonarQube to collect Technical Debt information and with Ptidej to detect code smells. Moreover, we extracted all available commit information from the git logs, the refactoring applied with Refactoring Miner, and fault information reported in the issue trackers (Jira). Using this information, we executed the SZZ algorithm to identify the fault-inducing and -fixing commits. We analyzed 78K commits from the selected 33 projects, detecting 1.8M SonarQube issues, 62K code smells, 28K faults and 57K refactorings. The project analysis took more than 200 days. In this paper, we describe the data retrieval pipeline together with the tools used for the analysis. The dataset is made available through CSV files and an SQLite database to facilitate queries on the data. The Technical Debt Dataset aims to open up diverse opportunities for Technical Debt research, enabling researchers to compare results on common projects.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {2–11},
numpages = {10},
keywords = {Dataset, Faults, Mining Software Repository, SZZ, Software Quality, SonarQube, Technical Debt},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@inproceedings{10.1145/3348445.3348453,
author = {Cynthia, Shamse Tasnim and Ripon, Shamim H.},
title = {Predicting and Classifying Software Faults: A Data Mining Approach},
year = {2019},
isbn = {9781450371957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3348445.3348453},
doi = {10.1145/3348445.3348453},
abstract = {In the field of software engineering, the detection of fault in the software has become a major topic to explore. With the help of data mining and machine learning approaches, this paper aims to denote whether a software is fault prone or not. In order to accomplish that this paper gives importance to compare between different machine learning approaches and by observing their performances we can conclude which models perform better to detect fault in the selected software modules. The dataset we have chosen to work on has imbalanced data. This paper research also worked with the imbalanced dataset and what results the imbalanced dataset gave when examined. The accuracy comparison, the performance of the different metrics can broadly help in software defect detection mechanism.},
booktitle = {Proceedings of the 7th International Conference on Computer and Communications Management},
pages = {143–147},
numpages = {5},
keywords = {prediction, data mining, association rules, Software faults, SVM, Adaboost},
location = {Bangkok, Thailand},
series = {ICCCM '19}
}

@article{10.1016/j.eswa.2016.06.005,
title = {A multiobjective weighted voting ensemble classifier based on differential evolution algorithm for text sentiment classification},
year = {2016},
issue_date = {November 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {62},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.06.005},
doi = {10.1016/j.eswa.2016.06.005},
abstract = {A novel multi-objective differential evolution algorithm based classifier ensemble for text sentiment classification. An empirical comparison of weighted and unweighted voting schemes. Extensive empirical analysis on metaheuristic based voting schemes for sentiment analysis. High classification accuracies for text sentiment classification (98.86% for Laptop dataset). Typically performed by supervised machine learning algorithms, sentiment analysis is highly useful for extracting subjective information from text documents online. Most approaches that use ensemble learning paradigms toward sentiment analysis involve feature engineering in order to enhance the predictive performance. In response, we sought to develop a paradigm of a multiobjective, optimization-based weighted voting scheme to assign appropriate weight values to classifiers and each output class based on the predictive performance of classification algorithms, all to enhance the predictive performance of sentiment classification. The proposed ensemble method is based on static classifier selection involving majority voting error and forward search, as well as a multiobjective differential evolution algorithm. Based on the static classifier selection scheme, our proposed ensemble method incorporates Bayesian logistic regression, na\"{\i}ve Bayes, linear discriminant analysis, logistic regression, and support vector machines as base learners, whose performance in terms of precision and recall values determines weight adjustment. Our experimental analysis of classification tasks, including sentiment analysis, software defect prediction, credit risk modeling, spam filtering, and semantic mapping, suggests that the proposed classification scheme can predict better than conventional ensemble learning methods such as AdaBoost, bagging, random subspace, and majority voting. Of all datasets examined, the laptop dataset showed the best classification accuracy (98.86%).},
journal = {Expert Syst. Appl.},
month = nov,
pages = {1–16},
numpages = {16}
}

@article{10.1016/j.future.2019.05.080,
author = {Zhao, Linchang and Shang, Zhaowei and Qin, Anyong and Zhang, Taiping and Zhao, Ling and Wei, Yu and Tang, Yuan Yan},
title = {A cost-sensitive meta-learning classifier: SPFCNN-Miner},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {100},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.05.080},
doi = {10.1016/j.future.2019.05.080},
journal = {Future Gener. Comput. Syst.},
month = nov,
pages = {1031–1043},
numpages = {13},
keywords = {Data mining, Siamese parallel fully-connected networks, Machine learning, Cost-sensitive learning, Few-shot learning, Meta learning}
}

@article{10.1007/s11219-020-09529-8,
author = {Al-Hawari, Assem and Najadat, Hassan and Shatnawi, Raed},
title = {Classification of application reviews into software maintenance tasks using data mining techniques},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09529-8},
doi = {10.1007/s11219-020-09529-8},
abstract = {Mobile application reviews are considered a rich source of information for software engineers to provide a general understanding of user requirements and technical feedback to avoid main programming issues. Previous researches have used traditional data mining techniques to classify user reviews into several software maintenance tasks. In this paper, we aim to use associative classification (AC) algorithms to investigate the performance of different classifiers to classify reviews into several software maintenance tasks. Also, we proposed a new AC approach for review mining (ACRM). Review classification needs preprocessing steps to apply natural language preprocessing and text analysis. Also, we studied the influence of two feature selection techniques (information gain and chi-square) on classifiers. Association rules give a better understanding of users’ intent since they discover the hidden patterns in words and features that are related to one of the maintenance tasks, and present it as class association rules (CARs). For testing the classifiers, we used two datasets that classify reviews into four different maintenance tasks. Results show that the highest accuracy was achieved by AC algorithms for both datasets. ACRM has the highest precision, recall, F-score, and accuracy. Feature selection helps improving the classifiers’ performance significantly.},
journal = {Software Quality Journal},
month = sep,
pages = {667–703},
numpages = {37},
keywords = {Interesting measures, Software reviews mining, Associative classification}
}

@inproceedings{10.1007/978-3-030-29551-6_57,
author = {Lahsoumi, Abir and Elouedi, Zied},
title = {Evidential Artificial Immune Recognition System},
year = {2019},
isbn = {978-3-030-29550-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29551-6_57},
doi = {10.1007/978-3-030-29551-6_57},
abstract = {Uncertainty is one of the main classification issues that must be handled carefully and not rejected in order to make better decisions. Artificial immune recognition system (AIRS) is an immune-inspired supervised learning classifier that has shown good and competitive classification results. It works perfectly in a certain context, however it is quite the opposite in an environment pervaded with uncertainty. To overcome this limitation, we propose a new approach combining the AIRS and belief function theory one of the well-know theories managing uncertainty. Experimentations on real data sets from the U.C.I machine learning repository show good performances of the proposed approach.},
booktitle = {Knowledge Science, Engineering and Management: 12th International Conference, KSEM 2019, Athens, Greece, August 28–30, 2019, Proceedings, Part I},
pages = {643–654},
numpages = {12},
keywords = {Belief function theory, Uncertainty, Classification, Artificial immune recognition system (AIRS)},
location = {Athens, Greece}
}

@inproceedings{10.1145/3236024.3236050,
author = {Chen, Di and Fu, Wei and Krishna, Rahul and Menzies, Tim},
title = {Applications of psychological science for actionable analytics},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236050},
doi = {10.1145/3236024.3236050},
abstract = {According to psychological scientists, humans understand models that most match their own internal models, which they characterize as lists of "heuristic"s (i.e. lists of very succinct rules). One such heuristic rule generator is the Fast-and-Frugal Trees (FFT) preferred by psychological scientists. Despite their successful use in many applied domains, FFTs have not been applied in software analytics. Accordingly, this paper assesses FFTs for software analytics.  We find that FFTs are remarkably effective in that their models are very succinct (5 lines or less describing a binary decision tree) while also outperforming result from very recent, top-level, conference papers. Also, when we restrict training data to operational attributes (i.e., those attributes that are frequently changed by developers), the performance of FFTs are not effected (while the performance of other learners can vary wildly).  Our conclusions are two-fold. Firstly, there is much that software analytics community could learn from psychological science. Secondly, proponents of complex methods should always baseline those methods against simpler alternatives. For example, FFTs could be used as a standard baseline learner against which other software analytics tools are compared.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {456–467},
numpages = {12},
keywords = {software analytics, psychological science, heuristics, empirical studies, defect prediction, Decision trees},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1109/ICSE.2019.00075,
author = {Yatish, Suraj and Jiarpakdee, Jirayus and Thongtanunam, Patanamon and Tantithamthavorn, Chakkrit},
title = {Mining software defects: should we consider affected releases?},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00075},
doi = {10.1109/ICSE.2019.00075},
abstract = {With the rise of the Mining Software Repositories (MSR) field, defect datasets extracted from software repositories play a foundational role in many empirical studies related to software quality. At the core of defect data preparation is the identification of post-release defects. Prior studies leverage many heuristics (e.g., keywords and issue IDs) to identify post-release defects. However, such the heuristic approach is based on several assumptions, which pose common threats to the validity of many studies. In this paper, we set out to investigate the nature of the difference of defect datasets generated by the heuristic approach and the realistic approach that leverages the earliest affected release that is realistically estimated by a software development team for a given defect. In addition, we investigate the impact of defect identification approaches on the predictive accuracy and the ranking of defective modules that are produced by defect models. Through a case study of defect datasets of 32 releases, we find that that the heuristic approach has a large impact on both defect count datasets and binary defect datasets. Surprisingly, we find that the heuristic approach has a minimal impact on defect count models, suggesting that future work should not be too concerned about defect count models that are constructed using heuristic defect datasets. On the other hand, using defect datasets generated by the realistic approach lead to an improvement in the predictive accuracy of defect classification models.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {654–665},
numpages = {12},
keywords = {software quality, mining software repositories, empirical software engineering, defect prediction models},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/3122787,
author = {Balkan, Ayca and Tabuada, Paulo and Deshmukh, Jyotirmoy V. and Jin, Xiaoqing and Kapinski, James},
title = {Underminer: A Framework for Automatically Identifying Nonconverging Behaviors in Black-Box System Models},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3122787},
doi = {10.1145/3122787},
abstract = {Evaluation of industrial embedded control system designs is a time-consuming and imperfect process. While an ideal process would apply a formal verification technique such as model checking or theorem proving, these techniques do not scale to industrial design problems, and it is often difficult to use these techniques to verify performance aspects of control system designs, such as stability or convergence. For industrial designs, engineers rely on testing processes to identify critical or unexpected behaviors. We propose a novel framework called Underminer to improve the testing process; this is an automated technique to identify nonconverging behaviors in embedded control system designs. Underminer treats the system as a black box and lets the designer indicate the model parameters, inputs, and outputs that are of interest. It differentiates convergent from nonconvergent behaviors using Convergence Classifier Functions (CCFs).The tool can be applied in the context of testing models created late in the controller development stage, where it assumes that the given model displays mostly convergent behavior and learns a CCF in an unsupervised fashion from such convergent model behaviors. This CCF is then used to guide a thorough exploration of the model with the help of optimization-guided techniques or adaptive sampling techniques, with the goal of identifying rare nonconvergent model behaviors. Underminer can also be used early in the development stage, where models may have some significant nonconvergent behaviors. Here, the framework permits designers to indicate their mental model for convergence by labeling behaviors as convergent/nonconvergent and then constructs a CCF using a supervised learning technique. In this use case, the goal is to use the CCF to test an improved design for the model. Underminer supports a number of convergence-like notions, such as those based on Lyapunov analysis and temporal logic, and also CCFs learned directly from labeled output behaviors using machine-learning techniques such as support vector machines and neural networks. We demonstrate the efficacy of Underminer by evaluating its performance on several academic as well as industrial examples.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = dec,
articleno = {20},
numpages = {28},
keywords = {stability, machine learning, formal methods, Automatic testing}
}

@article{10.1007/s11219-021-09568-9,
author = {Ulan, Maria and L\"{o}we, Welf and Ericsson, Morgan and Wingkvist, Anna},
title = {Copula-based software metrics aggregation},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09568-9},
doi = {10.1007/s11219-021-09568-9},
abstract = {A quality model is a conceptual decomposition of an abstract notion of quality into relevant, possibly conflicting characteristics and further into measurable metrics. For quality assessment and decision making, metrics values are aggregated to characteristics and ultimately to quality scores. Aggregation has often been problematic as quality models do not provide the semantics of aggregation. This makes it hard to formally reason about metrics, characteristics, and quality. We argue that aggregation needs to be interpretable and mathematically well defined in order to assess, to compare, and to improve quality. To address this challenge, we propose a probabilistic approach to aggregation and define quality scores based on joint distributions of absolute metrics values. To evaluate the proposed approach and its implementation under realistic conditions, we conduct empirical studies on bug prediction of ca. 5000 software classes, maintainability of ca. 15000 open-source software systems, and on the information quality of ca. 100000 real-world technical documents. We found that our approach is feasible, accurate, and scalable in performance.},
journal = {Software Quality Journal},
month = dec,
pages = {863–899},
numpages = {37},
keywords = {Copula, Probabilistic models, Multivariate statistical methods, Aggregation, Software metrics, Quantitative methods, Quality assessment}
}

@article{10.1016/j.patcog.2021.108164,
author = {Yang, Zhaohui and Shi, Miaojing and Xu, Chao and Ferrari, Vittorio and Avrithis, Yannis},
title = {Training object detectors from few weakly-labeled and many unlabeled images},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108164},
doi = {10.1016/j.patcog.2021.108164},
journal = {Pattern Recogn.},
month = dec,
numpages = {10},
keywords = {Unlabelled set, Semi-supervised learning, Weakly-supervised learning, Object detection}
}

@article{10.1007/s11219-018-9428-4,
author = {Eghan, Ellis E. and Alqahtani, Sultan S. and Forbes, Christopher and Rilling, Juergen},
title = {API trustworthiness: an ontological approach for software library adoption},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9428-4},
doi = {10.1007/s11219-018-9428-4},
abstract = {The globalization of the software industry has led to an emerging trend where software systems depend increasingly on the use of external open-source external libraries and application programming interfaces (APIs). While a significant body of research exists on identifying and recommending potentially reusable libraries to end users, very little is known on the potential direct and indirect impact of these external library recommendations on the quality and trustworthiness of a client’s project. In our research, we introduce a novel Ontological Trustworthiness Assessment Model (OntTAM), which supports (1) the automated analysis and assessment of quality attributes related to the trustworthiness of libraries and APIs in open-source systems and (2) provides developers with additional insights into the potential impact of reused libraries and APIs on the quality and trustworthiness of their project. We illustrate the applicability of our approach, by assessing the trustworthiness of libraries in terms of their API breaking changes, security vulnerabilities, and license violations and their potential impact on client projects.},
journal = {Software Quality Journal},
month = sep,
pages = {969–1014},
numpages = {46},
keywords = {Software security vulnerabilities, API breaking changes, License violations, Code reuse, Trustworthiness, Software quality}
}

@article{10.5555/2639037.2639042,
author = {Zazworka, Nico and Vetro', Antonio and Izurieta, Clemente and Wong, Sunny and Cai, Yuanfang and Seaman, Carolyn and Shull, Forrest},
title = {Comparing four approaches for technical debt identification},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
abstract = {Software systems accumulate technical debt (TD) when short-term goals in software development are traded for long-term goals (e.g., quick-and-dirty implementation to reach a release date versus a well-refactored implementation that supports the long-term health of the project). Some forms of TD accumulate over time in the form of source code that is difficult to work with and exhibits a variety of anomalies. A number of source code analysis techniques and tools have been proposed to potentially identify the code-level debt accumulated in a system. What has not yet been studied is if using multiple tools to detect TD can lead to benefits, that is, if different tools will flag the same or different source code components. Further, these techniques also lack investigation into the symptoms of TD "interest" that they lead to. To address this latter question, we also investigated whether TD, as identified by the source code analysis techniques, correlates with interest payments in the form of increased defect- and change-proneness. Comparing the results of different TD identification approaches to understand their commonalities and differences and to evaluate their relationship to indicators of future TD "interest." We selected four different TD identification techniques (code smells, automatic static analysis issues, grime buildup, and Modularity violations) and applied them to 13 versions of the Apache Hadoop open source software project. We collected and aggregated statistical measures to investigate whether the different techniques identified TD indicators in the same or different classes and whether those classes in turn exhibited high interest (in the form of a large number of defects and higher change-proneness). The outputs of the four approaches have very little overlap and are therefore pointing to different problems in the source code. Dispersed Coupling and Modularity violations were co-located in classes with higher defect-proneness. We also observed a strong relationship between Modularity violations and change-proneness. Our main contribution is an initial overview of the TD landscape, showing that different TD techniques are loosely coupled and therefore indicate problems in different locations of the source code. Moreover, our proxy interest indicators (change- and defect-proneness) correlate with only a small subset of TD indicators.},
journal = {Software Quality Journal},
month = sep,
pages = {403–426},
numpages = {24},
keywords = {Technical debt, Source code analysis, Software quality, Software maintenance, Modularity violations, Grime, Code smells, ASA}
}

@article{10.1016/j.datak.2008.10.005,
author = {Turhan, Burak and Bener, Ayse},
title = {Analysis of Naive Bayes' assumptions on software fault data: An empirical study},
year = {2009},
issue_date = {February, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {68},
number = {2},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2008.10.005},
doi = {10.1016/j.datak.2008.10.005},
abstract = {Software defect prediction is important for reducing test times by allocating testing resources effectively. In terms of predicting the defects in software, Naive Bayes outperforms a wide range of other methods. However, Naive Bayes assumes the 'independence' and 'equal importance' of attributes. In this work, we analyze these assumptions of Naive Bayes using public software defect data from NASA. Our analysis shows that independence assumption is not harmful for software defect data with PCA pre-processing. Our results also indicate that assigning weights to static code attributes may increase the prediction performance significantly, while removing the need for feature subset selection.},
journal = {Data Knowl. Eng.},
month = feb,
pages = {278–290},
numpages = {13},
keywords = {Software defect prediction, Naive Bayes, Empirical study}
}

@article{10.1007/s10664-020-09843-6,
author = {Krishna, Rahul and Menzies, Tim},
title = {Learning actionable analytics from multiple software projects},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09843-6},
doi = {10.1007/s10664-020-09843-6},
abstract = {The current generation of software analytics tools are mostly prediction algorithms (e.g. support vector machines, naive bayes, logistic regression, etc). While prediction is useful, after prediction comes planning about what actions to take in order to improve quality. This research seeks methods that generate demonstrably useful guidance on “what to do” within the context of a specific software project. Specifically, we propose XTREE (for within-project planning) and BELLTREE (for cross-project planning) to generating plans that can improve software quality. Each such plan has the property that, if followed, it reduces the expected number of future defect reports. To find this expected number, planning was first applied to data from release x. Next, we looked for change in release x + 1 that conformed to our plans. This procedure was applied using a range of planners from the literature, as well as XTREE. In 10 open-source JAVA systems, several hundreds of defects were reduced in sections of the code that conformed to XTREE’s plans. Further, when compared to other planners, XTREE’s plans were found to be easier to implement (since they were shorter) and more effective at reducing the expected number of defects.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3468–3500},
numpages = {33},
keywords = {Defect prediction, Bellwethers, Planning, Actionable analytics, Data mining}
}

@inproceedings{10.1145/3106237.3106258,
author = {Wang, Song and Nam, Jaechang and Tan, Lin},
title = {QTEP: quality-aware test case prioritization},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106258},
doi = {10.1145/3106237.3106258},
abstract = {Test case prioritization (TCP) is a practical activity in software testing for exposing faults earlier. Researchers have proposed many TCP techniques to reorder test cases. Among them, coverage-based TCPs have been widely investigated. Specifically, coverage-based TCP approaches leverage coverage information between source code and test cases, i.e., static code coverage and dynamic code coverage, to schedule test cases. Existing coverage-based TCP techniques mainly focus on maximizing coverage while often do not consider the likely distribution of faults in source code. However, software faults are not often equally distributed in source code, e.g., around 80% faults are located in about 20% source code. Intuitively, test cases that cover the faulty source code should have higher priorities, since they are more likely to find faults.  In this paper, we present a quality-aware test case prioritization technique, QTEP, to address the limitation of existing coverage-based TCP algorithms. In QTEP, we leverage code inspection techniques, i.e., a typical statistic defect prediction model and a typical static bug finder, to detect fault-prone source code and then adapt existing coverage-based TCP algorithms by considering the weighted source code in terms of fault-proneness. Our evaluation with 16 variant QTEP techniques on 33 different versions of 7 open source Java projects shows that QTEP could improve existing coverage-based TCP techniques for both regression and new test cases. Specifically, the improvement of the best variant of QTEP for regression test cases could be up to 15.0% and on average 7.6%, and for all test cases (both regression and new test cases), the improvement could be up to 10.0% and on average 5.0%.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {523–534},
numpages = {12},
keywords = {static bug finder, defect prediction, Test case prioritization},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3127005.3127014,
author = {Thompson, Christopher and Wagner, David},
title = {A Large-Scale Study of Modern Code Review and Security in Open Source Projects},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127014},
doi = {10.1145/3127005.3127014},
abstract = {Background: Evidence for the relationship between code review process and software security (and software quality) has the potential to help improve code review automation and tools, as well as provide a better understanding of the economics for improving software security and quality. Prior work in this area has primarily been limited to case studies of a small handful of software projects. Aims: We investigate the effect of modern code review on software security. We extend and generalize prior work that has looked at code review and software quality. Method: We gather a very large dataset from GitHub (3,126 projects in 143 languages, with 489,038 issues and 382,771 pull requests), and use a combination of quantification techniques and multiple regression modeling to study the relationship between code review coverage and participation and software quality and security. Results: We find that code review coverage has a significant effect on software security. We confirm prior results that found a relationship between code review coverage and software defects. Most notably, we find evidence of a negative relationship between code review of pull requests and the number of security bugs reported in a project. Conclusions: Our results suggest that implementing code review policies within the pull request model of development may have a positive effect on the quality and security of software.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {83–92},
numpages = {10},
keywords = {code review, mining software repositories, multiple regression models, quantification models, software quality, software security},
location = {Toronto, Canada},
series = {PROMISE}
}

@inproceedings{10.1007/978-3-030-87196-3_28,
author = {Wu, Yicheng and Xu, Minfeng and Ge, Zongyuan and Cai, Jianfei and Zhang, Lei},
title = {Semi-supervised Left Atrium Segmentation with Mutual Consistency&nbsp;Training},
year = {2021},
isbn = {978-3-030-87195-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87196-3_28},
doi = {10.1007/978-3-030-87196-3_28},
abstract = {Semi-supervised learning has attracted great attention in the field of machine learning, especially for medical image segmentation tasks, since it alleviates the heavy burden of collecting abundant densely annotated data for training. However, most of existing methods underestimate the importance of challenging regions (e.g. small branches or blurred edges) during training. We believe that these unlabeled regions may contain more crucial information to minimize the uncertainty prediction for the model and should be emphasized in the training process. Therefore, in this paper, we propose a novel Mutual Consistency Network (MC-Net) for semi-supervised left atrium segmentation from 3D MR images. Particularly, our MC-Net consists of one encoder and two slightly different decoders, and the prediction discrepancies of two decoders are transformed as an unsupervised loss by our designed cycled pseudo label scheme to encourage mutual consistency. Such mutual consistency encourages the two decoders to have consistent and low-entropy predictions and enables the model to gradually capture generalized features from these unlabeled challenging regions. We evaluate our MC-Net on the public Left Atrium (LA) database and it obtains impressive performance gains by exploiting the unlabeled data effectively. Our MC-Net outperforms six recent semi-supervised methods for left atrium segmentation, and sets the new state-of-the-art performance on the LA database.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II},
pages = {297–306},
numpages = {10},
keywords = {Cycled pseudo label, Mutual consistency, Semi-supervised learning},
location = {Strasbourg, France}
}

@inproceedings{10.1145/3364641.3364652,
author = {Caldeira, J\'{u}nea Eliza Brand\~{a}o and Yoshioka, S\'{e}rgio Roberto Imaeda and de Oliveira Rodrigues, Bruno Rafael and Parreiras, Fernando Silva},
title = {Multiobjective Optimization Using Evolutionary Algorithms in Agile Teams Allocation},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364652},
doi = {10.1145/3364641.3364652},
abstract = {Ensuring that the team meets project requirements is essential to ensure the software quality available in the market and the success of the project. In this context, this study evaluated three algorithms for optimizing software engineering problems: NSGAII, SPEA2 and MOCell, in order to support project managers in the composition of agile software development teams. These algorithms were tested in an experiment carried out in a software development company and evaluated in four projects recently executed by the company. The approach considered the characteristics of the project activities, available human resources, human resource profile, project constraints (scope and time for execution) and constraints established by the organization. The algorithms returned solutions with the number of resources needed to carry out the project, as well as resources such as more project qualification, lower cost, and productivity adequate for the term established by the client. The results showed that the three algorithms evaluated presented consistent performances. The NSGAII and SPEA2 had very similar results and behavior, whereas the MOCell presented a better performance in the computational effort and needed a larger population for its saturation.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {89–98},
numpages = {10},
keywords = {Resource Allocation, Multiobjective Optimization, Agile Teams},
location = {Fortaleza, Brazil},
series = {SBQS '19}
}

@inproceedings{10.1145/3278186.3278194,
author = {Santiago, Dionny and Clarke, Peter J. and Alt, Patrick and King, Tariq M.},
title = {Abstract flow learning for web application test generation},
year = {2018},
isbn = {9781450360531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278186.3278194},
doi = {10.1145/3278186.3278194},
abstract = {Achieving high software quality today involves manual analysis, test planning, documentation of testing strategy and test cases, and the development of scripts to support automated regression testing. To keep pace with software evolution, test artifacts must also be frequently updated. Although test automation practices help mitigate the cost of regression testing, a large gap exists between the current paradigm and fully automated software testing. Researchers and practitioners are realizing the potential for artificial intelligence and machine learning (ML) to help bridge the gap between the testing capabilities of humans and those of machines. This paper presents an ML approach that combines a language specification that includes a grammar that can be used to describe test flows, and a trainable test flow generation model, in order to generate tests in a way that is trainable, reusable across different applications, and generalizable to new applications.},
booktitle = {Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation},
pages = {49–55},
numpages = {7},
keywords = {Testing, Test generation, Machine learning, Language, Automation},
location = {Lake Buena Vista, FL, USA},
series = {A-TEST 2018}
}

@inproceedings{10.1145/3001867.3001874,
author = {Queiroz, Rodrigo and Berger, Thorsten and Czarnecki, Krzysztof},
title = {Towards predicting feature defects in software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001874},
doi = {10.1145/3001867.3001874},
abstract = {Defect-prediction techniques can enhance the quality assurance activities for software systems. For instance, they can be used to predict bugs in source files or functions. In the context of a software product line, such techniques could ideally be used for predicting defects in features or combinations of features, which would allow developers to focus quality assurance on the error-prone ones. In this preliminary case study, we investigate how defect prediction models can be used to identify defective features using machine-learning techniques. We adapt process metrics and evaluate and compare three classifiers using an open-source product line. Our results show that the technique can be effective. Our best scenario achieves an accuracy of 73 % for accurately predicting features as defective or clean using a Naive Bayes classifier. Based on the results we discuss directions for future work.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {58–62},
numpages = {5},
keywords = {software product lines, features, defect prediction},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.5555/3546258.3546440,
author = {Klink, Pascal and Abdulsamad, Hany and Belousov, Boris and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {A probabilistic interpretation of self-paced learning with applications to reinforcement learning},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {182},
numpages = {52},
keywords = {rl-as-inference, tempered inference, self-paced learning, reinforcement learning, curriculum learning}
}

@inproceedings{10.1145/3474085.3481541,
author = {Huang, Lianghua and Liu, Yu and Zhou, Xiangzeng and You, Ansheng and Li, Ming and Wang, Bin and Zhang, Yingya and Pan, Pan and Yinghui, Xu},
title = {Once and for All: Self-supervised Multi-modal Co-training on One-billion Videos at Alibaba},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481541},
doi = {10.1145/3474085.3481541},
abstract = {Videos grow to be one of the largest mediums on the Internet. E-commerce platforms like Alibaba need to process millions of video data across multimedia (e.g., visual, audio, image, and text) and on a variety of tasks (e.g., retrieval, tagging, and summary) every day. In this work, we aim to develop a once and for all pretraining technique for diverse modalities and downstream tasks. To achieve this, we make the following contributions: (1) We propose a self-supervised multi-modal co-training framework. It takes cross-modal pseudo-label consistency as the supervision and can jointly learn representations of multiple modalities. (2) We introduce several novel techniques (e.g., sliding-window subset sampling, coarse-to-fine clustering, fast spatial-temporal convolution and parallel data transmission and processing) to optimize the training process, making billion-scale stable training feasible. (3) We construct a large-scale multi-modal dataset consisting of 1.4 billion videos (~0.5 PB) and train our framework on it. The training takes only 4.6 days on an in-house 256 GPUs cluster, and it simultaneously produces pretrained video, audio, image, motion, and text networks. (4) Finetuning from our pretrained models, we obtain significant performance gains and faster convergence on diverse multimedia tasks at Alibaba. Furthermore, we also validate the learned representation on public datasets. Despite the domain gap between our commodity-centric pretraining and the action-centric evaluation data, we show superior results against state-of-the-arts.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1148–1156},
numpages = {9},
keywords = {self-supervised learning, once and for all, multi-modal, co-training},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3194718.3194730,
author = {Sarro, Federica},
title = {Predictive analytics for software testing: keynote paper},
year = {2018},
isbn = {9781450357418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194718.3194730},
doi = {10.1145/3194718.3194730},
abstract = {This keynote discusses the use of Predictive Analytics for Software Engineering, and in particular for Software Defect Prediction and Software Testing, by presenting the latest results achieved in these fields leveraging Artificial Intelligence, Search-based and Machine Learning methods, and by giving some directions for future work.},
booktitle = {Proceedings of the 11th International Workshop on Search-Based Software Testing},
pages = {1},
numpages = {1},
keywords = {search-based predictive modelling, predictive analytics},
location = {Gothenburg, Sweden},
series = {SBST '18}
}

@article{10.1007/s11219-019-09492-z,
author = {Bunke, Michaela and Sohr, Karsten},
title = {Towards supporting software assurance assessments by detecting security patterns},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09492-z},
doi = {10.1007/s11219-019-09492-z},
abstract = {Today, many tools exist that attempt to find possible vulnerabilities in Android applications, e.g., FlowDroid, Fortify, or AppScan. However, all these tools aim to detect vulnerabilities or (sometimes) tainted flows and present the reviewer detected possible issues of an analyzed Android application. None of these tools supports the identification of implemented security features in code, although this aspect is also relevant to developers as well as reviewers. To address this open problem, we present a program comprehension approach based on connected object process graphs (COPGs) containing interacting objects described by security patterns in this paper. The feasibility of our approach is evaluated qualitatively with 25 security-critical Android applications from Google Play with almost 7 million lines of code. We currently support 17 security pattern variants with about 199 correctly detected pattern instances in the apps. We also define a benchmark of non-trivial, security-critical Android apps, which can also be used for other security analysis tasks based on the static analysis framework Soot. With this benchmark, our analysis yields a precision of 99% and a recall of 80%. Finally, we discussed our approach and the developed tool with six software security experts from the SAFECode organization to obtain additional feedback.},
journal = {Software Quality Journal},
month = dec,
pages = {1711–1753},
numpages = {43},
keywords = {Static program analysis, Security patterns, Android, Software assessment}
}

@article{10.1109/TSE.2005.112,
author = {Gyimothy, Tibor and Ferenc, Rudolf and Siket, Istvan},
title = {Empirical Validation of Object-Oriented Metrics on Open Source Software for Fault Prediction},
year = {2005},
issue_date = {October 2005},
publisher = {IEEE Press},
volume = {31},
number = {10},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2005.112},
doi = {10.1109/TSE.2005.112},
abstract = {Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database called Bugzilla using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.},
journal = {IEEE Trans. Softw. Eng.},
month = oct,
pages = {897–910},
numpages = {14},
keywords = {reverse engineering, open source software, metrics validation, fault-proneness detection, compiler wrapping, Mozilla, Index Terms- Fact extraction, Columbus., C++, Bugzilla}
}

@article{10.1007/s11219-009-9079-6,
author = {Singh, Yogesh and Kaur, Arvinder and Malhotra, Ruchika},
title = {Empirical validation of object-oriented metrics for predicting fault proneness models},
year = {2010},
issue_date = {March     2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-009-9079-6},
doi = {10.1007/s11219-009-9079-6},
abstract = {Empirical validation of software metrics used              to predict software quality attributes is important to ensure their practical relevance in software organizations. The aim of this work is to find the relation of object-oriented (OO) metrics with fault proneness at different severity levels of faults. For this purpose, different prediction models have been developed using regression and machine learning methods. We evaluate and compare the performance of these methods to find which method performs better at different severity levels of faults and empirically validate OO metrics given by Chidamber and Kemerer. The results of the empirical study are based on public domain NASA data set. The performance of the predicted models was evaluated using Receiver Operating Characteristic (ROC) analysis. The results show that the area under the curve (measured from the ROC analysis) of models predicted using high severity faults is low as compared with the area under the curve of the model predicted with respect to medium and low severity faults. However, the number of faults in the classes correctly classified by predicted models with respect to high severity faults is not low. This study also shows that the performance of machine learning methods is better than logistic regression method with respect to all the severities of faults. Based on the results, it is reasonable to claim that models targeted at different severity levels of faults could help for planning and executing testing by focusing resources on fault-prone parts of the design and code that are likely to cause serious failures.},
journal = {Software Quality Journal},
month = mar,
pages = {3–35},
numpages = {33},
keywords = {Software quality, Receiver operating characteristics analysis, Object-oriented, Metrics, Fault prediction, Empirical validation}
}

@article{10.1016/j.jss.2004.05.001,
author = {Thwin, Mie Mie Thet and Quah, Tong-Seng},
title = {Application of neural networks for software quality prediction using object-oriented metrics},
year = {2005},
issue_date = {May 2005},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {2},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2004.05.001},
doi = {10.1016/j.jss.2004.05.001},
abstract = {This paper presents the application of neural networks in software quality estimation using object-oriented metrics. In this paper, two kinds of investigation are performed. The first on predicting the number of defects in a class and the second on predicting the number of lines changed per class. Two neural network models are used, they are Ward neural network and General Regression neural network (GRNN). Object-oriented design metrics concerning inheritance related measures, complexity measures, cohesion measures, coupling measures and memory allocation measures are used as the independent variables. GRNN network model is found to predict more accurately than Ward network model.},
journal = {J. Syst. Softw.},
month = may,
pages = {147–156},
numpages = {10}
}

@article{10.1007/s11219-018-9437-3,
author = {Ma, Tao and Ali, Shaukat and Yue, Tao and Elaasar, Maged},
title = {Testing self-healing cyber-physical systems under uncertainty: a fragility-oriented approach},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9437-3},
doi = {10.1007/s11219-018-9437-3},
abstract = {As an essential feature of smart cyber-physical systems (CPSs), self-healing behaviors play a major role in maintaining the normality of CPSs in the presence of faults and uncertainties. It is important to test whether self-healing behaviors can correctly heal faults under uncertainties to ensure their reliability. However, the autonomy of self-healing behaviors and impact of uncertainties make it challenging to conduct such testing. To this end, we devise a fragility-oriented testing approach, which is comprised of two novel algorithms: fragility-oriented testing (FOT) and uncertainty policy optimization (UPO). The two algorithms utilize the fragility, obtained from test executions, to learn the optimal policies for invoking operations and introducing uncertainties, respectively, to effectively detect faults. We evaluated their performance by comparing them against a coverage-oriented testing (COT) algorithm and a random uncertainty generation method (R). The evaluation results showed that the fault detection ability of FOT+UPO was significantly higher than the ones of FOT+R, COT+UPO, and COT+R, in 73 out of 81 cases. In the 73 cases, FOT+UPO detected more than 70% of faults, while the others detected 17% of faults, at the most.},
journal = {Software Quality Journal},
month = jun,
pages = {615–649},
numpages = {35},
keywords = {Uncertainty, Self-healing, Reinforcement learning, Model execution, Cyber-physical systems}
}

@inproceedings{10.1145/3278142.3278145,
author = {Tu, Huy and Nair, Vivek},
title = {Is one hyperparameter optimizer enough?},
year = {2018},
isbn = {9781450360562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278142.3278145},
doi = {10.1145/3278142.3278145},
abstract = {Hyperparameter tuning is the black art of automatically finding a good combination of control parameters for a data miner. While widely applied in empirical Software Engineering, there has not been much discussion on which hyperparameter tuner is best for software analytics.To address this gap in the literature, this paper applied a range of hyperparameter optimizers (grid search, random search, differential evolution, and Bayesian optimization) to a defect prediction problem. Surprisingly, no hyperparameter optimizer was observed to be “best” and, for one of the two evaluation measures studied here (F-measure), hyperparameter optimization, in 50% of cases, was no better than using default configurations. We conclude that hyperparameter optimization is more nuanced than previously believed. While such optimization can certainly lead to large improvements in the performance of classifiers used in software analytics, it remains to be seen which specific optimizers should be applied to a new dataset.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Software Analytics},
pages = {19–25},
numpages = {7},
keywords = {SBSE, Hyperparameter Tuning, Defect Prediction},
location = {Lake Buena Vista, FL, USA},
series = {SWAN 2018}
}

