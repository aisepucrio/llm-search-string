@inproceedings{10.1145/2970276.2970288,
author = {Schw\"{a}gerl, Felix and Westfechtel, Bernhard},
title = {SuperMod: tool support for collaborative filtered model-driven software product line engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970288},
doi = {10.1145/2970276.2970288},
abstract = {The increase in productivity implied by model-driven software product line engineering is weakened by the complexity exposed to the user having to manage a multi-variant model. Recently, a new paradigm has emerged: filtered software product line engineering transfers the established check-out/modify/commit workflow from version control to variability management, allowing to iteratively develop the multi-variant model in a single-variant view. This paper demonstrates SuperMod, a tool that supports collaborative filtered model-driven product line engineering, implemented for and with the Eclipse Modeling Framework. Concerning variability management, the tool offers capabilities for editing feature models and specifying feature configurations, both being well-known formalisms in product line engineering. Furthermore, collaborative editing of product lines is provided through distributed version control. The accompanying video shows that SuperMod seamlessly integrates into existing tool landscapes, reduces the complexity of multi-variant editing, automates a large part of variability management, and ensures consistency. A tool demonstration video is available here: http://youtu.be/5XOk3x5kjFc},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {822–827},
numpages = {6},
keywords = {Model-driven software engineering, filtered editing, software product line engineering, version control},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1007/978-3-319-35122-3_5,
author = {Schaefer, Ina and Seidl, Christoph and Cleophas, Loek and Watson, Bruce W.},
title = {Tax-PLEASE--Towards Taxonomy-Based Software Product Line Engineering},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_5},
doi = {10.1007/978-3-319-35122-3_5},
abstract = {Modern software systems, in particular in mobile and cloud-based applications, exist in many different variants in order to adapt to changing user requirements or application contexts. Software product line engineering allows developing these software systems by managed large-scale reuse in order to achieve shorter time to market. Traditional software product line engineering approaches use a domain variability model which only captures the configuration options of the product variants, but does not provide any guideline for designing and implementing reusable artifacts. In contrast, software taxonomies structure software domains from an abstract specification of the functionality to concrete implementable variants by successive correctness-preserving refinements. In this paper, we propose a novel software product line engineering process based on a taxonomy-based domain analysis. The taxonomy's hierarchy provides guidelines for designing and implementing the product line's reusable artifacts while at the same time specifying possible configuration options. By deriving reusable product line artifacts from a software taxonomy, the well-defined structuring of the reusable artifacts yields improved maintainability and evolvability of the product line.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {63–70},
numpages = {8},
keywords = {Software Product Line SPL, Taxonomy-Based Software Construction TABASCO},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {deep neural networks, machine learning, software product lines, transfer learning, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3475157,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Ayala, Inmaculada and Kr\"{u}ger, Jacob and Mosser, S\'{e}bastien},
title = {International Workshop on Variability Management for Modern Technologies (VM4ModernTech 2021)},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3475157},
doi = {10.1145/3461001.3475157},
abstract = {Variability is an inherent property of software systems that allows developers to deal with the needs of different customers and environments, creating a family of related systems. Variability can be managed in an opportunistic fashion, for example, using clone-and-own, or by employing a systematic approach, for instance, using a software product line (SPL). In the SPL community, variability management has been discussed for systems in various domains, such as defense, avionics, or finance, and for different platforms, such as desktops, web applications, or embedded systems. Unfortunately, other research communities---particularly those working on modern technologies, such as microservice architectures, cyber-physical systems, robotics, cloud computing, autonomous driving, or ML/AI-based systems---are less aware of the state-of-the-art in variability management, which is why they face similar problems and start to redeveloped the same solutions as the SPL community already did. With the International Workshop on Variability Management for Modern Technologies, we aim to foster and strengthen synergies between the communities researching variability management and modern technologies. More precisely, we aim to attract researchers and practitioners to contribute processes, techniques, tools, empirical studies, and problem descriptions or solutions that are related to reuse and variability management for modern technologies. By inviting different communities and establishing collaborations between them, we hope that the workshop can raise the interest of researchers outside the SPL community for variability management, and thus reduce the extent of costly redevelopments in research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {202},
numpages = {1},
keywords = {software architecture, variability management},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1007/978-3-319-35122-3_2,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Automated Composition of Service Mashups Through Software Product Line Engineering},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_2},
doi = {10.1007/978-3-319-35122-3_2},
abstract = {The growing number of online resources, including data and services, has motivated both researchers and practitioners to provide methods and tools for non-expert end-users to create desirable applications by putting these resources together leading to the so called mashups. In this paper, we focus on a class of mashups referred to as service mashups. A service mashup is built from existing services such that the developed service mashup offers added-value through new functionalities. We propose an approach which adopts concepts from software product line engineering and automated AI planning to support the automated composition of service mashups. One of the advantages of our work is that it allows non-experts to build and optimize desired mashups with little knowledge of service composition. We report on the results of the experimentation that we have performed which support the practicality and scalability of our proposed work.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {20–38},
numpages = {19},
keywords = {Automated composition, Feature model, Planning, Service mashups, Software product lines, Workflow optimization},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/3489849.3489948,
author = {Lebiedz, Jacek and Wiszniewski, Bogdan},
title = {CAVE applications: from craft manufacturing to product line engineering},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489948},
doi = {10.1145/3489849.3489948},
abstract = {Product line engineering model is suitable for engineering related software products in an efficient manner, taking advantage of their similarities while managing their differences. Our feature driven software product line (SPL) solution based on that model allows for instantiation of different CAVE products based on the set of core assets and driven by a set of common VR features with the minimal budget and time to market.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {57},
numpages = {2},
keywords = {VR application features, core assets, production stations},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3233027.3233038,
author = {Martinez, Jabier and T\"{e}rnava, Xhevahire and Ziadi, Tewfik},
title = {Software product line extraction from variability-rich systems: the robocode case study},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233038},
doi = {10.1145/3233027.3233038},
abstract = {The engineering of a Software Product Line (SPL), either by creating it from scratch or through the re-engineering of existing variants, it uses to be a project that spans several years with a high investment. It is often hard to analyse and quantify this investment, especially in the context of extractive SPL adoption when the related software variants are independently created by different developers following different system architectures and implementation conventions. This paper reports an experience on the creation of an SPL by re-engineering system variants implemented around an educational game called Robocode. The objective of this game is to program a bot (a battle tank) that battles against the bots of other developers. The world-wide Robocode community creates and maintains a large base of knowledge and implementations that are mainly organized in terms of features, although not presented as an SPL. Therefore, a group of master students analysed this variability-rich domain and extracted a Robocode SPL. We present the results of such extraction augmented with an analysis and a quantification regarding the spent time and effort. We believe that the results and the a-posteriori analysis can provide insights on global challenges on SPL adoption. We also provide all the elements to SPL educators to reproduce the teaching activity, and we make available this SPL to be used for any research purpose.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {132–142},
numpages = {11},
keywords = {education, extractive software product line adoption, reverse-engineering, robocode, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3336294.3336310,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {Industrial and Academic Software Product Line Research at SPLC: Perceptions of the Community},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336310},
doi = {10.1145/3336294.3336310},
abstract = {We present preliminary insights into the perception of researchers and practitioners of the software product line (SPL) community on previous, current, and future research efforts. We were particularly interested in up-and-coming and outdated topics and whether the views of academics and industry researchers differ. Also, we compared the views of the community with the results of an earlier literature survey published at SPLC 2018. We conducted a questionnaire-based survey with attendees of SPLC 2018. We received 33 responses (about a third of the attendees) from both, very experienced attendees and younger researchers, and from academics as well as industry researchers. We report preliminary findings regarding popular and unpopular SPL topics, topics requiring further work, and industry versus academic researchers' views. Differences between academic and industry researchers become visible only when analyzing comments on open questions. Most importantly, while topics popular among respondents are also popular in the literature, topics respondents think require further work have often already been well researched. We conclude that the SPL community needs to do a better job preserving and communicating existing knowledge and particularly also needs to widen its scope.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {189–194},
numpages = {6},
keywords = {SPLC, academia, industry, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3106195.3106223,
author = {Iglesias, Aitziber and Lu, Hong and Arellano, Crist\'{o}bal and Yue, Tao and Ali, Shaukat and Sagardui, Goiuria},
title = {Product Line Engineering of Monitoring Functionality in Industrial Cyber-Physical Systems: A Domain Analysis},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106223},
doi = {10.1145/3106195.3106223},
abstract = {In recent years, manufacturing technology is evolving and progressively becoming more dynamic and complex. This means that manufacturing technology (e.g., based on Industry 4.0) should be able to control the production process at runtime by monitoring physical elements and adapting itself. Such functionality is aimed at increasing production effectiveness and reducing the production cost. We argue that monitoring process can be viewed as a software product line having commonalities and variability. To support our argument, we analyzed and conducted domain analysis of two monitoring systems of Industrial Cyber-Physical Systems (ICPSs) from two industrial domains including automated warehouses and press machines. Based on the domain analysis, we present a common solution for monitoring including a software product line. With such product line, a user can configure, monitor, and visualize data of an ICPS at runtime. However, such solution could not handle the dynamic functionality related to monitoring of ICPS. Thus, we propose the use of dynamic product line and present a set of research questions that must be addressed for such solution.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {195–204},
numpages = {10},
keywords = {Cyber Physical System, Dynamic Software Product Line, Industrial domains, Software Product Line},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {NLTK, feature model extraction, natural language processing, requirements engineering, software product line},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1007/978-3-030-61362-4_5,
author = {Damiani, Ferruccio and Lienhardt, Michael and Paolini, Luca},
title = {On Slicing Software Product Line Signatures},
year = {2020},
isbn = {978-3-030-61361-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61362-4_5},
doi = {10.1007/978-3-030-61362-4_5},
abstract = {A Software Product Line (SPL) is a family of similar programs (called variants) generated from a common artifact base. Variability in an SPL can be documented in terms of abstract description of functionalities (called features): a feature model (FM) identifies each variant by a set of features (called a product). Delta-orientation is a flexible approach to implement SPLs. An SPL Signature (SPLS) is a variability-aware Application Programming Interface (API), i.e., an SPL where each variant is the API of a program. In this paper we introduce and formalize, by abstracting from SPL implementation approaches, the notion of slice of an SPLS K for a set of features F (i.e., an SPLS obtained from by K by hiding the features that are not in F). Moreover, we formulate the challenge of defining an efficient algorithm that, given a delta-oriented SPLS K and a set of features F, sreturns a delta-oriented SPLS that is an slice of K for F. Thus paving the way for further research on devising such an algorithm. The proposed notions are formalized for SPLs of programs written in an imperative version of Featherweight Java.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Verification Principles: 9th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2020, Rhodes, Greece, October 20–30, 2020, Proceedings, Part I},
pages = {81–102},
numpages = {22},
location = {Rhodes, Greece}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {clustering, configuration workflow, process discovery, process mining, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3297156.3297203,
author = {Hasbi, Muhamad and Budiardjo, Eko K. and Wibowo, Wahyu C.},
title = {Reverse Engineering in Software Product Line - A Systematic Literature Review},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297203},
doi = {10.1145/3297156.3297203},
abstract = {Reverse engineering is the information extraction process on system by identifying and analyzing the components that are part of that system. We analyze existing research that related with reverse engineering process on software product line. There are two product line processes according to Software product line engineering framework they are domain engineering process and application engineering process. We investigate reverse engineering in domain engineering process (domain requirements, domain design, and domain realization, domain quality assurance). We performed a systematic literature review. A manual search resulting 71 papers considered for analysis. Results: The majority of reverse engineering studied in three domain activity in domain engineering process. That is requirement engineering, domain design and domain realization. There are inconsistent correlations between features in the reverse engineering process. These approaches extract features without constraints between its features. Conclusions: Reverse engineering methods are needed that are able to identify and maintain a consistent correlation between features in application engineering and domain engineering in the reverse engineering process. Finally, we provide gaps from existing research and show opportunities for future research.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {174–179},
numpages = {6},
keywords = {Reverse engineering, domain engineering, software product line, systematic review},
location = {Shenzhen, China},
series = {CSAI '18}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Artificial intelligence, Automated feature selection, Inconsistencies, Industrial SPL tools, Literature review, Predictive analytics, Software product line}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {Software product lines, product-line analysis}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {configurable systems, machine learning, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2934466.2934483,
author = {Richenhagen, Johannes and Rumpe, Bernhard and Schlo\ss{}er, Axel and Schulze, Christoph and Thissen, Kevin and von Wenckstern, Michael},
title = {Test-driven semantical similarity analysis for software product line extraction},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934483},
doi = {10.1145/2934466.2934483},
abstract = {Software product line engineering rests upon the assumption that a set of products share a common base of similar functionality. The correct identification of similarities between different products can be a time-intensive task. Hence, this paper proposes an automated semantical similarity analysis supporting software product line extraction and maintenance. Under the assumption of an already identified compatible interface, the degree of semantical similarity is identified based on provided test cases. Therefore, the analysis can also be applied in a test-driven development. This is done by translating available test sequences for both components into two I/O extended finite automata and performing an abstraction of the defined behavior until a simulation relation is established. The test-based approach avoids complexity issues regarding the state space explosion problem, a common issue in model checking. The proposed approach is applied on different variants and versions of industrially used software components provided by an automotive supplier to demonstrate the method's applicability.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {174–183},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/1321631.1321730,
author = {Dhungana, Deepak and Rabiser, Rick and Gr\"{u}nbacher, Paul and Neumayer, Thomas},
title = {Integrated tool support for software product line engineering},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321730},
doi = {10.1145/1321631.1321730},
abstract = {Product line engineering comprises many heterogeneous activities such as capturing the variability of reusable assets, supporting the derivation of products from the product line, evolving the product line, or tailoring the approach to the specifics of a domain. The inherent complexity of product lines implicates that tool support is inevitable to facilitate smooth performance and to avoid costly errors. Product line engineering tools have to support heterogeneous stakeholders involved in diverse activities. Tool integration therefore is of particular importance to foster their seamless cooperation. However, the integration is difficult to achieve due to the diversity of models and work products. This paper describes the DOPLER tool suite which has been developed to provide such integrated support. The tool suite is flexible and extensible to support domain-specific needs},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {533–534},
numpages = {2},
keywords = {model evolution, multi-team modeling, product derivation, product line engineering, product line tools, variability modeling},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@inproceedings{10.1145/3349341.3349504,
author = {Huang, Chunfang and Wang, Xiangrong},
title = {Financial Innovation Based on Artificial Intelligence Technologies},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349504},
doi = {10.1145/3349341.3349504},
abstract = {Nowadays, the degree of the heated topic of artificial intelligence in the world reaches a new height. Due to the breakthrough of deep learning algorithm based on neural network, the level of artificial intelligence technologies has been enhanced significantly. The global financial industry is quietly changing under the catalysis of artificial intelligence. The frontier artificial intelligence technologies, such as the technology of expert system, machine learning and knowledge discovery in database are combed to explore the financial applications of artificial intelligence. Based on these key technologies, this paper proposed three applications of artificial intelligence in the financial field, including intelligent investment adviser, transaction forecast and financial regulation, discusses the key technologies of artificial intelligence and financial innovation products based on these technologies, such as the functions of the transaction prediction system based on artificial intelligence technologies include forecast analysis, index statistics, stock analysis and information retrieval, etc. The structures of the systems are drawn and the design principles are provided. Finally, to guard the safety of the applications of artificial intelligence, the paper gives the suggestions of enhancing identity authentication, introducing monitoring measures and limiting autonomy degree.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {750–754},
numpages = {5},
keywords = {Deep Learning, Financial Regulation, Intelligent Investment Adviser, Machine Learning, Transaction Forecast},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {configurable systems, machine learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.artmed.2021.102165,
author = {de Siqueira, Vilson Soares and Borges, Mois\'{e}s Marcos and Furtado, Rog\'{e}rio Gomes and Dourado, Colandy Nunes and da Costa, Ronaldo Martins},
title = {Artificial intelligence applied to support medical decisions for the automatic analysis of echocardiogram images: A systematic review},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102165},
doi = {10.1016/j.artmed.2021.102165},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {19},
keywords = {Echocardiogram, Echocardiography, Machine Learning, Deep Learning}
}

@inproceedings{10.1109/CEC48606.2020.9185675,
author = {Ibias, Alfredo and Llana, Luis},
title = {Feature Selection using Evolutionary Computation Techniques for Software Product Line Testing},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC48606.2020.9185675},
doi = {10.1109/CEC48606.2020.9185675},
abstract = {Software product lines are an excellent mechanism in the development of software. Testing software product lines is an intensive process where selecting the right features where to focus it can be a critical task. Selecting the best combination of features from a software product line is a complex problem addressed in the literature. In this paper, we address the problem of finding the combination of features with the highest probability of being requested from a software product line with probabilities. We use Evolutive Computation techniques to address this problem. Specifically, we use the Ant Colony Optimization algorithm to find the best combination of features. Our results report that our framework overcomes the limitations of the brute force algorithm.},
booktitle = {2020 IEEE Congress on Evolutionary Computation (CEC)},
pages = {1–8},
numpages = {8},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Variability management, Software product lines, Multiple-case study, Challenges}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {configuration management, microservice platforms, product families, technologies and concepts, variability management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1016/j.ic.2021.104787,
author = {Sioutis, Michael and Wolter, Diedrich},
title = {Dynamic branching in qualitative constraint-based reasoning via counting local models},
year = {2021},
issue_date = {Dec 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {281},
number = {C},
issn = {0890-5401},
url = {https://doi.org/10.1016/j.ic.2021.104787},
doi = {10.1016/j.ic.2021.104787},
journal = {Inf. Comput.},
month = dec,
numpages = {15},
keywords = {Qualitative constraints, Spatial and temporal reasoning, Counting local models, Dynamic branching, Adaptive algorithm}
}

@inproceedings{10.1145/3289402.3289504,
author = {Sebbaq, Hanane and Retbi, Asmaa and Idrissi, Mohammed Khalidi and Bennani, Samir},
title = {Software Product Line to overcome the variability issue in E-Learning: Systematic literature review},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289504},
doi = {10.1145/3289402.3289504},
abstract = {The disparity of educational technologies, pedagogies and learning styles implies a problem of variability when modeling E-learning systems. Furthermore, the current learning context, which has become very open and heterogeneous, raises the problem of automating the modeling, development and maintenance of personalized E-learning systems based on various pedagogies. For its part, the "Software Product Line" is a paradigm that aims to produce product families based on the principles of reuse, configuration and derivation. The main purpose of this literature review is to explore the different potential applications of "SPL" in the E-learning domain to figure out the problem of variability. We will adopt a protocol for a systematic review of literature, after which we will draw up an analysis report.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {4},
numpages = {8},
keywords = {E-learning, Software Product line, Variability, heterogeneity, scale, systematic literature review, variety},
location = {Rabat, Morocco},
series = {SITA'18}
}

@inproceedings{10.1145/3377024.3380451,
author = {Bencomo, Nelly},
title = {Next steps in variability management due to autonomous behaviour and runtime learning},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3380451},
doi = {10.1145/3377024.3380451},
abstract = {One of the basic principles in product lines is to delay design decisions related to offered functionality and quality to later phases of the life cycle [25]. Instead of deciding on what system to develop in advance, a set of assets and a common reference architecture are specified and implemented during the Domain Engineering process. Later on, during Application Engineering, specific systems are developed to satisfy the requirements reusing the assets and architecture [16]. Traditionally, this is during the Application Engineering when delayed design decisions are solved. The realization of this delay relies heavily on the use of variability in the development of product lines and systems. However, as systems become more interconnected and diverse, software architects cannot easily foresee the software variants and the interconnections between components. Consequently, a generic a priori model is conceived to specify the system's dynamic behaviour and architecture. The corresponding design decisions are left to be solved at runtime [13].Surprisingly, few research initiatives have investigated variability models at runtime [9]. Further, they have been applied only at the level of goals and architecture, which contrasts to the needs claimed by the variability community, i.e., Software Product Lines (SPLC) and Dynamic Software Product Lines (DSPL) [2, 10, 14, 22]. Especially, the vision of DSPL with their ability to support runtime updates with virtually zero downtime for products of a software product line, denotes the obvious need of variability models being used at runtime to adapt the corresponding programs. A main challenge for dealing with runtime variability is that it should support a wide range of product customizations under various scenarios that might be unknown until the execution time, as new product variants can be identified only at runtime [10, 11]. Contemporary variability models face the challenge of representing runtime variability to therefore allow the modification of variation points during the system's execution, and underpin the automation of the system's reconfiguration [15]. The runtime representation of feature models (i.e. the runtime model of features) is required to automate the decision making [9].Software automation and adaptation techniques have traditionally required a priori models for the dynamic behaviour of systems [17]. With the uncertainty present in the scenarios involved, the a priori model is difficult to define [20, 23, 26]. Even if foreseen, its maintenance is labour-intensive and, due to architecture decay, it is also prone to get out-of-date. However, the use of models@runtime does not necessarily require defining the system's behaviour model beforehand. Instead, different techniques such as machine learning, or mining software component interactions from system execution traces can be used to build a model which is in turn used to analyze, plan, and execute adaptations [18], and synthesize emergent software on the fly [7].Another well-known problem posed by the uncertainty that characterize autonomous systems is that different stakeholders (e.g. end users, operators and even developers) may not understand them due to the emergent behaviour. In other words, the running system may surprise its customers and/or developers [4]. The lack of support for explanation in these cases may compromise the trust to stakeholders, who may eventually stop using a system [12, 24]. I speculate that variability models can offer great support for (i) explanation to understand the diversity of the causes and triggers of decisions during execution and their corresponding effects using traceability [5], and (ii) better understand the behaviour of the system and its environment.Further, an extension and potentially reframing of the techniques associated with variability management may be needed to help taming uncertainty and support explanation and understanding of the systems. The use of new techniques such as machine learning exacerbates the current situation. However, at the same time machine learning techniques can also help and be used, for example, to explore the variability space [1]. What can the community do to face the challenges associated?We need to meaningfully incorporate techniques from areas such as artificial intelligence, machine learning, optimization, planning, decision theory, and bio-inspired computing into our variability management techniques to provide explanation and management of the diversity of decisions, their causes and the effects associated. My own previous work has progressed [3, 5, 6, 8, 11, 12, 19, 21] to reflect what was discussed above.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {2},
numpages = {2},
keywords = {autonomous systems, dynamic software product lines, dynamic variability, machine learning, uncertainty, variability management},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2648511.2648537,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina and Gimenes, Itana M. S. and Oizumi, Willian Nalepa},
title = {A search-based approach for software product line design},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648537},
doi = {10.1145/2648511.2648537},
abstract = {The Product Line Architecture (PLA) can be improved by taking into account key factors such as feature modularization, and by continuously evaluating its design according to metrics. Search-Based Software Engineering (SBSE) principles can be used to support an informed-design of PLAs. However, existing search-based design works address only traditional software design not considering intrinsic Software Product Line aspects. This paper presents MOA4PLA, a search-based approach to support the PLA design. It gives a multi-objective treatment to the design problem based on specific PLA metrics. A metamodel to represent the PLA and a novel search operator to improve feature modularization are proposed. Results point out that the application of MOA4PLA leads to PLA designs with well modularized features, contributing to improve features reusability and extensibility. It raises a set of solutions with different design trade-offs that can be used to improve the PLA design.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {237–241},
numpages = {5},
keywords = {multi-objective algorithms, searchbased PLA design, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1016/j.knosys.2019.104883,
author = {Ayala, Inmaculada and Amor, Mercedes and Horcas, Jose-Miguel and Fuentes, Lidia},
title = {A goal-driven software product line approach for evolving multi-agent systems in the Internet of Things},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {184},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.104883},
doi = {10.1016/j.knosys.2019.104883},
journal = {Know.-Based Syst.},
month = nov,
numpages = {18},
keywords = {Software product line, Evolution, Internet of Things, MAS-PL, Goal models, GORE}
}

@article{10.1016/j.chb.2017.04.026,
author = {Gharsellaoui, Hamza and Maazoun, Jihen and Bouassida, Nadia and Ahmed, Samir Ben and Ben-Abdallah, Hanene},
title = {A Software Product Line Design Based Approach for Real-time Scheduling of Reconfigurable Embedded Systems},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0747-5632},
url = {https://doi.org/10.1016/j.chb.2017.04.026},
doi = {10.1016/j.chb.2017.04.026},
journal = {Comput. Hum. Behav.},
month = feb,
numpages = {11},
keywords = {Real-time scheduling, Reconfigurable embedded systems, SPL design, UML marte}
}

@inproceedings{10.1145/2362536.2362580,
author = {Hamza, Haitham S. and Martinez, Jabier and Thurimella, Anil Kumar and Deogun, Jitender S.},
title = {Third International Workshop on Knowledge-Oriented Product Line Engineering (KOPLE 2012)},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362580},
doi = {10.1145/2362536.2362580},
abstract = {Software Product Line Engineering (PLE) exploits systematic reuse by identifying and methodically reusing software artifacts to develop different but related software systems. Developing Product Lines requires analysis skills to identify, model, and encode domain and product knowledge into artifacts that can be systematically reused across the development life-cycle. As such, Knowledge plays a paramount role in the success of the various activities of PLE. The objective of the KOPLE workshop series is to bring together SPL researchers and practitioners from academia and industry to investigate the role of Knowledge in PLE. Knowledge is usually encapsulated in PL architectures in a tacit or implicit way, and this may appear to be sufficient for industry to implement successful product lines. Nevertheless, KOPLE also aims to become a discussion forum about techniques and methods to convert from tacit to explicit Knowledge in PLE and to process and use this Knowledge for optimizing and innovating PLE processes.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {292–293},
numpages = {2},
keywords = {conceptual graphs, knowledge engineering, ontology, product lines, software reuse, tacit knowledge},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1007/s10270-015-0479-8,
author = {Devroey, Xavier and Perrouin, Gilles and Cordy, Maxime and Samih, Hamza and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Statistical prioritization for software product line testing: an experience report},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0479-8},
doi = {10.1007/s10270-015-0479-8},
abstract = {Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {153–171},
numpages = {19},
keywords = {D.2.5, D.2.7, Prioritization, Software product line testing, Statistical testing}
}

@article{10.1007/s10270-017-0614-9,
author = {Guizzo, Giovani and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {Applying design patterns in the search-based optimization of software product line architectures},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0614-9},
doi = {10.1007/s10270-017-0614-9},
abstract = {The design of the product line architecture (PLA) is a difficult activity that can benefit from the application of design patterns and from the use of a search-based optimization approach, which is generally guided by different objectives related, for instance, to cohesion, coupling and PLA extensibility. The use of design patterns for PLAs is a recent research field, not completely explored yet. Some works apply the patterns manually and for a specific domain. Approaches to search-based PLA design do not consider the usage of these patterns. To allow such use, this paper introduces a mutation operator named “Pattern-Driven Mutation Operator” that includes methods to automatically identify suitable scopes and apply the patterns Strategy, Bridge and Mediator with the search-based approach multi-objective optimization approach for PLA. A metamodel is proposed to represent and identify suitable scopes to receive each one of the patterns, avoiding the introduction of architectural anomalies. Empirical results are also presented, showing evidences that the use of the proposed operator produces a greater diversity of solutions and improves the quality of the PLAs obtained in the search-based optimization process, regarding the values of software metrics.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1487–1512},
numpages = {26},
keywords = {Design pattern, Search-based software engineering, Software product line architecture}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00014,
author = {Idowu, Samuel and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Asset management in machine learning: a survey},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00014},
doi = {10.1109/ICSE-SEIP52600.2021.00014},
abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {51–60},
numpages = {10},
keywords = {SE4AI, asset management, machine learning},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {preference-based multi-objective algorithms, search-based software engineering, software product line testing},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1007/978-3-031-08421-8_41,
author = {Giorgio, Lazzarinetti and Nicola, Massarenti and Fabio, Sgr\`{o} and Andrea, Salafia},
title = {Continuous Defect Prediction in CI/CD Pipelines: A Machine Learning-Based Framework},
year = {2021},
isbn = {978-3-031-08420-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08421-8_41},
doi = {10.1007/978-3-031-08421-8_41},
abstract = {Recent advances in information technology has led to an increasing number of applications to be developed and maintained daily by product teams. Ensuring that a software application works as expected and that it is absent of bugs requires a lot of time and resources. Thanks to the recent adoption of DevOps methodologies, it is often the case where code commits and application builds are centralized and standardized. Thanks to this new approach, it is now possible to retrieve log and build data to ease the development and management operations of product teams. However, even if such approaches include code control to detect unit or integration errors, they do not check for the presence of logical bugs that can raise after code builds. For such reasons in this work we propose a framework for continuous defect prediction based on machine learning algorithms trained on a publicly available dataset. The framework is composed of a machine learning model for detecting the presence of logical bugs in code on the basis of the available data generated by DevOps tools and a dashboard to monitor the software projects status. We also describe the serverless architecture we designed for hosting the aforementioned framework.},
booktitle = {AIxIA 2021 – Advances in Artificial Intelligence: 20th International Conference of the Italian Association for Artificial Intelligence, Virtual Event, December 1–3, 2021, Revised Selected Papers},
pages = {591–606},
numpages = {16},
keywords = {Continuous defect prediction, Machine learning, DevOps, Continuous integration}
}

@article{10.1007/s10664-014-9358-0,
author = {Koziolek, Heiko and Goldschmidt, Thomas and Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan and Gamer, Thomas and Aleksy, Markus},
title = {Assessing software product line potential: an exploratory industrial case study},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9358-0},
doi = {10.1007/s10664-014-9358-0},
abstract = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {411–448},
numpages = {38},
keywords = {Business case, Domain analysis, Software product lines}
}

@article{10.4018/IJWSR.2019010103,
author = {Sun, Chang-ai and Wang, Zhen and Wang, Ke and Xue, Tieheng and Aiello, Marco},
title = {Adaptive BPEL Service Compositions via Variability Management: A Methodology and Supporting Platform},
year = {2019},
issue_date = {January 2019},
publisher = {IGI Global},
address = {USA},
volume = {16},
number = {1},
issn = {1545-7362},
url = {https://doi.org/10.4018/IJWSR.2019010103},
doi = {10.4018/IJWSR.2019010103},
abstract = {Service-Oriented Architectures are a popular development paradigm to enable distributed applications constructed from independent web services. When coordinated, web services are an infrastructure to fulfill dynamic and vertical integration of business. They may face frequent changes of both requirements and execution environments. Static and predefined service compositions using business process execution language BPEL are not able to cater for such rapid and unpredictable context shifts. The authors propose a variability management-based adaptive and configurable service composition approach that treats changes as first-class citizens and consists of identifying, expressing, realizing, and managing changes of service compositions. The proposed approach is realized with a language called VxBPEL to support variability in service compositions and a platform for design, execution, analysis, and maintenance of VxBPEL-based service compositions. Four case studies validate the feasibility of the proposed approach while exhibiting good performance of the supporting platform.},
journal = {Int. J. Web Serv. Res.},
month = jan,
pages = {37–69},
numpages = {33},
keywords = {Adaptive Systems, Business Process Execution Language, Service Composition, Service Oriented Architectures, Variability Management}
}

@article{10.1016/j.advengsoft.2021.103029,
author = {Nagy, Enik\H{o} and Lovas, R\'{o}bert and Pintye, Istv\'{a}n and Hajnal, \'{A}kos and Kacsuk, P\'{e}ter},
title = {Cloud-agnostic architectures for machine learning based on Apache Spark},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {159},
number = {C},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2021.103029},
doi = {10.1016/j.advengsoft.2021.103029},
journal = {Adv. Eng. Softw.},
month = sep,
numpages = {9},
keywords = {Reference architectures, Big data, Artificial intelligence, Machine learning, Cloud computing, Orchestration, Distributed computing, Stream processing, Spark}
}

@article{10.1504/ijhpsa.2021.121025,
author = {Xie, Linjiang and Hang, Feilu and Guo, Wei and Lv, Yao and Ou, Wei and Shibly, F.H.A.},
title = {Network security defence system based on artificial intelligence and big data technology},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {3–4},
issn = {1751-6528},
url = {https://doi.org/10.1504/ijhpsa.2021.121025},
doi = {10.1504/ijhpsa.2021.121025},
abstract = {Communication network security is a defence against unwanted system modifications to access files and folders within a computer network. The increasing overlap between the physical and virtual realms of improved communication poses a problem of cybersecurity. In this research, the big data analytics-based security system (BDASS) has been proposed to improve the communication network's security defence system with artificial intelligence (AI). The big data sets representing multiple categories of data are used in big data analysis methods. AI offers algorithms that can think or learn and strengthen their behaviour. Automated systems currently are built on syntactic rules that are not necessarily sufficiently sophisticated to handle the degree of difficulty in the communication network system. The BDASS model achieves a less computation time ratio of 21.3%, misbehaviour detection ratio of 97.5%, attack prediction accuracy ratio of 95.6%, possibility ratio of 96.4%, and success rate of 98.7% compared to other methods.},
journal = {Int. J. High Perform. Syst. Archit.},
month = jan,
pages = {140–151},
numpages = {11},
keywords = {artificial intelligence, big data analytics, BDASS, big data analytics based security system, communication network security, cyber security}
}

@inproceedings{10.1145/1964138.1964139,
author = {Silva, Alan Pedro da and Costa, Evandro and Bittencourt, Ig Ibert and Brito, Patrick H. S. and Holanda, Olavo and Melo, Jean},
title = {Ontology-based software product line for building semantic web applications},
year = {2010},
isbn = {9781450305426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964138.1964139},
doi = {10.1145/1964138.1964139},
abstract = {The Software Product Lines (SPL) has proved very effective in building large-scale software. However, few works seek to adjust the approach of software product line to applications in the context of semantic web. This is because applications in this context assume the use of semantic services and intelligent agents. As a result, it is necessary that there are assets that provide adequate interoperability both semantic services and intelligent agents. In this sense, it is proposed in this paper the use of ontologies for the specification of entire a project of a SPL. With this, it can be a sufficiently formal specification that can be interpreted by both software engineers and computational algorithms.},
booktitle = {Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering},
articleno = {1},
numpages = {6},
keywords = {ontology, semantic web, software product line},
location = {Reno, Nevada},
series = {KOPLE '10}
}

@article{10.1145/3453444,
author = {Ashmore, Rob and Calinescu, Radu and Paterson, Colin},
title = {Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453444},
doi = {10.1145/3453444},
abstract = {Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic, and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence, and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our article provides a comprehensive survey of the state of the art in the assurance of ML, i.e., in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the machine learning lifecycle, i.e., of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The article begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {111},
numpages = {39},
keywords = {Machine learning lifecycle, assurance, assurance evidence, machine learning workflow, safety-critical systems}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Coverage, Debugging/fault localization, EXAM score, Genetic algorithm, Software product line, Test generation}
}

@inproceedings{10.1109/ASE.2015.106,
author = {Pietsch, Christopher and Kehrer, Timo and Kelter, Udo and Reuling, Dennis and Ohrndorf, Manuel},
title = {SiPL: a delta-based modeling framework for software product line engineering},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.106},
doi = {10.1109/ASE.2015.106},
abstract = {Model-based development has become a widely-used approach to implement software, e.g. for embedded systems. Models replace source code as primary executable artifacts in these cases. Software product line technologies for these domains must be able to generate models as instances of an SPL. This need is addressed among others by an implementation technology for SPLs known as delta modeling. Current approaches to delta modeling require deltas to be written manually using delta languages, and they offer only very limited support for creating and testing a network of deltas. This paper presents a new approach to delta modeling and a supporting tool suite: the abstract notion of a delta is refined to be a consistency-preserving edit script which is generated by comparing two models. The rich structure of edit scripts allows us to detect conflicts and further relations between deltas statically and to implement restructurings in delta sets such as the merging of two deltas. We illustrate the tooling using a case study.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {852–857},
numpages = {6},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {Software product line, configuration management, critical path analysis, product derivation, weighted approach}
}

@inproceedings{10.1145/2364412.2364425,
author = {Cordy, Maxime and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Towards an incremental automata-based approach for software product-line model checking},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364425},
doi = {10.1145/2364412.2364425},
abstract = {Most model-checking algorithms are based on automata theory. For instance, determining whether or not a transition system satisfies a Linear Temporal Logic (LTL) formula requires computing strongly connected component of its transition graph. In Software Product-Line (SPL) engineering, the model checking problem is more complex due to the huge amount of software products that may compose the line. Indeed, one has to determine the exact subset of those products that do not satisfy an intended property. Efficient dedicated verification methods have been recently developed to answer this problem. However, most of them does not allow incremental verification. In this paper, we introduce an automata-based incremental approach for SPL model checking. Our method makes use of previous results to determine whether or not the addition of conservative features (i.e., features that do not remove behaviour from the system) preserves the satisfaction of properties expressed in LTL. We provide a detailed description of the approach and propose algorithms that implement it. We discuss how our method can be combined with SPL dedicated verification methods, viz. Featured Transition Systems.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {74–81},
numpages = {8},
keywords = {model checking, modularity, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.eswa.2020.114161,
author = {Houssein, Essam H. and Emam, Marwa M. and Ali, Abdelmgeid A. and Suganthan, Ponnuthurai Nagaratnam},
title = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114161},
doi = {10.1016/j.eswa.2020.114161},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {20},
keywords = {Breast cancer classification, Convolutional neural network, Computer-aided diagnosis system (CAD), Deep learning, Histological images, Machine learning, Magnetic resonance imaging (MRI), Medical imaging modalities, Mammogram images, Ultrasound images, Thermography images}
}

@inproceedings{10.1145/2739482.2764650,
author = {Karimpour, Reza and Ruhe, Guenther},
title = {A Search Based Approach Towards Robust Optimization in Software Product Line Scoping},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764650},
doi = {10.1145/2739482.2764650},
abstract = {Software product line (SPL) scoping is important for planning upfront investment. One challenge with scoping comes from inaccuracies in estimated parameters and uncertainty in environment. In this paper, a method to incorporate uncertainty in SPL scoping optimization and its application to generate robust solutions is proposed. We model scoping optimization as a multi-objective problem with profit and stability as heuristics. To evaluate our proposal, a number of experiments are conducted. Analysis of results show that both performance stability and feasibility stability were improved providing the product line manager enhanced decision-making support.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1415–1416},
numpages = {2},
keywords = {multi-objective, robust optimization, software product line portfolio scoping, uncertainty},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {constraints and variability mining, machine learning, software product lines, software testing, variability modeling},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.future.2017.09.081,
author = {Doumbouya, Mamadou Bilo and Kamsu-Foguem, Bernard and Kenfack, Hugues and Foguem, Clovis},
title = {Argumentation graphs with constraint-based reasoning for collaborative expertise},
year = {2018},
issue_date = {April 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {81},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2017.09.081},
doi = {10.1016/j.future.2017.09.081},
abstract = {Collaborative processes are very important in telemedicine domain since they allow for making right decisions in complex situations with multidisciplinary staff. When modelling these collaborative processes, some inconsistencies can appear. In semantic modelling (conceptual graphs), these inconsistencies are verified using constraints. In this work, collaborative processes are represented using an argumentation system modelled in a conceptual graph formalism where inconsistencies could be particular bad attack relation between arguments. To overcome these inconsistencies, two solutions are proposed. The first one is to weight the arguments evolving in the argumentation system on the basis of the competencies of the health professionals and the credibility of the sources justifying their advice (arguments), and the second one is to model some law concepts as constraints in order to check their compliance of the collaborative process. Reasoning underlying the remote collaborative processes in complex decision making.Abstract argumentation framework with the directed graphs to represent propositionsConceptual graphs for ontological knowledge modelling and formal visual reasoning.Competencies and information sources for the weighting of shared advices/opinions.Constraints checking for conflicts detection according to medicallegal obligations.},
journal = {Future Gener. Comput. Syst.},
month = apr,
pages = {16–29},
numpages = {14},
keywords = {Argumentation theory, Conceptual graphs, Decision making, Inconsistencies, Medical deontology, Weighting}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.5555/1753235.1753266,
author = {Hubaux, Arnaud and Classen, Andreas and Heymans, Patrick},
title = {Formal modelling of feature configuration workflows},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {In software product line engineering, the configuration process can be a long and complex undertaking that involves many participants. When configuration is supported by feature diagrams, two challenges are to modularise the feature diagram into related chunks, and to schedule them as part of the configuration process. Existing work has only focused on the first of these challenges and, for the rest, assumes that feature diagram modules are configured sequentially. This paper addresses the second challenge. It suggests using YAWL, a state-of-the-art workflow language, to represent the configuration workflow while feature diagrams model the available configuration options. The principal contribution of the paper is a new combined formalism: feature configuration workflows. A formal semantics is provided so as to pave the way for unambiguous tool specification and safer reasoning about of the configuration process. The work is motivated and illustrated through a configuration scenario taken from the space industry.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {221–230},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Human-computer interaction, Machine Learning, Product Line Architecture},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1109/CEC.2018.8477803,
author = {Luiz Jakubovski Filho, Helson and Nascimento Ferreira, Thiago and Regina Vergilio, Silvia},
title = {Incorporating User Preferences in a Software Product Line Testing Hyper-Heuristic Approach},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC.2018.8477803},
doi = {10.1109/CEC.2018.8477803},
abstract = {To perform the variability testing of Software Product Lines (SPLs) a set of products, represented in the Feature Model (FM), should be selected. Such selection is impacted by conflicting factors and has been efficiently solved by Evolutionary Multi-objective Algorithms in combination with hyper-heuristics. However, many times there is a cost budget or coverage level to be satisfied during the test, which are difficult to be incorporated as objective functions. Due to this, the choice of the best solution to be used in practice is not always easy. To deal with this situation, this paper introduces a preference-based hyper-heuristic approach to solve this problem. The approach implements the preference-based algorithm r-NSGA-II working with the random and FRRMAB selection methods. This last one uses a reward function based on r-dominance concept that takes into consideration a Reference Point provided by the tester. Our approach outperforms existing approaches, as well as the traditional algorithm r-NSGA-II, generating a reduced number of non-interesting solutions from the tester's point of view, that is, considering the provided Region of Interest (ROI).},
booktitle = {2018 IEEE Congress on Evolutionary Computation (CEC)},
pages = {1–8},
numpages = {8},
location = {Rio de Janeiro, Brazil}
}

@inproceedings{10.1109/SERA.2007.41,
author = {Lee, Soon-Bok and Kim, Jin-Woo and Song, Chee-Yang and Baik, Doo-Kwon},
title = {An Approach to Analyzing Commonality and Variability of Features using Ontology in a Software Product Line Engineering},
year = {2007},
isbn = {0769528678},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SERA.2007.41},
doi = {10.1109/SERA.2007.41},
abstract = {In a product line engineering, several studies have been made on analysis of feature which determines commonality and variability of product. Fundamentally, because the studies are based on developer's intuition and domain expert's experience, stakeholders lack common understanding of feature and a feature analysis is informal and subjective. Moreover, the reusability of software products, which were developed, is insufficient. This paper proposes an approach to analyzing commonality and variability of features using semantic-based analysis criteria which is able to change feature model of specific domain to featureontology. For the purpose, first feature attributes were made, create a feature model following the Meta model, transform it into feature-ontology, and save it to Meta feature-ontology repository. Henceforth, when we construct a feature model of the same product line, commonality and variability of the features can be extracted, comparing it with Meta feature ontology through a semantic similarity analysis method, which is proposed. Furthermore, a tool for a semantic similarity-comparing algorithm was implemented and an experiment with an electronic approval system domain in order to show the efficiency of the approach Was conducted. A Meta feature model can definitely be created through this approach, to construct a high-quality feature model based on common understanding of a feature. The main contributions are a formulating a method of extracting commonality and variability from features using ontology based on semantic similarity mapping and a enhancement of reusability of feature model.},
booktitle = {Proceedings of the 5th ACIS International Conference on Software Engineering Research, Management &amp; Applications},
pages = {727–734},
numpages = {8},
series = {SERA '07}
}

@inproceedings{10.1145/2983990.2984000,
author = {Hanappi, Oliver and Hummer, Waldemar and Dustdar, Schahram},
title = {Asserting reliable convergence for configuration management scripts},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983990.2984000},
doi = {10.1145/2983990.2984000},
abstract = {The rise of elastically scaling applications that frequently deploy new machines has led to the adoption of DevOps practices across the cloud engineering stack. So-called configuration management tools utilize scripts that are based on declarative resource descriptions and make the system converge to the desired state. It is crucial for convergent configurations to be able to gracefully handle transient faults, e.g., network outages when downloading and installing software packages. In this paper we introduce a conceptual framework for asserting reliable convergence in configuration management. Based on a formal definition of configuration scripts and their resources, we utilize state transition graphs to test whether a script makes the system converge to the desired state under different conditions. In our generalized model, configuration actions are partially ordered, often resulting in prohibitively many possible execution orders. To reduce this problem space, we define and analyze a property called preservation, and we show that if preservation holds for all pairs of resources, then convergence holds for the entire configuration. Our implementation builds on Puppet, but the approach is equally applicable to other frameworks like Chef, Ansible, etc. We perform a comprehensive evaluation based on real world Puppet scripts and show the effectiveness of the approach. Our tool is able to detect all idempotence and convergence related issues in a set of existing Puppet scripts with known issues as well as some hitherto undiscovered bugs in a large random sample of scripts.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {328–343},
numpages = {16},
keywords = {Configuration Management, Convergence, Declarative Language, DevOps, Idempotence, Puppet, System Configuration Scripts, Testing},
location = {Amsterdam, Netherlands},
series = {OOPSLA 2016}
}

@inproceedings{10.1007/978-3-319-13365-2_20,
author = {Rahman, Musfiqur and Ripon, Shamim},
title = {Using Bayesian Networks to Model and Analyze Software Product Line Feature Model},
year = {2014},
isbn = {9783319133645},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-13365-2_20},
doi = {10.1007/978-3-319-13365-2_20},
abstract = {Proper management of requirements plays a significant role in the successful development of any software product family. Application of AI, Bayesian Network (BN) in particular, is gaining much interest in Software Engineering, mainly in predicting software defects and software reliability. Feature analysis and its associated decision making is a suitable target area where BN can make remarkable effect. In SPL, a feature tree portrays various types of features as well as captures the relationships among them. This paper applies BN in modeling and analyzing features in a feature tree. Various feature analysis rules are first modeled and then verified in BN. The verification confirms the definition of the rules and thus these rules can be used in various decision making stages in SPL.},
booktitle = {Proceedings of the 8th International Workshop on Multi-Disciplinary Trends in Artificial Intelligence - Volume 8875},
pages = {220–231},
numpages = {12},
keywords = {Bayesian Networks, Dead feature, False Optional, Software Product Line},
location = {Bangalore, India},
series = {MIWAI 2014}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Delta Modeling, Software Evolution, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1016/j.infsof.2021.106573,
author = {Zhang, Fanlong and Khoo, Siau-cheng},
title = {An empirical study on clone consistency prediction based on machine learning},
year = {2021},
issue_date = {Aug 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {136},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106573},
doi = {10.1016/j.infsof.2021.106573},
journal = {Inf. Softw. Technol.},
month = aug,
numpages = {16},
keywords = {Code clones, Clone consistent change, Clone consistency prediction, Software maintenance, Machine learning}
}

@article{10.1016/j.infsof.2012.09.007,
author = {Guana, Victor and Correal, Dario},
title = {Improving software product line configuration: A quality attribute-driven approach},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.09.007},
doi = {10.1016/j.infsof.2012.09.007},
abstract = {Context: During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product's intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task. Objective: Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets. Method: We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product's functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years. Results: Through our SPL experiment, we established that our approach improved in nearly 98% the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools. Conclusion: We can conclude that our domain-specific modeling approach significantly improves the software architect's decision making when selecting the most suitable combinations of reusable components in the context of a SPL.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {541–562},
numpages = {22},
keywords = {Domain specific modeling, Model driven - software product lines, Quality evaluation, Sensitivity points, Software architecture, Variability management}
}

@inproceedings{10.5220/0005156003160321,
author = {Alexandre, Fr\'{e}d\'{e}ric and Carrere, Maxime and Kassab, Randa},
title = {Feature, Configuration, History},
year = {2014},
isbn = {9789897580543},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005156003160321},
doi = {10.5220/0005156003160321},
abstract = {Artificial Neural Networks are very efficient adaptive models but one of their recognized weaknesses is about information representation, often carried out in an input vector without a structure. Beyond the classical elaboration of a hierarchical representation in a series of layers, we report here inspiration from neuroscience and argue for the design of heterogenous neural networks, processing information at feature, configuration and history levels of granularity, and interacting very efficiently for high-level and complex decision making. This framework is built from known characteristics of the sensory cortex, the hippocampus and the prefrontal cortex and is exemplified here in the case of pavlovian conditioning, but we propose that it can be advantageously applied in a wider extent, to design flexible and versatile information processing with neuronal computation.},
booktitle = {Proceedings of the International Joint Conference on Computational Intelligence - Volume 3},
pages = {316–321},
numpages = {6},
keywords = {Computational Neuroscience, Information Representation, Pavlovian Conditioning.},
location = {Rome, Italy},
series = {IJCCI 2014}
}

@inproceedings{10.1007/978-3-030-90785-3_16,
author = {Wang, Baoping and Wang, Wennan and Zhu, Linkai and Liu, Wenjian},
title = {Research on Cross-Project Software Defect Prediction Based on Machine Learning},
year = {2021},
isbn = {978-3-030-90784-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90785-3_16},
doi = {10.1007/978-3-030-90785-3_16},
abstract = {In recent years, machine learning technology has developed vigorously. The research on software defect prediction in the field of software engineering is increasingly adopting various algorithms of machine learning. This article has carried out a systematic literature review on the field of defect prediction. First, this article studies the development process of defect prediction, from correlation to prediction model. then this article studies the development process of cross-project defect prediction based on machine learning algorithms (naive Bayes, decision tree, random forest, neural network, etc.). Finally, this paper looks forward to the research difficulties and future directions of software defect prediction, such as imbalance in classification, cost of data labeling, and cross-project data distribution.},
booktitle = {Advances in Web-Based Learning – ICWL 2021: 20th International Conference, ICWL 2021, Macau, China, November 13–14, 2021, Proceedings},
pages = {160–165},
numpages = {6},
keywords = {Machine learning, Software defect prediction model, Metric},
location = {Macau, China}
}

@inproceedings{10.1145/3324884.3415281,
author = {Abdelkader, Hala},
title = {Towards robust production machine learning systems: managing dataset shift},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3415281},
doi = {10.1145/3324884.3415281},
abstract = {The advances in machine learning (ML) have stimulated the integration of their capabilities into software systems. However, there is a tangible gap between software engineering and machine learning practices, that is delaying the progress of intelligent services development. Software organisations are devoting effort to adjust the software engineering processes and practices to facilitate the integration of machine learning models. Machine learning researchers as well are focusing on improving the interpretability of machine learning models to support overall system robustness. Our research focuses on bridging this gap through a methodology that evaluates the robustness of machine learning-enabled software engineering systems. In particular, this methodology will automate the evaluation of the robustness properties of software systems against dataset shift problems in ML. It will also feature a notification mechanism that facilitates the debugging of ML components.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1164–1166},
numpages = {3},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1109/WAIN52551.2021.00021,
author = {Serban, Alex and van der Blom, Koen and Hoos, Holger and Visser, Joost},
title = {Practices for Engineering Trustworthy Machine Learning Applications},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00021},
doi = {10.1109/WAIN52551.2021.00021},
abstract = {Following the recent surge in adoption of machine learning (ML), the negative impact that improper use of ML can have on users and society is now also widely recognised. To address this issue, policy makers and other stakeholders, such as the European Commission or NIST, have proposed high-level guidelines aiming to promote trustworthy ML (i.e., lawful, ethical and robust). However, these guidelines do not specify actions to be taken by those involved in building ML systems. In this paper, we argue that guidelines related to the development of trustworthy ML can be translated to operational practices, and should become part of the ML development life cycle. Towards this goal, we ran a multi-vocal literature review, and mined operational practices from white and grey literature. Moreover, we launched a global survey to measure practice adoption and the effects of these practices. In total, we identified 14 new practices, and used them to complement an existing catalogue of ML engineering practices. Initial analysis of the survey results reveals that so far, practice adoption for trustworthy ML is relatively low. In particular, practices related to assuring security of ML components have very low adoption. Other practices enjoy slightly larger adoption, such as providing explanations to users. Our extended practice catalogue can be used by ML development teams to bridge the gap between high-level guidelines and actual development of trustworthy ML systems; it is open for review and contributions.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {97–100},
numpages = {4},
location = {Madrid, Spain}
}

@inproceedings{10.1145/1629716.1629720,
author = {Chae, Wonseok and Blume, Matthias},
title = {Language support for feature-oriented product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629720},
doi = {10.1145/1629716.1629720},
abstract = {Product line engineering is an emerging paradigm of developing a family of products. While product line analysis and design mainly focus on reasoning about commonality and variability of family members, product line implementation gives its attention to mechanisms of managing variability. In many cases, however, product line methods do not impose any specific synthesis mechanisms on product line implementation, so implementation details are left to developers. In our previous work, we adopted feature-oriented product line engineering to build a family of compilers and managed variations using the Standard ML module system. We demonstrated the applicability of this module system to product line implementation. Although we have benefited from the product line engineering paradigm, it mostly served us as a design paradigm to change the way we think about a set of closely related compilers, not to change the way we build them. The problem was that Standard ML did not fully realize this paradigm at the code level, which caused some difficulties when we were developing a set of compilers.In this paper, we address such issues with a language-based solution. MLPolyR is our choice of an implementation language. It supports three different programming styles. First, its first-class cases facilitate composable extensions at the expression levels. Second, its module language provides extensible and parameterized modules, which make large-scale extensible programming possible. Third, its macro system simplifies specification and composition of feature related code. We will show how the combination of these language features work together to facilitate the product line engineering paradigm.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {3–10},
numpages = {8},
keywords = {feature-oriented programming, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.1016/j.jss.2021.111031,
author = {Giray, G\"{o}rkem},
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111031},
doi = {10.1016/j.jss.2021.111031},
journal = {J. Syst. Softw.},
month = oct,
numpages = {35},
keywords = {Software engineering, Software development, Software process, Machine learning, Deep learning, Systematic literature review}
}

@inbook{10.5555/3454287.3455252,
author = {Jeong, Jisoo and Lee, Seungeui and Kim, Jeesoo and Kwak, Nojun},
title = {Consistency-based semi-supervised learning for object detection},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Specifically, the consistency constraint is applied not only for object classification but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage and two-stage detectors and the results show the effectiveness of our method.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {965},
numpages = {10}
}

@inproceedings{10.5555/3507788.3507798,
author = {Khan, Md Asif and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Garcon, Sylvain and Tauseef, Qasim},
title = {Failure prediction using machine learning in IBM WebSphere liberty continuous integration environment},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {The growing complexity and dependencies of software have increased the importance of testing to ensure that frequent changes do not adversely affect existing functionality. Moreover, continuous integration comes with unique challenges associated with maintaining a stable build environment. Several studies have shown that the testing environment becomes more efficient with proper test case prioritization techniques. However, an application's dynamic behavior makes it challenging to derive test case prioritization techniques for achieving optimal results. With the advance of machine learning, the context of an application execution can be analyzed to select and prioritize test suites more efficiently.Test suite prioritization techniques aim to reorder test suites' execution to deliver high quality, maintainable software at lower costs to meet specific objectives such as revealing failures earlier. The state-of-the-art techniques on test prioritization in a continuous integration environment focus on relatively small, single-language, unit-tested projects. This paper compares and analyzes Machine learning-based test suite prioritization technique on two large-scale dataset collected from a continuous integration environment Google and IBM respectively. We optimize hyperparameters and report on experiments' findings by using different machine learning algorithms for test suite prioritization. Our optimized algorithms prioritize test suites with 93% accuracy on average and require 20% fewer test suites to detect 80% of the failures than the test suites prioritized randomly.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {CI, continuous integration, machine learning, test prioritization},
location = {Toronto, Canada},
series = {CASCON '21}
}

@article{10.1007/s10515-014-0160-4,
author = {Devine, Thomas and Goseva-Popstojanova, Katerina and Krishnan, Sandeep and Lutz, Robyn R.},
title = {Assessment and cross-product prediction of software product line quality: accounting for reuse across products, over multiple releases},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0160-4},
doi = {10.1007/s10515-014-0160-4},
abstract = {The goals of cross-product reuse in a software product line (SPL) are to mitigate production costs and improve the quality. In addition to reuse across products, due to the evolutionary development process, a SPL also exhibits reuse across releases. In this paper, we empirically explore how the two types of reuse--reuse across products and reuse across releases--affect the quality of a SPL and our ability to accurately predict fault proneness. We measure the quality in terms of post-release faults and consider different levels of reuse across products (i.e., common, high-reuse variation, low-reuse variation, and single-use packages), over multiple releases. Assessment results showed that quality improved for common, low-reuse variation, and single-use packages as they evolved across releases. Surprisingly, within each release, among preexisting (`old') packages, the cross-product reuse did not affect the change and fault proneness. Cross-product predictions based on pre-release data accurately ranked the packages according to their post-release faults and predicted the 20 % most faulty packages. The predictions benefited from data available for other products in the product line, with models producing better results (1) when making predictions on smaller products (consisting mostly of common packages) rather than on larger products and (2) when trained on larger products rather than on smaller products.},
journal = {Automated Software Engg.},
month = jun,
pages = {253–302},
numpages = {50},
keywords = {Assessment, Cross-product prediction, Cross-product reuse, Cross-release reuse, Fault proneness prediction, Longitudinal study, Software product lines}
}

@inproceedings{10.1007/978-3-642-33347-7_16,
author = {Westphal, Matthias and Hu\'{e}, Julien},
title = {Nogoods in qualitative constraint-based reasoning},
year = {2012},
isbn = {9783642333460},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33347-7_16},
doi = {10.1007/978-3-642-33347-7_16},
abstract = {The prevalent method of increasing reasoning efficiency in the domain of qualitative constraint-based spatial and temporal reasoning is to use domain splitting based on so-called tractable subclasses. In this paper we analyze the application of nogood learning with restarts in combination with domain splitting. Previous results on nogood recording in the constraint satisfaction field feature learnt nogoods as a global constraint that allows for enforcing generalized arc consistency. We present an extension of such a technique capable of handling domain splitting, evaluate its benefits for qualitative constraint-based reasoning, and compare it with alternative approaches.},
booktitle = {Proceedings of the 35th Annual German Conference on Advances in Artificial Intelligence},
pages = {180–192},
numpages = {13},
location = {Saarbr\"{u}cken, Germany},
series = {KI'12}
}

@inproceedings{10.1109/SEAMS.2019.00015,
author = {Jamshidi, Pooyan and C\'{a}mara, Javier and Schmerl, Bradley and K\"{a}stner, Christian and Garlan, David},
title = {Machine learning meets quantitative planning: enabling self-adaptation in autonomous robots},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2019.00015},
doi = {10.1109/SEAMS.2019.00015},
abstract = {Modern cyber-physical systems (e.g., robotics systems) are typically composed of physical and software components, the characteristics of which are likely to change over time. Assumptions about parts of the system made at design time may not hold at run time, especially when a system is deployed for long periods (e.g., over decades). Self-adaptation is designed to find reconfigurations of systems to handle such run-time inconsistencies. Planners can be used to find and enact optimal reconfigurations in such an evolving context. However, for systems that are highly configurable, such planning becomes intractable due to the size of the adaptation space. To overcome this challenge, in this paper we explore an approach that (a) uses machine learning to find Pareto-optimal configurations without needing to explore every configuration and (b) restricts the search space to such configurations to make planning tractable. We explore this in the context of robot missions that need to consider task timeliness and energy consumption. An independent evaluation shows that our approach results in high-quality adaptation plans in uncertain and adversarial environments.},
booktitle = {Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {39–50},
numpages = {12},
keywords = {artificial intelligence, machine learning, quantitative planning, robotics systems, self-adaptive systems},
location = {Montreal, Quebec, Canada},
series = {SEAMS '19}
}

@article{10.1155/2021/9976306,
author = {Wang, Wei and Wu, Wenqing},
title = {Using Machine Learning Algorithms to Recognize Shuttlecock Movements},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/9976306},
doi = {10.1155/2021/9976306},
abstract = {Shuttlecock is an excellent traditional national sport in China. Because of its simplicity, convenience, and fun, it is loved by the broad masses of people, especially teenagers and children. The development of shuttlecock sports into a confrontational event is not long, and it takes a period of research to master the tactics and strategies of shuttlecock sports. Based on this, this article proposes the use of machine learning algorithms to recognize the movement of shuttlecock movements, aiming to provide more theoretical and technical support for shuttlecock competitions by identifying features through actions with the assistance of technical algorithms. This paper uses literature research methods, model methods, comparative analysis methods, and other methods to deeply study the motion characteristics of shuttlecock motion, the key algorithms of machine learning algorithms, and other theories and construct the shuttlecock motion recognition based on multiview clustering algorithm. The model analyzes the robustness and accuracy of the machine learning algorithm and other algorithms, such as a variety of performance comparisons, and the results of the shuttlecock motion recognition image. For the key movements of shuttlecock movement, disk, stretch, hook, wipe, knock, and abduction, the algorithm proposed in this paper has a good movement recognition rate, which can reach 91.2%. Although several similar actions can be recognized well, the average recognition accuracy rate can exceed 75%, and even through continuous image capture, the number of occurrences of the action can be automatically analyzed, which is beneficial to athletes. And the coach can better analyze tactics and research strategies.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {13}
}

@article{10.5555/2747015.2747184,
author = {da Silva, Ivonei Freitas and da Mota Silveira Neto, Paulo Anselmo and O'Leary, P\'{a}draig and de Almeida, Eduardo Santana and Meira, Silvio Romero de Lemos},
title = {Software product line scoping and requirements engineering in a small and medium-sized enterprise},
year = {2014},
issue_date = {February 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0164-1212},
abstract = {HighlightsWe described a detailed qualitative study on software product line scoping and requirements engineering.We examine weaknesses regarding the iterativeness, adaptability, and communication.Agile methods can mitigate the iterativeness, adaptability, and communication weaknesses. Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods.},
journal = {J. Syst. Softw.},
month = feb,
pages = {189–206},
numpages = {18},
keywords = {Agile methods, Requirements engineering, Software product line scoping}
}

@inproceedings{10.1109/ASE.2009.16,
author = {Lauenroth, Kim and Pohl, Klaus and Toehning, Simon},
title = {Model Checking of Domain Artifacts in Product Line Engineering},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.16},
doi = {10.1109/ASE.2009.16},
abstract = {In product line engineering individual products are derived from the domain artifacts of the product line. The reuse of the domain artifacts is constraint by the product line variability. Since domain artifacts are reused in several products, product line engineering benefits from the verification of domain artifacts. For verifying development artifacts, model checking is a well-established technique in single system development. However, existing model checking approaches do not incorporate the product line variability and are hence of limited use for verifying domain artifacts. In this paper we present an extended model checking approach which takes the product line variability into account when verifying domain artifacts. Our approach is thus able to verify that every permissible product (specified with I/O-automata) which can be derived from the product line fulfills the specified properties (specified with CTL). Moreover, we use two examples to validate the applicability of our approach and report on the preliminary validation results.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {269–280},
numpages = {12},
keywords = {Domain Artifact Verification, Model Checking, Product Line Engineering, Variability},
series = {ASE '09}
}

@article{10.1145/3437479.3437485,
author = {Yoo, Shin and Aleti, Aldeida and Turhan, Burak and Minku, Leandro L. and Miranskyy, Andriy and Meri\c{c}li, \c{C}etin},
title = {The 8th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3437479.3437485},
doi = {10.1145/3437479.3437485},
abstract = {The International Workshop on Realizing Arti cial Intelligence Synergies in Software Engineering (RAISE) aims to present the state of the art in the crossover between Software Engineering and Arti cial Intelligence. This workshop explored not only the appli- cation of AI techniques to SE problems but also the application of SE techniques to AI problems. Software has become critical for realizing functions central to our society. For example, software is essential for nancial and transport systems, energy generation and distribution systems, and safety-critical medical applications. Software development costs trillions of dollars each year yet, still, many of our software engineering methods remain mostly man- ual. If we can improve software production by smarter AI-based methods, even by small margins, then this would improve a crit- ical component of the international infrastructure, while freeing up tens of billions of dollars for other tasks.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {23–24},
numpages = {2}
}

@article{10.1155/2021/1057371,
author = {Yang, Yinghui and cheikhrouhou, omar},
title = {The Potential Energy of Artificial Intelligence Technology in University Education Reform from the Perspective of Communication Science},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/1057371},
doi = {10.1155/2021/1057371},
abstract = {In today’s rapid development of science and technology, science is everywhere in people’s lives, and science communication is everywhere. Science and communication are not only not far away but also very close. Since machine learning algorithms with deep learning as a theme have achieved great success in the fields of vision and speech recognition, as well as the large amount of data resources that cloud computing, big data, and other technologies can provide, the development speed of artificial intelligence has been greatly improved, and it has had a significant impact in various industries in the society, and the country has put forward the concept of intelligent education for this purpose. However, there have been few systematic discussions on the combination of artificial intelligence with education and teaching. Therefore, this article uses artificial intelligence technology to study the potential energy space of artificial intelligence technology in college education reform from the perspective of science communication, designs and implements an online education platform for colleges and universities, and conducts a trial of platform use in a domestic college and universities. Some teachers and students conduct a satisfaction survey after the platform is used, and the conclusions show that whether in the teacher group or the student group, most teachers and students are relatively satisfied with the online education platform designed in this article. The reform of college education includes many aspects. This article is a research study on the form of college education, changing from traditional offline education to online platform education. This research can provide a certain reference for the reform of college education.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {7}
}

@article{10.1016/j.future.2019.06.022,
author = {Raza, Muhammad and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Zhao, Ming and Rehman, Zia ur},
title = {A comparative analysis of machine learning models for quality pillar assessment of SaaS services by multi-class text classification of users’ reviews},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.06.022},
doi = {10.1016/j.future.2019.06.022},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {341–371},
numpages = {31},
keywords = {SaaS, Quality pillars, User reviews, Text classification, Machine learning approaches}
}

@inproceedings{10.1145/1808937.1808942,
author = {Estublier, Jacky and Dieng, Idrissa A. and Leveque, Thomas},
title = {Software product line evolution: the Selecta system},
year = {2010},
isbn = {9781605589688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808937.1808942},
doi = {10.1145/1808937.1808942},
abstract = {The current technology gives little room for the different kinds of evolution needed for any software product line (SPL): evolution of the associated engineering environment, evolution of the market and SPL scope, evolution of the products and variability. The paper describes how these different evolution needs are addressed in the CADSE and Selecta systems. The solution we propose uses metamodeling and generation for the engineering environment evolution, composition for scope and market evolution, a component database and a selection language for the product and variability evolution. The paper presents the Selecta system and shortly discusses the experience.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Product Line Approaches in Software Engineering},
pages = {32–39},
numpages = {8},
keywords = {IDE, evolution, product families, product lines, software environments},
location = {Cape Town, South Africa},
series = {PLEASE '10}
}

@inproceedings{10.1145/2815400.2815401,
author = {Tang, Chunqiang and Kooburat, Thawan and Venkatachalam, Pradeep and Chander, Akshay and Wen, Zhe and Narayanan, Aravind and Dowell, Patrick and Karl, Robert},
title = {Holistic configuration management at Facebook},
year = {2015},
isbn = {9781450338349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815400.2815401},
doi = {10.1145/2815400.2815401},
abstract = {Facebook's web site and mobile apps are very dynamic. Every day, they undergo thousands of online configuration changes, and execute trillions of configuration checks to personalize the product features experienced by hundreds of million of daily active users. For example, configuration changes help manage the rollouts of new product features, perform A/B testing experiments on mobile devices to identify the best echo-canceling parameters for VoIP, rebalance the load across global regions, and deploy the latest machine learning models to improve News Feed ranking. This paper gives a comprehensive description of the use cases, design, implementation, and usage statistics of a suite of tools that manage Facebook's configuration end-to-end, including the frontend products, backend systems, and mobile apps.},
booktitle = {Proceedings of the 25th Symposium on Operating Systems Principles},
pages = {328–343},
numpages = {16},
location = {Monterey, California},
series = {SOSP '15}
}

@inproceedings{10.1007/978-3-642-33176-3_7,
author = {ter Beek, Maurice H. and Muccini, Henry and Pelliccione, Patrizio},
title = {Assume-guarantee testing of evolving software product line architectures},
year = {2012},
isbn = {9783642331756},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33176-3_7},
doi = {10.1007/978-3-642-33176-3_7},
abstract = {Despite some work on testing software product lines, maintaining the quality of products when a software product line evolves is still an open problem. In this paper, we propose a novel assume-guarantee testing approach as a solution to the following research question: how can we verify the correct functioning of products of an software product line when core components evolve? The underlying idea is to retest only some of the products that conform to the software product line architecture and to infer, using assume-guarantee reasoning, the correctness of the other products. Assume-guarantee reasoning moreover permits the retesting of only those components that are affected by the changes.},
booktitle = {Proceedings of the 4th International Conference on Software Engineering for Resilient Systems},
pages = {91–105},
numpages = {15},
keywords = {assume-guarantee testing, compositional verification, evolving software product lines, software testing},
location = {Pisa, Italy},
series = {SERENE'12}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {Software engineering, computer-aided software engineering, software variability}
}

@article{10.1016/j.compbiomed.2021.104354,
author = {Makridis, Christos A. and Zhao, David Y. and Bejan, Cosmin A. and Alterovitz, Gil},
title = {Leveraging machine learning to characterize the role of socio-economic determinants on physical health and well-being among veterans},
year = {2021},
issue_date = {Jun 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {133},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104354},
doi = {10.1016/j.compbiomed.2021.104354},
journal = {Comput. Biol. Med.},
month = jun,
numpages = {8},
keywords = {Health informatics, Machine learning, Subjective well-being, Socioeconomics, Veterans}
}

@inproceedings{10.1109/WAIN52551.2021.00028,
author = {Lewis, Grace A. and Bellomo, Stephany and Ozkaya, Ipek},
title = {Characterizing and Detecting Mismatch in Machine-Learning-Enabled Systems},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00028},
doi = {10.1109/WAIN52551.2021.00028},
abstract = {Increasing availability of machine learning (ML) frameworks and tools, as well as their promise to improve solutions to data-driven decision problems, has resulted in popularity of using ML techniques in software systems. However, end-to-end development of ML-enabled systems, as well as their seamless deployment and operations, remain a challenge. One reason is that development and deployment of ML-enabled systems involves three distinct workflows, perspectives, and roles, which include data science, software engineering, and operations. These three distinct perspectives, when misaligned due to incorrect assumptions, cause ML mismatches which can result in failed systems. We conducted an interview and survey study where we collected and validated common types of mismatches that occur in end-to-end development of ML-enabled systems. Our analysis shows that how each role prioritizes the importance of relevant mismatches varies, potentially contributing to these mismatched assumptions. In addition, the mismatch categories we identified can be specified as machine readable descriptors contributing to improved ML-enabled system development. In this paper, we report our findings and their implications for improving end-to-end ML-enabled system development.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {133–140},
numpages = {8},
location = {Madrid, Spain}
}

@article{10.1016/j.procs.2018.08.100,
author = {Zykov, Sergey V. and Shumsky, Leonid D. and Tykushin, Anatoly V. and Tormasov, Alexander G.},
title = {Applicative-based automatic configuration management for virtual machines},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {126},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.08.100},
doi = {10.1016/j.procs.2018.08.100},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {1771–1778},
numpages = {8},
keywords = {virtual machine, automatic configuration, applicative computing}
}

@article{10.1007/s10515-019-00266-2,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong},
title = {Using multi-objective search and machine learning to infer rules constraining product configurations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00266-2},
doi = {10.1007/s10515-019-00266-2},
abstract = {Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.},
journal = {Automated Software Engg.},
month = jun,
pages = {1–62},
numpages = {62},
keywords = {Product line, Configuration, Rule mining, Multi-objective search, Machine learning, Interacting products}
}

@article{10.1007/s10515-011-0099-7,
author = {Bagheri, Ebrahim and Ensan, Faezeh and Gasevic, Dragan},
title = {Decision support for the software product line domain engineering lifecycle},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0099-7},
doi = {10.1007/s10515-011-0099-7},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of the target domain and through the development of comprehensive and variability-covering feature models. The feature models developed within the software product line development process need to cover the relevant features and aspects of the target domain. In other words, the feature models should be elaborate representations of the feature space of that domain. Given that feature models, i.e., software product line feature models, are developed mostly by domain analysts by sifting through domain documentation, corporate records and transcribed interviews, the process is a cumbersome and error-prone one. In this paper, we propose a decision support platform that assists domain analysts throughout the domain engineering lifecycle by: (1) automatically performing natural language processing tasks over domain documents and identifying important information for the domain analysts such as the features and integrity constraints that exist in the domain documents; (2) providing a collaboration platform around the domain documents such that multiple domain analysts can collaborate with each other during the process using a Wiki; (3) formulating semantic links between domain terminology with external widely used ontologies such as WordNet in order to disambiguate the terms used in domain documents; and (4) developing traceability links between the unstructured information available in the domain documents and their formal counterparts within the formal feature model representations. Results obtained from our controlled experimentations show that the decision support platform is effective in increasing the performance of the domain analysts during the domain engineering lifecycle in terms of both the coverage and accuracy measures.},
journal = {Automated Software Engg.},
month = sep,
pages = {335–377},
numpages = {43},
keywords = {Domain engineering, Feature models, NLP model inference, Software product lines}
}

@article{10.1016/j.knosys.2017.02.020,
author = {Prez-Ortiz, M. and Gutirrez, P.A. and Aylln-Tern, M.D. and Heaton, N. and Ciria, R. and Briceo, J. and Hervs-Martnez, C.},
title = {Synthetic semi-supervised learning in imbalanced domains},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.020},
doi = {10.1016/j.knosys.2017.02.020},
abstract = {Liver transplantation is a promising and widely-accepted treatment for patients with terminal liver disease. However, transplantation is restricted by the lack of suitable donors, resulting in significant waiting list deaths. This paper proposes a novel donor-recipient allocation system that uses machine learning to predict graft survival after transplantation using a dataset comprised of donor-recipient pairs from the Kings College Hospital (United Kingdom). The main novelty of the system is that it tackles the imbalanced nature of the dataset by considering semi-supervised learning, analysing its potential for obtaining more robust and equitable models in liver transplantation. We propose two different sources of unsupervised data for this specific problem (recent transplants and virtual donor-recipient pairs) and two methods for using these data during model construction (a semi-supervised algorithm and a label propagation scheme). The virtual pairs and the label propagation method are shown to alleviate the imbalanced distribution. The results of our experiments show that the use of synthetic and real unsupervised information helps to improve and stabilise the performance of the model and leads to fairer decisions with respect to the use of only supervised data. Moreover, the best model is combined with the Model for End-stage Liver Disease score (MELD), which is at the moment the most popular assignation methodology worldwide. By doing this, our decision-support system considers both the compatibility of the donor and the recipient (by our prediction system) and the recipient severity (via the MELD score), supporting then the principles of fairness and benefit.},
journal = {Know.-Based Syst.},
month = may,
pages = {75–87},
numpages = {13},
keywords = {Imbalanced classification, Liver transplantation, Machine learning, Semi-supervised learning, Support vector machines, Survival analysis, Transplant recipient}
}

@phdthesis{10.5555/AAI28716555,
author = {Eluri, Vijaya Kumar and Amir, Etemadi, and Thomas, Holzer, and P, Blackford, Joseph},
advisor = {Shahram, Sarkani, and Thomas, Mazzuchi,},
title = {Predicting Long-Time Contributors for GitHub Projects Using Machine Learning},
year = {2021},
isbn = {9798535595184},
publisher = {The George Washington University},
abstract = {Organizations typically develop software systems using non-developmental items and commercial off-the-shelf software. Many organizations are changing the way they create, capture, consume, and commercialize software by increasingly Open-Source Software projects. However, many OSS projects do not survive, as the survival of OSS projects depends mainly on retaining new contributors. While organizations driven by their business needs support some OSS projects, most OSS contributors are volunteers who contribute their work for free. While many join OSS projects, only a few contribute to an OSS project for a considerably long time. A Long-time contributor is defined as a contributor who joins a project and continues to contribute for more than T years; T is generally set to 1, 2, and 3 years. LTCs often contribute more code than non-LTCs.Most new contributors abandon a project without becoming LTCs. The data in this research shows that 98% of new contributors leave a project before three years. Various factors affect whether a new contributor becomes an LTC, including the new contributor's experience, expertise, the project's maturity, working environment, documentation, and task difficulty. Identifying factors that predict potential LTCs can enable project owners to gain insight into what matters to contributors and take action to retain new contributors for a long time. These actions could include mentoring, quickly responding to questions, providing timely code reviews, and merging contributions.This research investigates effective predictability of new contributors to OSS repositories becoming LTC based on repository and contributor meta-data collected from OSS repositories.  Compared to state-of-the-art models, the models built in this research use less than 50% features and produce better results. In 10-fold cross-validation, the precision, recall, F1-score, MCC, and AUC of the best state-of-the-art model are 0.546, 0.041, 0.075, 0.1446, and 0.908, respectively.},
note = {AAI28716555}
}

@inproceedings{10.1007/978-3-030-64148-1_12,
author = {Lwakatare, Lucy Ellen and Crnkovic, Ivica and R\r{a}nge, Ellinor and Bosch, Jan},
title = {From a Data Science Driven Process to a Continuous Delivery Process for Machine Learning Systems},
year = {2020},
isbn = {978-3-030-64147-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64148-1_12},
doi = {10.1007/978-3-030-64148-1_12},
abstract = {Development of machine learning (ML) enabled applications in real-world settings is challenging and requires the consideration of sound software engineering (SE) principles and practices. A large body of knowledge exists on the use of modern approaches to developing traditional software components, but not ML components. Using exploratory case study approach, this study investigates the adoption and use of existing software development approaches, specifically continuous delivery (CD), to development of ML components. Research data was collected using a multivocal literature review (MLR) and focus group technique with ten practitioners involved in developing ML-enabled systems at a large telecommunication company. The results of our MLR show that companies do not outright apply CD to the development of ML components rather as a result of improving their development practices and infrastructure over time. A process improvement conceptual model, that includes the description of CD application to ML components is developed and initially validated in the study.},
booktitle = {Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings},
pages = {185–201},
numpages = {17},
keywords = {Machine learning system, Software process, Continuous delivery},
location = {Turin, Italy}
}

@inproceedings{10.1007/978-3-030-26250-1_32,
author = {Robin, Jacques and Mazo, Raul and Madeira, Henrique and Barbosa, Raul and Diaz, Daniel and Abreu, Salvador},
title = {A Self-certifiable Architecture for Critical Systems Powered by Probabilistic Logic Artificial Intelligence},
year = {2019},
isbn = {978-3-030-26249-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26250-1_32},
doi = {10.1007/978-3-030-26250-1_32},
abstract = {We present a versatile architecture for AI-powered self-adaptive self-certifiable critical systems. It aims at supporting semi-automated low-cost re-certification for self-adaptive systems after each adaptation of their behavior to a persistent change in their operational environment throughout their lifecycle.},
booktitle = {Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings},
pages = {391–397},
numpages = {7},
keywords = {AI certification, Autonomic architecture, Argumentation, Rule-based constraint solving, Probabilistic logic machine learning},
location = {Turku, Finland}
}

@inproceedings{10.1007/978-3-030-26619-6_16,
author = {Tello, Ghalia and Gianini, Gabriele and Mizouni, Rabeb and Damiani, Ernesto},
title = {Machine Learning-Based Framework for Log-Lifting in Business Process Mining Applications},
year = {2019},
isbn = {978-3-030-26618-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26619-6_16},
doi = {10.1007/978-3-030-26619-6_16},
abstract = {Real-life event logs are typically much less structured and more complex than the predefined business activities they refer to. Most of the existing process mining techniques assume that there is a one-to-one mapping between process model activities and events recorded during process execution. Unfortunately, event logs and process model activities are defined at different levels of granularity. The challenges posed by this discrepancy can be addressed by means of log-lifting. In this work we develop a machine-learning-based framework aimed at bridging the abstraction level gap between logs and process models. The proposed framework operates of two main phases: log segmentation and machine-learning-based classification. The purpose of the segmentation phase is to identify the potential segment separators in a flow of low-level events, in which each segment corresponds to an unknown high-level activity. For this, we propose a segmentation algorithm based on maximum likelihood with n-gram analysis. In the second phase, event segments are mapped into their corresponding high-level activities using a supervised machine learning technique. Several machine learning classification methods are explored including ANNs, SVMs, and random forest. We demonstrate the applicability of our framework using a real-life event log provided by the SAP company. The results obtained show that a machine learning approach based on the random forest algorithm outperforms the other methods with an accuracy of 96.4%. The testing time was found to be around 0.01s, which makes the algorithm a good candidate for real-time deployment scenarios.},
booktitle = {Business Process Management: 17th International Conference, BPM 2019, Vienna, Austria, September 1–6, 2019, Proceedings},
pages = {232–249},
numpages = {18},
keywords = {Process mining, Segmentation, Log lifting, Machine learning},
location = {Vienna, Austria}
}

@inproceedings{10.1145/3382025.3414943,
author = {Th\"{u}m, Thomas},
title = {A BDD for Linux? the knowledge compilation challenge for variability},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414943},
doi = {10.1145/3382025.3414943},
abstract = {What is the number of valid configurations for Linux? How to generate uniform random samples for Linux? Can we create a binary decision diagram for Linux? It seems that the product-line community tries hard to answer such questions for Linux and other configurable systems. However, attempts are often not published due to the publication bias (i.e., unsuccessful attempts are not published). As a consequence, researchers keep trying by potentially spending redundant effort. The goal of this challenge is to guide research on these computationally complex problems and to foster the exchange between researchers and practitioners.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {16},
numpages = {6},
keywords = {artificial intelligence, binary decision diagrams, configurable system, decision models, feature models, knownledge compilation, product configuration, satisfiability solving, software configuration, software product line},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.4018/IJWP.2017070102,
author = {Sirqueira, Tassio Ferenzini Martins and Braga, Regina and Ara\'{u}jo, Marco Ant\^{o}nio P. and David, Jos\'{e} Maria N. and Campos, Fernanda and Str\"{o}ele, Victor},
title = {An Approach to Configuration Management of Scientific Workflows},
year = {2017},
issue_date = {July 2017},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {2},
issn = {1938-0194},
url = {https://doi.org/10.4018/IJWP.2017070102},
doi = {10.4018/IJWP.2017070102},
abstract = {A scientific software ecosystem aims to integrate all stages of an experiment and its related workflows, in order to solve complex problems. In this vein, in order to assure the experiment proper execution, any modification that occurs must be propagated to the associated workflows, which must be maintained and evolved for the successful conduction of the research. One way to ensure this control is through configuration management using data provenance. In this work, the authors use data provenance concepts and models, together with ontologies to provide an architecture for the storage and query of scientific experiment information. Considering the architecture, a proof of concept was conducted using workflows extracted from the myExperiment repository. The results are presented along the paper.},
journal = {Int. J. Web Portals},
month = jul,
pages = {20–46},
numpages = {27},
keywords = {Data Provenance, Ontology, Scientific Experiment Management, Workflow Maintenance and Evolution}
}

@inproceedings{10.1109/CLOUD.2014.104,
author = {Gherardi, Luca and Hunziker, Dominique and Mohanarajah, Gajamohan},
title = {A Software Product Line Approach for Configuring Cloud Robotics Applications},
year = {2014},
isbn = {9781479950638},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CLOUD.2014.104},
doi = {10.1109/CLOUD.2014.104},
abstract = {The computational requirements of the increasingly sophisticated algorithms used in today's robotics software applications have outpaced the onboard processors of the average robot. Furthermore, the development and configuration of these applications are difficult tasks that require expertise in diverse domains, including software engineering, control engineering, and computer vision. As a solution to these problems, this paper extends and integrates our previous works, which are based on two promising techniques: Cloud Robotics and Software Product Lines. Cloud Robotics provides a powerful and scalable environment to offload the computationally expensive algorithms resulting in low-cost processors and light-weight robots. Software Product Lines allow the end user to deploy and configure complex robotics applications without dealing with low-level problems such as configuring algorithms and designing architectures. This paper discusses the proposed method in depth, and demonstrates its advantages with a case study.},
booktitle = {Proceedings of the 2014 IEEE International Conference on Cloud Computing},
pages = {745–752},
numpages = {8},
keywords = {Cloud Computing, Robotics, Software Product Lines},
series = {CLOUD '14}
}

@article{10.1016/j.infsof.2019.01.008,
author = {Meqdadi, Omar and Alhindawi, Nouh and Alsakran, Jamal and Saifan, Ahmad and Migdadi, Hatim},
title = {Mining software repositories for adaptive change commits using machine learning techniques},
year = {2019},
issue_date = {May 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {109},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.01.008},
doi = {10.1016/j.infsof.2019.01.008},
journal = {Inf. Softw. Technol.},
month = may,
pages = {80–91},
numpages = {12},
keywords = {Code change metrics, Adaptive maintenance, Commit types, Maintenance classification, Machine learning}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Controlled experimentation, Feature model, Maintainability, Quality attributes, Software prediction model, Software product line, Structural complexity}
}

@article{10.1016/j.infsof.2012.11.008,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Goseva-Popstojanova, Katerina and Dorman, Karin S.},
title = {Predicting failure-proneness in an evolving software product line},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.008},
doi = {10.1016/j.infsof.2012.11.008},
abstract = {ContextPrevious work by researchers on 3years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. ObjectiveThe work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. MethodThis investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. ResultsOur experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. ConclusionAs the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1479–1495},
numpages = {17},
keywords = {Change metrics, Failure-prone files, Post-release defects, Prediction, Reuse, Software product lines}
}

@inproceedings{10.1109/WAIN52551.2021.00016,
author = {Staron, Miroslaw and Herg\`{e}s, Helena Odenstedt and Naredi, Silvana and Block, Linda and El-Merhi, Ali and Vithal, Richard and Elam, Mikael},
title = {Robust Machine Learning in Critical Care — Software Engineering and Medical Perspectives},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00016},
doi = {10.1109/WAIN52551.2021.00016},
abstract = {Using machine learning in clinical practice poses hard requirements on explainability, reliability, replicability and robustness of these systems. Therefore, developing reliable software for monitoring critically ill patients requires close collaboration between physicians and software engineers. However, these two different disciplines need to find own research perspectives in order to contribute to both the medical and the software engineering domain. In this paper, we address the problem of how to establish a collaboration where software engineering and medicine meets to design robust machine learning systems to be used in patient care. We describe how we designed software systems for monitoring patients under carotid endarterectomy, in particular focusing on the process of knowledge building in the research team. Our results show what to consider when setting up such a collaboration, how it develops over time and what kind of systems can be constructed based on it. We conclude that the main challenge is to find a good research team, where different competences are committed to a common goal.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {62–69},
numpages = {8},
location = {Madrid, Spain}
}

@inproceedings{10.1007/978-3-030-78361-7_26,
author = {Fujinuma, Ryota and Asahi, Yumi},
title = {Proposal of Credit Risk Model Using Machine Learning in Motorcycle Sales},
year = {2021},
isbn = {978-3-030-78360-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78361-7_26},
doi = {10.1007/978-3-030-78361-7_26},
abstract = {While the new BIS regulations are reviewing the way of thinking about loans all over the world, many people in Central and South America still have a vague way of thinking about loans. It is due to the global recession. As a result, companies have not been able to recover their manufacturing costs. Therefore, in this study, we create a classification model of customers who default and customers who do not default. Also, explore the characteristics of the default customers. This is because it is thought that it will be easier for companies to improve the loan problem and secure profits.In this study, we compare the accuracy of Random Forest and XG boost. Since the data handled in this study were unbalanced data, data expansion by Synthetic Minority Over-sampling Technique (SMOTE) was effective. Mainly the accuracy of Recall has increased by 30%. Feature selection is performed by correlation, which is one of the filter methods. This can be expected to have the effect of improving accuracy and the effect of improving the interpretability of the model. We were able to reduce it from 46 variables to 22 variables. Furthermore, the accuracy increased by 1% for Binary Accuracy and 1% for Recall. The accuracy decreased when the number of variables was reduced by 23 variables or more. This is probably because important features have been deleted. Shows the accuracy of the model. The accuracy of Random Forest is Binary Accuracy = 61.3%, Recall = 58.2%. The accuracy of XGboost is Binary Accuracy = 60.3%, Recall = 61.6%. Therefore, XG boost became the model that can identify the default of the customer than the random forest.Finally, SHApley Additive exPlanations (SHAP) analyzes what variables contribute to the model. From this analysis result, we will explore the characteristics of what kind of person is the default customer. The variables with the highest contribution were the type of vehicle purchased, the area where the customer lives, and credit information. It turns out that customers who have gone loan bankruptcy in the past tend to be loan bankruptcy again.},
booktitle = {Human Interface and the Management of Information. Information-Rich and Intelligent Environments: Thematic Area, HIMI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II},
pages = {353–363},
numpages = {11},
keywords = {Loan, Loan bankruptcy, Credit risk model, Machine learning}
}

@article{10.1007/s11219-011-9156-5,
author = {Roos-Frantz, Fabricia and Benavides, David and Ruiz-Cort\'{e}s, Antonio and Heuer, Andr\'{e} and Lauenroth, Kim},
title = {Quality-aware analysis in product line engineering with the orthogonal variability model},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9156-5},
doi = {10.1007/s11219-011-9156-5},
abstract = {Software product line engineering is about producing a set of similar products in a certain domain. A variability model documents the variability amongst products in a product line. The specification of variability can be extended with quality information, such as measurable quality attributes (e.g., CPU and memory consumption) and constraints on these attributes (e.g., memory consumption should be in a range of values). However, the wrong use of constraints may cause anomalies in the specification which must be detected (e.g., the model could represent no products). Furthermore, based on such quality information, it is possible to carry out quality-aware analyses, i.e., the product line engineer may want to verify whether it is possible to build a product that satisfies a desired quality. The challenge for quality-aware specification and analysis is threefold. First, there should be a way to specify quality information in variability models. Second, it should be possible to detect anomalies in the variability specification associated with quality information. Third, there should be mechanisms to verify the variability model to extract useful information, such as the possibility to build a product that fulfils certain quality conditions (e.g., is there any product that requires less than 512 MB of memory?). In this article, we present an approach for quality-aware analysis in software product lines using the orthogonal variability model (OVM) to represent variability. We propose to map variability represented in the OVM associated with quality information to a constraint satisfaction problem and to use an off-the-shelf constraint programming solver to automatically perform the verification task. To illustrate our approach, we use a product line in the automotive domain which is an example that was created in a national project by a leading car company. We have developed a prototype tool named FaMa-OVM, which works as a proof of concepts. We were able to identify void models, dead and false optional elements, and check whether the product line example satisfies quality conditions.},
journal = {Software Quality Journal},
month = sep,
pages = {519–565},
numpages = {47},
keywords = {Automated analysis, Orthogonal variability model, Quality modelling, Quality-aware analysis, Software product lines}
}

@article{10.1155/2021/4767388,
author = {Soleymani, Ali and Arabgol, Fatemeh and Shojae Chaeikar, Saman},
title = {A Novel Approach for Detecting DGA-Based Botnets in DNS Queries Using Machine Learning Techniques},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {2090-7141},
url = {https://doi.org/10.1155/2021/4767388},
doi = {10.1155/2021/4767388},
abstract = {In today’s security landscape, advanced threats are becoming increasingly difficult to detect as the pattern of attacks expands. Classical approaches that rely heavily on static matching, such as blacklisting or regular expression patterns, may be limited in flexibility or uncertainty in detecting malicious data in system data. This is where machine learning techniques can show their value and provide new insights and higher detection rates. The behavior of botnets that use domain-flux techniques to hide command and control channels was investigated in this research. The machine learning algorithm and text mining used to analyze the network DNS protocol and identify botnets were also described. For this purpose, extracted and labeled domain name datasets containing healthy and infected DGA botnet data were used. Data preprocessing techniques based on a text-mining approach were applied to explore domain name strings with n-gram analysis and PCA. Its performance is improved by extracting statistical features by principal component analysis. The performance of the proposed model has been evaluated using different classifiers of machine learning algorithms such as decision tree, support vector machine, random forest, and logistic regression. Experimental results show that the random forest algorithm can be used effectively in botnet detection and has the best botnet detection accuracy.},
journal = {J. Comput. Netw. Commun.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {artificial intelligence, configuration, feature model, planning techniques, software product line engineering},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.jss.2007.12.797,
author = {Ajila, Samuel A. and Kaba, Ali B.},
title = {Evolution support mechanisms for software product line process},
year = {2008},
issue_date = {October, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {10},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.12.797},
doi = {10.1016/j.jss.2007.12.797},
abstract = {Software product family process evolution needs specific support for incremental change. Product line process evolution involves in addition to identifying new requirements the building of a meta-process describing the migration from the old process to the new one. This paper presents basic mechanisms to support software product line process evolution. These mechanisms share four strategies - change identification, change impact, change propagation, and change validation. It also examines three kinds of evolution processes - architecture, product line, and product. In addition, change management mechanisms are identified. Specifically we propose support mechanisms for static local entity evolution and complex entity evolution including transient evolution process. An evolution model prototype based on dependency relationships structure of the various product line artifacts is developed.},
journal = {J. Syst. Softw.},
month = oct,
pages = {1784–1801},
numpages = {18},
keywords = {Feature-based object oriented model, Meta-process, Product line architecture, Software development process, Software product line process evolution, Transient process, Use case modeling}
}

@inproceedings{10.1145/2934466.2946047,
author = {Li, Li and Martinez, Jabier and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques and Traon, Yves Le},
title = {Mining families of android applications for extractive SPL adoption},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946047},
doi = {10.1145/2934466.2946047},
abstract = {The myriads of smart phones around the globe gave rise to a vast proliferation of mobile applications. These applications target an increasing number of user profiles and tasks. In this context, Android is a leading technology for their development and on-line markets are the main means for their distribution. In this paper we motivate, from two perspectives, the mining of these markets with the objective to identify families of apps variants in the wild. The first perspective is related to research activities where building realistic case studies for evaluating extractive SPL adoption techniques are needed. The second is related to a large-scale, world-wide and time-aware study of reuse practice in an industry which is now flourishing among all others within the software engineering community. This study is relevant to assess potential for SPLE practices adoption. We present initial implementations of the mining process and we discuss analyses of variant families.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {271–275},
numpages = {5},
keywords = {android, appvariants, mining software repositories, reverse engineering, software product line engineering},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/1858996.1859021,
author = {Kim, Chang Hwan Peter and Batory, Don and Khurshid, Sarfraz},
title = {Eliminating products to test in a software product line},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859021},
doi = {10.1145/1858996.1859021},
abstract = {A Software Product Line (SPL) is a family of programs where each program is defined by a unique combination of features. Developing a set of programs with commonalities and variabilities in this way can significantly reduce both the time and cost of software development. However, as the number of programs may be exponential in the number of features, testing an SPL, the phase to which the majority of software development is dedicated, becomes especially challenging [12].Indeed, scale is the biggest challenge in testing or checking the properties of programs in a product line. Even a product line with just 10 optional features has over a thousand (210) distinct programs. As an example of a situation where every program must be considered, suppose that every program of an SPL outputs a String that each feature might modify.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {139–142},
numpages = {4},
keywords = {feature oriented programming, software product lines, static analysis, testing},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@phdthesis{10.5555/AAI28022522,
author = {Islam, Md Johirul and Ciardo, Gianfranco and Prabhu, Gurpur and Tian, Jin and Sharma, Anuj},
advisor = {Hridesh, Rajan,},
title = {Towards Understanding the Challenges Faced by Machine Learning Software Developers and Enabling Automated Solutions},
year = {2020},
isbn = {9798672106496},
publisher = {Iowa State University},
address = {USA},
abstract = {Modern software systems are increasingly including machine learning (ML) as an integral component. However, we do not yet understand the difficulties faced by software developers when learning about ML libraries and using them within their systems. To fill that gap this thesis reports on a detailed (manual) examination of 3,243 highly-rated Q&amp;A posts related to ten ML libraries, namely Tensorflow, Keras, scikitlearn, Weka, Caffe, Theano, MLlib, Torch, Mahout, and H2O, on Stack Overflow, a popular online technical Q&amp;A forum. Our findings reveal the urgent need for software engineering (SE) research in this area.The second part of the thesis particularly focuses on understanding the Deep Neural Network (DNN) bug characteristics. We study 2,716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, their root causes and impacts, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software.While exploring the bug characteristics, our findings imply that repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. So, the third part of this thesis presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from Github for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns and the most common bug fix patterns are fixing data dimension and neural network connectivity.Finally, we propose an automatic technique to detect ML Application Programming Interface (API) misuses. We started with an empirical study to understand ML API misuses. Our study shows that ML API misuse is prevalent and distinct compared to non-ML API misuses. Inspired by these findings, we contributed Amimla (Api Misuse In Machine Learning Apis) an approach and a tool for ML API misuse detection. Amimla relies on several technical innovations. First, we proposed an abstract representation of ML pipelines to use in misuse detection. Second, we proposed an abstract representation of neural networks for deep learning related APIs. Third, we have developed a representation strategy for constraints on ML APIs. Finally, we have developed a misuse detection strategy for both single and multi-APIs. Our experimental evaluation shows that Amimla achieves a high average accuracy of ∼80% on two benchmarks of misuses from Stack Overflow and Github.},
note = {AAI28022522}
}

@article{10.1016/j.cosrev.2021.100376,
author = {Amutha, J. and Sharma, Sandeep and Sharma, Sanjay Kumar},
title = {Strategies based on various aspects of clustering in wireless sensor networks using classical, optimization and machine learning techniques: Review, taxonomy, research findings, challenges and future directions},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2021.100376},
doi = {10.1016/j.cosrev.2021.100376},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {43},
keywords = {Wireless Sensor Networks, Optimization, Machine learning, Routing, Security, Reliability}
}

@article{10.1155/2019/4368036,
author = {Deli\'{c}, Vlado and Peri\'{c}, Zoran and Se\v{c}ujski, Milan and Jakovljevi\'{c}, Nik\v{s}a and Nikoli\'{c}, Jelena and Mi\v{s}kovi\'{c}, Dragi\v{s}a and Simi\'{c}, Nikola and Suzi\'{c}, Sini\v{s}a and Deli\'{c}, Tijana and Gastaldo, Paolo},
title = {Speech Technology Progress Based on New Machine Learning Paradigm},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1687-5265},
url = {https://doi.org/10.1155/2019/4368036},
doi = {10.1155/2019/4368036},
abstract = {Speech technologies have been developed for decades as a typical signal processing area, while the last decade has brought a huge progress based on new machine learning paradigms. Owing not only to their intrinsic complexity but also to their relation with cognitive sciences, speech technologies are now viewed as a prime example of interdisciplinary knowledge area. This review article on speech signal analysis and processing, corresponding machine learning algorithms, and applied computational intelligence aims to give an insight into several fields, covering speech production and auditory perception, cognitive aspects of speech communication and language understanding, both speech recognition and text-to-speech synthesis in more details, and consequently the main directions in development of spoken dialogue systems. Additionally, the article discusses the concepts and recent advances in speech signal compression, coding, and transmission, including cognitive speech coding. To conclude, the main intention of this article is to highlight recent achievements and challenges based on new machine learning paradigms that, over the last decade, had an immense impact in the field of speech signal processing.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@inproceedings{10.5555/1732003.1732062,
author = {Chiang, Chia-Chu and Marshall, Bill},
title = {Applying software product line technology to prototyping of real-time object tracking},
year = {2009},
isbn = {9781424427932},
publisher = {IEEE Press},
abstract = {In this paper, a software product line (SPL) architecture that explicitly captures common features of real-time object tracking systems using Cricket wireless sensors is presented. A software product line process is also presented including user requirements, architecture design, component development, and systems integration. The focus lies on the application of SPL to object location tracking, such as supply chains and transportation. Thus, this paper introduces three prototypes of the SPL member system including shop navigator systems, low/no visibility navigation systems (LVNS), and games for mazes where each one is created from the software product line architecture. The prototypes are experimented with and lessons are learned.},
booktitle = {Proceedings of the 2009 IEEE International Conference on Systems, Man and Cybernetics},
pages = {2099–2104},
numpages = {6},
keywords = {cricket, incremental development, indoor positioning, object tracking, rapid prototyping, software architectural styles, software architecture, software product line (SPL)},
location = {San Antonio, TX, USA},
series = {SMC'09}
}

@article{10.1145/2000799.2000803,
author = {Dehlinger, Josh and Lutz, Robyn R.},
title = {Gaia-PL: A Product Line Engineering Approach for Efficiently Designing Multiagent Systems},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000799.2000803},
doi = {10.1145/2000799.2000803},
abstract = {Agent-oriented software engineering (AOSE) has provided powerful and natural, high-level abstractions in which software developers can understand, model and develop complex, distributed systems. Yet, the realization of AOSE partially depends on whether agent-based software systems can achieve reductions in development time and cost similar to other reuse-conscious development methods. Specifically, AOSE does not adequately address requirements specifications as reusable assets. Software product line engineering is a reuse technology that supports the systematic development of a set of similar software systems through understanding, controlling, and managing their common, core characteristics and their differing variation points. In this article, we present an extension to the Gaia AOSE methodology, named Gaia-PL (Gaia-Product Line), for agent-based distributed software systems that enables requirements specifications to be easily reused. We show how our methodology uses a product line perspective to promote reuse in agent-based software systems early in the development life cycle so that software assets can be reused throughout system development and evolution. We also present results from an application to show how Gaia-PL provided reuse that reduced the design and development effort for a large, multiagent system.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {17},
numpages = {27},
keywords = {Agent-oriented software engineering, software product line engineering}
}

@inproceedings{10.1145/1985441.1985458,
author = {Krishnan, Sandeep and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Empirical evaluation of reliability improvement in an evolving software product line},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985458},
doi = {10.1145/1985441.1985458},
abstract = {Reliability is important to software product-line developers since many product lines require reliable operation. It is typically assumed that as a software product line matures, its reliability improves. Since post-deployment failures impact reliability, we study this claim on an open-source software product line, Eclipse. We investigate the failure trend of common components (reused across all products), highreuse variation components (reused in five or six products) and low-reuse variation components (reused in one or two products) as Eclipse evolves. We also study how much the common and variation components change over time both in terms of addition of new files and modification of existing files. Quantitative results from mining and analysis of the Eclipse bug and release repositories show that as the product line evolves, fewer serious failures occur in components implementing commonality, and that these components also exhibit less change over time. These results were roughly as expected. However, contrary to expectation, components implementing variations, even when reused in five or more products, continue to evolve fairly rapidly. Perhaps as a result, the number of severe failures in variation components shows no uniform pattern of decrease over time. The paper describes and discusses this and related results.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {103–112},
numpages = {10},
keywords = {change, failures, reliability, reuse, software product lines},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.5555/2004685.2005507,
author = {Engstr\"{o}m, Emelie and Runeson, Per},
title = {Decision Support for Test Management and Scope Selection in a Software Product Line Context},
year = {2011},
isbn = {9780769543451},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In large software organizations with a product line development approach, system test planning and scope selection is a complex tasks for which tool support is needed. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of double testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests. This paper discusses the need and challenges of providing decision support for test planning and test selection in a product line context, and highlights possible paths towards a pragmatic implementation of context-specific decision support of various levels of automation. With existing regression testing approaches it is possible to provide automated decision support in a few specific cases, while test management in general may be supported through visualization of test execution coverage, the testing space and the delta between the sufficiently tested system and the system under test. A better understanding of the real world context and how to map research results to the same is needed.},
booktitle = {Proceedings of the 2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops},
pages = {262–265},
numpages = {4},
keywords = {decision support, regression testing, software product line testing, test coverage, test selection, visualization},
series = {ICSTW '11}
}

@article{10.1016/j.imavis.2016.11.013,
author = {Yue, Zongsheng and Meng, Deyu and He, Juan and Zhang, Gemeng},
title = {Semi-supervised learning through adaptive Laplacian graph trimming},
year = {2017},
issue_date = {April 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {60},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.11.013},
doi = {10.1016/j.imavis.2016.11.013},
abstract = {Graph-based semi-supervised learning (GSSL) attracts considerable attention in recent years. The performance of a general GSSL method relies on the quality of Laplacian weighted graph (LWR) composed of the similarity imposed on input examples. A key for constructing an effective LWR is on the proper selection of the neighborhood size K or on the construction of KNN graph or -neighbor graph on training samples, which constitutes the fundamental elements in LWR. Specifically, too large K or will result in shortcut phenomenon while too small ones cannot guarantee to represent a complete manifold structure underlying data. To this issue, this study attempts to propose a method, called adaptive Laplacian graph trimming (ALGT), to make an automatic tuning to cut improper inter-cluster shortcut edges while enhance the connection between intra-cluster samples, so as to adaptively fit a proper LWR from data. The superiority of the proposed method is substantiated by experimental results implemented on synthetic and UCI data sets. A method which can adaptively fit a proper Laplacian weighted graph from data.A penalty helping cut inter-cluster shortcuts and enhance intra-cluster connections.A graph-based SSL model is less sensitive to neighborhood size by integrating ALGT.Superiority of ALGT is verified by experimental results on synthetic and UCI data.},
journal = {Image Vision Comput.},
month = apr,
pages = {38–47},
numpages = {10},
keywords = {Graph Laplacian, Nearest neighborhood graph, Self-paced learning, Semi-supervised learning}
}

@inproceedings{10.1145/3430895.3460135,
author = {Bernius, Jan Philip and Krusche, Stephan and Bruegge, Bernd},
title = {A Machine Learning Approach for Suggesting Feedback in Textual Exercises in Large Courses},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460135},
doi = {10.1145/3430895.3460135},
abstract = {Open-ended textual exercises facilitate the comprehension of problem-solving skills. Students can learn from their mistakes when teachers provide individual feedback. However, courses with hundreds of students cause a heavy workload for teachers: providing individual feedback is mostly a manual, repetitive, and time-consuming activity.This paper presents CoFee, a machine learning approach designed to suggest computer-aided feedback in open-ended textual exercises. The approach uses topic modeling to split student answers into text segments and language embeddings to transform these segments. It then applies clustering to group the text segments by similarity so that the same feedback can be applied to all segments within the same cluster.We implemented this approach in a reference implementation called Athene and integrated it into Artemis. We used Athene to review 17 textual exercises in two large courses at the Technical University of Munich with 2,300 registered students and 53 teachers. On average, Athene suggested feedback for 26% of the submissions. Accordingly, 85% of these suggestions were accepted by the teachers, 5% were extended with a comment and then accepted, and 10% were changed.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {173–182},
numpages = {10},
keywords = {assessment support system, automatic assessment, education, feedback, grading, interactive learning, learning, software engineering},
location = {Virtual Event, Germany},
series = {L@S '21}
}

@article{10.1016/j.knosys.2018.04.006,
author = {Lee, Gichang and Jeong, Jaeyun and Seo, Seungwan and Kim, CzangYeob and Kang, Pilsung},
title = {Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network},
year = {2018},
issue_date = {July 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {152},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.04.006},
doi = {10.1016/j.knosys.2018.04.006},
abstract = {In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment (positive/negative) of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is a lack of information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network (CNN). In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. To verify the proposed methodology, we evaluated the classification accuracy and the rate of polarity words among high scoring words using two movie review datasets. Experimental results show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores.},
journal = {Know.-Based Syst.},
month = jul,
pages = {70–82},
numpages = {13},
keywords = {Class activation mapping, Convolutional neural network, Sentiment analysis, Weakly supervised learning, Word localization}
}

@inproceedings{10.1145/3383219.3383229,
author = {Li, Yang and Schulze, Sandro and Xu, Jiahua},
title = {Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383229},
doi = {10.1145/3383219.3383229},
abstract = {In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Feature Extraction, Feature Terms Identification, Requirement Documents, Software Product Lines},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/3382025.3414952,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Gasca, Rafael M. and Carmona-Fombella, Jose Antonio and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa},
title = {AMADEUS: towards the AutoMAteD secUrity teSting},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414952},
doi = {10.1145/3382025.3414952},
abstract = {The proper configuration of systems has become a fundamental factor to avoid cybersecurity risks. Thereby, the analysis of cybersecurity vulnerabilities is a mandatory task, but the number of vulnerabilities and system configurations that can be threatened is extremely high. In this paper, we propose a method that uses software product line techniques to analyse the vulnerable configuration of the systems. We propose a solution, entitled AMADEUS, to enable and support the automatic analysis and testing of cybersecurity vulnerabilities of configuration systems based on feature models. AMADEUS is a holistic solution that is able to automate the analysis of the specific infrastructures in the organisations, the existing vulnerabilities, and the possible configurations extracted from the vulnerability repositories. By using this information, AMADEUS generates automatically the feature models, that are used for reasoning capabilities to extract knowledge, such as to determine attack vectors with certain features. AMADEUS has been validated by demonstrating the capacities of feature models to support the threat scenario, in which a wide variety of vulnerabilities extracted from a real repository are involved. Furthermore, we open the door to new applications where software product line engineering and cybersecurity can be empowered.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {11},
numpages = {12},
keywords = {cybersecurity, feature model, pentesting, reasoning, testing, vulnerabilities, vulnerable configuration},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1007/s10664-019-09769-8,
author = {Ochodek, Miroslaw and Hebig, Regina and Meding, Wilhelm and Frost, Gert and Staron, Miroslaw},
title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09769-8},
doi = {10.1007/s10664-019-09769-8},
abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {220–265},
numpages = {46},
keywords = {Measurement, Machine learning, Action research, Code reviews}
}

@inproceedings{10.1007/978-3-030-58817-5_66,
author = {Kumari, Madhu and Singh, Ujjawal Kumar and Sharma, Meera},
title = {Entropy Based Machine Learning Models for Software Bug Severity Assessment in Cross Project Context},
year = {2020},
isbn = {978-3-030-58816-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58817-5_66},
doi = {10.1007/978-3-030-58817-5_66},
abstract = {There can be noise and uncertainty in the bug reports data as the bugs are reported by a heterogeneous group of users working across different countries. Bug description is an essential attribute that helps to predict other bug attributes, such as severity, priority, and time fixes. We need to consider the noise and confusion present in the text of the bug report, as it can impact the output of different machine learning techniques. Shannon entropy has been used in this paper to calculate summary uncertainty about the bug. Bug severity attribute tells about the type of impact the bug has on the functionality of the software. Correct bug severity estimation allows scheduling and repair bugs and hence help in resource and effort utilization. To predict the severity of the bug we need software project historical data to train the classifier. These training data are not always available in particular for new software projects. The solution which is called cross project prediction is to use the training data from other projects. Using bug priority, summary weight and summary entropy, we have proposed cross project bug severity assessment models. Results for proposed summary entropy based approach for bug severity prediction in cross project context show improved performance of the Accuracy and F-measure up to 70.23% and 93.72% respectively across all the machine learning techniques over existing work.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part VI},
pages = {939–953},
numpages = {15},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3236405.3237205,
author = {Ziadi, Tewfik and Martinez, Jabier and T\"{e}rnava, Xhevahire},
title = {Teaching projects and research objectives in SPL extraction},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3237205},
doi = {10.1145/3236405.3237205},
abstract = {This year at SPLC we present a teaching and research project where a group of master students analysed a variability-rich domain and extracted an SPL (The Robocode SPL). We present the results of such extraction augmented with an analysis and a quantification regarding the time and effort spent. The research objective was to get and share data about an end-to-end SPL extraction which is usually unavailable in industrial cases because of their large size, complexity, and duration. We provide all the material to replicate, reproduce or extend the case study so it can be easily reused for teaching by anyone in our community. However, we were asking ourselves how can we leverage such case study for teaching to pursue research objectives. In this position paper, we aim to outline our initial ideas that we want to enrich with the others' viewpoints during SPLTea. Towards planning the settings of future teaching projects around this Robocode SPL case study, which can be the timely research objectives that we can identify? Can we involve others in planning this project in their institutions to get further relevant results?},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {44–45},
numpages = {2},
keywords = {extractive software product line adoption, reverse-engineering, software product lines, teaching},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461001.3474301,
author = {Schmid, Klaus and Rabiser, Rick and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {Bridging the gap: voices from industry and research on industrial relevance of SPLC},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3474301},
doi = {10.1145/3461001.3474301},
abstract = {Product line engineering emerged from a fruitful interaction of applied research in academia, industry research, and software engineering practice. SPLC was created as the primary venue to exchange ideas on this emerging topic and integrate the communities. Yet, today, SPLC is mostly regarded as an academic conference with little industry participation. Since a strong integration of academia and industry is often seen positive, here, we try to better understand motivations for practitioners to visit academic conferences like SPLC and the impact this has on such conferences. This analysis is based on nine systematic interviews with practitioners and researchers, who have been members of the SPLC community and other leading software engineering communities for a long time. Our preliminary results clarify the relevance and interest of practitioners and researchers to exchange knowledge and learn when attending scientific software engineering conferences such as SPLC. Yet, the results also highlight the differences between the goals of industry and academic conference participants, which often lead to inefficiencies and even barriers for constructive interaction at scientific conferences such as SPLC. We use this as a basis for pointing out further discussion points, both from the perspective of the interviewees as well as the authors.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {184–189},
numpages = {6},
keywords = {SPLC, industry participation, industry-academia relations, scientific software engineering conferences, software product lines},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1016/j.eswa.2020.114176,
author = {AlOmar, Eman Abdullah and Peruma, Anthony and Mkaouer, Mohamed Wiem and Newman, Christian and Ouni, Ali and Kessentini, Marouane},
title = {How we refactor and how we document it? On the use of supervised machine learning algorithms to classify refactoring documentation},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114176},
doi = {10.1016/j.eswa.2020.114176},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {26},
keywords = {Refactoring, Software quality, Software engineering, Machine learning}
}

@article{10.5555/3322706.3362023,
author = {Chew, Rob and Wenger, Michael and Kery, Caroline and Nance, Jason and Richards, Keith and Hadley, Emily and Baumgartner, Peter},
title = {SMART: an open source data labeling platform for supervised learning},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {SMART is an open source web application designed to help data scientists and research teams efficiently build labeled training data sets for supervised machine learning tasks. SMART provides users with an intuitive interface for creating labeled data sets, supports active learning to help reduce the required amount of labeled data, and incorporates interrater reliability statistics to provide insight into label quality. SMART is designed to be platform agnostic and easily deployable to meet the needs of as many different research teams as possible. The project website contains links to the code repository and extensive user documentation.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2999–3003},
numpages = {5},
keywords = {active learning, data labeling, open source, software, supervised learning}
}

@inproceedings{10.1145/3307630.3342407,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Exploring the Variability of Interconnected Product Families with Relational Concept Analysis},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342407},
doi = {10.1145/3307630.3342407},
abstract = {Among the various directions that SPLE promotes, extractive adoption of complex product lines is especially valuable, provided that appropriate approaches are made available. Complex variability can be encoded in different ways, including the feature model (FM) formalism extended with multivalued attributes, UML-like cardinalities, and references connecting separate FMs. In this paper, we address the extraction of variability relationships depicting connections between systems from separate families. Because Formal Concept Analysis provides suitable knowledge structures to represent the variability of a given system family, we explore the relevance of Relational Concept Analysis, an FCA extension to take into account relationships between different families, to tackle this issue. We investigate a method to extract variability information from descriptions representing several inter-connected product families. It aims to be used to assist the design of inter-connected FMs, and to provide recommendations during product selection.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {199–206},
numpages = {8},
keywords = {complex software product line, relational concept analysis, reverse engineering, variability extraction},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3236405.3236426,
author = {Belarbi, Maouaheb},
title = {A methodological framework to enable the generation of code from DSML in SPL},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236426},
doi = {10.1145/3236405.3236426},
abstract = {Software Product Line has acquired a significant momentum at the end of the 1990ies since it allows the production of variable software systems corresponding to the same domain portfolio. The effectiveness of the derivation process depends on how well variability is defined and implemented which is a crucial topic area that was addressed among two essential trends: On the one hand, starting from Domain Specific Modelling Language to express domain requirements and automate the code generation with Model-Driven Engineering techniques and on the second hand, exploiting the soar of variability mechanisms.In this context, the current research presents a method that unifies the two aforementioned approaches to cover the overall strategies by defining a framework that allows a better code generation in terms of documentation, maintainability, rapidity,etc. The starting point is the usage of the Domain Specific Modelling Language to represent the stakeholders requirements. Then, the resulting meta-model will be converted into one our several Feature Diagrams on which variability mechanisms can be applied to generate all the family products.A preliminary experiment has been undertaken to design the methodology of the proposed software factory in a meta-model. The validation task was evaluated with an academic use case called HandiWeb developed to facilitate handicap persons access to the internet. The first results allow us to put the hand on the key challenges that must be resolved by the proposed methodology.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {64–71},
numpages = {8},
keywords = {DSML, SPL, methodology, software factory, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {classification, defect, feature, prediction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.eswa.2014.08.046,
author = {Kazemian, H.B. and Ahmed, S.},
title = {Comparisons of machine learning techniques for detecting malicious webpages},
year = {2015},
issue_date = {February 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.08.046},
doi = {10.1016/j.eswa.2014.08.046},
abstract = {3 supervised and 2 unsupervised techniques are modeled to detect malicious webpages.Supervised machine learning (ML) techniques accuracies are above 89%.Unsupervised ML techniques accuracies have at least a silhouette coefficient of 0.87.Information obtained from URLs, page links, semantics and visual features of webpages.Chrome extension, lightweight and heavyweight classifiers and online learning are used. This paper compares machine learning techniques for detecting malicious webpages. The conventional method of detecting malicious webpages is going through the black list and checking whether the webpages are listed. Black list is a list of webpages which are classified as malicious from a user's point of view. These black lists are created by trusted organizations and volunteers. They are then used by modern web browsers such as Chrome, Firefox, Internet Explorer, etc. However, black list is ineffective because of the frequent-changing nature of webpages, growing numbers of webpages that pose scalability issues and the crawlers' inability to visit intranet webpages that require computer operators to log in as authenticated users. In this paper therefore alternative and novel approaches are used by applying machine learning algorithms to detect malicious webpages. In this paper three supervised machine learning techniques such as K-Nearest Neighbor, Support Vector Machine and Naive Bayes Classifier, and two unsupervised machine learning techniques such as K-Means and Affinity Propagation are employed. Please note that K-Means and Affinity Propagation have not been applied to detection of malicious webpages by other researchers. All these machine learning techniques have been used to build predictive models to analyze large number of malicious and safe webpages. These webpages were downloaded by a concurrent crawler taking advantage of gevent. The webpages were parsed and various features such as content, URL and screenshot of webpages were extracted to feed into the machine learning models. Computer simulation results have produced an accuracy of up to 98% for the supervised techniques and silhouette coefficient of close to 0.96 for the unsupervised techniques. These predictive models have been applied in a practical context whereby Google Chrome can harness the predictive capabilities of the classifiers that have the advantages of both the lightweight and the heavyweight classifiers.},
journal = {Expert Syst. Appl.},
month = feb,
pages = {1166–1177},
numpages = {12},
keywords = {Affinity Propagation, K-Means, K-Nearest Neighbor, Naive Bayes, Supervised and unsupervised learning, Support Vector Machine}
}

@inproceedings{10.5555/3045390.3045466,
author = {Patrini, Giorgio and Nielsen, Frank and Nock, Richard and Carioni, Marcello},
title = {Loss factorization, weakly supervised learning and label noise robustness},
year = {2016},
publisher = {JMLR.org},
abstract = {We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the same loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator -- the focal quantity of this work -- which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation.Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like SGD and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {708–717},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{10.1145/3382025.3414968,
author = {Li, Yang and Schulze, Sandro and Scherrebeck, Helene Hvidegaard and Fogdal, Thomas Sorensen},
title = {Automated extraction of domain knowledge in practice: the case of feature extraction from requirements at danfoss},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414968},
doi = {10.1145/3382025.3414968},
abstract = {Software product line supports structured reuse of software artifacts in order to realize the maintenance and evolution of the typically large number of variants, which promotes the industrialization of software development, especially for software-intensive products. However, for a legacy system, it is non-trivial to gain information about commonalities and differences of the variants. Meanwhile, software requirements specifications as the initial artifacts can be used to achieve this information to generate a domain model. Unfortunately, manually analyzing these requirements is time-consuming and inefficient. To address this problem, we explored the usage of feature extraction techniques to automatically extract domain knowledge from requirements to assist domain engineers. In detail, we applied Doc2Vec and a clustering algorithm to process the requirements for achieving the initial feature tree. Moreover, we utilized key words/phrases extraction techniques to provide key information to domain engineers for further analyzing the extraction results. In particular, we developed a GUI to support the extraction process. The empirical evaluation indicates that most of the extracted features and terms are beneficial to improve the process of feature extraction.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {4},
numpages = {11},
keywords = {feature extraction, requirement documents, reverse engineering, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.procs.2019.12.133,
author = {Rahouma, Kamel H. and Ali, Ayman},
title = {Applying Machine Learning Technology to Optimize the Operational Cost of the Egyptian Optical Network},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {163},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.133},
doi = {10.1016/j.procs.2019.12.133},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {502–517},
numpages = {16},
keywords = {Machine Learning Technology, Optical Network, Operational Cost, Intelligent Universal Platform}
}

@inproceedings{10.1109/ASE.2009.90,
author = {Nguyen, Tung Thanh and Nguyen, Hoan Anh and Pham, Nam H. and Al-Kofahi, Jafar M. and Nguyen, Tien N.},
title = {Clone-Aware Configuration Management},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.90},
doi = {10.1109/ASE.2009.90},
abstract = {Recent research results show several benefits of the management of code clones. In this paper, we introduce Clever, a novel clone-aware software configuration management (SCM) system. In addition to traditional SCM functionality, Clever provides clone management support, including clone detection and update, clone change management, clone consistency validating, clone synchronizing, and clone merging. Clever represents source code and clones as (sub)trees in Abstract Syntax Trees (ASTs), measures code similarity based on structural characteristic vectors, and describes code changes as tree editing scripts. The key techniques of Clever include the algorithms to compute tree editing scripts; to detect and update code clones and their groups; and to analyze the changes of cloned code to validate their consistency and recommend the relevant synchronization. Our empirical study on many real-world programs shows that Clever is highly efficient and accurate in clone detection and updating, and provides useful analysis of clone changes.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {123–134},
numpages = {12},
keywords = {clone editing consistency, clone management, clone synchronization, clone-aware, tree edit},
series = {ASE '09}
}

@inproceedings{10.1007/978-3-540-88582-5_50,
author = {Jiang, Michael and Zhang, Jing and Zhao, Hong and Zhou, Yuanyuan},
title = {Enhancing Software Product Line Maintenance with Source Code Mining},
year = {2008},
isbn = {9783540885818},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-88582-5_50},
doi = {10.1007/978-3-540-88582-5_50},
abstract = {Large-scale reuse and accelerated software development have been some of the key attractions behind software product lines. Various strategies and processes have been developed to facilitate product line development, maintenance, and evolution. However, experiences with software product lines also showed that it is a rather challenging task to maintain software product lines and families over a long period of time. The time and effort needed to manage and maintain product lines increase and quality degrades as product lines evolve. Without proper methods and tools to support the evolution, the cost can outweigh the benefits.This paper describes an approach to simplifying the maintenance of software product lines and improving software quality by integrating traditional software maintenance practices with pattern-based source code mining for defect detection and correction. Our case studies were performed in an industrial setting where the evolution of multiple mobile phone models of a product line was investigated.},
booktitle = {Proceedings of the Third International Conference on Wireless Algorithms, Systems, and Applications},
pages = {538–547},
numpages = {10},
keywords = {Product Line, Reuse, Software Maintenance},
location = {Dallas, Texas},
series = {WASA '08}
}

@article{10.1016/j.infsof.2012.07.010,
author = {Buchmann, Thomas and Dotor, Alexander and Westfechtel, Bernhard},
title = {MOD2-SCM: A model-driven product line for software configuration management systems},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.010},
doi = {10.1016/j.infsof.2012.07.010},
abstract = {Context: Software Configuration Management (SCM) is the discipline of controlling the evolution of large and complex software systems. Over the years many different SCM systems sharing similar concepts have been implemented from scratch. Since these concepts usually are hard-wired into the respective program code, reuse is hardly possible. Objective: Our objective is to create a model-driven product line for SCM systems. By explicitly describing the different concepts using models, reuse can be performed on the modeling level. Since models are executable, the need for manual programming is eliminated. Furthermore, by providing a library of loosely coupled modules, we intend to support flexible composition of SCM systems. Method: We developed a method and a tool set for model-driven software product line engineering which we applied to the SCM domain. For domain analysis, we applied the FORM method, resulting in a layered feature model for SCM systems. Furthermore, we developed an executable object-oriented domain model which was annotated with features from the feature model. A specific SCM system is configured by selecting features from the feature model and elements of the domain model realizing these features. Results: Due to the orthogonality of both feature model and domain model, a very large number of SCM systems may be configured. We tested our approach by creating instances of the product line which mimic wide-spread systems such as CVS, GIT, Mercurial, and Subversion. Conclusion: The experiences gained from this project demonstrate the feasibility of our approach to model-driven software product line engineering. Furthermore, our work advances the state of the art in the domain of SCM systems since it support the modular composition of SCM systems at the model rather than the code level.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {630–650},
numpages = {21},
keywords = {Code generation, Executable models, Feature models, Model transformation, Model-driven software engineering, Software configuration management, Software product line engineering}
}

@inproceedings{10.1145/3382025.3414967,
author = {Lima, Jackson A. Prado and Mendon\c{c}a, Willian D. F. and Vergilio, Silvia R. and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Learning-based prioritization of test cases in continuous integration of highly-configurable software},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414967},
doi = {10.1145/3382025.3414967},
abstract = {Continuous Integration (CI) is a practice widely adopted in the industry to allow frequent integration of code changes. During the CI process, many test cases are executed multiple times a day, subject to time constraints. In this scenario, a learning-based approach, named COLEMAN, has been successfully applied. COLEMAN allows earlier execution of the most promising test cases to reveal faults. This approach considers CI particularities such as time budget and volatility of test cases, related to the fact that test cases can be added/removed along the CI cycles. In the CI of Highly Configuration System (HCS), many product variants must be tested, each one with different configuration options, but having test cases that are common to or reused from other variants. In this context, we found, by analogy, another particularity, the volatility of variants, that is, some variants can be included/discontinued along CI cycles. Considering this context, this work introduces two strategies for the application of COLEMAN in the CI of HCS: the Variant Test Set Strategy (VTS) that relies on the test set specific for each variant, and the Whole Test Set Strategy (WST) that prioritizes the test set composed by the union of the test cases of all variants. Both strategies are evaluated in a real-world HCS, considering three test budgets. The results show that the proposed strategies are applicable regarding the time spent for prioritization. They perform similarly regarding early fault detection, but WTS better mitigates the problem of beginning without knowledge, and is more suitable when a new variant to be tested is added.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {31},
numpages = {11},
keywords = {continuous integration, family of products, software product line, test case prioritization},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.3233/SW-190377,
author = {Hitzler, Pascal and Janowicz, Krzysztof and \L{}awrynowicz, Agnieszka},
title = {Creative AI: A new avenue for the&nbsp;Semantic&nbsp;Web?},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {11},
number = {1},
issn = {1570-0844},
url = {https://doi.org/10.3233/SW-190377},
doi = {10.3233/SW-190377},
abstract = {Computational Creativity (or artificial creativity) is a multidisciplinary field, researching how to construct computer programs that model, simulate, exhibit or enhance creative behaviour. This vision paper explores a potential of the Semantic Web and its technologies for creative AI. Possible uses of the Semantic Web and semantic technologies are discussed, regarding three types of creativity: i) exploratory creativity, ii) combinational creativity, and iii) transformational creativity and relevant research questions. For exploratory creativity, how can we explore the limits of what is possible, while remaining bound by a set of existing domain axioms, templates, and rules, expressed with semantic technologies? To achieve a combinational creativity, how can we combine or blend existing concepts, frames, ontology design patterns, and other constructs, and benefit from cross-fertilization? Ultimately, can we use ontologies and knowledge graphs, which describe an existing domain with its constraints and, applying a meta-rule for transformational creativity, start dropping constraints and adding new constraints to produce novel artifacts? Together with these new challenges, the paper also provides pointers to emerging and growing application domains of Semantic Web related to computational creativity: from recipe generation to scientific discovery and creative design.},
journal = {Semant. Web},
month = jan,
pages = {69–78},
numpages = {10},
keywords = {Computational creativity, artificial intelligence, Semantic Web, knowledge graph, ontology}
}

@inproceedings{10.1145/3399715.3400860,
author = {Reis, Thoralf and Bornschlegl, Marco X. and Hemmje, Matthias L.},
title = {Big Data Analysis, AI, and Visualization Workshop: Road Mapping Infrastructures for Artificial Intelligence Supporting Advanced Visual Big Data Analysis},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3400860},
doi = {10.1145/3399715.3400860},
abstract = {The overall scope and goal of the workshop is to bring together researchers active in the areas of Artificial Intelligence (AI), Big Data Analysis, and Visualization to achieve a road map, which can support the acceleration in research and data science activities by means of transforming, enriching, and deploying AI models and algorithms as well as intelligent advanced visual user interfaces supporting creation, configuration, management, and usage of distributed Big Data Analysis. Big Data Analysis and AI mutually support each other: AI-powered algorithms empower data scientists to analyze Big Data and thereby exploit its full potential whereas Big Data enables AI experts to comfortably design, validate, and deploy AI models. One of the workshop's objectives is the examination of the importance and necessity of a third, a more straightforward relationship of Big Data and AI: AI supporting all user stereotypes and organizations involved in Big Data Analysis on their exploration journey from raw input data to insight and effectuation.},
booktitle = {Proceedings of the 2020 International Conference on Advanced Visual Interfaces},
articleno = {109},
numpages = {2},
keywords = {AI, AI2VIS4BigData, Big Data Analysis, Visualization},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/2815782.2815799,
author = {Schaefer, Ina and Seidl, Christoph and Cleophas, Loek and Watson, Bruce W.},
title = {SPLicing TABASCO: Custom-Tailored Software Product Line Variants from Taxonomy-Based Toolkits},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815799},
doi = {10.1145/2815782.2815799},
abstract = {Taxonomy-Based Software Construction (TABASCO) applies extensive domain analyses to create conceptual hierarchies of algorithmic domains. Those are used as basis for the implementation of software toolkits. The monolithic structure of TABASCO-based toolkits restricts their adoption on resource-constrained or special-purpose devices. In this paper, we address this problem by applying Software Product Line (SPL) techniques to TABASCO-based toolkits: We use software taxonomies as input to creating a conceptual representation of variability as feature models of an SPL. We apply the variability realization mechanism delta modeling to transform realization artifacts, such as source code, to only contain elements for a particular selection of features. Our method is suitable for proactive, reactive and extractive SPL development so that it supports a seamless adoption and evolution of an SPL approach for TABASCO-based toolkits. We demonstrate the feasibility of the method with three case studies by proactively, reactively and extractively transforming TABASCO-based toolkits to SPLs, which allow derivation of variants with custom-tailored functionality.},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {34},
numpages = {10},
keywords = {Software Product Line (SPL) adoption, Taxonomy-Based Software Construction (TABASCO) toolkit},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3368089.3417063,
author = {Peng, Zi and Yang, Jinqiu and Chen, Tse-Hsun (Peter) and Ma, Lei},
title = {A first look at the integration of machine learning models in complex autonomous driving systems: a case study on Apollo},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417063},
doi = {10.1145/3368089.3417063},
abstract = {Autonomous Driving System (ADS) is one of the most promising and valuable large-scale machine learning (ML) powered systems. Hence, ADS has attracted much attention from academia and practitioners in recent years. Despite extensive study on ML models, it still lacks a comprehensive empirical study towards understanding the ML model roles, peculiar architecture, and complexity of ADS (i.e., various ML models and their relationship with non-trivial code logic). In this paper, we conduct an in-depth case study on Apollo, which is one of the state-of-the-art ADS, widely adopted by major automakers worldwide. We took the first step to reveal the integration of the underlying ML models and code logic in Apollo. In particular, we study the Apollo source code and present the underlying ML model system architecture. We present our findings on how the ML models interact with each other, and how the ML models are integrated with code logic to form a complex system. Finally, we inspect Apollo in a dynamic view and notice the heavy use of model-relevant components and the lack of adequate tests in general. Our study reveals potential maintenance challenges of complex ML-powered systems and identifies future directions to improve the quality assurance of ADS and general ML systems.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1240–1250},
numpages = {11},
keywords = {Autonomous driving systems, Empirical study, Machine learning, Model testing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.5555/1860967.1861221,
author = {Westphal, Matthias and W\"{o}lfl, Stefan and Li, Jason Jingshi},
title = {Restarts and Nogood Recording in Qualitative Constraint-based Reasoning},
year = {2010},
isbn = {9781607506058},
publisher = {IOS Press},
address = {NLD},
abstract = {This paper introduces restart and nogood recording techniques in the domain of qualitative spatial and temporal reasoning. Nogoods and restarts can be applied orthogonally to usual methods for solving qualitative constraint satisfaction problems. In particular, we propose a more general definition of nogoods that allows for exploiting information about nogoods and tractable subclasses during backtracking search. First evaluations of the proposed techniques show promising results.},
booktitle = {Proceedings of the 2010 Conference on ECAI 2010: 19th European Conference on Artificial Intelligence},
pages = {1093–1094},
numpages = {2}
}

@inproceedings{10.1145/3461001.3474452,
author = {He\ss{}, Tobias and Sundermann, Chico and Th\"{u}m, Thomas},
title = {On the scalability of building binary decision diagrams for current feature models},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3474452},
doi = {10.1145/3461001.3474452},
abstract = {Binary decision diagrams (BDD) have been proposed for numerous product-line analyses. These analyses typically exploit properties unique to decision diagrams, such as negation in constant time and space. Furthermore, the existence of a BDD representing the configuration space of a product line removes the need to employ SAT or #SAT solvers for their analysis. Recent work has shown that the performance of state-of-the-art BDD libraries is significantly lower than previously reported and hypothesized. In this work, we provide an assessment of the state-of-the-art of BDD scalability in this domain and explain why previous results on the scalability of BDDs do not apply to more recent product-line instances.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {131–135},
numpages = {5},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3472729,
author = {Abbas, Muhammad and Saadatmand, Mehrdad and Enoiu, Eduard Paul},
title = {Requirements-driven reuse recommendation},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3472729},
doi = {10.1145/3461001.3472729},
abstract = {This tutorial explores requirements-based reuse recommendation for product line assets in the context of clone-and-own product lines.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {210},
numpages = {1},
keywords = {SPL adoption, similarity, software reuse},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2020390.2020397,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Go\v{s}eva-Popstojanova, Katerina},
title = {Are change metrics good predictors for an evolving software product line?},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020397},
doi = {10.1145/2020390.2020397},
abstract = {Background: Previous research on three years of early data for an Eclipse product identified some predictors of failure-prone files that work well for that data set. Additionally, Eclipse has been used to explore characteristics of product line software in previous research.Aims: To assess whether change metrics are good predictors of failure-prone files over time for the family of products in the evolving Eclipse product line.Method: We repeat, to the extent possible, the decision tree portion of the prior study to assess our ability to replicate the method, and then extend it by including four more recent years of data. We compare the most prominent predictors with the previous study's results. We then look at the data for three additional Eclipse products as they evolved over time. We explore whether the set of good predictors change over time for one product and whether the set differs among products.Results: We find that change metrics are consistently good and incrementally better predictors across the evolving products in Eclipse. There is also some consistency regarding which change metrics are the best predictors.Conclusion: Change metrics are good predictors for failure-prone files for the Eclipse product line. A small subset of these change metrics is fairly stable and consistent across products and releases.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {7},
numpages = {10},
keywords = {change metrics, failure-prone files, post-release defects, prediction, reuse, software product lines},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@inproceedings{10.1109/ISISE.2010.112,
author = {Ren, Yongchang and Quan, Qiang and Xing, Tao and Chen, Xiaoji},
title = {Fuzzy Decision Analysis of the Software Configuration Management Tools Selection},
year = {2010},
isbn = {9780769543604},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISISE.2010.112},
doi = {10.1109/ISISE.2010.112},
abstract = {Configuration management tool play an important role in software project management, it can solve a lot of problems which trouble the software project managers, Configuration management tool selection involve in many factors, it is a multifactor, complex systems engineering problem. In this paper, use the method of fuzzy decision analysis to study. First, analysis the evaluation indicators of the configuration management tool, use the decision hierarchy diagram to analysis the function, performance, cost, service and other indicators, where after, study multi-objective fuzzy decision method, including constructing the factors index matrix, constructing the fuzzy matrix, and making decisions on programs, finally, use the configuration management tool to select the instance, explain the whole process of decision analysis methods. The results show that, fuzzy decision analysis method well solves the fuzzy and non-deterministic problem, in the configuration management tool selection, practical application should be using qualitative analysis and quantitative calculated combining method, flexibility to determine the evaluation index system and its weights value.},
booktitle = {Proceedings of the 2010 Third International Symposium on Information Science and Engineering},
pages = {295–297},
numpages = {3},
keywords = {SCM, Software configuration management, fuzzy decision analysis, tools selection},
series = {ISISE '10}
}

@article{10.1016/j.procs.2018.05.082,
author = {Hitesh and Kumari, A. Charan},
title = {Feature Selection Optimization in SPL using Genetic Algorithm},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.082},
doi = {10.1016/j.procs.2018.05.082},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {1477–1486},
numpages = {10},
keywords = {Software product line, Genetic Algorithm, Feature Model, Software Product Line Engineering}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {configuration, machine learning, multi-objective search, product line, rule mining},
location = {Berlin, Germany},
series = {GECCO '17}
}

@article{10.1016/j.cmpb.2018.06.010,
author = {Akbulut, Akhan and Ertugrul, Egemen and Topcu, Varol},
title = {Fetal health status prediction based on maternal clinical history using machine learning techniques},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {163},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2018.06.010},
doi = {10.1016/j.cmpb.2018.06.010},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
pages = {87–100},
numpages = {14},
keywords = {Machine learning, Medical diagnosis, Risk prediction, Pregnancy, Fetal health, Prognosis, m-Health}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {feature identification, requirement documents, reverse engineering, software product lines, variability extraction},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {SAT, benchmark, configurable systems, sampling, software product lines, variability model},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3473065,
author = {Michelon, Gabriela K. and Sotto-Mayor, Bruno and Martinez, Jabier and Arrieta, Aitor and Abreu, Rui and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Spectrum-based feature localization: a case study using ArgoUML},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473065},
doi = {10.1145/3461001.3473065},
abstract = {Feature localization (FL) is a basic activity in re-engineering legacy systems into software product lines. In this work, we explore the use of the Spectrum-based localization technique for this task. This technique is traditionally used for fault localization but with practical applications in other tasks like the dynamic FL approach that we propose. The ArgoUML SPL benchmark is used as a case study and we compare it with a previous hybrid (static and dynamic) approach from which we reuse the manual and testing execution traces of the features. We conclude that it is feasible and sound to use the Spectrum-based approach providing promising results in the benchmark metrics.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {126–130},
numpages = {5},
keywords = {ArgoUML SPL benchmark, dynamic feature localization, spectrum-based localization},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {energy efficiency, latency, mobile cloud computing, mobile edge computing, optimisation, software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3375627.3375858,
author = {Zucker, Julian and d'Leeuwen, Myraeka},
title = {Arbiter: A Domain-Specific Language for Ethical Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375858},
doi = {10.1145/3375627.3375858},
abstract = {The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {421–425},
numpages = {5},
keywords = {domain-specific languages, ethical machine learning},
location = {New York, NY, USA},
series = {AIES '20}
}

@article{10.1007/s10115-012-0528-3,
author = {Anchuri, Pranay and Zaki, Mohammed J. and Barkol, Omer and Bergman, Ruth and Felder, Yifat and Golan, Shahar and Sityon, Arik},
title = {Graph mining for discovering infrastructure patterns in configuration management databases},
year = {2012},
issue_date = {Dec 2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {3},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-012-0528-3},
doi = {10.1007/s10115-012-0528-3},
abstract = {A configuration management database (CMDB) can be considered to be a large graph representing the IT infrastructure entities and their interrelationships. Mining such graphs is challenging because they are large, complex, and multi-attributed and have many repeated labels. These characteristics pose challenges for graph mining algorithms, due to the increased cost of subgraph isomorphism (for support counting) and graph isomorphism (for eliminating duplicate patterns). The notion of pattern frequency or support is also more challenging in a single graph, since it has to be defined in terms of the number of its (potentially, exponentially many) embeddings. We present CMDB-Miner, a novel two-step method for mining infrastructure patterns from CMDB graphs. It first samples the set of maximal frequent patterns and then clusters them to extract the representative infrastructure patterns. We demonstrate the effectiveness of CMDB-Miner on real-world CMDB graphs, as well as synthetic graphs.},
journal = {Knowl. Inf. Syst.},
month = dec,
pages = {491–522},
numpages = {32},
keywords = {Configuration management databases, Frequent subgraphs, Single graph mining, Sparse graph mining}
}

@inproceedings{10.1007/978-3-030-38085-4_19,
author = {Christodoulopoulos, Konstantinos and Sartzetakis, Ippokratis and Soumplis, Polizois and Varvarigos, Emmanouel (Manos)},
title = {Machine Learning Assisted Quality of Transmission Estimation and Planning with Reduced Margins},
year = {2019},
isbn = {978-3-030-38084-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38085-4_19},
doi = {10.1007/978-3-030-38085-4_19},
abstract = {In optical transport networks, the Quality of Transmission (QoT) using a physical layer model (PLM) is estimated before establishing new or reconfiguring established optical connections. Traditionally, high margins are added to account for the model’s inaccuracy and the uncertainty in the current and evolving physical layer conditions, targeting uninterrupted operation for several years, until the end-of-life (EOL). Reducing the margins increases network efficiency but requires accurate QoT estimation. We present two machine learning (ML) assisted QoT estimators that leverage monitoring data of existing connections to understand the actual physical layer conditions and achieve high estimation accuracy. We then quantify the benefits of planning/upgrading a network over multiple periods with accurate QoT estimation as opposed to planning with EOL margins.},
booktitle = {Optical Network Design and Modeling: 23rd IFIP WG 6.10 International Conference, ONDM 2019, Athens, Greece, May 13–16, 2019, Proceedings},
pages = {211–222},
numpages = {12},
keywords = {Overprovisioning, Static network planning, End-of-life margins, Physical layer impairments, Monitoring, Cross-layer optimization, Incremental multi-period planning, Marginless},
location = {Athens, Greece}
}

@inproceedings{10.1145/2499777.2500717,
author = {Lee, Jaejoon},
title = {Dynamic feature deployment and composition for dynamic software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500717},
doi = {10.1145/2499777.2500717},
abstract = {We aim to tackle problems with feature interoperability in dynamic software product lines: a feature allows collaborations with other features not conceived when it is deployed. In this position paper, we propose a Dynamic Feature Deployment (DFD) idea, which is a model-driven approach to support seamless integration of new features and changes of product configuration at runtime. The approach is based on a feature-modelling technique that directly deals with flexibility of reusable software assets in software product line engineering. We also propose a Hybrid between Passive/Active Behaviours (Hy-PAB) architecture model to support two extreme sets of behaviours for DFD: an active coordinating behaviour that controls the interactions with other features, and a passive subordinating behaviour that allows other features to control their interactions with other features.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {114–116},
numpages = {3},
keywords = {dynamic software product line, feature interoperability, feature modelling, smart home systems, software architecture},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1007/978-3-642-04211-9_23,
author = {Nunes, Ingrid and Lucena, Carlos J. and Cowan, Donald and Alencar, Paulo},
title = {Building Service-Oriented User Agents Using a Software Product Line Approach},
year = {2009},
isbn = {9783642042102},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-04211-9_23},
doi = {10.1007/978-3-642-04211-9_23},
abstract = {This paper presents an approach to develop service-oriented user agents using the Software Product LineSPL engineering paradigm. The approach comprises activities and models to support building service-oriented customized agents that automate user tasks based on service orchestration involving multiple agents in open environments, and takes advantage of the synergy of Service-oriented ArchitectureSOA, Multi-agent SystemMAS and Software Product LineSPL. The domain-based process involves extended domain analysis with goals and variability, domain design with the specification of agent services and plans, and domain implementation.},
booktitle = {Proceedings of the 11th International Conference on Software Reuse: Formal Foundations of Reuse and Domain Engineering},
pages = {236–245},
numpages = {10},
keywords = {Multi-agent Systems, Personalization, Service-oriented Architectures, Software Product Lines, User Agents},
location = {Falls Church, Virginia},
series = {ICSR '09}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {automatic repair, fault, system evolution, variability model},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1504/ijkesdp.2020.112630,
author = {Moses, Beulah and Singhal, Shyam},
title = {Effort estimation in software development using story point: a machine learning approach},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {1},
issn = {1755-3210},
url = {https://doi.org/10.1504/ijkesdp.2020.112630},
doi = {10.1504/ijkesdp.2020.112630},
abstract = {Agile methodologies are besieged with problems and potential solutions around predictive insights on a project. These problems range from estimation, quality, to effort and duration requirements. Despite of having innumerable predictive models not a single reliable solution is available to estimate the duration and effort required to complete an agile project on an ongoing basis. This is due subjective nature of 'story point', and progressive elaboration of 'user story'. This paper analyses the relationship between story points and effort, across a sample of software development projects, in an organisation. A novel machine learning predictive model has been developed and is implemented across agile projects that infer relationship between 'effort' and 'story points', directly in contrast with agile literature. This predictive model was tested and worked accurately, reliably and effectively across various agile projects. This research can be extended to agile projects having sprints of less than fifty story points.},
journal = {Int. J. Knowl. Eng. Soft Data Paradigm.},
month = jan,
pages = {25–44},
numpages = {19},
keywords = {forecasting, estimation model, machine learning, regression, agile, scrum, story points, effort estimation}
}

@article{10.1007/s10586-020-03210-2,
author = {Chiang, Ron C.},
title = {Contention-aware container placement strategy for docker swarm with machine learning based clustering algorithms},
year = {2020},
issue_date = {Feb 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-020-03210-2},
doi = {10.1007/s10586-020-03210-2},
abstract = {Containerization technology utilizes operating system level virtualization to package applications to run with required libraries and are isolated from other processes on the same host. Lightweight and quick deployment make containers popular in many data centers. Running distributed applications in data centers usually involves multiple clusters of machines. Docker Swarm is a container orchestration tool for managing a cluster of Docker containers and their hosts. However, Docker Swarm’s scheduler does not consider resource utilization when placing containers in a cluster. This paper first investigated performance interference in container clusters. Our experimental study showed that distributed applications’ performance can be degraded when co-located with other containers which aggressively consume resources. A new scheduler is proposed to improve performance while keeping high resource utilization. The experimental results demonstrated that the proposed prototype with machine learning based clustering algorithms could effectively improve distributed applications’ performance by up to 14.5% with an average at around 12%. This work also provides theoretical bounds for the container placement problem.},
journal = {Cluster Computing},
month = nov,
pages = {13–23},
numpages = {11},
keywords = {Cloud computing, Container, Virtualization, Serverless Computing, Microservices}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {generating feature models, inductive generalization from examples, machine learning},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461002.3473074,
author = {Fantechi, Alessandro and Gnesi, Stefania and Livi, Samuele and Semini, Laura},
title = {A spaCy-based tool for extracting variability from NL requirements},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473074},
doi = {10.1145/3461002.3473074},
abstract = {In previous work, we have shown that ambiguity detection in requirements can also be used as a way to capture latent aspects of variability. Natural Language Processing (NLP) tools have been used for a lexical analysis aimed at ambiguity indicators detection, and we have studied the necessary adaptations to those tools for pointing at potential variability, essentially by adding specific dictionaries for variability. We have identified also some syntactic rules able to detect potential variability, such as disjunction between nouns or pairs of indicators in a subordinate proposition. This paper describes a new prototype NLP tool, based on the spaCy library, specifically designed to detect variability. The prototype is shown to preserve the same recall exhibited by previously used lexical tools, with a higher precision.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {32–35},
numpages = {4},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannem\"{u}ller, Martin and Matth\'{e}, Michael and Sch\"{u}rr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {dynamic software product lines, machine learning, performance-influence models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {product lines, sampling, sampling evalutaion},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.5555/1894483.1894487,
author = {Calmet, Jacques and Campbell, John A.},
title = {A revisited perspective on symbolic mathematical computing and artificial intelligence},
year = {2010},
isbn = {3642141277},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We provide a perspective on the current state and possible future of links between symbolic mathematical computing and artificial intelligence, on the occasion of the 10th biennial conference (AISMC, later AISC) devoted to those connections. It follows a similar perspective expressed for the first such conference in 1992 and then revised and expanded 5 years later. Issues related to the computational management of mathematical knowledge are highlighted.},
booktitle = {Proceedings of the 10th ASIC and 9th MKM International Conference, and 17th Calculemus Conference on Intelligent Computer Mathematics},
pages = {14–18},
numpages = {5},
keywords = {artificial intelligence, symbolic computation},
location = {Paris, France},
series = {AISC'10/MKM'10/Calculemus'10}
}

@inproceedings{10.1145/1080173.1080191,
author = {Jeon, Sung-eok and Ji, Chuanyi},
title = {Role of machine learning in configuration management of ad hoc wireless networks},
year = {2005},
isbn = {1595930264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1080173.1080191},
doi = {10.1145/1080173.1080191},
abstract = {In this work, we show that machine learning, e.g., graphical models, plays an important role for the self-configuration of ad hoc wireless network. The role of such a learning approach includes a simple representation of complex dependencies in the network and a distributed algorithm which can adaptively find a nearly optimal configuration.},
booktitle = {Proceedings of the 2005 ACM SIGCOMM Workshop on Mining Network Data},
pages = {223–224},
numpages = {2},
location = {Philadelphia, Pennsylvania, USA},
series = {MineNet '05}
}

@inproceedings{10.1145/3307630.3342419,
author = {Ghofrani, Javad and Kozegar, Ehsan and Bozorgmehr, Arezoo and Soorati, Mohammad Divband},
title = {Reusability in Artificial Neural Networks: An Empirical Study},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342419},
doi = {10.1145/3307630.3342419},
abstract = {Machine learning, especially deep learning has aroused interests of researchers and practitioners for the last few years in development of intelligent systems such as speech, natural language, and image processing. Software solutions based on machine learning techniques attract more attention as alternatives to conventional software systems. In this paper, we investigate how reusability techniques are applied in implementation of artificial neural networks (ANNs). We conducted an empirical study with an online survey among experts with experience in developing solutions with ANNs. We analyze the feedback of more than 100 experts to our survey. The results show existing challenges and some of the applied solutions in an intersection between reusability and ANNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {122–129},
numpages = {8},
keywords = {artificial neural networks, empirical study, reusability, survey, systematic reuse},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461002.3473948,
author = {Xu, Hao and Baarir, Souheib and Ziadi, Tewfik and Hillah, Lom Messan and Essodaigui, Siham and Bossu, Yves},
title = {Optimisation for the product configuration system of Renault: towards an integration of symmetries},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473948},
doi = {10.1145/3461002.3473948},
abstract = {The problem of configuring model variability is widespread in many different domains. Renault, a leading french automobile manufacturer, has developed its technology internally to model vehicle diversity. This technology relies on the approach known as knowledge compilation. Since its inception, continuous progress has been made in the tool while monitoring the latest developments from the software field and academia. However, the growing number of vehicle models brings potential risks and higher requirements for the tool. This paper presents a short reminder of Renault's technology principles and the improvements we intend to achieve by analyzing and leveraging notable data features of Renault problem instances. In particular, the aim is to exploit symmetry properties.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {86–90},
numpages = {5},
keywords = {SAT, knowledge compilation, product line, symmetries},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471144,
author = {Uta, Mathias and Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Tran, Thi Ngoc Trang and Helic, Denis},
title = {Evaluating recommender systems in feature model configuration},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471144},
doi = {10.1145/3461001.3471144},
abstract = {Configurators can be evaluated in various ways such as efficiency and completeness of solution search, optimality of the proposed solutions, usability of configurator user interfaces, and configuration consistency. Due to the increasing size and complexity of feature models, the integration of recommendation algorithms with feature model configurators becomes relevant. In this paper, we show how the output of a recommender system can be evaluated within the scope of feature model configuration scenarios. Overall, we argue that the discussed ways of measuring recommendation quality help developers to gain a broader view on evaluation techniques in constraint-based recommendation domains.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {58–63},
numpages = {6},
keywords = {configuration, evaluation, feature models, recommender systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {AutoML, NAS, configuration search, feature model, neural architecture search},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/978-3-030-55789-8_59,
author = {Abeyrathna, Kuruge Darshana and Granmo, Ole-Christoffer and Goodwin, Morten},
title = {Integer Weighted Regression Tsetlin Machines},
year = {2020},
isbn = {978-3-030-55788-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-55789-8_59},
doi = {10.1007/978-3-030-55789-8_59},
abstract = {The Regression Tsetlin Machine (RTM) addresses the lack of interpretability impeding state-of-the-art nonlinear regression models. It does this by using conjunctive clauses in propositional logic to capture the underlying non-linear frequent patterns in the data. These, in turn, are combined into a continuous output through summation, akin to a linear regression function, however, with non-linear components and binary weights. However, the resolution of the RTM output is proportional to the number of clauses employed. This means that computation cost increases with resolution. To address this problem, we here introduce integer weighted RTM clauses. Our integer weighted clause is a compact representation of multiple clauses that capture the same sub-pattern—w repeating clauses are turned into one, with an integer weight w. This reduces computation cost w times, and increases interpretability through a sparser representation. We introduce a novel learning scheme, based on so-called stochastic searching on the line. We evaluate the potential of the integer weighted RTM empirically using two artificial datasets. The results show that the integer weighted RTM is able to acquire on par or better accuracy using significantly less computational resources compared to regular RTM and an RTM with real-valued weights.},
booktitle = {Trends in Artificial Intelligence Theory and Applications. Artificial Intelligence Practices: 33rd International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2020, Kitakyushu, Japan, September 22-25, 2020, Proceedings},
pages = {686–694},
numpages = {9},
keywords = {Tsetlin machines, Regression tsetlin machines, Weighted tsetlin machines, Interpretable machine learning, Stochastic searching on the line},
location = {Kitakyushu, Japan}
}

@inproceedings{10.1145/2491627.2491629,
author = {Clements, Paul and Krueger, Charles and Shepherd, James and Winkler, Andrew},
title = {A PLE-based auditing method for protecting restricted content in derived products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491629},
doi = {10.1145/2491627.2491629},
abstract = {Many organizations that produce a portfolio of products for different customers need to ensure that sensitive or restricted content that may appear in some products must not appear in others. Examples of this need include complying with statutes in different countries of sale, protection of intellectual property developed specifically for one customer, and more. For organizations operating under these requirements and producing their products under a product line engineering paradigm that relies on automation in product derivation, there is a need for a method to ensure that the content restrictions have been met in the derived products. This paper describes an auditing method that meets this need. It was created for use in the Second Generation Product Line Engineering approach that is being applied by Lockheed Martin in their AEGIS ship combat system product line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {218–226},
numpages = {9},
keywords = {bill-of-features, feature modeling, feature profiles, hierarchical product lines, product audit, product baselines, product configurator, product derivation, product line engineering, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and J\'{e}z\'{e}quel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Software product line, Configurable system, Software variability, Software testing, Machine learning, Quality assurance}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: learning featured transition systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour of a whole family of software-based systems. Reasoning at the family level yields important economies of scale and quality improvements for a broad range of systems such as software product lines, adaptive and configurable systems. Yet, to fully benefit from the above advantages, a model of the system family's behaviour is necessary. Such a model is often prohibitively expensive to create manually due to the number of variants. For large long-lived systems with outdated specifications or for systems that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes to automate the learning of such models from existing artefacts. To advance research at a fundamental level, our learning target are Featured Transition Systems (FTS), an abstract formalism that can be used to provide a pivot semantics to a range of variability-aware state-based modelling languages. The main research questions addressed by this PhD project are: (1) Can we learn variability-aware models efficiently? (2) Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e., with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {active automata learning, featured transition systems, model learning, software product lines, variability mining},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471142,
author = {Gu\'{e}gain, \'{E}douard and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {On reducing the energy consumption of software product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471142},
doi = {10.1145/3461001.3471142},
abstract = {Along the last decade, several studies considered green software design as a key development concern to improve the energy efficiency of software. Yet, few techniques address this concern for Software Product Lines (SPL). In this paper, we therefore introduce two approaches to measure and reduce the energy consumption of a SPL by analyzing a limited set of products sampled from this SPL. While the first approach relies on the analysis of individual feature consumptions, the second one takes feature interactions into account to better mitigate energy consumption of resulting products.Our experimental results on a real-world SPL indicate that both approaches succeed to produce significant energy improvements on a large number of products, while consumption data was modeled from a small set of sampled products. Furthermore, we show that taking feature interactions into account leads to more products improved with higher energy savings per product.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {89–99},
numpages = {11},
keywords = {consumption, energy, measurement, mitigation, software product lines},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.5555/1885639.1885642,
author = {Bagheri, Ebrahim and Di Noia, Tommaso and Ragone, Azzurra and Gasevic, Dragan},
title = {Configuring software product line feature models based on Stakeholders' soft and hard requirements},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature modeling is a technique for capturing commonality and variability. Feature models symbolize a representation of the possible application configuration space, and can be customized based on specific domain requirements and stakeholder goals. Most feature model configuration processes neglect the need to have a holistic approach towards the integration and satisfaction of the stakeholder's soft and hard constraints, and the application-domain integrity constraints. In this paper, we will show how the structure and constraints of a feature model can be modeled uniformly through Propositional Logic extended with concrete domains, called P(N). Furthermore, we formalize the representation of soft constraints in fuzzy P(N) and explain how semi-automated feature model configuration is performed. The model configuration derivation process that we propose respects the soundness and completeness properties.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {16–31},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/3233027.3233033,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse engineering variability from requirement documents based on probabilistic relevance and word embedding},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233033},
doi = {10.1145/3233027.3233033},
abstract = {Feature and variability extraction from different artifacts is an indispensable activity to support systematic integration of single software systems and Software Product Line (SPL). Beyond manually extracting variability, a variety of approaches, such as feature location in source code and feature extraction in requirements, has been proposed to provide an automatic identification of features and their variation points. Compared with source code, requirements contain more complete variability information and provide traceability links to other artifacts from early development phases. In this paper, we propose a method to automatically extract features and relationships based on a probabilistic relevance and word embedding. In particular, our technique consists of three steps: First, we apply word2vec to obtain a prediction model, which we use to determine the word level similarity of requirements. Second, based on word level similarity and the significance of a word in a domain, we compute the requirements level similarity using probabilistic relevance. Third, we adopt hierarchical clustering to group features and we define four criteria to detect variation points between identified features. We perform a case study to evaluate the usability and robustness of our method and to compare it with the results of other related approaches. Initial results reveal that our approach identifies the majority of features correctly and also extracts variability information with reasonable accuracy.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {121–131},
numpages = {11},
keywords = {feature identification, requirement documents, reverse engineering, software product lines, variability extraction},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3106195.3106215,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-healing in Service Mashups Through Feature Adaptation},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106215},
doi = {10.1145/3106195.3106215},
abstract = {The composition of the functionality of multiple services into a single unique service mashup has received wide interest in the recent years. Given the distributed nature of these mashups where the constituent services can be located on different servers, it is possible that a change in the functionality or availability of a constituent service result in the failure of the service mashup. In this paper, we propose a novel method based on the Software Product Line Engineering (SPLE) paradigm which is able to find an alternate valid service mashup which has maximum possible number of original service mashup features in order to mitigate a service failure when complete recovery is not possible. This method also has an advantage that it can recover or mitigate the failure automatically without requiring the user to specify any adaptation rule or strategy. We show the practicality of our proposed approach through extensive experiments.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {94–103},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/1147249.1147254,
author = {Fischbein, Dario and Uchitel, Sebastian and Braberman, Victor},
title = {A foundation for behavioural conformance in software product line architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147254},
doi = {10.1145/1147249.1147254},
abstract = {Software product lines or families represent an emerging paradigm that is enabling companies to engineer applications with similar functionality and user requirements more effectively. Behaviour modelling at the architecture level has the potential for supporting behaviour analysis of entire product lines, as well as defining optional and variable behaviour for different products of a family. However, to do so rigorously, a well defined notion of behavioural conformance of a product to its product line must exist. In this paper we provide a discussion on the shortcomings of traditional behaviour modelling formalisms such as Labelled Transition Systems for characterising conformance and propose Modal Transition Systems as an alternative. We discuss existing semantics for such models, exposing their limitations and finally propose a novel semantics for Modal Transition Systems, branching semantics, that can provide the formal underpinning for a notion of behaviour conformance for software product line architectures.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {39–48},
numpages = {10},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1145/3233027.3233054,
author = {Kaindl, Hermann and Mannion, Mike},
title = {Software reuse and mass customisation: feature modelling vs. case-based reasoning},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233054},
doi = {10.1145/3233027.3233054},
abstract = {Several socio-economic trends are driving customer demands towards individualisation. Many suppliers are responding by offering supplier-led software product design customization choices ("mass customization"). Some are also offering customer-led software product design choices ("mass personalization"). This tutorial introduces these concepts and explores the implications for software product line development. One particular technical challenge is being able to respond to and manage at scale the increasing variety of common, supplier-led and customer-led features. We discuss two different approaches to address this challenge. One is grounded in feature modelling; the other in case-based reasoning. Both approaches aim to support the identification and selection of similar products. However they each place different emphases on these activities, use different product descriptions, and deploy different product derivation methods. Accordingly, each approach has different key properties, benefits and limitations.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {304},
numpages = {1},
keywords = {case-based reasoning, feature model, mass customisation, reuse},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1007/978-3-319-38921-9_12,
author = {Velez, Gorka and Quartulli, Marco and Martin, Angel and Otaegui, Oihana and Assem, Haytham},
title = {Machine Learning for Autonomic Network Management in a Connected Cars Scenario},
year = {2016},
isbn = {9783319389202},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-38921-9_12},
doi = {10.1007/978-3-319-38921-9_12},
abstract = {Current 4G networks are approaching the limits of what is possible with this generation of radio technology. Future 5G networks will be highly based on software, with the ultimate goal of being self-managed. Machine Learning is a key technology to reach the vision of a 5G self-managing network. This new paradigm will significantly impact on connected vehicles, fostering a new wave of possibilities. This paper presents a preliminary approach towards Autonomic Network Management on a connected cars scenario. The focus is on the machine learning part, which will allow forecasting resource demand requirements, detecting errors, attacks and outlier events, and responding and taking corrective actions.},
booktitle = {Proceedings of the 10th International Workshop on Communication Technologies for Vehicles - Volume 9669},
pages = {111–120},
numpages = {10},
keywords = {5G, Connected cars, Machine learning, Network management}
}

@article{10.5555/1120120.1644623,
author = {Wallace, Richard J. and Freuder, Eugene C.},
title = {Constraint-based reasoning and privacy/efficiency tradeoffs in multi-agent problem solving},
year = {2005},
issue_date = {January 2005},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {161},
number = {1–2},
issn = {0004-3702},
abstract = {Because of privacy concerns, agents may not want to reveal information that could be of use in problem solving. As a result, there are potentially important tradeoffs between maintaining privacy and enhancing search efficiency in these situations. In this work we show how quantitative assessments of privacy loss can be made within the framework of distributed constraint satisfaction. We also show how agents can make inferences about other agents' problems or subproblems from communications that carry no explicit private information. This can be done using constraint-based reasoning in a framework consisting of an ordinary CSP, which is only partly known, and a system of ''shadow CSPs'' that represent various forms of possibilistic knowledge. This kind of reasoning in combination with arc consistency processing can speed up search under conditions of limited communication, at the same time potentially undermining privacy. These effects are demonstrated in a simplified meeting scheduling problem where agents propose meetings consistent with their existing schedules while responding to other proposals by accepting or rejecting them. In this situation, we demonstrate some of the conditions under which privacy/efficiency tradeoffs emerge, as well as complications that arise when agents can reason effectively under conditions of partial ignorance.},
journal = {Artif. Intell.},
month = jan,
pages = {209–227},
numpages = {19},
keywords = {Distributed constraint satisfaction, Multiagent systems, Privacy}
}

@book{10.5555/199261,
editor = {Freuder, Eugene C. and Mackworth, Alan K.},
title = {Constraint-based reasoning},
year = {1994},
isbn = {0262560755},
publisher = {MIT Press},
address = {Cambridge, MA, USA}
}

@inproceedings{10.1145/2491627.2491646,
author = {Marijan, Dusica and Gotlieb, Arnaud and Sen, Sagar and Hervieu, Aymeric},
title = {Practical pairwise testing for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491646},
doi = {10.1145/2491627.2491646},
abstract = {One key challenge for software product lines is efficiently managing variability throughout their lifecycle. In this paper, we address the problem of variability in software product lines testing. We (1) identify a set of issues that must be addressed to make software product line testing work in practice and (2) provide a framework that combines a set of techniques to solve these issues. The framework integrates feature modelling, combinatorial interaction testing and constraint programming techniques. First, we extract variability in a software product line as a feature model with specified feature interdependencies. We then employ an algorithm that generates a minimal set of valid test cases covering all 2-way feature interactions for a given time interval. Furthermore, we evaluate the framework on an industrial SPL and show that using the framework saves time and provides better test coverage. In particular, our experiments show that the framework improves industrial testing practice in terms of (i) 17% smaller set of test cases that are (a) valid and (b) guarantee all 2-way feature coverage (as opposite to 19.2% 2-way feature coverage in the hand made test set), and (ii) full flexibility and adjustment of test generation to available testing time.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {227–235},
numpages = {9},
keywords = {feature modelling, software product lines, variability management},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {configurable systems, feature models, monte carlo tree search, software product lines, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342384,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {MetricHaven: More than 23,000 Metrics for Measuring Quality Attributes of Software Product Lines},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342384},
doi = {10.1145/3307630.3342384},
abstract = {Variability-aware metrics are designed to measure qualitative aspects of software product lines. As we identified in a prior SLR [6], there exist already many metrics that address code or variability separately, while the combination of both has been less researched. MetricHaven fills this gap, as it extensively supports combining information from code files and variability models. Further, we also enable the combination of well established single system metrics with novel variability-aware metrics, going beyond existing variability-aware metrics. Our tool supports most prominent single system and variability-aware code metrics. We provide configuration support for already implemented metrics, resulting in 23,342 metric variations. Further, we present an abstract syntax tree developed for MetricHaven, that allows the realization of additional code metrics.Tool: https://github.com/KernelHaven/MetricHavenVideo: https://youtu.be/vPEmD5Sr6gM},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {25–28},
numpages = {4},
keywords = {SPL, feature models, implementation, metrics, software product lines, variability models},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2791060.2791103,
author = {Mazo, Ra\'{u}l and Mu\~{n}oz-Fern\'{a}ndez, Juan C. and Rinc\'{o}n, Luisa and Salinesi, Camille and Tamura, Gabriel},
title = {VariaMos: an extensible tool for engineering (dynamic) product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791103},
doi = {10.1145/2791060.2791103},
abstract = {This paper presents the new release of VariaMos, a Java-based tool for defining variability modeling languages, modeling (dynamic) product lines and cyber-physical self-adaptive systems, and supporting automated verification, analysis, configuration and simulation of these models. In particular, we describe the characteristics of this new version regarding its first release: (1) the capability to create languages for modeling systems with variability, even with different views; (2) the capability to use the created language to model (dynamic) product lines; (3) the capability to analyze and configure these models according to the changing context and requirements; and (4) the capability to execute them over several simulation scenarios. Finally, we show how to use VariaMos with an example, and we compare it with other tools found in the literature.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {374–379},
numpages = {6},
keywords = {constraints, dynamic product line models, product line engineering, simulation, tool, variability},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3236404,
author = {Gazzillo, Paul and Koc, Ugur and Nguyen, ThanhVu and Wei, Shiyi},
title = {Localizing configurations in highly-configurable systems},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236404},
doi = {10.1145/3233027.3236404},
abstract = {The complexity of configurable systems has grown immensely, and it is only getting more complex. Such systems are a challenge for software testing and maintenance, because bugs and other defects can and do appear in any configuration. One common requirement for many development tasks is to identify the configurations that lead to a given defect or some other program behavior. We distill this requirement down to a challenge question: given a program location in a source file, what are valid configurations that include the location? The key obstacle is scalability. When there are thousands of configuration options, enumerating all combinations is exponential and infeasible. We provide a set of target programs of increasing difficulty and variations on the challenge question so that submitters of all experience levels can try out solutions. Our hope is to engage the community and stimulate new and interesting approaches to the problem of analyzing configurations.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {269–273},
numpages = {5},
keywords = {configurations, program analysis, testing, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/S0305-0548(99)00051-9,
author = {Deris, Safaai and Omatu, Sigeru and Ohta, Hiroshi},
title = {Timetable planning using the constraint-based reasoning},
year = {2000},
issue_date = {Aug.2000},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {27},
number = {9},
issn = {0305-0548},
url = {https://doi.org/10.1016/S0305-0548(99)00051-9},
doi = {10.1016/S0305-0548(99)00051-9},
journal = {Comput. Oper. Res.},
month = aug,
pages = {819–840},
numpages = {22},
keywords = {constraint satisfaction problem, constraint-based reasoning, timetabling}
}

@inproceedings{10.1007/978-3-030-87589-3_48,
author = {Machado Reyes, Diego and Chao, Hanqing and Homayounieh, Fatemeh and Hahn, Juergen and Kalra, Mannudeep K. and Yan, Pingkun},
title = {Cardiovascular Disease Risk Improves COVID-19 Patient Outcome Prediction},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_48},
doi = {10.1007/978-3-030-87589-3_48},
abstract = {The pandemic of coronavirus disease 2019 (COVID-19) has severely impacted the world. Several studies suggest an increased risk for COVID-19 patients with underlying cardiovascular diseases (CVD). However, it is challenging to quantify such risk factors and integrate them into patient condition evaluation. This paper presents machine learning methods to assess CVD risk scores from chest computed tomography together with laboratory data, demographics, and deep learning extracted lung imaging features to increase the outcome prediction accuracy for COVID-19 patients. The experimental results demonstrate an overall increase in prediction performance when the CVD severity score was added to the feature set. The machine learning methods obtained their best performance when all categories of the features were used for the patient outcome prediction. With the best attained area under the curve of 0.888, the presented research may assist physicians in clinical decision-making process on managing COVID-19 patients.},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {467–476},
numpages = {10},
keywords = {COVID-19, Machine learning, Cardiovascular disease, Chest CT, Severity score},
location = {Strasbourg, France}
}

@article{10.5555/3455716.3455773,
author = {Ma, Fan and Meng, Deyu and Dong, Xuanyi and Yang, Yi},
title = {Self-paced multi-view co-training},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a "draw without replacement" strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {57},
numpages = {38},
keywords = {co-training, self-paced learning, multi-view learning, semi-supervised learning, ε-expansion theory, probably approximately correct learnable}
}

@article{10.1016/j.future.2018.09.053,
author = {Cecchinel, Cyril and Fouquet, Fran\c{c}ois and Mosser, S\'{e}bastien and Collet, Philippe},
title = {Leveraging live machine learning and deep sleep to support a self-adaptive efficient configuration of battery powered sensors},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.053},
doi = {10.1016/j.future.2018.09.053},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {225–240},
numpages = {16}
}

@article{10.1023/A:1018931617448,
author = {Dershowitz, Nachum},
title = {Artificial intelligence: Retrospective/prospective},
year = {2000},
issue_date = {2000},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1–4},
issn = {1012-2443},
url = {https://doi.org/10.1023/A:1018931617448},
doi = {10.1023/A:1018931617448},
abstract = {Symbolic programming and formal reasoning are two significant areas to which research in artificial intelligence has contributed much. The emergence of global repositories of accessible data presents new challenges for knowledge representation and logical inference.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = jan,
pages = {3–5},
numpages = {3}
}

@article{10.1504/IJSN.2012.048493,
author = {Alsubhi, K. and Alhazmi, Y. and Bouabdallah, N. and Boutaba, R.},
title = {Security configuration management in intrusion detection and prevention systems},
year = {2012},
issue_date = {August 2012},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {1},
issn = {1747-8405},
url = {https://doi.org/10.1504/IJSN.2012.048493},
doi = {10.1504/IJSN.2012.048493},
abstract = {This paper aims to study the impact of security enforcement levels on the performance and usability of an enterprise information system. We develop a new analytical model to investigate the relationship between the Intrusion Detection and Prevention System performance and the rules mode selection. In particular, we analyze the IDPS rule-checking process along with its consequent action on the resulting security of the network and on the average service time per event. Simulation was conducted to validate our performance analysis study. The results demonstrate that it is desirable to strike a balance between system security and network performance.},
journal = {Int. J. Secur. Netw.},
month = aug,
pages = {30–39},
numpages = {10}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Exploit, Feature Model, Variability Model, Vulnerability, Vulnerability Analysis and Management},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1109/WAIN52551.2021.00008,
author = {Catovic, Armin and Cartwright, Carolyn and Gebreyesus, Yasmin Tesfaldet and Ferlin, Simone},
title = {Linnaeus: A highly reusable and adaptable ML based log classification pipeline},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00008},
doi = {10.1109/WAIN52551.2021.00008},
abstract = {Logs are a common way to record detailed run-time information in software. As modern software systems evolve in scale and complexity, logs have become indispensable to understanding the internal states of the system. At the same time however, manually inspecting logs has become impractical. In recent times, there has been more emphasis on statistical and machine learning (ML) based methods for analyzing logs. While the results have shown promise, most of the literature focuses on algorithms and state-of-the-art (SOTA), while largely ignoring the practical aspects. In this paper we demonstrate our end-to-end log classification pipeline, Linnaeus. Besides showing the more traditional ML flow, we also demonstrate our solutions for adaptability and re-use, integration towards large scale software development processes, and how we cope with lack of labelled data. We hope Linnaeus can serve as a blueprint for, and inspire the integration of, various ML based solutions in other large scale industrial settings.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {11–18},
numpages = {8},
location = {Madrid, Spain}
}

@inproceedings{10.1145/1321631.1321647,
author = {Sarma, Anita and Bortis, Gerald and van der Hoek, Andre},
title = {Towards supporting awareness of indirect conflicts across software configuration management workspaces},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321647},
doi = {10.1145/1321631.1321647},
abstract = {Workspace awareness techniques have been proposed to enhance the effectiveness of software configuration management systems in coordinating parallel work. These techniques share information regarding ongoing changes, so potential conflicts can be detected during development, instead of when changes are completed and committed to a repository. To date, however, workspace awareness techniques only address direct conflicts, which arise due to concurrent changes to the same artifact, but are unable to support indirect conflicts, which arise due to ongoing changes in one artifact affecting concurrent changes in an-other artifact. In this paper, we present a new, cross-workspace awareness technique that supports one particular kind of indirect conflict, namely those indirect conflicts caused by changes to class signatures. We introduce our approach, discuss its implementation in our workspace awareness tool Palant\'{\i}r, illustrate its potential through two pilot studies, and lay out how to generalize the technique to a broader set of indirect conflicts},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {94–103},
numpages = {10},
keywords = {awareness, configuration managment, direct conflicts, indirect conflicts, software configuration management},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@inproceedings{10.1145/3362789.3362923,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco J. and Ther\'{o}n, Roberto},
title = {Automatic generation of software interfaces for supporting decision-making processes. An application of domain engineering and machine learning},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362923},
doi = {10.1145/3362789.3362923},
abstract = {Information dashboards are sophisticated tools. Although they enable users to reach useful insights and support their decision-making challenges, a good design process is essential to obtain powerful tools. Users need to be part of these design processes, as they will be the consumers of the information displayed. But users are very diverse and can have different goals, beliefs, preferences, etc., and creating a new dashboard for each potential user is not viable. There exist several tools that allow users to configure their displays without requiring programming skills. However, users might not exactly know what they want to visualize or explore, also becoming the configuration process a tedious task. This research project aims to explore the automatic generation of user interfaces for supporting these decision-making processes. To tackle these challenges, a domain engineering, and machine learning approach is taken. The main goal is to automatize the design process of dashboards by learning from the context, including the end-users and the target data to be displayed.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1007–1011},
numpages = {5},
keywords = {Automatic generation, Domain engineering, High-level requirements, Information Dashboards, Meta-modeling},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@article{10.1145/1143489.1143493,
author = {Burgess, Mark},
title = {A control theory perspective on configuration management and Cfengine},
year = {2006},
issue_date = {April 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/1143489.1143493},
doi = {10.1145/1143489.1143493},
abstract = {Cfengine is an autonomous agent for the configuration of Unix-like operating systems. It works by implementing a hybrid feedback loop, with both disrcete and continuous elements.},
journal = {SIGBED Rev.},
month = apr,
pages = {12–16},
numpages = {5},
keywords = {configuration management, control theory}
}

@phdthesis{10.5555/1354508,
author = {Dehlinger, Joshua Jon},
advisor = {Lutz, Robyn R.},
title = {Incorporating product-line engineering techniques into agent-oriented software engineering for efficiently building safety-critical, multi-agent systems},
year = {2007},
isbn = {9780549154877},
publisher = {Iowa State University},
address = {USA},
abstract = {Safety-critical, agent-based systems are being developed without mechanisms and analysis techniques to discover, analyze and verify software requirements and prevent potential hazards. Agent-oriented, software-based approaches have provided powerful and natural high-level abstractions in which software developers can understand, model and develop complex, distributed systems. Yet, the realization of agent-oriented software development partially depends upon whether agent-based software systems can achieve reductions in development time and cost similar to other reuse-conscious software development methods. Further, agent-oriented software engineering (AOSE) currently does not adequately address: (1)&nbsp;requirements (specification) reuse in a way that is amenable to the reduction of the development cost by utilizing reusable assets, and (2)&nbsp;analysis techniques to evaluate safety. This dissertation offers our AOSE methodology, Gaia-PL (Gaia-Product Line) for open, agent-based distributed software systems to capture requirements specifications that can be easily reused. Our methodology uses a product-line perspective to promote reuse in agent-based, software systems early in the development lifecycle so that software assets can be reused throughout the development lifecycle and system evolution. The main contribution of this work is a requirements specification pattern that captures the dynamically changing design configurations of agents. Reuse is achieved by adopting a product-line approach into AOSE. Requirements specifications reuse is the ability to easily use previously defined requirements specifications from an earlier system and apply them to a new, slightly different system. This can significantly reduce the development time and cost of building an agent-based system.For safety-critical agent-based systems, this dissertation incorporates reuse-oriented safety analysis methods for AOSE to allow the discovery of new safety requirements and the verification that the design satisfies the safety requirements. Specifically, Product-Line Software Fault Tree Analysis (PL-SFTA) and its automated tool, PLFaultCAT (  P roduct-  L ine  Fault  Tree  C reation and  A nalysis  T ool), have been created to provide the technique and tool support for the safety analysis of safety-critical software product lines. The PL-SFTA allows for the identification of new safety requirements and the analysis of safety-critical requirements and requirement interactions. An AOSE-adapted Software Failure Modes, Effects and Criticality Analysis (SFMECA) technique has been created to support the derivation of a safety analysis asset using the specifications of Gaia-PL allowing for the identification of possible hazard scenarios and the failure points of specific agent roles. Using the assets generated via PL-SFTA and SFMECA, Bi-Directional Safety Analysis (BDSA) is shown to aid in the completeness of PL-SFTA and SFMECA, help verify the safety properties and strengthen the safety case when safety compliance to safety standards of the multi-agent system is necessary.Results from an application to a large, safety-critical, multi-agent system product-line show that Gaia-PL provides strong reuse capabilities. Evaluation of the Gaia-PL methodology used in conjunction with the PL-SFTA, SFMECA and BDSA safety analysis techniques shows that safety analysis of an agent-based software system is feasible, reusable and efficient.},
note = {AAI3274890}
}

@inproceedings{10.1145/3106195.3106201,
author = {Kim, Jongwook and Batory, Don and Dig, Danny},
title = {Refactoring Java Software Product Lines},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106201},
doi = {10.1145/3106195.3106201},
abstract = {Refactoring is a staple of Object-Oriented (OO) program development. It should be a staple of OO Software Product Line (SPL) development too. X15 is the first tool to support the refactoring of Java SPL codebases. X15 (1) uses Java custom annotations to encode variability in feature-based Java SPLs, (2) projects a view of an SPL product (a program that corresponds to a legal SPL configuration), and (3) allows programmers to edit and refactor the product, propagating changes back to the SPL codebase. Case studies apply 2316 refactorings in 8 public Java SPLs and show that X15 is as efficient, expressive, and scalable as a state-of-the-art feature-unaware Java refactoring engine.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {59–68},
numpages = {10},
keywords = {refactoring, software product lines},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3336294.3336307,
author = {Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Simao, Adenilso},
title = {Learning from Difference: An Automated Approach for Learning Family Models from Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336307},
doi = {10.1145/3336294.3336307},
abstract = {Substantial effort has been spent on extending specification notations and their associated reasoning techniques to software product lines (SPLs). Family-based analysis techniques operate on a single artifact, referred to as a family model, that is annotated with variability constraints. This modeling approach paves the way for efficient model-based testing and model checking for SPLs. Albeit reasonably efficient, the creation and maintenance of family models tend to be time consuming and error-prone, especially if there are crosscutting features. To tackle this issue, we introduce FFSMDiff, a fully automated technique to learn featured finite state machines (FFSM), a family-based formalism that unifies Mealy Machines from SPLs into a single representation. Our technique incorporates variability to compare and merge Mealy machines and annotate states and transitions with feature constraints. We evaluate our technique using 34 products derived from three different SPLs. Our results support the hypothesis that families of Mealy machines can be effectively merged into succinct FFSMs with fewer states, especially if there is high feature sharing among products. These indicate that FFSMDiff is an efficient family-based model learning technique.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {52–63},
numpages = {12},
keywords = {150% model, family model, model learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1109/COMPSAC.2008.106,
author = {Ploskas, Nikiforos and Berger, Michael and Zhang, Jiang and Wintterle, Gert-Joachim},
title = {A Knowledge Management Framework for Software Configuration Management},
year = {2008},
isbn = {9780769532622},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2008.106},
doi = {10.1109/COMPSAC.2008.106},
abstract = {This paper describes a Knowledge Management Framework for Software Configuration Management, which will enable efficient engineering, deployment, and run-time management of reconfigurable ambient intelligent services. Software Configuration Management (SCM) procedures are commonly initiated by device agents located in the users gateways. The Knowledge Management Framework makes use of Ontologies to represent knowledge required to perform SCM and to perform knowledge inference based on Description Logic reasoning. The work has been carried out within the European project COMANCHE that will utilize ontology models to support SCM. The COMANCHE ontology has been developed to provide a standard data model for the information that relates to SCM, and determine (infer) which SW Services need to be installed on the devices of users.},
booktitle = {Proceedings of the 2008 32nd Annual IEEE International Computer Software and Applications Conference},
pages = {593–598},
numpages = {6},
series = {COMPSAC '08}
}

@inproceedings{10.1145/2791060.2791110,
author = {McVoy, Larry},
title = {Preliminary product line support in BitKeeper},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791110},
doi = {10.1145/2791060.2791110},
abstract = {One of the challenges of implementing a product line process is finding the appropriate tools for automation. One of our larger customers was implementing a product line process by-hand in a labor intensive and fragile way. We collaborated with them to evolve our distributed version control system, BitKeeper, into a tool that could handle their performance and product line requirements. The resulting product line generated several complex CPUs (around a billion transistors each).In this paper, we describe their by-hand process for producing different variations of a computer processor; we'll provide some background on the distributed version control system they were using; we'll describe the architectural changes implemented in BitKeeper for supporting product line work flows; we'll describe some of the changes we did to increase performance and provide some benchmark results comparing BitKeeper to Git, and we'll describe the work flow resulting from using the new architecture to replace their by-hand process.In the final section we'll discuss the current limitations of the existing tool, and describe how we plan on evolving it to overcome those limitations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {245–252},
numpages = {8},
keywords = {code reuse, configuration management, software product lines, version control},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1007/978-3-030-89370-5_18,
author = {Tian, Yuze and Zhong, Xian and Liu, Wenxuan and Jia, Xuemei and Zhao, Shilei and Ye, Mang},
title = {Random Walk Erasing with Attention Calibration for Action Recognition},
year = {2021},
isbn = {978-3-030-89369-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89370-5_18},
doi = {10.1007/978-3-030-89370-5_18},
abstract = {Action recognition in videos has attracted growing research interests because of the explosive surveillance data in social security applications. In this process, due to the distraction and deviation of the network caused by occlusions, human action features usually suffer different degrees of performance degradation. Considering the occlusion scene in the wild, we find that the occluded objects usually move unpredictably but continuously. Thus, we propose a random walk erasing with attention calibration (RWEAC) for action recognition. Specifically, we introduce the random walk erasing (RWE) module to simulate the unknown occluded real conditions in frame sequence, expanding the diversity of data samples. In the case of erasing (or occlusion), the attention area is sparse. We leverage the attention calibration (AC) module to force the attention to stay stable in other regions of interest. In short, our novel RWEAC network enhances the ability to learn comprehensive features in a complex environment and make the feature representation robust. Experiments are conducted on the challenging video action recognition UCF101 and HMDB51 datasets. The extensive comparison results and ablation studies demonstrate the effectiveness and strength of the proposed method.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III},
pages = {236–251},
numpages = {16},
keywords = {Action recognition, Random walk erasing, Data augmentation, Attention calibration, Siamese network},
location = {Hanoi, Vietnam}
}

@inproceedings{10.1145/1111449.1111536,
author = {Stumptner, Markus and Thomas, Bruce},
title = {Constraint-based livespaces configuration management},
year = {2006},
isbn = {1595932879},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1111449.1111536},
doi = {10.1145/1111449.1111536},
abstract = {In this paper, we describe use of constraint-based methods for configuring ubiquitous workspaces. A declarative representation allows succinct, easily maintainable definitions of the dependencies inherent in setting up a meeting, and permits the use of general constraint reasoners for various standard tasks such as setting up meeting interfaces, switching between setting for different meetings, and saving and restoring settings. Personalisation techniques can be used for intelligently adapting the workspace to individual user needs.},
booktitle = {Proceedings of the 11th International Conference on Intelligent User Interfaces},
pages = {357–359},
numpages = {3},
keywords = {configuration, constraint satisfaction, liveSpaces, ubiquitous workspaces},
location = {Sydney, Australia},
series = {IUI '06}
}

@inproceedings{10.1145/3336294.3336315,
author = {Wolschke, Christian and Becker, Martin and Schneickert, S\"{o}ren and Adler, Rasmus and MacGregor, John},
title = {Industrial Perspective on Reuse of Safety Artifacts in Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336315},
doi = {10.1145/3336294.3336315},
abstract = {In the future, safety-critical industrial products will have to be maintained and variants will have to be produced. In order to do this economically, the safety artifacts of the components should also be reused. At present, however, it is still unclear how this reuse could take place. Moreover this reuse is complicated, by the different situations in the various industries involved and by the corresponding standards applied.Current industrial practice for certification processes relies on a component-based view of reuse. We investigate the possibilities of product lines with managed processes for reuse also across multiple domains.In order to identify the challenges and possible solutions, we conducted interviews with industry partners from the domains of ICT, Rail, Automotive, and Industrial Automation, and from small- and medium-sized enterprises to large organizations. The semi-structured interviews identified the characteristics of current safety engineering processes, the handling of general variety and reuse, the approach followed for safety artifacts, and the need for improvement.In addition, a detailed literature survey summarizes existing approaches. We investigate which modularity concepts exist for dealing with safety, how variability concepts integrate safety, by which means process models can consider safety, and how safety cases are evolved while maintenance takes place. An overview of similar research projects complements the analysis.The identified challenges and potential solution proposals show how safety is related to Software Product Lines.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {143–154},
numpages = {12},
keywords = {modular safety, open source certification, product line certification, safety reuse, safety standards},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.jss.2019.05.001,
author = {Kicsi, Andr\'{a}s and Csuvik, Viktor and Vid\'{a}cs, L\'{a}szl\'{o} and Horv\'{a}th, Ferenc and Besz\'{e}des, \'{A}rp\'{a}d and Gyim\'{o}thy, Tibor and Kocsis, Ferenc},
title = {Feature analysis using information retrieval, community detection and structural analysis methods in product line adoption},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.05.001},
doi = {10.1016/j.jss.2019.05.001},
journal = {J. Syst. Softw.},
month = sep,
pages = {70–90},
numpages = {21},
keywords = {Software product line, Feature extraction, Information retrieval, Community detection}
}

@inproceedings{10.1145/3109729.3109758,
author = {Ben Snaiba, Ziad and de Vink, Erik P. and Willemse, Tim A.C.},
title = {Family-Based Model Checking of SPL based on mCRL2},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109758},
doi = {10.1145/3109729.3109758},
abstract = {We discuss how the general-purpose model checker mCRL2 can be used for family-based verification of behavioral properties of software product lines. This is achieved by exploiting a feature-oriented extension of the modal μ-calculus for the specification of SPL properties, and for its model checking by encoding it back into the logic of mCRL2. Using the example of the well-known minepump SPL an illustration of the possibilities of the approach is given.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {Family-based model checking, Software Product Lines, mCRL2},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3336294.3336295,
author = {Beek, Maurice H. ter and Damiani, Ferruccio and Lienhardt, Michael and Mazzanti, Franco and Paolini, Luca},
title = {Static Analysis of Featured Transition Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336295},
doi = {10.1145/3336294.3336295},
abstract = {A Featured Transition System (FTS) is a formal behavioural model for software product lines, which represents the behaviour of all the products of an SPL in a single compact structure by associating transitions with features that condition their existence in products. In general, an FTS may contain featured transitions that are unreachable in any product (so called dead transitions) or, on the contrary, mandatorily present in all products for which their source state is reachable (so called false optional transitions), as well as states from which only for certain products progress is possible (so called hidden deadlocks). In this paper, we provide algorithms to analyse an FTS for such ambiguities and to transform an ambiguous FTS into an unambiguous FTS. The scope of our approach is twofold. First and foremost, an ambiguous model is typically undesired as it gives an unclear idea of the SPL. Second, an unambiguous FTS paves the way for efficient family-based model checking. We apply our approach to illustrative examples from the literature.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {39–51},
numpages = {13},
keywords = {behavioural model, featured transition systems, formal specification, software product lines, static analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3106195.3106207,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106207},
doi = {10.1145/3106195.3106207},
abstract = {Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering &amp; machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {133–142},
numpages = {10},
keywords = {Feature Identification, Natural Language Documents, Reverse Engineering, Software Product Lines, Systematic Literature Review, Variability Extraction},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {asset health, extensibility, industrial analytics, interoperability, knowledge, performance, reusability, software product line},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@book{10.5555/2974992,
author = {Fisher, Michael David and Gabbay, Dov M. and Vila, Lluis},
title = {Handbook of Temporal Reasoning in Artificial Intelligence},
year = {2005},
isbn = {9780080533360},
abstract = {This collection represents the primary reference work for researchers and students in the area of Temporal Reasoning in Artificial Intelligence. Temporal reasoning has a vital role to play in many areas, particularly Artificial Intelligence. Yet, until now, there has been no single volume collecting together the breadth of work in this area. This collection brings together the leading researchers in a range of relevant areas and provides an coherent description of the breadth of activity concerning temporal reasoning in the filed of Artificial Intelligence. Key Features: - Broad range: foundations; techniques and applications - Leading researchers around the world have written the chapters - Covers many vital applications - Source book for Artificial Intelligence, temporal reasoning - Approaches provide foundation for many future software systems ·Broad range: foundations; techniques and applications ·Leading researchers around the world have written the chapters ·Covers many vital applications ·Source book for Artificial Intelligence, temporal reasoning ·Approaches provide foundation for many future software systems Table of Contents Formal Theories of Time and Temproal Incidence, Lluis Vila. Eventualities, Antony Galton. Time Granularity, Jerome Euzenat and Angelo Montanari. Modal Varieties of Temporal Logic, Howard Barringer and Dov Gabbay. Temporal Qualification in Artificial Intelligence, Han Reichgelt and Lluis Vila. Computational Complexity of Temporal Constraint Problems, Thomas Drakengren and Peter Jonsson. Indefinite Constraint Databases with Temporal Information: Representational Power and Computational Complexity, Manolis Koubarakis. Processing Qualitative Temporal Constraints, Alfonso Gerevini. Theorem-Proving for Discrete Temporal Logic, Mark ReynoldsClare Dixon. Probabilistic Temporal Reasoning, Steve HanksDavid Madigan. Temporal Reasoning with iff-Abduction, Marc DeneckerKristof Van Belleghem. Temporal Description Logics, Alessandro ArtaleEnrico Franconi. Logic Programming and Reasoning about Actions, Chitta BaralMichael Gelfond. Temporal Databases Jan ChomickiDavid Toman. Temporal Reasoning in Agent-Based Systems Michael FisherMichael Wooldridge. Time in Planning Maris FoxDerek Long. Time in Automated Legal Reasoning Lluis VilaHajime Yoshino. Temporal Reasoning in Natural Language Alice ter Meulen. Temporal Reasoning in Medicine Elpida KeravnouYuval Shahar. Time in Qualitative Simulation Dan ClancyBenjamin Kuipers.}
}

@inproceedings{10.1145/2019136.2019173,
author = {Fukuda, Takeshi and Atarashi, Yoshitaka and Yoshimura, Kentaro},
title = {An approach to evaluate time-dependent changes in feature constraints},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019173},
doi = {10.1145/2019136.2019173},
abstract = {Feature selections mining is the process of discovering potentially feature associations and constraints in data. Especially, mining from time-series data obtains feature constraint trends. In this paper, we describe an approach to evaluate feature constraint trends and present results of two case studies. Feature selections mining was applied to a product transactions database at Hitachi. The product transactions had 148 optional features, and 8,372 products were derived from the product line. Both case studies focus on transaction-time periods: time series and time intervals. Feature selections mining discovered feature constraints around 100 rules in each study, and determined they constantly change.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {33},
numpages = {5},
keywords = {embedded systems, feature modeling, industry case study, software product line engineering},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2934466.2934484,
author = {Vasilevskiy, Anatoly and Chauvel, Franck and Haugen, \O{}ystein},
title = {Toward robust product realisation in software product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934484},
doi = {10.1145/2934466.2934484},
abstract = {Product derivation is a building process of products from selected features in software product lines (SPLs). Realisation paves the way for automatic product derivation. A realisation defines a mapping between abstract features in a feature tree and their implementation artefacts in a model, and therefore governs the derivation of a new product. We experience that a realisation is not always straightforward and robust against modifications in the model. In the paper, we introduce an approach to build robust realisations. It consists of automated planning techniques and a layered architecture to yield a product. We demonstrate how our approach can leverage modern means of software design, development and validation. We evaluate the approach on a use-case provided by an industry partner and compare our technique to the existing realisation layer in the Base Variability Resolution (BVR) language.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {184–193},
numpages = {10},
keywords = {automated planning, bvr, fragment substitution, model, product derivation, product line, realisation, variation point},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2647648.2647649,
author = {Raschke, Wolfgang and Zilli, Massimiliano and Loinig, Johannes and Weiss, Reinhold and Steger, Christian and Kreiner, Christian},
title = {Embedding research in the industrial field: a case of a transition to a software product line},
year = {2014},
isbn = {9781450330459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647648.2647649},
doi = {10.1145/2647648.2647649},
abstract = {Java Cards [4, 5] are small resource-constrained embedded systems that have to fulfill rigorous security requirements. Multiple application scenarios demand diverse product performance profiles which are targeted towards markets such as banking applications and mobile applications. In order to tailor the products to the customer's needs we implemented a Software Product Line (SPL). This paper reports on the industrial case of an adoption to a SPL during the development of a highly-secure software system. In order to provide a scientific method which allows the description of research in the field, we apply Action Research (AR). The rationale of AR is to foster the transition of knowledge from a mature research field to practical problems encountered in the daily routine. Thus, AR is capable of providing insights which might be overlooked in a traditional research approach. In this paper we follow the iterative AR process, and report on the successful transfer of knowledge from a research project to a real industrial application.},
booktitle = {Proceedings of the 2014 International Workshop on Long-Term Industrial Collaboration on Software Engineering},
pages = {3–8},
numpages = {6},
keywords = {action research, knowledge transfer, software reuse},
location = {Vasteras, Sweden},
series = {WISE '14}
}

@inproceedings{10.1109/ICICIC.2006.1,
author = {Xu, Hui and Xiao, Debao},
title = {A Common Ontology-Based Intelligent Configuration Management Model for IP Network Devices},
year = {2006},
isbn = {0769526160},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICICIC.2006.1},
doi = {10.1109/ICICIC.2006.1},
abstract = {As the Internet continues to grow, the tasks of configuration management for IP network devices are becoming more and more difficult. Over the past few years, much effort has been given to improve the deficiency of SNMP in the configuration management scope, but only few have succeeded to be standardized, the famous one of which is Netconf, developed by the IETF. Even the Netconf is still far from its aim to implement the automation of the network configuration. These days, ontology has become a promising technology and it seems that it may be used in the intelligent configuration management scope. This paper first discusses the application of ontology and three related languages (including OWL, SWRL and OWL-S) to intelligentize configuration management. A common ontology-based intelligent configuration management model for IP network devices is then presented in the paper. To check the feasibility of this model, the paper also provides a possible scenario.},
booktitle = {Proceedings of the First International Conference on Innovative Computing, Information and Control - Volume 1},
pages = {385–388},
numpages = {4},
series = {ICICIC '06}
}

@article{10.1017/S0890060405050043,
author = {Yeh, Jinn-Yi and Wu, Tai-Hsi},
title = {Solutions for product configuration management: An empirical study},
year = {2005},
issue_date = {January 2005},
publisher = {Cambridge University Press},
address = {USA},
volume = {19},
number = {1},
issn = {0890-0604},
url = {https://doi.org/10.1017/S0890060405050043},
doi = {10.1017/S0890060405050043},
abstract = {Customers can directly express their preferences on many options when ordering products today. Mass customization manufacturing thus has emerged as a new trend for its aiming to satisfy the needs of individual customers. This process of offering a wide product variety often induces an exponential growth in the volume of information and redundancy for data storage. Thus, a technique for managing product configuration is necessary, on the one hand, to provide customers faster configured and lower priced products, and on the other hand, to translate customers' needs into the product information needed for tendering and manufacturing. This paper presents a decision-making scheme through constructing a product family model (PFM) first, in which the relationship between product, modules, and components are defined. The PFM is then transformed into a product configuration network. A product configuration problem assuming that customers would like to have a minimum-cost and customized product can be easily solved by finding the shortest path in the corresponding product configuration network. Genetic algorithms (GAs), mathematical programming, and tree-searching methods such as uniform-cost search and iterative deepening A* are applied to obtain solutions to this problem. An empirical case is studied in this work as an example. Computational results show that the solution quality of GAs retains 93.89% for a complicated configuration problem. However, the running time of GAs outperforms the running time of other methods with a minimum speed factor of 25. This feature is very useful for a real-time system.},
journal = {Artif. Intell. Eng. Des. Anal. Manuf.},
month = jan,
pages = {39–47},
numpages = {9},
keywords = {Configuration, Genetic Algorithm, Mathematical Programming, Product Family Model, Tree Searching}
}

@inproceedings{10.1145/1135777.1135842,
author = {Nguyen, Tien N.},
title = {Model-based version and configuration management for a web engineering lifecycle},
year = {2006},
isbn = {1595933239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1135777.1135842},
doi = {10.1145/1135777.1135842},
abstract = {During a lifecycle of a large-scale Web application, Web developers produce a wide variety of inter-related Web objects. Following good Web engineering practice, developers often create them based on a Web application development method, which requires certain logical models for the development and maintenance process. Web development is dynamic, thus, those logical models as well as Web artifacts evolve over time. However, the task of managing their evolution is still very inefficient because design decisions in models are not directly accessible in existing file-based software configuration management repositories. Key limitations of existing Web version control tools include their inadequacy in representing semantics of design models and inability to manage the evolution of model-based objects and their logical connections to Web documents. This paper presents a framework that allows developers to manage versions and configurations of models and to capture changes to model-to-model relations among Web objects. Model-based objects, Web documents, and relations are directly represented and versioned in a structure-oriented manner.},
booktitle = {Proceedings of the 15th International Conference on World Wide Web},
pages = {437–446},
numpages = {10},
keywords = {model-based configuration management, versioned hypermedia, web engineering},
location = {Edinburgh, Scotland},
series = {WWW '06}
}

@article{10.1016/j.artmed.2021.102162,
author = {Naranjo, Lizbeth and P\'{e}rez, Carlos J. and Campos-Roca, Yolanda and Madruga, Mario},
title = {Replication-based regularization approaches to diagnose Reinke's edema by using voice recordings},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102162},
doi = {10.1016/j.artmed.2021.102162},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {10},
keywords = {Acoustic features, Classification, Reinke's edema, Regularization, Replicated measurements, Variable selection}
}

@inproceedings{10.1145/2364412.2364422,
author = {Damiani, Ferruccio and Owe, Olaf and Dovland, Johan and Schaefer, Ina and Johnsen, Einar Broch and Yu, Ingrid Chieh},
title = {A transformational proof system for delta-oriented programming},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364422},
doi = {10.1145/2364412.2364422},
abstract = {Delta-oriented programming is a modular, yet flexible technique to implement software product lines. To efficiently verify the specifications of all possible product variants of a product line, it is usually infeasible to generate all product variants and to verify them individually. To counter this problem, we propose a transformational proof system in which the specifications in a delta module describe changes to previous specifications. Our approach allows each delta module to be verified in isolation, based on symbolic assumptions for calls to methods which may be in other delta modules. When product variants are generated from delta modules, these assumptions are instantiated by the actual guarantees of the methods in the considered product variant and used to derive the specifications of this product variant.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {53–60},
numpages = {8},
keywords = {program verification, proof system, software product line},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {product and production configuration, product line of factories, smart factory, smart product, smart production},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2647908.2655964,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Using similarity metrics for mining variability from software repositories},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655964},
doi = {10.1145/2647908.2655964},
abstract = {Much activity within software product line engineering has been concerned with explicitly representing and exploiting commonality and variability at the feature level for the purpose of a particular engineering task e.g. requirements specification, design, coding, verification, product derivation process, but not for comparing how similar products in the product line are with each other. In contrast, a case-based approach to software development is concerned with descriptions and models as a set of software cases stored in a repository for the purpose of searching at a product level, typically as a foundation for new product development. New products are derived by finding the most similar product descriptions in the repository using similarity metrics.The new idea is to use such similarity metrics for mining variability from software repositories. In this sense, software product line engineering could be informed by the case-based approach. This approach requires defining and implementing such similarity metrics based on the representations used for the software cases in such a repository. It provides complementary benefits to the ones given through feature-based representations of variability and may help mining such variability.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {32–35},
numpages = {4},
keywords = {case-based reasoning, commonality and variability, feature-based representation, product lines, similarity metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2648511.2648526,
author = {Acher, Mathieu and Baudry, Benoit and Barais, Olivier and J\'{e}z\'{e}quel, Jean-Marc},
title = {Customization and 3D printing: a challenging playground for software product lines},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648526},
doi = {10.1145/2648511.2648526},
abstract = {3D printing is gaining more and more momentum to build customized product in a wide variety of fields. We conduct an exploratory study of Thingiverse, the most popular Website for sharing user-created 3D design files, in order to establish a possible connection with software product line (SPL) engineering. We report on the socio-technical aspects and current practices for modeling variability, implementing variability, configuring and deriving products, and reusing artefacts. We provide hints that SPL-alike techniques are practically used in 3D printing and thus relevant. Finally, we discuss why the customization in the 3D printing field represents a challenging playground for SPL engineering.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {142–146},
numpages = {5},
keywords = {3D printing, customization, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.5555/1892801.1892807,
author = {Tosic, Vladimir and Mennie, David and Pagurek, Bernard},
title = {Software configuration management related to the management of distributed systems and service-oriented architectures},
year = {2003},
isbn = {3540140360},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We summarize our three research projects related to software configuration management and discuss three challenges for the future research in software configuration management. The three projects that we discuss are dynamic service composition from service components, extending service components with multiple classes of service and mechanisms for their manipulation, and dynamic evolution of network management software. The three challenges to software configuration management research that we are interested in are: 1) managing dynamism and run-time change, 2) integration of software configuration management with other management areas and domains, and 3) management of Web Service compositions.},
booktitle = {Proceedings of the 2001 ICSE Workshops on SCM 2001, and SCM 2003 Conference on Software Configuration Management},
pages = {54–69},
numpages = {16},
location = {Toronto, Canada},
series = {SCM'01/SCM'03}
}

@inproceedings{10.5555/646972.713540,
author = {Kandt, Ronald Kirk},
title = {Software Configuration Management Principles and Best Practices},
year = {2002},
isbn = {3540002340},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper identifies fundamental principles and practices essential to the successful performance of a configuration management system. Practices are grouped into four categories that govern the management process, ensure product quality, protect software artifacts, and guide tool use. In addition, the practices are prioritized according to their effect on software products and processes and the coverage of the identified principles. When these practices should be applied in the software development lifecycle is discussed, as is the potential for automating and validating practices.},
booktitle = {Proceedings of the 4th International Conference on Product Focused Software Process Improvement},
pages = {300–313},
numpages = {14},
series = {PROFES '02}
}

@inproceedings{10.1145/2791060.2791069,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Empirical comparison of regression methods for variability-aware performance prediction},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791069},
doi = {10.1145/2791060.2791069},
abstract = {Product line engineering derives product variants by selecting features. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable product variant. We infer such a correlation using four regression methods based on small samples of measured configurations, without additional effort to detect feature interactions. We conduct experiments on six real-world case studies to evaluate the prediction accuracy of the regression methods. A key finding in our empirical study is that one regression method, called Bagging, is identified as the best to make accurate and robust predictions for the studied systems.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {186–190},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.5555/1251150.1251165,
author = {Narain, Sanjai},
title = {Network configuration management via model finding},
year = {2005},
publisher = {USENIX Association},
address = {USA},
abstract = {Complex, end-to-end network services are set up via the configuration method: each component has a finite number of configuration parameters each of which is set to a definite value. End-to-end network service requirements can be on connectivity, security, performance and fault-tolerance. However, there is a large conceptual gap between end-to-end requirements and detailed component configurations. To bridge this gap, a number of subsidiary requirements are created that constrain, for example, the protocols to be used, and the logical structures and associated policies to be set up at different protocol layers.By performing different types of reasoning with these requirements, different configuration tasks are accomplished. These include configuration synthesis, configuration error diagnosis, configuration error fixing, reconfiguration as requirements or components are added and deleted, and requirement verification. However, such reasoning is currently ad hoc. Network requirements are not even precisely specified hence automation of reasoning is impossible. This is a major reason for the high cost of network management and total cost of ownership. This paper shows how to formalize and automate such reasoning using a new logical system called Alloy.Alloy is based on the concept of model finding. Given a first-order logic formula and a domain of interpretation, Alloy tries to find whether the formula is satisfiable in that domain, i.e., whether it has a model. Alloy is used to build a Requirement Solver that takes as input a set of network components and requirements upon their configurations and determines component configurations satisfying those requirements.This Solver is used in different ways to accomplish the above reasoning tasks. The Solver is illustrated in depth by carrying out a variety of these tasks in the context of a realistic fault-tolerant virtual private network with remote access. Alloy uses modern satisfiability solvers that solve millions of constraints in millions of variables in seconds. However, poor requirements can easily nullify such speeds. The paper outlines approaches for writing efficient requirements. Finally, it outlines directions for future research.},
booktitle = {Proceedings of the 19th Conference on Large Installation System Administration Conference - Volume 19},
pages = {15},
numpages = {1},
location = {San Diego, CA},
series = {LISA '05}
}

@inproceedings{10.1145/1185448.1185636,
author = {Gurupur, Varadraj and Tanik, Urcun J},
title = {Software cultivation using the artificial intelligence design framework},
year = {2006},
isbn = {1595933158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1185448.1185636},
doi = {10.1145/1185448.1185636},
abstract = {All along the history of software engineering, traditional software development process has always been a labor intensive process. This is perhaps because we are still in the preliminary evolutionary stage of software production where the software has to be built by a group of software developers either from scratch or by combining and/or reusing the components that have already been developed. In this paper we propose a unique method of building software in a way that is analogous to the growth of any organism to the stage of an adult. In this process of software cultivation, the initial framework of the software system is built by a group of developers and then the system enhances its functionality by gathering domain knowledge from various regions of the Internet by using the Artificial Intelligence Design Framework (AIDF). The internal development of the system is guided by various design theories and methods such as Axiomatic design Theory (ADT) and Design Structure Matrix (DSM). The concept of AIDF was first presented to build a domain expert system in the field of Optical Backplane Engineering, but it was later found that it could be used to solve a wide range of problems including software development. In order to build a system that would design another system various methodologies and theories which have been proved and tested had to be utilized to build the framework for designing a system.Software development has been a process where lot of time and money has to be spent not only on the development process but also on a never-ending process of software maintenance. Sometimes this development process could face obstacles such as the creeping requirements problem where the requirements may change while the software development is still in process. Many software development processes may end in failure owing to the moving target problem due to the rapid change in the domain knowledge. Another way of looking at this problem would be that the domain expert will have to add changes to the domain expert system whenever the domain knowledge undergoes change in some form.The solution to this problem would be a migration from the process of software development to a process of software cultivation. This cultivation would be analogous to the cultivation of a plant in a pot, where we sow the seed and keep adding water and fertilizer to it. In this methodology, we create software and allow the software development framework to handle the process of software maintenance. Here, the process of software maintenance gets transformed into a process of software cultivation where the system consumes networked domain knowledge from the Internet and updates the software to meet the most recent requirements for a given problem domain. For instance, if we have developed a computer based system that helps in the design of optical fiber networks, a recent invention in that field could be used to update the system behavior.As part of the overall artificial intelligence approach, knowledge based engineering (KBE) plays an important role in the AIDF. There are many benefits of using KBE over traditional methods. The time to produce an engineering artifact can be substantially reduced by eliminating tedious engineering duties through automated reasoning. The wisdom of the organization and key people can be retained and accessed by future generations of workers. Concurrent engineering can be accomplished when disparate departments can meld their work together by working in parallel, as opposed to sequentially, enabled by a KBE system intelligently updating files in the background according to its programmed logic. The entire design process will become fully documented, pulling together scattered annotations, designs, suggestions, and established heuristics. Using new technologies, such as the Semantic Web, the KBE engineering system would enable designers to search repositories of parts globally, based on a sophisticated pre-recorded rule-base.One of the key recommendations made by the 2001 report Improving Engineering Design by the National Academy of Sciences was that "decision-making tools and decision theory" should be emphasized. In order to prevent problems occurring during the crucial conceptual stage of the design process, a framework needs to be created that enables the systematic collection, storage, and application of expert knowledge and state-of-the-art practices through machine intelligence. Addressing Suh's future projection of a "Thinking Design Machine" using software to assist engineers at the concept design stage, an Artificial Intelligence Design Framework (AIDF) is introduced. In order to help engineers get their design right the first time, this theory was shown to methodically create a valid design at the concept stage based on a set of axioms, corollaries, and theorems. In order to provide expert advice based on valid design theory, an AIDF is advanced using KBE techniques that capture expert domain knowledge and couple it to the Internet through the use of search agents and ontologies on the Semantic Web. An automated approach intended to get the design right the first time is accomplished by applying machine intelligence to sound axiomatic design theory. This validated approach is structured to prevent costly remedial work done at the end of the design cycle to correct for errors made in the conceptual design phase.The AIDF creates an efficient framework for design and redesign of a system. When a system is built from scratch and the domain knowledge is available on the Internet, sufficient initial design information should be made available to the AIDF. The AIDF will store this knowledge in the form of ontologies and RDF in the knowledge base and the database. The AIDF draws this information from the web agents and domain experts. It is the responsibility of the web agent to extract reliable information from the web, which can be greatly enhanced with Web Services and Semantic Web Technology. Once the information is stored in the AIDF, the AIDF starts rebuilding the software. First it redesigns the software and verifies the correctness of the design according to its rule-base and risk-mitigation algorithms. This verification is carried out in two stages. First the AIDF carries out the required analysis on the design and then if it finds that the design is fault free, it requests the user to verify the design. Once the design has been verified the code generator will generate the required code. If the designer considers that the design was inadequate or faulty, then the designer who also happens to be the user of the system makes recommendations into the system by considering the design as invalid. We need to bear in mind that the AIDF connects itself to the available domain knowledge available on the Internet and supplied directly by the domain expert. Of course, the AIDF does not intend to blindly pull a piece of code available to it and add it to the existing software without filtering through its rule-base and risk-mitigation algorithms. The idea of software cultivation using the AIDF is still in its embryonic stage and therefore, software quality assurance and other aspects associated with the process of software development could be dealt with as our research on software cultivation progresses.The knowledge base in Jena is represented in terms of RDF and ontologies. The ontologies provide a basis for the representation of domain knowledge. The RDF acts as a container for specifications and an excellent way of storing structured metadata. The domain knowledge of a particular system is very much subject to change and any change in the domain knowledge should be recorded and represented in a universally accepted way. Resource Description Framework (RDF) appears to be an excellent way of storing metadata. Jena, the Java technology tool for the Semantic Web, plays an integral role in this process. Jena identifies resources using the Uniform Resource Identifier (URI). The difference between the URI and the URL is that the URI can only identify a resource on the Web and is used predominantly in Web Services, whereas a URL not only identifies a resource on the Web but also locates it. This URI concept is used extensively in Jena. Jena contains library functions to build an RDF model. The complexity analysis for risk-mitigation and internal verification process is carried out using a combination of various analytical methodologies such as Conant theory, Axiomatic Design Theory, and Design Structure Matrix, in addition to risk analysis using methods such as Fault Tree Analysis (FTA) and Failure Mode and Effects Analysis (FMEA).This concept of building an AIDF requires some challenges to be met, most of which emanates from the reliability of the information available on the Internet. An immediate answer to that question would be that the AIDF accepts information that will fit into the system similar to that of a jigsaw puzzle. Sufficient amount of mathematical analysis will be applied on the incoming information content. On top of this, the web agents will derive domain information only from known sources. The AIDF is still in a conceptual phase and we are still looking at the technologies that will be used in building the AIDF.},
booktitle = {Proceedings of the 44th Annual ACM Southeast Conference},
pages = {786–787},
numpages = {2},
location = {Melbourne, Florida},
series = {ACMSE '06}
}

@inproceedings{10.5555/646089.680078,
author = {Estublier, Jacky},
title = {Objects Control for Software Configuration Management},
year = {2001},
isbn = {3540422153},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A major requirement in Software Engineering is to reduce the time to market. This requirement along with a demand for product sophistication and better quality has led to larger teams which in turn dramatically increases the pressure for more concurrent work in a distributed context.This paper, based on our experience in Software Configuration Management for large software systems, shows why object management in such a context requires specific facilities for the consistent management of objects in multiple copies, different locations and formats, accessed and changed simultaneously by many engineers.We present the solutions we have developed with our partner Dassault Syst mes, for the definition and enforcement of consistent concurrent engineering work, including a number of measures showing that scalability and efficiency are really tough issues.We argue that the scalability and efficiency constraints found in SCMcan only be met by a new architecture of SCM systems and by the development of a middleware layer that should be common to all SCM tools, and also usable by other applications sharing the same concerns.},
booktitle = {Proceedings of the 13th International Conference on Advanced Information Systems Engineering},
pages = {359–373},
numpages = {15},
keywords = {architecture, concurrent engineering, distribution, software configuration management, version control},
series = {CAiSE '01}
}

@inproceedings{10.5555/1416222.1416243,
author = {Fleischer, Paul and Kristensen, Lars M.},
title = {Modelling the configuration/management API middleware using coloured petri nets},
year = {2008},
isbn = {9789639799202},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {The Configuration/Management Application Programming Interface (CMAPI) is a vendor-specific API and middlewarelayer for configuration and management of components in embedded systems. CMAPI is used by TietoEnator Denmark in the implementation of a controller for the Generic Access Network (GAN) architecture. This paper presents a hierarchical Coloured Petri Net model of CMAPI that was developed in the process of applying Coloured Petri Nets and CPN Tools for the specification of the GAN controller.},
booktitle = {Proceedings of the 1st International Conference on Simulation Tools and Techniques for Communications, Networks and Systems &amp; Workshops},
articleno = {16},
numpages = {9},
keywords = {CPN tools, coloured petri nets, middleware, modelling, software specification},
location = {Marseille, France},
series = {Simutools '08}
}

@inproceedings{10.1109/DEPCOS-RELCOMEX.2006.4,
author = {Chou, I-Hsin and Fan, Chin-Feng},
title = {A Regulatory Software Maintenance Environment Using Agent-Based Software Configuration Management},
year = {2006},
isbn = {0769525652},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/DEPCOS-RELCOMEX.2006.4},
doi = {10.1109/DEPCOS-RELCOMEX.2006.4},
abstract = {When nuclear power plants adopt digital instrumentation and control (I&amp;C) systems, the inherent complexity and unpredictable nature of software create an unprecedented challenge for software maintenance. The difficulty often comes from lacking consistence among the vendors and the requirement of nuclear safety-related regulation. The aim of Regulatory Software Maintenance Environment (RSME) with agent-based software configuration management discipline is to provide guidelines and procedures for carrying out a variety of software maintenance activities through the establishment of a systematic approach to support the operation of nuclear power plant. This paper describes a few requirements of software maintenance environment, software maintenance model, RSME with agent-based architecture with XML technology and a prototype application of the digital I&amp;C of nuclear power plant.},
booktitle = {Proceedings of the International Conference on Dependability of Computer Systems},
pages = {264–275},
numpages = {12},
series = {DEPCOS-RELCOMEX '06}
}

@article{10.1613/jair.1.11688,
author = {Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
title = {Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11688},
doi = {10.1613/jair.1.11688},
abstract = {Interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as machine learning, computer vision, and natural language processing. Much of the growth in these fields has been made possible with deep learning, a sub-area of machine learning that uses artificial neural networks. This has created significant interest in the integration of vision and language. In this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey stimulates innovative thoughts and ideas to address the existing challenges and build new applications.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {1183–1317},
numpages = {135},
keywords = {natural language, machine learning, computer vision, deep learning}
}

@inproceedings{10.1109/SEmotion52567.2021.00011,
author = {Cummaudo, Alex and Graetsch, Ulrike Maria and Curumsing, Maheswaree K and Vasa, Rajesh and Barnett, Scott and Grundy, John},
title = {Emotions in Computer Vision Service Q&amp;A},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEmotion52567.2021.00011},
doi = {10.1109/SEmotion52567.2021.00011},
abstract = {Software developers are increasingly using cloud-based services that provide machine learning capabilities to implement ‘intelligent’ features. Studies show that incorporating machine learning into an application increases technical debt, creates data dependencies, and introduces uncertainty due to their non-deterministic behaviour. We know very little about the emotional state of software developers who have to deal with such issues; and the impacts on productivity. This paper presents a preliminary effort to better understand the emotions of developers when experiencing issues with these services with the wider goal of discovering potential service improvements. We conducted a landscape analysis of emotions found in 1,425 Stack Overflow questions about a specific and mature subset of these cloud-based services, namely those that provide computer vision techniques. To speed up the emotion identification process, we trialled an automatic approach using a pre-trained emotion classifier that was specifically trained on Stack Overflow content, EmoTxt, and manually verified its classification results. We found that the identified emotions vary for different types of questions, and a discrepancy exists between automatic and manual emotion analysis due to subjectivity.},
booktitle = {2021 IEEE/ACM Sixth International Workshop on Emotion Awareness in Software Engineering (SEmotion)},
pages = {13–18},
numpages = {6},
location = {Madrid, Spain}
}

@inproceedings{10.1145/2019136.2019162,
author = {Quinton, Cl\'{e}ment and Mosser, S\'{e}bastien and Parra, Carlos and Duchien, Laurence},
title = {Using multiple feature models to design applications for mobile phones},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019162},
doi = {10.1145/2019136.2019162},
abstract = {The design of a mobile phone application is a tedious task according to its intrinsic variability. Software designers must take into account in their development process the versatility of available platforms (e.g., Android, iPhone). In addition to this, the variety of existing devices and their divergences (e.g., frontal camera, GPS) introduce another layer of complexity in the development process. These two dimensions can be formalized as Software Product Lines (SPL), independently defined. In this paper, we use a dedicated metamodel to bridge the gap between an application SPL and a mobile device one. This meta-model is also the support for the product derivation process. The approach is implemented in a framework named Applide, and is used to successfully derive customer relationship management software on different devices.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {23},
numpages = {8},
keywords = {application for mobile phones, feature model, meta-model, smartphones, software product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1007/s10664-021-09944-w,
author = {Riom, Timoth\'{e} and Sawadogo, Arthur and Allix, Kevin and Bissyand\'{e}, Tegawend\'{e} F. and Moha, Naouel and Klein, Jacques},
title = {Revisiting the VCCFinder approach for the identification of vulnerability-contributing commits},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09944-w},
doi = {10.1007/s10664-021-09944-w},
abstract = {Detecting vulnerabilities in software is a constant race between development teams and potential attackers. While many static and dynamic approaches have focused on regularly analyzing the software in its entirety, a recent research direction has focused on the analysis of changes that are applied to the code. VCCFinder is a seminal approach in the literature that builds on machine learning to automatically detect whether an incoming commit will introduce some vulnerabilities. Given the influence of VCCFinder in the literature, we undertake an investigation into its performance as a state-of-the-art system. To that end, we propose to attempt a replication study on the VCCFinder supervised learning approach. The insights of our failure to replicate the results reported in the original publication informed the design of a new approach to identify vulnerability-contributing commits based on a semi-supervised learning technique with an alternate feature set. We provide all artefacts and a clear description of this approach as a new reproducible baseline for advancing research on machine learning-based identification of vulnerability-introducing commits.},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {30},
keywords = {Vulnerability detection, Machine learning, Replication, Software engineering}
}

@article{10.1007/s10270-020-00791-9,
author = {Westfechtel, Bernhard and Greiner, Sandra},
title = {Extending single- to multi-variant model transformations by trace-based propagation of variability annotations},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00791-9},
doi = {10.1007/s10270-020-00791-9},
abstract = {Model-driven engineering involves the construction of models on different levels of abstraction. Software engineers are supported by model transformations, which automate the transition from high- to low-level models. Product line engineering denotes a systematic process that aims at developing different product variants from a set of reusable assets. When model-driven engineering is combined with product line engineering, engineers have to deal with multi-variant models. In annotative approaches to product line engineering, model elements are decorated with annotations, i.e., Boolean expressions that define the product variants in which model elements are to be included. In model-driven product line engineering, domain engineers require multi-variant transformations, which create multi-variant target models from multi-variant source models. We propose a reuse-based gray-box approach to realizing multi-variant model transformations. We assume that single-variant transformations already exist, which have been developed for model-driven engineering, without considering product lines. Furthermore, we assume that single-variant transformations create traces, which comprise the steps executed in order to derive target models from source models. Single-variant transformations are extended into multi-variant transformations by trace-based propagation: after executing a single-variant transformation, the resulting single-variant target model is enriched with annotations that are calculated with the help of the transformation’s trace. This approach may be applied to single-variant transformations written in different languages and requires only access to the trace, not to the respective transformation definition. We also provide a correctness criterion for trace-based propagation, and a proof that this criterion is satisfied under the prerequisites of a formal computational model.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {853–888},
numpages = {36},
keywords = {Model transformation, Software product line, Annotative variability}
}

@inproceedings{10.1145/1852786.1852794,
author = {Murgia, Alessandro and Concas, Giulio and Marchesi, Michele and Tonelli, Roberto},
title = {A machine learning approach for text categorization of fixing-issue commits on CVS},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852794},
doi = {10.1145/1852786.1852794},
abstract = {We studied data mining from CVS repositories of two large OO projects, Eclipse and Netbeans, focusing on "fixing-issue" commits.We highlight common characteristics of issue reporting, and problems related to the identification of these messages, and compare static traditional approaches, like Knowledge Engineering, to dynamic approaches based on Machine Learning techniques. We compare for the first time performances of Machine Learning (ML) techniques to automatic classify "fixing-issues" among message commits. Our study calculates precision and recall of different Machine Learning Classifiers for the correct classification of issue-reporting commits. Our results show that some ML classifiers can correctly classify up to 99.9% of such commits.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {6},
numpages = {10},
keywords = {classifier, data mining, machine learning},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@inproceedings{10.1145/375735.376295,
author = {Giampapa, Joseph A. and Juarez-Espinosa, Octavio H. and Sycara, Katia P.},
title = {Configuration management for multi-agent systems},
year = {2001},
isbn = {158113326X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375735.376295},
doi = {10.1145/375735.376295},
abstract = {As heterogeneous distributed systems, multi-agent systems present some challenging configuration management issues.  There are the problems of knowing how to allocate agents to computers, launch them on remote hosts, and once the agents have been launched, how to monitor their runtime status so as to manage computing resources effectively.In this paper, we present the RETSINA Configuration Manager, emph{RECoMa}.  We describe its architecture, how it uses agent infrastructure such as service discovery, to assist the multi-agent system administrator in allocating, launching, and monitoring a heterogeneous distributed agent system in a distributed and networked computing environment.footnote{The authors would like to acknowledge the significant contributions of Matthew W. Easterday in his earlier and exhaustive implementations of configuration management programs and CM design proposals.  This research has been sponsored in part by DARPA Grant F-30602-98-2-0138 and the Office of Naval Research Grant N-00014-96-16-1-1222.}},
booktitle = {Proceedings of the Fifth International Conference on Autonomous Agents},
pages = {230–231},
numpages = {2},
location = {Montreal, Quebec, Canada},
series = {AGENTS '01}
}

@inproceedings{10.1145/2364412.2364439,
author = {Vale, Tassio and Figueiredo, Gustavo Bittencourt and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {A study on service identification methods for software product lines},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364439},
doi = {10.1145/2364412.2364439},
abstract = {The combination of service-orientation and software product line engineering, called Service-Oriented Product Line Engineering (SOPLE) have received attention by researchers and practitioners in the last years, and these areas can address issues of each other. One service-orientation issue is service identification. It consists of determining candidate services to a service-oriented environment based on pre-existing software artifacts, e.g., business process, source code, and so on. In order to provide a systematic identification of services, there are many available service identification methods in the literature, regarding different understanding of services, goals, and techniques. Due to this heterogeneity, this paper presents an in-depth comparison of service identification methods as well as a recommendation of the most suitable ones in the SOPLE context. This work can help the decision making of the most suitable method according to stakeholders' needs.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {156–163},
numpages = {8},
keywords = {service identification, service-oriented computing, service-oriented product lines, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/505532.505545,
author = {van der Hoek, Andr\'{e}},
title = {International workshop on software configuration management (SCM-10): new practices, new challenges, and new boundaries},
year = {2001},
issue_date = {November 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/505532.505545},
doi = {10.1145/505532.505545},
abstract = {This report provides a brief summary of SCM-10, the ICSE 2001 10th International Workshop on Software Configuration Management. The primary goal of this workshop was to broaden the scope of SCM and establish ties with other disciplines that are strongly related to SCM---whether requiring some form of novel, advanced SCM functionality or influencing the field of SCM with newly available technology. As demonstrated by the accepted set of position papers and the lively discussion in the workshop, SCM-10 succeeded in achieving this goal and raised many new and important questions to be addressed in the years to come.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {57–58},
numpages = {2},
keywords = {Configuration Management}
}

@inproceedings{10.1145/3109729.3109760,
author = {Basile, Davide and Di Giandomenico, Felicita and Gnesi, Stefania},
title = {FMCAT: Supporting Dynamic Service-based Product Lines},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109760},
doi = {10.1145/3109729.3109760},
abstract = {We describe FMCAT, a toolkit for Featured Modal Contract Automata (FMCA). FMCAT supports the analysis of dynamic service product lines, i.e., applications consisting of ensembles of interacting services organized as product lines. Services are modelled as FMCA, with features identifying obligations and requirements of services. Service requirements can be either permitted or necessary, whereas the latter are further partitioned according to their criticality. A notion of agreement among service contracts is used to characterise safety.We show how FMCAT can be used to (i) specify dynamic service product line, (ii) efficiently identify all valid products, and to synthesise a safe orchestration of services for either (iii) a single product, or (iv) the whole service product line. FMCAT exploits the theory of FMCA to efficiently perform the above tasks by only visiting a subset of valid products, and it is equipped with a GUI.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {3–8},
numpages = {6},
keywords = {Featured Modal Contract Automata Tool, Product line, Services},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2934466.2934485,
author = {Lape\~{n}a, Ra\'{u}l and Ballarin, Manuel and Cetina, Carlos},
title = {Towards clone-and-own support: locating relevant methods in legacy products},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934485},
doi = {10.1145/2934466.2934485},
abstract = {Clone-and-Own (CAO) is a common practice in families of software products consisting of reusing code from methods in legacy products in new developments. In industrial scenarios, CAO consumes high amounts of time and effort without guaranteeing good results. We propose a novel approach, Computer Assisted CAO (CACAO), that given the natural language requirements of a new product, and the legacy products from that family, ranks the legacy methods in the family for each of the new product requirements according to their relevancy to the new development. We evaluated our approach in the industrial domain of train control software. Without CACAO, software engineers tasked with the development of a new product had to manually review a total of 2200 methods in the family. Results show that CACAO can reduce the number of methods to be reviewed, and guide software engineers towards the identification of relevant legacy methods to be reused in the new product.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {194–203},
numpages = {10},
keywords = {clone and own, families of software products, software reuse},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1007/978-3-030-87589-3_19,
author = {Yan, Yutong and Conze, Pierre-Henri and Lamard, Mathieu and Zhang, Heng and Quellec, Gwenol\'{e} and Cochener, B\'{e}atrice and Coatrieux, Gouenou},
title = {Deep Active Learning for Dual-View Mammogram Analysis},
year = {2021},
isbn = {978-3-030-87588-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87589-3_19},
doi = {10.1007/978-3-030-87589-3_19},
abstract = {Supervised deep learning on medical imaging requires massive manual annotations, which are expertise-needed and time-consuming to perform. Active learning aims at reducing annotation efforts by adaptively selecting the most informative samples for labeling. We propose in this paper a novel deep active learning approach for dual-view mammogram analysis, especially for breast mass segmentation and detection, where the necessity of labeling is estimated by exploiting the consistency of predictions arising from craniocaudal (CC) and mediolateral-oblique (MLO) views. Intuitively, if mass segmentation or detection is robustly performed, prediction results achieved on CC and MLO views should be consistent. Exploiting the inter-view consistency is hence a good way to guide the sampling mechanism which iteratively selects the next image pairs to be labeled by an oracle. Experiments on public DDSM-CBIS and INbreast datasets demonstrate that comparable performance with respect to fully-supervised models can be reached using only 6.83% (9.56%) of labeled data for segmentation (detection). This suggests that combining dual-view mammogram analysis and active learning can strongly contribute to the development of computer-aided diagnosis systems.},
booktitle = {Machine Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings},
pages = {180–189},
numpages = {10},
keywords = {Breast cancer, Mass segmentation, Mass detection, Dual-view mammogram analysis, Active learning, Computer-aided diagnosis},
location = {Strasbourg, France}
}

@inproceedings{10.1145/2635868.2661684,
author = {Joseph, Harry Raymond},
title = {Software programmer management: a machine learning and human computer interaction framework for optimal task assignment},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2661684},
doi = {10.1145/2635868.2661684},
abstract = {This paper attempts optimal task assignment at the enterprise-level by assigning complexity metrics to the programming tasks and predicting task completion times for each of these tasks based on a machine learning framework that factors in programmer attributes. The framework also considers real-time programmer state by using a simple EEG device to detect programmer mood. A final task assignment is made using a PDTS solver.},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {826–828},
numpages = {3},
keywords = {Complexity metrics, completion time prediction, task assignment},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.5555/1892801.1892806,
author = {Chu-Carroll, Mark C. and Wright, James},
title = {Supporting distributed collaboration through multidimensional software configuration management},
year = {2003},
isbn = {3540140360},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In recent years, new software development methodologies and styles have become popular. In particular, many applications are being developed in the open-source community by groups of loosely coordinated programmers scattered across the globe.This style of widely distributed collaboration creates a suite of new problems for software development. Instead of being able to knock on the door of a collaborator, all communication between programmers working together on a system must be mediated through the computer. But at the same time, the bandwidth available for communication is dramatically more limited than those available to local collaborators.In this paper, we present a new SCM system called Stellation which is specifically designed to address the limits of current SCM systems, particularly when those systems are applied to large projects developed in a geographically distributed environment. Stellation attempts to enhance communication and collaboration between programmers by providing a mechanism called multidimensionality that allows them to share viewpoints on the structure and organization of the system; by providing a hierarchical branching mechanism that allows the granularity of coordination to be varied for different purposes; and by providing a mechanism for integrating programming language knowledge into the system, allowing it to be used for organizational and coordination purposes.},
booktitle = {Proceedings of the 2001 ICSE Workshops on SCM 2001, and SCM 2003 Conference on Software Configuration Management},
pages = {40–53},
numpages = {14},
location = {Toronto, Canada},
series = {SCM'01/SCM'03}
}

@article{10.5555/170656.170683,
author = {Murtagh, Niall},
title = {Engineering design through constraint-based reasoning},
year = {1993},
issue_date = {September 1993},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {14},
number = {3},
issn = {0738-4602},
journal = {AI Mag.},
month = sep,
pages = {100–101},
numpages = {2}
}

@article{10.1007/s10586-019-03012-1,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e} and Ther\'{o}n, Roberto and Amo Filv\`{a}, Daniel and Fonseca Escudero, David},
title = {Connecting domain-specific features to source code: towards the automatization of dashboard generation},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-019-03012-1},
doi = {10.1007/s10586-019-03012-1},
abstract = {Dashboards are useful tools for generating knowledge and support decision-making processes, but the extended use of technologies and the increasingly available data asks for user-friendly tools that allow any user profile to exploit their data. Building tailored dashboards for any potential user profile would involve several resources and long development times, taking into account that dashboards can be framed in very different contexts that should be studied during the design processes to provide practical tools. This situation leads to the necessity of searching for methodologies that could accelerate these processes. The software product line paradigm is one recurrent method that can decrease the time-to-market of products by reusing generic core assets that can be tuned or configured to meet specific requirements. However, although this paradigm can solve issues regarding development times, the configuration of the dashboard is still a complex challenge; users’ goals, datasets, and context must be thoroughly studied to obtain a dashboard that fulfills the users’ necessities and that fosters insight delivery. This paper outlines the benefits and a potential approach to automatically configuring information dashboards by leveraging domain commonalities and code templates. The main goal is to test the functionality of a workflow that can connect external algorithms, such as artificial intelligence algorithms, to infer dashboard features and feed a generator based on the software product line paradigm.},
journal = {Cluster Computing},
month = sep,
pages = {1803–1816},
numpages = {14},
keywords = {SPL, Domain engineering, Meta-model, Information dashboards, Feature model, Artificial intelligence, Automatic configuration}
}

@inproceedings{10.5555/525587.851324,
author = {Berghoff, Juergen and Drobnik, Oswald and Lingnau, Anselm and Moench, Christian},
title = {Agent-Based Configuration Management of Distributed Applications},
year = {1996},
isbn = {0818673958},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Mobile agents provide a new paradigm for distributed computation. Their unique properties appear well suited to the configuration management of large and complex distributed applications. After outlining our concept of configuration management, we describe a basic agent infra- structure and its extensions for configuration management of distributed applications. Then we consider aspects of concurrent agents and discuss the advantages of the agent based approach through an example.},
booktitle = {Proceedings of the 3rd International Conference on Configurable Distributed Systems},
pages = {52},
keywords = {Configuration management, distributed applications, mobile agents},
series = {ICCDS '96}
}

@inproceedings{10.1145/3106195.3106206,
author = {Arcaini, Paolo and Gargantini, Angelo and Vavassori, Paolo},
title = {Automated Repairing of Variability Models},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106206},
doi = {10.1145/3106195.3106206},
abstract = {Variability models are a common means for describing the commonalities and differences in Software Product Lines (SPL); configurations of the SPL that respect the constraints imposed by the variability model define the problem space. The same variability is usually also captured in the final implementation through implementation constraints, defined in terms of preprocessor directives, build files, build-time errors, etc. Configurations satisfying the implementation constraints and producing correct (compilable) programs define the solution space. Since sometimes the variability model is defined after the implementation exists, it could wrongly assess the validity of some system configurations, i.e., it could consider acceptable some configurations (not belonging to the solution space) that do not permit to obtain a correct program. We here propose an approach that automatically repairs variability models such that the configurations they consider valid are also part of the solution space. Experiments show that some existing variability models are indeed faulty and can be repaired by our approach.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {9–18},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.5555/2503308.2503354,
author = {Gould, Stephen},
title = {DARWIN: a framework for machine learning and computer vision research and development},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
issn = {1532-4435},
abstract = {We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data flows.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {3533–3537},
numpages = {5},
keywords = {computer vision, graphical models, machine learning, open-source software}
}

@inproceedings{10.1007/978-3-030-64694-3_17,
author = {Benmerzoug, Amine and Yessad, Lamia and Ziadi, Tewfik},
title = {Analyzing the Impact of Refactoring Variants on Feature Location},
year = {2020},
isbn = {978-3-030-64693-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64694-3_17},
doi = {10.1007/978-3-030-64694-3_17},
abstract = {Due to the increasing importance of feature location process, several studies evaluate the performance of different techniques based on IR strategies and a set of software variants as input artifacts. The proposed techniques attempt to improve the results obtained but it is often a difficult task. None of the existing feature location techniques considers the changing nature of the input artifacts, which may undergo series of refactoring changes. In this paper, we investigate the impact of refactoring variants on the feature location techniques. We first evaluate the performance of two techniques through the ArgoUML SPL benchmark when the variants are refactored. We then discuss the degraded results and the possibility of restoring them. Finally, we outline a process of variant alignment that aims to preserve the performance of the feature location.},
booktitle = {Reuse in Emerging Software Engineering Practices: 19th International Conference on Software and Systems Reuse, ICSR 2020, Hammamet, Tunisia, December 2–4, 2020, Proceedings},
pages = {279–291},
numpages = {13},
keywords = {Software Product Line, Feature location, Refactoring},
location = {Hammamet, Tunisia}
}

@article{10.1145/605466.605482,
author = {Chu-Carroll, Mark C. and Wright, James and Shields, David},
title = {Supporting aggregation in fine grained software configuration management},
year = {2002},
issue_date = {November 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/605466.605482},
doi = {10.1145/605466.605482},
abstract = {Fine-grained software configuration management offers substantial benefits for large-scale collaborative software development, enabling a variety of interesting and useful features including complexity management, support for aspect-oriented software development, and support for communication and coordination within software engineering teams, described in [4]. However, fine granularity by itself is not sufficient to achieve these benefits. Most of the benefits of fine granularity result from the ability to combine fine-grained artifacts in various ways: supporting multiple overlapping organizations of program source by combining fine-grained artifacts into virtual source files (VSFs); supporting coordination by allowing developers to precisely mark the set of artifacts affected by a change; associating products from different phases of the development process; etc.In this paper, we describe how a general aggregation mechanism can be used to support the various functionality enabled by fine grained SCM. We present a set of requirements that an aggregation facility must provide in order to yield these benefits, and we provide a description of the implementation of such an aggregation system in our experimental SCM system.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {99–108},
numpages = {10},
keywords = {aggregation, dynamic program organization, fine grained storage}
}

@article{10.1023/A:1018682001890,
author = {Thompson, S. M.},
title = {Configuration management — keeping it all together},
year = {1997},
issue_date = {July 1997},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1358-3948},
url = {https://doi.org/10.1023/A:1018682001890},
doi = {10.1023/A:1018682001890},
abstract = {Any project, programme or organisation, working in any environment with released information, managing its change, maintaining traceability, and ensuring results always meet expectations, needs configuration management. Software projects introduce additional complexities — multiple developers working on the same item at the same time, the need for compatibility with other products and systems, targeting releases for multiple platforms, and supporting multiple versions (for example development and released versions). This paper describes the configuration management processes that support and manage products through their entire life cycle as they change and evolve.},
journal = {BT Technology Journal},
month = jul,
pages = {48–60},
numpages = {13}
}

@inproceedings{10.1145/3109729.3109734,
author = {Marc\'{e}n, Ana C. and Font, Jaime and Pastor, \'{O}scar and Cetina, Carlos},
title = {Towards Feature Location in Models through a Learning to Rank Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109734},
doi = {10.1145/3109729.3109734},
abstract = {In this work, we propose a feature location approach to discover software artifacts that implement the feature functionality in a model. Given a model and a feature description, model fragments extracted from the model and the feature description are encoded based on a domain ontology. Then, a Learning to Rank algorithm is used to train a classifier that is based on the model fragments and feature description encoded. Finally, the classifier assesses the similarity between a population of model fragments and the target feature being located to find the set of most suitable feature realizations. We have evaluated the approach with an industrial case study, locating features with mean precision and recall values of around 73.75% and 73.31%, respectively (the sanity check obtains less than 35%).},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {recommender systems, runtime decision-making, self-configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.procs.2019.12.173,
author = {Chemingui, Houssem and Gam, Ines and Mazo, Ra\'{u}l and Salinesi, Camille and Ghezala, Henda Ben},
title = {Product Line Configuration Meets Process Mining},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {164},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.173},
doi = {10.1016/j.procs.2019.12.173},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {199–210},
numpages = {12},
keywords = {Product line engineering, configuration process, process mining, enhancing, configuration difficulties}
}

@inproceedings{10.1145/3168365.3168378,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Towards the Extraction of Variability Information to Assist Variability Modelling of Complex Product Lines},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168378},
doi = {10.1145/3168365.3168378},
abstract = {Software product line engineering gathers a set of methods that rely on systematic reuse and mass customisation to reduce the development time and cost of a set of similar software systems. Boolean feature models are the de facto standard used to represent product line variability in terms of features, a feature being a distinguishable characteristic of one or several softwares. The extractive adoption of a product line from a set of individually developed softwares requires to extract variability information from a collection of software descriptions to model their variability. With the appearance of more and more complex software systems, software product line engineering faces new challenges including variability extraction and modelling. Extensions of boolean feature models, as multi-valued attributes or UML-like cardinalities have since been proposed to support variability modelling in complex product lines. In this paper, we propose research directions to address the issue of extracting more complex variability information, as a part of extended feature models synthesis from software descriptions. We consider the capabilities of Formal Concept Analysis, a mathematical framework for knowledge discovery, along with two of its extensions called Pattern Structures and Relational Concept Analysis, to answer this problematic. These frameworks bring theoretical foundations to complex variability extraction algorithms.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {113–120},
numpages = {8},
keywords = {Reverse Engineering, Software Product Line, Variability Extraction},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3336294.3336297,
author = {Munoz, Daniel-Jesus and Oh, Jeho and Pinto, M\'{o}nica and Fuentes, Lidia and Batory, Don},
title = {Uniform Random Sampling Product Configurations of Feature Models That Have Numerical Features},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336297},
doi = {10.1145/3336294.3336297},
abstract = {Analyses of Software Product Lines (SPLs) rely on automated solvers to navigate complex dependencies among features and find legal configurations. Often these analyses do not support numerical features with constraints because propositional formulas use only Boolean variables. Some automated solvers can represent numerical features natively, but are limited in their ability to count and Uniform Random Sample (URS) configurations, which are key operations to derive unbiased statistics on configuration spaces.Bit-blasting is a technique to encode numerical constraints as propositional formulas. We use bit-blasting to encode Boolean and numerical constraints so that we can exploit existing #SAT solvers to count and URS configurations. Compared to state-of-art Satisfiability Modulo Theory and Constraint Programming solvers, our approach has two advantages: 1) faster and more scalable configuration counting and 2) reliable URS of SPL configurations. We also show that our work can be used to extend prior SAT-based SPL analyses to support numerical features and constraints.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {289–301},
numpages = {13},
keywords = {bit-blasting, feature model, model counting, numerical features, propositional formula, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/587051.587067,
author = {Chu-Carroll, Mark C. and Wright, James and Shields, David},
title = {Supporting aggregation in fine grained software configuration management},
year = {2002},
isbn = {1581135149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/587051.587067},
doi = {10.1145/587051.587067},
abstract = {Fine-grained software configuration management offers substantial benefits for large-scale collaborative software development, enabling a variety of interesting and useful features including complexity management, support for aspect-oriented software development, and support for communication and coordination within software engineering teams, as described in [4]. However, fine granularity by itself is not sufficient to achieve these benefits. Most of the benefits of fine granularity result from the ability to combine fine-grained artifacts in various ways: supporting multiple overlapping organizations of program source by combining fine-grained artifacts into virtual source files (VSFs); supporting coordination by allowing developers to precisely mark the set of artifacts affected by a change; associating products from different phases of the development process; etc.In this paper, we describe how a general aggregation mechanism can be used to support the various functionality enabled by fine grained SCM. We present a set of requirements that an aggregation facility must provide in order to yield these benefits, and we provide a description of the implementation of such an aggregation system in our experimental SCM system.},
booktitle = {Proceedings of the 10th ACM SIGSOFT Symposium on Foundations of Software Engineering},
pages = {99–108},
numpages = {10},
keywords = {aggregation, dynamic program organization, fine grained storage},
location = {Charleston, South Carolina, USA},
series = {SIGSOFT '02/FSE-10}
}

@inproceedings{10.1145/2791060.2791093,
author = {Souto, Sabrina and Gopinath, Divya and d'Amorim, Marcelo and Marinov, Darko and Khurshid, Sarfraz and Batory, Don},
title = {Faster bug detection for software product lines with incomplete feature models},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791093},
doi = {10.1145/2791060.2791093},
abstract = {A software product line (SPL) is a family of programs that are differentiated by features --- increments in functionality. Systematically testing an SPL is challenging because it requires running each test of a test suite against a combinatorial number of programs. Feature models capture dependencies among features and can (1) reduce the space of programs to test and (2) enable accurate categorization of failing tests as failures of programs or the tests themselves, not as failures due to illegal combinations of features. In practice, sadly, feature models are not always available.We introduce SPLif, the first approach for testing SPLs that does not require the a priori availability of feature models. Our insight is to use a profile of passing and failing test runs to quickly identify failures that are indicative of real problems in test or code rather than specious failures due to illegal feature combinations.Experimental results on five SPLs and one large configurable system (GCC) demonstrate the effectiveness of our approach. SPLif enabled the discovery of five news bugs in GCC, three of which have already been fixed.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {151–160},
numpages = {10},
keywords = {GCC, feature models, software testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/3369393,
author = {Ding, Yuhang and Fan, Hehe and Xu, Mingliang and Yang, Yi},
title = {Adaptive Exploration for Unsupervised Person Re-identification},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3369393},
doi = {10.1145/3369393},
abstract = {Due to domain bias, directly deploying a deep person re-identification (re-ID) model trained on one dataset often achieves considerably poor accuracy on another dataset. In this article, we propose an Adaptive Exploration (AE) method to address the domain-shift problem for re-ID in an unsupervised manner. Specifically, in the target domain, the re-ID model is inducted to (1) maximize distances between all person images and (2) minimize distances between similar person images. In the first case, by treating each person image as an individual class, a non-parametric classifier with a feature memory is exploited to encourage person images to move far away from each other. In the second case, according to a similarity threshold, our method adaptively selects neighborhoods for each person image in the feature space. By treating these similar person images as the same class, the non-parametric classifier forces them to stay closer. However, a problem of the adaptive selection is that, when an image has too many neighborhoods, it is more likely to attract other images as its neighborhoods. As a result, a minority of images may select a large number of neighborhoods while a majority of images has only a few neighborhoods. To address this issue, we additionally integrate a balance strategy into the adaptive selection. We evaluate our methods with two protocols. The first one is called “target-only re-ID”, in which only the unlabeled target data is used for training. The second one is called “domain adaptive re-ID”, in which both the source data and the target data are used during training. Experimental results on large-scale re-ID datasets demonstrate the effectiveness of our method. Our code has been released at https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {3},
numpages = {19},
keywords = {Person re-identification, deep learning, domain adaptation, unsupervised learning}
}

@inproceedings{10.5555/1884371.1884417,
author = {S\o{}gaard, Anders and Rish\o{}j, Christian},
title = {The effect of semi-supervised learning on parsing long distance dependencies in German and Swedish},
year = {2010},
isbn = {3642147690},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper shows how the best data-driven dependency parsers available today [1] can be improved by learning from unlabeled data. We focus on German and Swedish and show that labeled attachment scores improve by 1.5%-2.5%. Error analysis shows that improvements are primarily due to better recovery of long distance dependencies.},
booktitle = {Proceedings of the 7th International Conference on Advances in Natural Language Processing},
pages = {406–417},
numpages = {12},
keywords = {dependency parsing, long distance dependencies, semi-supervised learning},
location = {Reykjavik, Iceland},
series = {IceTAL'10}
}

@inproceedings{10.1145/1858996.1859064,
author = {Boucher, Quentin and Classen, Andreas and Heymans, Patrick and Bourdoux, Arnaud and Demonceau, Laurent},
title = {Tag and prune: a pragmatic approach to software product line implementation},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859064},
doi = {10.1145/1858996.1859064},
abstract = {To realise variability at the code level, product line methods classically advocate usage of inheritance, components, frameworks, aspects or generative techniques. However, these might require unaffordable paradigm shifts for the developers if the software was not thought at the outset as a product line. Furthermore, these techniques can be conflicting with a company's coding practices or external regulations.These concerns were the motivation for the industry-university collaboration described in this paper where we develop a minimally intrusive coding technique based on tags. It is supported by a toolchain and is now in use in the partner company for the development of flight grade satellite communication software libraries.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {333–336},
numpages = {4},
keywords = {code tagging, feature diagram},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1609/aaai.v33i01.33015725,
author = {Zhang, Biqiao and Kong, Yuqing and Essl, Georg and Provost, Emily Mower},
title = {undefined-similarity preservation loss for soft labels: a demonstration on cross-corpus speech emotion recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015725},
doi = {10.1609/aaai.v33i01.33015725},
abstract = {In this paper, we propose a Deep Metric Learning (DML) approach that supports soft labels. DML seeks to learn representations that encode the similarity between examples through deep neural networks. DML generally presupposes that data can be divided into discrete classes using hard labels. However, some tasks, such as our exemplary domain of speech emotion recognition (SER), work with inherently subjective data, data for which it may not be possible to identify a single hard label. We propose a family of loss functions, undefined-Similarity Preservation Loss (undefined-SPL), based on the dual form of undefined-divergence for DML with soft labels. We show that the minimizer of undefined-SPL preserves the pairwise label similarities in the learned feature embeddings. We demonstrate the efficacy of the proposed loss function on the task of cross-corpus SER with soft labels. Our approach, which combines undefined-SPL and classification loss, significantly outperforms a baseline SER system with the same structure but trained with only classification loss in most experiments. We show that the presented techniques are more robust to over-training and can learn an embedding space in which the similarity between examples is meaningful.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {702},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/1540438.1540455,
author = {Bruegge, Bernd and David, Joern and Helming, Jonas and Koegel, Maximilian},
title = {Classification of tasks using machine learning},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540455},
doi = {10.1145/1540438.1540455},
abstract = {Categorizing software engineering artifacts, tasks in our case, is often a prerequisite for analysis and research. As an example, categorizing tasks according to their activity allows for a post-mortem analysis of the life cycle model of a project and can be used as a foundation for software metrics. Many categorical attributes of software artifacts are often not entered correctly or are not entered at all. For example, we observed a significant number of obsolete tasks that were not categorized as such.In this paper, we present an approach for the automatic classification of tasks in software development projects using machine learning. We evaluated our technique by two sample applications from the domain of project management: Tasks are classified according to activity and relevance, respectively. Project-relevant characteristics are learned by the classifier from the project history. Five-fold cross-validation of both applications resulted in classification accuracies of 80.51% (six categories) and 83.72% (two categories). Our approach is also applicable to other types of artifacts and categorizations within a unified software engineering model.},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {12},
numpages = {11},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@inproceedings{10.1145/2600428.2609601,
author = {Cormack, Gordon V. and Grossman, Maura R.},
title = {Evaluation of machine-learning protocols for technology-assisted review in electronic discovery},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609601},
doi = {10.1145/2600428.2609601},
abstract = {Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P&lt;0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P&lt;0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of "stabilization" -- determining when training is adequate, and therefore may stop.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {153–162},
numpages = {10},
keywords = {e-discovery, electronic discovery, predictive coding, technology-assisted review},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@article{10.1145/3243316,
author = {Fan, Hehe and Zheng, Liang and Yan, Chenggang and Yang, Yi},
title = {Unsupervised Person Re-identification: Clustering and Fine-tuning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3243316},
doi = {10.1145/3243316},
abstract = {The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {83},
numpages = {18},
keywords = {Large-scale person re-identification, clustering, convolutional neural network, unsupervised learning}
}

@inproceedings{10.1145/2019136.2019150,
author = {Serajzadeh, Hadi and Shams, Fereidoon},
title = {The application of swarm intelligence in service-oriented product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019150},
doi = {10.1145/2019136.2019150},
abstract = {Changing markets and environments has made the ability to rapidly adapt to these changes a necessity in software systems. However the costs of changing and adapting systems to new requirements still remains an unsolved issue. In this context service-oriented software product lines were introduced with the aim to combine the reusability of software product line with the flexibility of service-oriented architecture. Although this approach helps build flexible software systems with high levels of reuse, certain issues are raised. The main issue is the complexity that a service-oriented product line will face. Developing systems from internal and external assets, taking into consideration the variety and number of these assets, can cause problems in deciding which asset is best suited for the system. To help solve these issues we propose the use of approaches based on artificial intelligence. In this paper we show how swarm intelligence can be used in service-oriented product lines to reduce complexity and find optimal solutions for the development of software systems. We also present an example of the application of swarm intelligence in finding the optimal product for a service-oriented product line.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {12},
numpages = {7},
keywords = {optimization, service-oriented product line, swarm intelligence},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.dsp.2021.103205,
author = {Pourebrahim, Yousef and Razzazi, Farbod and Sameti, Hossein},
title = {Semi-supervised parallel shared encoders for speech emotion recognition},
year = {2021},
issue_date = {Nov 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {118},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103205},
doi = {10.1016/j.dsp.2021.103205},
journal = {Digit. Signal Process.},
month = nov,
numpages = {11},
keywords = {Semi-supervised learning, Speech emotion recognition, Domain adaptation, Deep neural networks}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Feature Model Validation, Machine Learning, Natural Language Processing, Requirements Engineering, Software Product Line},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/3109729.3109745,
author = {Markiegi, Urtzi},
title = {Test optimisation for Highly-Configurable Cyber-Physical Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109745},
doi = {10.1145/3109729.3109745},
abstract = {Cyber-Physical Systems (CPS) have become one of the core-enabling technologies for multiple domains, such as manufacturing, healthcare, energy and transportation. Furthermore, these domains are demanding CPS to be highly-configurable in order to respond to multiple and changing market requirements. Testing these Highly-Configurable Cyber-Physical Systems (HCCPS) is challenging. First, when working with CPSs, considerable time is required in order to tackle physical processes during testing. And secondly, in highly-configurable systems, a large number of system variants need to be tested. Consequently, reducing HCCPS testing time is essential.In this context, a research work is presented to reduce the overall testing time of HCCPS, focusing on a merged strategy of product and test cases optimisation. In particular, two approaches are proposed in order to achieve the testing time reduction. The first approach aims to reduce the HCCPS testing time by an iterative allocation of products and test cases. The second approach aims to reduce the HCCPS testing time by a feedback driven dynamic and iterative allocation of products and test cases.A preliminary experiment has been undertaken to test the iterative allocation approach. In this experiment, products to be tested are selected and prioritised. Next, multiple testing iterations are perform until the time-budget is consumed. In each iteration a small number of test cases are allocated for each of the products to be tested. The experiment was evaluated with an academic HCCPS and preliminary results suggest that the proposed approach reduces the fault detection time when compared with traditional approaches.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {139–144},
numpages = {6},
keywords = {Cyber-Physical Systems, Fault Detection, Highly-Configurable Systems, Product Line Testing, Search-Based Software Engineering, Software Engineering},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/1964138.1964143,
author = {Hamza, Haitham S. and Aly, Gamal M.},
title = {Using product line architectures to leverage systematic reuse of business knowledge: an industrial experience},
year = {2010},
isbn = {9781450305426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964138.1964143},
doi = {10.1145/1964138.1964143},
abstract = {Software Product Line Engineering (PLE) exploits systematic reuse by identifying and methodically reusing software artifacts to develop different but related software systems. Developing product lines requires analysis skills to identify, model, and encode domain and product knowledge into artifacts that can be systematically reused across the development life-cycle. As such, knowledge plays a paramount role in the success of the various activities of PLE. This paper investigates the role of PLE in identifying and codifying tacit business knowledge in two industrial case studies in the domain of Enterprise Resource Planning (ERP) systems.},
booktitle = {Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering},
articleno = {5},
numpages = {5},
keywords = {enterprise resource planning (ERP) systems, product-line engineering, software product lines, systematic reuse},
location = {Reno, Nevada},
series = {KOPLE '10}
}

@inproceedings{10.1145/2934466.2934469,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof and Yu, Huiqun},
title = {A mathematical model of performance-relevant feature interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934469},
doi = {10.1145/2934466.2934469},
abstract = {Modern software systems have grown significantly in their size and complexity, therefore understanding how software systems behave when there are many configuration options, also called features, is no longer a trivial task. This is primarily due to the potentially complex interactions among the features. In this paper, we propose a novel mathematical model for performance-relevant, or quantitative in general, feature interactions, based on the theory of Boolean functions. Moreover, we provide two algorithms for detecting all such interactions with little measurement effort and potentially guaranteed accuracy and confidence level. Empirical results on real-world configurable systems demonstrated the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {25–34},
numpages = {10},
keywords = {boolean functions, feature interactions, fourier transform, performance},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1145/302342.1096842,
author = {Bielikova, Maria},
title = {Book review: Constraint-Based Reasoning by E.C. Freuder and A.K. Mackworth (Eds.) (The MIT Press Cambridge, MA, 1994 )},
year = {1998},
issue_date = {Dec. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3–4},
issn = {0163-5719},
url = {https://doi.org/10.1145/302342.1096842},
doi = {10.1145/302342.1096842},
abstract = {This book is reprinted from Artificial Intelligence: An International Journal, Volume 58, Numbers 1--3, 1992. There are reported new results in constraint-based reasoning. Constraint-based reasoning has a long tradition in artificial intelligence. Many problems to which artificial intelligence techniques are often applied can be described as constraint satisfaction problems.},
journal = {SIGART Bull.},
month = dec,
pages = {39–41},
numpages = {3}
}

@article{10.1016/j.neunet.2021.03.022,
author = {Zhong, Yongjian and Du, Bo and Xu, Chang},
title = {Learning to reweight examples in multi-label classification},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.03.022},
doi = {10.1016/j.neunet.2021.03.022},
journal = {Neural Netw.},
month = oct,
pages = {428–436},
numpages = {9},
keywords = {Multi-label classification, Self-paced learning, Reweight instance}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {knowledge sharing, online learning, product-line management, self-adaptation, software design, software product-lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.patcog.2021.108164,
author = {Yang, Zhaohui and Shi, Miaojing and Xu, Chao and Ferrari, Vittorio and Avrithis, Yannis},
title = {Training object detectors from few weakly-labeled and many unlabeled images},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108164},
doi = {10.1016/j.patcog.2021.108164},
journal = {Pattern Recogn.},
month = dec,
numpages = {10},
keywords = {Object detection, Weakly-supervised learning, Semi-supervised learning, Unlabelled set}
}

@inproceedings{10.1145/111062.111082,
author = {Lubkin, David},
title = {Heterogeneous configuration management with DSEE},
year = {1991},
isbn = {0897914295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/111062.111082},
doi = {10.1145/111062.111082},
booktitle = {Proceedings of the 3rd International Workshop on Software Configuration Management},
pages = {153–160},
numpages = {8},
location = {Trondheim, Norway},
series = {SCM '91}
}

@inproceedings{10.1145/2362536.2362554,
author = {Martini, Antonio and Pareto, Lars and Bosch, Jan},
title = {Enablers and inhibitors for speed with reuse},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362554},
doi = {10.1145/2362536.2362554},
abstract = {An open issue in industry is software reuse in the context of large scale Agile product development. The speed offered by agile practices is needed to hit the market, while reuse is needed for long-term productivity, efficiency, and profit. The paper presents an empirical investigation of factors influencing speed and reuse in three large product developing organizations seeking to implement Agile practices. The paper identifies, through a multiple case study with 3 organizations, 114 business-, process-, organizational-, architecture-, knowledge- and communication factors with positive or negative influences on reuse, speed or both. Contributions are a categorized inventory of influencing factors, a display for organizing factors for the purpose of process improvement work, and a list of key improvement areas to address when implementing reuse in organizations striving to become more Agile. Categories identified include good factors with positive influences on reuse or speed, harmful factors with negative influences, and complex factors involving inverse or ambiguous relationships. Key improvement areas in the studied organizations are intra-organizational communication practices, reuse awareness and practices, architectural integration and variability management. Results are intended to support process improvement work in the direction of Agile product development. Feedback on results from the studied organizations has been that the inventory captures current situations, and is useful for software process improvement work.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {116–125},
numpages = {10},
keywords = {agile software development, embedded systems, enablers, inhibitors, software process improvement (SPI), software reuse, speed},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.5555/186999.187010,
author = {Dahl, Veronica and Sidebottom, Greg and Ueberla, Joerg},
title = {Automatic configuration through constraint based reasoning},
year = {1993},
issue_date = {1993},
publisher = {JAI Press, Inc.},
address = {USA},
volume = {6},
number = {4},
issn = {0894-9077},
journal = {Int. J. Expert Syst.},
month = apr,
pages = {561–579},
numpages = {19}
}

@inproceedings{10.1145/67312.67350,
author = {Fidelak, M. and Guesgen, H. W. and Voss, H.},
title = {Temporal aspects in constraint-based reasoning},
year = {1989},
isbn = {0897913205},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67312.67350},
doi = {10.1145/67312.67350},
booktitle = {Proceedings of the 2nd International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 2},
pages = {794–802},
numpages = {9},
location = {Tullahoma, Tennessee, USA},
series = {IEA/AIE '89}
}

@inproceedings{10.1007/978-3-030-87196-3_28,
author = {Wu, Yicheng and Xu, Minfeng and Ge, Zongyuan and Cai, Jianfei and Zhang, Lei},
title = {Semi-supervised Left Atrium Segmentation with Mutual Consistency&nbsp;Training},
year = {2021},
isbn = {978-3-030-87195-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87196-3_28},
doi = {10.1007/978-3-030-87196-3_28},
abstract = {Semi-supervised learning has attracted great attention in the field of machine learning, especially for medical image segmentation tasks, since it alleviates the heavy burden of collecting abundant densely annotated data for training. However, most of existing methods underestimate the importance of challenging regions (e.g. small branches or blurred edges) during training. We believe that these unlabeled regions may contain more crucial information to minimize the uncertainty prediction for the model and should be emphasized in the training process. Therefore, in this paper, we propose a novel Mutual Consistency Network (MC-Net) for semi-supervised left atrium segmentation from 3D MR images. Particularly, our MC-Net consists of one encoder and two slightly different decoders, and the prediction discrepancies of two decoders are transformed as an unsupervised loss by our designed cycled pseudo label scheme to encourage mutual consistency. Such mutual consistency encourages the two decoders to have consistent and low-entropy predictions and enables the model to gradually capture generalized features from these unlabeled challenging regions. We evaluate our MC-Net on the public Left Atrium (LA) database and it obtains impressive performance gains by exploiting the unlabeled data effectively. Our MC-Net outperforms six recent semi-supervised methods for left atrium segmentation, and sets the new state-of-the-art performance on the LA database.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II},
pages = {297–306},
numpages = {10},
keywords = {Semi-supervised learning, Mutual consistency, Cycled pseudo label},
location = {Strasbourg, France}
}

@article{10.1609/aimag.v14i3.1061,
author = {Murtagh, Niall},
title = {Engineering Design through Constraint‐Based Reasoning},
year = {1993},
issue_date = {September 1993},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {14},
number = {3},
issn = {0738-4602},
url = {https://doi.org/10.1609/aimag.v14i3.1061},
doi = {10.1609/aimag.v14i3.1061},
journal = {AI Mag.},
month = sep,
pages = {100–102},
numpages = {3}
}

@inproceedings{10.1145/2791060.2791068,
author = {B\'{e}can, Guillaume and Behjati, Razieh and Gotlieb, Arnaud and Acher, Mathieu},
title = {Synthesis of attributed feature models from product descriptions},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791068},
doi = {10.1145/2791060.2791068},
abstract = {Many real-world product lines are only represented as nonhierarchical collections of distinct products, described by their configuration values. As the manual preparation of feature models is a tedious and labour-intensive activity, some techniques have been proposed to automatically generate boolean feature models from product descriptions. However, none of these techniques is capable of synthesizing feature attributes and relations among attributes, despite the huge relevance of attributes for documenting software product lines. In this paper, we introduce for the first time an algorithmic and parametrizable approach for computing a legal and appropriate hierarchy of features, including feature groups, typed feature attributes, domain values and relations among these attributes. We have performed an empirical evaluation by using both randomized configuration matrices and real-world examples. The initial results of our evaluation show that our approach can scale up to matrices containing 2,000 attributed features, and 200,000 distinct configurations in a couple of minutes.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {1–10},
numpages = {10},
keywords = {attributed feature models, product descriptions},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1007/978-3-030-58577-8_17,
author = {Pan, Lili and Ai, Shijie and Ren, Yazhou and Xu, Zenglin},
title = {Self-Paced Deep Regression Forests with Consideration on Underrepresented Examples},
year = {2020},
isbn = {978-3-030-58576-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58577-8_17},
doi = {10.1007/978-3-030-58577-8_17},
abstract = {Deep discriminative models (e.g.&nbsp;deep regression forests, deep neural decision forests) have achieved remarkable success recently to solve problems such as facial age estimation and head pose estimation. Most existing methods pursue robust and unbiased solutions either through learning discriminative features, or reweighting samples. We argue what is more desirable is learning gradually to discriminate like our human beings, and hence we resort to self-paced learning (SPL). Then, a natural question arises: can self-paced regime lead deep discriminative models to achieve more robust and less biased solutions? To this end, this paper proposes a new deep discriminative model—self-paced deep regression forests with consideration on underrepresented examples (SPUDRFs). It tackles the fundamental ranking and selecting problem in SPL from a new perspective: fairness. This paradigm is fundamental and could be easily combined with a variety of deep discriminative models (DDMs). Extensive experiments on two computer vision tasks, i.e., facial age estimation and head pose estimation, demonstrate the efficacy of SPUDRFs, where state-of-the-art performances are achieved.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX},
pages = {271–287},
numpages = {17},
keywords = {Underrepresented examples, Self-paced learning, Entropy, Deep regression forests},
location = {Glasgow, United Kingdom}
}

@article{10.5555/3546258.3546440,
author = {Klink, Pascal and Abdulsamad, Hany and Belousov, Boris and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {A probabilistic interpretation of self-paced learning with applications to reinforcement learning},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {182},
numpages = {52},
keywords = {curriculum learning, reinforcement learning, self-paced learning, tempered inference, rl-as-inference}
}

@inproceedings{10.1007/978-3-030-58545-7_45,
author = {Li, Junbing and Zhang, Changqing and Zhu, Pengfei and Wu, Baoyuan and Chen, Lei and Hu, Qinghua},
title = {SPL-MLL: Selecting Predictable Landmarks for Multi-label Learning},
year = {2020},
isbn = {978-3-030-58544-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58545-7_45},
doi = {10.1007/978-3-030-58545-7_45},
abstract = {Although significant progress achieved, multi-label classification is still challenging due to the complexity of correlations among different labels. Furthermore, modeling the relationships between input and some (dull) classes further increases the difficulty of accurately predicting all possible labels. In this work, we propose to select a small subset of labels as landmarks which are easy to predict according to input (predictable) and can well recover the other possible labels (representative). Different from existing methods which separate the landmark selection and landmark prediction in the 2-step manner, the proposed algorithm, termed Selecting Predictable Landmarks for Multi-Label Learning (SPL-MLL), jointly conducts landmark selection, landmark prediction, and label recovery in a unified framework, to ensure both the representativeness and predictableness for selected landmarks. We employ the Alternating Direction Method (ADM) to solve our problem. Empirical studies on real-world datasets show that our method achieves superior classification performance over other state-of-the-art methods.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX},
pages = {783–799},
numpages = {17},
keywords = {Multi-label learning, Predictable landmarks, A unified framework},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/72910.73353,
author = {Thomson, R. and Sommerville, I.},
title = {Configuration management using SySL},
year = {1989},
isbn = {0897913345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/72910.73353},
doi = {10.1145/72910.73353},
booktitle = {Proceedings of the 2nd International Workshop on Software Configuration Management},
pages = {106–109},
numpages = {4},
location = {Princeton, New Jersey, USA},
series = {SCM '89}
}

@inproceedings{10.5555/3504035.3505007,
author = {Yaman, Fusun and Adler, Aaron and Beal, Jacob},
title = {AI challenges in synthetic biology engineering},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {A wide variety of Artificial Intelligence (AI) techniques, from expert systems to machine learning to robotics, are needed in the field of synthetic biology. This paper describes the design-build-test engineering cycle and lists some challenges in which AI can help.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {972},
numpages = {2},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1007/s10664-015-9401-9,
author = {Jonsson, Leif and Borg, Markus and Broman, David and Sandahl, Kristian and Eldh, Sigrid and Runeson, Per},
title = {Automated bug assignment: Ensemble-based machine learning in large scale industrial contexts},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9401-9},
doi = {10.1007/s10664-015-9401-9},
abstract = {Bug report assignment is an important part of software maintenance. In particular, incorrect assignments of bug reports to development teams can be very expensive in large software development projects. Several studies propose automating bug assignment techniques using machine learning in open source software contexts, but no study exists for large-scale proprietary projects in industry. The goal of this study is to evaluate automated bug assignment techniques that are based on machine learning classification. In particular, we study the state-of-the-art ensemble learner Stacked Generalization (SG) that combines several classifiers. We collect more than 50,000 bug reports from five development projects from two companies in different domains. We implement automated bug assignment and evaluate the performance in a set of controlled experiments. We show that SG scales to large scale industrial application and that it outperforms the use of individual classifiers for bug assignment, reaching prediction accuracies from 50 % to 89 % when large training sets are used. In addition, we show how old training data can decrease the prediction accuracy of bug assignment. We advice industry to use SG for bug assignment in proprietary contexts, using at least 2,000 bug reports for training. Finally, we highlight the importance of not solely relying on results from cross-validation when evaluating automated bug assignment.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1533–1578},
numpages = {46},
keywords = {Bug assignment, Bug reports, Classification, Ensemble learning, Industrial scale; Large scale, Machine learning}
}

@inproceedings{10.1145/2791060.2793677,
author = {D\"{u}dder, Boris and Rehof, Jakob and Heineman, George T.},
title = {Synthesizing type-safe compositions in feature oriented software designs using staged composition},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2793677},
doi = {10.1145/2791060.2793677},
abstract = {The composition of features that interact with each other is challenging. Algebraic formalisms have been proposed by various authors to describe feature compositions and their interactions. The intention of feature compositions is the composition of fragments of documents of any kind to a product that fulfills users' requirements expressed by a feature selection. These modules often include code modules of typed programming languages whereas the proposed algebraic formalism is agnostic to types. This situation can lead to product code which is not type correct. In addition, types can carry semantic information on a program or module. We present a type system and connect it to an algebraic formalism thereby allowing automatic synthesis of feature compositions yielding well-typed programs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {398–401},
numpages = {4},
keywords = {automatic program synthesis, combinatory logic, feature composition, type theory},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.5555/1865499.1865575,
author = {Murtagh, Niall and Shimura, Masamichi},
title = {Parametric engineering design using constraint-based reasoning},
year = {1990},
isbn = {026251057X},
publisher = {AAAI Press},
abstract = {Conventional methods for the parametric design of engineering structures rely on the iterative re-use of analysis programs in order to converge on a satisfactory solution. Since finite element and other analysis programs require considerable computer resources, this research proposes a general method to minimize their use, by utilizing constraint-based reasoning to carry out redesign. A problem-solver, consisting of constraint networks which express basic relationships between individual design parameters and variables, is attached to the analysis programs. Once an initial design description has been set out using the conventional analysis programs, the networks can then reason about required adjustments in order to find a consistent set of parameter values. We describe how global constraints representing standard design behavioral equations are decomposed to form binary constraint networks. The networks use approximate reasoning to determine dependencies between key parameters, and after an adjustment has been made, use exact relationship information to update only those parts of the design description that are affected by the adjustment. We illustrate the ideas by taking as an example the design of a continuous prestressed concrete beam.},
booktitle = {Proceedings of the Eighth National Conference on Artificial Intelligence - Volume 1},
pages = {505–510},
numpages = {6},
location = {Boston, Massachusetts},
series = {AAAI'90}
}

@inproceedings{10.1007/978-3-030-89370-5_24,
author = {Xu, Lu and Zhong, Xian and Liu, Wenxuan and Zhao, Shilei and Yang, Zhengwei and Zhong, Luo},
title = {Subspace Enhancement and Colorization Network for Infrared Video Action Recognition},
year = {2021},
isbn = {978-3-030-89369-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89370-5_24},
doi = {10.1007/978-3-030-89370-5_24},
abstract = {Human action recognition is an essential area of research in the field of computer vision. However, existing methods ignore the essence of infrared image spectral imaging. Compared with the visible modality with all three channels, the infrared modality with approximate single-channel pays more attention to the lightness contrast and loses the channel information. Therefore, we explore channel duplication and tend to investigate more appropriate feature presentations. We propose a subspace enhancement and colorization network (S2ECNet) to recognize infrared video action recognition. Specifically, we apply the subspace enhancement (S2E) module to promote edge contour extraction with subspace. Meanwhile, a subspace colorization (S2C) module is utilized for better completing missing semantic information. What is more, the optical flow provides effective supplements for temporal information. Experiments conducted on the infrared action recognition dataset InfAR demonstrates the competitiveness of the proposed method compared with the state-of-the-arts.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III},
pages = {321–336},
numpages = {16},
keywords = {Infrared video action recognition, Subspace enhancement, Subspace colorization, Optical flow, Feature fusion},
location = {Hanoi, Vietnam}
}

@inproceedings{10.1145/1294261.1294284,
author = {Su, Ya-Yunn and Attariyan, Mona and Flinn, Jason},
title = {AutoBash: improving configuration management with operating system causality analysis},
year = {2007},
isbn = {9781595935915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294261.1294284},
doi = {10.1145/1294261.1294284},
abstract = {AutoBash is a set of interactive tools that helps users and system administrators manage configurations. AutoBash leverages causal tracking support implemented within our modified Linux kernel to understand the inputs (causal dependencies) and outputs (causal effects) of configuration actions. It uses OS-level speculative execution to try possible actions, examine their effects, and roll them back when necessary. AutoBash automates many of the tedious parts of trying to fix a misconfiguration, including searching through possible solutions, testing whether a particular solution fixes a problem, and undoing changes to persistent and transient state when a solution fails. Our results show that AutoBash correctly identifies the solution to several CVS, gcc cross-compiler, and Apache configuration errors. We also show that causal analysis reduces AutoBash's search time by an average of 35% and solution verification time by an average of 70%.},
booktitle = {Proceedings of Twenty-First ACM SIGOPS Symposium on Operating Systems Principles},
pages = {237–250},
numpages = {14},
keywords = {causality, configuration management, speculative execution},
location = {Stevenson, Washington, USA},
series = {SOSP '07}
}

@inproceedings{10.5555/1051937.1051956,
author = {Couch, Alva and Hart, John and Idhaw, Elizabeth G. and Kallas, Dominic},
title = {Seeking Closure in an Open World: A Behavioral Agent Approach to Configuration Management},
year = {2003},
publisher = {USENIX Association},
address = {USA},
abstract = {We present a new model of configuration management based upon a hierarchy of simple communicating autonomous agents. Each of these agents is responsible for a "closure": a domain of "semantic predictability" in which declarative commands to the agent have a simple, persistent, portable, and documented effect upon subsequent observable behavior. Closures are built bottom-up to form a management hierarchy based upon the pre-existing dependencies between subsystems in a complex system. Closure agents decompose configuration management via a modularity of effect and behavior that promises to eventually lead to self-organizing systems driven entirely by behavioral specifications, where a system's configuration is free of details that have no observable effect upon system.},
booktitle = {Proceedings of the 17th USENIX Conference on System Administration},
pages = {125–148},
numpages = {24},
location = {San Diego, CA},
series = {LISA '03}
}

@phdthesis{10.5555/1925641,
author = {Ganapathi, Archana Sulochana},
advisor = {Patterson, David},
title = {Predicting and optimizing system utilization and performance via statistical machine learning},
year = {2009},
isbn = {9781109749519},
publisher = {University of California at Berkeley},
address = {USA},
abstract = {The complexity of modern computer systems makes performance modeling an invaluable resource for guiding crucial decisions such as workload management, configuration management, and resource provisioning. With continually evolving systems, it is difficult to obtain ground truth about system behavior. Moreover, system management policies must adapt to changes in workload and configuration to continue making efficient decisions. Thus, we require data-driven modeling techniques that auto-extract relationships between a system’s input workload, its configuration parameters, and consequent performance. This dissertation argues that statistical machine learning (SML) techniques are a powerful asset to system performance modeling. We present an SML-based methodology that extracts correlations between a workload’s pre-execution characteristics or configuration parameters, and post-execution performance observations. We leverage these correlations for performance prediction and optimization. We present three success stories that validate the usefulness of our methodology on storage and compute based parallel systems. In all three scenarios, we outperform state-of-the-art alternatives. Our results strongly suggest the use of SML-based performance modeling to improve the quality of system management decisions.},
note = {AAI3402606}
}

@article{10.1145/280277.280280,
author = {Conradi, Reidar and Westfechtel, Bernhard},
title = {Version models for software configuration management},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/280277.280280},
doi = {10.1145/280277.280280},
abstract = {After more than 20 years of research and practice in software configuration management (SCM), constructing consistent configurations of versioned software products still remains a challenge. This article focuses on the version models underlying both commercial systems and research prototypes. It provides an overview and classification of different versioning paradigms and defines and relates fundamental concepts such as revisions, variants, configurations, and changes. In particular, we focus on intensional versioning, that is, construction of versions based on configuration rules. Finally, we provide an overview of systems that have had significant impact on the development of the SCM discipline and classify them according to a detailed taxonomy.},
journal = {ACM Comput. Surv.},
month = jun,
pages = {232–282},
numpages = {51},
keywords = {changes, configuration rules, configurations, revisions, variants, versions}
}

@article{10.1016/j.jss.2014.08.034,
author = {Alsawalqah, Hamad I. and Kang, Sungwon and Lee, Jihyun},
title = {A method to optimize the scope of a software product platform based on end-user features},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.08.034},
doi = {10.1016/j.jss.2014.08.034},
abstract = {A novel method to optimize the scope of a software product platform is proposed.The method is supported with a mathematical formulation and an optimization solver.Depending on the input parameters and the objectives, competing scopes can exist.The method shows how trade-off analysis can be performed among competing scopes.The results of the method were validated as "satisfiable" to "very satisfiable". ContextDue to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development. AimThis paper proposes a novel method to find the optimized scope of a software product platform based on end-user features. MethodThe proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives. ResultsIn a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope.},
journal = {J. Syst. Softw.},
month = dec,
pages = {79–106},
numpages = {28},
keywords = {Commonality decision, Product platform scope, Software product line engineering}
}

@inproceedings{10.1145/2499777.2500714,
author = {Huang, Changyun and Kamei, Yasutaka and Yamashita, Kazuhiro and Ubayashi, Naoyasu},
title = {Using alloy to support feature-based DSL construction for mining software repositories},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500714},
doi = {10.1145/2499777.2500714},
abstract = {The Mining Software Repositories (MSR) field reveals knowledge for software development by analyzing data stored in repositories such as source control and bug trace systems. In order to reveal the knowledge, MSR researchers need to perform complicated procedures iteratively. To help the complex work of MSR practitioners, we study the construction of domain specific languages (DSLs) for MSR. We have conducted feature-oriented domain analysis (FODA) on MSR and developed a DSL based on the feature model. In this paper, we expand our previous work and propose to construct not a single DSL but a DSL family. A DSL family consists of a series of DSLs with commonality in their domain but suitable to specific applications of MSR. To readily construct these DSLs, we use Alloy to encode the feature model. Our encoding includes not only the DSL features and their relations but also some composition rules that can be used to generate the syntax of DSLs. Based on this, we can automatically derive the language elements to construct DSLs suitable to specific purposes of MSR.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {86–89},
numpages = {4},
keywords = {DSL, FODA, SPL, mining software repositories},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.5555/645476.654177,
author = {Han, Jiawei},
title = {Constraint-Based Reasoning in Deductive Databases},
year = {1991},
isbn = {0818621389},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the Seventh International Conference on Data Engineering},
pages = {257–265},
numpages = {9}
}

@inproceedings{10.1007/978-3-030-21290-2_42,
author = {Reinhartz-Berger, Iris and Shimshoni, Ilan and Abdal, Aviva},
title = {Behavior-Derived Variability Analysis: Mining Views for Comparison and Evaluation},
year = {2019},
isbn = {978-3-030-21289-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-21290-2_42},
doi = {10.1007/978-3-030-21290-2_42},
abstract = {The large variety of computerized solutions (software and information systems) calls for a systematic approach to their comparison and evaluation. Different methods have been proposed over the years for analyzing the similarity and variability of systems. These methods get artifacts, such as requirements, design models, or code, of different systems (commonly in the same domain), identify and calculate their similarities, and represent the variability in models, such as feature diagrams. Most methods rely on implementation considerations of the input systems and generate outcomes based on predefined, fixed strategies of comparison (referred to as variability views). In this paper, we introduce an approach for mining relevant views for comparison and evaluation, based on the input artifacts. Particularly, we equip SOVA – a Semantic and Ontological Variability Analysis method – with data mining techniques in order to identify relevant views that highlight variability or similarity of the input artifacts (natural language requirement documents). The comparison is done using entropy and Rand index measures. The method and its outcomes are evaluated on a case of three photo sharing applications.},
booktitle = {Advanced Information Systems Engineering: 31st International Conference, CAiSE 2019, Rome, Italy, June 3–7, 2019, Proceedings},
pages = {675–690},
numpages = {16},
keywords = {Software Product Line Engineering, Variability analysis, Requirements specifications, Feature diagrams},
location = {Rome, Italy}
}

@article{10.1007/s42979-021-00541-8,
author = {Saber, Takfarinas and Brevet, David and Botterweck, Goetz and Ventresque, Anthony},
title = {Reparation in Evolutionary Algorithms for Multi-objective Feature Selection in Large Software Product Lines},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {3},
url = {https://doi.org/10.1007/s42979-021-00541-8},
doi = {10.1007/s42979-021-00541-8},
abstract = {Software Product Lines Engineering is the area of software engineering that aims to systematise the modelling, creation and improvement of groups of interconnected software systems by formally expressing possible alternative products in the form of Feature Models. Deriving a software product/system from a feature model is called Feature Configuration. Engineers select the subset of features (software components) from a feature model that suits their needs, while respecting the underlying relationships/constraints of the system–which is challenging on its own. Since there exist several (and often antagonistic) perspectives on which the quality of software could be assessed, the problem is even more challenging as it becomes a multi-objective optimisation problem. Current multi-objective feature selection in software product line approaches (e.g., SATIBEA) combine the scalability of a genetic algorithm (IBEA) with a solution reparation approach based on a SAT solver or one of its derivatives. In this paper, we propose MILPIBEA, a novel hybrid algorithm which combines IBEA with the accuracy of a mixed-integer linear programming (MILP) reparation. We show that the MILP reparation modifies fewer features from the original infeasible solutions than the SAT reparation and in a shorter time. We also demonstrate that MILPIBEA outperforms SATIBEA on average on various multi-objective performance metrics, especially on the largest feature models. The other major challenge in software engineering in general and in software product lines, in particular, is evolution. While the change in software components is common in the software engineering industry, the particular case of multi-objective optimisation of evolving software product lines is not well-tackled yet. We show that MILPIBEA is not only able to better take advantage of the evolution than SATIBEA, but it is also the one that continues to improve the quality of the solutions when SATIBEA stagnates. Overall, IBEA performs better when combined with MILP instead of SAT reparation when optimising the multi-objective feature selection in large and evolving software product lines.},
journal = {SN Comput. Sci.},
month = mar,
numpages = {14},
keywords = {Software product line, Feature selection, Multi-objective optimisation, Evolutionary algorithm, Reparation, Mixed-integer linear programming}
}

@inproceedings{10.1145/3462757.3466101,
author = {McConnell, Devin J. and Zhu, James and Pandya, Sachin and Aguiar, Derek},
title = {Case-level prediction of motion outcomes in civil litigation},
year = {2021},
isbn = {9781450385268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462757.3466101},
doi = {10.1145/3462757.3466101},
abstract = {Lawyers regularly predict court outcomes to make strategic decisions, including when, if at all, to sue or settle, what to argue, and how to reduce their clients' liability risk. Yet, lawyer predictions tend to be poorly calibrated and biased, which exacerbate unjustifiable disparities in civil case outcomes. Current machine learning (ML) approaches for predicting court outcomes are typically constrained to final dispositions or are based on features unavailable in real-time during litigation, like judicial opinions. Here, we present the first ML-based methods to support lawyer and client decision making in real-time for motion filings in civil proceedings. Using the State of Connecticut Judicial Branch administrative data and court case documents, we trained six classifiers to predict motion to strike outcomes in tort and vehicular cases between July 1, 2004 and February 18, 2019. Integrating dense word embeddings from complaint documents, which contain information specific to the claims alleged, with the Judicial Branch data improved classification accuracy across all models. Subsequent models defined using a novel attorney case-entropy feature, dense word embeddings using corpus specific TF-IDF weightings, and algorithmic classification rules yielded the best predictor, Adaboost, with a classification accuracy of 64.4%. An analysis of feature importance weights confirmed the usefulness of incorporating attorney case-entropy and natural language features from complaint documents. Since all features used in model training are available during litigation, these methods will help lawyers make better predictions than they otherwise could given disparities in lawyer and client resources. All ML models, training code, and evaluation scripts are available at https://github.com/aguiarlab/motionpredict.},
booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
pages = {99–108},
numpages = {10},
location = {S\~{a}o Paulo, Brazil},
series = {ICAIL '21}
}

@inproceedings{10.5555/1661445.1661545,
author = {Westphal, Matthias and W\"{o}lfl, Stefan},
title = {Qualitative CSP, finite CSP, and SAT: comparing methods for qualitative constraint-based reasoning},
year = {2009},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Qualitative Spatial and Temporal Reasoning (QSR) is concerned with constraint-based formalisms for representing, and reasoning with, spatial and temporal information over infinite domains. Within the QSR community it has been a widely accepted assumption that genuine qualitative reasoning methods outperform other reasoning methods that are applicable to encodings of qualitative CSP instances. Recently this assumption has been tackled by several authors, who proposed to encode qualitative CSP instances as finite CSP or SAT instances. In this paper we report on the results of a broad empirical study in which we compared the performance of several reasoners on instances from different qualitative formalisms. Our results show that for small-sized qualitative calculi (e.g., Allen's interval algebra and RCC-8) a state-of-theart implementation of QSR methods currently gives the most efficient performance. However, on recently suggested large-size calculi, e.g., OPRA4, finite CSP encodings provide a considerable performance gain. These results confirm a conjecture by Bessi\`{e}re stating that support-based constraint propagation algorithms provide better performance for large-sized qualitative calculi.},
booktitle = {Proceedings of the 21st International Joint Conference on Artificial Intelligence},
pages = {628–633},
numpages = {6},
location = {Pasadena, California, USA},
series = {IJCAI'09}
}

@inproceedings{10.5555/1753235.1753241,
author = {John, Isabel and Eisenbarth, Michael},
title = {A decade of scoping: a survey},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Scoping can be defined as the process of deciding in which parts of an organization's products, features and domains systematic reuse is economically useful. It generally is the first phase in product line engineering. For a decade now scoping has been recognized as a discipline of it's own in product line engineering. So it's time to look at what has been done in scoping in the last years and what is still to be done. In this survey, we identify and characterize existing scoping approaches with the main goal to derive open areas and research questions for further research in scoping. We analyze and compare existing approaches and derive open and partially addressed research questions that can be tackled by researchers in product line engineering in the next years.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {31–40},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.5555/3495724.3496169,
author = {Parvaneh, Amin and Abbasnejad, Ehsan and Teney, Damien and Shi, Javen Qinfeng and van den Hengel, Anton},
title = {Counterfactual vision-and-language navigation: unravelling the unseen},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The task of vision-and-language navigation (VLN) requires an agent to follow text instructions to find its way through simulated household environments. A prominent challenge is to train an agent capable of generalising to new environments at test time, rather than one that simply memorises trajectories and visual details observed during training. We propose a new learning strategy that learns both from observations and generated counterfactual environments. We describe an effective algorithm to generate counterfactual observations on the fly for VLN, as linear combinations of existing environments. Simultaneously, we encourage the agent's actions to remain stable between original and counterfactual environments through our novel training objective – effectively removing spurious features that would otherwise bias the agent. Our experiments show that this technique provides significant improvements in generalisation on benchmarks for Room-to-Room navigation and Embodied Question Answering.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {445},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1007/s10270-020-00803-8,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat and Nie, Kunming},
title = {A framework for automated multi-stage and multi-step product configuration of cyber-physical systems},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00803-8},
doi = {10.1007/s10270-020-00803-8},
abstract = {Product line engineering (PLE) has been employed to large-scale cyber-physical systems (CPSs) to provide customization based on users’ needs. A PLE methodology can be characterized by its support for capturing and managing the abstractions as commonalities and variabilities and the automation of the configuration process for effective selection and customization of reusable artifacts. The automation of a configuration process heavily relies on the captured abstractions and formally specified constraints using a well-defined modeling methodology. Based on the results of our previous work and a thorough literature review, in this paper, we propose a conceptual framework to support multi-stage and multi-step automated product configuration of CPSs, including a comprehensive classification of constraints and a list of automated functionalities of a CPS configuration solution. Such a framework can serve as a guide for researchers and practitioners to evaluate an existing CPS PLE solution or devise a novel CPS PLE solution. To validate the framework, we conducted three real-world case studies. Results show that the framework fulfills all the requirements of the case studies in terms of capturing and managing variabilities and constraints. Results of the literature review indicate that the framework covers all the functionalities concerned by the literature, suggesting that the framework is complete for enabling the maximum automation of configuration in CPS PLE.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {211–265},
numpages = {55},
keywords = {Cyber-physical systems, Product line engineering, Automated configuration, Multi-stage and multi-step configuration process, Constraint classification, Variability modeling, Real-world case studies}
}

@article{10.1016/j.infsof.2019.106241,
author = {Perkusich, Mirko and Chaves e Silva, Lenardo and Costa, Alexandre and Ramos, Felipe and Saraiva, Renata and Freire, Arthur and Dilorenzo, Ednaldo and Dantas, Emanuel and Santos, Danilo and Gorg\^{o}nio, Kyller and Almeida, Hyggo and Perkusich, Angelo},
title = {Intelligent software engineering in the context of agile software development: A systematic literature review},
year = {2020},
issue_date = {Mar 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {119},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106241},
doi = {10.1016/j.infsof.2019.106241},
journal = {Inf. Softw. Technol.},
month = mar,
numpages = {19},
keywords = {Intelligent software engineering, Agile software development, Search-based software engineering, Machine learning, Bayesian networks, Artificial intelligence}
}

@inproceedings{10.1145/3371425.3371432,
author = {Song, Yoojeong and Lee, Jongwoo},
title = {Design of stock price prediction model with various configuration of input features},
year = {2019},
isbn = {9781450376334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371425.3371432},
doi = {10.1145/3371425.3371432},
abstract = {The interest rate of Korea has remained around 1.75 % annually for years. However, the interest rates of developed countries such as the United States of America are 2.25 ~ 2.5%. Korea is very low compared to this, so it is hard to save money on a deposit or installment saving. Therefore, many people want to use stock investment methods to gain high interest rates despite the high risk. Many people are predicting whether stock prices will rise or fall for investments on their subjective opinion. However, in the field of computer engineering, many people try to predict the stock price using artificial neural network, which has been proven to have good performance through many studies. The direction of stock forecasting research using artificial neural networks is very diverse such as model structure, composition of input feature, composition of target vector and so on. In this paper, we design three stock price prediction model with various input features that have specific characteristic. We hypothesized that, for effective stock price prediction through artificial neural networks, using implicit meaning data. We also questioned which of the implicit data would be most predictive. To prove it, we suggest three stock price prediction model. We implemented these three models and experimented to performance evaluation. Through this, we find out what kind of features would be effective for stock price prediction.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing},
articleno = {3},
numpages = {5},
keywords = {artificial neural network, binary feature, input feature configuration, stock prediction, technical analysis},
location = {Sanya, China},
series = {AIIPCC '19}
}

@article{10.5555/3322706.3361993,
author = {Glimsdal, Sondre and Granmo, Ole-Christoffer},
title = {Thompson sampling guided stochastic searching on the line for deceptive environments with applications to root-finding problems},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {The multi-armed bandit problem forms the foundation for solving a wide range of online stochastic optimization problems through a simple, yet effective mechanism. One simply casts the problem as a gambler who repeatedly pulls one out of N slot machine arms, eliciting random rewards. Learning of reward probabilities is then combined with reward maximization, by carefully balancing reward exploration against reward exploitation. In this paper, we address a particularly intriguing variant of the multi-armed bandit problem, referred to as the Stochastic Point Location (SPL) problem. The gambler is here only told whether the optimal arm (point) lies to the "left" or to the "right" of the arm pulled, with the feedback being erroneous with probability 1 - π. This formulation thus targets optimization in continuous action spaces with both informative and deceptive feedback. To tackle this class of problems, we formulate a compact and scalable Bayesian representation of the solution space that simultaneously captures both the location of the optimal arm as well as the probability of receiving correct feedback. We further introduce the accompanying Thompson Sampling guided Stochastic Point Location (TS-SPL) scheme for balancing exploration against exploitation. By learning π, TS-SPL also supports deceptive environments that are lying about the direction of the optimal arm. This, in turn, allows us to address the fundamental Stochastic Root Finding (SRF) problem. Empirical results demonstrate that our scheme deals with both deceptive and informative environments, significantly outperforming competing algorithms both for SRF and SPL.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1910–1933},
numpages = {24},
keywords = {thompson sampling, stochastic point location, searching on the line, probabilistic bisection search, deceptive environment}
}

@inproceedings{10.5555/581506.581527,
author = {Botha, Martin and von Solms, Rossouw and Perry, Kent and Loubser, Edwin and Yamoyany, George},
title = {The utilization of artificial intelligence in a hybrid intrusion detection system},
year = {2002},
isbn = {1581135963},
publisher = {South African Institute for Computer Scientists and Information Technologists},
address = {ZAF},
abstract = {Computer security, and intrusion detection in particular, have become increasingly important in today's business environment, to ensure safe and trusted commerce between business partners as well as effective organisational functioning. Various approaches to intrusion detection are currently being utilized, but unfortunately in practice these approaches are relatively ineffective and inefficient. New means and ways that will minimize these shortcomings must, therefore, continuously be researched and defined. This paper will propose a proactive and dynamic model, based on trend analysis, fuzzy logic and neural networks that could be utilized to minimise and control intrusion in an organisation's computer system.},
booktitle = {Proceedings of the 2002 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists on Enablement through Technology},
pages = {149–155},
numpages = {7},
keywords = {fuzzy logic, intrusion detection, intrusion detection systems, neural network, pattern recognition},
location = {Port Elizabeth, South Africa},
series = {SAICSIT '02}
}

@inproceedings{10.1007/978-3-030-23502-4_14,
author = {Sondur, Sanjeev and Kant, Krishna},
title = {Towards Automated Configuration of Cloud Storage Gateways: A Data Driven Approach},
year = {2019},
isbn = {978-3-030-23501-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-23502-4_14},
doi = {10.1007/978-3-030-23502-4_14},
abstract = {Cloud storage gateways (CSGs) are an essential part of enterprises to take advantage of the scale and flexibility of cloud object store. A CSG provides clients the impression of a locally configured large size block-based storage device, which needs to be mapped to remote cloud storage which is invariably object based. Proper configuration of the cloud storage gateway is extremely challenging because of numerous parameters involved and interactions among them. In this paper, we study this problem for a commercial CSG product that is typical of offerings in the market. We explore how machine learning techniques can be exploited both for the forward problem (i.e. predicting performance from the configuration parameters) and backward problem (i.e. predicting configuration parameter values from the target performance). Based on extensive testing with real world customer workloads, we show that it is possible to achieve excellent prediction accuracy while ensuring that the model is not overfitted to the data.},
booktitle = {Cloud Computing – CLOUD 2019: 12th International Conference, Held as Part of the Services Conference Federation, SCF 2019, San Diego, CA, USA, June 25–30, 2019, Proceedings},
pages = {192–207},
numpages = {16},
keywords = {Machine learning, Configuration management, Performance, Object store, Cloud storage gateway},
location = {San Diego, CA, USA}
}

@article{10.1007/s11219-020-09522-1,
author = {Bhushan, Megha and Negi, Arun and Samant, Piyush and Goel, Shivani and Kumar, Ajay},
title = {A classification and systematic review of product line feature model defects},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09522-1},
doi = {10.1007/s11219-020-09522-1},
abstract = {Product line (PL)-based development is a thriving research area to develop software-intensive systems. Feature models (FMs) facilitate derivation of valid products from a PL by managing commonalities and variabilities among software products. However, the researchers in academia as well as in the industries experience difficulties in quality assessment of FMs. The increasing complexity and size of FMs may lead to defects, which outweigh the benefits of PL. This paper provides a systematic literature review and key research issues related to the FM defects in PL. We derive a typology of FM defects according to their level of importance. The information on defects’ identification and explanations are provided with formalization. Further, corrective explanations are presented which incorporates various techniques used to fix defects with their implementation. This information would help software engineering community by enabling developers or modelers to find the types of defects and their causes and to choose an appropriate technique to fix defects in order to produce defect-free products from FMs, thereby enhancing the overall quality of PL-based development.},
journal = {Software Quality Journal},
month = dec,
pages = {1507–1550},
numpages = {44},
keywords = {Quality, Product line model, Defect, Software product line, Feature model}
}

@inproceedings{10.5555/276478.276534,
author = {Benic, D.},
title = {Artificial intelligence support system for manufacturing planning and control},
year = {1996},
isbn = {3122828087},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the Fourth International Conference on Advanced Manufacturing Systems and Technology},
pages = {283–290},
numpages = {8},
keywords = {production systems, production planing and control, neural networks, intelligent manufacturing, genetic algorithms, constraint-based reasoning},
location = {Udine, Italy},
series = {AMST '96}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Configurable systems, Machine learning, Software product lines, Systematic literature review}
}

@article{10.1145/571681.571689,
author = {Estublier, Jacky and Leblang, David and Clemm, Geoff and Conradi, Reidar and Tichy, Walter and van der Hoek, Andr\'{e} and Wiborg-Weber, Darcy},
title = {Impact of the research community on the field of software configuration management: summary of an impact project report},
year = {2002},
issue_date = {September 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/571681.571689},
doi = {10.1145/571681.571689},
abstract = {Software Configuration Management (SCM) is an important discipline in professional software development and maintenance. The importance of SCM has increased as programs have become larger and more complex and mission/life-critical. This paper discusses the evolution of SCM technology from the early days of software development to present and the impact university and industrial research has had along the way. It also includes a survey of the industrial state-of-the-practice and research directions.The paper published here is not intended to be a definitive assessment. Rather, our intention is to solicit comments and corrections from the community to help refine the work. If you would like to provide further information, please contact the first author. A longer version of this report can be found at http://wwwadele.imag.fr/SCMImpact.pdf.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {31–39},
numpages = {9},
keywords = {software quality, software engineering, software configuration management, industrial impact}
}

@inproceedings{10.1145/3474085.3481541,
author = {Huang, Lianghua and Liu, Yu and Zhou, Xiangzeng and You, Ansheng and Li, Ming and Wang, Bin and Zhang, Yingya and Pan, Pan and Yinghui, Xu},
title = {Once and for All: Self-supervised Multi-modal Co-training on One-billion Videos at Alibaba},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481541},
doi = {10.1145/3474085.3481541},
abstract = {Videos grow to be one of the largest mediums on the Internet. E-commerce platforms like Alibaba need to process millions of video data across multimedia (e.g., visual, audio, image, and text) and on a variety of tasks (e.g., retrieval, tagging, and summary) every day. In this work, we aim to develop a once and for all pretraining technique for diverse modalities and downstream tasks. To achieve this, we make the following contributions: (1) We propose a self-supervised multi-modal co-training framework. It takes cross-modal pseudo-label consistency as the supervision and can jointly learn representations of multiple modalities. (2) We introduce several novel techniques (e.g., sliding-window subset sampling, coarse-to-fine clustering, fast spatial-temporal convolution and parallel data transmission and processing) to optimize the training process, making billion-scale stable training feasible. (3) We construct a large-scale multi-modal dataset consisting of 1.4 billion videos (~0.5 PB) and train our framework on it. The training takes only 4.6 days on an in-house 256 GPUs cluster, and it simultaneously produces pretrained video, audio, image, motion, and text networks. (4) Finetuning from our pretrained models, we obtain significant performance gains and faster convergence on diverse multimedia tasks at Alibaba. Furthermore, we also validate the learned representation on public datasets. Despite the domain gap between our commodity-centric pretraining and the action-centric evaluation data, we show superior results against state-of-the-arts.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1148–1156},
numpages = {9},
keywords = {self-supervised learning, once and for all, multi-modal, co-training},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.5555/307845.308010,
author = {Pagurek, Bernard and Li, Yanrong and Bieszczad, Andrzej and Susilo, Gatot},
title = {Network configuration management in heterogeneous ATM environments},
year = {1999},
isbn = {3540647201},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the Second International Workshop on Intelligent Agents for Telecommunication Applications},
pages = {72–88},
numpages = {17},
location = {Paris, France},
series = {IATA '98}
}

@inproceedings{10.1145/2791060.2791096,
author = {F\'{e}derle, \'{E}dipo Luis and do Nascimento Ferreira, Thiago and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {OPLA-tool: a support tool for search-based product line architecture design},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791096},
doi = {10.1145/2791060.2791096},
abstract = {The Product Line Architecture (PLA) design is a complex task, influenced by many factors such as feature modularization and PLA extensibility, which are usually evaluated according to different metrics. Hence, the PLA design is an optimization problem and problems like that have been successfully solved in the Search-Based Software Engineering (SBSE) area, by using metaheuristics such as Genetic Algorithm. Considering this fact, this paper introduces a tool named OPLA-Tool, conceived to provide computer support to a search-based approach for PLA design. OPLA-Tool implements all the steps necessary to use multi-objective optimization algorithms, including PLA transformations and visualization through a graphical interface. OPLA-Tool receives as input a PLA at the class diagram level, and produces a set of good alternative diagrams in terms of cohesion, feature modularization and reduction of crosscutting concerns.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {370–373},
numpages = {4},
keywords = {search-based software engineering, product line architecture design, multi-objective evolutionary algorithms},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1007/978-3-030-91265-9_11,
author = {Wei, Shaozhi and Mo, Ran and Xiong, Pu and Zhang, Siyuan and Zhao, Yang and Li, Zengyang},
title = {Predicting and Monitoring Bug-Proneness at the Feature Level},
year = {2021},
isbn = {978-3-030-91264-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91265-9_11},
doi = {10.1007/978-3-030-91265-9_11},
abstract = {Enabling quick feature modification and delivery is important for a project’s success. Obtaining early estimates of software features’ bug-proneness is helpful for effectively allocating resources to the bug-prone features requiring further fixes. Researchers have proposed various studies on bug prediction at different granularity levels, such as class level, package level, method level, etc. However, there exists little work building predictive models at the feature level. In this paper, we investigated how to predict bug-prone features and monitor their evolution. More specifically, we first identified a project’s features and their involved files. Next, we collected a suite of code metrics and selected a relevant set of metrics as attributes to be used for six machine learning algorithms to predict bug-prone features. Through our evaluation, we have presented that using the machine learning algorithms with an appropriate set of code metrics, we can build effective models of bug prediction at the feature level. Furthermore, we build regression models to monitor growth trends of bug-prone features, which shows how these features accumulate bug-proneness over time.},
booktitle = {Dependable Software Engineering. Theories, Tools, and Applications: 7th International Symposium, SETTA 2021, Beijing, China, November 25–27, 2021, Proceedings},
pages = {201–218},
numpages = {18},
keywords = {Feature bug prediction, Machine learning, Code metrics},
location = {Beijing, China}
}

@inproceedings{10.1145/3268866.3268889,
author = {Robinson, Carl Peter and Li, Baihua and Meng, Qinggang and Pain, Matthew},
title = {Effectiveness of Surface Electromyography in Pattern Classification for Upper Limb Amputees},
year = {2018},
isbn = {9781450365246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268866.3268889},
doi = {10.1145/3268866.3268889},
abstract = {This study was undertaken to explore 18 time domain (TD) and time-frequency domain (TFD) feature configurations to determine the most discriminative feature sets for classification. Features were extracted from the surface electromyography (sEMG) signal of 17 hand and wrist movements and used to perform a series of classification trials with the random forest classifier. Movement datasets for 11 intact subjects and 9 amputees from the NinaPro online database repository were used. The aim was to identify any optimum configurations that combined features from both domains and whether there was consistency across subject type for any standout features. This work built on our previous research to incorporate the TFD, using a Discrete Wavelet Transform with a Daubechies wavelet. Findings report configurations containing the same features combined from both domains perform best across subject type (TD: root mean square (RMS), waveform length, and slope sign changes; TFD: RMS, standard deviation, and energy). These mixed-domain configurations can yield optimal performance (intact subjects: 90.98%; amputee subjects: 75.16%), but with only limited improvement on single-domain configurations. This suggests there is limited scope in attempting to build a single absolute feature configuration and more focus should be put on enhancing the classification methodology for adaptivity and robustness under actual operating conditions.},
booktitle = {Proceedings of the 2018 International Conference on Artificial Intelligence and Pattern Recognition},
pages = {107–112},
numpages = {6},
keywords = {Surface Electromyography, Myoelectric Control, Machine Learning, Feature Extraction, Classification},
location = {Beijing, China},
series = {AIPR '18}
}

@article{10.1007/s10462-020-09907-5,
author = {Uma Maheswari, S. and Shahina, A. and Nayeemulla Khan, A.},
title = {Understanding Lombard speech: a review of compensation techniques towards improving speech based recognition systems},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {4},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09907-5},
doi = {10.1007/s10462-020-09907-5},
abstract = {Building voice-based Artificial Intelligence (AI) systems that can efficiently interact with humans through speech has become plausible today due to rapid strides in efficient data-driven AI techniques. Such a human–machine voice interaction in real world would often involve a noisy ambience, where humans tend to speak with additional vocal effort than in a quiet ambience, to mitigate the noise-induced suppression of vocal self-feedback. This noise induced change in the vocal effort is called Lombard speech. In order to build intelligent conversational devices that can operate in a noisy ambience, it is imperative to study the characteristics and processing of Lombard speech. Though the progress of research on Lombard speech started several decades ago, it needs to be explored further in the current scenario which is seeing an explosion of voice-driven applications. The system designed to work with normal speech spoken in a quiet ambience fails to provide the same performance in changing environmental contexts. Different contexts lead to different styles of Lombard speech and hence there arises a need for efficient ways of handling variations in speaking styles in noise. The Lombard speech is also more intelligible than normal speech of a speaker. Applications like public announcement systems with speech output interface should talk with varying degrees of vocal effort to enhance naturalness in a way that humans adapt to speak in noise, in real time. This review article is an attempt to summarize the progress of work on the possible ways of processing Lombard speech to build smart and robust human–machine interactive systems with speech input–output interface, irrespective of operating environmental contexts, for different application needs. This article is a comprehensive review of the studies on Lombard speech, highlighting the key differences observed in acoustic and perceptual analysis of Lombard speech and detailing the Lombard effect compensation methods towards improving the robustness of speech based recognition systems.},
journal = {Artif. Intell. Rev.},
month = apr,
pages = {2495–2523},
numpages = {29},
keywords = {Lombard speech synthesis, Lombard effect compensation, Automatic recognition systems, Perceptual analysis, Acoustic analysis, Lombard speech}
}

@inproceedings{10.1145/111062.111067,
author = {Berrada, K. and Lopez, F. and Minot, R.},
title = {VMCM, a PCTE based version and configuration management system},
year = {1991},
isbn = {0897914295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/111062.111067},
doi = {10.1145/111062.111067},
booktitle = {Proceedings of the 3rd International Workshop on Software Configuration Management},
pages = {43–52},
numpages = {10},
location = {Trondheim, Norway},
series = {SCM '91}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Utility Elicitation, Software Product Line, Economic Value, Decision Process, Configuration Process}
}

@inproceedings{10.1007/978-3-030-93046-2_7,
author = {Guo, Jingwen and Lu, Zhisheng and Wang, Ti and Huang, Weibo and Liu, Hong},
title = {Object Goal Visual Navigation Using Semantic Spatial Relationships},
year = {2021},
isbn = {978-3-030-93045-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-93046-2_7},
doi = {10.1007/978-3-030-93046-2_7},
abstract = {The target-driven visual navigation is a popular learning-based method and has been successfully applied to a wide range of applications. However, it has some disadvantages, including being ineffective at adapting to unseen environments. In this paper, a navigation method based on Semantic Spatial Relationships (SSR) is proposed and is shown to have more reliable performance when dealing with novel conditions. The construction of joint semantic hierarchical feature vector allows for learning implicit relationship between current observation and target objects, which benefits from construction of prior knowledge graph and semantic space. This differs from the traditional target driven methods, which integrate the visual input vector directly into the reinforcement learning path planning module. Moreover, the proposed method takes both local and global features of observed image into consideration and is thus less conservative and more robust in regards to random scenes. An additional analysis indicates that the proposed SSR performs well on classical metrics. The effectiveness of the proposed SSR model is demonstrated comparing with state-of-the-art methods in unknown scenes.},
booktitle = {Artificial Intelligence: First CAAI International Conference, CICAI 2021, Hangzhou, China, June 5–6, 2021, Proceedings, Part I},
pages = {77–88},
numpages = {12},
keywords = {Hierarchical relationship, Semantic graph, Visual navigation},
location = {Hangzhou, China}
}

@article{10.5555/3455716.3455897,
author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
title = {Curriculum learning for reinforcement learning domains: a framework and survey},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {181},
numpages = {50},
keywords = {transfer learning, reinforcement learning, curriculum learning}
}

@article{10.1016/j.engappai.2019.08.015,
author = {Tavasoli, Hanane and Oommen, B. John and Yazidi, Anis},
title = {On utilizing weak estimators to achieve the online classification of data streams},
year = {2019},
issue_date = {Nov 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {86},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2019.08.015},
doi = {10.1016/j.engappai.2019.08.015},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
pages = {11–31},
numpages = {21},
keywords = {Classification in data streams, Non-stationary environments, Learning automata, Weak estimators}
}

@inproceedings{10.1145/2364412.2364444,
author = {Filho, Jo\~{a}o Bosco Ferreira and Barais, Olivier and Baudry, Benoit and Viana, Windson and Andrade, Rossana M. C.},
title = {An approach for semantic enrichment of software product lines},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364444},
doi = {10.1145/2364412.2364444},
abstract = {Software Product Lines (SPLs) have evolved and gained attention as one of the most promising approaches for software reuse. Feature models are the main technique to represent domain variability in SPLs. However, there are other domain aspects, besides variability, which cannot be expressed in a feature model. Also, these diagrams were not designed to facilitate information retrieval, interoperability and inference. In contrast, ontologies seem to be the best solution to meet these requirements. Therefore, this work presents an approach for semantic enrichment of SPLs using ontologies. Our proposal provides methods to add domain information besides variability description, and a top-ontology that specifies generic concepts and relations in an SPL, working as a guide model for information addition. The proposed approach reuses the existing SPL feature model, adding semantic descriptions in a less intrusive way than modifying the feature model notation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {188–195},
numpages = {8},
keywords = {software product lines, ontology, knowledge},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.infsof.2021.106669,
author = {Soliman, Mohamed and Avgeriou, Paris and Li, Yikun},
title = {Architectural design decisions that incur technical debt — An industrial case study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106669},
doi = {10.1016/j.infsof.2021.106669},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {17},
keywords = {Architectural technical debt, Architectural knowledge, Architectural design decisions, Technical debt}
}

@inproceedings{10.1145/2019136.2019169,
author = {Duran-Limon, Hector A. and Castillo-Barrera, Francisco E. and Lopez-Herrejon, Roberto E.},
title = {Towards an ontology-based approach for deriving product architectures},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019169},
doi = {10.1145/2019136.2019169},
abstract = {Software product line (SPL) engineering has proven to improve software quality and shorten costs and development time. An important aspect in the product line development process involves variability, which is the ability of a system for being customised, changed, or extended. Approaches are required for modelling and resolving variability as well as for verifying the selections. In this paper, we outline our ongoing research towards an approach that automates the derivation of product architectures from an SPL architecture. The proposed approach relies on ontology-based reasoning and model-driven techniques, the former supports the validation of the generated architectures and the generation of the transformation rules while the latter realises the actual target product architectures. We sketch our approach with a voice over IP case example.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {29},
numpages = {5},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1007/s10270-011-0220-1,
author = {Hubaux, Arnaud and Heymans, Patrick and Schobbens, Pierre-Yves and Deridder, Dirk and Abbasi, Ebrahim Khalil},
title = {Supporting multiple perspectives in feature-based configuration},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-011-0220-1},
doi = {10.1007/s10270-011-0220-1},
abstract = {Feature diagrams have become commonplace in software product line engineering as a means to document variability early in the life cycle. Over the years, their application has also been extended to assist stakeholders in the configuration of software products. However, existing feature-based configuration techniques offer little support for tailoring configuration views to the profiles of the various stakeholders. In this paper, we propose a lightweight, yet formal and flexible, mechanism to leverage multidimensional separation of concerns in feature-based configuration. We propose a technique to specify concerns in feature diagrams and to generate automatically concern-specific configuration views. Three alternative visualisations are proposed. Our contributions are motivated and illustrated through excerpts from a real web-based meeting management application which was also used for a preliminary evaluation. We also report on the progress made in the development of a tool supporting multi-view feature-based configuration.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {641–663},
numpages = {23},
keywords = {Software product line engineering, Separation of concerns, Multi-view, Feature-based configuration, Feature diagram}
}

@inproceedings{10.5555/54764.54774,
author = {Lavency, P. and Vanhoedenaghe, M.},
title = {Knowledge based configuration management},
year = {1988},
isbn = {0818608420},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
booktitle = {Proceedings of the Twenty-First Annual Hawaii International Conference on Software Track},
pages = {83–92},
numpages = {10},
location = {Kailua-Kona, Hawaii, USA}
}

@inproceedings{10.1007/978-3-030-88361-4_26,
author = {Corcho, Oscar and Chaves-Fraga, David and Toledo, Jhon and Arenas-Guerrero, Juli\'{a}n and Badenes-Olmedo, Carlos and Wang, Mingxue and Peng, Hu and Burrett, Nicholas and Mora, Jos\'{e} and Zhang, Puchao},
title = {A High-Level Ontology Network for ICT Infrastructures},
year = {2021},
isbn = {978-3-030-88360-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88361-4_26},
doi = {10.1007/978-3-030-88361-4_26},
abstract = {The ICT infrastructures of medium and large organisations that offer ICT services (infrastructure, platforms, software, applications, etc.) are becoming increasingly complex. Nowadays, these environments combine all sorts of hardware (e.g., CPUs, GPUs, storage elements, network equipment) and software (e.g., virtual machines, servers, microservices, services, products, AI models). Tracking, understanding and acting upon all the data produced in the context of such environments is hence challenging. Configuration management databases have been so far widely used to store and provide access to relevant information and views on these components and on their relationships. However, different databases are organised according to different schemas. Despite existing efforts in standardising the main entities relevant for configuration management, there is not yet a core set of ontologies that describes these environments homogeneously, and which can be easily extended when new types of items appear. This paper presents an ontology network created with the purpose of serving as an initial step towards an homogeneous representation of this domain, and which has been already used to produce a knowledge graph for a large ICT company.},
booktitle = {The Semantic Web – ISWC 2021: 20th International Semantic Web Conference, ISWC 2021, Virtual Event, October 24–28, 2021, Proceedings},
pages = {446–462},
numpages = {17},
keywords = {Knowledge graph, Ontology network, Configuration management database}
}

@inproceedings{10.1145/3377930.3390215,
author = {Silva, Diego Fernandes da and Okada, Luiz Fernando and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Enhancing search-based product line design with crossover operators},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390215},
doi = {10.1145/3377930.3390215},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA designing has been formulated as a multi-objective optimization problem and successfully solved by a state-of-the-art search-based approach. However, the majority of empirical studies optimize PLA designs without applying one of the fundamental genetic operators: the crossover. An operator for PLA design, named Feature-driven Crossover, was proposed in a previous study. In spite of the promising results, this operator occasionally generated incomplete solutions. To overcome these limitations, this paper aims to enhance the search-based PLA design optimization by improving the Feature-driven Crossover and introducing a novel crossover operator specific for PLA design. The proposed operators were evaluated in two well-studied PLA designs, using three experimental configurations of NSGA-II in comparison with a baseline that uses only mutation operators. Empirical results show the usefulness and efficiency of the presented operators on reaching consistent solutions. We also observed that the two operators complement each other, leading to PLA design solutions with better feature modularization than the baseline experiment.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1250–1258},
numpages = {9},
keywords = {software product line, software architecture, recombination operators, multi-objective evolutionary algorithm},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1017/S0269888900001090,
author = {Gomes, Carla P.},
title = {Artificial intelligence and operations research: challenges and opportunities in planning and scheduling},
year = {2000},
issue_date = {March 2000},
publisher = {Cambridge University Press},
address = {USA},
volume = {15},
number = {1},
issn = {0269-8889},
url = {https://doi.org/10.1017/S0269888900001090},
doi = {10.1017/S0269888900001090},
abstract = {Both the Artificial Intelligence (AI) and the Operations Research (OR) communities are interested in developing techniques for solving hard combinatorial problems, in particular in the domain of planning and scheduling. AI approaches encompass a rich collection of knowledge representation formalisms for dealing with a wide variety of real-world problems. Some examples are constraint programming representations, logical formalisms, declarative and functional programming languages such as Prolog and Lisp, Bayesian models, rule-based formalism, etc. The downside of such rich representations is that in general they lead to intractable problems, and we therefore often cannot use such formalisms for handling realistic size problems. OR, on the other hand, has focused on more tractable representations, such as linear programming formulations. OR-based techniques have demonstrated the ability to identify optimal and locally optimal solutions for well-defined problem spaces. In general, however, OR solutions are restricted to rigid models with limited expressive power. AI techniques, on the other hand, provide richer and more flexible representations of real-world problems, supporting efficient constraint-based reasoning mechanisms as well as mixed initiative frameworks, which allow the human expertise to be in the loop. The challenge lies in providing representations that are expressive enough to describe real-world problems and at the same time guaranteeing good and fast solutions.},
journal = {Knowl. Eng. Rev.},
month = mar,
pages = {1–10},
numpages = {10}
}

@article{10.1016/j.sigpro.2019.107332,
author = {Shi, Caijuan and Gu, Zhibin and Duan, Changyu and Tian, Qi},
title = {Multi-view adaptive semi-supervised feature selection with the self-paced learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {168},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2019.107332},
doi = {10.1016/j.sigpro.2019.107332},
journal = {Signal Process.},
month = mar,
numpages = {11},
keywords = {Multi-view learning, Semi-supervised feature selection, Self-paced learning, Graph-based semi-supervised learning}
}

@inproceedings{10.5555/1753235.1753267,
author = {Mendonca, Marcilio and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {SAT-based analysis of feature models is easy},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Feature models are a popular variability modeling notation used in product line engineering. Automated analyses of feature models, such as consistency checking and interactive or offline product selection, often rely on translating models to propositional logic and using satisfiability (SAT) solvers.Efficiency of individual satisfiability-based analyses has been reported previously. We generalize and quantify these studies with a series of independent experiments. We show that previously reported efficiency is not incidental. Unlike with the general SAT instances, which fall into easy and hard classes, the instances induced by feature modeling are easy throughout the spectrum of realistic models. In particular, the phenomenon of phase transition is not observed for realistic feature models.Our main practical conclusion is a general encouragement for researchers to continued development of SAT-based methods to further exploit this efficiency in future.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {231–240},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.3103/S1060992X19020048,
author = {Yakovenko, A. A.},
title = {A Hybrid Learning Approach for Adaptive Classification of Acoustic Signals Using the Simulated Responses of Auditory Nerve Fibers},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {2},
issn = {1060-992X},
url = {https://doi.org/10.3103/S1060992X19020048},
doi = {10.3103/S1060992X19020048},
journal = {Opt. Mem. Neural Netw.},
month = apr,
pages = {118–128},
numpages = {11},
keywords = {unsupervised learning, self-organizing maps, radial basis functions, neural responses, machine perception, auditory periphery model, adaptive pattern classification}
}

@inproceedings{10.1145/3480433.3480447,
author = {Selitskiy, Stanislav and Christou, Nikolaos and Selitskaya, Natalya},
title = {Isolating Uncertainty of the Face Expression Recognition with the Meta-Learning Supervisor Neural Network},
year = {2021},
isbn = {9781450384148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3480433.3480447},
doi = {10.1145/3480433.3480447},
abstract = {We investigate whether the well-known poor performance of the head-on usage of the convolutional neural networks for the facial expression recognition task may be improved in terms of reducing the false positive and false negative errors. An uncertainty isolating technique is used that introduces an additional “unknown” class. A self-attention supervisor artificial neural network is used to “learn about learning” of the underlying convolutional neural networks, in particular, to learn patterns of the underlying neural network parameters that accompany wrong or correct verdicts. A novel data set containing artistic makeup and occlusions images is used to aggravate the problem of the training data not representing the test data distribution.},
booktitle = {2021 5th International Conference on Artificial Intelligence and Virtual Reality (AIVR)},
pages = {104–112},
numpages = {9},
keywords = {Uncertainty isolation, Self-attention, Meta-learning, Face expression recognition},
location = {Kumamoto, Japan},
series = {AIVR 2021}
}

@inproceedings{10.1145/222124.222151,
author = {Zeller, Andreas},
title = {A unified version model for configuration management},
year = {1995},
isbn = {0897917162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/222124.222151},
doi = {10.1145/222124.222151},
booktitle = {Proceedings of the 3rd ACM SIGSOFT Symposium on Foundations of Software Engineering},
pages = {151–160},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGSOFT '95}
}

@inproceedings{10.1007/978-3-030-63486-5_23,
author = {Yuan, Fangming and Neubert, Peer and Protzel, Peter},
title = {LocalSPED: A Classification Pipeline that Can Learn Local Features for Place Recognition Using a Small Training Set},
year = {2020},
isbn = {978-3-030-63485-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63486-5_23},
doi = {10.1007/978-3-030-63486-5_23},
abstract = {Visual place recognition is a key component for visual-SLAM. The current state-of-art methods use CNNs (Convolutional Neural Networks) to extract either a holistic descriptor or local features from the images. In recent work, a holistic descriptor method with the name SPED was proposed. In this paper, SPED is extended to a local feature configuration called LocalSPED by applying several modifications and by introducing a novel feature pooling method. Several variations of SPED and LocalSPED are trained on a smaller dataset and their performances are evaluated on several benchmark datasets. In the experiments, LocalSPED handles the decreased training set size significantly better than the original SPED approach and provides better place recognition results.},
booktitle = {Towards Autonomous Robotic Systems: 21st Annual Conference, TAROS 2020, Nottingham, UK, September 16, 2020, Proceedings},
pages = {209–213},
numpages = {5},
keywords = {Robotics, Local features, Place recognition},
location = {Nottingham, United Kingdom}
}

@article{10.1007/s10664-013-9254-z,
author = {Chen, Ning and Hoi, Steven C. and Xiao, Xiaokui},
title = {Software process evaluation: a machine learning framework with application to defect management process},
year = {2014},
issue_date = {December  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9254-z},
doi = {10.1007/s10664-013-9254-z},
abstract = {Software process evaluation is important to improve software development and the quality of software products in a software organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they usually suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In this study, we mainly focus on the procedure aspect of software processes, and formulate the problem as a sequence (with additional information, e.g., time, roles, etc.) classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to evaluate the execution of a software process more objectively. To validate the efficacy of our approach, we apply it to evaluate the execution of a defect management (DM) process in nine real industrial software projects. Our empirical results show that our approach is effective and promising in providing a more objective and quantitative measurement for the DM process evaluation task. Furthermore, we conduct a comprehensive empirical study to compare our proposed machine learning approach with an existing conventional approach (i.e., artifacts inspection). Finally, we analyze the advantages and disadvantages of both approaches in detail.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {1531–1564},
numpages = {34},
keywords = {Software process evaluation, Sequence classification, Machine learning, Defect management process}
}

@inproceedings{10.5555/1864659.1864674,
author = {Tan, Ming and Lafond, Carol and Jakobson, Gabriel and Young, Gary},
title = {Supporting performance and configuration management of GTE cellular networks},
year = {1996},
isbn = {9780262510912},
publisher = {AAAI Press},
abstract = {GTE Laboratories, in cooperation with GTE Mobilnet, has developed and deployed PERFFEX (PERFormance Expert), an intelligent system for performance and configuration management of cellular networks. PERFEX assists cellular network performance and radio engineers in the analysis of large volumes of cellular network performance and configuration data. It helps them locate and determine the probable causes of performance problems, and provides intelligent suggestions about how to correct them. The system combines an expert cellular network performance tuning capability with a map-based graphical user interface, data visualization programs, and a set of special cellular engineering tools. PERFEX is in daily use at more than 25 GTE Mobile Switching Centers. Since the first deployment of the system in late 1993, PERFEX has become a major GTE cellular network performance optimization tool.},
booktitle = {Proceedings of the Eighth Annual Conference on Innovative Applications of Artificial Intelligence},
pages = {1556–1563},
numpages = {8},
location = {Portland, Oregon},
series = {IAAI'96}
}

@article{10.1016/j.asoc.2016.07.048,
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah and Jalab, Hamid A.},
title = {Extracting features from online software reviews to aid requirements reuse},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.048},
doi = {10.1016/j.asoc.2016.07.048},
abstract = {Display Omitted The extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have the access.Online reviews for software products can be used as input for features extraction to assist requirements reuse.Techniques from unsupervised learning and Natural Language Processing is employed as a propose solutions to Requirements Reuse problem.The approach obtained a precision of 87% (62% average) and a recall of 86% (82% average), when evaluated against the truth data set created manually. Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. In Requirements Reuse (RR), the extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have access to these software artefacts. Due to organisational privacy, SRS are always kept confidential and not easily available to the public. As alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the RR process. The aim of this paper is to propose a semi-automated approach, known as Feature Extraction for Reuse of Natural Language requirements (FENL), to extract phrases that can represent software features from software reviews in the absence of SRS as a way to initiate the RR process. FENL is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. In the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (IR) area. As a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. As for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. The performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and F-Measure.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1297–1315},
numpages = {19},
keywords = {Unsupervised learning, Software engineering, Requirements reuse, Natural language processing, Latent semantic analysis}
}

@article{10.1504/ijbm.2021.112219,
author = {Gao, Feng and Luo, Daizhong and Ma, Xinqiang},
title = {Research on facial expression recognition of video stream based on OpenCV},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {1},
issn = {1755-8301},
url = {https://doi.org/10.1504/ijbm.2021.112219},
doi = {10.1504/ijbm.2021.112219},
abstract = {In order to overcome the poor performance of expression similarity measurement in traditional video stream facial expression recognition methods, an OpenCV based facial expression recognition method is proposed. In this method, the video stream face detection image is obtained by the window detection of various features in each position for the video stream image through the cascade classifier, and the image preprocessing is implemented. Based on OpenCV, the most important eyes and mouth in the facial expression are modeled, the eye feature model and mouth feature model are constructed, and the facial expression recognition of the video stream is realised through the constructed model. The experimental results show that the performance of expression similarity measurement is better, and the recognition rate of different expressions is more than 90%.},
journal = {Int. J. Biometrics},
month = jan,
pages = {114–129},
numpages = {15},
keywords = {facial expression recognition, face, video stream, OpenCV}
}

@inproceedings{10.5555/3504035.3504869,
author = {Fan, Xin and Liu, Risheng and Huyan, Kang and Feng, Yuyao and Luo, Zhongxuan},
title = {Self-reinforced cascaded regression for face alignment},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Cascaded regression is prevailing in face alignment thanks to its accuracy and robustness, but typically demands manually annotated examples having low discrepancy between shape-indexed features and shape updates. In this paper, we propose a self-reinforced strategy that iteratively expands the quantity and improves the quality of training examples, thus upgrading the performance of cascaded regression itself. The reinforced term evaluates the example quality upon the consistence on both local appearance and global geometry of human faces, and constitutes the example evolution by the philosophy of "survival of the fittest". We train a set of discriminative classifiers, each associated with one landmark label, to prune those examples with inconsistent local appearance, and further validate the geometric relationship among groups of labeled landmarks against the common global geometry derived from a projective invariant. We embed this generic strategy into typical cascaded regressions, and the alignment results on several benchmark data sets demonstrate its effectiveness to predict good examples starting from a small subset.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {834},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.1145/2019136.2019178,
author = {Brataas, Gunnar and Jiang, Shanshan and Reichle, Roland and Geihs, Kurt},
title = {Performance property prediction supporting variability for adaptive mobile systems},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019178},
doi = {10.1145/2019136.2019178},
abstract = {A performance property prediction (PPP) method for component-based self-adaptive applications is presented. Such performance properties are required by an adaptation middleware for reasoning about adaptation activities. Our PPP method is based on the Structure and Performance (SP) framework, a conceptually simple, yet powerful performance modelling framework based on matrices. The main contribution of this paper are the integration of SP-based PPP into a comprehensive model- and variability-based adaptation framework for context-aware mobile applications. A meta model for the SP method is described. The framework is demonstrated using a practical example.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {37},
numpages = {8},
keywords = {mobile systems, autonomic computing},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.engappai.2021.104473,
author = {Liu, Ze-yu and Liu, Jian-wei and Zuo, Xin and Hu, Ming-fei},
title = {Multi-scale iterative refinement network for RGB-D salient object detection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2021.104473},
doi = {10.1016/j.engappai.2021.104473},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
numpages = {16},
keywords = {Multi-scale refinement, RGB-D image, Salient object detection}
}

@article{10.1007/s10664-021-09940-0,
author = {Cashman, Mikaela and Firestone, Justin and Cohen, Myra B. and Thianniwet, Thammasak and Niu, Wei},
title = {An empirical investigation of organic software product lines},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09940-0},
doi = {10.1007/s10664-021-09940-0},
abstract = {Software product line engineering is a best practice for managing reuse in families of software systems that is increasingly being applied to novel and emerging domains. In this work we investigate the use of software product line engineering in one of these new domains, synthetic biology. In synthetic biology living organisms are programmed to perform new functions or improve existing functions. These programs are designed and constructed using small building blocks made out of DNA. We conjecture that there are families of products that consist of common and variable DNA parts, and we can leverage product line engineering to help synthetic biologists build, evolve, and reuse DNA parts. In this paper we perform an investigation of domain engineering that leverages an open-source repository of more than 45,000 reusable DNA parts. We show the feasibility of these new types of product line models by identifying features and related artifacts in up to 93.5% of products, and that there is indeed both commonality and variability. We then construct feature models for four commonly engineered functions leading to product lines ranging from 10 to 7.5 \texttimes{} 1020 products. In a case study we demonstrate how we can use the feature models to help guide new experimentation in aspects of application engineering. Finally, in an empirical study we demonstrate the effectiveness and efficiency of automated reverse engineering on both complete and incomplete sets of products. In the process of these studies, we highlight key challenges and uncovered limitations of existing SPL techniques and tools which provide a roadmap for making SPL engineering applicable to new and emerging domains.},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {43},
keywords = {BioBricks, Reverse engineering, Synthetic biology, Software product lines}
}

@article{10.1145/1842713.1842717,
author = {Robinson, William N. and Ding, Yi},
title = {A survey of customization support in agent-based business process simulation tools},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/1842713.1842717},
doi = {10.1145/1842713.1842717},
abstract = {Agent-based business process simulation has grown in popularity, in part because of its analysis capabilities. The analyses depend on the kinds of simulations that can be built, adapted, and extended, which in turn depend on the underlying simulation framework. We report the results of our analysis of 19 agent-based process simulation tools and their simulation frameworks. We conclude that a growing number of simulation tools are using component-based software techniques. Nevertheless, most simulation tools do not directly support requirements models, their transformation into executable simulations, or the management of model variants over time. Such practices are becoming more widely applied in software engineering under the term software product line engineering (SPLE). Based on our analysis, agent-based process simulation tools may improve their customization capacity by: (1) supporting object modeling more completely and (2) supporting software product line engineering issues.},
journal = {ACM Trans. Model. Comput. Simul.},
month = oct,
articleno = {14},
numpages = {29},
keywords = {software product line engineering, modularity, event-driven simulation, encapsulation, application frameworks, Agent-based modeling}
}

@inproceedings{10.1007/978-3-030-78609-0_28,
author = {Sun, Ying and Sun, Yanfei and Wu, Fei and Jing, Xiao-Yuan},
title = {Deep Adversarial Learning Based Heterogeneous Defect Prediction},
year = {2021},
isbn = {978-3-030-78608-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78609-0_28},
doi = {10.1007/978-3-030-78609-0_28},
abstract = {Cross-project defect prediction (CPDP) is a hot study that predicts defects in the new project by utilizing the model trained on the data from other projects. However, existing CPDP methods usually assume that source and target projects have the same metrics. Heterogeneous defect prediction (HDP) is proposed and has attracted increasing attention, which refers to the metric sets from source and target projects are different in CPDP. HDP conducts prediction model using the instances with heterogeneous metrics from external projects and then use this model to predict defect-prone software instances in source project. However, building HDP methods is challenging including the distribution difference between source and target projects with heterogeneous metrics. In this paper, we propose a Deep adversarial learning based HDP (DHDP) approach. DHDP leverages deep neural network to learn nonlinear transformation for each project to obtain common feature represent, which the heterogeneous data from different projects can be compared directly. DHDP consists of two parts: a discriminator and a classifier that compete with each other. A classifier tries to minimize the similarity across classes and maximize the inter-class similarity. A discriminator tries to distinguish the source of instances that is source or target project on the common feature space. Expensive experiments are performed on 10 public projects from two datasets in terms of F-measure and G-measure. The experimental results show that DHDP gains superior prediction performance improvement compared to a range of competing methods.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part I},
pages = {326–337},
numpages = {12},
keywords = {Heterogeneous defect prediction, Metric learning, Adversarial learning},
location = {Dublin, Ireland}
}

@inproceedings{10.1007/978-3-030-50334-5_13,
author = {Allen, Donald M. and Goloubew, Dmitry},
title = {Customer Self-remediation of Proactive Network Issue Detection and Notification},
year = {2020},
isbn = {978-3-030-50333-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-50334-5_13},
doi = {10.1007/978-3-030-50334-5_13},
abstract = {Improving computer network availability has been a focus of researchers for the past 30 years and considerable investigation into the use of AI and Machine Learning, primarily in the operate space has been conducted. Previous efforts have been primarily reactive in nature, monitoring networks, developing base models, and trying to predict future failures based on those models. This approach has shown limited success due to the dynamic nature of network equipment and function. Cisco has been developing capabilities over the last decade to proactively analysis network devices and identify issues that could impact a networks availability. In the current approach issues are identified to the customer and it is the customer’s responsibility to identify the issues that they determine need to be fixed. The capability has been trialed over the last 2 years and the research discussed in this paper is focused on the analysis of their actions. Machine Learning is applied to the issue consumption data set and observations made on the features that can be used to predict which issues will be fixed.},
booktitle = {Artificial Intelligence in HCI: First International Conference, AI-HCI 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings},
pages = {197–210},
numpages = {14},
keywords = {Machine Learning, Proactive issue detection, Proactive issue remediation, Network management},
location = {Copenhagen, Denmark}
}

@article{10.1016/j.infsof.2009.11.001,
author = {Rabiser, Rick and Gr\"{u}nbacher, Paul and Dhungana, Deepak},
title = {Requirements for product derivation support: Results from a systematic literature review and an expert survey},
year = {2010},
issue_date = {March, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.001},
doi = {10.1016/j.infsof.2009.11.001},
abstract = {Context: An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support. Objective: Our aim is to identify and validate requirements for tool-supported product derivation. Method: We identify the requirements through a systematic literature review and validate them with an expert survey. Results: We discuss the resulting requirements and provide implementation examples from existing product derivation approaches. Conclusions: We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {324–346},
numpages = {23},
keywords = {Systematic literature review, Software product line, Product line engineering, Product derivation}
}

@inproceedings{10.1007/978-3-030-79463-7_35,
author = {Kawalerowicz, Marcin and Madeyski, Lech},
title = {Continuous Build Outcome Prediction: A Small-N Experiment in Settings of a Real Software Project},
year = {2021},
isbn = {978-3-030-79462-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79463-7_35},
doi = {10.1007/978-3-030-79463-7_35},
abstract = {We explain the idea of Continuous Build Outcome Prediction (CBOP) practice that uses classification to label the possible build results (success or failure) based on historical data and metrics (features) derived from the software repository. Additionally, we present a preliminary empirical evaluation of CBOP in a real live software project. In a small-n repeated-measure with two conditions and replicates experiment, we study whether CBOP will reduce the Failed Build Ratio (FBR). Surprisingly, the result of the study indicates a slight increase in FBR while using the CBOP, although the effect size is very small. A plausible explanation of the revealed phenomenon may come from the authority principle, which is rarely discussed in the software engineering context in general, and AI-supported software development practices in particular.},
booktitle = {Advances and Trends in Artificial Intelligence. From Theory to Practice: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part II},
pages = {412–425},
numpages = {14},
keywords = {Machine learning, Continuous integration, Agile experimentation, Software defect prediction},
location = {Kuala Lumpur, Malaysia}
}

@inproceedings{10.1145/75308.75321,
author = {Moriconi, M.},
title = {A practical approach to semantic configuration management},
year = {1989},
isbn = {0897913426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/75308.75321},
doi = {10.1145/75308.75321},
abstract = {A configuration management (CM) tool is supposed to build a consistent software system following incremental changes to the system. The notion of consistency usually is purely syntactic, having to do with the sorts of properties analyzed by compilers. Semantic consistency traditionally has been studied in the field of formal methods and has been considered an impractical goal for CM.Although the semantic CM problem is undecidable, it is possible to obtain a structural approximation of the semantic effects of a change in a finite number of steps. Our approximation technique is formalized in logic and is based on information-theoretic properties of programs. The method in its present form applies to many but not all software systems, and it is programming-language independent. To the best of our knowledge, the semantic CM problem has not been formalized previously in nonsemantic terms, and we believe that our simplified formulation offers the potential for considerably more powerful debugging and configuration management tools.},
booktitle = {Proceedings of the ACM SIGSOFT '89 Third Symposium on Software Testing, Analysis, and Verification},
pages = {103–113},
numpages = {11},
location = {Key West, Florida, USA},
series = {TAV3}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {many-objective problems, dimensionality reduction, Software product line testing},
location = {Natal, Brazil},
series = {SAST '20}
}

@inproceedings{10.5555/36494.36498,
author = {Bernard, Y. and Lacroix, M. and Lavency, P. and Vanhoedenaghe, M.},
title = {Configuration management in an open environment},
year = {1987},
isbn = {038718712X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proc. of the 1st European Software Engineering Conference on ESEC '87},
pages = {35–43},
numpages = {9},
location = {Strasbourg, France}
}

@inproceedings{10.1145/3397481.3450678,
author = {Reyes, Guillermo and Alles, Alexandra},
title = {Multi-modal Multi-scale Attention Guidance in Cyber-Physical Environments},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397481.3450678},
doi = {10.1145/3397481.3450678},
abstract = {This work proposes a new method for guiding a user’s attention towards objects of interest in a cyber-physical environment (CPE). CPEs are environments that contain several computing systems that interact with each other and with the physical world. These environments contain several sensors (cameras, eye trackers, etc.) and output devices (lamps, screens, speakers, etc.). These devices can be used to first track the user’s position, orientation, and focus of attention to then find the most suitable output device to guide the user’s attention towards a target object. We argue that the most suitable device in this context is the one that attracts attention closest to the target and is salient enough to capture the user’s attention. The method is implemented as a function which estimates the ”closeness” and ”salience” of each visual and auditive output device in the environment. Some parameters of this method are then evaluated through a user study in the context of a virtual reality supermarket. The results show that multi-modal guidance can lead to better guiding performance. However, this depends on the set parameters.},
booktitle = {Proceedings of the 26th International Conference on Intelligent User Interfaces},
pages = {356–365},
numpages = {10},
keywords = {Attention, Attention Guidance, Cyber-Physical Environments, Intelligent Environments, Multi-modal, Multi-scale},
location = {College Station, TX, USA},
series = {IUI '21}
}

@inproceedings{10.1109/ASE.2011.6100070,
author = {Chen, Ning and Hoi, Steven C. H. and Xiao, Xiaokui},
title = {Software process evaluation: A machine learning approach},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100070},
doi = {10.1109/ASE.2011.6100070},
abstract = {Software process evaluation is essential to improve software development and the quality of software products in an organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In particular, we formulate the problem as a sequence classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to objectively evaluate the quality and performance of a software process. To validate the efficacy of our approach, we apply it to evaluate the defect management process performed in four real industrial software projects. Our empirical results show that our approach is effective and promising in providing an objective and quantitative measurement for software process evaluation.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {333–342},
numpages = {10},
series = {ASE '11}
}

@article{10.1007/s10664-020-09853-4,
author = {Hajri, Ines and Goknil, Arda and Pastore, Fabrizio and Briand, Lionel C.},
title = {Automating system test case classification and prioritization for use case-driven testing in product lines},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09853-4},
doi = {10.1007/s10664-020-09853-4},
abstract = {Product Line Engineering (PLE) is a crucial practice in many software development environments where software systems are complex and developed for multiple customers with varying needs. At the same time, many development processes are use case-driven and this strongly influences their requirements engineering and system testing practices. In this paper, we propose, apply, and assess an automated system test case classification and prioritization approach specifically targeting system testing in the context of use case-driven development of product families. Our approach provides: (i) automated support to classify, for a new product in a product family, relevant and valid system test cases associated with previous products, and (ii) automated prioritization of system test cases using multiple risk factors such as fault-proneness of requirements and requirements volatility in a product family. Our evaluation was performed in the context of an industrial product family in the automotive domain. Results provide empirical evidence that we propose a practical and beneficial way to classify and prioritize system test cases for industrial product lines.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3711–3769},
numpages = {59},
keywords = {Requirements engineering, Automotive, Test case selection and prioritization, Regression testing, Use case driven development, Product Line Engineering}
}

@inproceedings{10.1145/64135.65023,
author = {Heimbigner, Dennis and Krane, Steven},
title = {A graph transform model for configuration management environments},
year = {1988},
isbn = {089791290X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/64135.65023},
doi = {10.1145/64135.65023},
abstract = {A model for software configuration management that subsumes several existing systems is described. It is patterned after compiler models in which programs are transformed by multiple phases ending in an executable program. We model configuration management as transforming a high-level specification of a software product to be produced into a complete specification capable of being executed to construct the product. This transformational approach is used to model four existing systems and to compare and contrast their operation.},
booktitle = {Proceedings of the Third ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments},
pages = {216–225},
numpages = {10},
location = {Boston, Massachusetts, USA},
series = {SDE 3}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Web System, Software Product Line, Machine Learning, Energy Aware},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@article{10.1016/j.jss.2021.111027,
author = {Jiang, Zijian and Zhong, Hao and Meng, Na},
title = {Investigating and recommending co-changed entities for JavaScript programs},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111027},
doi = {10.1016/j.jss.2021.111027},
journal = {J. Syst. Softw.},
month = oct,
numpages = {19},
keywords = {JavaScript, Machine learning, Change suggestion, Multi-entity edit}
}

@inproceedings{10.5555/3367471.3367606,
author = {Yang, Liang and Chen, Zhiyang and Gu, Junhua and Guo, Yuanfang},
title = {Dual self-paced graph convolutional network: towards reducing attribute distortions induced by topology},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {The success of graph convolutional neural networks (GCNNs) based semi-supervised node classification is credited to the attribute smoothing (propagating) over the topology. However, the attributes may be interfered by the utilization of the topology information. This distortion will induce a certain amount of misclassifications of the nodes, which can be correctly predicted with only the attributes. By analyzing the impact of the edges in attribute propagations, the simple edges, which connect two nodes with similar attributes, should be given priority during the training process compared to the complex ones according to curriculum learning. To reduce the distortions induced by the topology while exploit more potentials of the attribute information, Dual Self-Paced Graph Convolutional Network (DSP-GCN) is proposed in this paper. Specifically, the unlabelled nodes with confidently predicted labels are gradually added into the training set in the node-level self-paced learning, while edges are gradually, from the simple edges to the complex ones, added into the graph during the training process in the edge-level self-paced learning. These two learning strategies are designed to mutually reinforce each other by coupling the selections of the edges and unlabelled nodes. Experimental results of transductive semi-supervised node classification on many real networks indicate that the proposed DSP-GCN has successfully reduced the attribute distortions induced by the topology while it gives superior performances with only one graph convolutional layer.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {4062–4069},
numpages = {8},
location = {Macao, China},
series = {IJCAI'19}
}

@inproceedings{10.1007/978-3-030-76352-7_18,
author = {Liu, Xiaotong and Tong, Yingbei and Xu, Anbang and Akkiraju, Rama},
title = {Using Language Models to Pre-train Features for Optimizing Information Technology Operations Management Tasks},
year = {2020},
isbn = {978-3-030-76351-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-76352-7_18},
doi = {10.1007/978-3-030-76352-7_18},
abstract = {Information Technology (IT) Operations management is a vexing problem for most companies that rely on IT systems for mission-critical business applications. While IT operators are increasingly leveraging analytical tools powered by artificial intelligence (AI), the volume, the variety and the complexity of data generated in the IT Operations domain poses significant challenges in managing the applications. In this work, we present an approach to leveraging language models to pre-train features for optimizing IT Operations management tasks such as anomaly prediction from logs. Specifically, using log-based anomaly prediction as the task, we show that the machine learning models built using language models (embeddings) trained with IT Operations domain data as features outperform those AI models built using language models with general-purpose data as features. Furthermore, we present our empirical results outlining the influence of factors such as the type of language models, the type of input data, and the diversity of input data, on the prediction accuracy of our log anomaly prediction model when language models trained from IT Operations domain data are used as features. We also present the run-time inference performance of log anomaly prediction models built using language models as features in an IT Operations production environment.},
booktitle = {Service-Oriented Computing  – ICSOC 2020 Workshops: AIOps, CFTIC, STRAPS, AI-PA, AI-IOTS, and Satellite Events, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {150–161},
numpages = {12},
keywords = {Anomaly detection, Language modeling, AI for IT operations},
location = {Dubai, United Arab Emirates}
}

@article{10.1016/j.engappai.2018.06.002,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Yu, Yang and Yuan, Bo-Wen and Tang, Jia-Fu},
title = {Integration of an improved dynamic ensemble selection approach to enhance one-vs-one scheme},
year = {2018},
issue_date = {Sep 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2018.06.002},
doi = {10.1016/j.engappai.2018.06.002},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {43–53},
numpages = {11},
keywords = {Multi-class classification, Decomposition strategy, One-vs-one, Heterogeneous ensemble, Dynamic selection}
}

@inproceedings{10.5555/3504035.3504690,
author = {Neill, James O' and Buitelaar, Paul},
title = {Few shot transfer learning between word relatedness and similarity tasks using a gated recurrent siamese network},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Word similarity and word relatedness are fundamental to natural language processing and more generally, understanding how humans relate concepts in semantic memory. A growing number of datasets are being proposed as evaluation benchmarks, however, the heterogeneity and focus of each respective dataset makes it difficult to draw plausible conclusions as to how a unified semantic model would perform. Additionally, we want to identify the transferability of knowledge obtained from one task to another, within the same domain and across domains. Hence, this paper first presents an evaluation and comparison of eight chosen datasets tested using the best performing regression models. As a baseline, we present regression models that incorporate both lexical features and word embeddings to produce consistent and competitive results compared to the state of the art. We present our main contribution, the best performing model across seven of the eight datasets - a Gated Recurrent Siamese Network that learns relationships between lexical word definitions. A parameter transfer learning strategy is employed for the Siamese Network. Subsequently, we present a secondary contribution which is the best performing non-sequential model: an Inductive and Transductive Transfer Learning strategy for transferring decision trees within a Random Forest to a target task that is learned from only few instances. The method involves measuring semantic distance between hidden factored matrix representations of decision tree traversal matrices.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {655},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1145/43857.43869,
author = {Arango, Guillermo and Freeman, Peter},
title = {Application of artificial intelligence},
year = {1988},
issue_date = {Jan. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/43857.43869},
doi = {10.1145/43857.43869},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {32–38},
numpages = {7}
}

@article{10.1007/s11042-020-10443-1,
author = {Rao, Champakamala Sundar and Karunakara, K.},
title = {A comprehensive review on brain tumor segmentation and classification of MRI images},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10443-1},
doi = {10.1007/s11042-020-10443-1},
abstract = {In the analysis of medical images, one of the challenging tasks is the recognition of brain tumours via medical resonance images (MRIs). The diagnosis process is still tedious due to its complexity and considerable variety in tissues of tumor perception. Therefore, the necessities of tumor identification techniques are improving nowadays for medical applications. In the past decades, different approaches in the segmentation of various precisions and complexity degree have been accomplished, which depends on the simplicity and the benchmark of the technique. An overview of this analysis is to give out the summary of the semi-automatic techniques for brain tumor segmentation and classification utilizing MRI. An enormous amount of MRI based image data is accomplished using deep learning approaches. There are several works, dealing on the conventional approaches for MRI-based segmentation of brain tumor. Alternatively, in this review, we revealed the latest trends in the methods of deep learning. Initially, we explain the several threads in MRI pre-processing, including registration of image, rectification of bias field, and non-brain tissue dismissal. And terminally, the present state evaluation of algorithm is offered and forecasting the growths to systematise the MRI-based brain tumor into a regular cyclic routine in the clinical field are focussed.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {17611–17643},
numpages = {33},
keywords = {Image processing, Tissue, Bias field, Segmentation, Brain tumor, MRI}
}

@inproceedings{10.1609/aaai.v33i01.33019005,
author = {Wu, Xiang and Huang, Huaibo and Patel, Vishal M. and He, Ran and Sun, Zhenan},
title = {Disentangled variational representation for heterogeneous face recognition},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33019005},
doi = {10.1609/aaai.v33i01.33019005},
abstract = {Visible (VIS) to near infrared (NIR) face matching is a challenging problem due to the significant domain discrepancy between the domains and a lack of sufficient data for training cross-modal matching algorithms. Existing approaches attempt to tackle this problem by either synthesizing visible faces from NIR faces, extracting domain-invariant features from these modalities, or projecting heterogeneous data onto a common latent space for cross-modal matching. In this paper, we take a different approach in which we make use of the Disentangled Variational Representation (DVR) for cross-modal matching. First, we model a face representation with an intrinsic identity information and its within-person variations. By exploring the disentangled latent variable space, a variational lower bound is employed to optimize the approximate posterior for NIR and VIS representations. Second, aiming at obtaining more compact and discriminative disentangled latent space, we impose a minimization of the identity information for the same subject and a relaxed correlation alignment constraint between the NIR and VIS modality variations. An alternative optimization scheme is proposed for the disentangled variational representation part and the heterogeneous face recognition network part. The mutual promotion between these two parts effectively reduces the NIR and VIS domain discrepancy and alleviates over-fitting. Extensive experiments on three challenging NIR-VIS heterogeneous face recognition databases demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1105},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/3483899.3483905,
author = {Freire, Willian and Tonh\~{a}o, Simone and Bonetti, Tiago and Shigenaga, Marcelo and Cadette, William and Felizardo, Fernando and Amaral, Aline and OliveiraJr, Edson and Colanzi, Thelma},
title = {On the configuration of multi-objective evolutionary algorithms for PLA design optimization},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483905},
doi = {10.1145/3483899.3483905},
abstract = {Search-based algorithms have been successfully applied in the Product Line Architecture (PLA) optimization using the seminal approach called Multi-Objective Approach for Product-Line Architecture Design (MOA4PLA). This approach produces a set of alternative PLA designs intending to improve the different factors being optimized. Currently, the MOA4PLA uses the NSGA-II algorithm, a multi-objective evolutionary algorithm (MOEA) that can optimize several architectural properties simultaneously. Despite the promising results, studying the best values for the algorithm parameters is essential to obtain even better results. This is also crucial to ease the adoption of MOA4PLA by newcomers or non-expert companies willing to start using search-based software engineering to PLA design. Three crossover operators for the PLA design optimization were proposed recently. However, reference values for parameters have not been defined for PLA design optimization using crossover operators. In this context, the objective of this work is conducting an experimental study to discover which are the most effective crossover operators and the best values to configure the MOEA parameters, such as population size, number of generations, and mutation and crossover rates. A quantitative analysis based on quality indicators and statistical tests was performed using four PLA designs to determine the most suitable parameter values to the search-based algorithm. Empirical results pointed out the best combination of crossover operators and the most suitable values to configure MOA4PLA.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {software product line, software architecture, recombination operators, Multi-objective evolutionary algorithm},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@article{10.1016/j.eswa.2018.04.033,
author = {Sreevani and Murthy, C.A. and Chanda, Bhabatosh},
title = {Generation of compound features based on feature interaction for classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {108},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2018.04.033},
doi = {10.1016/j.eswa.2018.04.033},
journal = {Expert Syst. Appl.},
month = oct,
pages = {61–73},
numpages = {13},
keywords = {Feature extraction, Feature selection, Compound features, Semi-features, Information theory, Feature interaction, Mutual information}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Video testing, Variability modeling, Software product line engineering, Feature modeling, Domain-specific languages, Configuration, Automated reasoning}
}

@article{10.1016/j.ijar.2007.03.006,
author = {Peterson, Leif E. and Coleman, Matthew A.},
title = {Machine learning-based receiver operating characteristic (ROC) curves for crisp and fuzzy classification of DNA microarrays in cancer research},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {47},
number = {1},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2007.03.006},
doi = {10.1016/j.ijar.2007.03.006},
abstract = {Receiver operating characteristic (ROC) curves were generated to obtain classification area under the curve (AUC) as a function of feature standardization, fuzzification, and sample size from nine large sets of cancer-related DNA microarrays. Classifiers used included k-nearest neighbor (kNN), naive Bayes classifier (NBC), linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), learning vector quantization (LVQ1), logistic regression (LOG), polytomous logistic regression (PLOG), artificial neural networks (ANN), particle swarm optimization (PSO), constricted particle swarm optimization (CPSO), kernel regression (RBF), radial basis function networks (RBFN), gradient descent support vector machines (SVMGD), and least squares support vector machines (SVMLS). For each data set, AUC was determined for a number of combinations of sample size, total sum[-log(p)] of feature t-tests, with and without feature standardization and with (fuzzy) and without (crisp) fuzzification of features. Altogether, a total of 2,123,530 classification runs were made. At the greatest level of sample size, ANN resulted in a fitted AUC of 90%, while PSO resulted in the lowest fitted AUC of 72.1%. AUC values derived from 4NN were the most dependent on sample size, while PSO was the least. ANN depended the most on total statistical significance of features used based on sum[-log(p)], whereas PSO was the least dependent. Standardization of features increased AUC by 8.1% for PSO and -0.2% for QDA, while fuzzification increased AUC by 9.4% for PSO and reduced AUC by 3.8% for QDA. AUC determination in planned microarray experiments without standardization and fuzzification of features will benefit the most if CPSO is used for lower levels of feature significance (i.e., sum[-log(p)]~50) and ANN is used for greater levels of significance (i.e., sum[-log(p)]~500). When only standardization of features is performed, studies are likely to benefit most by using CPSO for low levels of feature statistical significance and LVQ1 for greater levels of significance. Studies involving only fuzzification of features should employ LVQ1 because of the substantial gain in AUC observed and low expense of LVQ1. Lastly, PSO resulted in significantly greater levels of AUC (89.5% average) when feature standardization and fuzzification were performed. In consideration of the data sets used and factors influencing AUC which were investigated, if low-expense computation is desired then LVQ1 is recommended. However, if computational expense is of less concern, then PSO or CPSO is recommended.},
journal = {Int. J. Approx. Reasoning},
month = jan,
pages = {17–36},
numpages = {20},
keywords = {Soft computing, Receiver operator characteristic (ROC) curve, Gene expression, Fuzzy classification, DNA microarrays, Area under the curve (AUC)}
}

@article{10.5555/3322706.3361988,
author = {Zhou, Zhixin and Amini, Arash A.},
title = {Analysis of spectral clustering algorithms for community detection: the general bipartite setting},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a k-means type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1774–1820},
numpages = {47},
keywords = {sub-Gaussian biclustering, stochastic block model, spectral clustering, regularization of random graphs, graphon clustering, community detection, bipartite networks}
}

@inproceedings{10.1145/1101908.1101987,
author = {Ge, Guozheng and Whitehead, E. James},
title = {Bamboo: an architecture modeling and code generation framework for configuration management systems},
year = {2005},
isbn = {1581139934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101908.1101987},
doi = {10.1145/1101908.1101987},
abstract = {We describe an architecture modeling and code generation framework called Bamboo. Using Bamboo, engineers design SCM repository and feature models, and then generate a running SCM system from the models.},
booktitle = {Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering},
pages = {427–428},
numpages = {2},
keywords = {software modeling and generation, configuration management},
location = {Long Beach, CA, USA},
series = {ASE '05}
}

@article{10.1016/j.ins.2021.05.008,
author = {Zhang, Nana and Ying, Shi and Ding, Weiping and Zhu, Kun and Zhu, Dandan},
title = {WGNCS: A robust hybrid cross-version defect model via multi-objective optimization and deep enhanced feature representation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {570},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.05.008},
doi = {10.1016/j.ins.2021.05.008},
journal = {Inf. Sci.},
month = sep,
pages = {545–576},
numpages = {32},
keywords = {Convolutional neural network, Wasserstein GAN with Gradient Penalty, Deep learning techniques, Multi-objective feature selection, Cross-version defect prediction}
}

@article{10.1007/s10270-016-0516-2,
author = {Damiani, Ferruccio and Faitelson, David and Gladisch, Christoph and Tyszberowicz, Shmuel},
title = {A novel model-based testing approach for software product lines},
year = {2017},
issue_date = {October   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0516-2},
doi = {10.1007/s10270-016-0516-2},
abstract = {Model-based testing relies on a model of the system under test. FineFit is a framework for model-based testing of Java programs. In the FineFit approach, the model is expressed by a set of tables based on Parnas tables. A software product line is a family of programs (the products) with well-defined commonalities and variabilities that are developed by (re)using common artifacts. In this paper, we address the issue of using the FineFit approach to support the development of correct software product lines. We specify a software product line as a specification product line where each product is a FineFit specification of the corresponding software product. The main challenge is to concisely specify the software product line while retaining the readability of the specification of a single system. To address this, we used delta-oriented programming, a recently proposed flexible approach for implementing software product lines, and developed: (1) delta tables as a means to apply the delta-oriented programming idea to the specification of software product lines; and (2) DeltaFineFit as a novel model-based testing approach for software product lines.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {1223–1251},
numpages = {29},
keywords = {Software product line, Refinement, Model-based testing, Java, Delta-oriented programming, Alloy}
}

@inproceedings{10.5555/3367243.3367442,
author = {Li, Longyuan and Yan, Junchi and Yang, Xiaokang and Jin, Yaohui},
title = {Learning interpretable deep state space model for probabilistic time series forecasting},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Probabilistic time series forecasting involves estimating the distribution of future based on its history, which is essential for risk management in downstream decision-making. We propose a deep state space model for probabilistic time series forecasting whereby the non-linear emission model and transition model are parameterized by networks and the dependency is modeled by recurrent neural nets. We take the automatic relevance determination (ARD) view and devise a network to exploit the exogenous variables in addition to time series. In particular, our ARD network can incorporate the uncertainty of the exogenous variables and eventually helps identify useful exogenous variables and suppress those irrelevant for forecasting. The distribution of multi-step ahead forecasts are approximated by Monte Carlo simulation. We show in experiments that our model produces accurate and sharp probabilistic forecasts. The estimated uncertainty of our forecasting also realistically increases over time, in a spontaneous manner.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {2901–2908},
numpages = {8},
location = {Macao, China},
series = {IJCAI'19}
}

@inproceedings{10.1109/WAIN52551.2021.00019,
author = {Granlund, Tuomas and Kopponen, Aleksi and Stirbu, Vlad and Myllyaho, Lalli and Mikkonen, Tommi},
title = {MLOps Challenges in Multi-Organization Setup: Experiences from Two Real-World Cases},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00019},
doi = {10.1109/WAIN52551.2021.00019},
abstract = {The emerging age of connected, digital world means that there are tons of data, distributed to various organizations and their databases. Since this data can be confidential in nature, it cannot always be openly shared in seek of artificial intelligence (AI) and machine learning (ML) solutions. Instead, we need integration mechanisms, analogous to integration patterns in information systems, to create multi-organization AI/ML systems. In this paper, we present two real-world cases. First, we study integration between two organizations in detail. Second, we address scaling of AI/ML to multi-organization context. The setup we assume is that of continuous deployment, often referred to DevOps in software development. When also ML components are deployed in a similar fashion, term MLOps is used. Towards the end of the paper, we list the main observations and draw some final conclusions. Finally, we propose some directions for future work.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {82–88},
numpages = {7},
location = {Madrid, Spain}
}

@inproceedings{10.5555/776816.776870,
author = {Sarma, Anita and Noroozi, Zahra and van der Hoek, Andr\'{e}},
title = {Palant\'{\i}r: raising awareness among configuration management workspaces},
year = {2003},
isbn = {076951877X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Current configuration management systems promote workspaces that isolate developers from each other. This isolation is both good and bad. It is good, because developers make their changes without any interference from changes made concurrently by other developers. It is bad, because not knowing which artifacts are changing in parallel regularly leads to problems when changes are promoted from workspaces into a central configuration management repository. Overcoming the bad isolation, while retaining the good isolation, is a matter of raising awareness among developers, an issue traditionally ignored by the discipline of configuration management. To fill this void, we have developed Palant\'{\i}r, a novel workspace awareness tool that complements existing configuration management systems by providing developers with insight into other workspaces. In particular, the tool informs a developer of which other developers change which other artifacts, calculates a simple measure of severity of those changes, and graphically displays the information in a configurable and generally non-obtrusive manner. To illustrate the use of Palant\'{\i}r, we demonstrate how it integrates with two representative configuration management systems.},
booktitle = {Proceedings of the 25th International Conference on Software Engineering},
pages = {444–454},
numpages = {11},
location = {Portland, Oregon},
series = {ICSE '03}
}

@inproceedings{10.1145/3321408.3326676,
author = {Yan, Liu and Hu, Wenxin and Han, Longzhe},
title = {Optimize SPL test cases with adaptive simulated annealing genetic algorithm},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3326676},
doi = {10.1145/3321408.3326676},
abstract = {In Software Product Line (SPL) testing, reduced test suite with high coverage is useful for early features interaction detection. sGA (simplified genetic algorithm) and SAGA(simulated annealing genetic algorithm) can generate high coverage test suite. However, small probability mutations in updating test suite may reduce search efficiency and thus miss better solutions. An improved test cases generation method based on ASAGA (Adaptive simulated annealing genetic algorithm) is proposed. Experiments on SPLOT (Software Product Lines Online Tools) feature models show that the proposed hybrid ASAGA method can ensure local optimization accuracy and achieve smaller-size test suite with higher coverage.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {148},
numpages = {7},
keywords = {test case, software test, similarity measurement, feature model, ASAGA},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1007/978-3-030-68007-7_9,
author = {Reis, Thoralf and Bornschlegl, Marco X. and Hemmje, Matthias L.},
title = {AI2VIS4BigData: Qualitative Evaluation of an AI-Based Big Data Analysis and Visualization Reference Model},
year = {2020},
isbn = {978-3-030-68006-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-68007-7_9},
doi = {10.1007/978-3-030-68007-7_9},
abstract = {AI2VIS4BigData is a reference model for Artificial Intelligence (AI) - based Big Data Analysis and Visualization that provides uniform terminology and logical entity-relationships to scientists and professionals working in this application domain. It thereby enables re-utilization of concepts and software, prevents reinventing the wheel and facilitates collaboration scenarios. AI2VIS4BigData was systematically derived from two foundation reference models utilizing reasoned assumptions. These assumptions required subjective decisions which were not evaluated right away. This publication targets to change that through presenting two qualitative evaluation approaches that were conducted in the course of an official satellite workshop of an international conference. Selected scientific and industrial experts participated thereby in an expert round table workshop and a survey. The validation results confirm the reference model’s practical applicability and legitimate the substantial majority of subjective decisions that were taken in the course of the reference model derivation. This publication concludes with outlining five research fields for future work that comprise the non-validated subjective decisions.},
booktitle = {Advanced Visual Interfaces. Supporting Artificial Intelligence and Big Data Applications: AVI 2020 Workshops, AVI-BDA and ITAVIS, Ischia, Italy, June 9, 2020 and September 29, 2020, Revised Selected Papers},
pages = {136–162},
numpages = {27},
keywords = {Visualization, Big data analysis, AI, Reference model, Evaluation, AI2VIS4BigData}
}

@inproceedings{10.1007/978-3-030-62463-7_33,
author = {Lei, Tianwei and Xue, Jingfeng and Han, Weijie},
title = {Cross-Project Software Defect Prediction Based on Feature Selection and Transfer Learning},
year = {2020},
isbn = {978-3-030-62462-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-62463-7_33},
doi = {10.1007/978-3-030-62463-7_33},
abstract = {Cross-project software defect prediction solves the problem that traditional defect prediction can’t get enough data, but how to apply the model learned from the data of different mechanisms to the target data set is a new problem. At the same time, there is the problem that information redundancy in the training process leads to low accuracy. Based on the difference of projects, this paper uses MIC to filter features to solve the problem of information redundancy. At the same time, combined with the TrAdaboost algorithm, which is based on the idea of aggravating multiple classification error samples, this paper proposes a cross-project software prediction method based on feature selection and migration learning. Experimental results show that the algorithm proposed in this paper has better experimental results on AUC and F1.},
booktitle = {Machine Learning for Cyber Security: Third International Conference, ML4CS 2020, Guangzhou, China, October 8–10, 2020, Proceedings, Part III},
pages = {363–371},
numpages = {9},
keywords = {Cross-project software defect prediction, MIC, TrAdaboost, Transfer learning},
location = {Guangzhou, China}
}

@phdthesis{10.5555/AAI29100061,
author = {Hardalupas, Mahi},
advisor = {David, Plaut, and David, Danks, and Sandra, Mitchell, and Mazviita, Chirimuuta, and Colin, Allen,},
title = {How Neural is a Neural Net? Bio-Inspired Computational Models and Their Impact on the Multiple Realization Debate},
year = {2021},
isbn = {9798426887909},
publisher = {University of Pittsburgh},
address = {USA},
abstract = {My dissertation introduces a new account of multiple realization called 'engineered multiple realization' and applies it to cases of artificial intelligence research in computational neuroscience. Multiple realization has had an illustrious philosophical history, where multiple realization is when a higher-level (psychological) kind can be realized by several different lower-level (physical) kinds. There are two threads in the multiple realization literature: one situated in philosophy of mind and the other in philosophy of science. In philosophy of mind, multiple realization is typically seen as arbitrating a debate between metaphysical accounts of the mind, namely functionalism and identity theory. Philosophers of science look to how multiple realization is connected to scientific practice, but many question what it is useful for outside of philosophy of mind.My dissertation addresses this gap by drawing on cases from machine learning and computational neuroscience to show there is a useful form of multiple realization based on engineering practice. It differs from previous accounts in three ways. First, it reintroduces the link between engineering and multiple realization, which has been mostly neglected in current debates. Second, it is explicitly perspectival, where what counts as multiple realization depends on your perspective. Third, it locates the utility of engineered multiple realization in its ability to support constraint-based reasoning in science. This provides an answer to concerns about the utility of multiple realization in the philosophy of science literature and explains how deep neural networks can provide understanding of the brain. The first half of this dissertation proposes my account of Engineered Multiple Realization and applies it to scientific cases. The second half considers implications and connections to the modelling literature.},
note = {AAI29100061}
}

@inproceedings{10.1145/3474624.3476016,
author = {Bezerra, Carla and Lima, Rafael and Silva, Publio},
title = {DyMMer 2.0: A Tool for Dynamic Modeling and Evaluation of Feature Model},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476016},
doi = {10.1145/3474624.3476016},
abstract = {Managing dynamic variability has motivated several researchers to combine Dynamic Software Product Lines (DSPLs) practices with runtime variability mechanisms. By combining these approaches, a DSPL acquires important features, ranging from the ability to reconfigure by changing the context, adding or removing features, crash recovery, and re-adaptation based on changes in the model’s features. Feature model (FM) is an important artifact of a DPSL and there is a lack of tools that support the modeling of this artifact. We have extended the DyMMer tool for modeling FM of DSPLs from an adaptation mechanism based on MAPE-K to solve this problem. We migrated the DyMMer tool to a web version and incorporated new features: (i) modeling of FMs from SPLs and DSPLs, (ii) development of an adaptation mechanism for FM of DSPLs, (iii) repository of FMs, (iv) inclusion of thresholds for measures, and (v) user authentication. We believe that this tool is useful for research in the area of DSPLs, and also for dynamic domain modeling and evaluation. Video: https://youtu.be/WVHW6bI8ois},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {121–126},
numpages = {6},
keywords = {Modeling, Feature Model, Dynamic Software Product Line},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/3427921.3450243,
author = {Samoaa, Hazem and Leitner, Philipp},
title = {An Exploratory Study of the Impact of Parameterization on JMH Measurement Results in Open-Source Projects},
year = {2021},
isbn = {9781450381949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427921.3450243},
doi = {10.1145/3427921.3450243},
abstract = {The Java Microbenchmarking Harness (JMH) is a widely used tool for testing performance-critical code on a low level. One of the key features of JMH is the support for user-defined parameters, which allows executing the same benchmark with different workloads. However, a benchmark configured with n parameters with m different values each requires JMH to execute the benchmark mn times (once for each combination of configured parameter values). Consequently, even fairly modest parameterization leads to a combinatorial explosion of benchmarks that have to be executed, hence dramatically increasing execution time. However, so far no research has investigated how this type of parameterization is used in practice, and how important different parameters are to benchmarking results. In this paper, we statistically study how strongly different user parameters impact benchmark measurements for 126 JMH benchmarks from five well-known open source projects. We show that 40% of the studied metric parameters have no correlation with the resulting measurement, i.e., testing with different values in these parameters does not lead to any insights. If there is a correlation, it is often strongly predictable following a power law, linear, or step function curve. Our results provide a first understanding of practical usage of user-defined JMH parameters, and how they correlate with the measurements produced by benchmarks. We further show that a machine learning model based on Random Forest ensembles can be used to predict the measured performance of an untested metric parameter value with an accuracy of 93% or higher for all but one benchmark class, demonstrating that given sufficient training data JMH performance test results for different parameterizations are highly predictable.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {213–224},
numpages = {12},
keywords = {machine learning, java microbenchmarking harness (JMH), benchmark parametrization, benchmark measurements},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.1016/j.infsof.2020.106380,
author = {Mo, Ran and Yin, Zhen},
title = {Exploring software bug-proneness based on evolutionary clique modeling and analysis},
year = {2020},
issue_date = {Dec 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {128},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106380},
doi = {10.1016/j.infsof.2020.106380},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {10},
keywords = {Co-change analysis, Mining repository, Software bug-proneness, Software design}
}

@inproceedings{10.1109/WAIN52551.2021.00015,
author = {Lavazza, Luigi and Morasca, Sandro},
title = {Understanding and Modeling AI-Intensive System Development},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00015},
doi = {10.1109/WAIN52551.2021.00015},
abstract = {Developers of AI-Intensive Systems—i.e., systems that involve both “traditional” software and Artificial Intelligence—are recognizing the need to organize development systematically and use engineered methods and tools. Since an AI-Intensive System (AIIS) relies heavily on software, it is expected that Software Engineering (SE) methods and tools can help. However, AIIS development differs from the development of “traditional” software systems in a few substantial aspects. Hence, traditional SE methods and tools are not suitable or sufficient by themselves and need to be adapted and extended.A quest for “SE for AI” methods and tools has started. We believe that, in this effort, we should learn from experience and avoid repeating some of the mistakes made in the quest for SE in past years. To this end, a fundamental instrument is a set of concepts and a notation to deal with AIIS and the problems that characterize their development processes.In this paper, we propose to describe AIIS via a notation that was proposed for SE and embeds a set of concepts that are suitable to represent AIIS as well. We demonstrate the usage of the notation by modeling some characteristics that are particularly relevant for AIIS.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {55–61},
numpages = {7},
location = {Madrid, Spain}
}

@article{10.1007/s00034-021-01657-1,
author = {Pravin, Sheena Christabel and Palanivelan, M.},
title = {A Hybrid Deep Ensemble for Speech Disfluency Classification},
year = {2021},
issue_date = {Aug 2021},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {40},
number = {8},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-021-01657-1},
doi = {10.1007/s00034-021-01657-1},
abstract = {In this paper, a novel Hybrid Deep Ensemble (HDE) is proposed for automatic speech disfluency classification on a sparse speech dataset. Categorizations of speech disfluencies for diagnosis of speech disorders have so long relied on sophisticated deep learning models. Such a task can be accomplished by a straightforward approach with high accuracy by the proposed model which is an optimal combination of diverse machine learning and deep learning algorithms in a hierarchical arrangement which includes a deep autoencoder that yields the compressed latent features. The proposed model has shown considerable improvement in downgrading processing time overcoming the issues of cumbersome hyper-parameter tuning and huge data demand of the deep learning algorithms with high classification accuracy. Experimental results show that the proposed Hybrid Deep Ensemble has superior performance compared to the individual base learners, and the deep neural network as well. The proposed model and the baseline models were evaluated in terms of Cohen’s kappa coefficient, Hamming loss, Jaccard score, F-score and classification accuracy.},
journal = {Circuits Syst. Signal Process.},
month = aug,
pages = {3968–3995},
numpages = {28},
keywords = {Latent features, Deep autoencoder, Sparse speech dataset, Speech disfluency classification, Hybrid Deep Ensemble}
}

@article{10.1016/j.jss.2018.05.069,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-adaptation of service compositions through product line reconfiguration},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.05.069},
doi = {10.1016/j.jss.2018.05.069},
journal = {J. Syst. Softw.},
month = oct,
pages = {84–105},
numpages = {22},
keywords = {Self adaptation, Software product lines, Feature model, Service composition}
}

@article{10.1007/s11063-020-10286-9,
author = {Li, Li and Zhao, Kaiyi and Li, Sicong and Sun, Ruizhi and Cai, Saihua},
title = {Extreme Learning Machine for Supervised Classification with Self-paced Learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10286-9},
doi = {10.1007/s11063-020-10286-9},
abstract = {The extreme learning machine (ELM), a typical machine learning algorithm based on feedforward neural network, has been widely used in classification, clustering, regression and feature learning. However, the traditional ELM learns all samples at once, and sample weights of traditional methods are defined before the learning process and they will not change during the learning process. So, its performance is vulnerable to noisy data and outliers, finding a way to solve this problem is meaningful. In this work, we propose a model of self-paced ELM named SP-ELM for binary classification and multi-classification originated from the self-paced learning paradigm. Concretely, the algorithm takes the importance of samples into account according to the loss of predicted value and real value, and it establishes the model from the simple samples to complex samples. By setting certain restrictions, the influence of complex data on the model is reduced. Four different self-paced regularization terms are adopted in the paper to select the instances. Experimental results demonstrate the effectiveness and of the proposed method by comparing it with other improved ELMs.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1723–1744},
numpages = {22},
keywords = {Accuracy, Self-paced learning, Extreme learning machine, Classification}
}

@article{10.1007/s10009-019-00528-0,
author = {Dimovski, Aleksandar S.},
title = {CTL⋆ family-based model checking using variability abstractions and modal transition systems},
year = {2020},
issue_date = {Feb 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {1},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-019-00528-0},
doi = {10.1007/s10009-019-00528-0},
abstract = {Variational systems can produce a (potentially huge) number of related systems, known as products or variants, by using features (configuration options) to mark the variable functionality. In many of the application domains, their rigorous verification and analysis are very important, yet the appropriate tools rarely are able to analyse variational systems. Recently, this problem was addressed by designing specialized so-called family-based model checking algorithms, which allow simultaneous verification of all variants in a single run by exploiting the commonalities between the variants. Yet, their computational cost still greatly depends on the number of variants (the size of configuration space), which is often huge. Moreover, their implementation and maintenance represent a costly research and development task. One of the most promising approaches to fighting the configuration space explosion problem is variability abstractions, which simplify variability away from variational systems. In this work, we show how to achieve efficient family-based model checking of CTL⋆ temporal properties using variability abstractions and off-the-shelf (single-system) tools. We use variability abstractions for deriving abstract family-based model checking, where the variability model of a variational system is replaced with an abstract (smaller) version of it, called modal transition system, which preserves the satisfaction of both universal and existential temporal properties, as expressible in CTL⋆. Modal transition systems contain two kinds of transitions, termed may- and must-transitions, which are defined by the conservative (over-approximating) abstractions and their dual (under-approximating) abstractions, respectively. The variability abstractions can be combined with different partitionings of the configuration space to infer suitable divide-and-conquer verification plans for the given variational system. We illustrate the practicality of this approach for several variational systems using the standard version of (single-system) NuSMV model checker.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = feb,
pages = {35–55},
numpages = {21},
keywords = {CTL* temporal logic, Featured transition systems, Modal transition systems, Abstract interpretation, Family-based model checking, Software product line engineering}
}

@inproceedings{10.5555/3060832.3060891,
author = {Pi, Te and Li, Xi and Zhang, Zhongfei and Meng, Deyu and Wu, Fei and Xiao, Jun and Zhuang, Yueting},
title = {Self-paced boost learning for classification},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Effectiveness and robustness are two essential aspects of supervised learning studies. For effective learning, ensemble methods are developed to build a strong effective model from ensemble of weak models. For robust learning, self-paced learning (SPL) is proposed to learn in a self-controlled pace from easy samples to complex ones. Motivated by simultaneously enhancing the learning effectiveness and robustness, we propose a unified framework, Self-Paced Boost Learning (SPBL). With an adaptive from-easy-to-hard pace in boosting process, SPBL asymptotically guides the model to focus more on the insufficiently learned samples with higher reliability. Via a max-margin boosting optimization with self-paced sample selection, SPBL is capable of capturing the intrinsic inter-class discriminative patterns while ensuring the reliability of the samples involved in learning. We formulate SPBL as a fully-corrective optimization for classification. The experiments on several real-world datasets show the superiority of SPBL in terms of both effectiveness and robustness.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1932–1938},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@article{10.5555/2946645.3053434,
author = {Szab\'{o}, Zolt\'{a}n and Sriperumbudur, Bharath K. and P\'{o}czos, Barnab\'{a}s and Gretton, Arthur},
title = {Learning theory for distribution regression},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax optimal rate (Caponnetto and De Vito, 2007; Steinwart et al., 2009). This result answers a 17-year-old open question, establishing the consistency of the classical set kernel (Haussler, 1999; G\"{a}rtner et al., 2002) in regression. We also cover consistency for more recent kernels on distributions, including those due to Christmann and Steinwart (2010).},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5272–5311},
numpages = {40},
keywords = {two-Stage sampled distribution regression, multi-instance learning, minimax optimality, mean embedding, Kernel ridge regression}
}

@inproceedings{10.1109/WAIN52551.2021.00027,
author = {Lanubile, Filippo and Calefato, Fabio and Quaranta, Luigi and Amoruso, Maddalena and Fumarola, Fabio and Filannino, Michele},
title = {Towards Productizing AI/ML Models: An Industry Perspective from Data Scientists},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00027},
doi = {10.1109/WAIN52551.2021.00027},
abstract = {The transition from AI/ML models to production-ready AI-based systems is a challenge for both data scientists and software engineers. In this paper, we report the results of a workshop conducted in a consulting company to understand how this transition is perceived by practitioners. Starting from the need for making AI experiments reproducible, the main themes that emerged are related to the use of the Jupyter Notebook as the primary prototyping tool, and the lack of support for software engineering best practices as well as data science specific functionalities.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {129–132},
numpages = {4},
location = {Madrid, Spain}
}

@inproceedings{10.5555/3524938.3524952,
author = {Ahn, Sungsoo and Seo, Younggyo and Shin, Jinwoo},
title = {Learning what to defer for maximum independent sets},
year = {2020},
publisher = {JMLR.org},
abstract = {Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the number of elements in the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme, coined learning what to defer (LwD), where the agent adaptively shrinks or stretch the number of stages by learning to distribute the element-wise decisions of the solution at each stage. We apply the proposed framework to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {14},
numpages = {11},
series = {ICML'20}
}

@inproceedings{10.1007/978-3-030-86380-7_23,
author = {Krysi\'{n}ska, Izabela and Morzy, Miko\l{}aj and Kajdanowicz, Tomasz},
title = {Curriculum Learning Revisited: Incremental Batch Learning with Instance Typicality Ranking},
year = {2021},
isbn = {978-3-030-86379-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86380-7_23},
doi = {10.1007/978-3-030-86380-7_23},
abstract = {The technique of curriculum learning mimics cognitive mechanisms observed in human learning, where simpler concepts are presented prior to gradual introduction of more difficult concepts. Until now, the major obstacle for curriculum methods was the lack of a reliable method for estimating the difficulty of training instances. In this paper we show that, instead of trying to assess the difficulty of learning instances, a simple graph-based method of computing the typicality of instances can be used in conjunction with curriculum methods. We design new batch schedulers which organize ordered instances into batches of varying size and learning difficulty. Our method does not require any changes to the architecture of trained models, we improve the training merely by manipulating the order and frequency of instance presentation to the model.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part IV},
pages = {279–291},
numpages = {13},
keywords = {Batch training, Typicality, Curriculum learning},
location = {Bratislava, Slovakia}
}

@article{10.1016/j.neucom.2019.11.001,
author = {Li, Huafeng and Zhou, Weiyan and Yu, Zhengtao and Yang, Biao and Jin, Huaiping},
title = {Person re-identification with dictionary learning regularized by stretching regularization and label consistency constraint},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {379},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.001},
doi = {10.1016/j.neucom.2019.11.001},
journal = {Neurocomput.},
month = feb,
pages = {356–369},
numpages = {14},
keywords = {Stretch regularization, Label consistency constraint, Dictionary learning, Person re-identification}
}

@inproceedings{10.1145/2647908.2655969,
author = {ter Beek, Maurice H. and Mazzanti, Franco},
title = {VMC: recent advances and challenges ahead},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655969},
doi = {10.1145/2647908.2655969},
abstract = {The variability model checker VMC accepts a product family specified as a Modal Transition System (MTS) with additional variability constraints. Consequently, it offers behavioral variability analyses over both the family and its valid product behavior. This ranges from product derivation and simulation to efficient on-the-fly model checking of logical properties expressed in a variability-aware version of action-based CTL. In this paper, we first explain the reasons and assumptions underlying the choice for a modeling and analysis framework based on MTSs. Subsequently, we present recent advances on proving inheritance of behavioral analysis properties from a product family to its valid products. Finally, we illustrate challenges remaining for the future.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {70–77},
numpages = {8},
keywords = {product families, model checking, behavioral variability},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1016/j.infsof.2019.07.003,
author = {Zhou, Tianchi and Sun, Xiaobing and Xia, Xin and Li, Bin and Chen, Xiang},
title = {Improving defect prediction with deep forest},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.07.003},
doi = {10.1016/j.infsof.2019.07.003},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {204–216},
numpages = {13},
keywords = {Empirical evaluation, Cascade strategy, Deep forest, Software defect prediction}
}

@inproceedings{10.1145/3416505.3423564,
author = {Borovits, Nemania and Kumara, Indika and Krishnan, Parvathy and Palma, Stefano Dalla and Di Nucci, Dario and Palomba, Fabio and Tamburri, Damian A. and van den Heuvel, Willem-Jan},
title = {DeepIaC: deep learning-based linguistic anti-pattern detection in IaC},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423564},
doi = {10.1145/3416505.3423564},
abstract = {Linguistic anti-patterns are recurring poor practices concerning inconsistencies among the naming, documentation, and implementation of an entity. They impede readability, understandability, and maintainability of source code. This paper attempts to detect linguistic anti-patterns in infrastructure as code (IaC) scripts used to provision and manage computing environments. In particular, we consider inconsistencies between the logic/body of IaC code units and their names. To this end, we propose a novel automated approach that employs word embeddings and deep learning techniques. We build and use the abstract syntax tree of IaC code units to create their code embedments. Our experiments with a dataset systematically extracted from open source repositories show that our approach yields an accuracy between 0.785 and 0.915 in detecting inconsistencies.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {Word2Vec, Linguistic Anti-patterns, Infrastructure Code, IaC, Defects, Deep Learning, Code Embedding},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@inproceedings{10.5555/1753235.1753245,
author = {Cetina, Carlos and Haugen, \O{}ystein and Zhang, Xiaorui and Fleurey, Franck and Pelechano, Vicente},
title = {Strategies for variability transformation at run-time},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {More and more approaches propose to use Software Product Lines (SPLs) modelling techniques to implement dynamic adaptive systems. The resulting Dynamic Software Product Lines (DSPLs) present new challenges since the variability transformations used to derive alternative configurations have to be intensively used at runtime. This paper proposes to use the Common Variability Language (CVL) for modelling runtime variability and evaluates a set of alternative strategies for implementing the associated variability transformations. All the proposed strategies have been implemented and evaluated on the case-study of a smart-home system. Results show that the proposed strategies provide the same reconfiguration service with significant differences in quality-of-service.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {61–70},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.specom.2019.10.003,
author = {Stasak, Brian and Epps, Julien and Goecke, Roland},
title = {Automatic depression classification based on affective read sentences: Opportunities for text-dependent analysis},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.003},
doi = {10.1016/j.specom.2019.10.003},
journal = {Speech Commun.},
month = dec,
pages = {1–14},
numpages = {14},
keywords = {Valence, Speech elicitation, Machine learning, Paralinguistics, Digital medicine, Digital phenotyping}
}

@inproceedings{10.5555/3504035.3504406,
author = {Gong, Tieliang and Wang, Guangtao and Ye, Jieping and Xu, Zongben and Lin, Ming},
title = {Margin based PU learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {The PU learning problem concerns about learning from positive and unlabeled data. A popular heuristic is to iteratively enlarge training set based on some margin-based criterion. However, little theoretical analysis has been conducted to support the success of these heuristic methods. In this work, we show that not all margin-based heuristic rules are able to improve the learned classifiers iteratively. We find that a so-called large positive margin oracle is necessary to guarantee the success of PU learning. Under this oracle, a provable positive-margin based PU learning algorithm is proposed for linear regression and classification under the truncated Gaussian distributions. The proposed algorithm is able to reduce the recovering error geometrically proportional to the positive margin. Extensive experiments on real-world datasets verify our theory and the state-of-the-art performance of the proposed PU learning algorithm.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {371},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1016/j.artint.2016.09.001,
author = {Bessiere, Christian and Fargier, Hlne and Lecoutre, Christophe},
title = {Computing and restoring global inverse consistency in interactive constraint satisfaction},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {241},
number = {C},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2016.09.001},
doi = {10.1016/j.artint.2016.09.001},
abstract = {Some applications require the interactive resolution of a constraint problem by a human user. In such cases, it is highly desirable that the person who interactively solves the problem is not given the choice to select values that do not lead to solutions. We call this property global inverse consistency. Existing systems simulate this either by maintaining arc consistency after each assignment performed by the user or by compiling offline the problem as a multi-valued decision diagram. In this article, we define several questions related to global inverse consistency and analyze their complexity. Despite their theoretical intractability, we propose several algorithms for enforcing and restoring global inverse consistency and we show that the best version is efficient enough to be used in an interactive setting on several configuration and design problems.},
journal = {Artif. Intell.},
month = dec,
pages = {153–169},
numpages = {17},
keywords = {Global inverse consistency, Constraint satisfaction problems, Configuration}
}

@inproceedings{10.5555/3524938.3525259,
author = {Geng, Sinong and Nassif, Houssam and Manzanares, Carlos A. and Reppen, A. Max and Sircar, Ronnie},
title = {Deep PQR: solving inverse reinforcement learning using anchor actions},
year = {2020},
publisher = {JMLR.org},
abstract = {We propose a reward function estimation framework for inverse reinforcement learning with deep energy-based policies. We name our method PQR, as it sequentially estimates the Policy, the Q- function, and the Reward function by deep learning. PQR does not assume that the reward solely depends on the state, instead it allows for a dependency on the choice of action. Moreover, PQR allows for stochastic state transitions. To accomplish this, we assume the existence of one anchor action whose reward is known, typically the action of doing nothing, yielding no reward. We present both estimators and algorithms for the PQR method. When the environment transition is known, we prove that the PQR reward estimator uniquely recovers the true reward. With unknown transitions, we bound the estimation error of PQR. Finally, the performance of PQR is demonstrated by synthetic and real-world datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {321},
numpages = {11},
series = {ICML'20}
}

@article{10.1145/3477428,
author = {Liu, Liu and Isaacman, Sibren and Kremer, Ulrich},
title = {An Adaptive Application Framework with Customizable Quality Metrics},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/3477428},
doi = {10.1145/3477428},
abstract = {Many embedded environments require applications to produce outcomes under different, potentially changing, resource constraints. Relaxing application semantics through approximations enables trading off resource usage for outcome quality. Although quality is a highly subjective notion, previous work assumes given, fixed low-level quality metrics that often lack a strong correlation to a user’s higher-level quality experience. Users may also change their minds with respect to their quality expectations depending on the resource budgets they are willing to dedicate to an execution. This motivates the need for an adaptive application framework where users provide execution budgets and a customized quality notion. This article presents a novel adaptive program graph representation that enables user-level, customizable quality based on basic quality aspects defined by application developers. Developers also define application configuration spaces, with possible customization to eliminate undesirable configurations. At runtime, the graph enables the dynamic selection of the configuration with maximal customized quality within the user-provided resource budget.An adaptive application framework based on our novel graph representation has been implemented on Android and Linux platforms and evaluated on eight benchmark programs, four with fully customizable quality. Using custom quality instead of the default quality, users may improve their subjective quality experience value by up to 3.59\texttimes{}, with 1.76\texttimes{} on average under different resource constraints. Developers are able to exploit their application structure knowledge to define configuration spaces that are on average 68.7% smaller as compared to existing, structure-oblivious approaches. The overhead of dynamic reconfiguration averages less than 1.84% of the overall application execution time.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = nov,
articleno = {13},
numpages = {33},
keywords = {QoS, configuration management, Approximate computing}
}

@inproceedings{10.1609/aaai.v33i01.33015117,
author = {Tang, Ying-Peng and Huang, Sheng-Jun},
title = {Self-paced active learning: query the right thing at the right time},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33015117},
doi = {10.1609/aaai.v33i01.33015117},
abstract = {Active learning queries labels from the oracle for the most valuable instances to reduce the labeling cost. In many active learning studies, informative and representative instances are preferred because they are expected to have higher potential value for improving the model. Recently, the results in self-paced learning show that training the model with easy examples first and then gradually with harder examples can improve the performance. While informative and representative instances could be easy or hard, querying valuable but hard examples at early stage may lead to waste of labeling cost. In this paper, we propose a self-paced active learning approach to simultaneously consider the potential value and easiness of an instance, and try to train the model with least cost by querying the right thing at the right time. Experimental results show that the proposed approach is superior to state-of-the-art batch mode active learning methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {628},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1109/SPLC.2008.28,
author = {Chae, Wonseok and Blume, Matthias},
title = {Building a Family of Compilers},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.28},
doi = {10.1109/SPLC.2008.28},
abstract = {We have developed and maintained a set of closely related compilers. Although much of their code is duplicated and shared, they have been maintained separately because they are treated as different compilers. Even if they were merged together, the combined code would become too complicated to serve as the base for another extension. We describe our experience to address this problem by adopting the product line engineering paradigm to build a family of compilers. This paradigm encourages developers to focus on developing a set of compilers rather than on developing one particular compiler. We show engineering activities for a family of compilers from product line analysis through product line architecture design to product line component design. Then, we present how to build particular compilers from core assets resulting from the previous activities and how to take advantage of modern programming language technology to organize this task. Our experience demonstrates that the product line engineering as a developing paradigm can ease the construction of a family of compilers.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {307–316},
numpages = {10},
keywords = {standard ml, product line engineering, module system, feature-oriented, compilers},
series = {SPLC '08}
}

@inproceedings{10.1007/978-3-030-73197-7_29,
author = {Du, Yuntao and Chen, Yinghao and Cui, Fengli and Zhang, Xiaowen and Wang, Chongjun},
title = {Cross-Domain Error Minimization for Unsupervised Domain Adaptation},
year = {2021},
isbn = {978-3-030-73196-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73197-7_29},
doi = {10.1007/978-3-030-73197-7_29},
abstract = {Unsupervised domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Previous methods focus on learning domain-invariant features to decrease the discrepancy between the feature distributions as well as minimizing the source error and have made remarkable progress. However, a recently proposed theory reveals that such a strategy is not sufficient for a successful domain adaptation. It shows that besides a small source error, both the discrepancy between the feature distributions and the discrepancy between the labeling functions should be small across domains. The discrepancy between the labeling functions is essentially the cross-domain errors which are ignored by existing methods. To overcome this issue, in this paper, a novel method is proposed to integrate all the objectives into a unified optimization framework. Moreover, the incorrect pseudo labels widely used in previous methods can lead to error accumulation during learning. To alleviate this problem, the pseudo labels are obtained by utilizing structural information of the target domain besides source classifier and we propose a curriculum learning based strategy to select the target samples with more accurate pseudo-labels during training. Comprehensive experiments are conducted, and the results validate that our approach outperforms state-of-the-art methods.},
booktitle = {Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11–14, 2021, Proceedings, Part II},
pages = {429–448},
numpages = {20},
keywords = {Cross-domain errors, Domain adaptation, Transfer learning},
location = {Taipei, Taiwan}
}

@article{10.1016/j.ins.2019.12.015,
author = {Xiao, Yanshan and Yang, Xiaozhou and Liu, Bo},
title = {A new self-paced method for multiple instance boosting learning},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {515},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.12.015},
doi = {10.1016/j.ins.2019.12.015},
journal = {Inf. Sci.},
month = apr,
pages = {80–90},
numpages = {11},
keywords = {Self-Paced learning, Multiple instance boost learning, Multiple instance learning}
}

@inproceedings{10.1145/2647908.2655961,
author = {Seidl, Christoph and Domachowska, Irena},
title = {Teaching variability engineering to cognitive psychologists},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655961},
doi = {10.1145/2647908.2655961},
abstract = {In research of cognitive psychology, experiments to measure cognitive processes may be run in many similar yet slightly different configurations. Variability engineering offers techniques to handle variable configurations both conceptually and technically. However, these techniques are largely unknown to cognitive psychologists so that experiment configurations are specified informally or too coarse grain. This is problematic, because it becomes difficult to get an overview of paradigm configurations used in the so far conducted experiments. Variability engineering techniques provide, i.a., concise notations for capturing variability in software and can also be used to express the configurable nature of a wide range of experiments in cognitive psychology. Furthermore, it enables cognitive psychologists to structure configuration knowledge, to identify suitably similar experiment setups and to more efficiently identify individual configuration options as relevant reasons for a particular effect in the outcome of an experiment. In this paper, we present experiences with teaching variability engineering to cognitive psychologists along with a suitable curriculum.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {16–23},
numpages = {8},
keywords = {variability engineering, teaching, feature model, cognitive psychology},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1007/978-3-030-68851-6_29,
author = {Baloch, Muhammad Zubair and Hussain, Shahid and Afzal, Humaira and Mufti, Muhammad Rafiq and Ahmad, Bashir},
title = {Software Developer Recommendation in Terms of Reducing Bug Tossing Length},
year = {2020},
isbn = {978-3-030-68850-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-68851-6_29},
doi = {10.1007/978-3-030-68851-6_29},
abstract = {In close software development, it is easy for the project manager to recommend the right developer to resolve a bug that is reported by an end-user. However, in the case of open-source software developments, where most developers are engaged on different project either on the same or different repositories. Due to their agile involvement on repositories, bug triaging might be slow and increases the Bug Tossing Length (BTL) which is encounter as the time between reporting and resolving bugs. In open-source software repositories like GitHub, numerous developers are involved with well-known projects to resolve the issue reported by end-users. The assignment of the reported bug to an appropriate developer may lead to a reduced BTL time. Though, several metrics based and Machine Learning (ML) based approaches have been introduced to recommend the appropriate developer on the bases of several parameters. However, few studies are related to the recommendation of developers on the bases of their historical information regarding their attempts to reduce the BTL. To address this issue, we have proposed a new approach to recommend a developer for bug triaging on the bases of their involvement in reducing the BTL. In the proposed study, the model is trained once and new bug reports are automatically assigned to relevant developers. In this regard, we exploit the proposed methodology through using the XGBoost, Support Vector Machine, Random Forest, Decision Tree, KNearest Neighbor, and Na\"{\i}ve Bayes for the recommendation of the developer for a reported bug. We used widely-known two datasets namely Eclipse, and Mozilla. The experimental result indicate the effectiveness of proposed methodology in terms of developer recommendation for a new reported bug.},
booktitle = {Security, Privacy, and Anonymity in Computation, Communication, and Storage: 13th International Conference, SpaCCS 2020, Nanjing, China, December 18-20, 2020, Proceedings},
pages = {396–407},
numpages = {12},
keywords = {Machine learning, Bugs report, Github, Open source software, Recommendation},
location = {Nanjing, China}
}

@inproceedings{10.1145/3379597.3387513,
author = {Antal, G\'{a}bor and Keleti, M\'{a}rton and Hegedundefineds, P\'{e}ter},
title = {Exploring the Security Awareness of the Python and JavaScript Open Source Communities},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387513},
doi = {10.1145/3379597.3387513},
abstract = {Software security is undoubtedly a major concern in today's software engineering. Although the level of awareness of security issues is often high, practical experiences show that neither preventive actions nor reactions to possible issues are always addressed properly in reality. By analyzing large quantities of commits in the open-source communities, we can categorize the vulnerabilities mitigated by the developers and study their distribution, resolution time, etc. to learn and improve security management processes and practices.With the help of the Software Heritage Graph Dataset, we investigated the commits of two of the most popular script languages - Python and JavaScript - projects collected from public repositories and identified those that mitigate a certain vulnerability in the code (i.e. vulnerability resolution commits). On the one hand, we identified the types of vulnerabilities (in terms of CWE groups) referred to in commit messages and compared their numbers within the two communities. On the other hand, we examined the average time elapsing between the publish date of a vulnerability and the first reference to it in a commit.We found that there is a large intersection in the vulnerability types mitigated by the two communities, but most prevalent vulnerabilities are specific to language. Moreover, neither the JavaScript nor the Python community reacts very fast to appearing security vulnerabilities in general with only a couple of exceptions for certain CWE groups.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {16–20},
numpages = {5},
keywords = {vulnerability, software security, Python, JavaScript, CWE, CVE},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1007/978-3-030-43680-3_11,
author = {Saber, Takfarinas and Brevet, David and Botterweck, Goetz and Ventresque, Anthony},
title = {MILPIBEA: Algorithm for&nbsp;Multi-objective Features Selection in&nbsp;(Evolving) Software Product Lines},
year = {2020},
isbn = {978-3-030-43679-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-43680-3_11},
doi = {10.1007/978-3-030-43680-3_11},
abstract = {Software Product Lines Engineering (SPLE) proposes techniques to model, create and improve groups of related software systems in a systematic way, with different alternatives formally expressed, e.g., as Feature Models. Selecting the ‘best’ software system(s) turns into a problem of improving the quality of selected subsets of software features (components) from feature models, or as it is widely known, Feature Configuration. When there are different independent dimensions to assess how good a software product is, the problem becomes even more challenging – it is then a multi-objective optimisation problem. Another big issue for software systems is evolution where software components change. This is common in the industry but, as far as we know, there is no algorithm designed to the particular case of multi-objective optimisation of evolving software product lines. In this paper we present MILPIBEA, a novel hybrid algorithm which combines the scalability of a genetic algorithm (IBEA) with the accuracy of a mixed-integer linear programming solver (IBM ILOG CPLEX). We also study the behaviour of our solution (MILPIBEA) in contrast with SATIBEA, a state-of-the-art algorithm in static software product lines. We demonstrate that MILPIBEA outperforms SATIBEA on average, especially for the most challenging problem instances, and that MILPIBEA is the one that continues to improve the quality of the solutions when SATIBEA stagnates (in the evolving context).},
booktitle = {Evolutionary Computation in Combinatorial Optimization: 20th European Conference, EvoCOP 2020, Held as Part of EvoStar 2020, Seville, Spain, April 15–17, 2020, Proceedings},
pages = {164–179},
numpages = {16},
keywords = {Mixed-integer linear programming, Evolutionary algorithm, Multi-objective optimisation, Feature selection, Software product line},
location = {Seville, Spain}
}

@inproceedings{10.1145/3468264.3468603,
author = {Ding, Yi and Pervaiz, Ahsan and Carbin, Michael and Hoffmann, Henry},
title = {Generalizable and interpretable learning for configuration extrapolation},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468603},
doi = {10.1145/3468264.3468603},
abstract = {Modern software applications are increasingly configurable, which puts a burden on users to tune these configurations for their target hardware and workloads. To help users, machine learning techniques can model the complex relationships between software configuration parameters and performance. While powerful, these learners have two major drawbacks: (1) they rarely incorporate prior knowledge and (2) they produce outputs that are not interpretable by users. These limitations make it difficult to (1) leverage information a user has already collected (e.g., tuning for new hardware using the best configurations from old hardware) and (2) gain insights into the learner’s behavior (e.g., understanding why the learner chose different configurations on different hardware or for different workloads). To address these issues, this paper presents two configuration optimization tools, GIL and GIL+, using the proposed generalizable and interpretable learning approaches. To incorporate prior knowledge, the proposed tools (1) start from known configurations, (2) iteratively construct a new linear model, (3) extrapolate better performance configurations from that model, and (4) repeat. Since the base learners are linear models, these tools are inherently interpretable. We enhance this property with a graphical representation of how they arrived at the highest performance configuration. We evaluate GIL and GIL+ by using them to configure Apache Spark workloads on different hardware platforms and find that, compared to prior work, GIL and GIL+ produce comparable, and sometimes even better performance configurations, but with interpretable results.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {728–740},
numpages = {13},
keywords = {machine learning, interpretability, generalizability, Configuration},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1007/978-3-030-52237-7_5,
author = {Carpenter, Dan and Emerson, Andrew and Mott, Bradford W. and Saleh, Asmalina and Glazewski, Krista D. and Hmelo-Silver, Cindy E. and Lester, James C.},
title = {Detecting Off-Task Behavior from Student Dialogue in Game-Based Collaborative Learning},
year = {2020},
isbn = {978-3-030-52236-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-52237-7_5},
doi = {10.1007/978-3-030-52237-7_5},
abstract = {Collaborative game-based learning environments integrate game-based learning and collaborative learning. These environments present students with a shared objective and provide them with a means to communicate, which allows them to share information, ask questions, construct explanations, and work together toward their shared goal. A key challenge in collaborative learning is that students may engage in unproductive discourse, which may affect learning activities and outcomes. Collaborative game-based learning environments that can detect this off-task behavior in real-time have the potential to enhance collaboration between students by redirecting the conversation back to more productive topics. This paper investigates the use of dialogue analysis to classify student conversational utterances as either off-task or on-task. Using classroom data collected from 13 groups of four students, we trained off-task dialogue models for text messages from a group chat feature integrated into Crystal Island: EcoJourneys, a collaborative game-based learning environment for middle school ecosystem science. We evaluate the effectiveness of the off-task dialogue models, which use different word embeddings (i.e., word2vec, ELMo, and BERT), as well as predictive off-task dialogue models that capture varying amounts of contextual information from the chat log. Results indicate that predictive off-task dialogue models that incorporate a window of recent context and represent the sequential nature of the chat messages achieve higher predictive performance compared to models that do not leverage this information. These findings suggest that off-task dialogue models for collaborative game-based learning environments can reliably recognize and predict students’ off-task behavior, which introduces the opportunity to adaptively scaffold collaborative dialogue.},
booktitle = {Artificial Intelligence in Education: 21st International Conference, AIED 2020, Ifrane, Morocco, July 6–10, 2020, Proceedings, Part I},
pages = {55–66},
numpages = {12},
keywords = {Dialogue analysis, Game-based learning environments, Collaborative game-based learning, Computer-supported collaborative learning, Off-task behavior},
location = {Ifrane, Morocco}
}

@article{10.1613/jair.1.11366,
author = {Balaban, Edward and Johnson, Stephen B. and Kochenderfer, Mykel J.},
title = {Unifying system health management and automated decision making},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11366},
doi = {10.1613/jair.1.11366},
abstract = {Health management of complex dynamic systems has evolved from simple automated alarms into a subfield of artificial intelligence with techniques for analyzing off-nominal conditions and generating responses. This evolution took place largely apart from the development of automated system control, planning, and scheduling (generally referred to in this work as decision making). While there have been efforts to establish an information exchange between system health management and decision making, successful practical implementations of integrated architectures remain limited. This article proposes that rather than being treated as connected yet distinct entities, system health management and decision making should be unified in their formulations. Enabled by advances in modeling and algorithms, we believe that a unified approach will increase systems' resilience to faults and improve their effectiveness. We overview the prevalent system health management methodology, illustrate its limitations through numerical examples, and describe a proposed unified approach. We then show how typical system health management concepts are accommodated in the proposed approach without loss of functionality or generality. A computational complexity analysis of the unified approach is also provided.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {487–518},
numpages = {32}
}

@inproceedings{10.3115/1218955.1219031,
author = {Niu, Cheng and Li, Wei and Srihari, Rohini K.},
title = {Weakly supervised learning for cross-document person name disambiguation supported by information extraction},
year = {2004},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1218955.1219031},
doi = {10.3115/1218955.1219031},
abstract = {It is fairly common that different people are associated with the same name. In tracking person entities in a large document pool, it is important to determine whether multiple mentions of the same name across documents refer to the same entity or not. Previous approach to this problem involves measuring context similarity only based on co-occurring words. This paper presents a new algorithm using information extraction support in addition to co-occurring words. A learning scheme with minimal supervision is developed within the Bayesian framework. Maximum entropy modeling is then used to represent the probability distribution of context similarities based on heterogeneous features. Statistical annealing is applied to derive the final entity coreference chains by globally fitting the pairwise context similarities. Benchmarking shows that our new approach significantly outperforms the existing algorithm by 25 percentage points in overall F-measure.},
booktitle = {Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics},
pages = {597–es},
location = {Barcelona, Spain},
series = {ACL '04}
}

@article{10.1007/s10664-021-09966-4,
author = {Tuarob, Suppawong and Assavakamhaenghan, Noppadol and Tanaphantaruk, Waralee and Suwanworaboon, Ponlakit and Hassan, Saeed-Ul and Choetkiertikul, Morakot},
title = {Automatic team recommendation for collaborative software development},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09966-4},
doi = {10.1007/s10664-021-09966-4},
abstract = {In large-scale collaborative software development, building a team of software practitioners can be challenging, mainly due to overloading choices of candidate members to fill in each role. Furthermore, having to understand all members’ diverse backgrounds, and anticipate team compatibility could significantly complicate and attenuate such a team formation process. Current solutions that aim to automatically suggest software practitioners for a task merely target particular roles, such as developers, reviewers, and integrators. While these existing approaches could alleviate issues presented by choice overloading, they fail to address team compatibility while members collaborate. In this paper, we propose RECAST, an intelligent recommendation system that suggests team configurations that satisfy not only the role requirements, but also the necessary technical skills and teamwork compatibility, given task description and a task assignee. Specifically, RECAST uses Max-Logit to intelligently enumerate and rank teams based on the team-fitness scores. Machine learning algorithms are adapted to generate a scoring function that learns from heterogenous features characterizing effective software teams in large-scale collaborative software development. RECAST is evaluated against a state-of-the-art team recommendation algorithm using three well-known open-source software project datasets. The evaluation results are promising, illustrating that our proposed method outperforms the baselines in terms of team recommendation with 646% improvement (MRR) using the exact-match evaluation protocol.},
journal = {Empirical Softw. Engg.},
month = jul,
numpages = {53},
keywords = {Machine learning, Collaborative software development, Team recommendation}
}

@inproceedings{10.1007/978-3-319-24888-2_3,
author = {Ma, Guangkai and Gao, Yaozong and Wang, Li and Wu, Ligang and Shen, Dinggang},
title = {Soft-Split Random Forest for Anatomy Labeling},
year = {2015},
isbn = {978-3-319-24887-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-24888-2_3},
doi = {10.1007/978-3-319-24888-2_3},
abstract = {Random Forest (RF) has been widely used in the learning-based labeling. In RF, each sample is directed from the root to each leaf based on the decisions made in the interior nodes, also called splitting nodes. The splitting nodes assign a testing sample to either left or right child based on the learned splitting function. The final prediction is determined as the average of label probability distributions stored in all arrived leaf nodes. For ambiguous testing samples, which often lie near the splitting boundaries, the conventional splitting function, also referred to as hard split function, tends to make wrong assignments, hence leading to wrong predictions. To overcome this limitation, we propose a novel soft-split random forest (SSRF) framework to improve the reliability of node splitting and finally the accuracy of classification. Specifically, a soft split function is employed to assign a testing sample into both left and right child nodes with their certain probabilities, which can effectively reduce influence of the wrong node assignment on the prediction accuracy. As a result, each testing sample can arrive at multiple leaf nodes, and their respective results can be fused to obtain the final prediction according to the weights accumulated along the path from the root node to each leaf node. Besides, considering the importance of context information, we also adopt a Haar-features based context model to iteratively refine the classification map. We have comprehensively evaluated our method on two public datasets, respectively, for labeling hippocampus in MR images and also labeling three organs in Head &amp; Neck CT images. Compared with the hard-split RF (HSRF), our method achieved a notable improvement in labeling accuracy.},
booktitle = {Machine Learning in Medical Imaging: 6th International Workshop, MLMI 2015, Held in Conjunction with MICCAI 2015, Munich, Germany, October 5, 2015, Proceedings},
pages = {17–25},
numpages = {9},
location = {Munich, Germany}
}

@article{10.1016/j.neucom.2019.06.075,
author = {Xue, Yani and Li, Miqing and Shepperd, Martin and Lauria, Stasha and Liu, Xiaohui},
title = {A novel aggregation-based dominance for Pareto-based evolutionary algorithms to configure software product lines},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {364},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.075},
doi = {10.1016/j.neucom.2019.06.075},
journal = {Neurocomput.},
month = oct,
pages = {32–48},
numpages = {17},
keywords = {Multi-objective optimization, Evolutionary algorithm, Software product line, Optimal feature selection}
}

@article{10.1016/j.artint.2011.10.003,
author = {Mossakowski, Till and Moratz, Reinhard},
title = {Qualitative reasoning about relative direction of oriented points},
year = {2012},
issue_date = {April, 2012},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {180–181},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2011.10.003},
doi = {10.1016/j.artint.2011.10.003},
abstract = {An important issue in qualitative spatial reasoning is the representation of relative directions. In this paper we present simple geometric rules that enable reasoning about the relative direction between oriented points. This framework, the oriented point algebra OPRA"m, has a scalable granularity m. We develop a simple algorithm for computing the OPRA"m composition tables and prove its correctness. Using a composition table, algebraic closure for a set of OPRA"m statements is very useful for solving spatial navigation tasks. It turns out that scalable granularity is useful in these navigation tasks.},
journal = {Artif. Intell.},
month = apr,
pages = {34–45},
numpages = {12},
keywords = {Qualitative spatial reasoning, Qualitative simulation, Constraint-based reasoning}
}

@inproceedings{10.1145/2975969.2975974,
author = {F\"{o}rd\H{o}s, Vikt\'{o}ria and Cesarini, Francesco},
title = {CRDTs for the configuration of distributed Erlang systems},
year = {2016},
isbn = {9781450344319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2975969.2975974},
doi = {10.1145/2975969.2975974},
abstract = {CRDT (Conflict-free replicated data type) is a data type that supports conflict free resolution of concurrent, distributed updates. It is often mentioned alongside storage systems that are distributed, fault-tolerant and reliable. These are similar properties and features of Erlang/OTP systems. What distributed Erlang/OTP systems lack, however, is a standardised way to configure multiple nodes. OTP middleware allows you to set configuration parameters called application environment variables on a node basis, they can be updated at runtime, but will not survive a restart unless persisted in the business logic of the system. There is no widely adopted solution to address this omission. In some installations, changes are done manually in the Erlang shell and persisted by editing the configuration files. In others, changes and updates are implemented as part of a new releases and deployed through an upgrade procedure. These tools expect a happy path, and rarely take network failures and consistency into consideration. As a result, issues have been known to cause outages and have left the system in an inconsistent state, with no automated means of detecting the root cause of the problem. In this paper, we introduce a configuration management approach designed for distributed Erlang/OTP systems. They are systems which often trade consistency for availability and scalability, making them a perfect fit for CRDTs. We use a proprietary tool called WombatOAM to update environment variables and check their consistency on both node and cluster-levels. Inconsistencies and failed updates are detected and reported in the form of an alarms, and the history and status of all performed changes are logged, facilitating troubleshooting and recovery efforts. In this paper, we show our approaches to configuration management, and discuss how we approached the issue of consistency in the presence of unreliable networks. We present a qualitative evaluation and a case study to assess the capabilities of WombatOAM’s CRDT based configuration management feature.},
booktitle = {Proceedings of the 15th International Workshop on Erlang},
pages = {42–53},
numpages = {12},
keywords = {WombatOAM, Erlang, Elixir, DevOps, Configuration management, CRDT},
location = {Nara, Japan},
series = {Erlang 2016}
}

@inbook{10.5555/110135.110148,
author = {Aunay, O. and Aunay, S. and Chorlay, D. and Touzot, G. and Vayssade, M.},
title = {Using artificial intelligence in an open software architecture for modelling in engineering},
year = {1991},
isbn = {0130482730},
publisher = {Ellis Horwood},
address = {USA},
booktitle = {Artificial Intelligence in Computational Engineering},
pages = {227–252},
numpages = {26}
}

@article{10.1016/j.patcog.2019.107173,
author = {Song, Liangchen and Wang, Cheng and Zhang, Lefei and Du, Bo and Zhang, Qian and Huang, Chang and Wang, Xinggang},
title = {Unsupervised domain adaptive re-identification: Theory and practice},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {102},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107173},
doi = {10.1016/j.patcog.2019.107173},
journal = {Pattern Recogn.},
month = jun,
numpages = {11},
keywords = {Unsupervised domain adaptation, Person re-identification}
}

@article{10.4018/IJRSDA.2016070101,
author = {Ripon, Shamim H and Kamal, Sarwar and Hossain, Saddam and Dey, Nilanjan},
title = {Theoretical Analysis of Different Classifiers under Reduction Rough Data Set: A Brief Proposal},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {3},
issn = {2334-4598},
url = {https://doi.org/10.4018/IJRSDA.2016070101},
doi = {10.4018/IJRSDA.2016070101},
abstract = {Rough set plays vital role to overcome the complexities, vagueness, uncertainty, imprecision, and incomplete data during features analysis. Classification is tested on certain dataset that maintain an exact class and review process where key attributes decide the class positions. To assess efficient and automated learning, algorithms are used over training datasets. Generally, classification is supervised learning whereas clustering is unsupervised. Classifications under mathematical models deal with mining rules and machine learning. The Objective of this work is to establish a strong theoretical and manual analysis among three popular classifier namely K-nearest neighbor K-NN, Naive Bayes and Apriori algorithm. Hybridization with rough sets among these three classifiers enables enable to address larger datasets. Performances of three classifiers have tested in absence and presence of rough sets. This work is in the phase of implementation for DNA Deoxyribonucleic Acid datasets and it will design automated system to assess classifier under machine learning environment.},
journal = {Int. J. Rough Sets Data Anal.},
month = jul,
pages = {1–20},
numpages = {20},
keywords = {Rough Set, Naive Bayes, K-NN, DNA, Apriori Algorithm}
}

@inproceedings{10.1007/978-3-030-87007-2_27,
author = {Aladics, Tam\'{a}s and J\'{a}sz, Judit and Ferenc, Rudolf},
title = {Bug Prediction Using Source Code Embedding Based on Doc2Vec},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_27},
doi = {10.1007/978-3-030-87007-2_27},
abstract = {Bug prediction is a resource demanding task that is hard to automate using static source code analysis. In many fields of computer science, machine learning has proven to be extremely useful in tasks like this, however, for it to work we need a way to use source code as input. We propose a simple, but meaningful representation for source code based on its abstract syntax tree and the Doc2Vec embedding algorithm. This representation maps the source code to a fixed length vector which can be used for various upstream tasks – one of which is bug prediction. We measured this approach’s validity by itself and its effectiveness compared to bug prediction based solely on code metrics. We also experimented on numerous machine learning approaches to check the connection between different embedding parameters with different machine learning models. Our results show that this representation provides meaningful information as it improves the bug prediction accuracy in most cases, and is always at least as good as only using code metrics as features.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {382–397},
numpages = {16},
keywords = {Doc2Vec, Java, Bug prediction, Code metrics, Source code embedding},
location = {Cagliari, Italy}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacit\'{u}a, Ricardo and Sep\'{u}lveda, Samuel and Mazo, Ra\'{u}l},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Models, Classification, Framework, Software product lines, Feature model}
}

@inproceedings{10.1007/978-3-030-78612-0_5,
author = {Xu, Haitao and Duan, Ruifeng and Yang, Shengsong and Guo, Lei},
title = {An Empirical Study on Data Sampling for Just-in-Time Defect Prediction},
year = {2021},
isbn = {978-3-030-78611-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78612-0_5},
doi = {10.1007/978-3-030-78612-0_5},
abstract = {In this paper, the impact of Data Sampling on Just-in-Time defect prediction is explored. We find that there is a significant negative relationship between the class imbalance ratio of the dataset and the performance of the instant software defect prediction model. Secondly although most software defect data are not as unbalanced as expected, a moderate degree of imbalance is sufficient to affect the performance of traditional learning. This means that if the training data for immediate software defects show moderate or more severe imbalances, one need not expect good defect prediction performance and the data sampling approach to balancing the training data can improve the performance of the model. Finally, the empirical approach shows that although the under-sampling method slightly improves model performance, the different sampling methods do not have a substantial impact on the evaluation of immediate software defect prediction models.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part II},
pages = {54–69},
numpages = {16},
keywords = {Empirical study, Just-in-time defect, Data sampling},
location = {Dublin, Ireland}
}

@inproceedings{10.1007/978-3-030-89363-7_28,
author = {Dai, Huan and Zhang, Yupei and Yun, Yue and Shang, Xuequn},
title = {An Improved Deep Model for Knowledge Tracing and Question-Difficulty Discovery},
year = {2021},
isbn = {978-3-030-89362-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89363-7_28},
doi = {10.1007/978-3-030-89363-7_28},
abstract = {Knowledge Tracing (KT) aims to analyze a student’s acquisition of skills over time by examining the student’s performance on questions of those skills. In recent years, a recurrent neural network model called deep knowledge tracing (DKT) has been proposed to handle the knowledge tracing task and literature has shown that DKT generally outperforms traditional methods. However, DKT and its variants often lead to oscillation results on a skill’s state may due to it ignoring the skill’s difficulty or the question’s difficulty. As a result, even when a student performs well on a skill, the prediction of that skill’s mastery level decreases instead, and vice versa. This is undesirable and unreasonable because student’s performance is expected to transit gradually over time. In this paper, we propose to learn the knowledge tracing model in a “simple-to-difficult” process, leading to a method of Self-paced Deep Knowledge Tracing (SPDKT). SPDKT learns the difficulty of per question from the student’s responses to optimize the question’s order and smooth the learning process. With mitigating the cause of oscillations, SPDKT has the capability of robustness to the puzzling questions. The experiments on real-world datasets show SPDKT achieves state-of-the-art performance on question response prediction and reaches interesting interpretations in education.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part II},
pages = {362–375},
numpages = {14},
keywords = {Personalized education, Deep learning, Self-paced learning, Knowledge tracing},
location = {Hanoi, Vietnam}
}

@inproceedings{10.1145/3422392.3422498,
author = {Freire, Willian Marques and Massago, Mamoru and Zavadski, Arthur Cattaneo and Malachini, Aline Maria and Amaral, Miotto and Colanzi, Thelma Elita},
title = {OPLA-Tool v2.0: a Tool for Product Line Architecture Design Optimization},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422498},
doi = {10.1145/3422392.3422498},
abstract = {The Multi-objective Optimization Approach for Product Line Architecture Design (MOA4PLA) is the seminal approach that successfully optimizes Product Line Architecture (PLA) design using search algorithms. The tool named OPLA-Tool was developed in order to automate the use of MOA4PLA. Over time, the customization of the tool to suit the needs of new research and application scenarios led to several problems. The main problems identified in the original version of OPLA-Tool are environment configuration, maintainability and usability problems, and PLA design modeling and visualization. Such problems motivated the development of a new version of this tool: OPLA-Tool v2.0, presented in this work. In this version, those problems were solved by the source code refactoring, migration to a web-based graphical user interface (GUI) and inclusion of a new support tool for PLA modeling and visualization. Furthermore, OPLA-Tool v2.0 has new functionalities, such as new objective functions, new search operators, intelligent interaction with users during the optimization process, multi-user authentication and simultaneous execution of several experiments to PLA optimization. Such a new version of OPLA-Tool is an important achievement to PLA design optimization as it provides an easier and more complete way to automate this task.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {818–823},
numpages = {6},
keywords = {Software product line, multi-objective evolutionary algorithms, product line architecture},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/3340482.3342742,
author = {Borg, Markus and Svensson, Oscar and Berg, Kristian and Hansson, Daniel},
title = {SZZ unleashed: an open implementation of the SZZ algorithm - featuring example usage in a study of just-in-time bug prediction for the Jenkins project},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342742},
doi = {10.1145/3340482.3342742},
abstract = {Machine learning applications in software engineering often rely on detailed information about bugs. While issue trackers often contain information about when bugs were fixed, details about when they were introduced to the system are often absent. As a remedy, researchers often rely on the SZZ algorithm as a heuristic approach to identify bug-introducing software changes. Unfortunately, as reported in a recent systematic literature review, few researchers have made their SZZ implementations publicly available. Consequently, there is a risk that research effort is wasted as new projects based on SZZ output need to initially reimplement the approach. Furthermore, there is a risk that newly developed (closed source) SZZ implementations have not been properly tested, thus conducting research based on their output might introduce threats to validity. We present SZZ Unleashed, an open implementation of the SZZ algorithm for git repositories. This paper describes our implementation along with a usage example for the Jenkins project, and conclude with an illustrative study on just-in-time bug prediction. We hope to continue evolving SZZ Unleashed on GitHub, and warmly invite the community to contribute.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {mining software repositories, issue tracking, defect prediction, SZZ},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@inproceedings{10.1145/1621607.1621633,
author = {Sanen, Frans and Truyen, Eddy and Joosen, Wouter},
title = {Mapping problem-space to solution-space features: a feature interaction approach},
year = {2009},
isbn = {9781605584942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1621607.1621633},
doi = {10.1145/1621607.1621633},
abstract = {Mapping problem-space features into solution-space features is a fundamental configuration problem in software product line engineering. A configuration problem is defined as generating the most optimal combination of software features given a requirements specification and given a set of configuration rules. Current approaches however provide little support for expressing complex configuration rules between problem and solution space that support incomplete requirements specifications. In this paper, we propose an approach to model complex configuration rules based on a generalization of the concept of problem-solution feature interactions. These are interactions between solution-space features that only arise in specific problem contexts. The use of an existing tool to support our approach is also discussed: we use the DLV answer set solver to express a particular configuration problem as a logic program whose answer set corresponds to the optimal combinations of solution-space features. We motivate and illustrate our approach with a case study in the field of managing dynamic adaptations in distributed software, where the goal is to generate an optimal protocol for accommodating a given adaptation.},
booktitle = {Proceedings of the Eighth International Conference on Generative Programming and Component Engineering},
pages = {167–176},
numpages = {10},
keywords = {software product line engineering, problem-solution feature interactions, distributed runtime adaptation, default logic, configuration knowledge, DLV},
location = {Denver, Colorado, USA},
series = {GPCE '09}
}

@inproceedings{10.1145/2364412.2364442,
author = {Cavalcante, Everton and Almeida, Andr\'{e} and Batista, Thais and Cacho, N\'{e}lio and Lopes, Frederico and Delicato, Flavia C. and Sena, Thiago and Pires, Paulo F.},
title = {Exploiting software product lines to develop cloud computing applications},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364442},
doi = {10.1145/2364412.2364442},
abstract = {With the advance of the Cloud Computing paradigm, new challenges in terms of models, tools, and techniques to support developers to design, build and deploy complex software systems that make full use of the cloud technology arise. In the heterogeneous scenario of this new paradigm, the development of applications using cloud services becomes hard, and the software product lines (SPL) approach is potentially promising for this context since specificities of the cloud platforms, such as services heterogeneity, pricing model, and other aspects can be catered as variabilities to core features. In this perspective, this paper (i) proposes a seamless adaptation of the SPL-based development to include important features of cloud-based applications, and (ii) reports the experience of developing HW-CSPL, a SPL for the Health Watcher (HW) System, which allows citizens to register complaints and consult information regarding the public health system of a city. Several functionalities of this system were implemented using different Cloud Computing platforms, and run time specificities of this application deployed on the cloud were analyzed, as well as other information such as change impact and pricing.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {179–187},
numpages = {9},
keywords = {software product lines, services, health watcher system, cloud platforms, cloud computing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.5555/3466184.3466446,
author = {Rodriguez, Brodderick and Yilmaz, Levent},
title = {Learning rule-based explanatory models from exploratory multi-simulation for decision-support under uncertainty},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Exploratory modeling and simulation is an effective strategy when there are substantial contextual uncertainty and representational ambiguity in problem formulation. However, two significant challenges impede the use of an ensemble of models in exploratory simulation. The first challenge involves streamlining the maintenance and synthesis of multiple models from plausible features that are identified from and subject to the constraints of the research hypothesis. The second challenge is making sense of the data generated by multi-simulation over a model ensemble. To address both challenges, we introduce a computational framework that integrates feature-driven variability management with an anticipatory learning classifier system to generate explanatory rules from multi-simulation data.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2293–2304},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1609/aaai.v33i01.33014015,
author = {Jiang, Yue and Lian, Zhouhui and Tang, Yingmin and Xiao, Jianguo},
title = {SCFont: structure-guided chinese font generation via deep stacked networks},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014015},
doi = {10.1609/aaai.v33i01.33014015},
abstract = {Automatic generation of Chinese fonts that consist of large numbers of glyphs with complicated structures is now still a challenging and ongoing problem in areas of AI and Computer Graphics (CG). Traditional CG-based methods typically rely heavily on manual interventions, while recently-popularized deep learning-based end-to-end approaches often obtain synthesis results with incorrect structures and/or serious artifacts. To address those problems, this paper proposes a structure-guided Chinese font generation system, SCFont, by using deep stacked networks. The key idea is to integrate the domain knowledge of Chinese characters with deep generative networks to ensure that high-quality glyphs with correct structures can be synthesized. More specifically, we first apply a CNN model to learn how to transfer the writing trajectories with separated strokes in the reference font style into those in the target style. Then, we train another CNN model learning how to recover shape details on the contour for synthesized writing trajectories. Experimental results validate the superiority of the proposed SCFont compared to the state of the art in both visual and quantitative assessments.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {493},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1007/s10664-020-09911-x,
author = {Ramos-Guti\'{e}rrez, Bel\'{e}n and Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Discovering configuration workflows from existing logs using process mining},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09911-x},
doi = {10.1007/s10664-020-09911-x},
abstract = {Variability models are used to build configurators, for guiding users through the configuration process to reach the desired setting that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the design options that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suits stakeholders according to previous configurations. For example, when configuring a Linux distribution the configuration process starts by choosing the network or the graphic card and then, other packages concerning a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), a framework that can automatically assist determining the configuration workflow that better fits the configuration logs generated by user activities given a set of logs of previous configurations and a variability model. COLOSSI is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Derived from the possible complexity of both logs and the discovered processes, often, it is necessary to divide the traces into small ones. This provides an easier configuration workflow to be understood and followed by the user during the configuration process. In this paper, we apply and compare four different techniques for the traces clustering: greedy, backtracking, genetic and hierarchical algorithms. Our proposal is validated in three different scenarios, to show its feasibility, an ERP configuration, a Smart Farming, and a Computer Configuration. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering along with the necessity to apply clustering techniques for the trace preparation in the context of configuration workflows.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {41},
keywords = {Clustering, Process discovery, Process mining, Configuration workflow, Variability}
}

@article{10.1016/j.comcom.2015.01.006,
author = {Marnerides, A.K. and Malinowski, S. and Morla, R. and Kim, H.S.},
title = {Fault diagnosis in DSL networks using support vector machines},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {62},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2015.01.006},
doi = {10.1016/j.comcom.2015.01.006},
abstract = {The adequate operation for a number of service distribution networks relies on the effective maintenance and fault management of their underlay DSL infrastructure. Thus, new tools are required in order to adequately monitor and further diagnose anomalies that other segments of the DSL network cannot identify due to the pragmatic issues raised by hardware or software misconfigurations. In this work we present a fundamentally new approach for classifying known DSL-level anomalies by exploiting the properties of novelty detection via the employment of one-class Support Vector Machines (SVMs). By virtue of the imbalance residing in the training samples that consequently lead to problematic prediction outcomes when used within two-class formulations, we adopt the properties of one-class classification and construct models for independently identifying and classifying a single type of a DSL-level anomaly. Given the fact that the greater number of the installed Digital Subscriber Line Access Multiplexers (DSLAMs) within the DSL network of a large European ISP were misconfigured, thus unable to accurately flag anomalous events, we utilize as inference solutions the models derived by the one-class SVM formulations built by the known labels as flagged by the much smaller number of correctly configured DSLAMs in the same network in order to aid the classification aspect against the monitored unlabeled events. By reaching an average over 95% on a number of classification accuracy metrics such as precision, recall and F-score we show that one-class SVM classifiers overcome the biased classification outcomes achieved by the traditional two-class formulations and that they may constitute as viable and promising components within the design of future network fault management strategies. In addition, we demonstrate their superiority over commonly used two-class machine learning approaches such as Decision Trees and Bayesian Networks that has been used in the same context within past solutions.},
journal = {Comput. Commun.},
month = may,
pages = {72–84},
numpages = {13},
keywords = {Support vector machines, Supervised learning, One-class classifiers, Network management, DSL anomalies}
}

@article{10.1007/s00500-020-05005-4,
author = {Malhotra, Ruchika and Lata, Kusum},
title = {A systematic literature review on empirical studies towards prediction of software maintainability},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05005-4},
doi = {10.1007/s00500-020-05005-4},
abstract = {Software maintainability prediction in the earlier stages of software development involves the construction of models for the accurate estimation of maintenance effort. This guides the software practitioners to manage the resources optimally. This study aims at systematically reviewing the prediction models from January 1990 to October 2019 for predicting software maintainability. We analyze the effectiveness of these models according to various aspects. To meet the goal of the research, we have identified 36 research papers. On investigating these papers, we found that various machine learning (ML), statistical (ST), and hybridized (HB) techniques have been applied to develop prediction models to predict software maintainability. The significant finding of this review is that the overall performance of ML-based models is better than that of ST models. The use of HB techniques for prediction of software maintainability is limited. The results of this review revealed that software maintainability prediction (SMP) models developed using ML techniques outperformed models developed using ST techniques. Also, the prediction performance of few models developed using HB techniques is encouraging, yet no conclusive results about the performance of HB techniques could be reported because different HB techniques are applied in a few studies.},
journal = {Soft Comput.},
month = nov,
pages = {16655–16677},
numpages = {23},
keywords = {Hybridized techniques, Statistical techniques, Machine learning techniques, Software maintainability, Software maintenance}
}

@article{10.1109/MS.2020.2985775,
author = {Kim, Miryung},
title = {Software Engineering for Data Analytics},
year = {2020},
issue_date = {July-Aug. 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {37},
number = {4},
issn = {0740-7459},
url = {https://doi.org/10.1109/MS.2020.2985775},
doi = {10.1109/MS.2020.2985775},
abstract = {We are at an inflection point where software engineering meets the data-centric world of big data, machine learning, and artificial intelligence. In this article, I summarize findings from studies of professional data scientists and discuss my perspectives on open research problems to improve data-centric software development.},
journal = {IEEE Softw.},
month = jul,
pages = {36–42},
numpages = {7}
}

@inproceedings{10.1609/aaai.v33i01.33014951,
author = {Shu, Yang and Cao, Zhangjie and Long, Mingsheng and Wang, Jianmin},
title = {Transferable curriculum for weakly-supervised domain adaptation},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014951},
doi = {10.1609/aaai.v33i01.33014951},
abstract = {Domain adaptation improves a target task by knowledge transfer from a source domain with rich annotations. It is not uncommon that "source-domain engineering" becomes a cumbersome process in domain adaptation: the high-quality source domains highly related to the target domain are hardly available. Thus, weakly-supervised domain adaptation has been introduced to address this difficulty, where we can tolerate the source domain with noises in labels, features, or both. As such, for a particular target task, we simply collect the source domain with coarse labeling or corrupted data. In this paper, we try to address two entangled challenges of weakly-supervised domain adaptation: sample noises of the source domain and distribution shift across domains. To disentangle these challenges, a Transferable Curriculum Learning (TCL) approach is proposed to train the deep networks, guided by a transferable curriculum informing which of the source examples are noiseless and transferable. The approach enhances positive transfer from clean source examples to the target and mitigates negative transfer of noisy source examples. A thorough evaluation shows that our approach significantly outperforms the state-of-the-art on weakly-supervised domain adaptation tasks.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {608},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1007/s10270-017-0610-0,
author = {Guo, Jianmei and Liang, Jia Hui and Shi, Kai and Yang, Dingyu and Zhang, Jingsong and Czarnecki, Krzysztof and Ganesh, Vijay and Yu, Huiqun},
title = {SMTIBEA: a hybrid multi-objective optimization algorithm for configuring large constrained software product lines},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0610-0},
doi = {10.1007/s10270-017-0610-0},
abstract = {A key challenge to software product line engineering is to explore a huge space of various products and to find optimal or near-optimal solutions that satisfy all predefined constraints and balance multiple often competing objectives. To address this challenge, we propose a hybrid multi-objective optimization algorithm called SMTIBEA that combines the indicator-based evolutionary algorithm (IBEA) with the satisfiability modulo theories (SMT) solving. We evaluated the proposed algorithm on five large, constrained, real-world SPLs. Compared to the state-of-the-art, our approach significantly extends the expressiveness of constraints and simultaneously achieves a comparable performance. Furthermore, we investigate the performance influence of the SMT solving on two evolutionary operators of the IBEA.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1447–1466},
numpages = {20},
keywords = {Software product lines, Search-based software engineering, Multi-objective evolutionary algorithms, Feature models, Constraint solving}
}

@inproceedings{10.5220/0005679102800287,
author = {Diedrich, Alexander and B\"{o}ttcher, Bj\"{o}rn and Niggemann, Oliver},
title = {Exposing Design Mistakes During Requirements Engineering by Solving Constraint Satisfaction Problems to Obtain Minimum Correction Subsets},
year = {2016},
isbn = {9789897581724},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005679102800287},
doi = {10.5220/0005679102800287},
abstract = {In recent years, the complexity of production plants and therefore of the underlying automation systems has grown significantly. This makes the manual design of automation systems increasingly difficult. As a result, errors are found only during production, plant modifications are hindered by not maintainable automation solutions and criteria such as energy efficiency or cost are often not optimized. This work shows how utilizing Minimum Correction Subsets (MCS) of a Constraint Satisfaction Problem improves the collaboration of automation system designers and prevents inconsistent requirements and thus subsequent errors in the design. This opens up a new field of application for constraint satisfaction techniques. As a use case, an example from the field of automation system design is presented. To meet the automation industry\^{a} s requirement for standardised solutions that assure reliability, the calculation of MCS is formulated in such a way that most constraint solvers can be used without any extensions. Experimental results with typical problems demonstrate the practicalness concerning runtime and hardware resources.},
booktitle = {Proceedings of the 8th International Conference on Agents and Artificial Intelligence},
pages = {280–287},
numpages = {8},
keywords = {Product Line Engineering, Minimum Correction Subsets., Feature Models, Constraint Satisfaction},
location = {Rome, Italy},
series = {ICAART 2016}
}

@article{10.1002/smr.2156,
author = {Maltesque, Guest Editors and Catolino, Gemma and Ferrucci, Filomena},
title = {An extensive evaluation of ensemble techniques for software change prediction},
year = {2019},
issue_date = {September 2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {31},
number = {9},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2156},
doi = {10.1002/smr.2156},
abstract = {Predicting the areas of the source code having a higher likelihood to change in the future represents an important activity to allow developers to plan preventive maintenance operations. For this reason, several change prediction models have been proposed. Moreover, research community demonstrated how different classifiers impact on the performance of devised models as well as classifiers tend to perform similarly even though they are able to correctly predict the change proneness of different code elements, possibly indicating the presence of some complementarity among them. In this paper, we deeper investigated whether the use of ensemble approaches, ie, machine learning techniques able to combine multiple classifiers, can improve the performances of change prediction models. Specifically, we built three change prediction models based on different predictors, ie, product‐, process‐ metrics‐, and developer‐related factors, comparing the performances of four ensemble techniques (ie, Boosting, Random Forest, Bagging, and Voting) with those of standard machine learning classifiers (ie, Logistic Regression, Naive Bayes, Simple Logistic, and Multilayer Perceptron). The study was conducted on 33 releases of 10 open‐source systems, and the results showed how ensemble methods and in particular Random Forest provide a significant improvement of more than 10% in terms of F measure. Indeed, the statistical analyses conducted confirm the superiority of this ensemble technique. Moreover, the model built using developer‐related factors performed better than the other models that exploit product and process metrics and achieves an overall median of F measure around 77%.
We built three change prediction models based on different predictors, ie, product‐, process‐, and developer‐related metrics.IWe compared the performances of the models between four standard machine learning classifiers and four ensemble technique.The study was conducted on 33 releases of 10 open source systems, and the results showed how ensemble methods and in particular Random Forest provide a significant improvement of more than 10% in terms of F measure.


image
image},
journal = {J. Softw. Evol. Process},
month = oct,
numpages = {15},
keywords = {machine learning, ensemble techniques, empirical study, change prediction}
}

@inproceedings{10.1609/aaai.v33i01.33014910,
author = {Shi, Shu-Ting and Li, Ming and Lo, David and Thung, Ferdian and Huo, Xuan},
title = {Automatic code review by learning the revision of source code},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014910},
doi = {10.1609/aaai.v33i01.33014910},
abstract = {Code review is the process of manual inspection on the revision of the source code in order to find out whether the revised source code eventually meets the revision requirements. However, manual code review is time-consuming, and automating such the code review process will alleviate the burden of code reviewers and speed up the software maintenance process. To construct the model for automatic code review, the characteristics of the revisions of source code (i.e., the difference between the two pieces of source code) should be properly captured and modeled. Unfortunately, most of the existing techniques can easily model the overall correlation between two pieces of source code, but not for the "difference" between two pieces of source code. In this paper, we propose a novel deep model named DACE for automatic code review. Such a model is able to learn revision features by contrasting the revised hunks from the original and revised source code with respect to the code context containing the hunks. Experimental results on six open source software projects indicate by learning the revision features, DACE can outperform the competing approaches in automatic code review.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {603},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1109/SPLC.2011.47,
author = {Chen, Sheng and Erwig, Martin},
title = {Optimizing the Product Derivation Process},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.47},
doi = {10.1109/SPLC.2011.47},
abstract = {Feature modeling is widely used in software product-line engineering to capture the commonalities and variabilities within an application domain. As feature models evolve, they can become very complex with respect to the number of features and the dependencies among them, which can cause the product derivation based on feature selection to become quite time consuming and error prone. We address this problem by presenting techniques to find good feature selection sequences that are based on the number of products that contain a particular feature and the impact of a selected feature on the selection of other features. Specifically, we identify a feature selection strategy, which brings up highly selective features early for selection. By prioritizing feature selection based on the selectivity of features our technique makes the feature selection process more efficient. Moreover, our approach helps with the problem of unexpected side effects of feature selection in later stages of the selection process, which is commonly considered a difficult problem. We have run our algorithm on the e-Shop and Berkeley DB feature models and also on some automatically generated feature models. The evaluation results demonstrate that our techniques can shorten the product derivation processes significantly.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {35–44},
numpages = {10},
keywords = {Feature Selection, Feature Model, Decision Sequence},
series = {SPLC '11}
}

@inproceedings{10.1145/3368826.3377923,
author = {Shaikhha, Amir and Schleich, Maximilian and Ghita, Alexandru and Olteanu, Dan},
title = {Multi-layer optimizations for end-to-end data analytics},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377923},
doi = {10.1145/3368826.3377923},
abstract = {We consider the problem of training machine learning models over multi-relational data. The mainstream approach is to first construct the training dataset using a feature extraction query over input database and then use a statistical software package of choice to train the model. In this paper we introduce Iterative Functional Aggregate Queries (IFAQ), a framework that realizes an alternative approach. IFAQ treats the feature extraction query and the learning task as one program given in the IFAQ's domain-specific language, which captures a subset of Python commonly used in Jupyter notebooks for rapid prototyping of machine learning applications. The program is subject to several layers of IFAQ optimizations, such as algebraic transformations, loop transformations, schema specialization, data layout optimizations, and finally compilation into efficient low-level C++ code specialized for the given workload and data.  We show that a Scala implementation of IFAQ can outperform mlpack, Scikit, and TensorFlow by several orders of magnitude for linear regression and regression tree models over several relational datasets.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {145–157},
numpages = {13},
keywords = {In-Database Machine Learning, Multi-Query Optimization, Query Compilation},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@inproceedings{10.1145/3358331.3358361,
author = {Li, Jinfa and Li, Qianqian and Jiang, Hongbing},
title = {Research Hotspots and Development Trend of Product Data Management},
year = {2019},
isbn = {9781450372022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358331.3358361},
doi = {10.1145/3358331.3358361},
abstract = {With the rapid development of information industry and the rapid rise of big data technology, enterprises have adopted product data management (PDM) for information construction. Using CNKI database of China Knowledge Network as data source, the keywords of "product data management" or "PDM" were retrieved, and the keywords of PDM were extracted by co-word Analysis Using cluster analysis, multi-dimension analysis, social network analysis and strategic coordinate map analysis, this paper discusses the focus areas of product data management in China, and makes Visual Analysis With a view to enterprise product data management to provide a certain reference. Results a total of 2168 articles were included Through cluster analysis, it is concluded that enterprise information software integration technology, product structure and configuration management, PDM system project management, product design and development product, whole life cycle, key technology of PDM system, application of PDM system information field are the current hot research.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing},
articleno = {30},
numpages = {7},
keywords = {product data management, co-word analysis, cluster analysis},
location = {Dublin, Ireland},
series = {AIAM 2019}
}

@inproceedings{10.5555/518898.785716,
author = {Kwon, Oh-Cheon and Shin, Gyu-Sang and Boldyreff, Cornelia and Munro, Malcolm},
title = {Maintenance with Reuse: An Integrated Approach Based on Software Configuration Management},
year = {1999},
isbn = {0769505090},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Software reuse has recently been considered the best solution to enhance the productivity of a software development team and to reduce maintenance costs. In addition, Software Configuration Management (SCM) is a central part of software maintenance as it is associated with changing existing software, and is a discipline for controlling these changes and improving the maintenance process, thereby enhancing the quality of a software product. Thus, both software reuse and SCM have been proposed for making a significant improvement in productivity, quality and cost. However, so far these two technologies have been investigated separately. In order for software reuse and SCM to produce effects by synergy, both approaches require to be introduced into a maintenance environment together. Since software reuse and SCM, and software reuse and software maintenance have many similarities and relationships in their activities, these disciplines can be integrated within a software maintenance environment. The authors have therefore developed an integrated process model for Maintenance with Reuse (MwR) that supports SCMfor a reuse library which is actively maintained for use in a maintenance environment. In this paper, the MwR model and its prototype are described, followed by evaluation using a case study and conclusions.},
booktitle = {Proceedings of the Sixth Asia Pacific Software Engineering Conference},
pages = {507},
keywords = {Tool for Evolution of a Reusable and Reconfigurable Assets Library(TERRA), Software Configuration Management(SCM), Reuse, Maintenance with Reuse(MwR), Maintenance},
series = {APSEC '99}
}

@inproceedings{10.1145/99277.99283,
author = {Minsky, Naftaly H. and Rozenshtein, David},
title = {Configuration management by consensus: an application of law-governed systems},
year = {1990},
isbn = {089791418X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/99277.99283},
doi = {10.1145/99277.99283},
abstract = {It is self-evident that if one wants to model and control the cooperative process of software development, one must provide for cooperative decision making. In particular, one should be able to base the decision on whether and how to carry out a given operation on the consensus of several, possibly independent, agents. It is important to emphasize that this is not just a matter of computing the conjunction of some set of conditions. One must also provide a mechanism for establishing any desired consensus structure, which would specify who is allowed to state which kinds of concerns regarding this operation, and what the relationship among these concerns should be.In this paper we propose a general framework for such decision making by consensus, which is based on the concept of law-governed software development. As a concrete application domain in which to illustrate this framework, we consider here the issue of configuration binding.},
booktitle = {Proceedings of the Fourth ACM SIGSOFT Symposium on Software Development Environments},
pages = {44–55},
numpages = {12},
location = {Irvine, California, USA},
series = {SDE 4}
}

@article{10.4018/jismd.2012100101,
author = {Asadi, Mohsen and Mohabbati, Bardia and Ga\v{s}evic, Dragan and Bagheri, Ebrahim and Hatala, Marek},
title = {Developing Semantically-Enabled Families of Method-Oriented Architectures},
year = {2012},
issue_date = {October 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012100101},
doi = {10.4018/jismd.2012100101},
abstract = {Method Engineering ME aims to improve software development methods by creating and proposing adaptation frameworks whereby methods are created to provide suitable matches with the requirements of the organization and address project concerns and fit specific situations. Therefore, methods are defined and modularized into components stored in method repositories. The assembly of appropriate methods depends on the particularities of each project, and rapid method construction is inevitable in the reuse and management of existing methods. The ME discipline aims at providing engineering capability for optimizing, reusing, and ensuring flexibility and adaptability of methods; there are three key research challenges which can be observed in the literature: 1 the lack of standards and tooling support for defining, publishing, discovering, and retrieving methods which are only locally used by their providers without been largely adapted by other organizations; 2 dynamic adaptation and assembly of methods with respect to imposed continuous changes or evolutions of the project lifecycle; and 3 variability management in software methods in order to enable rapid and effective construction, assembly and adaptation of existing methods with respect to particular situations. The authors propose semantically-enabled families of method-oriented architecture by applying service-oriented product line engineering principles and employing Semantic Web technologies.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {1–26},
numpages = {26},
keywords = {Software Product Line, Software Development, Semantic Web, Method Oriented Architecture MOA, Method Engineering}
}

@inproceedings{10.1109/ISM.2005.24,
author = {Dulva Hina, Manolo and Ramdane-Cherif, Amar and Tadj, Chakib},
title = {A Ubiquitous Context-sensitive Multimodal Multimedia Computing System and Its Machine Learning-based Reconfiguration at the Architectural Level},
year = {2005},
isbn = {0769524893},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISM.2005.24},
doi = {10.1109/ISM.2005.24},
abstract = {In this paper, we present our work on a ubiquitous context-sensitive multimodal multimedia computing system that progressively acquires machine knowledge. This ubiquitous computing system supports an automatic selection of media and modalities deemed appropriate for the user's context and user's profile. The ability of the system to do so constitutes its acquired knowledge. The decision making for media/modality selection takes into account if the user has some special needs due to disability. The architecture of the system is designed to be pervasive and is conceived to resist failure. In case of one or more components being missing or found defective, the machine would resist failure by reconfiguring itself dynamically in the architectural level. It finds alternative replacement to the failed component using its acquired knowledge.},
booktitle = {Proceedings of the Seventh IEEE International Symposium on Multimedia},
pages = {585–591},
numpages = {7},
series = {ISM '05}
}

@article{10.1016/j.knosys.2019.105185,
author = {Liang, Naiyao and Yang, Zuyuan and Li, Zhenni and Xie, Shengli and Su, Chun-Yi},
title = {Semi-supervised multi-view clustering with Graph-regularized Partially Shared Non-negative Matrix Factorization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105185},
doi = {10.1016/j.knosys.2019.105185},
journal = {Know.-Based Syst.},
month = feb,
numpages = {10},
keywords = {Non-negative matrix factorization, Multi-view clustering, Semi-supervised learning, Graph-regularization}
}

@inproceedings{10.1145/3416505.3423563,
author = {Palma, Stefano Dalla and Mohammadi, Majid and Di Nucci, Dario and Tamburri, Damian A.},
title = {Singling the odd ones out: a novelty detection approach to find defects in infrastructure-as-code},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423563},
doi = {10.1145/3416505.3423563},
abstract = {Infrastructure-as-Code (IaC) is increasingly adopted. However, little is known about how to best maintain and evolve it. Previous studies focused on defining Machine-Learning models to predict defect-prone blueprints using supervised binary classification. This class of techniques uses both defective and non-defective instances in the training phase. Furthermore, the high imbalance between defective and non-defective samples makes the training more difficult and leads to unreliable classifiers. In this work, we tackle the defect-prediction problem from a different perspective using novelty detection and evaluate the performance of three techniques, namely OneClassSVM, LocalOutlierFactor, and IsolationForest, and compare their performance with a baseline RandomForest binary classifier. Such models are trained using only non-defective samples: defective data points are treated as novelty because the number of defective samples is too little compared to defective ones. We conduct an empirical study on an extremely-imbalanced dataset consisting of 85 real-world Ansible projects containing only small amounts of defective instances. We found that novelty detection techniques can recognize defects with a high level of precision and recall, an AUC-PR up to 0.86, and an MCC up to 0.31. We deem our results can influence the current trends in defect detection and put forward a new research path toward dealing with this problem.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {31–36},
numpages = {6},
keywords = {Novelty Detection, Infrastructure-as-Code, Defect Prediction},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@article{10.5555/2946645.2946709,
author = {Adi, Yossi and Keshet, Joseph},
title = {StructED: risk minimization in structured prediction},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Structured tasks are distinctive: each task has its own measure of performance, such as the word error rate in speech recognition, the BLEU score in machine translation, the NDCG score in information retrieval, or the intersection-over-union score in visual object segmentation. This paper presents STRUCTED, a software package for learning structured prediction models with training methods that aimed at optimizing the task measure of performance. The package was written in Java and released under the MIT license. It can be downloaded from http://adiyoss.github.io/StructED/.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2282–2286},
numpages = {5},
keywords = {structured prediction, structural SVM, direct loss minimization, CRF}
}

@inproceedings{10.1145/3297280.3297479,
author = {Ne\v{s}i\'{c}, Damir and Nyberg, Mattias and Gallina, Barbara},
title = {Constructing product-line safety cases from contract-based specifications},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297479},
doi = {10.1145/3297280.3297479},
abstract = {Safety cases are used to argue that safety-critical systems satisfy the requirements that are determined to mitigate the potential hazards in the systems operating environment. Although typically a manual task, safety cases have been successfully created for systems without many configuration options. However, in highly configurable systems, typically developed as a Product Line (PL), arguing about each possible configuration, and ensuring the completeness of the safety case are still open research problems. This paper presents a novel and general approach, based on Contract-Based Specification (CBS), for the construction of a safety case for an arbitrary PL. Starting from a general CBS framework, we present a PL extensions that allows expressing configurable systems and preserves the properties of the original CBS framework. Then, we define the transformation from arbitrary PL models, created using extended CBS framework, to a safety case argumentation-structure, expressed using the Goal Structuring Notation. Finally, the approach is exemplified on a simplified, but real, and currently produced system by Scania CV AB.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2022–2031},
numpages = {10},
keywords = {contract-based specification, product line engineering, safety case},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1016/j.patcog.2017.10.005,
author = {Zhou, Sanping and Wang, Jinjun and Meng, Deyu and Xin, Xiaomeng and Li, Yubing and Gong, Yihong and Zheng, Nanning},
title = {Deep self-paced learning for person re-identification},
year = {2018},
issue_date = {April 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.10.005},
doi = {10.1016/j.patcog.2017.10.005},
abstract = {We propose a novel deep self-paced learning algorithm to supervise the learning of deep neural network, in which a soft polynomial regularizer term is proposed to gradually involve the faithful samples into training process in a self-paced manner.We optimize the gradient back-propagation of relative distance metric by introducing a symmetric regularizer term, which can convert the back-propagation from the asymmetric mode to a symmetric one.We build an effective part-based deep neural network, in which features of different body parts are first discriminately learned in the convolutional layers and then fused in the fully connected layers. Person re-identification(Re-ID) usually suffers from noisy samples with background clutter and mutual occlusion, which makes it extremely difficult to distinguish different individuals across the disjoint camera views. In this paper, we propose a novel deep self-paced learning(DSPL) algorithm to alleviate this problem, in which we apply a self-paced constraint and symmetric regularization to help the relative distance metric training the deep neural network, so as to learn the stable and discriminative features for person Re-ID. Firstly, we propose a soft polynomial regularizer term which can derive the adaptive weights to samples based on both the training loss and model age. As a result, the high-confidence fidelity samples will be emphasized and the low-confidence noisy samples will be suppressed at early stage of the whole training process. Such a learning regime is naturally implemented under a self-paced learning(SPL) framework, in which samples weights are adaptively updated based on both model age and sample loss using an alternative optimization method. Secondly, we introduce a symmetric regularizer term to revise the asymmetric gradient back-propagation derived by the relative distance metric, so as to simultaneously minimize the intra-class distance and maximize the inter-class distance in each triplet unit. Finally, we build a part-based deep neural network, in which the features of different body parts are first discriminately learned in the lower convolutional layers and then fused in the higher fully connected layers. Experiments on several benchmark datasets have demonstrated the superior performance of our method as compared with the state-of-the-art approaches.},
journal = {Pattern Recogn.},
month = apr,
pages = {739–751},
numpages = {13},
keywords = {Self-paced learning, Person re-identification, Metric learning, Convolutional neural network}
}

@article{10.5555/3455716.3455938,
author = {Weinshall, Daphna and Amir, Dan},
title = {Theory of curriculum learning, with convex loss functions},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Curriculum Learning is motivated by human cognition, where teaching often involves gradually exposing the learner to examples in a meaningful order, from easy to hard. Although methods based on this concept have been empirically shown to improve performance of several machine learning algorithms, no theoretical analysis has been provided even for simple cases. To address this shortfall, we start by formulating an ideal definition of difficulty score - the loss of the optimal hypothesis at a given datapoint. We analyze the possible contribution of curriculum learning based on this score in two convex problems - linear regression, and binary classification by hinge loss minimization. We show that in both cases, the convergence rate of SGD optimization decreases monotonically with the difficulty score, in accordance with earlier empirical results. We also prove that when the difficulty score is fixed, the convergence rate of SGD optimization is monotonically increasing with respect to the loss of the current hypothesis at each point. We discuss how these results settle some confusion in the literature where two apparently opposing heuristics are reported to improve performance: curriculum learning in which easier points are given priority, vs hard data mining where the more difficult points are sought out.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {222},
numpages = {19},
keywords = {hinge loss minimization, linear regression, curriculum learning}
}

@article{10.1016/j.infsof.2015.11.004,
author = {Heradio, Ruben and Perez-Morago, Hector and Fernandez-Amoros, David and Javier Cabrerizo, Francisco and Herrera-Viedma, Enrique},
title = {A bibliometric analysis of 20 years of research on software product lines},
year = {2016},
issue_date = {April 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {72},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.11.004},
doi = {10.1016/j.infsof.2015.11.004},
abstract = {Context: Software product line engineering has proven to be an efficient paradigm to developing families of similar software systems at lower costs, in shorter time, and with higher quality.Objective: This paper analyzes the literature on product lines from 1995 to 2014, identifying the most influential publications, the most researched topics, and how the interest in those topics has evolved along the way.Method: Bibliographic data have been gathered from ISI Web of Science and Scopus. The data have been examined using two prominent bibliometric approaches: science mapping and performance analysis.Results: According to the study carried out, (i) software architecture was the initial motor of research in SPL; (ii) work on systematic software reuse has been essential for the development of the area; and (iii) feature modeling has been the most important topic for the last fifteen years, having the best evolution behavior in terms of number of published papers and received citations.Conclusion: Science mapping has been used to identify the main researched topics, the evolution of the interest in those topics and the relationships among topics. Performance analysis has been used to recognize the most influential papers, the journals and conferences that have published most papers, how numerous is the literature on product lines and what is its distribution over time.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {1–15},
numpages = {15},
keywords = {Software product lines, Science mapping, Performance analysis, Bibliometrics}
}

@inproceedings{10.1145/3377929.3390050,
author = {Saidani, Islem and Ouni, Ali and Chouchen, Moataz and Mkaouer, Mohamed Wiem},
title = {On the prediction of continuous integration build failures using search-based software engineering},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3390050},
doi = {10.1145/3377929.3390050},
abstract = {Continuous Integration (CI) aims at supporting developers in integrating code changes quickly through automated building. However, in such context, the build process is typically time and resource-consuming. As a response, the use of machine learning (ML) techniques has been proposed to cut the expenses of CI build time by predicting its outcome. Nevertheless, the existing ML-based solutions are challenged by problems related mainly to the imbalanced distribution of successful and failed builds. To deal with this issue, we introduce a novel approach based on Multi-Objective Genetic Programming (MOGP) to build a prediction model. Our approach aims at finding the best prediction rules based on two conflicting objective functions to deal with both minority and majority classes. We evaluated our approach on a benchmark of 15,383 builds. The results reveal that our technique outperforms state-of-the-art approaches by providing a better balance between both failed and passed builds.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {313–314},
numpages = {2},
keywords = {search-based software engineering, multi-objective optimization, machine learning, continuous integration, build prediction},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1002/smr.2271,
author = {Tariq, Sidra and Usman, Muhammad and Fong, Alvis C.M.},
title = {Selecting best predictors from large software repositories for highly accurate software effort estimation},
year = {2020},
issue_date = {October 2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {32},
number = {10},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2271},
doi = {10.1002/smr.2271},
abstract = {Accurate prediction of software effort is important for planning, scheduling, and allocating resources. However, software effort estimation has been a challenging task. Although numerous estimation models have been proposed, few achieve anything close to accurate prediction of software development effort. To achieve optimal results, machine learning techniques have recently been employed for predicting software development effort using relatively large software repositories. However, some issues remain unresolved, and this paper aims to address the following issues. First, feature selection methods often neglected the information rich variables present in the dataset. Second, selection of important features was done through statistical methods, which lack domain knowledge. Third, missing values in the data that significantly influence the prediction outcome was not efficiently handled. Fourth, majority of the literature neglected advanced evaluation measures, which thoroughly evaluate the ability of learning models to produce accurate results. To address the above issues, a machine learning‐based model has been proposed in this paper, which not only allows effective preprocessing of data but also provides highly accurate prediction results with minimum error rate. The purpose is to best identify attributes (predictors) from large software repositories that are most influential in the estimation of effort. In addition, we apply MMRE for better performance analysis.Accurate prediction of software effort is important for many purposes. However, it remains a challenging task. To facilitate application of machine learning to the problem of software effort estimation, we propose a novel approach toward finding the best attributes that are most influential in the estimation of effort. Furthermore, we propose a machine learning‐based model that not only allows effective preprocessing of data but also provides highly accurate prediction results with minimum error rate.


image
image},
journal = {J. Softw. Evol. Process},
month = oct,
numpages = {19},
keywords = {predictors, features, machine learning, effort estimation}
}

@article{10.1016/j.future.2019.07.013,
author = {Shen, Rongbo and Yan, Kezhou and Tian, Kuan and Jiang, Cheng and Zhou, Ke},
title = {Breast mass detection from the digitized X-ray mammograms based on the combination of deep active learning and self-paced learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.07.013},
doi = {10.1016/j.future.2019.07.013},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {668–679},
numpages = {12},
keywords = {Self-paced learning, Deep active learning, Mass detection, Mammography, Breast cancer}
}

@inproceedings{10.5555/20924.20927,
author = {Taylor, B},
title = {A database approach to configuration management for large projects},
year = {1986},
isbn = {0818606487},
publisher = {IEEE Press},
booktitle = {The Institute of Electrical and Electronics Engineers, Inc on Conference on Software Maintenance--1985},
pages = {15–23},
numpages = {9},
location = {Washington, D.C, USA}
}

@article{10.1016/j.knosys.2021.107476,
author = {Wang, Jun and Zhang, Xiaofang and Chen, Lin},
title = {How well do pre-trained contextual language representations recommend labels for GitHub issues?},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {232},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107476},
doi = {10.1016/j.knosys.2021.107476},
journal = {Know.-Based Syst.},
month = nov,
numpages = {11},
keywords = {Language model, Data analysis, Issue labeling, Deep learning}
}

@inproceedings{10.1145/3365609.3365857,
author = {Gupta, Arpit and Mac-Stoker, Chris and Willinger, Walter},
title = {An Effort to Democratize Networking Research in the Era of AI/ML},
year = {2019},
isbn = {9781450370202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365609.3365857},
doi = {10.1145/3365609.3365857},
abstract = {A growing concern within today's networking community is that with the proliferation of Artificial Intelligence/Machine Learning (AI/ML) techniques, a lack of access to real-world production networks is putting academic researchers at a significant disadvantage. Indeed, compared to a select few research groups in industry that can leverage access to their global-scale production networks in their data-driven efforts to develop and evaluate learning models, academic researchers not only struggle to get their hands on real-world data sets but find it almost impossible to adequately train and assess their learning models under realistic conditions.In this paper, we argue that when appropriately instrumented and properly managed, enterprise networks in the form of university or campus networks can serve as real-world production networks and can, because of their ubiquity, help create a more level playing field for academic researchers. Their various limitations notwithstanding, as real-world production networks, such enterprise networks can (i) serve as unique sources for some of the rich data that will enable these researchers to influence or advance the current state-of-the-art in AI/ML for networking and (ii) also function as much-needed test beds where newly developed AI/ML-based tools can be evaluated or "road-tested" prior to their actual deployment in the production network. We discuss new research challenges that arise from this proposed dual role of campus networks and comment on the opportunities our proposal affords for both academic and industry researchers to benefit from the advantages and limitations of their respective production environments in their common quest to advance the development and evaluation of AI/ML-based tools to the point where they can be deployed in practice.},
booktitle = {Proceedings of the 18th ACM Workshop on Hot Topics in Networks},
pages = {93–100},
numpages = {8},
location = {Princeton, NJ, USA},
series = {HotNets '19}
}

@article{10.1016/j.engappai.2018.06.010,
author = {Chin, Cheng Siong and Ji, Xi},
title = {Adaptive online sequential extreme learning machine for frequency-dependent noise data on offshore oil rig},
year = {2018},
issue_date = {Sep 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2018.06.010},
doi = {10.1016/j.engappai.2018.06.010},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {226–241},
numpages = {16},
keywords = {Root mean square error, Training time, Noise prediction, Oil-rig, Extreme learning machine, Multiple frequency dependent data}
}

@article{10.1155/2021/5558561,
author = {Shao, Yanli and Zhao, Jingru and Wang, Xingqi and Wu, Weiwei and Fang, Jinglong and Gao, Honghao},
title = {Research on Cross-Company Defect Prediction Method to Improve Software Security},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/5558561},
doi = {10.1155/2021/5558561},
abstract = {As the scale and complexity of software increase, software security issues have become the focus of society. Software defect prediction (SDP) is an important means to assist developers in discovering and repairing potential defects that may endanger software security in advance and improving software security and reliability. Currently, cross-project defect prediction (CPDP) and cross-company defect prediction (CCDP) are widely studied to improve the defect prediction performance, but there are still problems such as inconsistent metrics and large differences in data distribution between source and target projects. Therefore, a new CCDP method based on metric matching and sample weight setting is proposed in this study. First, a clustering-based metric matching method is proposed. The multigranularity metric feature vector is extracted to unify the metric dimension while maximally retaining the information contained in the metrics. Then use metric clustering to eliminate metric redundancy and extract representative metrics through principal component analysis (PCA) to support one-to-one metric matching. This strategy not only solves the metric inconsistent and redundancy problem but also transforms the cross-company heterogeneous defect prediction problem into a homogeneous problem. Second, a sample weight setting method is proposed to transform the source data distribution. Wherein the statistical source sample frequency information is set as an impact factor to increase the weight of source samples that are more similar to the target samples, which improves the data distribution similarity between the source and target projects, thereby building a more accurate prediction model. Finally, after the above two-step processing, some classical machine learning methods are applied to build the prediction model, and 12 project datasets in NASA and PROMISE are used for performance comparison. Experimental results prove that the proposed method has superior prediction performance over other mainstream CCDP methods.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {19}
}

@inproceedings{10.1145/3078971.3079003,
author = {Liang, Junwei and Jiang, Lu and Meng, Deyu and Hauptmann, Alexander},
title = {Leveraging Multi-modal Prior Knowledge for Large-scale Concept Learning in Noisy Web Data},
year = {2017},
isbn = {9781450347013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078971.3079003},
doi = {10.1145/3078971.3079003},
abstract = {Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web is associated with rich but noisy contextual information, such as the title and other multi-modal information, which provides weak annotations or labels about the video content. To tackle the problem of large-scale noisy learning, We propose a novel method called Multi-modal WEbly-Labeled Learning (WELL-MM), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human. WELL-MM introduces a novel multi-modal approach to incorporate meaningful prior knowledge called curriculum from the noisy web videos. We empirically study the curriculum constructed from the multi-modal features of the Internet videos and images. The comprehensive experimental results on FCVID and YFCC100M demonstrate that WELL-MM outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data. In addition, the results also verify that WELL-MM is robust to the level of noisiness in the video data. Notably, WELL-MM trained on sufficient noisy web labels is able to achieve a better accuracy to supervised learning methods trained on the clean manually labeled data.},
booktitle = {Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
pages = {32–40},
numpages = {9},
keywords = {webly-supervised learning, web label, video understanding, prior knowledge, noisy data, concept detection, big data},
location = {Bucharest, Romania},
series = {ICMR '17}
}

@inproceedings{10.1145/3177148.3180085,
author = {Surendranath, Ajay and Jayagopi, Dinesh Babu},
title = {Curriculum Learning for Depth Estimation with Deep Convolutional Neural Networks},
year = {2018},
isbn = {9781450352901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177148.3180085},
doi = {10.1145/3177148.3180085},
abstract = {Curriculum learning is a machine learning technique adapted from the way humans acquire knowledge and skills, initially mastering simple tasks and progressing to more complex tasks. The work explores curriculum training by creating multiple levels of dataset with increasing complexity on which the trainings are performed. The experiments demonstrated that there is an average of 12% improvement test loss when compared to a non-curriculum approach. The experiment also demonstrates the advantage of creating synthetic dataset and how it aids in the overall improvement of accuracy. An improvement of 26% is attained on the test error loss when curriculum trained model was compared to training on a limited real world dataset. The work also goes onto propose a novel learning approach, the Self Paced Learning approach with Error-Diversity (SPL-ED) An overall reduction of 32% in the test loss is observed when compared to the non-curriculum training limited to real-world dataset.},
booktitle = {Proceedings of the 2nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence},
pages = {95–100},
numpages = {6},
keywords = {Depth Estimation, Curriculum Learning},
location = {Rabat, Morocco},
series = {MedPRAI '18}
}

@article{10.1007/s10472-021-09736-4,
author = {Prestwich, S. D. and Freuder, E. C. and O’Sullivan, B. and Browne, D.},
title = {Classifier-based constraint acquisition},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {89},
number = {7},
issn = {1012-2443},
url = {https://doi.org/10.1007/s10472-021-09736-4},
doi = {10.1007/s10472-021-09736-4},
abstract = {Modeling a combinatorial problem is a hard and error-prone task requiring significant expertise. Constraint acquisition methods attempt to automate this process by learning constraints from examples of solutions and (usually) non-solutions. Active methods query an oracle while passive methods do not. We propose a known but not widely-used application of machine learning to constraint acquisition: training a classifier to discriminate between solutions and non-solutions, then deriving a constraint model from the trained classifier. We discuss a wide range of possible new acquisition methods with useful properties inherited from classifiers. We also show the potential of this approach using a Naive Bayes classifier, obtaining a new passive acquisition algorithm that is considerably faster than existing methods, scalable to large constraint sets, and robust under errors.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = jul,
pages = {655–674},
numpages = {20},
keywords = {68R99, 68Q32, 68T99, Boolean satisfiability, Bayesian, Classifier, Constraint acquisition}
}

@inproceedings{10.1007/978-3-030-22999-3_4,
author = {Havelock, Jessica and Oommen, B. John and Granmo, Ole-Christoffer},
title = {On Using “Stochastic Learning on the Line” to Design Novel Distance Estimation Methods for Three-Dimensional Environments},
year = {2019},
isbn = {978-3-030-22998-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-22999-3_4},
doi = {10.1007/978-3-030-22999-3_4},
abstract = {We consider the unsolved problem of Distance Estimation (DE) when the inputs are the x and y coordinates (i.e., the latitudinal and longitudinal positions) of the points under consideration, and the elevation/altitudes of the points specified, for example, in terms of their z coordinates (3DDE). The aim of the problem is to yield an accurate value for the real (road) distance between the points specified by all the three coordinates of the cities in question (This is a typical problem encountered in a GISs and GPSs.). In our setting, the distance between any pair of cities is assumed to be computed by merely having access to the coordinates and known inter-city distances of a small subset of the cities, where these are also specified in terms of their 3D coordinates. The 2D variant of the problem has, typically, been tackled by utilizing parametric functions called “Distance Estimation Functions” (DEFs). To solve the 3D problem, we resort to the Adaptive Tertiary Search (ATS) strategy, proposed by Oommen et al., to affect the learning. By utilizing the information provided in the 3D coordinates of the nodes and the true road distances from this subset, we propose a scheme to estimate the inter-nodal distances. In this regard, we use the ATS strategy to calculate the best parameters for the DEF. While “Goodness-of-Fit” (GoF) functions can be used to show that the results are competitive, we show that they are rather not necessary to compute the parameters. Our results demonstrate the power of the scheme, even though we completely move away from the traditional GoF-based paradigm that has been used for four decades. Our results conclude that the 3DDE yields results that are far superior to those obtained by the corresponding 2DDE.},
booktitle = {Advances and Trends in Artificial Intelligence. From Theory to Practice: 32nd International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2019, Graz, Austria, July 9–11, 2019, Proceedings},
pages = {39–49},
numpages = {11},
keywords = {Stochastic Point Location, Adaptive Tertiary Search, Learning Automata, Estimating real-life distances, Road distance estimation},
location = {Graz, Austria}
}

@inproceedings{10.1145/3409073.3409084,
author = {Congyi, Deng and Guangshun, Shi},
title = {Method for Detecting Android Malware Based on Ensemble Learning},
year = {2020},
isbn = {9781450377645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409073.3409084},
doi = {10.1145/3409073.3409084},
abstract = {In recent years, we have become increasingly dependent on smart devices. Android is an operating system mainly used on mobile devices, where hundreds of millions of users can download various apps through many application stores. Under these circumstances, a large number of malicious apps can be put into the application stores by developers to achieve the purpose of attacking, controlling user devices, and even stealing user information and property. Therefore, it is necessary to identify malwares in mass apps through analysis and detection to remind users. We propose an idea of detecting and discriminating Android malware based on an ensemble learning method. Firstly, a static analysis of AndroidManifest file in APK is performed to extract features such as permission calls, component calls, and intents in system. Then we use XGBoost method, an implementation of ensemble learning, to detect malicious applications. The conclusion is that this system performs very well in Android malware detection.},
booktitle = {Proceedings of the 2020 5th International Conference on Machine Learning Technologies},
pages = {28–31},
numpages = {4},
keywords = {Static Analysis, Malware Detection, Ensemble Learning, Android Malware},
location = {Beijing, China},
series = {ICMLT '20}
}

@inproceedings{10.5555/3524938.3525166,
author = {Degenne, R\'{e}my and Shao, Han and Koolen, Wouter M.},
title = {Structure adaptive algorithms for stochastic bandits},
year = {2020},
publisher = {JMLR.org},
abstract = {We study reward maximisation in a wide class of structured stochastic multi-armed bandit problems, where the mean rewards of arms satisfy some given structural constraints, e.g. linear, unimodal, sparse, etc. Our aim is to develop methods that are flexible (in that they easily adapt to different structures), powerful (in that they perform well empirically and/or provably match instance-dependent lower bounds) and efficient in that the per-round computational burden is small. We develop asymptotically optimal algorithms from instance-dependent lower-bounds using iterative saddle-point solvers. Our approach generalises recent iterative methods for pure exploration to reward maximisation, where a major challenge arises from the estimation of the suboptimality gaps and their reciprocals. Still we manage to achieve all the above desiderata. Notably, our technique avoids the computational cost of the full-blown saddle point oracle employed by previous work, while at the same time enabling finite-time regret bounds. Our experiments reveal that our method successfully leverages the structural assumptions, while its regret is at worst comparable to that of vanilla UCB.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {228},
numpages = {10},
series = {ICML'20}
}

@inproceedings{10.1145/3377814.3381714,
author = {K\"{a}stner, Christian and Kang, Eunsuk},
title = {Teaching software engineering for AI-enabled systems},
year = {2020},
isbn = {9781450371247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377814.3381714},
doi = {10.1145/3377814.3381714},
abstract = {Software engineers have significant expertise to offer when building intelligent systems, drawing on decades of experience and methods for building systems that are scalable, responsive and robust, even when built on unreliable components. Systems with artificial-intelligence or machine-learning (ML) components raise new challenges and require careful engineering. We designed a new course to teach software-engineering skills to students with a background in ML. We specifically go beyond traditional ML courses that teach modeling techniques under artificial conditions and focus, in lecture and assignments, on realism with large and changing datasets, robust and evolvable infrastructure, and purposeful requirements engineering that considers ethics and fairness as well. We describe the course and our infrastructure and share experience and all material from teaching the course for the first time.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering Education and Training},
pages = {45–48},
numpages = {4},
location = {Seoul, South Korea},
series = {ICSE-SEET '20}
}

@article{10.1155/2019/8127869,
author = {Zhu, Qi and Yuan, Ning and Guan, Donghai and Deng, Ke},
title = {Cognitive Driven Multilayer Self-Paced Learning with Misclassified Samples},
year = {2019},
issue_date = {2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2019},
issn = {1076-2787},
url = {https://doi.org/10.1155/2019/8127869},
doi = {10.1155/2019/8127869},
abstract = {In recent years, self-paced learning (SPL) has attracted much attention due to its improvement to nonconvex optimization based machine learning algorithms. As a methodology introduced from human learning, SPL dynamically evaluates the learning difficulty of each sample and provides the weighted learning model against the negative effects from hard-learning samples. In this study, we proposed a cognitive driven SPL method, i.e., retrospective robust self-paced learning (R2SPL), which is inspired by the following two issues in human learning process: the misclassified samples are more impressive in upcoming learning, and the model of the follow-up learning process based on large number of samples can be used to reduce the risk of poor generalization in initial learning phase. We simultaneously estimated the degrees of learning-difficulty and misclassified in each step of SPL and proposed a framework to construct multilevel SPL for improving the robustness of the initial learning phase of SPL. The proposed method can be viewed as a multilayer model and the output of the previous layer can guide constructing robust initialization model of the next layer. The experimental results show that the R2SPL outperforms the conventional self-paced learning models in classification task.},
journal = {Complex.},
month = jan,
numpages = {10}
}

@inproceedings{10.1007/978-3-030-74251-5_8,
author = {Esteves, Diego and Marcelino, Jos\'{e} and Chawla, Piyush and Fischer, Asja and Lehmann, Jens},
title = {HORUS-NER: A Multimodal Named Entity Recognition Framework for Noisy Data},
year = {2021},
isbn = {978-3-030-74250-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-74251-5_8},
doi = {10.1007/978-3-030-74251-5_8},
abstract = {Recent work based on Deep Learning presents state-of-the-art (SOTA) performance in the named entity recognition (NER) task. However, such models still have the performance drastically reduced in noisy data (e.g., social media, search engines), when compared to the formal domain (e.g., newswire). Thus, designing and exploring new methods and architectures is highly necessary to overcome current challenges. In this paper, we shift the focus of existing solutions to an entirely different perspective. We investigate the potential of embedding word-level features extracted from images and news. We performed a very comprehensive study in order to validate the hypothesis that images and news (obtained from an external source) may boost the task on noisy data, revealing very interesting findings. When our proposed architecture is used: (1) We beat SOTA in precision with simple CRFs models (2) The overall performance of decision trees-based models can be drastically improved. (3) Our approach overcomes off-the-shelf models for this task. (4) Images and text consistently increased recall over different datasets for SOTA, but at cost of precision. All experiment configurations, data and models are publicly available to the research community at},
booktitle = {Advances in Intelligent Data Analysis XIX: 19th International Symposium on Intelligent Data Analysis, IDA 2021, Porto, Portugal, April 26–28, 2021, Proceedings},
pages = {89–100},
numpages = {12},
keywords = {Multi-modal, Text, Images, Information retrieval, Noisy text, WNUT, Named entity recognition},
location = {Porto, Portugal}
}

@inproceedings{10.5555/1623755.1623763,
author = {Havens, William S. and Rehfuss, Paul Stephen},
title = {Platypus: a constraint-based reasoning system},
year = {1989},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Platypus is a constraint-based reasoning engine for synthesis, diagnosis and other recognition tasks. While its target applications are similar to those of many rule-based expert system shells, the architecture of the underlying reasoning engine is not. Platypus is part of our goal at Tektronix of defining an architecture for Smart Instruments, In this architecture, a model-based knowledge representation provides rules for constructing object-centered descriptions from the input data. The descriptions produced make explicit the entities recognized in the task domain, their identifying parameters and the semantic constraints that exist among the entities. Platypus is described using a synthesis task, the configuration of Tektronix 4300-series workstations.},
booktitle = {Proceedings of the 11th International Joint Conference on Artificial Intelligence - Volume 1},
pages = {48–53},
numpages = {6},
location = {Detroit, Michigan},
series = {IJCAI'89}
}

@article{10.1007/s13748-019-00182-2,
author = {Alonso-Abad, Jes\'{u}s M. and L\'{o}pez-Nozal, Carlos and Maudes-Raedo, Jes\'{u}s M. and Marticorena-S\'{a}nchez, Ra\'{u}l},
title = {Label prediction on issue tracking systems using text mining},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {3},
url = {https://doi.org/10.1007/s13748-019-00182-2},
doi = {10.1007/s13748-019-00182-2},
abstract = {Issue tracking systems are overall change-management tools in software development. The issue-solving life cycle is a complex socio-technical activity that requires team discussion and knowledge sharing between members. In that process, issue classification facilitates an understanding of issues and their analysis. Issue tracking systems permit the tagging of issues with default labels (e.g., bug, enhancement) or with customized team labels (e.g., test failures, performance). However, a current problem is that many issues in open-source projects remain unlabeled. The aim of this paper is to improve maintenance tasks in development teams, evaluating models that can suggest a label for an issue using its text comments. We analyze data on issues from several GitHub trending projects, first by extracting issue information and then by applying text mining classifiers (i.e., support vector machine and naive Bayes multinomial). The results suggest that very suitable classifiers may be obtained to label the issues or, at least, to suggest the most suitable candidate labels.},
journal = {Prog. in Artif. Intell.},
month = sep,
pages = {325–342},
numpages = {18},
keywords = {Label prediction, Text mining, Issue tracker system, Experimentation in software engineering, Text classifier}
}

@article{10.1016/j.neucom.2019.04.066,
author = {Zhu, Qi and Yuan, Ning and Huang, Jiashuang and Hao, Xiaoke and Zhang, Daoqiang},
title = {Multi-modal AD classification via self-paced latent correlation analysis},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {355},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.04.066},
doi = {10.1016/j.neucom.2019.04.066},
journal = {Neurocomput.},
month = aug,
pages = {143–154},
numpages = {12},
keywords = {Computer-aided diagnosis, Self-paced learning, Low-rank, Feature extraction, Multi-modal fusion}
}

@article{10.1016/j.knosys.2017.03.026,
author = {Zhang, Zhong-Liang and Luo, Xing-Gang and Garca, Salvador and Tang, Jia-Fu and Herrera, Francisco},
title = {Exploring the effectiveness of dynamic ensemble selection in the one-versus-one scheme},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.03.026},
doi = {10.1016/j.knosys.2017.03.026},
abstract = {The One-versus-One (OVO) strategy is one of the most common and effective techniques to deal with multi-class classification problems. The basic idea of an OVO scheme is to divide a multi-class classification problem into several easier-to-solve binary classification problems with considering each possible pair of classes from the original problem, which is then built into a binary classifier by an independent base learner. In this study, we propose a novel methodology which attempts to select a group of base classifiers in each pairwise dataset for each unknown pattern. To implement this, the Dynamic Ensemble Selection (DES) method based on a competence measure is employed to select the most appropriate ensemble in each binary classification problem derived from the OVO decomposition. In order to verify the validity and effectiveness of our proposed method, we carry out a thorough experimental study. We first compare our proposal with several state-of-the-art approaches. Then, we perform the comparison of several well-known aggregation strategies to combine the binary ensemble obtained by Dynamic Ensemble Selection. Finally, we explore whether further improvement can be achieved by considering the competence-based method in OVO scheme. The extracted findings drawn from the empirical analysis are supported by the proper statistical analysis and indicate that there is a positive synergy between the DES method and the Distance-based Relative Competence Weighting (DRCW) approach for the OVO scheme.},
journal = {Know.-Based Syst.},
month = jun,
pages = {53–63},
numpages = {11},
keywords = {Pairwise learning, One-versus-One, Multi-classification, Dynamic ensemble selection, Decomposition strategies}
}

@inproceedings{10.1145/3383972.3384014,
author = {Lee, Da-Young and Ko, Uram and Aitkazin, Ibrahim and Park, SangUn and Tak, Hae-Sung and Cho, Hwan-Gue},
title = {A Fast Detecting Method for Clone Functions Using Global Alignment of Token Sequences},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384014},
doi = {10.1145/3383972.3384014},
abstract = {In large software projects, proper source code reuse can make development more efficient, but a lot of duplicate code and error code reuse can be a major cause of difficult system maintenance. Efficient clone code detection for large project can help manage the project. However, most of the clone detection methods are difficult to perform on adaptive analysis that adjusts specificity or sensitivity according to the type of clone to be detected. Therefore, when a user wants to find a particular type of clone in a large project, they must analyze it repeatedly using various tools to adjust the options. In this study, we propose a clone detection system based on the global sequence alignment. Lex based token analysis models and global alignment algorithm-based clone detection models were able to detect not only exact matches but also various types of clones by setting lower bound scores. Using features of the global alignment score calculation method to eliminate functions that cannot be clone candidates in advance, alignment analysis was possible even for large projects, and the execution time was predicted. For clone functions, we visualized the matching area, which is the result of alignment analysis, to represent clone information more efficiently.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {17–22},
numpages = {6},
keywords = {global alignment, code analysis, clone function, Clone detection},
location = {Shenzhen, China},
series = {ICMLC '20}
}

@inproceedings{10.1007/978-3-030-62463-7_4,
author = {Sun, Chao and Tang, Mingjing and Liang, Li and Zou, Wei},
title = {Software Entity Recognition Method Based on BERT Embedding},
year = {2020},
isbn = {978-3-030-62462-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-62463-7_4},
doi = {10.1007/978-3-030-62463-7_4},
abstract = {The global open source software ecosystem contains rich information in the field of software engineering. The existing analysis methods for the text content of the knowledge community in this field are mainly focus on the structural relationship and rule-based association and mining. This paper proposes a software entity recognition method based on BERT word embedding. Firstly, the BiLSTM-CRF model is constructed, and the entity recognition model is constructed by combining the word vector embedding in software engineering field. Then, the word vector in the input layer of the model is improved by introducing the BERT pre-training language model. In the process of pre-training of BERT, the pre-training data should be constructed based on the discussion content of Stack Overflow software Q &amp; A community. Then, we use these data to pre-training the BERT model, so as to obtain the word vector representation suitable for software engineering field, improving the effect of entity recognition in software engineering field, and solving the problem that the traditional word vector embedding is mostly based on the general domain data training, which is not fully suitable for software engineering field, and can’t well represent the context semantic information. At the same time, to solve the problem that there are few annotated data in the field of software, this paper tries to extends the data appropriately by the method of model prediction and dictionary matching, and carries out experimental test. Finally, this paper uses the method of deep learning to realize the entity recognition in the field of software engineering, so as to provide support for the extraction of software entities, the construction of software knowledge base, and the intelligent application of software engineering.},
booktitle = {Machine Learning for Cyber Security: Third International Conference, ML4CS 2020, Guangzhou, China, October 8–10, 2020, Proceedings, Part III},
pages = {33–47},
numpages = {15},
keywords = {Stack overflow, BERT model, Entity recognition},
location = {Guangzhou, China}
}

@article{10.1016/j.neucom.2018.04.075,
author = {Xu, Wei and Liu, Wei and Huang, Xiaolin and Yang, Jie and Qiu, Song},
title = {Multi-modal self-paced learning for image classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {309},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.075},
doi = {10.1016/j.neucom.2018.04.075},
journal = {Neurocomput.},
month = oct,
pages = {134–144},
numpages = {11},
keywords = {Multi-modal, Self-paced learning, Curriculum learning, Image classification}
}

@inproceedings{10.1145/1655925.1656013,
author = {Alsawalqah, Hamad I. and Abotsi, Komi S. and Lee, Dan Hyung},
title = {An automated mechanism for organizing and retrieving core asset artifacts for product derivation in SPL},
year = {2009},
isbn = {9781605587103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1655925.1656013},
doi = {10.1145/1655925.1656013},
abstract = {Software Product Line, SPL, is a software development strategy in which products are developed from a common set of core assets in a prescribed way with product specific features to satisfy specific market segment [1]. The SPL development process is carried out in two phases: the first phase is about building core assets called domain engineering, which has gained a lot of researchers' attention. The second step is about instantiating the specifics of the products by adding to the common part the specific features that identify the product from the other application engineering. For large and complex domains, it is argued that organizing and retrieving the development of artifacts from the core asset required by the application under development is a way of shortening the application development time, thus reduces the time to market. In this paper, we propose an automation mechanism for organizing the core assets using feature based organization to divide the customized domain feature model based on the application features and their dependencies. When that retrieval step where the artifacts are represented by relations that inherit the dependencies between the features in each division of the feature model, takes place, the final result is a set of development artifacts with their traceability links to be customized based on the application variability model and integrated with the application specific artifacts. To demonstrate our work, we applied this mechanism on a watch, a case study in the digital watch domain.},
booktitle = {Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human},
pages = {480–485},
numpages = {6},
keywords = {software product line, product derivation, ontology, feature model, digital watch},
location = {Seoul, Korea},
series = {ICIS '09}
}

@article{10.1016/j.neucom.2021.05.039,
author = {Nie, Lun Yiu and Gao, Cuiyun and Zhong, Zhicong and Lam, Wai and Liu, Yang and Xu, Zenglin},
title = {CoreGen: Contextualized Code Representation Learning for Commit Message Generation},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {459},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.05.039},
doi = {10.1016/j.neucom.2021.05.039},
journal = {Neurocomput.},
month = oct,
pages = {97–107},
numpages = {11},
keywords = {Contextualized code representation, Self-supervised learning, Code-to-text generation, Code representation learning, Commit message generation}
}

@article{10.1002/smr.2343,
author = {Hosni, Mohamed and Idri, Ali and Abran, Alain},
title = {On the value of filter feature selection techniques in homogeneous ensembles effort estimation},
year = {2021},
issue_date = {June 2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {33},
number = {6},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2343},
doi = {10.1002/smr.2343},
abstract = {Software development effort estimation (SDEE) remains as the principal activity in software project management planning. Over the past four decades, several methods have been proposed to estimate the effort required to develop a software system, including more recently machine learning (ML) techniques. Because ML performance accuracy depends on the features that feed the ML technique, selecting the appropriate features in the preprocessing data step is important. This paper investigates three filter feature selection techniques to check the predictive capability of four single ML techniques: K‐nearest neighbor, support vector regression, multilayer perceptron, and decision trees and their homogeneous ensembles over six well‐known datasets. Furthermore, the single and ensembles techniques were optimized using the grid search optimization method. The results suggest that the three filter feature selection techniques investigated improve the reasonability and the accuracy performance of the four single techniques. Moreover, the homogeneous ensembles are statistically more accurate than the single techniques. Finally, adopting a random process (i.e., random subspace method) to select the inputs feature for ML technique is not always effective to generate an accurate homogeneous ensemble.Three filter feature selection techniques were investigated to check the predictive capability of four single machine learning techniques and their homogeneous ensembles over six well‐known datasets. The three filters investigated improve the reasonability and the performance of the four single techniques. Moreover, the homogeneous ensembles are statistically more accurate than the single techniques. Finally, adopting a random process to select the inputs feature for machine learning technique is not always effective to generate an accurate homogeneous ensemble.


image
image},
journal = {J. Softw. Evol. Process},
month = jun,
numpages = {38},
keywords = {software development effort estimation, machine learning, filter, feature selection, ensemble effort estimation}
}

@article{10.1007/s00500-021-05934-8,
author = {Huang, Xuan and Hu, Zhenlong and Lin, Lin},
title = {RETRACTED ARTICLE: Deep clustering based on embedded auto-encoder},
year = {2021},
issue_date = {Jan 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05934-8},
doi = {10.1007/s00500-021-05934-8},
abstract = {Deep clustering is a new research direction that combines deep learning and clustering. It performs feature representation and cluster assignments simultaneously, and its clustering performance is significantly superior to traditional clustering algorithms. The auto-encoder is a neural network model, which can learn the hidden features of the input object to achieve nonlinear dimensionality reduction. This paper proposes the embedded auto-encoder network model; specifically, the auto-encoder is embedded into the encoder unit and the decoder unit of the prototype auto-encoder, respectively. To ensure effectively cluster high-dimensional objects, the encoder of model first encodes the raw features of the input objects, and obtains a cluster-friendly feature representation. Then, in the model training stage, by adding smoothness constraints to the objective function of the encoder, the representation capabilities of the hidden layer coding are significantly improved. Finally, the adaptive self-paced learning threshold is determined according to the median distance between the object and its corresponding the centroid, and the fine-tuning sample of the model is automatically selected. Experimental results on multiple image datasets have shown that our model has fewer parameters, higher efficiency and the comprehensive clustering performance is significantly superior to the state-of-the-art clustering methods.},
journal = {Soft Comput.},
month = jun,
pages = {1075–1090},
numpages = {16},
keywords = {Deep clustering, The embedded auto-encoder, Feature representation}
}

@inproceedings{10.5555/3524938.3525116,
author = {Choo, Davin and Grunau, Christoph and Portmann, Julian and Rozho\v{n}, V\'{a}clav},
title = {k-means++: few more steps yield constant approximation},
year = {2020},
publisher = {JMLR.org},
abstract = {The k-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is a state-of-the-art algorithm for solving the k-means clustering problem and is known to give an O(log k)-approximation in expectation. Recently, Lattanzi and Sohler (ICML 2019) proposed augmenting k-means++ with O(k log log k) local search steps to yield a constant approximation (in expectation) to the k-means clustering problem. In this paper, we improve their analysis to show that, for any arbitrarily small constant ε &gt; 0, with only εk additional local search steps, one can achieve a constant approximation guarantee (with high probability in k), resolving an open problem in their paper.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {178},
numpages = {9},
series = {ICML'20}
}

@inproceedings{10.1145/3469213.3471322,
author = {Liao, Huali},
title = {Design of School-Enterprise-Students Information Sharing APP Based on the Internet Plus},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3471322},
doi = {10.1145/3469213.3471322},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {294},
numpages = {4},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@article{10.1007/s10922-013-9289-x,
author = {Bashar, Abul and Parr, Gerard and Mcclean, Sally and Scotney, Bryan and Nauck, Detlef},
title = {Application of Bayesian Networks for Autonomic Network Management},
year = {2014},
issue_date = {April     2014},
publisher = {Plenum Press},
address = {USA},
volume = {22},
number = {2},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-013-9289-x},
doi = {10.1007/s10922-013-9289-x},
abstract = {The ever evolving telecommunication networks in terms of their technology, infrastructure, and supported services have always posed challenges to the network managers to come up with an efficient Network Management System (NMS) for effective network management. The need for automated and efficient management of the current networks, more specifically the Next Generation Network (NGN), is the subject addressed in this research. A detailed description of the management challenges in the context of current networks is presented and then this work enlists the desired features and characteristics of an efficient NMS. It then proposes that there is a need to apply Artificial Intelligence (AI) and Machine Learning (ML) approaches for enhancing and automating the functions of NMS. The first contribution of this work is a comprehensive survey of the AI and ML approaches applied to the domain of NM. The second contribution of this work is that it presents the reasoning and evidence to support the choice of Bayesian Networks (BN) as a viable solution for ML-based NMS. The final contribution of this work is that it proposes and implements three novel NM solutions based on the BN approach, namely BN-based Admission Control (BNAC), BN-based Distributed Admission Control (BNDAC) and BN-based Intelligent Traffic Engineering (BNITE), along with the description of algorithms underpinning the proposed framework.},
journal = {J. Netw. Syst. Manage.},
month = apr,
pages = {174–207},
numpages = {34},
keywords = {Next Generation Networks, Network Management, Machine Learning, Intelligent Traffic Engineering, Data Mining, Call Admission Control, Bayesian Networks, Artificial Intelligence}
}

@inproceedings{10.1145/2110147.2110161,
author = {Lienhardt, Michael and Clarke, Dave},
title = {Row types for delta-oriented programming},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110161},
doi = {10.1145/2110147.2110161},
abstract = {Delta-oriented programming (DOP) provides a technique for implementing Software Product Lines based on modifications (add, remove, modify) to a core program. Unfortunately, such modifications can introduce errors into a program, especially when type signatures of classes are modified in a non-monotonic fashion. To deal with this problem we present a type system for delta-oriented programs based on row polymorphism. This exercise elucidates the close correspondence between delta-oriented programs and row polymorphism.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {121–128},
numpages = {8},
keywords = {structural typing, software product line engineering, delta-oriented programming},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@article{10.1016/j.infsof.2013.02.007,
author = {Santos Rocha, Roberto dos and Fantinato, Marcelo},
title = {The use of software product lines for business process management},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.02.007},
doi = {10.1016/j.infsof.2013.02.007},
abstract = {ContextBusiness Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT. ObjectivePresenting the results of a study undertaken to seek and assess PL approaches for BPM through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic PL approaches for BPM. MethodA SLR was conducted with four research questions formulated to evaluate PL approaches for BPM. Results63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that PLs only partially address the BPM lifecycle since the last business process phase is not a current concern on the found approaches. ConclusionsThe found PL approaches for BPM only cover partially the BPM lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic PL proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that PL approaches for BPM are still at an early stage and gaining maturity.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1355–1373},
numpages = {19},
keywords = {Software product line, PL, Business process management, BPM}
}

@inproceedings{10.1007/978-3-030-00308-1_33,
author = {O’Keeffe, Simon and Villing, Rudi},
title = {A Benchmark Data Set and Evaluation of Deep Learning Architectures for Ball Detection in the RoboCup SPL},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_33},
doi = {10.1007/978-3-030-00308-1_33},
abstract = {This paper presents a benchmark data set for evaluating ball detection algorithms in the RoboCup Soccer Standard Platform League. We created a labelled data set of images with and without ball derived from vision log files recorded by multiple NAO robots in various lighting conditions. The data set contains 5209 labelled ball image regions and 10924 non-ball regions. Non-ball image regions all contain features that had been classified as a potential ball candidate by an existing ball detector. The data set was used to train and evaluate 252 different Deep Convolutional Neural Network (CNN) architectures for ball detection. In order to control computational requirements, this evaluation focused on networks with 2–5 layers that could feasibly run in the vision and cognition cycle of a NAO robot using two cameras at full frame rate (2&nbsp;\texttimes{}&nbsp;30&nbsp;Hz). The results show that the classification performance of the networks is quite insensitive to the details of the network design including input image size, number of layers and number of outputs at each layer. In an effort to reduce the computational requirements of CNNs we evaluated XNOR-Net architectures which quantize the weights and activations of a neural network to binary values. We examined XNOR-Nets corresponding to the real-valued CNNs we had already tested in order to quantify the effect on classification metrics. The results indicate that ball classification performance degrades by 12% on average when changing from real-valued CNN to corresponding XNOR-Net.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {398–409},
numpages = {12},
keywords = {XNOR-Net, Ball detection, Deep learning, Convolution neural network},
location = {Nagoya, Japan}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {variability, developer study, configuration management and life cycle, Dimensions of software configuration},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1002/smr.1926,
author = {Horcas, Jos\'{e} Miguel and Monteil, Julien and Bouroche, M\'{e}lanie and Pinto, M\'{o}nica and Fuentes, Lidia and Clarke, Siobh\'{a}n},
title = {Context‐dependent reconfiguration of autonomous vehicles in mixed traffic},
year = {2018},
issue_date = {April 2018},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {30},
number = {4},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.1926},
doi = {10.1002/smr.1926},
abstract = {Human drivers naturally adapt their behaviour depending on the traffic conditions, such as the current weather and road type. Autonomous vehicles need to do the same, in a way that is both safe and efficient in traffic composed of both conventional and autonomous vehicles. In this paper, we demonstrate the applicability of a reconfigurable vehicle controller agent for autonomous vehicles that adapts the parameters of a used car‐following model at runtime, so as to maintain a high degree of traffic quality (efficiency and safety) under different weather conditions. We follow a dynamic software product line approach to model the variability of the car‐following model parameters, context changes and traffic quality, and generate specific configurations for each particular context. Under realistic conditions, autonomous vehicles have only a very local knowledge of other vehicles' variables. We investigate a distributed model predictive controller agent for autonomous vehicles to estimate their behavioural parameters at runtime, based on their available knowledge of the system. We show that autonomous vehicles with the proposed reconfigurable controller agent lead to behaviour similar to that achieved by human drivers, depending on the context.
–
The variability of the autonomous vehicles' behaviour, traffic context and quality is modelled in a Dynamic Software Product Line.–
The behaviour of the autonomous vehicles is adapted at runtime based on the current traffic context.–
A model predictive controller agent for autonomous vehicles optimises their behavioural parameters based on available knowledge in the traffic network.


image
image},
journal = {J. Softw. Evol. Process},
month = apr,
numpages = {15},
keywords = {traffic quality, reconfiguration, dynamic software product line, car‐following model, autonomous vehicles}
}

@article{10.1016/j.jss.2019.05.026,
author = {Umer, Qasim and Liu, Hui and Sultan, Yasir},
title = {Sentiment based approval prediction for enhancement reports},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.05.026},
doi = {10.1016/j.jss.2019.05.026},
journal = {J. Syst. Softw.},
month = sep,
pages = {57–69},
numpages = {13},
keywords = {Classification, Machine learning algorithms, Enhancement reports}
}

@article{10.1007/s00521-018-3478-1,
author = {Gu, Nannan and Fan, Pengying and Fan, Mingyu and Wang, Di},
title = {Structure regularized self-paced learning for robust semi-supervised pattern classification},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3478-1},
doi = {10.1007/s00521-018-3478-1},
abstract = {Semi-supervised classification is a hot topic in pattern recognition and machine learning. However, in presence of heavy noise and outliers, the unlabeled training data could be very challenging or even misleading for the semi-supervised classifier. In this paper, we propose a novel structure regularized self-paced learning method for semi-supervised classification problems, which can efficiently learn partially labeled training data sequentially from the simple to the complex ones. The proposed formulation consists of three components: a cost function defined by a mixture of losses, a functional complexity regularizer, and a self-paced regularizer; and the corresponding optimization algorithm involves three iterative steps: classifier updating, sample importance calculating, and pseudo-labeling. In the proposed method, the cost function for classifier updating and sample importance calculating is defined as a combination of the label fitting loss and manifold smoothness loss. Then, the importance of the pseudo-labeled and unlabeled samples is adaptively calculated by the novel cost. Unlabeled samples with high importance values are pseudo-labeled with their current predictions. In this way, labels are efficiently propagated from the labeled samples to the unlabeled ones in the robust self-paced manner. Experimental results on several benchmark data sets are provided to show the effectiveness of the proposed method.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {6559–6574},
numpages = {16},
keywords = {Locally linear coding, Manifold learning, Self-paced learning, Pattern classification, Semi-supervised classification}
}

@inproceedings{10.1145/3379597.3387500,
author = {Fry, Tanner and Dey, Tapajit and Karnauch, Andrey and Mockus, Audris},
title = {A Dataset and an Approach for Identity Resolution of 38 Million Author IDs extracted from 2B Git Commits},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387500},
doi = {10.1145/3379597.3387500},
abstract = {The data collected from open source projects provide means to model large software ecosystems, but often suffer from data quality issues, specifically, multiple author identification strings in code commits might actually be associated with one developer. While many methods have been proposed for addressing this problem, they are either heuristics requiring manual tweaking, or require too much calculation time to do pairwise comparisons for 38M author IDs in, for example, the World of Code collection. In this paper, we propose a method that finds all author IDs belonging to a single developer in this entire dataset, and share the list of all author IDs that were found to have aliases. To do this, we first create blocks of potentially connected author IDs and then use a machine learning model to predict which of these potentially related IDs belong to the same developer. We processed around 38 million author IDs and found around 14.8 million IDs to have an alias, which belong to 5.4 million different developers, with the median number of aliases being 2 per developer. This dataset can be used to create more accurate models of developer behaviour at the entire OSS ecosystem level and can be used to provide a service to rapidly resolve new author IDs.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {518–522},
numpages = {5},
keywords = {Machine Learning, Identity Resolution, Heuristics, Git Commits, Data Sharing},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@article{10.2478/cait-2020-0018,
author = {Angara, Jayasri and Prasad, Srinivas and Sridevi, Gutta},
title = {DevOps Project Management Tools for Sprint Planning, Estimation and Execution Maturity},
year = {2020},
issue_date = {Jun 2020},
publisher = {Walter de Gruyter GmbH},
address = {Berlin, DEU},
volume = {20},
number = {2},
issn = {1314-4081},
url = {https://doi.org/10.2478/cait-2020-0018},
doi = {10.2478/cait-2020-0018},
abstract = {The goal of DevOps is to cut down the project timelines, increase the productivity, and manage rapid development-deployment cycles without impacting business and quality. It requires efficient sprint management. The objective of this paper is to develop different sprint level project management tools for quick project level Go/No-Go decision making (using real-time projects data and machine learning), sprint estimation technique (gamified-consensus based), statistical understanding of overall project management maturity, project sentiment &amp; perception. An attempt is made to device a model to calibrate the perception or the tone of a project culture using sentiment analysis.},
journal = {Cybern. Inf. Technol.},
month = jun,
pages = {79–92},
numpages = {14},
keywords = {sentimental analysis, planning poker, effort estimation, Machine Learning (ML), DevOps}
}

@inproceedings{10.1007/978-3-030-78270-2_74,
author = {Yun, Yue and Dai, Huan and Cao, Ruoqi and Zhang, Yupei and Shang, Xuequn},
title = {Self-paced Graph Memory Network for Student GPA Prediction and Abnormal Student Detection},
year = {2021},
isbn = {978-3-030-78269-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78270-2_74},
doi = {10.1007/978-3-030-78270-2_74},
abstract = {Student learning performance prediction (SLPP) is a crucial step in high school education. However, traditional methods fail to consider abnormal students. In this study, we organized every student’s learning data as a graph to use the schema of graph memory networks (GMNs). To distinguish the students and make GMNs learn robustly, we proposed to train GMNs in an “easy-to-hard” process, leading to self-paced graph memory network (SPGMN). SPGMN chooses the low-difficult samples as a batch to tune the model parameters in each training iteration. This approach not only improves the robustness but also rearranges the student sample from normal to abnormal. The experiment results show that SPGMN achieves a higher prediction accuracy and more robustness in comparison with traditional methods. The resulted student sequence reveals the abnormal student has a different pattern in course selection to normal students.},
booktitle = {Artificial Intelligence in Education: 22nd International Conference, AIED 2021, Utrecht, The Netherlands, June 14–18, 2021, Proceedings, Part II},
pages = {417–421},
numpages = {5},
keywords = {Abnormal student detection, Graph memory networks, Self-paced learning, Student learning performance prediction},
location = {Utrecht, The Netherlands}
}

@article{10.1016/j.eswa.2015.02.020,
author = {Dermeval, Diego and Ten\'{o}rio, Thyago and Bittencourt, Ig Ibert and Silva, Alan and Isotani, Seiji and Ribeiro, M\'{a}rcio},
title = {Ontology-based feature modeling},
year = {2015},
issue_date = {July 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {11},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.02.020},
doi = {10.1016/j.eswa.2015.02.020},
abstract = {We compare two ontology-based feature modeling styles by conducting an experiment.The results show that ontology factor has statistical significance in all metrics.The results show that the ontology based on instances is more flexible.The results show that the ontology based on instances demands less time to change. A software product line (SPL) is a set of software systems that have a particular set of common features and that satisfy the needs of a particular market segment or mission. Feature modeling is one of the key activities involved in the design of SPLs. The feature diagram produced in this activity captures the commonalities and variabilities of SPLs. In some complex domains (e.g., ubiquitous computing, autonomic systems and context-aware computing), it is difficult to foresee all functionalities and variabilities a specific SPL may require. Thus, Dynamic Software Product Lines (DSPLs) bind variation points at runtime to adapt to fluctuations in user needs as well as to adapt to changes in the environment. In this context, relying on formal representations of feature models is important to allow them to be automatically analyzed during system execution. Among the mechanisms used for representing and analyzing feature models, description logic (DL) based approaches demand to be better investigated in DSPLs since it provides capabilities, such as automated inconsistency detection, reasoning efficiency, scalability and expressivity. Ontology is the most common way to represent feature models knowledge based on DL reasoners. Previous works conceived ontologies for feature modeling either based on OWL classes and properties or based on OWL individuals. However, considering change or evolution scenarios of feature models, we need to compare whether a class-based or an individual-based feature modeling style is recommended to describe feature models to support SPLs, and especially its capabilities to deal with changes in feature models, as required by DSPLs. In this paper, we conduct a controlled experiment to empirically compare two approaches based on each one of these modeling styles in several changing scenarios (e.g., add/remove mandatory feature, add/remove optional feature and so on). We measure time to perform changes, structural impact of changes (flexibility) and correctness for performing changes in our experiment. Our results indicate that using OWL individuals requires less time to change and is more flexible than using OWL classes and properties. These results provide insightful assumptions towards the definition of an approach relying on reasoning capabilities of ontologies that can effectively support products reconfiguration in the context of DSPL.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {4950–4964},
numpages = {15},
keywords = {Software product line, Ontology, Feature modeling, Empirical software engineering}
}

@inproceedings{10.1145/3368089.3409721,
author = {Liu, Liu and Isaacman, Sibren and Kremer, Ulrich},
title = {Global cost/quality management across multiple applications},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409721},
doi = {10.1145/3368089.3409721},
abstract = {Approximation is a technique that optimizes the balance between application outcome quality and its resource usage. Trading quality for performance has been investigated for single application scenarios, but not for environments where multiple approximate applications may run concurrently on the same machine, interfering with each other by sharing machine resources. Applying existing, single application techniques to this multi-programming environment may lead to configuration space size explosion, or result in poor overall application quality outcomes.  Our new RAPID-M system is the first cross-application con-figuration management framework. It reduces the problem size by clustering configurations of individual applications into local"similarity buckets". The global cross-applications configuration selection is based on these local bucket spaces. RAPID-M dynamically assigns buckets to applications such that overall quality is maximized while respecting individual application cost budgets.Once assigned a bucket, reconfigurations within buckets may be performed locally with minimal impact on global selections. Experimental results using six configurable applications show that even large configuration spaces of complex applications can be clustered into a small number of buckets, resulting in search space size reductions of up to 9 orders of magnitude for our six applications. RAPID-M constructs performance cost models with an average prediction error of ≤3%. For our application execution traces, RAPID-M dynamically selects configurations that lower the budget violation rate by 33.9% with an average budget exceeding rate of 6.6% as compared to other possible approaches. RAPID-M successfully finishes 22.75% more executions which translates to a 1.52X global output quality increase under high system loads. Theo verhead ofRAPID-Mis within≤1% of application execution times.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {350–361},
numpages = {12},
keywords = {Performance Prediction, Multi-Programming, Global Configuration Management, Approximate Computing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3318299.3318327,
author = {Ma, Shenglan and Yang, Lingling and Wang, Hao and Xiao, Hong and Dai, Hong-Ning and Cheng, Shuhan and Wang, Tongsen},
title = {MHDT: A Deep-Learning-Based Text Detection Algorithm for Unstructured Data in Banking},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318327},
doi = {10.1145/3318299.3318327},
abstract = {Text detection in natural scene images becomes highly demanded for unstructured data in banking. In this paper, we propose a new deep learning algorithm called MSER, Hu-moment and Deep learning for Text detection (MHDT) based on Maximum Stable Extremal Regions (MSER) and Hu-moment features. Firstly, we extract MSERs as candidate characters. Secondly, a character classifier is introduced with Hu-moment features to reduce the number of input for clustering. After single linkage clustering, a text classifier trained from a Deep Brief Network is used to delete non-text. The proposed algorithm is evaluated on the ICDAR database, and the experimental results show that the proposed algorithm yields high precision and recall rate.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {295–300},
numpages = {6},
keywords = {unstructured data, deep learning, Text detection},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@article{10.1016/j.jss.2013.10.010,
author = {White, Jules and Galindo, Jos\'{e} A. and Saxena, Tripti and Dougherty, Brian and Benavides, David and Schmidt, Douglas C.},
title = {Evolving feature model configurations in software product lines},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {87},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.10.010},
doi = {10.1016/j.jss.2013.10.010},
abstract = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps.},
journal = {J. Syst. Softw.},
month = jan,
pages = {119–136},
numpages = {18},
keywords = {Software product line, Multi-step configuration, Feature model}
}

@article{10.1007/s10472-011-9264-8,
author = {Oddi, Angelo and Rasconi, Riccardo and Cesta, Amedeo and Smith, Stephen F.},
title = {Solving job shop scheduling with setup times through constraint-based iterative sampling: an experimental analysis},
year = {2011},
issue_date = {July      2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {62},
number = {3–4},
issn = {1012-2443},
url = {https://doi.org/10.1007/s10472-011-9264-8},
doi = {10.1007/s10472-011-9264-8},
abstract = {This paper presents a heuristic algorithm for solving a job-shop scheduling problem with sequence dependent setup times and min/max separation constraints among the activities (SDST-JSSP/max). The algorithm relies on a core constraint-based search procedure, which generates consistent orderings of activities that require the same resource by incrementally imposing precedence constraints on a temporally feasible solution. Key to the effectiveness of the search procedure is a conflict sampling method biased toward selection of most critical conflicts and coupled with a non-deterministic choice heuristic to guide the base conflict resolution process. This constraint-based search is then embedded within a larger iterative-sampling search framework to broaden search space coverage and promote solution optimization. The efficacy of the overall heuristic algorithm is demonstrated empirically both on a set of previously studied job-shop scheduling benchmark problems with sequence dependent setup times and by introducing a new benchmark with setups and generalized precedence constraints.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = jul,
pages = {371–402},
numpages = {32},
keywords = {Setup times, Random-restart, Job-shop scheduling, Generalized precedence constraints, Constraint-based reasoning, 68W20, 68T20, 68M20}
}

@inproceedings{10.1007/978-3-030-32692-0_49,
author = {Peng, Shiqi and Lai, Bolin and Yao, Guangyu and Zhang, Xiaoyun and Zhang, Ya and Wang, Yan-Feng and Zhao, Hui},
title = {Learning-Based Bone Quality Classification Method for Spinal Metastasis},
year = {2019},
isbn = {978-3-030-32691-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32692-0_49},
doi = {10.1007/978-3-030-32692-0_49},
abstract = {Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries. Early detection of spinal metastasis is critical for accurate staging and optimal treatment. The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists. In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images. We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance. MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks. Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary classification sub-tasks, i.e., whether blastic and whether lytic, and leverage a multiple layer perceptron to combine their predictions. In order to make the model more robust and generalize better, self-paced learning is adopted to gradually involve from easy to more complex samples into the training process. The proposed learning-based method is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our method significantly outperforms an 121-layer DenseNet classifier in sensitivities by +12.54%, +7.23% and +29.06% for blastic, mixed and lytic lesions, respectively, meanwhile +12.33%, +23.21% and +34.25% at vertebrae level.},
booktitle = {Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings},
pages = {426–434},
numpages = {9},
keywords = {Spinal metastasis, Bone quality classification, Multi-task learning, Self-paced learning},
location = {Shenzhen, China}
}

@article{10.1016/j.jss.2018.07.054,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.054},
doi = {10.1016/j.jss.2018.07.054},
journal = {J. Syst. Softw.},
month = oct,
pages = {511–532},
numpages = {22},
keywords = {Systematic literature review, Product configuration, Extended product line}
}

@article{10.1007/s10664-018-9656-z,
author = {Blincoe, Kelly and Dehghan, Ali and Salaou, Abdoul-Djawadou and Neal, Adam and Linaker, Johan and Damian, Daniela},
title = {High-level software requirements and iteration changes: a predictive model},
year = {2019},
issue_date = {Jun 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9656-z},
doi = {10.1007/s10664-018-9656-z},
abstract = {Knowing whether a software feature will be completed in its planned iteration can help with release planning decisions. However, existing research has focused on predictions of only low-level software tasks, like bug fixes. In this paper, we describe a mixed-method empirical study on three large IBM projects. We investigated the types of iteration changes that occur. We show that up to 54% of high-level requirements do not make their planned iteration. Requirements are most often pushed out to the next iteration, but high-level requirements are also commonly moved to the next minor or major release or returned to the product or release backlog. We developed and evaluated a model that uses machine learning to predict if a high-level requirement will be completed within its planned iteration. The model includes 29 features that were engineered based on prior work, interviews with IBM developers, and domain knowledge. Predictions were made at four different stages of the requirement lifetime. Our model is able to achieve up to 100% precision. We ranked the importance of our model features and found that some features are highly dependent on project and prediction stage. However, some features (e.g., the time remaining in the iteration and creator of the requirement) emerge as important across all projects and stages. We conclude with a discussion on future research directions.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1610–1648},
numpages = {39},
keywords = {Software requirements, Release planning, Mining software repositories, Machine learning, Completion prediction}
}

@article{10.1155/2021/4327896,
author = {Xie, Shu-Tong and He, Zong-Bao and Chen, Qiong and Chen, Rong-Xin and Kong, Qing-Zhao and Song, Cun-Ying and Huang, Jiwei},
title = {Predicting Learning Behavior Using Log Data in Blended Teaching},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4327896},
doi = {10.1155/2021/4327896},
abstract = {Online and offline blended teaching mode, the future trend of higher education, has recently been widely used in colleges around the globe. In the article, we conducted a study on students’ learning behavior analysis and student performance prediction based on the data about students’ behavior logs in three consecutive years of blended teaching in a college’s “Java Language Programming” course. Firstly, the data from diverse platforms such as MOOC, Rain Classroom, PTA, and cnBlog are integrated and preprocessed. Secondly, a novel multiclass classification framework, combining the genetic algorithm (GA) and the error correcting output codes (ECOC) method, is developed to predict the grade levels of students. In the framework, GA is designed to realize both the feature selection and binary classifier selection to fit the ECOC models. Finally, key factors affecting grades are identified in line with the optimal subset of features selected by GA, which can be analyzed for teaching significance. The results show that the multiclass classification algorithm designed in this article can effectively predict grades compared with other algorithms. In addition, the selected subset of features corresponding to learning behaviors is pedagogically instructive.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@inproceedings{10.1007/978-3-030-87007-2_22,
author = {Mathioudaki, Maria and Tsoukalas, Dimitrios and Siavvas, Miltiadis and Kehagias, Dionysios},
title = {Technical Debt Forecasting Based on Deep Learning Techniques},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_22},
doi = {10.1007/978-3-030-87007-2_22},
abstract = {Technical debt (TD) is a metaphor commonly used to reflect the consequences of quality compromises that can derive short-term benefits but may result in quality decay of software products in the long run. While a broad variety of methods and tools have been proposed over the years for the identification and quantification of TD during the software development cycle, it is not until recently that researchers have turned their interest towards methods aiming to forecast the future TD evolution of a software project. Predicting the future value of TD could facilitate decision-making tasks regarding software maintenance and assist developers and project managers in taking proactive actions regarding TD repayment. In previous relevant studies, time series analysis and Machine Learning techniques have been employed in order to generate meaningful TD forecasts. While these approaches have been proven capable of producing reliable TD predictions, their predictive performance has been observed to decrease significantly for long-term predictions. To this end, in the present paper we investigate whether the adoption of Deep Learning may lead to more accurate long-term TD prediction. For this purpose, Deep Learning models are constructed, evaluated, and compared based on a dataset of five popular real-world software applications. The results of our analysis indicate that the adoption of Deep Learning results in TD forecasting models with sufficient predictive performance up&nbsp;to 150 steps ahead into the future.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {306–322},
numpages = {17},
keywords = {Deep learning, Forecasting, Technical debt, Software quality},
location = {Cagliari, Italy}
}

@inproceedings{10.1007/978-3-030-90439-5_26,
author = {Lakshya},
title = {Behaviour of Sample Selection Techniques Under Explicit Regularization},
year = {2021},
isbn = {978-3-030-90438-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90439-5_26},
doi = {10.1007/978-3-030-90439-5_26},
abstract = {There is a multitude of sample selection-based learning strategies that have been developed for learning with noisy labels. However, It has also been indicated in the literature that perhaps early stopping is better than fully training the model for getting better performance. It leads us to wonder about the behavior of the sample selection strategies under explicit regularization. To this end, we considered four of the most fundamental sample selection-based models MentorNet, Coteaching, Coteaching-plus and JoCor. We provide empirical results of applying explicit L2 regularization to the above-mentioned approaches. We also compared the results with a baseline - a vanilla CNN model trained with just regularization. We show that under explicit regularization, the pre-conceived ranking of the approaches might change. We also show several instances where the baseline was able to outperform some or all of the existing approaches. Moreover, we show that under explicit regularization, the performance gap between the approaches can also reduce.},
booktitle = {Advances in Visual Computing: 16th International Symposium, ISVC 2021, Virtual Event, October 4-6, 2021, Proceedings, Part I},
pages = {331–340},
numpages = {10}
}

@inproceedings{10.1145/3469213.3470319,
author = {Liu, Xinan and Song, Haiping and Du, Hong and Zhou, Ping and Shen, Huaibo and Ye, Honghui},
title = {Design of a Cross-platform Coordinated Control System of Task Load Resources},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470319},
doi = {10.1145/3469213.3470319},
abstract = {In allusion to the autonomous coordination problem of reconnaissance, strike and protection resources of unmanned cluster, a control system solution based on load resource level coordination is proposed in this paper, this solution is directly command different resources on the load by unmanned cluster, goes beyond the command and control of the platform this level, can realize coordination more directly and efficiently; for different types of loads, the command and control system only needs to configure the capacity attributes of load reconnaissance, strike, protection and other resources from the resource library, it has good inclusiveness and compatibility for different types of loads; it provides the possibility for the same load to conduct multiple tasks simultaneously in the future.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {116},
numpages = {6},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@article{10.1016/j.eswa.2014.12.040,
author = {Fossaceca, John M. and Mazzuchi, Thomas A. and Sarkani, Shahram},
title = {MARK-ELM},
year = {2015},
issue_date = {May 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.12.040},
doi = {10.1016/j.eswa.2014.12.040},
abstract = {Apply Multiple Kernel Boosting and Multiclass KELM to Network Intrusion Detection.Tested approach on several machine learning datasets and the KDD Cup 99 dataset.Utilized Fractional Polynomial Kernels for the Network ID problem for the first time.Requires no feature selection, minimal pre-processing and works on imbalanced data.Achieves superior detection rates and lower false alarm rates than other approaches. Detection of cyber-based attacks on computer networks continues to be a relevant and challenging area of research. Daily reports of incidents appear in public media including major ex-filtrations of data for the purposes of stealing identities, credit card numbers, and intellectual property as well as to take control of network resources. Methods used by attackers constantly change in order to defeat techniques employed by information technology (IT) teams intended to discover or block intrusions. "Zero Day" attacks whose "signatures" are not yet in IT databases are continually being uncovered. Machine learning approaches have been widely used to increase the effectiveness of intrusion detection platforms. While some machine learning techniques are effective at detecting certain types of attacks, there are no known methods that can be applied universally and achieve consistent results for multiple attack types. The focus of our research is the development of a framework that combines the outputs of multiple learners in order to improve the efficacy of network intrusion on data that contains instances of multiple classes of attacks. We have chosen the Extreme Learning Machine (ELM) as the core learning algorithm due to recent research that suggests that ELMs are straightforward to implement, computationally efficient and have excellent learning performance characteristics on par with the Support Vector Machine (SVM), one of the most widely used and best performing machine learning platforms (Liu, Gao, &amp; Li, 2012). We introduce the novel Multiple Adaptive Reduced Kernel Extreme Learning Machine (MARK-ELM) which combines Multiple Kernel Boosting (Xia &amp; Hoi, 2013) with the Multiple Classification Reduced Kernel ELM (Deng, Zheng, &amp; Zhang, 2013). We tested this approach on several machine learning datasets as well as the KDD Cup 99 (Hettich &amp; Bay, 1999) intrusion detection dataset. Our results indicate that MARK-ELM works well for the majority of University of California, Irvine (UCI) Machine Learning Repository small datasets and is scalable for larger datasets. For UCI datasets we achieved performance similar to the MKBoost Support Vector Machine (SVM) approach. In our experiments we demonstrate that MARK-ELM achieves superior detection rates and much lower false alarm rates than other approaches on intrusion detection data.},
journal = {Expert Syst. Appl.},
month = may,
pages = {4062–4080},
numpages = {19},
keywords = {Network Intrusion Detection, Multiple Kernel Learning, Multiclass Classification, Machine Learning, Kernel Selection, KDD Cup 1999, Fractional Polynomial Kernels, Extreme Learning Machine, Ensemble Learning, Cyber security, Adaptive Boosting}
}

@article{10.1016/S0140-3664(97)00094-7,
author = {Kumar, G.Prem and Venkataram, P.},
title = {Review: Artificial intelligence approaches to network management: recent advances and a survey},
year = {1997},
issue_date = {December, 1997},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {20},
number = {15},
issn = {0140-3664},
url = {https://doi.org/10.1016/S0140-3664(97)00094-7},
doi = {10.1016/S0140-3664(97)00094-7},
abstract = {With increased number of new services and users being added to the communication network, management of such networks becomes crucial to provide assured quality of service. Finding skilled managers is often a problem. To alleviate this problem and also to provide assistance to the available network managers, network management has to be automated. Many attempts have been made in this direction and it is a promising area of interest to researchers in both academia and industry. In this paper, a review of the management complexities in present day networks and artificial intelligence approaches to network management are presented.},
journal = {Comput. Commun.},
month = dec,
pages = {1313–1322},
numpages = {10},
keywords = {Network management, Integrated network management, Blackboard architecture, Artificial intelligence, AI in networks}
}

@article{10.1007/s11761-018-0238-0,
author = {S\'{a}nchez, Manuel and Aguilar, Jose and Exposito, Ernesto},
title = {Fog computing for the integration of agents and web services in an autonomic reflexive middleware},
year = {2018},
issue_date = {December  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3–4},
issn = {1863-2386},
url = {https://doi.org/10.1007/s11761-018-0238-0},
doi = {10.1007/s11761-018-0238-0},
abstract = {Service-oriented architecture (SOA) has emerged as a dominant architecture for interoperability between applications, by using a weakly coupled model based on the flexibility provided by web services, which has led to a wide range of applications, which is known as cloud computing. On the other hand, multi-agent system (MAS) is widely used in the industry, because it provides an appropriate solution to complex problems, in a proactive and intelligent way. Specifically, intelligent environments (smart city, smart classroom, cyber-physical system, and smart factory) obtain great benefits by using both architectures, because MAS endows intelligence to the environment, while SOA enables users to interact with cloud services, which improve the capabilities of the devices deployed in the environment. Additionally, the fog computing paradigm extends the cloud computing paradigm to be closer to the things that produce and act on the intelligent environment, allowing to deal with issues like mobility, real time, low latency, geo-localization, among other aspects. In this sense, in this article we present a middleware, which not only is capable of allowing MAS and SOA to communicate in a bidirectional and transparent way, but also it uses the fog computing paradigm autonomously, according to the context and to the system load factor. Additionally, we analyze the performance of the incorporation of the fog computing paradigm in our middleware and compare it with other works.},
journal = {Serv. Oriented Comput. Appl.},
month = dec,
pages = {333–347},
numpages = {15},
keywords = {SOA, MAS, Intelligent environment, Integration, Fog computing, Cloud computing}
}

@article{10.1016/j.compind.2011.08.003,
author = {Huang, Yi and Williams, Brian C. and Zheng, Li},
title = {Reactive, model-based monitoring in RFID-enabled manufacturing},
year = {2011},
issue_date = {October, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {62},
number = {8–9},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2011.08.003},
doi = {10.1016/j.compind.2011.08.003},
abstract = {Radio frequency identification (RFID) technology is widely applied throughout the manufacturing lifecycle to track important objects. It is imperative to establish a reactive monitoring system to convert large volumes of RFID data into meaningful information. Two major challenges must therefore be addressed: modeling complexity and monitoring efficiency. When the relationship between high-level ''untagged'' elements (business processes, indicators) and low-level ''tagged'' elements (persons, objects) becomes intricate, it will become difficult to define, maintain and reason by only using traditional rule-based event processing approaches. In order to resolve this problem, the paper proposes a reactive, model-based approach to monitor important elements by estimating their most likely states according to RFID information and a constraint-based model. The approach contains two phases. (1) The model-based programming phase formalizes RFID information and flexibly embeds them into the model that specifies the monitoring rules. (2) The model-based monitoring phase interprets the model and estimates desired states efficiently. Therefore, the RFID-enabled manufacturing units are capable of responding in a timely manner to unexpected disturbances or other important issues.},
journal = {Comput. Ind.},
month = oct,
pages = {811–819},
numpages = {9},
keywords = {Reactive monitoring, RFID-enabled manufacturing, Model-based programming, Model-based monitoring, Mode estimation, Constraint-based reasoning}
}

@article{10.1016/j.infsof.2021.106665,
author = {Panichella, Sebastiano and Canfora, Gerardo and Di Sorbo, Andrea},
title = {“Won’t We Fix this Issue?” Qualitative characterization and automated identification of wontfix issues on GitHub},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106665},
doi = {10.1016/j.infsof.2021.106665},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {14},
keywords = {Machine learning, Empirical study, Issue management, Issue tracking}
}

@article{10.1016/j.scico.2013.07.016,
author = {Bettini, Lorenzo and Damiani, Ferruccio and Schaefer, Ina},
title = {Implementing type-safe software product lines using parametric traits},
year = {2015},
issue_date = {January 2015},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {97},
number = {P3},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2013.07.016},
doi = {10.1016/j.scico.2013.07.016},
abstract = {A software product line (SPL) is a set of related software systems with well-defined commonality and variability that are developed by reusing common artifacts. In this paper, we present a novel technique for implementing SPLs by exploiting mechanisms for fine-grained reuse which are orthogonal to class-based inheritance. In our approach the concepts of type, behavior, and state are separated into different and orthogonal linguistic concepts: interfaces, traits and classes, respectively. We formalize our proposal by means of Featherweight Parametric Trait Java (FPTJ), a minimal core calculus where units of product functionality are modeled by parametric traits. Traits are a well-known construct for fine-grained reuse of behavior. Parametric traits are traits parameterized by interface names and class names. Parametric traits are applied to interface names and class names to generate traits that can be assembled in other (possibly parametric) traits or in classes that are used to build products. The composition of product functionality is realized by explicit operators of the calculus, allowing code manipulations for modeling product variability. The FPTJ type system ensures that the products in the SPL are type-safe by inspecting the parametric traits and classes shared by different products only once. Therefore, type-safety of an extension of a (type-safe) FPTJ SPL can be guaranteed by inspecting only the newly added parts. We present a technique for implementing SPLs by mechanisms for fine-grained reuse.We formalize our proposal by means of a minimal core calculus.The type system ensures that all the products in the SPL are type-safe.Type-safety of SPL extensions can be checked by inspecting only newly added parts.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {282–308},
numpages = {27},
keywords = {Type system, Trait, Software product line, Feature model, Featherweight Java}
}

@inproceedings{10.1145/2857546.2857608,
author = {Rahmat, Azizah and Kassim, Suzana and Selamat, Mohd Hasan and Hassan, Sa'adah},
title = {Actor in Multi Product Line},
year = {2016},
isbn = {9781450341424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857546.2857608},
doi = {10.1145/2857546.2857608},
abstract = {Software product line (SPL) involved variability modeling in domain engineering that will be matched to the respected application engineering. Several researches existed within the scope of mapping from reference architecture (RA) in domain engineering to system architecture in application engineering within the same domain. However, the mapping of cross domain RA or Multi Product Line (MPL) required more systematic mapping due to the several participating product line architecture (PLA) that will further instantiated to specific system architecture. The objective of this paper was to propose an actor-oriented approach in the mapping process of reference architecture, product line architecture and system architecture of MPL. Since the reference architecture consisted of several components, the scope of this research was within the functional decomposition or source code level. The experiment was involving the runtime behavior of the java code. The code with actor-oriented approach had shown the least amount of time taken to complete the main method compared to the non-actor-oriented approach. In conclusion, actor-oriented approach performs better performance in the mapping of reference architecture to product line architecture and system architecture. For future work, the consistency of the mapping will be evaluated.},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
articleno = {61},
numpages = {8},
keywords = {reference architecture, multi product line, cross-domain reference architecture, actor, Software product line},
location = {Danang, Viet Nam},
series = {IMCOM '16}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00010,
author = {Saini, Nishrith and Britto, Ricardo},
title = {Using machine intelligence to prioritise code review requests},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00010},
doi = {10.1109/ICSE-SEIP52600.2021.00010},
abstract = {Modern Code Review (MCR) is the process of reviewing new code changes that need to be merged with an existing codebase. As a developer, one may receive many code review requests every day, i.e., the review requests need to be prioritised. Manually prioritising review requests is a challenging and time-consuming process. To address the above problem, we conducted an industrial case study at Ericsson aiming at developing a tool called Pineapple, which uses a Bayesian Network to prioritise code review requests. To validate our approach/tool, we deployed it in a live software development project at Ericsson, wherein more than 150 developers develop a telecommunication product. We focused on evaluating the predictive performance, feasibility, and usefulness of our approach. The results indicate that Pineapple has competent predictive performance (RMSE = 0.21 and MAE = 0.15). Furthermore, around 82.6% of Pineapple's users believe the tool can support code review request prioritisation by providing reliable results, and around 56.5% of the users believe it helps reducing code review lead time. As future work, we plan to evaluate Pineapple's predictive performance, usefulness, and feasibility through a longitudinal investigation.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {11–20},
numpages = {10},
keywords = {prioritisation, modern code review, machine reasoning, machine learning, machine intelligence, bayesian networks},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.5555/3524938.3525278,
author = {Gopi, Sivakanth and Gulhane, Pankaj and Kulkarni, Janardhan and Shen, Judy Hanwen and Shokouhi, Milad and Yekhanin, Sergey},
title = {Differentially private set union},
year = {2020},
publisher = {JMLR.org},
abstract = {We study the basic operation of set union in the global model of differential privacy. In this problem, we are given a universe U of items, possibly of infinite size, and a database D of users. Each user i contributes a subset Wi ⊆ U of items. We want an (ε,δ)-differentially private Algorithm which outputs a subset S ⊂ UiWi such that the size of S is as large as possible. The problem arises in countless real world applications, and is particularly ubiquitous in natural language processing (NLP) applications. For example, discovering words, sentences, n-grams etc., from private text data belonging to users is an instance of the set union problem. In this paper we design new algorithms for this problem that significantly outperform the best known algorithms.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {340},
numpages = {10},
series = {ICML'20}
}

@article{10.1002/smr.2323,
author = {Rafi, Saima and Yu, Wu and Akbar, Muhammad Azeem and Mahmood, Sajjad and Alsanad, Ahmed and Gumaei, Abdu},
title = {Readiness model for DevOps implementation in software organizations},
year = {2021},
issue_date = {April 2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {33},
number = {4},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2323},
doi = {10.1002/smr.2323},
abstract = {DevOps is a new software engineering paradigm adopted by various software organizations to develop the quality software within time and budget. The implementation of DevOps practices is critical, and there are no guidelines to assess and improve the DevOps activities in software organizations. Hence, there is a need to develop a readiness model for DevOps (RMDevOps) with an aim to assist the practitioners for implementation of DevOps practices in software firms. To achieve the study objective, we conducted a systematic literature review (SLR) study to identify the critical challenges and associated best practices of DevOps. A total of 18 challenges and 73 best practices were identified from the 69 primary studies. The identified challenges and best practices were further evaluated by conducting a survey with industry practitioners. The RMDevOps was developed based on other well‐established models in software engineering domain, for example, software process improvement readiness model (SPIRM) and software outsourcing vendor readiness model (SOVRM). Finally, case studies were conducted with three different organizations with an aim to validate the developed model. The results show that the RMDevOps is effective to assess and improve the DevOps practices in software organizations.},
journal = {J. Softw. Evol. Process},
month = apr,
numpages = {25},
keywords = {readiness model, guidelines, case study, best practices}
}

@inproceedings{10.1145/3469213.3470252,
author = {Huang, Ruchang and Qi, Xiaopeng and He, Jing},
title = {Task-oriented system test parameter automatic detection technology},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470252},
doi = {10.1145/3469213.3470252},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {52},
numpages = {8},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@article{10.1002/smr.2292,
author = {Akbar, Muhammad Azeem and Shameem, Mohammad and Khan, Arif Ali and Nadeem, Mohammad and Alsanad, Ahmed and Gumaei, Abdu},
title = {A fuzzy analytical hierarchy process to prioritize the success factors of requirement change management in global software development},
year = {2021},
issue_date = {February 2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {33},
number = {2},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2292},
doi = {10.1002/smr.2292},
abstract = {Planning and managing of requirement change management (RCM) process in global software development (GSD) are a complicated task, but the RCM plays a significant role in developing the quality software within time and budget. The key aim of this study is to prioritize the factors that could positively influence the RCM program in GSD context. To achieve the study objective, the questionnaire survey study was conducted to get the feedback of the practitioners concerning the success factors of RCM in GSD context. Moreover, the fuzzy analytical hierarchy process (FAHP) was applied. The application of FAHP is novel in this research domain as it has been effectively applied previously in various other research areas, for example, supplier selection, electronics and electrical, personnel selection, and agile software development. The results of this study will provide the prioritization‐based taxonomy of RCM success factors and also contribute by introducing the novel FAHP approach in the research domain of RCM in GSD. The FAHP approach assists the practitioners to reduce the uncertainty and vague opinions of RCM experts.},
journal = {J. Softw. Evol. Process},
month = feb,
numpages = {38},
keywords = {requirement change management (RCM), global software development (GSD), fuzzy analytical hierarchy process (FAHP)}
}

@inproceedings{10.1109/ASE.2019.00052,
author = {Horton, Eric and Parnin, Chris},
title = {V2: fast detection of configuration drift in Python},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00052},
doi = {10.1109/ASE.2019.00052},
abstract = {Code snippets are prevalent, but are hard to reuse because they often lack an accompanying environment configuration. Most are not actively maintained, allowing for drift between the most recent possible configuration and the code snippet as the snippet becomes out-of-date over time. Recent work has identified the problem of validating and detecting out-of-date code snippets as the most important consideration for code reuse. However, determining if a snippet is correct, but simply out-of-date, is a non-trivial task. In the best case, breaking changes are well documented, allowing developers to manually determine when a code snippet contains an out-of-date API usage. In the worst case, determining if and when a breaking change was made requires an exhaustive search through previous dependency versions.We present V2, a strategy for determining if a code snippet is out-of-date by detecting discrete instances of configuration drift, where the snippet uses an API which has since undergone a breaking change. Each instance of configuration drift is classified by a failure encountered during validation and a configuration patch, consisting of dependency version changes, which fixes the underlying fault. V2 uses feedback-directed search to explore the possible configuration space for a code snippet, reducing the number of potential environment configurations that need to be validated. When run on a corpus of public Python snippets from prior research, V2 identifies 248 instances of configuration drift.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {477–488},
numpages = {12},
keywords = {environment inference, dependencies, configuration repair, configuration management, configuration drift},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1016/j.infsof.2019.07.009,
author = {Gomes, Luiz Alberto Ferreira and Torres, Ricardo da Silva and C\^{o}rtes, Mario L\'{u}cio},
title = {Bug report severity level prediction in open source software: A survey and research opportunities},
year = {2019},
issue_date = {Nov 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {115},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.07.009},
doi = {10.1016/j.infsof.2019.07.009},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {58–78},
numpages = {21},
keywords = {Machine learning, Systematic mapping, Software repositories, Severity level prediction, Bug reports, Bug tracking systems, Software maintenance}
}

@inproceedings{10.1007/978-3-030-82136-4_43,
author = {Zhu, Ziye and Wang, Yu and Li, Yun},
title = {TroBo: A Novel Deep Transfer Model for&nbsp;Enhancing Cross-Project Bug&nbsp;Localization},
year = {2021},
isbn = {978-3-030-82135-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82136-4_43},
doi = {10.1007/978-3-030-82136-4_43},
abstract = {Bug localization, which aims to locate buggy files in the software project by leveraging bug reports, plays an important role in software quality control. Recently, many automatic bug localization methods based on historical bug-fix data (i.e., bug reports labeled with corresponding buggy code files) have been proposed. However, the lack of bug-fix data for software projects in the early stages of development limits the performance of most existing supervised learning methods. To address this issue, we propose a deep transfer bug localization model called TroBo, which can transfer shared knowledge from label-rich source project to the target project. Specifically, we accomplish the knowledge transfer on both the bug report and code file. For processing bug reports, which belong to informal text data, we design a soft attention-based module to alleviate the noise problem. For processing code files, we apply an adversarial strategy to learn the project-shared features, and additionally extract project-exclusive features for each project. Furthermore, a project-aware classifier is introduced in TroBo to avoid redundancy between shared and exclusive features. Extensive experiments on four large-scale real-world projects demonstrate that our model significantly outperforms the state-of-the-art techniques.},
booktitle = {Knowledge Science, Engineering and Management: 14th International Conference, KSEM 2021, Tokyo, Japan, August 14–16, 2021, Proceedings, Part I},
pages = {529–541},
numpages = {13},
keywords = {Bug-fix data, Attention mechanism, Adversarial training, Transfer learning, Bug localization},
location = {Tokyo, Japan}
}

@inproceedings{10.5555/3524938.3525504,
author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U. and Jaggi, Martin},
title = {Extrapolation for large-batch training in deep learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Deep learning networks are typically trained by Stochastic Gradient Descent (SGD) methods that iteratively improve the model parameters by estimating a gradient on a very small fraction of the training data. A major roadblock faced when increasing the batch size to a substantial fraction of the training data for reducing training time is the persistent degradation in performance (generalization gap). To address this issue, recent work propose to add small perturbations to the model parameters when computing the stochastic gradients and report improved generalization performance due to smoothing effects. However, this approach is poorly understood; it requires often model-specific noise and fine-tuning.To alleviate these drawbacks, we propose to use instead computationally efficient extrapolation (extragradient) to stabilize the optimization trajectory while still benefiting from smoothing to avoid sharp minima. This principled approach is well grounded from an optimization perspective and we show that a host of variations can be covered in a unified framework that we propose. We prove the convergence of this novel scheme and rigorously evaluate its empirical performance on ResNet, LSTM, and Transformer. We demonstrate that in a variety of experiments the scheme allows scaling to much larger batch sizes than before whilst reaching or surpassing SOTA accuracy.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {566},
numpages = {11},
series = {ICML'20}
}

@inproceedings{10.1007/978-3-030-26061-3_3,
author = {Akhtiamov, Oleg and Fedotov, Dmitrii and Minker, Wolfgang},
title = {A Comparative Study of Classical and Deep Classifiers for Textual Addressee Detection in Human-Human-Machine Conversations},
year = {2019},
isbn = {978-3-030-26060-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26061-3_3},
doi = {10.1007/978-3-030-26061-3_3},
abstract = {The problem of addressee detection (AD) arises in multiparty conversations involving several dialogue agents. In order to maintain such conversations in a realistic manner, an automatic spoken dialogue system is supposed to distinguish between computer- and human-directed utterances since the latter utterances either need to be processed in a specific way or should be completely ignored by the system. In the present paper, we consider AD to be a text classification problem and model three aspects of users’ speech (syntactical, lexical, and semantical) that are relevant to AD in German. We compare simple classifiers operating with supervised text representations learned from in-domain data and more advanced neural network-based models operating with unsupervised text representations learned from in- and out-of-domain data. The latter models provide a small yet significant AD performance improvement over the classical ones on the Smart Video Corpus. A neural network-based semantical model determines the context of the first four words of an utterance to be the most informative for AD, significantly surpasses syntactical and lexical text classifiers and keeps up with a baseline multimodal metaclassifier that utilises acoustical information in addition to textual data. We also propose an effective approach to building representations for out-of-vocabulary words.},
booktitle = {Speech and Computer: 21st International Conference, SPECOM 2019, Istanbul, Turkey, August 20–25, 2019, Proceedings},
pages = {20–30},
numpages = {11},
keywords = {Spoken dialogue system, Human-computer interaction, Speaking style, Text classification},
location = {Istanbul, Turkey}
}

@inproceedings{10.1145/3469213.3470404,
author = {Liu, Kai and Xiao, Qinghai and Wang, Lijun},
title = {Design of application evaluation platform for cryptography in IoT system},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470404},
doi = {10.1145/3469213.3470404},
abstract = {In order to improve network security protection capabilities, according to the requirements of the classified protection 2.0 of cybersecurity, the network is divided into five grades. Networks that involve national security and social public interests must be graded and evaluated. In the level evaluation stage, the cryptographic products used on the network need to be approved and comply with the regulations of State Cryptography Administration Office of Security Commercial Code Administration (OSCCA). The use of commercial cryptographic technology in the network must meet the standardization requirements. Based on this, the evaluation platform of commercial cryptography application was designed. The functional design of the platform is based on the cryptographic technology of the existing information system, summarizing the application characteristics of the mainstream commercial cryptographic technology in the network, including 7 types of testing items, and testing the cryptographic functions of the system according to the cryptographic industry standards and national standards. Verify the authenticity of the information source, the integrity of the data, the confidentiality of the information, and the non-repudiation of the behavior. At present, the main functions of the platform have been designed and built, and all functions such as user management, database services, and core testing have been verified. The later will be further improved to promote the standardization and correctness of commercial cryptographic system applications. The system self-inspection and self-inspection monitoring methods derived from the evaluation platform of commercial cryptography application are provided to the system builders and operators, providing effective commercial cryptographic system testing tools for cryptographic evaluation agencies, and powerful monitoring methods for supervision and inspection agencies.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {197},
numpages = {7},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/3379597.3387457,
author = {Pecorelli, Fabiano and Palomba, Fabio and Khomh, Foutse and De Lucia, Andrea},
title = {Developer-Driven Code Smell Prioritization},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387457},
doi = {10.1145/3379597.3387457},
abstract = {Code smells are symptoms of poor implementation choices applied during software evolution. While previous research has devoted effort in the definition of automated solutions to detect them, still little is known on how to support developers when prioritizing them. Some works attempted to deliver solutions that can rank smell instances based on their severity, computed on the basis of software metrics. However, this may not be enough since it has been shown that the recommendations provided by current approaches do not take the developer's perception of design issues into account. In this paper, we perform a first step toward the concept of developer-driven code smell prioritization and propose an approach based on machine learning able to rank code smells according to the perceived criticality that developers assign to them. We evaluate our technique in an empirical study to investigate its accuracy and the features that are more relevant for classifying the developer's perception. Finally, we compare our approach with a state-of-the-art technique. Key findings show that the our solution has an F-Measure up to 85% and outperforms the baseline approach.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {220–231},
numpages = {12},
keywords = {Machine Learning for Software Engineering, Empirical Software Engineering, Code smells},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1109/ICSE43902.2021.00099,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-Box Performance-Influence Models: A Profiling and Learning Approach},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00099},
doi = {10.1109/ICSE43902.2021.00099},
abstract = {Many modern software systems are highly configurable, allowing the user to tune them for performance and more. Current performance modeling approaches aim at finding performance-optimal configurations by building performance models in a black-box manner. While these models provide accurate estimates, they cannot pinpoint causes of observed performance behavior to specific code regions. This does not only hinder system understanding, but it also complicates tracing the influence of configuration options to individual methods.We propose a white-box approach that models configuration-dependent performance behavior at the method level. This allows us to predict the influence of configuration decisions on individual methods, supporting system understanding and performance debugging. The approach consists of two steps: First, we use a coarse-grained profiler and learn performance-influence models for all methods, potentially identifying some methods that are highly configuration- and performance-sensitive, causing inaccurate predictions. Second, we re-measure these methods with a fine-grained profiler and learn more accurate models, at higher cost, though. By means of 9 real-world Java software systems, we demonstrate that our approach can efficiently identify configuration-relevant methods and learn accurate performance-influence models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1059–1071},
numpages = {13},
keywords = {software variability, software product lines, performance, Configuration management},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1016/j.neucom.2019.06.072,
author = {Xu, Wei and Liu, Wei and Chi, Haoyuan and Qiu, Song and Jin, Yu},
title = {Self-paced learning with privileged information},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {362},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.072},
doi = {10.1016/j.neucom.2019.06.072},
journal = {Neurocomput.},
month = oct,
pages = {147–155},
numpages = {9},
keywords = {Learning with privileged information, Self-paced learning, Curriculum learning}
}

@article{10.1287/inte.2020.1064,
author = {Bogojeska, Jasmina and Giurgiu, Ioana and Stark, George and Wiesmann, Dorothea},
title = {IBM Predictive Analytics Reduces Server Downtime},
year = {2021},
issue_date = {January-February 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {51},
number = {1},
issn = {0092-2102},
url = {https://doi.org/10.1287/inte.2020.1064},
doi = {10.1287/inte.2020.1064},
abstract = {IBM has deployed its Predictive Analytics for Server Incident Reduction (PASIR) solution to more than 360 information technology (IT) environments worldwide since 2013. These environments, covering sectors from banking to travel to e-commerce, are serviced by IBM support groups. Incidents occurring on servers, including problem descriptions and resolutions, are documented in client account-specific ticket management systems. PASIR uses machine learning to classify the incident tickets within an IT environment and identify high-impact incidents that involve server outages by using the respective ticket descriptions and resolutions. It then correlates these high-impact tickets with server properties and utilization measurements to identify problematic server configurations. Finally, for such configurations, PASIR uses statistical multivariate analysis and simulation methods to prescribe improvement and modernization actions. In this paper, we present the results achieved from deploying this solution. We describe the PASIR approach, from ticket classification to the recommendations of remediation actions (e.g., hardware and software upgrades). We demonstrate the model’s effectiveness by comparing predictions on the impact of prescriptive actions with actual system improvements. Since 2013, we have applied PASIR to more than 840,000 client servers, resulting in more precise upgrade spending and environmental stability, thus saving our clients an estimated $7 billion.},
journal = {Interfaces},
month = feb,
pages = {63–75},
numpages = {13},
keywords = {Edelman Award, text classification, random forest, gradient boosting machine, machine learning, predictive analytics}
}

@inbook{10.5555/3454287.3455282,
author = {Saxena, Shreyas and Tuzel, Oncel and DeCoste, Dennis},
title = {Data parameters: a new family of parameters for learning a differentiable curriculum},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent works have shown that learning from easier instances first can help deep neural networks (DNNs) generalize better. However, knowing which data to present during different stages of training is a challenging problem. In this work, we address this problem by introducing data parameters. More specifically, we equip each sample and class in a dataset with a learnable parameter (data parameters), which governs their importance in the learning process. During training, at each iteration, as we update the model parameters, we also update the data parameters. These updates are done by gradient descent and do not require hand-crafted rules or design. When applied to image classification task on CIFAR10, CIFAR100, WebVision and ImageNet datasets, and object detection task on KITTI dataset, learning a dynamic curriculum via data parameters leads to consistent gains, without any increase in model complexity or training time. When applied to a noisy dataset, the proposed method learns to learn from clean images and improves over the state-of-the-art methods by 14%. To the best of our knowledge, our work is the first curriculum learning method to show gains on large scale image classification and detection tasks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {995},
numpages = {11}
}

@article{10.1016/j.jss.2019.02.028,
author = {Jakubovski Filho, Helson Luiz and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Preference based multi-objective algorithms applied to the variability testing of software product lines},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.028},
doi = {10.1016/j.jss.2019.02.028},
journal = {J. Syst. Softw.},
month = may,
pages = {194–209},
numpages = {16},
keywords = {Preference-Based algorithms, Search-Based software engineering, Software product line testing}
}

@article{10.1109/TSE.2007.70773,
author = {Kim, Sunghun and Whitehead, E. James and Zhang, Yi},
title = {Classifying Software Changes: Clean or Buggy?},
year = {2008},
issue_date = {March 2008},
publisher = {IEEE Press},
volume = {34},
number = {2},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2007.70773},
doi = {10.1109/TSE.2007.70773},
abstract = {This paper introduces a new technique for finding latent software bugs called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes, or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project, as stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean with 78% accuracy and 65% buggy change recall (on average). Change classification has several desirable qualities: (1) the prediction granularity is small (a change to a single file), (2) predictions do not require semantic information about the source code, (3) the technique works for a broad array of project types and programming languages, and (4) predictions can be made immediately upon completion of a change. Contributions of the paper include a description of the change classification approach, techniques for extracting features from source code and change histories, a characterization of the performance of change classification across 12 open source projects, and evaluation of the predictive power of different groups of features.},
journal = {IEEE Trans. Softw. Eng.},
month = mar,
pages = {181–196},
numpages = {16},
keywords = {classification, and association rules, Software maintenance, Metrics/Measurement, Data mining, Configuration Management, Clustering}
}

@article{10.3233/JIFS-210246,
author = {Alagarsamy, Ramachandran and Arunpraksh, R. and Ganapathy, Sannasi and Rajagopal, Aghila and Kavitha, R.J.},
title = {A fuzzy content recommendation system using similarity analysis, content ranking and clustering},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-210246},
doi = {10.3233/JIFS-210246},
abstract = {Recently, the e-learners are drastically increased from the last two decades. Everything is learnt through internet without help of the tutor as well. For this purpose, the e-learners are required more e-learning applications that are able to supply optimal and satisfied data based on their capability. No content recommendation system is available for recommending suitable contents to the learners. For this purpose, this paper proposes a new semantic and fuzzy aware content recommendation system for retrieving the suitable content for the users. In this content recommendation system, we propose two content pre-processing algorithms namely Target Keyword based Data Pre-processing Algorithm (TKDPA) and Intelligent Anova-T Residual Algorithm (IAATRA) for selecting the more relevant features from the document. Moreover, a new Fuzzy rule based Similarity Matching algorithm (FRSMA) is proposed and used in this system for finding the similarity between the two terms and also rank them by using the newly proposed Similarity and Temporal aware Weighted Document Ranking Algorithm (STWDRA). In addition, a content clustering process is also incorporated for gathering relevant content. Finally, a new Fuzzy, Target Keyword and Similarity Score based Content Recommendation Algorithm (FTKSCRA) is also proposed for recommending the more relevant content to the learners accurately. The experiments have been conducted for evaluating the proposed content recommendation system and proved as better than the existing recommendation systems in terms of precision, recall, f-measure and prediction accuracy.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6429–6441},
numpages = {13},
keywords = {fuzzy rules and annova-T, semantic analysis, content recommendation, clustering, content ranking, Fuzzy logic}
}

@article{10.1007/s11042-019-7498-3,
author = {Kaur, Taranjit and Saini, Barjinder Singh and Gupta, Savita},
title = {An adaptive fuzzy K-nearest neighbor approach for MR brain tumor image classification using parameter free bat optimization algorithm},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {15},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7498-3},
doi = {10.1007/s11042-019-7498-3},
abstract = {This paper presents an automatic diagnosis system for the tumor grade classification through magnetic resonance imaging (MRI). The diagnosis system involves a region of interest (ROI) delineation using intensity and edge magnitude based multilevel thresholding algorithm. Then the intensity and the texture attributes are extracted from the segregated ROI. Subsequently, a combined approach known as Fisher+ Parameter-Free BAT (PFreeBAT) optimization is employed to derive the optimal feature subset. Finally, a novel learning approach dubbed as PFree BAT enhanced fuzzy K-nearest neighbor (FKNN) is proposed by combining FKNN with PFree BAT for the classification of MR images into two categories: High and Low-Grade. In PFree BAT enhanced FKNN, the model parameters, i.e., neighborhood size k and the fuzzy strength parameter m are adaptively specified by the PFree BAT optimization approach. Integrating PFree BAT with FKNN enhances the classification capability of the FKNN. The diagnostic system is rigorously evaluated on four MR images datasets including images from BRATS 2012 database and the Harvard repository using classification performance metrics. The empirical results illustrate that the diagnostic system reached to ceiling level of accuracy on the test MR image dataset via 5-fold cross-validation mechanism. Additionally, the proposed PFree BAT enhanced FKNN is evaluated on the Parkinson dataset (PD) from the UCI repository having the pre-extracted feature space. The proposed PFree BAT enhanced FKNN reached to an average accuracy of 98% and 97.45%. with and without feature selection on PD dataset. Moreover, solely to contrast, the performance of the proposed PFree BAT enhanced FKNN with the existing FKNN variants the experimentations were also done on six other standard datasets from KEEL repository. The results indicate that the proposed learning strategy achieves the best value of accuracy in contrast to the existing FKNN variants.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {21853–21890},
numpages = {38},
keywords = {Model parameters, Diagnosis system, PFree BAT optimization, Fuzzy K-nearest neighbor}
}

@article{10.1016/j.eswa.2021.114781,
author = {Kerr, Emmett and McGinnity, T.M. and Coleman, Sonya and Shepherd, Andrea},
title = {Human vital sign determination using tactile sensing and fuzzy triage system},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {175},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114781},
doi = {10.1016/j.eswa.2021.114781},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {16},
keywords = {Human vital sign detection, Classification, Artificial intelligence, Tactile sensing, Signal processing, Automated triage, Fuzzy systems}
}

@article{10.1007/s00354-021-00126-2,
author = {Li, Peipei and Wu, Man and He, Junhong and Hu, Xuegang},
title = {Recurring Drift Detection and Model Selection-Based Ensemble Classification for Data Streams with Unlabeled Data},
year = {2021},
issue_date = {Aug 2021},
publisher = {Ohmsha},
address = {JPN},
volume = {39},
number = {2},
issn = {0288-3635},
url = {https://doi.org/10.1007/s00354-021-00126-2},
doi = {10.1007/s00354-021-00126-2},
abstract = {Data stream classification is widely popular in the field of network monitoring, sensor network and electronic commerce, etc. However, in the real-world applications, recurring concept drifting and label missing in data streams seriously aggravate the difficulty on the classification solutions. And this challenge has received little attention from the research community. Motivated by this, we propose a new ensemble classification approach based on the recurring concept drifting detection and model selection for data streams with unlabeled data. First, we build an ensemble model based on the classifiers and clusters. To improve the classification accuracy, we use the ensemble model to predict each data chunk and partition clusters according to the distribution of predicted class labels. Second, we adopt a new concept drifting detection method based on the divergence of concept distributions between adjoining data chunks to distinguish recurring concept drifts. All historical new concepts will be maintained. Meanwhile, we introduce the time-stamp-based weights for base models in the ensemble model. In the selection of the base model, we consider the time-stamp-based weight and the divergence between concept distributions simultaneously. Finally, extensive experiments conducted on four benchmark data sets show that our approach can quickly adapt to data streams with recurring concept drifts, and improve the classification accuracy compared to several state-of-the-art classification algorithms for data streams with concept drifts and unlabeled data.},
journal = {New Gen. Comput.},
month = aug,
pages = {341–376},
numpages = {36},
keywords = {Unlabeled data, Recurring concept drift, Ensemble learning, Data stream classification}
}

@inproceedings{10.1609/aaai.v33i01.33019398,
author = {Adler, Aaron and Samouelian, Peter and Atighetchi, Michael and Fu, Yat},
title = {Remote management of boundary protection devices with information restrictions},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33019398},
doi = {10.1609/aaai.v33i01.33019398},
abstract = {Boundary Protection Devices (BPDs) are used by US Government mission partners to regulate the flow of information across networks of differing security levels. BPDs provide several critical functions, including preventing unauthorized sharing, sanitizing information, and preventing cyber attacks. Their application in national security and critical infrastructure environments (e.g., military missions, nuclear power plants, clean water distribution systems) calls for a comprehensive load monitoring system that provides resilience and scalability, as well as an automated and vendor neutral configuration management system that can efficiently respond to security threats at machine speed. Their design as one-way traffic control systems, however, presents challenges for dynamic load adaptation techniques that require access to application server performance metrics across network boundaries. Moreover, the structured review and approval process that regulates their configuration and use presents two significant challenges: (1) Adaptation techniques that alter the configuration of BPDs must be predictable, understandable, and pre-approved by administrators, and (2) Software can be installed on BPDs only after completing a stringent accreditation process. These challenges often lead to manual configuration management practices, which are inefficient or ineffective in many cases. The Hammerhead prototype, developed as part of the SHARC project, addresses these challenges using knowledge representation, a rule-oriented adaptation bundle format, and an extensible, open-source constraint solver.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1153},
numpages = {6},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.5555/3540261.3541537,
author = {Peng, Jizong and Wang, Ping and Desrosiers, Christian and Pedersoli, Marco},
title = {Self-paced contrastive learning for semi-supervised medical image segmentation with meta-labels},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The contrastive pre-training of a recognition model on a large dataset of unlabeled data often boosts the model's performance on downstream tasks like image classification. However, in domains such as medical imaging, collecting unlabeled data can be challenging and expensive. In this work, we consider the task of medical image segmentation and adapt contrastive learning with meta-label annotations to scenarios where no additional unlabeled data is available. Meta-labels, such as the location of a 2D slice in a 3D MRI scan, often come for free during the acquisition process. We use these meta-labels to pre-train the image encoder, as well as in a semi-supervised learning step that leverages a reduced set of annotated data. A self-paced learning strategy exploiting the weak annotations is proposed to further help the learning process and discriminate useful labels from noise. Results on five medical image segmentation datasets show that our approach: i) highly boosts the performance of a model trained on a few scans, ii) outperforms previous contrastive and semi-supervised approaches, and iii) reaches close to the performance of a model trained on the full data.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1276},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-65310-1_20,
author = {Metzger, Andreas and Quinton, Cl\'{e}ment and Mann, Zolt\'{a}n \'{A}d\'{a}m and Baresi, Luciano and Pohl, Klaus},
title = {Feature Model-Guided Online Reinforcement Learning for Self-Adaptive Services},
year = {2020},
isbn = {978-3-030-65309-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65310-1_20},
doi = {10.1007/978-3-030-65310-1_20},
abstract = {A self-adaptive service can maintain its QoS requirements in the presence of dynamic environment changes. To develop a self-adaptive service, service engineers have to create self-adaptation logic encoding when the service should execute which adaptation actions. However, developing self-adaptation logic may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. Online reinforcement learning addresses design time uncertainty by learning suitable adaptation actions through interactions with the environment at runtime. To learn more about its environment, reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens has an impact on the performance of the learning process. We focus on two problems related to how a service’s adaptation actions are explored: (1) Existing solutions randomly explore adaptation actions and thus may exhibit slow learning if there are many possible adaptation actions to choose from. (2) Existing solutions are unaware of service evolution, and thus may explore new adaptation actions introduced during such evolution rather late. We propose novel exploration strategies that use feature models (from software product line engineering) to guide exploration in the presence of many adaptation actions and in the presence of service evolution. Experimental results for a self-adaptive cloud management service indicate an average speed-up of the learning process of 58.8% in the presence of many adaptation actions, and of 61.3% in the presence of service evolution. The improved learning performance in turn led to an average QoS improvement of 7.8% and 23.7% respectively
.},
booktitle = {Service-Oriented Computing: 18th International Conference, ICSOC 2020, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {269–286},
numpages = {18},
keywords = {Cloud service, Feature model, Reinforcement learning, Adaptation},
location = {Dubai, United Arab Emirates}
}

@article{10.1016/j.knosys.2016.05.048,
author = {Zhang, Zhongliang and Krawczyk, Bartosz and Garc\`{\i}a, Salvador and Rosales-P\'{e}rez, Alejandro and Herrera, Francisco},
title = {Empowering one-vs-one decomposition with ensemble learning for multi-class imbalanced data},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.05.048},
doi = {10.1016/j.knosys.2016.05.048},
abstract = {Extending binary ensemble techniques to multi-class imbalanced data.OVO scheme enhancement for multi-class imbalanced data by ensemble learning.A complete experimental study of comparison of the ensemble learning techniques with OVO.Study of the impact of base classifiers used in the proposed scenario. Multi-class imbalance classification problems occur in many real-world applications, which suffer from the quite different distribution of classes. Decomposition strategies are well-known techniques to address the classification problems involving multiple classes. Among them binary approaches using one-vs-one and one-vs-all has gained a significant attention from the research community. They allow to divide multi-class problems into several easier-to-solve two-class sub-problems. In this study we develop an exhaustive empirical analysis to explore the possibility of empowering the one-vs-one scheme for multi-class imbalance classification problems with applying binary ensemble learning approaches. We examine several state-of-the-art ensemble learning methods proposed for addressing the imbalance problems to solve the pairwise tasks derived from the multi-class data set. Then the aggregation strategy is employed to combine the binary ensemble outputs to reconstruct the original multi-class task. We present a detailed experimental study of the proposed approach, supported by the statistical analysis. The results indicate the high effectiveness of ensemble learning with one-vs-one scheme in dealing with the multi-class imbalance classification problems.},
journal = {Know.-Based Syst.},
month = aug,
pages = {251–263},
numpages = {13},
keywords = {Multi-class classification, Imbalanced data, Ensemble learning, Classifier combination, Binary decomposition}
}

@article{10.1145/3322122,
author = {Gong, Chen and Yang, Jian and Tao, Dacheng},
title = {Multi-Modal Curriculum Learning over Graphs},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3322122},
doi = {10.1145/3322122},
abstract = {Curriculum Learning (CL) is a recently proposed learning paradigm that aims to achieve satisfactory performance by properly organizing the learning sequence from simple curriculum examples to more difficult ones. Up to now, few works have been done to explore CL for the data with graph structure. Therefore, this article proposes a novel CL algorithm that can be utilized to guide the Label Propagation (LP) over graphs, of which the target is to “learn” the labels of unlabeled examples on the graphs. Specifically, we assume that different unlabeled examples have different levels of difficulty for propagation, and their label learning should follow a simple-to-difficult sequence with the updated curricula. Furthermore, considering that the practical data are often characterized by multiple modalities, every modality in our method is associated with a “teacher” that not only evaluates the difficulties of examples from its own viewpoint, but also cooperates with other teachers to generate the overall simplest curriculum examples for propagation. By taking the curriculums suggested by the teachers as a whole, the common preference (i.e., commonality) of teachers on selecting the simplest examples can be discovered by a row-sparse matrix, and their distinct opinions (i.e., individuality) are captured by a sparse noise matrix. As a result, an accurate curriculum sequence can be established and the propagation quality can thus be improved. Theoretically, we prove that the propagation risk bound is closely related to the examples’ difficulty information, and empirically, we show that our method can generate higher accuracy than the state-of-the-art CL approach and LP algorithms on various multi-modal tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {35},
numpages = {25},
keywords = {semi-supervised learning, multi-modal learning, label propagation, Curriculum learning}
}

@inproceedings{10.1145/3421766.3421874,
author = {Mingwei, Zhou},
title = {Discussion on the relationship between clean room and traditional software engineering methods and practices},
year = {2020},
isbn = {9781450375535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421766.3421874},
doi = {10.1145/3421766.3421874},
abstract = {Clean room software engineering is a formal software development method that can strictly engineer software development and eliminate defects before they can cause serious harm. The clean room software engineering model has its own advantages and disadvantages in use. To use it in traditional software engineering, it is necessary to clarify the relationship between it and traditional software engineering methods and practices. Based on the technology and principles of the clean room software process, this paper gets the advantages and disadvantages of the clean room. Corresponding the traditional method-based software process to the key technology of the clean room, making the traditional management-based software engineering method and the tailored clean room compatible, and the software engineering practice of the clean room software process is compared with the traditional software engineering practice. To discuss the clean room and traditional software engineering, in order to expand the use of clean room methods, improve the clean room software process, and improve the quality of software.},
booktitle = {Proceedings of the 2nd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {18–22},
numpages = {5},
keywords = {Cleanroom traditional software engineering box specifications advantages and disadvantages software process improvement},
location = {Manchester, United Kingdom},
series = {AIAM2020}
}

@article{10.1016/j.eswa.2021.115218,
author = {Serrano-P\'{e}rez, Jonathan and Sucar, L. Enrique},
title = {Artificial datasets for hierarchical classification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115218},
doi = {10.1016/j.eswa.2021.115218},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {13},
keywords = {Evaluation, Hierarchical classification, Artificial datasets}
}

@article{10.1016/j.jisa.2021.102880,
author = {Araujo, Frederico and Ayoade, Gbadebo and Al-Naami, Khaled and Gao, Yang and Hamlen, Kevin W. and Khan, Latifur},
title = {Crook-sourced intrusion detection as a service},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {61},
number = {C},
issn = {2214-2126},
url = {https://doi.org/10.1016/j.jisa.2021.102880},
doi = {10.1016/j.jisa.2021.102880},
journal = {J. Inf. Secur. Appl.},
month = sep,
numpages = {17},
keywords = {Software-as-a-service, Cloud computing, Cyberdeception, Honeypots, Neural networks, Datasets, Intrusion detection}
}

@article{10.1155/2021/4513610,
author = {Chen, Ling-qing and Wu, Mei-ting and Pan, Li-fang and Zheng, Ru-bin and Liu, KunHong},
title = {Grade Prediction in Blended Learning Using Multisource Data},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4513610},
doi = {10.1155/2021/4513610},
abstract = {Today, blended learning is widely carried out in many colleges. Different online learning platforms have accumulated a large number of fine granularity records of students’ learning behavior, which provides us with an excellent opportunity to analyze students’ learning behavior. In this paper, based on the behavior log data in four consecutive years of blended learning in a college’s programming course, we propose a novel multiclassification frame to predict students’ learning outcomes. First, the data obtained from diverse platforms, i.e., MOOC, Cnblogs, Programming Teaching Assistant (PTA) system, and Rain Classroom, are integrated and preprocessed. Second, a novel error-correcting output codes (ECOC) multiclassification framework, based on genetic algorithm (GA) and ternary bitwise calculator, is designed to effectively predict the grade levels of students by optimizing the code-matrix, feature subset, and binary classifiers of ECOC. Experimental results show that the proposed algorithm in this paper significantly outperforms other alternatives in predicting students’ grades. In addition, the performance of the algorithm can be further improved by adding the grades of prerequisite courses.},
journal = {Sci. Program.},
month = jan,
numpages = {15}
}

@inproceedings{10.5555/2820656.2820667,
author = {Buchmann, Thomas and Baumgartl, Johannes and Henrich, Dominik and Westfechtel, Bernhard},
title = {Robots and their variability: a societal challenge and a potential solution},
year = {2015},
publisher = {IEEE Press},
abstract = {A robot is essentially a real-time, distributed embedded system operating in a physical environment. Often, control and communication paths within the system are tightly coupled to the actual hardware configuration of the robot. Furthermore, the domain contains a high amount of variability on different levels, ranging from hardware, over software to the environment in which the robot is operated. Today, special robots are used in households to perform monotonous and recurring tasks like vacuuming or mowing the lawn. In the future there may be robots that can be configured and programmed for more complicated tasks, like washing dishes or cleaning up or to assist elderly people. Nowadays, programming a robot is a highly complex and challenging task, which can be carried out only by programmers with dedicated background in robotics. Societal acceptance of robots can only be achieved, if they are easy to program. In this paper we present our approach to provide customized programming environments enabling programmers without background knowledge in robotics to specify robot programs. Our solution was realized using product line techniques.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {27–30},
numpages = {4},
keywords = {software product line, robot, model-driven development, code generation, DSL},
location = {Florence, Italy},
series = {PLEASE '15}
}

@article{10.1016/j.neucom.2019.03.062,
author = {Ren, Yazhou and Que, Xiaofan and Yao, Dezhong and Xu, Zenglin},
title = {Self-paced multi-task clustering},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {350},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.03.062},
doi = {10.1016/j.neucom.2019.03.062},
journal = {Neurocomput.},
month = jul,
pages = {212–220},
numpages = {9},
keywords = {Soft weighting, Non-convexity, Self-paced learning, Multi-task clustering}
}

@article{10.1109/TSMCC.2004.841919,
author = {Chiu, Chaochang and Hsu, P. -L.},
title = {A constraint-based genetic algorithm approach for mining classification rules},
year = {2005},
issue_date = {May 2005},
publisher = {IEEE Press},
volume = {35},
number = {2},
issn = {1094-6977},
url = {https://doi.org/10.1109/TSMCC.2004.841919},
doi = {10.1109/TSMCC.2004.841919},
abstract = {Data mining is an information extraction process that aims to discover valuable knowledge in databases. Existing genetic algorithms (GAs) designed for rule induction evaluates the rules as a whole via a fitness function. Major drawbacks of GAs for rule induction include computation inefficiency, accuracy and rule expressiveness. In this paper, we propose a constraint-based genetic algorithm (CBGA) approach to reveal more accurate and significant classification rules. This approach allows constraints to be specified as relationships among attributes according to predefined requirements, user's preferences, or partial knowledge in the form of a constraint network. The constraint-based reasoning is employed to produce valid chromosomes using constraint propagation to ensure the genes to comply with the predefined constraint network. The proposed approach is compared with a regular GA and C4.5 using two UCI repository data sets. Better classification accurate rates from CBGA are demonstrated.},
journal = {Trans. Sys. Man Cyber Part C},
month = may,
pages = {205–220},
numpages = {16},
keywords = {rules induction, genetic algorithms, data mining, constraint satisfaction problems, Constraint-based reasoning}
}

@article{10.1016/j.knosys.2013.01.018,
author = {Fern\'{a}Ndez, Alberto and L\'{o}Pez, Victoria and Galar, Mikel and Del Jesus, Mar\'{\i}A Jos\'{e} and Herrera, Francisco},
title = {Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {42},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2013.01.018},
doi = {10.1016/j.knosys.2013.01.018},
abstract = {The imbalanced class problem is related to the real-world application of classification in engineering. It is characterised by a very different distribution of examples among the classes. The condition of multiple imbalanced classes is more restrictive when the aim of the final system is to obtain the most accurate precision for each of the concepts of the problem. The goal of this work is to provide a thorough experimental analysis that will allow us to determine the behaviour of the different approaches proposed in the specialised literature. First, we will make use of binarization schemes, i.e., one versus one and one versus all, in order to apply the standard approaches to solving binary class imbalanced problems. Second, we will apply several ad hoc procedures which have been designed for the scenario of imbalanced data-sets with multiple classes. This experimental study will include several well-known algorithms from the literature such as decision trees, support vector machines and instance-based learning, with the intention of obtaining global conclusions from different classification paradigms. The extracted findings will be supported by a statistical comparative analysis using more than 20 data-sets from the KEEL repository.},
journal = {Know.-Based Syst.},
month = apr,
pages = {97–110},
numpages = {14},
keywords = {Preprocessing, Pairwise learning, Multi-classification, Imbalanced data-sets, Cost-sensitive learning}
}

@article{10.1016/j.ins.2018.06.014,
author = {Ma, Zilu and Liu, Shiqi and Meng, Deyu and Zhang, Yong and Lo, SioLong and Han, Zhi},
title = {On Convergence Properties of Implicit Self-paced Objective},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {462},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.06.014},
doi = {10.1016/j.ins.2018.06.014},
journal = {Inf. Sci.},
month = sep,
pages = {132–140},
numpages = {9},
keywords = {99-00, 00-01, Convergence, Non-convex optimization, Machine learning, Self-paced learning}
}

@article{10.1016/j.micpro.2021.103964,
author = {Gokilavani, N. and Bharathi, B.},
title = {Multi-Objective based test case selection and prioritization for distributed cloud environment},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {82},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2021.103964},
doi = {10.1016/j.micpro.2021.103964},
journal = {Microprocess. Microsyst.},
month = apr,
numpages = {6},
keywords = {Cloud environment, Software testing, Similarity-based clustering, Test case prioritization, Test case selection, Particle swarm optimization, Software product line}
}

@article{10.1016/j.patcog.2009.12.012,
author = {Derrac, Joaqu\'{\i}n and Garc\'{\i}a, Salvador and Herrera, Francisco},
title = {IFS-CoCo: Instance and feature selection based on cooperative coevolution with nearest neighbor rule},
year = {2010},
issue_date = {June, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {43},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2009.12.012},
doi = {10.1016/j.patcog.2009.12.012},
abstract = {Feature and instance selection are two effective data reduction processes which can be applied to classification tasks obtaining promising results. Although both processes are defined separately, it is possible to apply them simultaneously. This paper proposes an evolutionary model to perform feature and instance selection in nearest neighbor classification. It is based on cooperative coevolution, which has been applied to many computational problems with great success. The proposed approach is compared with a wide range of evolutionary feature and instance selection methods for classification. The results contrasted through non-parametric statistical tests show that our model outperforms previously proposed evolutionary approaches for performing data reduction processes in combination with the nearest neighbor rule.},
journal = {Pattern Recogn.},
month = jun,
pages = {2082–2105},
numpages = {24},
keywords = {Nearest neighbor, Instance selection, Feature selection, Evolutionary algorithms, Cooperative coevolution}
}

@inproceedings{10.1007/978-3-030-77385-4_42,
author = {Halilaj, Lavdim and Dindorkar, Ishan and L\"{u}ttin, J\"{u}rgen and Rothermel, Susanne},
title = {A Knowledge Graph-Based Approach for Situation Comprehension in Driving Scenarios},
year = {2021},
isbn = {978-3-030-77384-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77385-4_42},
doi = {10.1007/978-3-030-77385-4_42},
abstract = {Making an informed and right decision poses huge challenges for drivers in day-to-day traffic situations. This task vastly depends on many subjective and objective factors, including the current driver state, her destination, personal preferences and abilities as well as surrounding environment. In this paper, we present CoSI (Context and Situation Intelligence), a Knowledge Graph (KG)-based approach for fusing and organizing heterogeneous types and sources of information. The KG serves as a coherence layer representing information in the form of entities and their inter-relationships augmented with additional semantic axioms. Harnessing the power of axiomatic rules and reasoning capabilities enables inferring additional knowledge from what is already encoded. Thus, dedicated components exploit and consume the semantically enriched information to perform tasks such as situation classification, difficulty assessment, and trajectory prediction. Further, we generated a synthetic dataset to simulate real driving scenarios with a large range of driving styles and vehicle configurations. We use KG embedding techniques based on a Graph Neural Network (GNN) architecture for a classification task of driving situations and achieve over 95% accuracy whereas vector-based approaches achieve only 75% accuracy for the same task. The results suggest that the KG-based information representation combined with GNN are well suited for situation understanding tasks as required in driver assistance and automated driving systems.},
booktitle = {The Semantic Web: 18th International Conference, ESWC 2021, Virtual Event, June 6–10, 2021, Proceedings},
pages = {699–716},
numpages = {18},
keywords = {Graph neural network, Knowledge graph embedding, Knowledge graph, Situation comprehension}
}

@inproceedings{10.1145/3319535.3339815,
author = {Cao, Yulong and Xiao, Chaowei and Cyr, Benjamin and Zhou, Yimeng and Park, Won and Rampazzi, Sara and Chen, Qi Alfred and Fu, Kevin and Mao, Z. Morley},
title = {Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3339815},
doi = {10.1145/3319535.3339815},
abstract = {In Autonomous Vehicles (AVs), one fundamental pillar is perception,which leverages sensors like cameras and LiDARs (Light Detection and Ranging) to understand the driving environment. Due to its direct impact on road safety, multiple prior efforts have been made to study its the security of perception systems. In contrast to prior work that concentrates on camera-based perception, in this work we perform the first security study of LiDAR-based perception in AV settings, which is highly important but unexplored. We consider LiDAR spoofing attacks as the threat model and set the attack goal as spoofing obstacles close to the front of a victim AV. We find that blindly applying LiDAR spoofing is insufficient to achieve this goal due to the machine learning-based object detection process.Thus, we then explore the possibility of strategically controlling the spoofed attack to fool the machine learning model. We formulate this task as an optimization problem and design modeling methods for the input perturbation function and the objective function.We also identify the inherent limitations of directly solving the problem using optimization and design an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75%. As a case study to understand the attack impact at the AV driving decision level, we construct and evaluate two attack scenarios that may damage road safety and mobility.We also discuss defense directions at the AV system, sensor, and machine learning model levels.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2267–2281},
numpages = {15},
keywords = {sensor attack, autonomous driving, adversarial machine learning},
location = {London, United Kingdom},
series = {CCS '19}
}

@inproceedings{10.5555/1885639.1885667,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Gasevic, Dragan and Soltani, Samaneh},
title = {Stratified analytic hierarchy process: prioritization and selection of software features},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product line engineering allows for the rapid development of variants of a domain specific application by using a common set of reusable assets often known as core assets. Variability modeling is a critical issue in product line engineering, where the use of feature modeling is one of most commonly used formalisms. To support an effective and automated derivation of concrete products for a product family, staged configuration has been proposed in the research literature. In this paper, we propose the integration of well-known requirements engineering principles into stage configuration. Being inspired by the well-established Preview requirements engineering framework, we initially propose an extension of feature models with capabilities for capturing business oriented requirements. This representation enables a more effective capturing of stakeholders' preferences over the business requirements and objectives (e.g.,. implementation costs or security) in the form of fuzzy linguistic variables (e.g., high, medium, and low). On top of this extension, we propose a novel method, the Stratified Analytic Hierarchy process, which first helps to rank and select the most relevant high level business objectives for the target stakeholders (e.g., security over implementation costs), and then helps to rank and select the most relevant features from the feature model to be used as the starting point in the staged configuration process. Besides a complete formalization of the process, we define the place of our proposal in existing software product line lifecycles as well as demonstrate the use of our proposal on the widely-used e-Shop case study. Finally, we report on the results of our user study, which indicates a high appreciation of the proposed method by the participating industrial software developers. The tool support for S-AHP is also introduced.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {300–315},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.procs.2017.08.206,
author = {Mani, Neel and Helfert, Markus and Pahl, Claus},
title = {A Domain-specific Rule Generation Using Model-Driven Architecture in Controlled Variability Model},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.08.206},
doi = {10.1016/j.procs.2017.08.206},
abstract = {The business environment changes rapidly and needs to adapt to the enterprise business systems must be considered for new types of requirements to accept changes in the business strategies and processes. This raises new challenges that the traditional development approaches cannot always provide a complete solution in an efficient way. However, most of the current proposals for automatic generation are not devised to cope with rapid integration of the changes in the business requirement of end user (stakeholders and customers) resource. Domain-specific Rules constitute a key element for domain specific enterprise application, allowing configuration of changes, and management of the domain constraint within a domain. In this paper, we propose an approach to the development of an automatic generation of the domain-specific rules by using variability feature model and ontology definition of domain model concepts coming from Software product line engineering and Model Driven Architecture. We provide a process approach to generate a domain-specific rule based on the end user requirement.},
journal = {Procedia Comput. Sci.},
month = sep,
pages = {2354–2362},
numpages = {9},
keywords = {Variability Model, Rule Generation, Model Driven Architecture, Domain-specific rules, Business Process Model}
}

@article{10.1007/s13748-020-00205-3,
author = {Ram\'{\i}rez, Aurora and Delgado-P\'{e}rez, Pedro and Ferrer, Javier and Romero, Jos\'{e} Ra\'{u}l and Medina-Bulo, Inmaculada and Chicano, Francisco},
title = {A systematic literature review of the SBSE research community in Spain},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {2},
url = {https://doi.org/10.1007/s13748-020-00205-3},
doi = {10.1007/s13748-020-00205-3},
abstract = {Since its appearance in 2001, search-based software engineering has allowed software engineers to use optimisation techniques to automate distinctive human problems related to software management and development. The scientific community in Spain has not been alien to these advances. Their contributions cover both the optimisation of software engineering tasks and the proposal of new search algorithms. This review compiles the research efforts of this community in the area. With this aim, we propose a protocol to describe the review process, including the search sources, inclusion and exclusion criteria of candidate papers, the data extraction procedure and the categorisation of primary studies. After retrieving more than 3700 papers, 232 primary studies have been selected, whose analysis gives a precise picture of the current research state of the community, trends and future challenges. With 145 authors from 19 distinct institutions, results show that a diversity of tasks, including software planning, requirements, design and testing, and a large variety of techniques has been used, from exact search to evolutionary computation and swarm intelligence. Further, since 2015, specific scientific events have helped to bring together the community, improving collaborations, financial funding and internationalisation.},
journal = {Prog. in Artif. Intell.},
month = jun,
pages = {113–128},
numpages = {16},
keywords = {Spanish community, Research trends, Systematic review, Search-based software engineering}
}

@inproceedings{10.1109/ASE.2011.6100118,
author = {Soltani, Samaneh and Asadi, Mohsen and Hatala, Marek and Gasevic, Dragan and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on stakeholders' business concerns},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100118},
doi = {10.1109/ASE.2011.6100118},
abstract = {In Software Product Line Engineering, concrete products of a family can be generated through a configuration process over a feature model. The configuration process selects features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from all the available features in the feature model is a cumbersome task because 1) the stakeholders may have diverse business concerns and limited resources that they can spend on a product and 2) features may have negative and positive contributions on different business concern. Many configurations techniques have been proposed to facilitate software developers' tasks through automated product derivation. However, most of the current proposals for automatic configuration are not devised to cope with business oriented requirements and stakeholders' resource limitations. We propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy the stakeholders' business concerns and resource limitations. We also provide tooling support to facilitate the use of our framework.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {536–539},
numpages = {4},
series = {ASE '11}
}

@article{10.1016/j.future.2019.12.027,
author = {Al-Sayed, Mustafa M. and Hassan, Hesham A. and Omara, Fatma A.},
title = {An intelligent cloud service discovery framework},
year = {2020},
issue_date = {May 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.12.027},
doi = {10.1016/j.future.2019.12.027},
journal = {Future Gener. Comput. Syst.},
month = may,
pages = {438–466},
numpages = {29},
keywords = {Non-functional features, Functional features, Cloud service discovery, Information retrieval, OBDA, Cloud ontology, Cloud computing}
}

@article{10.1002/smr.2252,
author = {Akbar, Muhammad Azeem and Mahmood, Sajjad and Alsanad, Ahmed and Alsanad, Abeer Abdul‐Aziz and Gumaei, Abdu and Qadri, Syed Furqan},
title = {A multivocal study to improve the implementation of global requirements change management process: A client‐vendor prospective},
year = {2020},
issue_date = {August 2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {32},
number = {8},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2252},
doi = {10.1002/smr.2252},
abstract = {Software development is a complex task, and the introduction of multi‐site development teams spread across the globe makes it even harder. Requirement changes during the software development process are inevitable, and failing to manage evolving requirements is one of the contributors to project failures. Requirements change management (RCM) becomes difficult in global software development (GSD) projects due to the need to communicate and coordinate between stakeholders in a distributed environment. This research work aims to identify the factors that have positive impacts on RCM activities in GSD. We conducted a multi‐vocal literature review (MLR) and a questionnaire survey study to identify the RCM success factors. The results of the t‐test (ie, t = 0.0347 and P = .700 &gt; .05) and correlation (rs (25) = 0.573, P = .003) indicates that there is no significant difference between the findings of MLR and the questionnaire survey. The identified factors were further analyzed in the context of client and vendor organizations to better understand the RCM success factors in both types of GSD firms. The findings of this study can provide a useful framework for tackling problems associated with RCM in a GSD environment that is significant to the progression of GSD firms.},
journal = {J. Softw. Evol. Process},
month = aug,
numpages = {28},
keywords = {vendor, requirements change management (RCM), multivocal literature review (MLR), global software development (GSD), empirical investigation, client}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Systematic mapping study, Software product line, Search based software engineering, Metaheuristics, Evolutionary algorithm}
}

@inproceedings{10.5555/3367032.3367200,
author = {Terra-Neves, Miguel and Lynce, In\^{e}s and Manquinho, Vasco},
title = {Integrating Pseudo-Boolean constraint reasoning in multi-objective evolutionary algorithms},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Constraint-based reasoning methods thrive in solving problem instances with a tight solution space. On the other hand, evolutionary algorithms are usually effective when it is not hard to satisfy the problem constraints. This dichotomy has been observed in many optimization problems. In the particular case of Multi-Objective Combinatorial Optimization (MOCO), new recently proposed constraint-based algorithms have been shown to outperform more established evolutionary approaches when a given problem instance is hard to satisfy. In this paper, we propose the integration of constraint-based procedures in evolutionary algorithms for solving MOCO. First, a new core-based smart mutation operator is applied to individuals that do not satisfy all problem constraints. Additionally, a new smart improvement operator based on Minimal Correction Subsets is used to improve the quality of the population. Experimental results clearly show that the integration of these operators greatly improves multi-objective evolutionary algorithms MOEA/D and NSGAII. Moreover, even on problem instances with a tight solution space, the newly proposed algorithms outperform the state-of-the-art constraint-based approaches for MOCO.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {1184–1190},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

@article{10.1007/s00500-021-05766-6,
author = {Agudelo, Oscar Esneider Acosta and Mar\'{\i}n, Carlos Enrique Montenegro and Crespo, Rub\'{e}n Gonz\'{a}lez},
title = {Sound measurement and automatic vehicle classification and counting applied to road traffic noise characterization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {18},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05766-6},
doi = {10.1007/s00500-021-05766-6},
abstract = {Increase in population density in large cities has increased the environmental noise present in these environments, causing negative effects on human health. There are different sources of environmental noise; however, noise from road traffic is the most prevalent in cities. Therefore, it is necessary to have tools that allow noise characterization to establish strategies that permit obtaining levels that do not affect the quality of life of people. This research discusses the implementation of a system that allows the acquisition of data to characterize the noise generated by road traffic. First, the methodology for obtaining acoustic indicators with an electret measurement microphone is described, so that it adjusts to the data collection needs for road traffic noise analyses. Then, an approach for the classification and counting of automatic vehicular traffic through deep learning is presented. Results showed that there were differences of 0.2 dBA in terms of RMSE between a type 1 sound level meter and the measurement microphone used. With reference to vehicle classification and counting for four categories, the approximate error is between 3.3% and -15.5%.},
journal = {Soft Comput.},
month = sep,
pages = {12075–12087},
numpages = {13},
keywords = {Deep learning, Classification, Vehicle, Road traffic, Environmental noise}
}

@article{10.1016/j.comcom.2021.07.002,
author = {Ben Slimen, Yosra and Balcerzak, Joanna and Pag\`{e}s, Albert and Agraz, Fernando and Spadaro, Salvatore and Koutsopoulos, Konstantinos and Al-Bado, Mustafa and Truong, Thuy and Giardina, Pietro G. and Bernini, Giacomo},
title = {Quality of perception prediction in 5G slices for e-Health services using user-perceived QoS},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {178},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2021.07.002},
doi = {10.1016/j.comcom.2021.07.002},
journal = {Comput. Commun.},
month = oct,
pages = {1–13},
numpages = {13},
keywords = {Network cognitive management, Machine learning, Quality of experience, 5G network slicing}
}

@article{10.1016/j.patcog.2018.11.030,
author = {Wang, Xiaohong and Jiang, Xudong and Ren, Jianfeng},
title = {Blood vessel segmentation from fundus image by a cascade classification framework},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.11.030},
doi = {10.1016/j.patcog.2018.11.030},
journal = {Pattern Recogn.},
month = apr,
pages = {331–341},
numpages = {11},
keywords = {Dimensionality reduction, Cascade classification, Retinal vessel segmentation, Fundus image}
}

@inproceedings{10.1109/ICTAI.2014.144,
author = {Oliveira, Pedro and Souza, Matheus and Braga, Ronyerison and Brito, Ricardo and Rab\^{e}lo, Ricardo Lira and Neto, Pedro Santos},
title = {Athena: A Visual Tool to Support the Development of Computational Intelligence Systems},
year = {2014},
isbn = {9781479965724},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2014.144},
doi = {10.1109/ICTAI.2014.144},
abstract = {Computational Intelligence (CI) embraces techniques designed to address complex real-world problems in which traditional approaches are ineffective or infeasible. Some of these techniques are being used to solve several complex problems, such as the team allocation, building products portfolios in a software product line and test case selection/prioritization. However, despite the usefulness of these applications, the development of solutions based in CI techniques is not a trivial activity, since it involves the implementation/adaptation of algorithms to specific context and problems. This work presents Athena, a visual tool developed aiming at offering a simple approach to develop CI-based software systems. In order to do this, we proposed a drag-and-drop approach, which we called CI as a Service (CIaaS). Based on a preliminary study, we can state that Athena can help researchers to save time during the development of computational intelligence approaches.},
booktitle = {Proceedings of the 2014 IEEE 26th International Conference on Tools with Artificial Intelligence},
pages = {950–959},
numpages = {10},
keywords = {Visual Programming, Tool, Service, Computational Intelligence, Artificial Intelligence},
series = {ICTAI '14}
}

@article{10.1007/s00500-019-04503-4,
author = {Abboud, Ralph and Tekli, Joe},
title = {Integration of nonparametric fuzzy classification with an evolutionary-developmental framework to perform music sentiment-based analysis and composition},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {13},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-04503-4},
doi = {10.1007/s00500-019-04503-4},
abstract = {Over the past years, several approaches have been developed to create algorithmic music composers. Most existing solutions focus on composing music that appears theoretically correct or interesting to the listener. However, few methods have targeted sentiment-based music composition: generating music that expresses human emotions. The few existing methods are restricted in the spectrum of emotions they can express (usually to two dimensions: valence and arousal) as well as the level of sophistication of the music they compose (usually monophonic, following translation-based, predefined templates or heuristic textures). In this paper, we introduce a new algorithmic framework for autonomous music sentiment-based expression and composition, titled MUSEC, that perceives an extensible set of six primary human emotions (e.g., anger, fear, joy, love, sadness, and surprise) expressed by a MIDI musical file and then composes (creates) new polyphonic (pseudo) thematic, and diversified musical pieces that express these emotions. Unlike existing solutions, MUSEC is: (i) a hybrid crossover between supervised learning (SL, to learn sentiments from music) and evolutionary computation (for music composition, MC), where SL serves at the fitness function of MC to compose music that expresses target sentiments, (ii) extensible in the panel of emotions it can convey, producing pieces that reflect a target crisp sentiment (e.g., love) or a collection of fuzzy sentiments (e.g., 65% happy, 20% sad, and 15% angry), compared with crisp-only or two-dimensional (valence/arousal) sentiment models used in existing solutions, (iii) adopts the evolutionary-developmental model, using an extensive set of specially designed music-theoretic mutation operators (trille, staccato, repeat, compress, etc.), stochastically orchestrated to add atomic (individual chord-level) and thematic (chord pattern-level) variability to the composed polyphonic pieces, compared with traditional evolutionary solutions producing monophonic and non-thematic music. We conducted a large battery of tests to evaluate MUSEC’s effectiveness and efficiency in both sentiment analysis and composition. It was trained on a specially constructed set of 120 MIDI pieces, including 70 sentiment-annotated pieces: the first significant dataset of sentiment-labeled MIDI music made available online as a benchmark for future research in this area. Results are encouraging and highlight the potential of our approach in different application domains, ranging over music information retrieval, music composition, assistive music therapy, and emotional intelligence.},
journal = {Soft Comput.},
month = jul,
pages = {9875–9925},
numpages = {51},
keywords = {Fuzzy classification, Supervised learning, Algorithmic composition, Evolutionary algorithms, MIDI, Music sentiment analysis}
}

@inproceedings{10.1145/3395260.3395275,
author = {Liu, Bingran},
title = {Neural Question Generation based on Seq2Seq},
year = {2020},
isbn = {9781450377072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395260.3395275},
doi = {10.1145/3395260.3395275},
abstract = {Neural Question Generation is the use of deep neural networks to extract target answers from a given article or paragraph and generate questions based on the target answers. There is a problem in the previous NQG(Neural Question Generation) model, and the generated question does not explicitly connect with the context in the target answer, resulting in a large part of the generated question containing the target answer and the accuracy is not high. In this paper, a QG model based on seq2seq is used, which consists of encode and decoder, and adds the attention mechanism and copy mechanism. We use special tags to replace the target answer of the original paragraph, and use the paragraph and target answer as input to reduce the number of incorrect questions, including the correct answer. Through the partial copy mechanism based on character overlap, we can make the generation problem have higher overlap and relevance at the word level and the input document. Experiments show that our proposed model performs better than before.},
booktitle = {Proceedings of the 2020 5th International Conference on Mathematics and Artificial Intelligence},
pages = {119–123},
numpages = {5},
keywords = {Seq2seq model, Question generation, Deep neural network},
location = {Chengdu, China},
series = {ICMAI '20}
}

@article{10.3233/THC-218026,
author = {Zhou, Zhiming and Huang, Haihui and Liang, Yong},
title = {Cancer classification and biomarker selection via a penalized logsum network-based logistic regression model},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {29},
number = {S1},
issn = {0928-7329},
url = {https://doi.org/10.3233/THC-218026},
doi = {10.3233/THC-218026},
journal = {Technol. Health Care},
month = jan,
pages = {287–295},
numpages = {9},
keywords = {network-based knowledge, log-sum penalty, gene selection, Regularization}
}

@article{10.1016/j.knosys.2019.105424,
author = {Liu, Xiaoshuang and Luo, Senlin and Pan, Limin},
title = {Robust boosting via self-sampling},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {193},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105424},
doi = {10.1016/j.knosys.2019.105424},
journal = {Know.-Based Syst.},
month = apr,
numpages = {10},
keywords = {Self-sampling, Robustness, Loss function, Boosting}
}

@article{10.1016/j.procs.2019.12.024,
author = {Ivan, Ion and Budacu, Eduard and Despa, Mihai Liviu},
title = {Using profiling to assemble an agile collaborative software development team made up of freelancers},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {162},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.024},
doi = {10.1016/j.procs.2019.12.024},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {562–570},
numpages = {9},
keywords = {machine learning, agile, collaborative teams, profiling, data analysis}
}

@article{10.1007/s10489-020-01730-3,
author = {Zhu, Wenjie and Peng, Bo and Wu, Han and Wang, Binhao},
title = {Query set centered sparse projection learning for set based image classification},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01730-3},
doi = {10.1007/s10489-020-01730-3},
abstract = {Set based image classification technology has been developed successfully in recent decades. Previous approaches dispose set based image classification by employing all the gallery sets to learn metrics or construct the model using a typical number of parameters. However, they are based on the assumption that the global structure is consistent with the local structure, which is rigid in real applications. Additionally, the participation of all gallery sets increases the influence of outliers. This paper conducts this task via sparse projection learning by employing ℓ2,1 norm from the perspective of the query set. Instead of involving all the image sets, this work devotes to searching for a local region, which is centered with a query set and constructed by the candidates selected from different classes in the gallery sets. By maximizing the inter-class while minimizing the intra-class of the candidates from the gallery sets from the query set, this work can learn a discriminate and sparse projection for image set feature extraction. In order to learn the projection, an alternative updating algorithm to solve the optimization problem is proposed and the convergence and complexity are analyzed. Finally, the distance is measured in the discriminate low-dimensional space using Euclidean distance between the central data point of the query set and the central one of images from the same class. The proposed approach learns the projection in the local set centered with the query set with ℓ2,1 norm, which contributes to more discriminative feature. Compared with the existing algorithms, the experiments on the challenging databases demonstrate that the proposed simple yet effective approach obtains the best classification accuracy with comparable time cost.},
journal = {Applied Intelligence},
month = oct,
pages = {3400–3411},
numpages = {12},
keywords = {Discriminate subspace learning, Set based image classification, Sparse projection learning, Query set}
}

@article{10.1007/s10664-021-10026-0,
author = {Silva, Camila Costa and Galster, Matthias and Gilson, Fabian},
title = {Topic modeling in software engineering research},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10026-0},
doi = {10.1007/s10664-021-10026-0},
abstract = {Topic modeling using models such as Latent Dirichlet Allocation (LDA) is a text mining technique to extract human-readable semantic “topics” (i.e., word clusters) from a corpus of textual documents. In software engineering, topic modeling has been used to analyze textual data in empirical studies (e.g., to find out what developers talk about online), but also to build new techniques to support software engineering tasks (e.g., to support source code comprehension). Topic modeling needs to be applied carefully (e.g., depending on the type of textual data analyzed and modeling parameters). Our study aims at describing how topic modeling has been applied in software engineering research with a focus on four aspects: (1) which topic models and modeling techniques have been applied, (2) which textual inputs have been used for topic modeling, (3) how textual data was “prepared” (i.e., pre-processed) for topic modeling, and (4) how generated topics (i.e., word clusters) were named to give them a human-understandable meaning. We analyzed topic modeling as applied in 111 papers from ten highly-ranked software engineering venues (five journals and five conferences) published between 2009 and 2020. We found that (1) LDA and LDA-based techniques are the most frequent topic modeling techniques, (2) developer communication and bug reports have been modelled most, (3) data pre-processing and modeling parameters vary quite a bit and are often vaguely reported, and (4) manual topic naming (such as deducting names based on frequent words in a topic) is common.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {62},
keywords = {Literature analysis, Natural language processing, Text mining, Topic modeling}
}

@article{10.1109/TCBB.2015.2476790,
author = {Deng, Su-Ping and Zhu, Lin and Huang, De-Shuang},
title = {Predicting hub genes associated with cervical cancer through gene co-expression networks},
year = {2016},
issue_date = {January/February 2016},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {13},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2476790},
doi = {10.1109/TCBB.2015.2476790},
abstract = {Cervical cancer is the third most common malignancy in women worldwide. It remains a leading cause of cancer-related death for women in developing countries. In order to contribute to the treatment of the cervical cancer, in our work, we try to find a few key genes resulting in the cervical cancer. Employing functions of several bioinformatics tools, we selected 143 differentially expressed genes (DEGs) associated with the cervical cancer. The results of bioinformatics analysis show that these DEGs play important roles in the development of cervical cancer. Through comparing two differential co-expression networks (DCNs) at two different states, we found a common sub-network and two differential sub-networks as well as some hub genes in three sub-networks. Moreover, some of the hub genes have been reported to be related to the cervical cancer. Those hub genes were analyzed from Gene Ontology function enrichment, pathway enrichment and protein binding three aspects. The results can help us understand the development of the cervical cancer and guide further experiments about the cervical cancer.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {27–35},
numpages = {9},
keywords = {hub genes, differentially expressed genes, co-expression network, cervical cancer}
}

@inbook{10.5555/3454287.3454459,
author = {Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
title = {Meta-weight-net: learning an explicit mapping for sample weighting},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods. Source code is available at https://github.com/xjtushujun/meta-weight-net.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {172},
numpages = {12}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {deep learning, constraint solving, configurable systems, AutoML},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1016/j.infsof.2019.05.009,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh and Marston, Chad},
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.009},
doi = {10.1016/j.infsof.2019.05.009},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {131–145},
numpages = {15},
keywords = {Case study, User trust, Business-to-business, Machine learning, Big data}
}

@inproceedings{10.1145/3077981.3078031,
author = {Robinson, Carl Peter and Li, Baihua and Meng, Qinggang and Pain, Matthew T.G.},
title = {Pattern Classification of Hand Movements using Time Domain Features of Electromyography},
year = {2017},
isbn = {9781450352093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077981.3078031},
doi = {10.1145/3077981.3078031},
abstract = {Myoelectric control of prostheses is a long-established technique, using surface electromyography (sEMG) to detect the electrical signals of muscle activity and perform subsequent mechanical actions. Despite several decades' research, robust, responsive and intuitive control schemes remain elusive. Current commercial hardware advances offer a variety of movements but the control systems are unnatural, using sequential switching methods triggered by specific sEMG signals. However, recent research with pattern recognition and simultaneous and proportional control shows good promise for natural myoelectric control. This paper investigates several sEMG time domain features using a series of hand movements performed by 11 subjects, taken from a benchmark database, to determine if optimal classification accuracy is dependent on feature set size. The features were extracted from the data using a sliding window process and applied to five machine learning classifiers, of which Random Forest consistently performed best. Results suggest a few simple features such as Root Mean Square and Waveform Length achieve comparable performance to using the entire feature set, when identifying the hand movements, although further work is required for feature optimisation.},
booktitle = {Proceedings of the 4th International Conference on Movement Computing},
articleno = {27},
numpages = {6},
keywords = {Time domain features, Myoelectric control, Machine learning, Electromyography},
location = {London, United Kingdom},
series = {MOCO '17}
}

@inproceedings{10.1109/SPLC.2008.73,
author = {Benavides, David and Ruiz-Cort\'{e}s, Antonio and Batory, Don and Heymans, Patrick},
title = {First International Workshop on Analysis of Software Product Lines (ASPL'08)},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.73},
doi = {10.1109/SPLC.2008.73},
abstract = {The automation of software product line (SPL) analyses is of growing interest to both practitioners and researchers. In particular, automated analyses of variability models (like feature or decision models) and languages that foster declarative specifications of programs using those models are now common. We note that many of the problems that SPL engineers face are related to configuration problems that have been addressed by the Artificial Intelligence (AI) community. Indeed, the SPL community is using some of their results, e.g., BDD, CSP and SAT solvers.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {385},
series = {SPLC '08}
}

@article{10.1016/j.artint.2010.10.004,
author = {Gerevini, Alfonso E. and Saetti, Alessandro},
title = {Computing the minimal relations in point-based qualitative temporal reasoning through metagraph closure},
year = {2011},
issue_date = {February, 2011},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {175},
number = {2},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2010.10.004},
doi = {10.1016/j.artint.2010.10.004},
abstract = {Computing the minimal representation of a given set of constraints (a CSP) over the Point Algebra (PA) is a fundamental temporal reasoning problem. The main property of a minimal CSP over PA is that the strongest entailed relation between any pair of variables in the CSP can be derived in constant time. We study some new methods for solving this problem which exploit and extend two prominent graph-based representations of a CSP over PA: the timegraph and the series-parallel (SP) metagraph. Essentially, these are graphs partitioned into sets of chains and series-parallel subgraphs, respectively, on which the search is supported by a metagraph data structure. The proposed approach is based on computing the metagraph closure for these representations, which can be accomplished by some methods studied in the paper. In comparison with the known techniques based on enforcing path consistency, under certain conditions about the structure of the input CSP and the size of the generated metagraph, the proposed metagraph closure approach has better worst-case time and space complexity. Moreover, for every sparse CSP over the convex PA, the time complexity is reduced to O(n^2) from O(n^3), where n is the number of variables involved in the CSP. An extensive experimental analysis presented in the paper compares the proposed techniques and other known algorithms. These experimental results identify the best performing methods and show that, in practice, for CSPs exhibiting chain or SP-graph structure and randomly generated (both sparse and dense) CSPs, the metagraph closure approach is significantly faster than the approach based on enforcing path consistency.},
journal = {Artif. Intell.},
month = feb,
pages = {556–585},
numpages = {30},
keywords = {Tractable reasoning, Temporal relations and constraints, Temporal CSP, Simple interval algebra, Qualitative temporal reasoning, Point calculus, Point algebra, Minimal relations, Graph-based reasoning, Constraint-based reasoning}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@article{10.1016/j.infsof.2006.08.001,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Classifying variability modeling techniques},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.001},
doi = {10.1016/j.infsof.2006.08.001},
abstract = {Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {717–739},
numpages = {23},
keywords = {Variability modeling, Variability management, Software product family, Classification}
}

@inproceedings{10.1145/2897053.2897058,
author = {Sharifloo, Amir Molzam and Metzger, Andreas and Quinton, Cl\'{e}ment and Baresi, Luciano and Pohl, Klaus},
title = {Learning and evolution in dynamic software product lines},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897058},
doi = {10.1145/2897053.2897058},
abstract = {A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {158–164},
numpages = {7},
keywords = {machine learning, evolution, dynamic software product lines, adaptation},
location = {Austin, Texas},
series = {SEAMS '16}
}

@inproceedings{10.1145/3302333.3302343,
author = {Cruz, Daniel and Figueiredo, Eduardo and Martinez, Jabier},
title = {A Literature Review and Comparison of Three Feature Location Techniques using ArgoUML-SPL},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302343},
doi = {10.1145/3302333.3302343},
abstract = {Over the last decades, the adoption of Software Product Line (SPL) engineering for supporting software reuse has increased. An SPL can be extracted from one single product or from a family of related software products, and feature location strategies are widely used for variability mining. Several feature location strategies have been proposed in the literature and they usually aim to map a feature to its source code implementation. In this paper, we present a systematic literature review that identifies and characterizes existing feature location strategies. We also evaluated three different strategies based on textual information retrieval in the context of the ArgoUML-SPL feature location case study. In this evaluation, we compare the strategies based on their ability to correctly identify the source code of several features from ArgoUML-SPL ground truth. We then discuss the strengths and weaknesses of each feature location strategy.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {10},
keywords = {variability mining, software product lines, reverse engineering, feature location, benchmark},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/2829966.2829967,
author = {Danieli, Morena and Riccardi, Giuseppe and Alam, Firoj},
title = {Emotion Unfolding and Affective Scenes: A Case Study in Spoken Conversations},
year = {2015},
isbn = {9781450339889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2829966.2829967},
doi = {10.1145/2829966.2829967},
abstract = {The manifestation of human emotions evolves over time and space. Most of the work on affective computing research is limited to the association of context-free signal segments, such as utterances and images, to basic emotions. In this paper, we discuss the hypothesis that interpreting emotions requires a conceptual description of their dynamics within the context of their manifestations. We describe the unfolding of emotions through the proposed affective scene framework. Affective scenes are defined in terms of who first expresses the variation in their emotional state in a conversation, how this affects the other speaker's emotional appraisal and response, and which modifications occur from the initial through the final state of the scene. This conceptual framework is applied and evaluated on real human-human conversations drawn from call centers. We show that the automatic classification of affective scenes achieves more than satisfactory results and it benefits from acoustic, lexical and psycholinguistic features of the speech and linguistics signals.},
booktitle = {Proceedings of the International Workshop on Emotion Representations and Modelling for Companion Technologies},
pages = {5–11},
numpages = {7},
keywords = {spoken conversation, machine learning, emotion, computational paralinguistics, affective scene},
location = {Seattle, Washington, USA},
series = {ERM4CT '15}
}

@article{10.1007/s10664-019-09686-w,
author = {Minku, Leandro L.},
title = {A novel online supervised hyperparameter tuning procedure applied to cross-company software effort estimation},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09686-w},
doi = {10.1007/s10664-019-09686-w},
abstract = {Software effort estimation is an online supervised learning problem, where new training projects may become available over time. In this scenario, the Cross-Company (CC) approach Dycom can drastically reduce the number of Within-Company (WC) projects needed for training, saving their collection cost. However, Dycom requires CC projects to be split into subsets. Both the number and composition of such subsets can affect Dycom’s predictive performance. Even though clustering methods could be used to automatically create CC subsets, there are no procedures for automatically tuning the number of clusters over time in online supervised scenarios. This paper proposes the first procedure for that. An investigation of Dycom using six clustering methods and three automated tuning procedures is performed, to check whether clustering with automated tuning can create well performing CC splits. A case study with the ISBSG Repository shows that the proposed tuning procedure in combination with a simple threshold-based clustering method is the most successful in enabling Dycom to drastically reduce (by a factor of 10) the number of required WC training projects, while maintaining (or even improving) predictive performance in comparison with a corresponding WC model. A detailed analysis is provided to understand the conditions under which this approach does or does not work well. Overall, the proposed online supervised tuning procedure was generally successful in enabling a very simple threshold-based clustering approach to obtain the most competitive Dycom results. This demonstrates the value of automatically tuning hyperparameters over time in a supervised way.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {3153–3204},
numpages = {52},
keywords = {Hyperparameter tuning, Online learning, Concept drift, Transfer learning, Cross-company learning, Software effort estimation}
}

@article{10.1016/j.procs.2019.12.135,
author = {Jamil, Muhammad Abid and Nour, Mohamed K and Alhindi, Ahmad and Awang Abhubakar, Normi Sham and Arif, Muhammad and Aljabri, Tareq Fahad},
title = {Towards Software Product Lines Optimization Using Evolutionary Algorithms},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {163},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.135},
doi = {10.1016/j.procs.2019.12.135},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {527–537},
numpages = {11},
keywords = {Multi-objective Algorithms, Feature Models, Software Product Lines, Software Testing, Search Based Software Engineering}
}

@article{10.1145/3364684,
author = {Bianchini, Ricardo and Fontoura, Marcus and Cortez, Eli and Bonde, Anand and Muzio, Alexandre and Constantin, Ana-Maria and Moscibroda, Thomas and Magalhaes, Gabriel and Bablani, Girish and Russinovich, Mark},
title = {Toward ML-centric cloud platforms},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3364684},
doi = {10.1145/3364684},
abstract = {Exploring the opportunities to use ML, the possible designs, and our experience with Microsoft Azure.},
journal = {Commun. ACM},
month = jan,
pages = {50–59},
numpages = {10}
}

@inproceedings{10.1007/978-3-030-67658-2_34,
author = {Yamaguchi, Akihiro and Maya, Shigeru and Ueno, Ken},
title = {RLTS: Robust Learning Time-Series Shapelets},
year = {2020},
isbn = {978-3-030-67657-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67658-2_34},
doi = {10.1007/978-3-030-67658-2_34},
abstract = {Shapelets are time-series segments effective for classifying time-series instances. Joint learning of both classifiers and shapelets has been studied in recent years because such a method provides both superior classification performance and interpretable results. For robust learning, we introduce Self-Paced Learning (SPL) and adaptive robust losses into this method. The SPL method can assign latent instance weights by considering not only classification losses but also understandable shapelet discovery. Furthermore, the adaptive robustness introduced into feature vectors is jointly learned with shapelets, a classifier, and latent instance weights. We demonstrate the superiority of AUC and the validity of our approach on UCR time-series datasets.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part I},
pages = {595–611},
numpages = {17},
keywords = {Time-series shapelets, Self-paced learning, Robust losses},
location = {Ghent, Belgium}
}

@inproceedings{10.1109/COMPSAC.2015.64,
author = {He, Xiao and Fu, Yanmei and Sun, Chang-Ai and Ma, Zhiyi and Shao, Weizhong},
title = {Towards Model-Driven Variability-Based Flexible Service Compositions},
year = {2015},
isbn = {9781467365642},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2015.64},
doi = {10.1109/COMPSAC.2015.64},
abstract = {In cloud computing, variability becomes a crucial ability of process-based cloud applications. Existing solutions either focused on modeling variability in the architectural model or tried to support dynamic variability management in implementation. An integrated approach that can inherit the virtues from both categories is expected. The paper aims to fill the gap by proposing a model-driven variability-based service composition approach. We propose VxUML to model the variability in the architecture model. Then, we define a set of model transformation rules to convert VxUML into VxBPEL (an extension to standard BPEL supporting variability at the implementation level). Finally, we implement a prototype tool, and present a case study to demonstrate the feasibility of our approach.},
booktitle = {Proceedings of the 2015 IEEE 39th Annual Computer Software and Applications Conference - Volume 02},
pages = {298–303},
numpages = {6},
keywords = {Variability management, Service composition, Model-driven engineering, Model transformation},
series = {COMPSAC '15}
}

@inproceedings{10.1145/3324884.3416627,
author = {Li, Mingyang and Shi, Lin and Yang, Ye and Wang, Qing},
title = {A deep multitask learning approach for requirements discovery and annotation from open forum},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416627},
doi = {10.1145/3324884.3416627},
abstract = {The ability in rapidly learning and adapting to evolving user needs is key to modern business successes. Existing methods are based on text mining and machine learning techniques to analyze user comments and feedback, and often constrained by heavy reliance on manually codified rules or insufficient training data. Multitask learning (MTL) is an effective approach with many successful applications, with the potential to address these limitations associated with requirements analysis tasks. In this paper, we propose a deep MTL-based approach, DEMAR, to address these limitations when discovering requirements from massive issue reports and annotating the sentences in support of automated requirements analysis. DEMAR consists of three main phases: (1) data augmentation phase, for data preparation and allowing data sharing beyond single task learning; (2) model construction phase, for constructing the MTL-based model for requirements discovery and requirements annotation tasks; and (3) model training phase, enabling eavesdropping by shared loss function between the two related tasks. Evaluation results from eight open-source projects show that, the proposed multitask learning approach outperforms two state-of-the-art approaches (CNC and FRA) and six common machine learning algorithms, with the precision of 91% and the recall of 83% for requirements discovery task, and the overall accuracy of 83% for requirements annotation task. The proposed approach provides a novel and effective way to jointly learn two related requirements analysis tasks. We believe that it also sheds light on further directions of exploring multitask learning in solving other software engineering problems.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {336–348},
numpages = {13},
keywords = {deep learning, multitask learning, requirements annotation, requirements discovery},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1109/MODELS-C.2019.00047,
author = {Zhao, Xin and Gray, Jeff},
title = {BESMER: an approach for bad smells summarization in systems models},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00047},
doi = {10.1109/MODELS-C.2019.00047},
abstract = {Bad smells are surface indications of potential problems with source code quality and have been investigated deeply within the purview of object-oriented programming. However, there has not been much research conducted to understand bad smells within the context of systems models. Moreover, the majority of bad smells in existing literature have been suggested by experienced developers and researchers who may view "smells" differently from inexperienced developers. To this end, we propose BESMER, our project that categorizes bad smells in systems models by mining discussion forum posts of end-users of a specific systems modeling tool. Specifically, this paper describes how our three-level discovery mechanism based on machine learning techniques assisted us in finding bad smells from the LabVIEW online discussion forum. Our experimental results not only confirm that end-users also encounter the bad smells proposed by experts, but also reveal new bad smells for LabVIEW models. We also present some implications discovered from the examination of user posts and list areas of future work based on our current findings. As far as we know, this is the first paper to investigate bad smells from an end-user's perspective within the context of systems models.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {304–313},
numpages = {10},
keywords = {LabVIEW, bad smells, discussion forum, machine learning, systems modeling},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.5555/3172077.3172181,
author = {Li, Hao and Gong, Maoguo},
title = {Self-paced convolutional neural networks},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks. In order to distinguish the reliable data from the noisy and confusing data, we improve CNNs with self-paced learning (SPL) for enhancing the learning robustness of CNNs. In the proposed self-paced convolutional network (SPCN), each sample is assigned to a weight to reflect the easiness of the sample. Then a dynamic self-paced function is incorporated into the leaning objective of CNN to jointly learn the parameters of CNN and the latent weight variable. SPCN learns the samples from easy to complex and the sample weights can dynamically control the learning rates for converging to better values. To gain more insights of SPCN, theoretical studies are conducted to show that SPCN converges to a stationary solution and is robust to the noisy and confusing data. Experimental results on MNIST and  rectangles  datasets demonstrate that the proposed method outperforms baseline methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2110–2116},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1007/978-3-030-58208-1_3,
author = {Tajiri, Yui and Mimura, Mamoru},
title = {Detection of Malicious PowerShell Using Word-Level Language Models},
year = {2020},
isbn = {978-3-030-58207-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58208-1_3},
doi = {10.1007/978-3-030-58208-1_3},
abstract = {There is a growing tendency for cybercriminals to abuse legitimate tools installed on the target computers for cyberattacks. In particular, the use of PowerShell provided by Microsoft has been increasing every year and has become a threat. In previous studies, a method to detect malicious PowerShell commands using character-level deep learning was proposed. The proposed method combines traditional natural language processing and character-level convolutional neural networks. This method, however, requires time for dynamic analysis. This paper proposes a method to classify unknown PowerShell without dynamic analysis. Our method uses feature vectors extracted from malicious and benign PowerShell scripts using word-level language models for classification. The datasets were generated from benign and malicious PowerShell scripts obtained from Hybrid Analysis, and benign PowerShell scripts obtained from GitHub, which are imbalanced. The experimental result shows that the combination of the LSI and XGBoost produces the highest detection rate. The maximum accuracy achieves approximately 0.95 on the imbalanced dataset. Furthermore, over 50% of unknown malicious PowerShell scripts could be detected in time series analysis without dynamic analysis.},
booktitle = {Advances in Information and Computer Security: 15th International Workshop on Security, IWSEC 2020, Fukui, Japan, September 2–4, 2020, Proceedings},
pages = {39–56},
numpages = {18},
keywords = {XGBoost, Doc2Vec, Latent Semantic Indexing, PowerShell},
location = {Fukui, Japan}
}

@article{10.1002/smr.2158,
author = {Grano, Giovanni and Titov, Timofey V. and Panichella, Sebastiano and Gall, Harald C.},
title = {Branch coverage prediction in automated testing},
year = {2019},
issue_date = {September 2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {31},
number = {9},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2158},
doi = {10.1002/smr.2158},
abstract = {Software testing is crucial in continuous integration (CI). Ideally, at every commit, all the test cases should be executed, and moreover, new test cases should be generated for the new source code. This is especially true in a Continuous Test Generation (CTG) environment, where the automatic generation of test cases is integrated into the continuous integration pipeline. In this context, developers want to achieve a certain minimum level of coverage for every software build. However, executing all the test cases and, moreover, generating new ones for all the classes at every commit is not feasible. As a consequence, developers have to select which subset of classes has to be tested and/or targeted by test‐case generation. We argue that knowing a priori the branch coverage that can be achieved with test‐data generation tools can help developers into taking informed decision about those issues. In this paper, we investigate the possibility to use source‐code metrics to predict the coverage achieved by test‐data generation tools. We use four different categories of source‐code features and assess the prediction on a large data set involving more than 3'000 Java classes. We compare different machine learning algorithms and conduct a fine‐grained feature analysis aimed at investigating the factors that most impact the prediction accuracy. Moreover, we extend our investigation to four different search budgets. Our evaluation shows that the best model achieves an average 0.15 and 0.21 MAE on nested cross‐validation over the different budgets, respectively, on evosuite and randoop. Finally, the discussion of the results demonstrate the relevance of coupling‐related features for the prediction accuracy.In this paper, we predict the coverage achieved by test‐data generator tools using source‐code metrics. We build a Random Forest Regressor model with an average MAE of 0.2. This results substantially improve the performance of the state‐of‐art predictor.


image
image},
journal = {J. Softw. Evol. Process},
month = oct,
numpages = {18},
keywords = {software testing, machine learning, coverage prediction, automated software testing}
}

@article{10.1007/s11042-020-09956-6,
author = {Yadav, Hitesh and Chhikara, Rita and Kumari, A. Charan},
title = {A novel hybrid approach for feature selection in software product lines},
year = {2021},
issue_date = {Feb 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {4},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09956-6},
doi = {10.1007/s11042-020-09956-6},
abstract = {Software Product Line (SPL) customizes software by combining various existing features of the software with multiple variants. The main challenge is selecting valid features considering the constraints of the feature model. To solve this challenge, a hybrid approach is proposed to optimize the feature selection problem in software product lines. The Hybrid approach ‘Hyper-PSOBBO’ is a combination of Particle Swarm Optimization (PSO), Biogeography-Based Optimization (BBO) and hyper-heuristic algorithms. The proposed algorithm has been compared with Bird Swarm Algorithm (BSA), PSO, BBO, Firefly, Genetic Algorithm (GA) and Hyper-heuristic. All these algorithms are performed in a set of 10 feature models that vary from a small set of 100 to a high-quality data set of 5000. The detailed empirical analysis in terms of performance has been carried out on these feature models. The results of the study indicate that the performance of the proposed method is higher to other state-of-the-art algorithms.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {4919–4942},
numpages = {24},
keywords = {Feature model (FM), Software product lines (SPL), Bird swarm optimization (BSA), Genetic algorithm (GA), Firefly, Biogeography-based optimization, Hyper-heuristic, Particle swarm optimization}
}

@article{10.1016/j.neucom.2014.06.096,
author = {Liu, Weifeng and Liu, Hongli and Tao, Dapeng and Wang, Yanjiang and Lu, Ke},
title = {Manifold regularized kernel logistic regression for web image annotation},
year = {2016},
issue_date = {Jan 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {172},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.06.096},
doi = {10.1016/j.neucom.2014.06.096},
journal = {Neurocomput.},
month = jan,
pages = {3–8},
numpages = {6},
keywords = {Manifold regularization, Kernel logistic regression, Laplacian eigenmaps, Semi-supervised learning, Image annotation}
}

@article{10.1145/1183236.1183242,
author = {Crawford, Diane},
title = {Editorial pointers},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183242},
doi = {10.1145/1183236.1183242},
journal = {Commun. ACM},
month = dec,
pages = {5},
numpages = {2}
}

@article{10.1007/s10515-021-00287-w,
author = {Gadelha, Guilherme and Ramalho, Franklin and Massoni, Tiago},
title = {Traceability recovery between bug reports and test cases-a Mozilla Firefox case study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-021-00287-w},
doi = {10.1007/s10515-021-00287-w},
abstract = {Automatic recovery of traceability between software artifacts may promote early detection of issues and better calculate change impact. Information Retrieval (IR) techniques have been proposed for the task, but they differ considerably in input parameters and results. It is difficult to assess results when those techniques are applied in isolation, usually in small or medium-sized software projects. Recently, multilayered approaches to machine learning, in special Deep Learning (DL), have achieved success in text classification through their capacity to model complex relationships among data. In this article, we apply several IR and DL techniques for investing automatic traceability between bug reports and manual test cases, using historical data from the Mozilla Firefox’s Quality Assurance (QA) team. In this case study, we assess the following IR techniques: LSI, LDA, and BM25, in addition to a DL architecture called Convolutional Neural Networks (CNNs), through the use of Word Embeddings. In this context of traceability, we observe poor performances from three out of the four studied techniques. Only the LSI technique presented acceptable results, standing out even over the state-of-the-art BM25 technique. The obtained results suggest that the semi-automatic application of the LSI technique – with an appropriate combination of thresholds – may be feasible for real-world software projects.},
journal = {Automated Software Engg.},
month = nov,
numpages = {46},
keywords = {Deep learning, Information retrieval, Traceability, Test cases, System features, Bug reports}
}

@inproceedings{10.5555/3053577.3053583,
author = {Hubaux, Arnaud and Jannach, Dietmar and Drescher, Conrad and Murta, Leonardo and M\"{a}nnist\"{o}, Tomi and Czarnecki, Krzysztof and Heymans, Patrick and Nguyen, Tien and Zanker, Markus},
title = {Unifying software and product configuration: a research roadmap},
year = {2012},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {For more than 30 years, knowledge-based product configuration systems have been successfully applied in many industrial domains. Correspondingly, a large number of advanced techniques and algorithms have been developed in academia and industry to support different aspects of configuration reasoning. While traditional research in the field focused on the configuration of physical artefacts, recognition of the business value of customizable software products led to the emergence of software product line engineering. Despite the significant overlap in research interests, the two fields mainly evolved in isolation. Only limited attempts were made at combining the approaches developed in the different fields. In this paper, we first aim to give an overview of commonalities and differences between software product line engineering and product configuration. We then identify opportunities for cross-fertilization between these fields and finally develop a research agenda to combine their respective techniques. Ultimately, this should lead to a unified configuration approach.},
booktitle = {Proceedings of the 2012 International Conference on Configuration - Volume 958},
pages = {31–35},
numpages = {5},
location = {Montpellier, France},
series = {CONFWS'12}
}

@inproceedings{10.1145/3358331.3358369,
author = {Shang, Huiping and Wang, Jianxing and Zhao, Junxiang},
title = {Design and Implementation of SCC System based on Cloud Architecture},
year = {2019},
isbn = {9781450372022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358331.3358369},
doi = {10.1145/3358331.3358369},
abstract = {This paper addresses a service-oriented, customized on-demand, integrated cloud architecture for the satellite control center system. The cloud architecture is a three-layer hierarchy including service/business layer, platform layer and hardware layer. Based on the cloud architecture, the SCC system is designed to integrate computing, storage, and networking resources to form a powerful converged infrastructure platform. The SCC software are implemented as eleven functional services: data display software, data processing software, data storage and management software, database software, orbit calculation and analysis software, control calculation and analysis software, duplex management software, command operation software, monitor and control software, interface computer software and time service software. Details of the overall system architecture and the integration of the above software services are included in the paper. The paper concludes that design and implementation of efficient, reliable, scalable and flexible ground control system is an attractive but challenging area, and there is still work to be done.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing},
articleno = {38},
numpages = {6},
keywords = {virtualization, software service, cloud management software, cloud architecture, VM, TT&amp;C, SCC},
location = {Dublin, Ireland},
series = {AIAM 2019}
}

@phdthesis{10.5555/AAI28544034,
author = {Khoshmanesh, Seyedehzahra and Samik, Basu, and Andrew, Miner, and Hridesh, Rajan, and Karin, Dorman,},
advisor = {R, Lutz, Robyn},
title = {Learning Feature Interactions with and without Specifications},
year = {2021},
isbn = {9798544278207},
publisher = {Iowa State University},
address = {USA},
abstract = {Developers of software product lines and highly configurable systems reuse and combine features (units of functionality) to build new or customize existing products. However, features can interact in ways that are contrary to developers' intent. Predicting whether a new combination of features will produce an unwanted or even hazardous feature interaction is a continuing challenge. Current techniques to detect unwanted feature interactions are costly, slow, and inadequate. In this thesis, we investigate how to detect unwanted feature interactions early in development and that are scalable to large software product lines or highly configurable systems. First, we propose a similarity-based method to identify unwanted feature interactions much earlier in the development process for early detection. It uses knowledge of prior feature interactions stored with the software product line's feature model to help find unwanted interactions between a new feature and existing features. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.Moreover, to learn and automate the detection, we show how detecting unwanted feature interactions can be effectively represented as a link prediction problem. We investigate six link-based similarity metrics and evaluate our approach on a software product line benchmark. Results show that the best machine learning algorithms achieve an accuracy of 0.75 to 1 for classifying feature interactions.Finally, we develop a new approach based on program analysis that extracts feature-relevant learning models from the source code to obtain more semantic details of unwanted feature interactions. The method is capable of learning feature interactions whether constraints on feature combinations are specified or not. If specifications of feature constraints are unavailable, as is common in real-world systems, our approach infers the constraints using feature-related data-flow dependency information. Experimental evaluation on three software product line benchmarks and a highly configurable system shows that this approach is fast and effective.The contribution is to support developers by automatically detecting those feature combinations in a new product or version that can interact in unwanted or unrecognized ways. This enables a better understanding of hidden interactions and identifies software components that should be tested together because their features interact in some configurations.},
note = {AAI28544034}
}

@inproceedings{10.1007/978-3-319-65340-2_44,
author = {Cunha, Pedro and Ferreira, Andr\'{e} and Cortez, Paulo},
title = {Mining Rational Team Concert Repositories: A&nbsp;Case Study on a Software Project},
year = {2017},
isbn = {978-3-319-65339-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-65340-2_44},
doi = {10.1007/978-3-319-65340-2_44},
abstract = {Software repositories are key to support the development of software. In this article, we present a Mining Software Repositories (MSR) approach that considered a two-year software project repository, set using the Rational Team Concert (RTC) tool. Such MSR was designed in terms of three main components: RTC data extraction, RTC data mining and design of RTC intelligence dashboard. In particular, we focus more on the data extraction component, although we also present mining and dashboard outcomes. Interesting results were achieved, revealing a potential of the proposed MSR to improve the software project planning/development agility and quality.},
booktitle = {Progress in Artificial Intelligence: 18th EPIA Conference on Artificial Intelligence, EPIA 2017, Porto, Portugal, September 5-8, 2017, Proceedings},
pages = {537–548},
numpages = {12},
keywords = {Association rules, Data mining, Software engineering},
location = {Porto, Portugal}
}

@inproceedings{10.1109/WAIN52551.2021.00024,
author = {M\"{a}kinen, Sasu and Skogstr\"{o}m, Henrik and Laaksonen, Eero and Mikkonen, Tommi},
title = {Who Needs MLOps: What Data Scientists Seek to Accomplish and How Can MLOps Help?},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WAIN52551.2021.00024},
doi = {10.1109/WAIN52551.2021.00024},
abstract = {Following continuous software engineering practices, there has been an increasing interest in rapid deployment of machine learning (ML) features, called MLOps. In this paper, we study the importance of MLOps in the context of data scientists’ daily activities, based on a survey where we collected responses from 331 professionals from 63 different countries in ML domain, indicating on what they were working on in the last three months. Based on the results, up to 40% respondents say that they work with both models and infrastructure; the majority of the work revolves around relational and time series data; and the largest categories of problems to be solved are predictive analysis, time series data, and computer vision. The biggest perceived problems revolve around data, although there is some awareness of problems related to deploying models to production and related procedures. To hypothesise, we believe that organisations represented in the survey can be divided to three categories – (i) figuring out how to best use data; (ii) focusing on building the first models and getting them to production; and (iii) managing several models, their versions and training datasets, as well as retraining and frequent deployment of retrained models. In the results, the majority of respondents are in category (i) or (ii), focusing on data and models; however the benefits of MLOps only emerge in category (iii) when there is a need for frequent retraining and redeployment. Hence, setting up an MLOps pipeline is a natural step to take, when an organization takes the step from ML as a proof-of-concept to ML as a part of nominal activities.},
booktitle = {2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)},
pages = {109–112},
numpages = {4},
location = {Madrid, Spain}
}

@article{10.1016/j.compind.2015.08.004,
author = {Leit\~{a}o, Paulo and Colombo, Armando Walter and Karnouskos, Stamatis},
title = {Industrial automation based on cyber-physical systems technologies},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {81},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2015.08.004},
doi = {10.1016/j.compind.2015.08.004},
abstract = {Roadmap for the development of industrial cyber-physical systems.Description of 4 prototype implementations for industrial automation based on cyber-physical systems technologies.Overview of key CPS challenges to increase Technology Readiness Levels. Cyber-Physical Systems (CPS) is an emergent approach that focuses on the integration of computational applications with physical devices, being designed as a network of interacting cyber and physical elements. CPS control and monitor real-world physical infrastructures and thus is starting having a high impact in industrial automation. As such design, implementation and operation of CPS and management of the resulting automation infrastructure is of key importance for the industry. In this work, an overview of key aspects of industrial CPS, their technologies and emerging directions, as well as challenges for their implementation is presented. Based on the hands-on experiences gathered from four European innovation projects over the last decade (i.e. SOCRADES, IMC-AESOP, GRACE and ARUM), a key challenges have been identified and a prioritization and timeline are pointed out with the aim to increase Technology Readiness Levels and lead to their usage in industrial automation environments.},
journal = {Comput. Ind.},
month = sep,
pages = {11–25},
numpages = {15},
keywords = {Service-oriented architectures, Self-organization, Next generation of industrial systems, Multi-agent systems, Cyber-physical systems, Collaborative automation}
}

@inproceedings{10.5555/3491440.3491568,
author = {Guo, Dan and Wang, Yang and Song, Peipei and Wang, Meng},
title = {Recurrent relational memory network for unsupervised image captioning},
year = {2021},
isbn = {9780999241165},
abstract = {Unsupervised image captioning with no annotations is an emerging challenge in computer vision, where the existing arts usually adopt GAN (Generative Adversarial Networks) models. In this paper, we propose a novel memory-based network rather than GAN, named Recurrent Relational Memory Network (R2M). Unlike complicated and sensitive adversarial learning that non-ideally performs for long sentence generation, R2M implements a concepts-to-sentence memory translator through two-stage memory mechanisms: fusion and recurrent memories, correlating the relational reasoning between common visual concepts and the generated words for long periods. R2M encodes visual context through unsupervised training on images, while enabling the memory to learn from irrelevant textual corpus via supervised fashion. Our solution enjoys less learnable parameters and higher computational efficiency than GAN-based methods, which heavily bear parameter sensitivity. We experimentally validate the superiority of R2M than state-of-the-arts on all benchmark datasets.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {128},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@article{10.1007/s10270-020-00814-5,
author = {Mussbacher, Gunter and Combemale, Benoit and Kienzle, J\"{o}rg and Abrah\~{a}o, Silvia and Ali, Hyacinth and Bencomo, Nelly and B\'{u}r, M\'{a}rton and Burgue\~{n}o, Loli and Engels, Gregor and Jeanjean, Pierre and J\'{e}z\'{e}quel, Jean-Marc and K\"{u}hn, Thomas and Mosser, S\'{e}bastien and Sahraoui, Houari and Syriani, Eugene and Varr\'{o}, D\'{a}niel and Weyssow, Martin},
title = {Opportunities in intelligent modeling assistance},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00814-5},
doi = {10.1007/s10270-020-00814-5},
abstract = {Modeling is requiring increasingly larger efforts while becoming indispensable given the complexity of the problems we are solving. Modelers face high cognitive load to understand a multitude of complex abstractions and their relationships. There is an urgent need to better support tool builders to ultimately provide modelers with intelligent modeling assistance that learns from previous modeling experiences, automatically derives modeling knowledge, and provides context-aware assistance. However, current intelligent modeling assistants (IMAs) lack adaptability and flexibility for tool builders, and do not facilitate understanding the differences and commonalities of IMAs for modelers. Such a patchwork of limited IMAs is a lost opportunity to provide modelers with better support for the creative and rigorous aspects of software engineering. In this expert voice, we present a conceptual reference framework (RF-IMA) and its properties to identify the foundations for intelligent modeling assistance. For tool builders, RF-IMA aims to help build IMAs more systematically. For modelers, RF-IMA aims to facilitate comprehension, comparison, and integration of IMAs, and ultimately to provide more intelligent support. We envision a momentum in the modeling community that leads to the implementation of RF-IMA and consequently future IMAs. We identify open challenges that need to be addressed to realize the opportunities provided by intelligent modeling assistance.},
journal = {Softw. Syst. Model.},
month = sep,
pages = {1045–1053},
numpages = {9},
keywords = {Feedback, Development data, Artificial intelligence, Integrated development environment, Intelligent modeling assistance, Model-based software engineering}
}

@inproceedings{10.1109/SPLC.2008.11,
author = {Niu, Nan and Easterbrook, Steve},
title = {On-Demand Cluster Analysis for Product Line Functional Requirements},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.11},
doi = {10.1109/SPLC.2008.11},
abstract = {We propose an on-demand clustering framework for analyzing the functional requirements in a product line. Our approach is novel in that the objects to be clustered capture the domain's action themes at a primitive level, and the essential attributes are uncovered via semantic analysis. We provide automatic support to complement domain analysis by quickly identifying important entities and functionalities. A second contribution is our recognition of stakeholders' different goals in cluster analysis, e.g., feature identification for users versus system decomposition for designers. We thus advance the literature by examining requirements clusters that overlap and those causing a minimal information loss, and by facilitating the discovery of product line variabilities. A proof-of-concept example is presented to show the applicability and usefulness of our approach.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {87–96},
numpages = {10},
keywords = {requirements clustering, overlapping clustering, information-theoretic clustering, functional requirements profiles},
series = {SPLC '08}
}

@inproceedings{10.1145/3459637.3482458,
author = {Keramati, Mahsa and Zohrevand, Zahra and Gl\"{a}sser, Uwe},
title = {Norma: A Hybrid Feature Alignment for Class-Aware Unsupervised Domain Adaptation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482458},
doi = {10.1145/3459637.3482458},
abstract = {Unsupervised domain adaptation is the problem of transferring extracted knowledge from a labeled source domain to an unlabeled target domain. To achieve discriminative domain adaptation recent studies take advantage of target sample pseudo-labels to impose class-aware distribution alignment across the source and target domains. Still, they have some shortcomings such as making decisions based on inaccurate pseudo-labeled samples that mislead the adaptation process. In this paper, we propose a progressive deep feature alignment, called Norma, to tackle class-aware unsupervised domain adaptation for image classification by enforcing inter-class compactness and intra-class discrepancy through a hybrid learning process. To this end, Norma's optimization process is defined based on a novel triplet loss which not only addresses soft prototype alignment but also pushes away multiple negative centroids. Also, to extract maximum discriminative domain knowledge per iteration, we propose a joint positive and negative learning procedure along with an uncertainty-guided progressive pseudo-labeling on the basis of prototype-based clustering and conditional probability. Our experimental results on several benchmarks demonstrate that Norma outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {833–843},
numpages = {11},
keywords = {transfer learning, pseudo-labeling, negative learning, image classification, deep-metric learning, class-aware alignment, adversarial unsupervised domain adaptation},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1016/j.jksuci.2017.07.006,
author = {Lal, Sangeeta and Sardana, Neetu and Sureka, Ashish},
title = {Three-level learning for improving cross-project logging prediction for if-blocks},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {31},
number = {4},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2017.07.006},
doi = {10.1016/j.jksuci.2017.07.006},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = oct,
pages = {481–496},
numpages = {16}
}

@article{10.5555/2946645.3007037,
author = {Wei, Ermo and Luke, Sean},
title = {Lenient learning in independent-learner stochastic cooperative games},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We introduce the Lenient Multiagent Reinforcement Learning 2 (LMRL2) algorithm for independent-learner stochastic cooperative games. LMRL2 is designed to overcome a pathology called relative overgeneralization, and to do so while still performing well in games with stochastic transitions, stochastic rewards, and miscoordination. We discuss the existing literature, then compare LMRL2 against other algorithms drawn from the literature which can be used for games of this kind: traditional ("Distributed") Q-learning, Hysteretic Q-learning, WoLF-PHC, SOoN, and (for repeated games only) FMQ. The results show that LMRL2 is very effective in both of our measures (complete and correct policies), and is found in the top rank more often than any other technique. LMRL2 is also easy to tune: though it has many available parameters, almost all of them stay at default settings. Generally the algorithm is optimally tuned with a single parameter, if any. We then examine and discuss a number of side-issues and options for LMRL2.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2914–2955},
numpages = {42},
keywords = {reinforcement learning, multiagent learning, lenient learning, independent learner, game theory}
}

@article{10.1007/s00500-020-05150-w,
author = {Akbar, Muhammad Azeem and Mahmood, Sajjad and Shafiq, Muhammad and Alsanad, Ahmed and Alsanad, Abeer Abdul-Aziz and Gumaei, Abdu},
title = {RETRACTED ARTICLE: Identification and prioritization of DevOps success factors using fuzzy-AHP approach},
year = {2020},
issue_date = {Feb 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {4},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05150-w},
doi = {10.1007/s00500-020-05150-w},
abstract = {DevOps (development and operations) is a collaborative and multidisciplinary organizational effort to automate continuous delivery of a software project with an aim to improve software quality. The implementation of DevOps practices is not straightforward as there are several complexities associated with it. The aim of this study is to identify and prioritize the factors that positively influence the DevOps practices in software organizations. Using a systematic literature review, 19 factors were identified. The identified factors were further validated with experts via a questionnaire survey study. Finally, Fuzzy Analytical Hierarchy Process (FAHP) was used to prioritize the identified success factors. The results indicate that “DevOps security pipeline,” “use system orchestration” and “attempt matrix organization and transparency” factors are the highest ranked success factors for the successful implementation of DevOps practices. The FAHP analysis is novel in this research area as it provides the prioritization based taxonomy of the identified factors which will assist the researchers and practitioners to focus on the critical areas that are significant for the successful adoption of DevOps practices.},
journal = {Soft Comput.},
month = jul,
pages = {1907–1931},
numpages = {25},
keywords = {DevOps, Fuzzy AHP, Success factors, Systematic literature review, Empirical investigation}
}

@inproceedings{10.1109/ICTAI.2014.107,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {A Comparative Analysis of Two Multi-objective Evolutionary Algorithms in Product Line Architecture Design Optimization},
year = {2014},
isbn = {9781479965724},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2014.107},
doi = {10.1109/ICTAI.2014.107},
abstract = {The Product Line Architecture (PLA) design is a multi-objective optimization problem that can be properly solved with search-based algorithms. However, search-based PLA design is an incipient research field. Due to this, works in this field have addressed main points to solve the problem: adequate representation, specific search operators and suitable evaluation fitness functions. Similarly what happens in the search-based design of traditional software, existing works on search-based PLA design use NSGA-II, without evaluating the characteristics of this algorithm, such as the use of crossover operator. Considering this fact, this paper reports results from a comparative analysis of two algorithms, NSGA-II and PAES, to the PLA design problem. PAES was chosen because it implements a different evolution strategy that does not employ crossover. An experimental study was carried out with nine PLAs and results of the conducted study attest that NSGA-II performs better than PAES in the PLA design context.},
booktitle = {Proceedings of the 2014 IEEE 26th International Conference on Tools with Artificial Intelligence},
pages = {681–688},
numpages = {8},
keywords = {software product line, product line architecture design, multi-objective algorithms},
series = {ICTAI '14}
}

@article{10.1007/s10664-020-09856-1,
author = {Ros, Rasmus and Hammar, Mikael},
title = {Data-driven software design with Constraint Oriented Multi-variate Bandit Optimization (COMBO)},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09856-1},
doi = {10.1007/s10664-020-09856-1},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3841–3872},
numpages = {32},
keywords = {Combinatorial optimization, Multi-armed bandits, Machine learning, A/B testing, Continuous experimentation}
}

@article{10.1016/j.neucom.2019.11.104,
author = {Ren, Yazhou and Huang, Shudong and Zhao, Peng and Han, Minghao and Xu, Zenglin},
title = {Self-paced and auto-weighted multi-view clustering},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {383},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.104},
doi = {10.1016/j.neucom.2019.11.104},
journal = {Neurocomput.},
month = mar,
pages = {248–256},
numpages = {9},
keywords = {Soft weighting, Multi-view clustering, Self-paced learning}
}

@article{10.1007/s10462-018-9675-6,
author = {Vikatos, Pantelis and Gryllos, Prokopios and Makris, Christos},
title = {Marketing campaign targeting using bridge extraction in multiplex social network},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-018-9675-6},
doi = {10.1007/s10462-018-9675-6},
abstract = {In this paper, we introduce a methodology for improving the targeting of marketing campaigns using bridge prediction in communities based on the information of multilayer online social networks. The campaign strategy involves the identification of nodes with high brand loyalty and top-ranking nodes in terms of participation in bridges that will be involved in the evolution of the graph. Our approach is based on an efficient classification model combining topological characteristics of crawled social graphs with sentiment and linguistic traits of user-nodes, popularity in social media as well as meta path-based features of multilayer networks. To validate our approach we present a set of experimental results using a well-defined dataset from Twitter and Foursquare. Our methodology is useful to recommendation systems as well as to marketers who are interested to use social influence and run effective marketing campaigns.},
journal = {Artif. Intell. Rev.},
month = jan,
pages = {703–724},
numpages = {22},
keywords = {Sentiment analysis, Graph mining, Link prediction, Influence metric, Social marketing}
}

@inproceedings{10.1007/978-3-030-26061-3_6,
author = {Avci, Umut and Akkurt, Gamze and Unay, Devrim},
title = {A Pattern Mining Approach in Feature Extraction for Emotion Recognition from Speech},
year = {2019},
isbn = {978-3-030-26060-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26061-3_6},
doi = {10.1007/978-3-030-26061-3_6},
abstract = {We address the problem of recognizing emotions from speech using features derived from emotional patterns. Because much work in the field focuses on using low-level acoustic features, we explicitly study whether high-level features are useful for classifying emotions. For this purpose, we convert a continuous speech signal to a discretized signal and extract discriminative patterns that are capable of distinguishing distinct emotions from each other. Extracted patterns are then used to create a feature set to be fed into a classifier. Experimental results show that patterns alone are good predictors of emotions. When used to build a classifier, pattern features achieve accuracy gains up&nbsp;to 25% compared to state-of-the-art acoustic features.},
booktitle = {Speech and Computer: 21st International Conference, SPECOM 2019, Istanbul, Turkey, August 20–25, 2019, Proceedings},
pages = {54–63},
numpages = {10},
keywords = {Feature extraction, Pattern mining, Speech processing, Emotion recognition},
location = {Istanbul, Turkey}
}

@inproceedings{10.1145/3302333.3302350,
author = {Garc\'{\i}a, Sergio and Str\"{u}ber, Daniel and Brugali, Davide and Di Fava, Alessandro and Schillinger, Philipp and Pelliccione, Patrizio and Berger, Thorsten},
title = {Variability Modeling of Service Robots: Experiences and Challenges},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302350},
doi = {10.1145/3302333.3302350},
abstract = {Sensing, planning, controlling, and reasoning, are human-like capabilities that can be artificially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specifically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specific variability challenges in robotics software engineering and to inspire other researchers to advance this field.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {6},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3434780.3436640,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a Pe\~{n}alvo, Francisco Jos\'{e} and Theron, Roberto},
title = {Advances in the use of domain engineering to support feature identification and generation of information visualizations},
year = {2021},
isbn = {9781450388504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434780.3436640},
doi = {10.1145/3434780.3436640},
abstract = {Information visualization tools are widely used to better understand large and complex datasets. However, to make the most out of them, it is necessary to rely on proper designs that consider not only the data to be displayed, but also the audience and the context. There are tools that already allow users to configure their displays without requiring programming skills, but this research project aims at exploring the automatic generation of information visualizations and dashboards in order to avoid the configuration process, and select the most suitable features of these tools taking into account their contexts. To address this problem, a domain engineering, and machine learning approach is proposed.},
booktitle = {Eighth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {1053–1056},
numpages = {4},
keywords = {Meta-modeling, Machine Learning, Information Dashboards, High-level requirements, Domain engineering, Automatic generation},
location = {Salamanca, Spain},
series = {TEEM'20}
}

@article{10.1155/2012/512159,
author = {Mazzeo, Pier Luigi and Leo, Marco and Spagnolo, Paolo and Nitti, Massimiliano},
title = {Soccer ball detection by comparing different feature extraction methodologies},
year = {2012},
issue_date = {January 2012},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2012},
issn = {1687-7470},
url = {https://doi.org/10.1155/2012/512159},
doi = {10.1155/2012/512159},
abstract = {This paper presents a comparison of different feature extraction methods for automatically recognizing soccer ball patterns through a probabilistic analysis. It contributes to investigate different well-known feature extraction approaches applied in a soccer environment, in order tomeasure robustness accuracy and detection performances. This work, evaluating differentmethodologies, permits to select the one which achieves best performances in terms of detection rate and CPU processing time. The effectiveness of the differentmethodologies is demonstrated by a huge number of experiments on real ball examples under challenging conditions.},
journal = {Adv. in Artif. Intell.},
month = jan,
articleno = {6},
numpages = {1}
}

@article{10.1155/2021/5089236,
author = {Chen, Yu and Tang, Zhong and Ding, Baiyuan},
title = {Research on the Construction of Intelligent Community Emergency Service Platform Based on Convolutional Neural Network},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/5089236},
doi = {10.1155/2021/5089236},
abstract = {Aiming at the shortcomings of the existing community emergency service platform, such as single function, poor scalability, and strong subjectivity, an intelligent community emergency service platform based on convolutional neural network was constructed. Firstly, the requirements analysis of the emergency service platform was carried out, and the functional demand of the emergency service platform was analyzed from the aspects of community environment, safety, infrastructure, health management, emergency response, and so on. Secondly, through logistics network, big data, cloud computing, artificial intelligence, and all kinds of applications, the intelligent community emergency service platform was designed. Finally, a semantic matching emergency question answering system based on convolutional neural network was developed to provide key technical support for the emergency preparation stage of intelligent community. The results show that the intelligent community emergency service platform plays an important role in preventing community emergency events and taking active and effective measures to ensure the health and safety of community residents.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@inproceedings{10.1109/ESEM.2017.14,
author = {Falessi, Davide and Russo, Barbara and Mullen, Kathleen},
title = {What if i had no smells?},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.14},
doi = {10.1109/ESEM.2017.14},
abstract = {What would have happened if I did not have any code smell? This is an interesting question that no previous study, to the best of our knowledge, has tried to answer. In this paper, we present a method for implementing a what-if scenario analysis estimating the number of defective files in the absence of smells. Our industrial case study shows that 20% of the total defective files were likely avoidable by avoiding smells. Such estimation needs to be used with the due care though as it is based on a hypothetical history (i.e., zero number of smells and same process and product change characteristics). Specifically, the number of defective files could even increase for some types of smells. In addition, we note that in some circumstances, accepting code with smells might still be a good option for a company.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {78–84},
numpages = {7},
keywords = {technical debt, software estimation, machine learning, code smells},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1109/ASE.2013.6693103,
author = {Pohl, Richard and Stricker, Vanessa and Pohl, Klaus},
title = {Measuring the structural complexity of feature models},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693103},
doi = {10.1109/ASE.2013.6693103},
abstract = {The automated analysis of feature models (FM) is based on SAT, BDD, and CSP - known NP-complete problems. Therefore, the analysis could have an exponential worst-case execution time. However, for many practical relevant analysis cases, state-of-the-art (SOTA) analysis tools quite successfully master the problem of exponential worst-case execution time based on heuristics. So far, however, very little is known about the structure of FMs that cause the cases in which the execution time (hardness) for analyzing a given FM increases unpredictably for SOTA analysis tools. In this paper, we propose to use width measures from graph theory to characterize the structural complexity of FMs as a basis for an estimation of the hardness of analysis operations on FMs with SOTA analysis tools. We present an experiment that we use to analyze the reasonability of graph width measures as metric for the structural complexity of FMs and the hardness of FM analysis. Such a complexity metric can be used as a basis for a unified method to systematically improve SOTA analysis tools.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {454–464},
numpages = {11},
keywords = {software product line, performance measurement, feature model, automated analysis},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@article{10.1007/s11219-021-09550-5,
author = {Alkharabsheh, Khalid and Crespo, Yania and Fern\'{a}ndez-Delgado, Manuel and Viqueira, Jos\'{e} R. and Taboada, Jos\'{e} A.},
title = {Exploratory study of the impact of project domain and size category on the detection of the God class design smell},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09550-5},
doi = {10.1007/s11219-021-09550-5},
abstract = {Design smell detection has proven to be an efficient strategy to improve software quality and consequently decrease maintainability expenses. This work explores the influence of the&nbsp;information  about&nbsp;project context expressed as project domain and size category information, on the automatic detection of the god class design smell by machine learning techniques. A set of experiments using eight classifiers to detect god classes was conducted on a dataset containing 12, 587 classes from 24 Java projects. The results show that classifiers change their behavior when they are used on datasets that differ in these kinds of project information. The results show that god class design smell detection can be improved by feeding machine learning classifiers with this project context information.},
journal = {Software Quality Journal},
month = jun,
pages = {197–237},
numpages = {41},
keywords = {God class, Project context information, Software metrics, Machine learning, Design smell detection}
}

@article{10.1007/s10506-021-09285-5,
author = {Castellanos-Ardila, Julieth Patricia and Gallina, Barbara and Governatori, Guido},
title = {Compliance-aware engineering process plans: the case of space software engineering processes},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0924-8463},
url = {https://doi.org/10.1007/s10506-021-09285-5},
doi = {10.1007/s10506-021-09285-5},
abstract = {Safety-critical systems manufacturers have the duty of care, i.e., they should take correct steps while performing acts that could foreseeably harm others. Commonly, industry standards prescribe reasonable steps in their process requirements, which regulatory bodies trust. Manufacturers perform careful documentation of compliance with each requirement to show that they act under acceptable criteria. To facilitate this task, a safety-centered planning-time framework, called ACCEPT, has been proposed. Based on compliance-by-design, ACCEPT capabilities (i.e., processes and standards modeling, and automatic compliance checking) permit to design Compliance-aware Engineering Process Plans (CaEPP), which are able to show the planning-time allocation of standard demands, i.e., if the elements set down by the standard requirements are present at given points in the engineering process plan. In this paper, we perform a case study to understand if the ACCEPT produced models could support the planning of space software engineering processes. Space software is safety and mission-critical, and it is often the result of industrial cooperation. Such cooperation is coordinated through compliance with relevant standards. In the European context, ECSS-E-ST-40C is the de-facto standard for space software production. The planning of processes in compliance with project-specific ECSS-E-ST-40C applicable requirements is mandatory during contractual agreements. Our analysis is based on qualitative criteria targeting the effort dictated by task demands required to create a CaEPP for software development with ACCEPT. Initial observations show that the effort required to model compliance and processes artifacts is significant. However, such an effort pays off in the long term since models are, to some extend, reusable and flexible. The coverage level of the models is also analyzed based on design decisions. In our opinion, such a level is adequate since it responds to the information needs required by the ECSS-E-ST-40C framework.},
journal = {Artif. Intell. Law},
month = dec,
pages = {587–627},
numpages = {41},
keywords = {ECSS-E-ST-40C, Software process plan, Process compliance checking}
}

@article{10.1016/j.neucom.2019.10.018,
author = {Ding, Deqiong and Yang, Xiaogao and Xia, Fei and Ma, Tiefeng and Liu, Haiyun and Tang, Chang},
title = {Unsupervised feature selection via adaptive hypergraph regularized latent representation learning},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {378},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.10.018},
doi = {10.1016/j.neucom.2019.10.018},
journal = {Neurocomput.},
month = feb,
pages = {79–97},
numpages = {19},
keywords = {99-00, 00-01, Local structure preservation, Latent representation learning, Hypergraph learning, Unsupervised feature selection}
}

@inproceedings{10.1007/978-3-030-00308-1_3,
author = {Hess, Timm and Mundt, Martin and Weis, Tobias and Ramesh, Visvanathan},
title = {Large-Scale Stochastic Scene Generation and Semantic Annotation for Deep Convolutional Neural Network Training in the RoboCup SPL},
year = {2017},
isbn = {978-3-030-00307-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-00308-1_3},
doi = {10.1007/978-3-030-00308-1_3},
abstract = {Object detection and classification are essential tasks for any robotics scenario, where data-driven approaches, specifically deep learning techniques, have been widely adopted in recent years. However, in the context of the RoboCup standard platform league these methods have not yet gained comparable popularity in large part due to the lack of (publicly) available large enough data sets that involve a tedious gathering and error-prone manual annotation process. We propose a framework for stochastic scene generation, rendering and automatic creation of semantically annotated ground truth masks. Used as training data in conjunction with deep convolutional neural networks we demonstrate compelling classification accuracy on real-world data in a multi-class setting. An evaluation on multiple neural network architectures with varying depth and representational capacity, corresponding run-times on current NAO-H25 hardware, and required sampled training data is provided.},
booktitle = {RoboCup 2017: Robot World Cup XXI},
pages = {33–44},
numpages = {12},
keywords = {Static Head Pose, Robotics, Standard Platform League (SPL), RoboCup SPL, Deep Convolutional Neural Networks},
location = {Nagoya, Japan}
}

@inproceedings{10.1145/3127479.3131621,
author = {Traub, Jonas and Bre\ss{}, Sebastian and Rabl, Tilmann and Katsifodimos, Asterios and Markl, Volker},
title = {Optimized on-demand data streaming from sensor nodes},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3131621},
doi = {10.1145/3127479.3131621},
abstract = {Real-time sensor data enables diverse applications such as smart metering, traffic monitoring, and sport analysis. In the Internet of Things, billions of sensor nodes form a sensor cloud and offer data streams to analysis systems. However, it is impossible to transfer all available data with maximal frequencies to all applications. Therefore, we need to tailor data streams to the demand of applications.We contribute a technique that optimizes communication costs while maintaining the desired accuracy. Our technique schedules reads across huge amounts of sensors based on the data-demands of a huge amount of concurrent queries. We introduce user-defined sampling functions that define the data-demand of queries and facilitate various adaptive sampling techniques, which decrease the amount of transferred data. Moreover, we share sensor reads and data transfers among queries. Our experiments with real-world data show that our approach saves up to 87% in data transmissions.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {586–597},
numpages = {12},
keywords = {user-defined sampling, sensor sharing, sensor data, real-time analysis, oversampling, on-demand streaming, adaptive sampling},
location = {Santa Clara, California},
series = {SoCC '17}
}

@article{10.1016/j.cviu.2021.103255,
author = {Landi, Federico and Baraldi, Lorenzo and Cornia, Marcella and Corsini, Massimiliano and Cucchiara, Rita},
title = {Multimodal attention networks for low-level vision-and-language navigation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {210},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2021.103255},
doi = {10.1016/j.cviu.2021.103255},
journal = {Comput. Vis. Image Underst.},
month = sep,
numpages = {9},
keywords = {Multi-modal attention, Embodied AI, Vision-and-language navigation, 68T45, 68T40, 68T01}
}

@inproceedings{10.1145/2970276.2970336,
author = {Li, Yi and Zhu, Chenguang and Rubin, Julia and Chechik, Marsha},
title = {Precise semantic history slicing through dynamic delta refinement},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970336},
doi = {10.1145/2970276.2970336},
abstract = {Semantic history slicing solves the problem of extracting changes related to a particular high-level functionality from the software version histories. State-of-the-art techniques combine static program analysis and dynamic execution tracing to infer an over-approximated set of changes that can preserve the functional behaviors captured by a test suite. However, due to the conservative nature of such techniques, the sliced histories may contain irrelevant changes. In this paper, we propose a divide-and-conquer-style partitioning approach enhanced by dynamic delta refinement to produce minimal semantic history slices. We utilize deltas in dynamic invariants generated from successive test executions to learn significance of changes with respect to the target functionality. Empirical results indicate that these measurements accurately rank changes according to their relevance to the desired test behaviors and thus partition history slices in an efficient and effective manner.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {495–506},
numpages = {12},
keywords = {software configuration management, program analysis, dynamic invariants, Semantic history slicing},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1109/PLEASE.2015.15,
author = {Buchmann, Thomas and Baumgartl, Johannes and Henrich, Dominik and Westfechtel, Bernhard},
title = {Robots and their Variability -- A Societal Challenge and a Potential Solution},
year = {2015},
isbn = {9781467370615},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PLEASE.2015.15},
doi = {10.1109/PLEASE.2015.15},
abstract = {A robot is essentially a real-time, distributed embedded system operating in a physical environment. Often, control and communication paths within the system are tightly coupled to the actual hardware configuration of the robot. Furthermore, the domain contains a high amount of variability on different levels, ranging from hardware, over software to the environment in which the robot is operated. Today, special robots are used in households to perform monotonous and recurring tasks like vacuuming or mowing the lawn. In the future there may be robots that can be configured and programmed for more complicated tasks, like washing dishes or cleaning up or to assist elderly people. Nowadays, programming a robot is a highly complex and challenging task, which can be carried out only by programmers with dedicated background in robotics. Societal acceptance of robots can only be achieved, if they are easy to program. In this paper we present our approach to provide customized programming environments enabling programmers without background knowledge in robotics to specify robot programs. Our solution was realized using product line techniques.},
booktitle = {Proceedings of the 2015 IEEE/ACM 5th International Workshop on Product LinE Approaches in Software Engineering},
pages = {27–30},
numpages = {4},
keywords = {software product line, robot, model-driven development, dsl, code generation},
series = {PLEASE '15}
}

@inproceedings{10.5555/3540261.3542579,
author = {Ostapenko, Oleksiy and Rodr\'{\i}guez, Pau and Caccia, Massimo and Charlin, Laurent},
title = {Continual learning via local module composition},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modularity is a compelling solution to continual learning (CL), the problem of modeling sequences of related tasks. Learning and then composing modules to solve different tasks provides an abstraction to address the principal challenges of CL including catastrophic forgetting, backward and forward transfer across tasks, and sub-linear model growth. We introduce local module composition (LMC), an approach to modular CL where each module is provided a local structural component that estimates a module's relevance to the input. Dynamic module composition is performed layer-wise based on local relevance scores. We demonstrate that agnosticity to task identities (IDs) arises from (local) structural learning that is module-specific as opposed to the task- and/or model-specific as in previous works, making LMC applicable to more CL settings compared to previous works. In addition, LMC also tracks statistics about the input distribution and adds new modules when outlier samples are detected. In the first set of experiments, LMC performs favorably compared to existing methods on the recent Continual Transfer-learning Benchmark without requiring task identities. In another study, we show that the locality of structural learning allows LMC to interpolate to related but unseen tasks (OOD), as well as to compose modular networks trained independently on different task sequences into a third modular network without any fine-tuning. Finally, in search for limitations of LMC we study it on more challenging sequences of 30 and 100 tasks, demonstrating that local module selection becomes much more challenging in presence of a large number of candidate modules. In this setting best performing LMC spawns much fewer modules compared to an oracle based baseline, however it reaches a lower overall accuracy.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2318},
numpages = {15},
series = {NIPS '21}
}

@article{10.5555/1090725.1644608,
author = {Gerevini, Alfonso},
title = {Incremental qualitative temporal reasoning: Algorithms for the Point Algebra and the ORD-Horn class},
year = {2005},
issue_date = {August 2005},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {166},
number = {1–2},
issn = {0004-3702},
abstract = {In many applications of temporal reasoning we are interested in processing temporal information incrementally. In particular, given a set of temporal constraints (a temporal CSP) and a new constraint, we want to maintain certain properties of the extended temporal CSP (e.g., a solution), rather than recomputing them from scratch. The Point Algebra (PA) and the Interval Algebra (IA) are two well-known frameworks for qualitative temporal reasoning. The reasoning algorithms for PA and the tractable fragments of IA, such as Nebel and Burckert's maximal tractable class of relations (ORD-Horn), have originally been designed for ''static'' reasoning. In this paper, we study the incremental version of the fundamental reasoning problems in the context of these tractable classes. We propose a collection of new polynomial algorithms that can amortize their complexity when processing a sequence of input constraints to incrementally decide satisfiability, to maintain a solution, or to update the minimal representation of the CSP. Our incremental algorithms improve the total time complexity of using existing static techniques by a factor of O(n) or O(n^2), where n is the number of the variables involved by the temporal CSP. An experimental analysis focused on constraints over PA confirms the computational advantage of our incremental approach.},
journal = {Artif. Intell.},
month = aug,
pages = {37–80},
numpages = {44},
keywords = {Tractable reasoning, Qualitative temporal reasoning, Point calculus, Point algebra, Interval calculus, Interval algebra, Incremental reasoning, Constraint-based reasoning, Constraint satisfaction}
}

@article{10.1016/j.eswa.2007.09.057,
author = {Wu, Chia-Wei and Tsai, Richard Tzong-Han and Lee, Cheng-Wei and Hsu, Wen-Lian},
title = {Web taxonomy integration with hierarchical shrinkage algorithm and fine-grained relations},
year = {2008},
issue_date = {November, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {35},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.09.057},
doi = {10.1016/j.eswa.2007.09.057},
abstract = {We address the problem of integrating web taxonomies from different real Internet applications. Integrating web taxonomies is to transfer instances from a source to target taxonomy. Unlike the conventional text categorization problem, in taxonomy integration, the source taxonomy contains extra information that can be used to improve the categorization. The major existing methods can be divided in two types: those that use neighboring categories to smooth the document term vector and those that consider the semantic relationship between corresponding categories of the target and source taxonomies to facilitate categorization. In contrast to the first type of approach, which only uses a flattened hierarchy for smoothing, we apply a hierarchy shrinkage algorithm to smooth child documents by their parents. We also discuss the effect of using different hierarchical levels for smoothing. To extend the second type of approach, we extract fine-grain semantic relationships, which consider the relationships between lower-level categories. In addition, we use the cosine similarity to measure the semantic relationships, which achieves better performance than existing methods. Finally, we integrate the existing approaches and the proposed methods into one machine learning model to find the best feature configuration. The results of experiments on real Internet data demonstrate that our system outperforms standard text classifiers by about 10%.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {2123–2131},
numpages = {9},
keywords = {Web taxonomy integration, Text categorization, Shrinkage algorithm}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {multi-objective optimisation, multi-criteria decision analysis, design-space exploration, decision modelling},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@article{10.1016/j.jss.2019.06.003,
author = {Capilla, Rafael and Fuentes, Lidia and Lochau, Malte},
title = {Software variability in dynamic environments},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.003},
doi = {10.1016/j.jss.2019.06.003},
journal = {J. Syst. Softw.},
month = oct,
pages = {62–64},
numpages = {3}
}

@inproceedings{10.1145/3377930.3389815,
author = {Binder, Martin and Moosbauer, Julia and Thomas, Janek and Bischl, Bernd},
title = {Multi-objective hyperparameter tuning and feature selection using filter ensembles},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3389815},
doi = {10.1145/3377930.3389815},
abstract = {Both feature selection and hyperparameter tuning are key tasks in machine learning. Hyperparameter tuning is often useful to increase model performance, while feature selection is undertaken to attain sparse models. Sparsity may yield better model interpretability and lower cost of data acquisition, data handling and model inference. While sparsity may have a beneficial or detrimental effect on predictive performance, a small drop in performance may be acceptable in return for a substantial gain in sparseness. We therefore treat feature selection as a multi-objective optimization task. We perform hyperparameter tuning and feature selection simultaneously because the choice of features of a model may influence what hyperparameters perform well.We present, benchmark, and compare two different approaches for multi-objective joint hyperparameter optimization and feature selection: The first uses multi-objective model-based optimization. The second is an evolutionary NSGA-II-based wrapper approach to feature selection which incorporates specialized sampling, mutation and recombination operators. Both methods make use of parameterized filter ensembles.While model-based optimization needs fewer objective evaluations to achieve good performance, it incurs computational overhead compared to the NSGA-II, so the preferred choice depends on the cost of evaluating a model on given data.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {471–479},
numpages = {9},
keywords = {multiobjective optimization, model-based optimization, hyperparameter optimization, feature selection, evolutionary algorithms},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.5555/3291291.3291298,
author = {Islam, Nayreet and Azim, Akramul},
title = {Assuring the runtime behavior of self-adaptive cyber-physical systems using feature modeling},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {A self-adaptive cyber-physical system (SACPS) can adjust its behavior and configurations at runtime in response to varying requirements obtained from the system and the environment. With the increasing use of the SACPS in different application domains, such variations are becoming more common. Users today expect the SACPS to guarantee its functional and timing behavior even in adverse environmental situations. However, uncertainties in the SACPS environment impose challenges on assuring the runtime behavior during system design.Software product line engineering (SPLE) is considered as a useful technique for handling varying requirements. In this paper, we present an approach for assuring the runtime behavior of the SACPS by applying an SPLE technique such as feature modeling. By representing the feature-based model at design time, we characterize the possible adaptation requirements to reusable configurations. The proposed approach aims to model two dynamic variability dimensions: 1) environment variability that describes the conditions under which the SACPS must adapt, and 2) structural variability, that defines the resulting architectural configurations. To validate our approach, the experimental analysis is performed using two case studies: 1) a traffic monitoring SACPS and 2) an automotive SACPS. We demonstrate that the proposed feature-based modeling approach can be used to achieve adaptivity which allows the SACPS to assure functional (defining execution of the correct set of adaptive tasks) and non-functional (defining execution of SACPS in the expected mode) correctness at runtime. The experimental results show that the feature-based SACPS demonstrates significant improvement in terms of self-configuration time, self-adaptation time and scalability with less probability of failure in different environmental situations.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {48–59},
numpages = {12},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@article{10.1016/j.procs.2019.12.210,
author = {Loiro, Carina and Castro, H\'{e}lio and \'{A}vila, Paulo and Cruz-Cunha, Maria Manuela and Putnik, Goran D. and Ferreira, Lu\'{\i}s},
title = {Agile Project Management: A Communicational Workflow Proposal},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {164},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.210},
doi = {10.1016/j.procs.2019.12.210},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {485–490},
numpages = {6},
keywords = {Communicational, Team, Agility, Agile Manufacturing, Agile Project Management}
}

@article{10.1016/j.neucom.2014.12.100,
author = {Garcia, Lu\'{\i}s P.F. and Carvalho, Andr\'{e} C.P.L.F. de and Lorena, Ana C.},
title = {Noise detection in the meta-learning level},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {176},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.12.100},
doi = {10.1016/j.neucom.2014.12.100},
abstract = {The presence of noise in real data sets can harm the predictive performance of machine learning algorithms. There are several noise filtering techniques whose goal is to improve the quality of the data in classification tasks. These techniques usually scan the data for noise identification in a preprocessing step. Nonetheless, this is a non-trivial task and some noisy data can remain unidentified, while safe data can also be removed. The bias of each filtering technique influences its performance on a particular data set. Therefore, there is no single technique that can be considered the best for all domains or data distribution and choosing a particular filter is not straightforward. Meta-learning has been largely used in the last years to support the recommendation of the most suitable machine learning algorithm(s) for a new data set. This paper presents a meta-learning recommendation system able to predict the expected performance of noise filters in noisy data identification tasks. For such, a meta-base is created, containing meta-features extracted from several corrupted data sets along with the performance of some noise filters when applied to these data sets. Next, regression models are induced from this meta-base to predict the expected performance of the investigated filters in the identification of noisy data. The experimental results show that meta-learning can provide a good recommendation of the most promising filters to be applied to new classification data sets.},
journal = {Neurocomput.},
month = feb,
pages = {14–25},
numpages = {12},
keywords = {Noise identification, Meta-learning, Complexity measures, Characterization measures}
}

@article{10.1016/j.compeleceng.2017.11.002,
author = {AbuZeina, Dia and Al-Anzi, Fawaz S.},
title = {Employing fisher discriminant analysis for Arabic text classification},
year = {2018},
issue_date = {February 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.11.002},
doi = {10.1016/j.compeleceng.2017.11.002},
abstract = {Linear discriminant analysis (LDA) is proposed for Arabic text classification.LDA employs less dimensions, which is helpful for sizable textual feature vectors.Despite that LDA is semantic loss feature reduction method, it shows useful results. Fisher's discriminant analysis; also called linear discriminant analysis (LDA), is a popular dimensionality reduction technique that is widely used for features extraction. LDA aims at finding an optimal linear transformation based on maximizing a class separability. Even though LDA shows useful results in various pattern recognition problems, such as face recognition, less attention has been devoted to employing this technique in Arabic information retrieval tasks. In particular, the sizable feature vectors in textual data enforces to implement dimensionality reduction techniques such as LDA. In this paper, we empirically investigated an LDA based method for Arabic text classification. We used a corpus that contains 2,000 documents belonging to five categories. The experimental results showed that the performance of semantic loss LDA based method was almost the same as the semantic rich singular value decomposition (SVD), and that is indication that LDA is a promising method for text mining applications. Display Omitted},
journal = {Comput. Electr. Eng.},
month = feb,
pages = {474–486},
numpages = {13},
keywords = {Text, Linear discriminant analysis, Fisher, Eigenvectors, Classification, Arabic}
}

@article{10.1016/j.imavis.2016.06.005,
author = {Leng, Mengjun and Moutafis, Panagiotis and Kakadiaris, Ioannis A.},
title = {Joint prototype and metric learning for image set classification: Application to video face identification},
year = {2017},
issue_date = {Feb 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {58},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2016.06.005},
doi = {10.1016/j.imavis.2016.06.005},
journal = {Image Vision Comput.},
month = feb,
pages = {204–213},
numpages = {10},
keywords = {Video face recognition, Prototype learning, Metric learning, Image set classification}
}

@inproceedings{10.1145/3387940.3392206,
author = {Capiluppi, Andrea and Ajienka, Nemitari},
title = {Towards A Dependency-Driven Taxonomy of Software Types},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392206},
doi = {10.1145/3387940.3392206},
abstract = {Context: The evidence on software health and ecosystems could be improved if there was a systematic way to identify the types of software for which empirical evidence applies. Results and guidelines on software health are unlikely to be globally applicable: the context and the domain where the evidence has been tested are more likely to influence the results on software maintenance and health.Objective: The objectives of this paper are (i) to discuss the implications of adopting a specific taxonomy of software types, and (ii) to define, where possible, dependencies or similarities between parts of the taxonomy.Method: We discuss bottom-up and top-down taxonomies, and we show how different taxonomies fare against each other. We also propose two case studies, based on software projects divided in categories and sub-categories.Results: We show that one taxonomy does not consistently represent another taxonomy's categories. We also show that it is possible to establish directional dependencies (e.g., 'larger than') between attributes of different categories, and sub-categories.Conclusion: This paper establishes the need of directional-driven dependencies between categories of software types, that have an immediate effect on their maintenance and their relative software health.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {687–694},
numpages = {8},
keywords = {OO (object-oriented), Machine Learning, Latent Dirichlet Allocation, FOSS, Expert Opinions, Application Domains},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1177/0165551515591724,
author = {Onan, Aytu\u{g}},
title = {Classifier and feature set ensembles for web page classification},
year = {2016},
issue_date = {4 2016},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {42},
number = {2},
issn = {0165-5515},
url = {https://doi.org/10.1177/0165551515591724},
doi = {10.1177/0165551515591724},
abstract = {Web page classification is an important research direction on web mining. The abundant amount of data available on the web makes it essential to develop efficient and robust models for web mining tasks. Web page classification is the process of assigning a web page to a particular predefined category based on labelled data. It serves for several other web mining tasks, such as focused web crawling, web link analysis and contextual advertising. Machine learning and data mining methods have been successfully applied for several web mining tasks, including web page classification. Multiple classifier systems are a promising research direction in machine learning, which aims to combine several classifiers by differentiating base classifiers and/or dataset distributions so that more robust classification models can be built. This paper presents a comparative analysis of four different feature selections correlation, consistency, information gain and chi-square-based feature selection and four different ensemble learning methods Boosting, Bagging, Dagging and Random Subspace based on four different base learners naive Bayes, K-nearest neighbour algorithm, C4.5 algorithm and FURIA algorithm. The article examines the predictive performance of ensemble methods for web page classification. The experimental results indicate that feature selection and ensemble learning can enhance the predictive performance of classifiers in web page classification. For the DMOZ-50 dataset, the highest average predictive performance 88.1% is obtained with the combination of consistency-based feature selection with AdaBoost and naive Bayes algorithms, which is a promising result for web page classification. Experimental results indicate that Bagging and Random Subspace ensemble methods and correlation-based and consistency-based feature selection methods obtain better results in terms of accuracy rates.},
journal = {J. Inf. Sci.},
month = apr,
pages = {150–165},
numpages = {16},
keywords = {web page classification, multiple classifiers, Ensemble learning}
}

@article{10.1016/j.cose.2019.101578,
author = {Durkota, Karel and Lis\'{y}, Viliam and Bo\v{s}ansk\'{y}, Branislav and Kiekintveld, Christopher and P\v{e}chou\v{c}ek, Michal},
title = {Hardening networks against strategic attackers using attack graph games},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {87},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2019.101578},
doi = {10.1016/j.cose.2019.101578},
journal = {Comput. Secur.},
month = nov,
numpages = {25},
keywords = {Heuristic algorithms, Stackelberg equilibrium, Network configuration management, Honeypot, Network security, Game theory}
}

@article{10.1155/2020/8845932,
author = {Xu, Hui and Cheng, Hongju},
title = {Distinguishing Hand Drawing Style Based on Multilevel Analytics Framework},
year = {2020},
issue_date = {2020},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2020},
issn = {1530-8669},
url = {https://doi.org/10.1155/2020/8845932},
doi = {10.1155/2020/8845932},
abstract = {Hand drawing is an indispensable professional skill in the fields of environmental design, industrial design, architectural engineering, civil engineering, and other engineering design education. Students usually imitate masterpieces to practice basic skills, which is an important link for a beginner. A system for digital management requires a function for an automatic recommendation task of different brushwork skill expressions. Thus, the classification method for brushwork is to combine hand-crafted features generated by DCNN and then use the final features for input to a tree structure classification scheme. The method improvement of the other deep learning models has effectiveness in distinguishing art ontology attributes.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {10}
}

@inproceedings{10.1145/3430984.3431022,
author = {Virk, Jitender Singh and Bathula, Deepti R.},
title = {Domain-Specific, Semi-Supervised Transfer Learning for Medical Imaging},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431022},
doi = {10.1145/3430984.3431022},
abstract = {Limited availability of annotated medical imaging data poses a challenge for deep learning algorithms. Although transfer learning minimizes this hurdle in general, knowledge transfer across disparate domains is shown to be less effective. On the other hand, smaller architectures were found to be more compelling in learning better features. Consequently, we propose a lightweight architecture that uses mixed asymmetric kernels (MAKNet) to reduce the number of parameters significantly. Additionally, we train the proposed architecture using semi-supervised learning to provide pseudo-labels for a large medical dataset to assist with transfer learning. The proposed MAKNet provides better classification performance with fewer parameters than popular architectures. Experimental results also highlight the importance of domain-specific knowledge for effective transfer learning. Additionally, we interrogate the proposed network with integrated gradients and perturbation methods to establish the superior quality of the learned features.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {145–153},
numpages = {9},
keywords = {transfer learning, pseudo-labelling, mixed-kernels neural networks, integrated gradients, image perturbations, domain-specific, Semi-supervised learning, Neural networks, CT scans},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@article{10.3233/HIS-2011-0140,
author = {Ahumada, Hern\'{a}n and Grinblat, Guillermo L. and Uzal, Lucas C. and Ceccatto, Alejandro and Granitto, Pablo M.},
title = {Evaluation of a new hybrid algorithm for highly imbalanced classification problems},
year = {2011},
issue_date = {October 2011},
publisher = {IOS Press},
address = {NLD},
volume = {8},
number = {4},
issn = {1448-5869},
url = {https://doi.org/10.3233/HIS-2011-0140},
doi = {10.3233/HIS-2011-0140},
abstract = {Many times in classification problems, particularly in critical real world applications, one of the classes has much less samples than the others usually known as the class imbalance problem. In this work we discuss and evaluate the use of the REPMAC algorithm to solve imbalanced problems. Using a clustering method, REPMAC recursively splits the majority class in several subsets, creating a decision tree, until the resulting sub-problems are balanced or easy to solve. We use two diverse clustering methods and three different classifiers coupled with REPMAC to evaluate the new method on several benchmark datasets spanning a wide range of number of features, samples and imbalance degree. We also apply our method to a real world problem, the identification of weed seeds. We find that the good performance of REPMAC is almost independent of the classifier or the clustering method coupled to it, which suggests that its success is mostly related to the use of an appropriate strategy to cope with imbalanced problems.},
journal = {Int. J. Hybrid Intell. Syst.},
month = oct,
pages = {199–211},
numpages = {13},
keywords = {Hybrid Systems, Clustering, Classification, Class Imbalance}
}

@article{10.1145/2000791.2000794,
author = {Anvik, John and Murphy, Gail C.},
title = {Reducing the effort of bug report triage: Recommenders for development-oriented decisions},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000791.2000794},
doi = {10.1145/2000791.2000794},
abstract = {A key collaborative hub for many software development projects is the bug report repository. Although its use can improve the software development process in a number of ways, reports added to the repository need to be triaged. A triager determines if a report is meaningful. Meaningful reports are then organized for integration into the project's development process.To assist triagers with their work, this article presents a machine learning approach to create recommenders that assist with a variety of decisions aimed at streamlining the development process. The recommenders created with this approach are accurate; for instance, recommenders for which developer to assign a report that we have created using this approach have a precision between 70% and 98% over five open source projects. As the configuration of a recommender for a particular project can require substantial effort and be time consuming, we also present an approach to assist the configuration of such recommenders that significantly lowers the cost of putting a recommender in place for a project. We show that recommenders for which developer should fix a bug can be quickly configured with this approach and that the configured recommenders are within 15% precision of hand-tuned developer recommenders.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {10},
numpages = {35},
keywords = {task assignment, recommendation, machine learning, configuration assistance, Bug report triage}
}

@inproceedings{10.1145/3139923.3139928,
author = {Moriano, Pablo and Pendleton, Jared and Rich, Steven and Camp, L Jean},
title = {Insider Threat Event Detection in User-System Interactions},
year = {2017},
isbn = {9781450351775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139923.3139928},
doi = {10.1145/3139923.3139928},
abstract = {Detection of insider threats relies on monitoring individuals and their interactions with organizational resources. Identification of anomalous insiders typically relies on supervised learning models that use labeled data. However, such labeled data is not easily obtainable. The labeled data that does exist is also limited by current insider threat detection methods and undetected insiders would not be included. These models also inherently assume that the insider threat is not rapidly evolving between model generation and use of the model in detection. Yet there is a large body of research that illustrates that the insider threat changes significantly after some types of precipitating events, such as layoffs, significant restructuring, and plant or facility closure. To capture this temporal evolution of user-system interactions, we use an unsupervised learning framework to evaluate whether potential insider threat events are triggered following precipitating events. The analysis leverages a bipartite graph of user and system interactions. The approach shows a clear correlation between precipitating events and the number of apparent anomalies. The results of our empirical analysis show a clear shift in behaviors after events which have previously been shown to increase insider activity, specifically precipitating events. We argue that this metadata about the level of insider threat behaviors validates the potential of the approach. We apply our method to a dataset that comprises interactions between engineers and software components in an enterprise version control system spanning more than 22 years. We use this unlabeled dataset and automatically detect statistically significant events. We show that there is statistically significant evidence that a subset of users diversify their committing behavior after precipitating events have been announced. Although these findings do not constitute detection of insider threat events per se, they do identify patterns of potentially malicious high-risk insider behavior. They reinforce the idea that insider operations can be motivated by the insiders' environment. Our proposed framework outperforms algorithms based on naive random approaches and algorithms using volume dependent statistics. This graph mining technique has potential for early detection of insider threat behavior in user-system interactions independent of the volume of interactions. The proposed method also enables organizations without a corpus of identified insider threats to train its own anomaly detection system.},
booktitle = {Proceedings of the 2017 International Workshop on Managing Insider Security Threats},
pages = {1–12},
numpages = {12},
keywords = {insider threat, ibm rational clearcase, graph mining, community structure, bipartite graph, anomaly detection},
location = {Dallas, Texas, USA},
series = {MIST '17}
}

@article{10.1007/s11063-020-10213-y,
author = {Guo, Shikai and Zhang, Xinyi and Yang, Xi and Chen, Rong and Guo, Chen and Li, Hui and Li, Tingting},
title = {Developer Activity Motivated Bug Triaging: Via Convolutional Neural Network},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10213-y},
doi = {10.1007/s11063-020-10213-y},
abstract = {As bugs become prevalent in software development, bug triaging has become one of the most important activities in software maintenance. To decrease the time cost in manual work, text classification techniques have been applied in automatic bug triaging. In this paper, we present a new automatic bug triaging approach which is based on convolution neural network (CNN) and developer activities. Firstly, we implement the word vector representation of the text features in bug report by using Word2vec. Then, we combine CNN with batch normalization, pooling and full connection approach to learn from the word vector representation of bug report with known fixers. In addition, we also study the recent activities of the developers which can effectively distinguish similar bug reports and get a more suitable developer recommendation list. We empirically investigate the accuracy of automatic bug triaging on three large open source projects, namely Eclipse, Mozilla and NetBeans. The experimental results show that our approach can effectively improve the performance of automatic bug triaging.},
journal = {Neural Process. Lett.},
month = jun,
pages = {2589–2606},
numpages = {18},
keywords = {Deep learning, Convolution neural network, Mining software repositories, Bug triage}
}

@inproceedings{10.1145/3450268.3453517,
author = {Kang, Zhuangwei and Barve, Yogesh D. and Bao, Shunxing and Dubey, Abhishek and Gokhale, Aniruddha},
title = {Configuration Tuning for Distributed IoT Message Systems Using Deep Reinforcement Learning: Poster Abstract},
year = {2021},
isbn = {9781450383547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450268.3453517},
doi = {10.1145/3450268.3453517},
abstract = {Distributed messaging systems (DMSs) are often equipped with a large number of configurable parameters that enable users to define application run-time behaviors and information dissemination rules. However, the resulting high-dimensional configuration space makes it difficult for users to determine the best configuration that can maximize application QoS under a variety of operational conditions. This poster introduces a novel, automatic knob tuning framework called DMSConfig. DMSConfig explores the configuration space by interacting with a data-driven environment prediction model(a DMS simulator), which eliminates the prohibitive cost of conducting online interactions with the production environment. DMSConfig employs the deep deterministic policy gradient (DDPG) method and a custom reward mechanism to learn and make configuration decisions based on predicted DMS states and performance. Our initial experimental results, conducted on a single-broker Kafka cluster, show that DMSConfig significantly outperforms the default configuration and has better adaptability to CPU and bandwidth-limited environments. We also confirm that DMSConfig produces fewer violations of latency constraints than three prevalent parameter tuning tools.},
booktitle = {Proceedings of the International Conference on Internet-of-Things Design and Implementation},
pages = {273–274},
numpages = {2},
keywords = {System Configuration, Publish/Subscribe Middleware, Policy-based RL Algorithm},
location = {Charlottesvle, VA, USA},
series = {IoTDI '21}
}

@article{10.1016/j.is.2012.11.010,
author = {Gr\"{o}Ner, Gerd and Bo\v{s}Kovi\'{c}, Marko and Silva Parreiras, Fernando and Ga\v{s}Evi\'{c}, Dragan},
title = {Modeling and validation of business process families},
year = {2013},
issue_date = {July, 2013},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {38},
number = {5},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2012.11.010},
doi = {10.1016/j.is.2012.11.010},
abstract = {Process modeling is an expensive task that needs to encompass requirements of different stakeholders, assure compliance with different standards, and enable the flexible adaptivity to newly emerging requirements in today's dynamic global market. Identifying reusability of process models is a promising direction towards reducing the costs of process modeling. Recent research has offered several solutions. Such solutions promote effective and formally sound methods for variability modeling and configuration management. However, ensuring behavioral validity of reused process models with respect to the original process models (often referred to as reference process models) is still an open research challenge. To address this challenge, in this paper, we propose the notion of business process families by building upon the well-known software engineering discipline-software product line engineering. Business process families comprise (i) a variability modeling perspective, (ii) a process model template (or reference model), and (iii) mappings between (i) and (ii). For business process families, we propose a correct validation algorithm ensuring that each member of a business process family adheres to the core intended behavior that is specified in the process model template. The proposed validation approach is based on the use of Description Logics, variability is represented by using the well-known Feature Models and behavior of process models is considered in terms of control flow patterns. The paper also reports on the experience gained in two external trial cases and results obtained by measuring the tractability of the implementation of the proposed validation approach.},
journal = {Inf. Syst.},
month = jul,
pages = {709–726},
numpages = {18},
keywords = {Validation, Process model variability, Process model configuration, Control flow relations, Business process families}
}

@inproceedings{10.1007/978-3-030-82017-6_11,
author = {H\"{o}hn, Sviatlana and Faradouris, Niko},
title = {What Does It Cost to Deploy an XAI System: A Case Study in Legacy Systems},
year = {2021},
isbn = {978-3-030-82016-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82017-6_11},
doi = {10.1007/978-3-030-82017-6_11},
abstract = {Enterprise Resource Planning (ERP) software is used by businesses and extended via customisation. Automated custom code analysis and migration is a critical issue at ERP release upgrade times. Despite research advances, automated code analysis and transformation require a huge amount of manual work related to parser adaptation, rule extension and post-processing. These operations become unmanageable if the frequency of updates increases from yearly to monthly intervals. This article describes how the process of custom code analysis to custom code transformation can be automated in an explainable way. We develop an aggregate taxonomy for explainability and analyse the requirements based on roles. We explain in which steps on the new code migration process machine learning is used. Further, we analyse additional effort needed to make the new way of code migration explainable to different stakeholders.},
booktitle = {Explainable and Transparent AI and Multi-Agent Systems: Third International Workshop, EXTRAAMAS 2021, Virtual Event, May 3–7, 2021, Revised Selected Papers},
pages = {173–186},
numpages = {14},
keywords = {Explainability taxonomy, Multi-modal conversational interfaces, Explainable automated source-code transformation}
}

@article{10.1007/s11390-021-0235-1,
author = {Zhang, Jing-Xuan and Tao, Chuan-Qi and Huang, Zhi-Qiu and Chen, Xin},
title = {Discovering API Directives from API Specifications with Text Classification},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {36},
number = {4},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-021-0235-1},
doi = {10.1007/s11390-021-0235-1},
abstract = {Application programming interface (API) libraries are extensively used by developers. To correctly program with APIs and avoid bugs, developers shall pay attention to API directives, which illustrate the constraints of APIs. Unfortunately, API directives usually have diverse morphologies, making it time-consuming and error-prone for developers to discover all the relevant API directives. In this paper, we propose an approach leveraging text classification to discover API directives from API specifications. Specifically, given a set of training sentences in API specifications, our approach first characterizes each sentence by three groups of features. Then, to deal with the unequal distribution between API directives and non-directives, our approach employs an under-sampling strategy to split the imbalanced training set into several subsets and trains several classifiers. Given a new sentence in an API specification, our approach synthesizes the trained classifiers to predict whether it is an API directive. We have evaluated our approach over a publicly available annotated API directive corpus. The experimental results reveal that our approach achieves an F-measure value of up to 82.08%. In addition, our approach statistically outperforms the state-of-the-art approach by up to 29.67% in terms of F-measure.},
journal = {J. Comput. Sci. Technol.},
month = jul,
pages = {922–943},
numpages = {22},
keywords = {text classification, imbalanced learning, API specification, Application programming interface (API) directive}
}

@inproceedings{10.1007/978-3-642-33666-9_46,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel and Walawege, Suneth},
title = {A product line modeling and configuration methodology to support model-based testing: an industrial case study},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_46},
doi = {10.1007/978-3-642-33666-9_46},
abstract = {Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behavioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {726–742},
numpages = {17},
keywords = {product line engineering, model-based testing, behavioral variability, aspect-oriented modeling, UML state machine},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1145/3338906.3342811,
author = {Atlee, Joanne M.},
title = {Living with feature interactions (keynote)},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342811},
doi = {10.1145/3338906.3342811},
abstract = {Feature-oriented software development enables rapid software creation and evolution, through incremental and parallel feature development or through product line engineering. However, in practice, features are often not separate concerns. They behave differently in the presence of other features, and they sometimes interfere with each other in surprising ways.  This talk will explore challenges in feature interactions and their resolutions. Resolution strategies can tackle large classes of interactions, but are imperfect and incomplete, leading to research opportunities in software architecture, composition semantics, and verification.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1},
numpages = {1},
keywords = {Software Correctness, Feature-Oriented Software Development, Feature Interactions},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/3540261.3541724,
author = {Weihs, Luca and Jain, Unnat and Liu, Iou-Jen and Salvador, Jordi and Lazebnik, Svetlana and Kembhavi, Aniruddha and Schwing, Alexander},
title = {Bridging the imitation gap by adaptive insubordination},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In practice, imitation learning is preferred over pure reinforcement learning whenever it is possible to design a teaching agent to provide expert supervision. However, we show that when the teaching agent makes decisions with access to privileged information that is unavailable to the student, this information is marginalized during imitation learning, resulting in an "imitation gap" and, potentially, poor results. Prior work bridges this gap via a progression from imitation learning to reinforcement learning. While often successful, gradual progression fails for tasks that require frequent switches between exploration and memorization. To better address these tasks and alleviate the imitation gap we propose 'Adaptive Insubordination' (ADVISOR). ADVISOR dynamically weights imitation and reward-based reinforcement learning losses during training, enabling on-the-fly switching between imitation and exploration. On a suite of challenging tasks set within gridworlds, multi-agent particle environments, and high-fidelity 3D simulators, we show that on-the-fly switching with ADVISOR outperforms pure imitation, pure reinforcement learning, as well as their sequential and parallel combinations.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1463},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-030-75775-5_24,
author = {Amendola, Giovanni and Berei, Tobias and Ricca, Francesco},
title = {Testing in ASP: Revisited Language and Programming Environment},
year = {2021},
isbn = {978-3-030-75774-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-75775-5_24},
doi = {10.1007/978-3-030-75775-5_24},
abstract = {Unit testing frameworks are nowadays considered a best practice, foregone in almost all modern software development processes, to achieve rapid development of correct specifications. The first unit testing specification language for Answer Set Programming (ASP) was proposed in 2011 as a feature of the ASPIDE development environment. Later, a more portable unit testing language was included in the LANA annotation language. In this paper we propose a revisited unit testing specification language that allows one to inline tests within ASP program and an ASP-based test execution mechanism. Moreover, we present a programming environment supporting test driven development (TDD) of ASP programs with our language.},
booktitle = {Logics in Artificial Intelligence: 17th European Conference, JELIA 2021, Virtual Event, May 17–20, 2021, Proceedings},
pages = {362–376},
numpages = {15},
keywords = {Test-driven development, Unit testing, Answer Set Programming}
}

@article{10.5555/1577069.1577086,
author = {Li, Junning and Wang, Z. Jane},
title = {Controlling the False Discovery Rate of the Association/Causality Structure Learned with the PC Algorithm},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {In real world applications, graphical statistical models are not only a tool for operations such as classification or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level, and a heuristic modification of the method is able to control the FDR more accurately around the user-specified level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models.},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {475–514},
numpages = {40}
}

@article{10.1007/s11263-018-1112-4,
author = {Zhang, Dingwen and Han, Junwei and Zhao, Long and Meng, Deyu},
title = {Leveraging Prior-Knowledge for Weakly Supervised Object Detection Under a Collaborative Self-Paced Curriculum Learning Framework},
year = {2019},
issue_date = {April     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {127},
number = {4},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-018-1112-4},
doi = {10.1007/s11263-018-1112-4},
abstract = {Weakly supervised object detection is an interesting yet challenging research topic in computer vision community, which aims at learning object models to localize and detect the corresponding objects of interest only under the supervision of image-level annotation. For addressing this problem, this paper establishes a novel weakly supervised learning framework to leverage both the instance-level prior-knowledge and the image-level prior-knowledge based on a novel collaborative self-paced curriculum learning (C-SPCL) regime. Under the weak supervision, C-SPCL can leverage helpful prior-knowledge throughout the whole learning process and collaborate the instance-level confidence inference with the image-level confidence inference in a robust way. Comprehensive experiments on benchmark datasets demonstrate the superior capacity of the proposed C-SPCL regime and the proposed whole framework as compared with state-of-the-art methods along this research line.},
journal = {Int. J. Comput. Vision},
month = apr,
pages = {363–380},
numpages = {18},
keywords = {Weakly supervised learning, Self-paced larning, Object detection}
}

@article{10.1016/j.artmed.2016.05.004,
title = {An ensemble method for extracting adverse drug events from social media},
year = {2016},
issue_date = {June 2016},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {70},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2016.05.004},
doi = {10.1016/j.artmed.2016.05.004},
abstract = {We propose a relation extraction system to distinguish between adverse drug events (ADEs) and non-ADEs on social media.We develop a feature-based method, investigate the effectiveness of feature selection, and analyze the contributions of different features.We investigate whether kernel-based methods can effectively extract ADEs from social media.We propose several classifier ensembles to further enhance ADE extraction capabilities. ObjectiveBecause adverse drug events (ADEs) are a serious health problem and a leading cause of death, it is of vital importance to identify them correctly and in a timely manner. With the development of Web 2.0, social media has become a large data source for information on ADEs. The objective of this study is to develop a relation extraction system that uses natural language processing techniques to effectively distinguish between ADEs and non-ADEs in informal text on social media. Methods and materialsWe develop a feature-based approach that utilizes various lexical, syntactic, and semantic features. Information-gain-based feature selection is performed to address high-dimensional features. Then, we evaluate the effectiveness of four well-known kernel-based approaches (i.e., subset tree kernel, tree kernel, shortest dependency path kernel, and all-paths graph kernel) and several ensembles that are generated by adopting different combination methods (i.e., majority voting, weighted averaging, and stacked generalization). All of the approaches are tested using three data sets: two health-related discussion forums and one general social networking site (i.e., Twitter). ResultsWhen investigating the contribution of each feature subset, the feature-based approach attains the best area under the receiver operating characteristics curve (AUC) values, which are 78.6%, 72.2%, and 79.2% on the three data sets. When individual methods are used, we attain the best AUC values of 82.1%, 73.2%, and 77.0% using the subset tree kernel, shortest dependency path kernel, and feature-based approach on the three data sets, respectively. When using classifier ensembles, we achieve the best AUC values of 84.5%, 77.3%, and 84.5% on the three data sets, outperforming the baselines. ConclusionsOur experimental results indicate that ADE extraction from social media can benefit from feature selection. With respect to the effectiveness of different feature subsets, lexical features and semantic features can enhance the ADE extraction capability. Kernel-based approaches, which can stay away from the feature sparsity issue, are qualified to address the ADE extraction problem. Combining different individual classifiers using suitable combination methods can further enhance the ADE extraction effectiveness.},
journal = {Artif. Intell. Med.},
month = jun,
pages = {62–76},
numpages = {15}
}

@article{10.1016/j.engappai.2007.11.006,
author = {Pan, Nai-Hsin and Hsaio, Po-Wen and Chen, Kuei-Yen},
title = {A study of project scheduling optimization using Tabu Search algorithm},
year = {2008},
issue_date = {October, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {21},
number = {7},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2007.11.006},
doi = {10.1016/j.engappai.2007.11.006},
abstract = {The research will focus on investigating a more efficient alternative to solve resource-constrained project scheduling problems (RCPSP). Also, instead of using traditional Tabu Search (TS) algorithm and the other Artificial Intelligence (AI)-based heuristic approaches, this research develops an improved TS model by modifying the way of finding a starting solution instead of traditional TS algorithm to solve the problems described above. The model can effectively provide better results in reducing project duration for solving RCPSP compared to traditional TS-based search techniques and Artificial Intelligence (AI)-based approaches. The paper suggests the optimum parameters' setting values for the models the paper presented through sensitivity analysis to obtain better solution quality more easily. Also, the method the paper presented provides a good user interface linking with existing commercial software systems to help for the practitioner in the application of project management in the real world.},
journal = {Eng. Appl. Artif. Intell.},
month = oct,
pages = {1101–1112},
numpages = {12},
keywords = {Tabu Search, Project management, Optimization, Multi-resource project scheduling, Heuristic methods}
}

@inproceedings{10.1145/3375959.3375967,
author = {Wolde, Behailu Getachew and Boltana, Abiot Sinamo},
title = {Combinatorial Testing Approach for Cloud Mobility Service},
year = {2020},
isbn = {9781450372633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375959.3375967},
doi = {10.1145/3375959.3375967},
abstract = {Currently, software product becomes an essential component in running many stakeholders' activities. For instance, the industries mostly use cloud services to execute their important business functionality. However, by a few input's parameter interacting, this functionality can be pended. Such constraint poses challenging to cover various features of failure especially in ensuring cloud application. One way is to devise a strategy to cover input parameters' characteristics based on Combinatorial testing approach. This technique includes all possible combinations of test inputs for detecting bugs on the System Under Test (SUT). The paper explains the Combinatorial covering arrays to generate relatively exhaustive testing by modeling features of sample services using Feature IDE plugin in Eclipse IDE. This way, we build the input domain model to represent coverage of the existing mobility service running on NEMo Mobility cloud platform. Using this model, covering arrays is applied to generate t-way test cases by leveraging IPOg algorithm, which is implemented in a CiTLab. As a test case management, the JUnit testing framework uses test stubs to validate the test methods of generated test cases on the specified service (SUT).},
booktitle = {Proceedings of the 2019 2nd Artificial Intelligence and Cloud Computing Conference},
pages = {6–13},
numpages = {8},
keywords = {Software Testing, Feature Model, Combinatorial Testing, Cloud Mobility Service, CiTLAB},
location = {Kobe, Japan},
series = {AICCC '19}
}

@inproceedings{10.1145/3490035.3490262,
author = {Bansal, Rahul and Biswas, Soma},
title = {CT-DANN: co-teaching meets DANN for wild unsupervised domain adaptation},
year = {2021},
isbn = {9781450375962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490035.3490262},
doi = {10.1145/3490035.3490262},
abstract = {Unsupervised domain adaptation aims at leveraging supervision from an annotated source domain for performing tasks like classification/segmentation on an unsupervised target domain. However, a large enough related dataset with clean annotations may not be always available in real scenarios, since annotations are usually obtained from crowd sourcing, and thus are noisy. Here, we consider a more realistic and challenging setting, wild unsupervised domain adaptation (WUDA), where the source domain annotations can be noisy. Standard domain adaptation approaches which directly use these noisy source labels and the unlabeled targets for the domain adaptation task perform poorly, due to severe negative transfer from the noisy source domain. In this work, we propose a novel end-to-end framework, termed CT-DANN (Co-teaching meets DANN), which seamlessly integrates a state-of-the-art approach for handling noisy labels (Co-teaching) with a standard domain adaptation framework (DANN). CT-DANN effectively utilizes all the source samples after accounting for both their noisy labels as well as transferability with respect to the target domain. Extensive experiments on three benchmark datasets with different types and levels of noise and comparison with state-of-the-art WUDA approach justify the effectiveness of the proposed framework.},
booktitle = {Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {5},
numpages = {8},
keywords = {wild unsupervised domain adaptation, source data weighting, noisy source data, co-teaching},
location = {Jodhpur, India},
series = {ICVGIP '21}
}

@inproceedings{10.1145/3395035.3425182,
author = {Kaya, Heysem and Verkholyak, Oxana and Markitantov, Maxim and Karpov, Alexey},
title = {Combining Clustering and Functionals based Acoustic Feature Representations for Classification of Baby Sounds},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425182},
doi = {10.1145/3395035.3425182},
abstract = {This paper investigates different fusion strategies as well as provides insights on their effectiveness alongside standalone classifiers in the framework of paralinguistic analysis of infant vocalizations. The combinations of such systems as Support Vector Machines (SVM) and Extreme Learning Machines (ELM) based classifiers, as well as its weighted kernel version are explored, training systems on different acoustic feature representations and implementing weighted score-level fusion of the predictions. The proposed framework is tested on INTERSPEECH ComParE-2019 Baby Sounds corpus, which is a collection of Home Bank infant vocalization corpora annotated for five classes. Adhering to the challenge protocol, using a single test set submission we outperform the challenge baseline Unweighted Average Recall (UAR) score and achieve a comparable result to the state-of-the-art.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {509–513},
numpages = {5},
keywords = {support vector machines, information fusion, extreme learning machines, computational paralinguistics, baby sounds classification},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@inproceedings{10.1007/978-3-030-92273-3_25,
author = {Zheng, Jinfang and Xie, Jinyang and Lyu, Chen and Lyu, Lei},
title = {SS-CCN: Scale Self-guided Crowd Counting Network},
year = {2021},
isbn = {978-3-030-92272-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-92273-3_25},
doi = {10.1007/978-3-030-92273-3_25},
abstract = {With the emergence of deep learning, many CNN-based methods have achieved competitive performance in crowd counting, in which how to effectively solve the scale variation problem plays a key role. To tackle with the problem, we present an innovative scale self-guided crowd counting network (SS-CCN) by taking full advantage of scale information in a multi-level network. The proposed SS-CCN highlights crowd information by applying scale enhancement and scale-aware attention modules in multi-level features. Moreover, semantic attention module is applied on deep layers to extract semantic information. Besides, the fine-grained residual module is proposed to further refine the crowd information. Furthermore, we pioneer a scale pyramid loss with different loss functions applied to different scales. Integrating the proposed module, our method can effectively solve the scale variation problem. Extensive experimental results on several public datasets show that our proposed SS-CCN achieves satisfactory and superior performance compared to the state-of-the-art methods.},
booktitle = {Neural Information Processing: 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 8–12, 2021, Proceedings, Part IV},
pages = {299–310},
numpages = {12},
keywords = {Scale pyramid loss, Scale-aware, Attention mechanism, Deep learning, Crowd counting},
location = {Sanur, Bali, Indonesia}
}

@article{10.1016/j.artint.2011.07.004,
author = {Moratz, Reinhard and L\"{u}Cke, Dominik and Mossakowski, Till},
title = {A condensed semantics for qualitative spatial reasoning about oriented straight line segments},
year = {2011},
issue_date = {October, 2011},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {175},
number = {16–17},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2011.07.004},
doi = {10.1016/j.artint.2011.07.004},
abstract = {More than 15 years ago, a set of qualitative spatial relations between oriented straight line segments (dipoles) was suggested by Schlieder. However, it turned out to be difficult to establish a sound constraint calculus based on these relations. In this paper, we present the results of a new investigation into dipole constraint calculi which uses algebraic methods to derive sound results on the composition of relations of dipole calculi. This new method, which we call condensed semantics, is based on an abstract symbolic model of a specific fragment of our domain. It is based on the fact that qualitative dipole relations are invariant under orientation preserving affine transformations. The dipole calculi allow for a straightforward representation of prototypical reasoning tasks for spatial agents. As an example, we show how to generate survey knowledge from local observations in a street network. The example illustrates the fast constraint-based reasoning capabilities of dipole calculi. We integrate our results into two reasoning tools which are publicly available.},
journal = {Artif. Intell.},
month = oct,
pages = {2099–2127},
numpages = {29},
keywords = {Relation algebra, Qualitative spatial reasoning, Affine geometry}
}

@inproceedings{10.1145/3299819.3299823,
author = {Lu, Ming and Wang, Lijuan and Wang, Youyan and Fan, Zhicheng and Feng, Yatong and Liu, Xiaodong and Zhao, Xiaofang},
title = {An Orchestration Framework for a Global Multi-Cloud},
year = {2018},
isbn = {9781450366236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299819.3299823},
doi = {10.1145/3299819.3299823},
abstract = {Orchestration management in a global multi-cloud environment encounters many challenges, such as the centralized management of global cloud computing and application resources, more diverse cloud platforms and APIs, differentiated service catalogs. Network latency and instability between cloud platforms in various countries and accessibility between data centers of different security levels also makes orchestration not easy to manage. Orchestration tools, such as Ansible[1], has high requirements for many server ports and network quality. In a complex network environment, SaltStack[2] or Puppet[3], cannot deal with the multi-cloud management of large-scale computing and storage resource nodes. Apache Ambari[4], for applications that run on different cloud computing service providers, it lacks effective management capabilities. Therefore, it is difficult for common orchestration management tools to overcome these problems. In this paper, we propose a global multi-cloud orchestration framework (MCOF), which converts the orchestration instructions initiated from the MCOF master into a standardized orchestration definition model that is distributed to the MCOF workers inside each data center through the message queue. Then the MCOF workers perform the orchestration activities suitable for the corresponding cloud service provider behind the data center firewall to adapt to the complex cloud platform operating environment, and achieve standardization, efficiency, quality, reliability, and traceable orchestration management.},
booktitle = {Proceedings of the 2018 Artificial Intelligence and Cloud Computing Conference},
pages = {58–62},
numpages = {5},
keywords = {OpenStack, Multi-Cloud, IaaS orchestration, Cloud Management},
location = {Tokyo, Japan},
series = {AICCC '18}
}

@inproceedings{10.5555/2145432.2145538,
author = {Sun, Weiwei and Xu, Jia},
title = {Enhancing Chinese word segmentation using unlabeled data},
year = {2011},
isbn = {9781937284114},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper investigates improving supervised word segmentation accuracy with unlabeled data. Both large-scale in-domain data and small-scale document text are considered. We present a unified solution to include features derived from unlabeled data to a discriminative learning model. For the large-scale data, we derive string statistics from Gigaword to assist a character-based segmenter. In addition, we introduce the idea about transductive, document-level segmentation, which is designed to improve the system recall for out-of-vocabulary (OOV) words which appear more than once inside a document. Novel features result in relative error reductions of 13.8% and 15.4% in terms of F-score and the recall of OOV words respectively.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
pages = {970–979},
numpages = {10},
location = {Edinburgh, United Kingdom},
series = {EMNLP '11}
}

@inproceedings{10.5555/3504035.3504312,
author = {Huang, Wenjun and Liang, Chao and Yu, Yi and Wang, Zheng and Ruan, Weijian and Hu, Ruimin},
title = {Video-based person re-identification via self paced weighting},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Person re-identification (re-id) is a fundamental technique to associate various person images, captured by different surveillance cameras, to the same person. Compared to the single image based person re-id methods, video-based person re-id has attracted widespread attentions because extra space-time information and more appearance cues that can be used to greatly improve the matching performance. However, most existing video-based person re-id methods equally treat all video frames, ignoring their quality discrepancy caused by object occlusion and motions, which is a common phenomenon in real surveillance scenario. Based on this finding, we propose a novel video-based person re-id method via self paced weighting (SPW). Firstly, we propose a self paced outlier detection method to evaluate the noise degree of video sub sequences. Thereafter, a weighted multi-pair distance metric learning approach is adopted to measure the distance of two person image sequences. Experimental results on two public datasets demonstrate the superiority of the proposed method over current state-of-the-art work.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {277},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1007/s00146-007-0116-3,
author = {Bundy, Alan},
title = {AI Bridges and Dreams},
year = {2007},
issue_date = {Jun 2007},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {4},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-007-0116-3},
doi = {10.1007/s00146-007-0116-3},
abstract = {This paper is a modified version of my acceptance lecture for the 1986 SPL-Insight Award. It turned into something of a personal credo -describing my view of the nature of AIthe potential social benefit of applied AIthe importance of basic AI researchthe role of logic and the methodology of rational constructionthe interplay of applied and basic AI research, andthe importance of funding basic AI. These points are knitted together by an analogy between AI and structural engineering: in particular, between building expert systems and building bridges.},
journal = {AI Soc.},
month = jun,
pages = {659–668},
numpages = {10}
}

@article{10.1016/S0004-3702(99)00077-6,
author = {Weigel, Rainer and Faltings, Boi},
title = {Compiling constraint satisfaction problems},
year = {1999},
issue_date = {Dec. 1999},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {115},
number = {2},
issn = {0004-3702},
url = {https://doi.org/10.1016/S0004-3702(99)00077-6},
doi = {10.1016/S0004-3702(99)00077-6},
journal = {Artif. Intell.},
month = dec,
pages = {257–287},
numpages = {31},
keywords = {interchangeability, constraint-based reasoning, compilation}
}

@inproceedings{10.5555/3495724.3496750,
author = {Zhang, Dingwen and Tian, Haibin and Han, Jungong},
title = {Few-cost salient object detection with adversarial-paced learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Detecting and segmenting salient objects from given image scenes has received great attention in recent years. A fundamental challenge in training the existing deep saliency detection models is the requirement of large amounts of annotated data. While gathering large quantities of training data becomes cheap and easy, annotating the data is an expensive process in terms of time, labor and human expertise. To address this problem, this paper proposes to learn the effective salient object detection model based on the manual annotation on a few training images only, thus dramatically alleviating human labor in training models. To this end, we name this task as the few-cost salient object detection and propose an adversarial-paced learning (APL)-based framework to facilitate the few-cost learning scenario. Essentially, APL is derived from the self-paced learning (SPL) regime but it infers the robust learning pace through the data-driven adversarial learning mechanism rather than the heuristic design of the learning regularizer. Comprehensive experiments on four widely-used benchmark datasets demonstrate that the proposed method can effectively approach to the existing supervised deep salient object detection models with only 1k human-annotated training images.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1026},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.5555/3288992.3288997,
author = {Vinci, Giuseppe and Ventura, Val\'{e}rie and Smith, Matthew A. and Kass, Robert E.},
title = {Adjusted regularization of cortical covariance},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {45},
number = {2},
issn = {0929-5313},
abstract = {It is now common to record dozens to hundreds or more neurons simultaneously, and to ask how the network activity changes across experimental conditions. A natural framework for addressing questions of functional connectivity is to apply Gaussian graphical modeling to neural data, where each edge in the graph corresponds to a non-zero partial correlation between neurons. Because the number of possible edges is large, one strategy for estimating the graph has been to apply methods that aim to identify large sparse effects using an L1$L_{1}$ penalty. However, the partial correlations found in neural spike count data are neither large nor sparse, so techniques that perform well in sparse settings will typically perform poorly in the context of neural spike count data. Fortunately, the correlated firing for any pair of cortical neurons depends strongly on both their distance apart and the features for which they are tuned. We introduce a method that takes advantage of these known, strong effects by allowing the penalty to depend on them: thus, for example, the connection between pairs of neurons that are close together will be penalized less than pairs that are far apart. We show through simulations that this physiologically-motivated procedure performs substantially better than off-the-shelf generic tools, and we illustrate by applying the methodology to populations of neurons recorded with multielectrode arrays implanted in macaque visual cortex areas V1 and V4.},
journal = {J. Comput. Neurosci.},
month = oct,
pages = {83–101},
numpages = {19},
keywords = {Penalized maximum likelihood estimation, Macaque visual cortex, High-dimensional estimation, Graphical lasso, Gaussian graphical model, Functional connectivity, False discovery rate, Bayesian inference}
}

@inproceedings{10.1007/978-3-030-67832-6_30,
author = {Wang, Fei and Ding, Youdong and Liang, Huan and Wen, Jing},
title = {Discriminative and Selective Pseudo-Labeling for Domain Adaptation},
year = {2021},
isbn = {978-3-030-67831-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67832-6_30},
doi = {10.1007/978-3-030-67832-6_30},
abstract = {Unsupervised domain adaptation aims to transfer the knowledge of source domain to a related but not labeled target domain. Due to the lack of label information of target domain, most existing methods train a weak classifier and directly apply to pseudo-labeling which may downgrade adaptation performance. To address this problem, in this paper, we propose a novel discriminative and selective pseudo-labeling (DSPL) method for domain adaptation. Specifically, we first match the marginal distributions of two domains and increase inter-class distance simultaneously. Then a feature transformation method is proposed to learn a low-dimensional transfer subspace which is discriminative enough. Finally, after data has formed good clusters, we introduce a structured prediction based selective pseudo-labeling strategy which is able to sufficiently exploit target data structure. We conduct extensive experiments on three popular visual datasets, demonstrating the good domian adaptation performance of our method.},
booktitle = {MultiMedia Modeling: 27th International Conference, MMM 2021, Prague, Czech Republic, June 22–24, 2021, Proceedings, Part I},
pages = {365–377},
numpages = {13},
keywords = {Discriminative learned subspace, Pseudo-labeling, Unsupervised domain adaptation},
location = {Prague, Czech Republic}
}

@article{10.1007/s11227-017-2011-0,
author = {Mih\u{a}\'{z}Escu, Marian Cristian and Popescu, Paul \'{z}Tefan and Popescu, Elvira},
title = {Data analysis on social media traces for detection of "spam" and "don't care" learners},
year = {2017},
issue_date = {October   2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {73},
number = {10},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-017-2011-0},
doi = {10.1007/s11227-017-2011-0},
abstract = {Classification methods are becoming more and more useful as part of the standard data analyst's toolbox in many application domains. The specific data and domain characteristics of social media tools used in online educational contexts present the challenging problem of training high-quality classifiers that bring important insight into activity patterns of learners. Currently, standard and also very successful model for classification tasks is represented by decision trees. In this paper, we introduce a custom-designed data analysis pipeline for predicting "spam" and "don't care" learners from eMUSE online educational environment. The trained classifiers rely on social media traces as independent variables and on final grade of the learner as dependent variables. Current analysis evaluates performed activities of learners and the similarity of two derived data models. Experiments performed on social media traces from five years and 285 learners show satisfactory classification results that may be further used in productive environment. Accurate identification of "spam" and "don't care" users may have further a great impact on producing better classification models for the rest of the "regular" learners.},
journal = {J. Supercomput.},
month = oct,
pages = {4302–4323},
numpages = {22},
keywords = {Spam learners, Social media tools, Ranking, Online educational environment, Classification}
}

@article{10.1016/j.neucom.2021.04.070,
author = {Yang, Zhao and Liu, Jiehao and Liu, Tie and Zhu, Yuanxin and Wang, Li and Tao, Dapeng},
title = {Equidistant distribution loss for person re-identification},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {455},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.04.070},
doi = {10.1016/j.neucom.2021.04.070},
journal = {Neurocomput.},
month = sep,
pages = {255–264},
numpages = {10},
keywords = {Imbalance learning, Equidistant distribution loss, Person re-identification}
}

@inproceedings{10.1109/ASE.2015.44,
author = {Martinez, Jabier and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques and Traon, Yves le},
title = {Automating the extraction of model-based software product lines from model variants},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.44},
doi = {10.1109/ASE.2015.44},
abstract = {We address the problem of automating 1) the analysis of existing similar model variants and 2) migrating them into a software product line. Our approach, named MoVa2PL, considers the identification of variability and commonality in model variants, as well as the extraction of a CVL-compliant Model-based Software Product Line (MSPL) from the features identified on these variants. MoVa2PL builds on a generic representation of models making it suitable to any MOF-based models. We apply our approach on variants of the open source ArgoUML UML modeling tool as well as on variants of an Inflight Entertainment System. Evaluation with these large and complex case studies contributed to show how our feature identification with structural constraints discovery and the MSPL generation process are implemented to make the approach valid (i.e., the extracted software product line can be used to regenerate all variants considered) and sound (i.e., derived variants which did not exist are at least structurally valid).},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {396–406},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.3233/JIFS-169892,
author = {Kang, Yan and Li, Hao and Lu, Chenyang and Pu, Bin and Hsieh, Wen-Hsiang},
title = {A transfer learning algorithm for automatic requirement model generation},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {36},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169892},
doi = {10.3233/JIFS-169892},
abstract = {In this paper, we present a novel method for data-mining large informal product descriptions rather than extracting requirement features from proprietary project repositories. Our algorithm hybridizes deep-learning algorithms such as word2vec and recurrent neural network (RNN) with classical techniques to improve the performance of text analysis. Given the inaccuracy and incompleteness of the software requirement descriptions on websites, the instance-transfer learning method is utilized to construct a robust classifier and predict domain feature knowledge based on domain knowledge similar to the target domain. The bagging clustering algorithm is utilized with multiple clustering algorithms to help select transfer instances. [Author to confirm changes.]The RNN-based algorithm is utilized as a useful alternative to predict missing features by studying the requirement descriptions of a related software system, while word2vec is utilized to extract sensible feature keywords for the specific software domain. [Author to confirm changes.]Our RNN model for every subclass is based on the clustering result, and we construct subclass classifiers to recommend requirement keywords. Requirement features recommended by our algorithm potentially increase opportunities for requirement classification, promote software requirement quality, and deliver more reliable software products. We explain the details of implementation and perform experimental work on real requirement descriptions to establish its worth.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1183–1191},
numpages = {9},
keywords = {software requirement, feature model, transfer learning, RNN, Word2vec}
}

@inproceedings{10.1007/978-3-030-59618-7_4,
author = {Yu, Xu and He, Yadong and Xu, Biao and Du, Junwei and Jiang, Feng and Gong, Dunwei},
title = {An FM Developer Recommendation Algorithm by Considering Explicit Information and ID Information},
year = {2020},
isbn = {978-3-030-59617-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59618-7_4},
doi = {10.1007/978-3-030-59618-7_4},
abstract = {Recently, the developer recommendation on crowdsourcing software platform is of great research significance since an increasingly large number of tasks and developers have gathered on the platforms. In order to solve the problem of cold-start, the existing developer recommendation algorithms usually only use explicit information but not ID information to represent tasks and developers, which causes poor performance. In view of the shortcomings of the existing developer recommendation algorithms, this paper proposes an FM recommendation algorithm based on explicit to implicit feature mapping relationship modeling. This algorithm firstly integrates fully the ID information, explicit information and rating interaction between the completed task and the existing developers by using FM algorithm in order to get the implicit features related to their ID information. Secondly, for the completed tasks and existing developers, a deep regression model is established to learn the mapping relationship from explicit features to implicit features. Then, for the cold-start task or the cold-start developer, the implicit features are determined by the explicit features according to the deep regression model. Finally, the ratings in the cold-start scene can be predicted by the trained FM model with the explicit and implicit features. The simulation results on Topcoder platform show that the proposed algorithm has obvious advantages over the comparison algorithm in precision and recall.},
booktitle = {Web Services – ICWS 2020: 27th International Conference, Held as Part of the Services Conference Federation, SCF 2020, Honolulu, HI, USA, September 18–20, 2020, Proceedings},
pages = {49–60},
numpages = {12},
keywords = {Crowdsourcing software development, Developer recommendation, Cold-start problem, Deep regression model, FM algorithm},
location = {Honolulu, HI, USA}
}

@article{10.1145/1183236.1183239,
author = {Bichler, Martin and Kalagnanam, Jayant R.},
title = {Software frameworks for advanced procurement auction markets},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183239},
doi = {10.1145/1183236.1183239},
abstract = {A range of versatile auction formats are coming that allow more flexibility in specifying demand and supply.},
journal = {Commun. ACM},
month = dec,
pages = {104–108},
numpages = {5}
}

@article{10.1016/j.asoc.2016.05.020,
author = {Sachdeva, Jainy and Kumar, Vinod and Gupta, Indra and Khandelwal, Niranjan and Ahuja, Chirag Kamal},
title = {A package-SFERCB-"Segmentation, feature extraction, reduction and classification analysis by both SVM and ANN for brain tumors"},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.05.020},
doi = {10.1016/j.asoc.2016.05.020},
abstract = {An interactive computer aided dignostic (CAD) system for assisting inexperience young radiologists is developed. The difficulty in brain tumors classification is due to similar size, shape, location, hetrogeniety, presence of oedema, cystic and isointense regions has been the key feature of this research. Genetic Algorithm is employed as it is an easy concept and is well understood by radiologists without going in much depth of engineering.Display Omitted Brain tumors as segmented regions of interests (SROIs) by content based active contour model (CBAC).Feature extraction-intensity and texture based features.Feature reduction by Genetic Algorithm.Classification by Hybrid Models-GA-SVM and GA-ANN. The objective of this experimentation is to develop an interactive CAD system for assisting radiologists in multiclass brain tumor classification. The study is performed on a diversified dataset of 428 post contrast T1-weighted MR images of 55 patients and publically available dataset of 260 post contrast T1-weighted MR images of 10 patients. The first dataset includes primary brain tumors such as Astrocytoma (AS), Glioblastoma Multiforme (GBM), childhood tumor-Medulloblastoma (MED) and Meningioma (MEN), along with secondary tumor-Metastatic (MET). The second dataset consists of Astrocytoma (AS), Low Grade Glioma (LGL) and Meningioma (MEN). The tumor regions are marked by content based active contour (CBAC) model. The regions are than saved as segmented regions of interest (SROIs). 71 intensity and texture feature set is extracted from these SROIs. The features are specifically selected based on the pathological details of brain tumors provided by the radiologist. Genetic Algorithm (GA) selects the set of optimal features from this input set. Two hybrid machine learning models are implemented using GA with support vector machine (SVM) and artificial neural network (ANN) (GA-SVM and GA-ANN) and are tested on two different datasets. GA-SVM is proposed for finding preliminary probability in identifying tumor class and GA-ANN is used for confirmation of accuracy. Test results of the first dataset show that the GA optimization technique has enhanced the overall accuracy of SVM from 79.3% to 91.7% and of ANN from 75.6% to 94.9%. Individual class accuracies delivered by GA-SVM are: AS-89.8%, GBM-83.3%, MED-95.6%, MEN-91.8%, and MET-97.1%. Individual class accuracies delivered by GA-ANN classifier are: AS-96.6%, GBM-86.6%, MED-93.3%, MEN-96%, MET-100%. Similar results are obtained for the second dataset. The overall accuracy of SVM has increased from 80.8% to 89% and that of ANN has increased from 77.5% to 94.1%. Individual class accuracies delivered by GA-SVM are: AS-85.3%, LGL-88.8%, MEN-93%. Individual class accuracies delivered by GA-ANN classifier are: AS-92.6%, LGL-94.4%, MED-95.3%. It is observed from the experiments that GA-ANN classifier has provided better results than GA-SVM. Further, it is observed that along with providing finer results, GA-SVM provides advantage in speed whereas GA-ANN provides advantage in accuracy. The combined results from both the classifiers will benefit the radiologists in forming a better decision for classifying brain tumors.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {151–167},
numpages = {17},
keywords = {Genetic Algorithm (GA), GA-SVM, GA-ANN, Brain tumors}
}

@article{10.1016/j.infsof.2019.05.007,
author = {Ebrahimi, Neda and Trabelsi, Abdelaziz and Islam, Md. Shariful and Hamou-Lhadj, Abdelwahab and Khanmohammadi, Kobra},
title = {An HMM-based approach for automatic detection and classification of duplicate bug reports},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.007},
doi = {10.1016/j.infsof.2019.05.007},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {98–109},
numpages = {12},
keywords = {Mining software repositories, Machine learning, Hidden Markov models, Stack traces, Duplicate bug reports}
}

@inproceedings{10.5555/3297863.3297912,
author = {Fisher, Douglas H.},
title = {A selected summary of AI for computational sustainability},
year = {2017},
publisher = {AAAI Press},
abstract = {This paper and summary talk broadly survey computational sustainability research. Rather than a detailed treatment of the research projects in the area, which is beyond the scope of the paper and talk, the paper includes a meta-survey, pointing to edited collections and overviews in the literature for the interested reader. Computational sustainability research has been broadly characterized by AI methods employed, sustainability areas addressed, and contributions made to (typically, human) decision-making. The paper addresses these characterizations as well, which will facilitate a deeper synthesis later, to include the potential for developing sophisticated and holistic AI decision-making and advisory agents.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {4852–4857},
numpages = {6},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.1007/s10586-017-1108-9,
author = {Ilavarasi, A. K. and Sathiyabhama, B.},
title = {An evolutionary feature set decomposition based anonymization for classification workloads: Privacy Preserving Data Mining},
year = {2017},
issue_date = {Dec 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-017-1108-9},
doi = {10.1007/s10586-017-1108-9},
abstract = {Privacy has become an important concern while publishing micro data about a population. The emerging area called privacy preserving data mining (PPDM) focus on individual privacy without compromising data mining results. An adversarial exploitation of published data poses a risk of information disclosure about individuals. On the other hand, imposing privacy constraints on the data results in substantial information loss and compromises the legitimate data analysis. Motivated by the increasing growth of PPDM algorithms, we first investigate the privacy implications and the crosscutting issues between privacy versus utility of data. We present a privacy model that embeds the anonymization procedure in to a learning algorithm and this has mitigated the additional overheads imposed on data mining tasks. Our primary concern about PPDM is that the utility of data should not be compromised by the transformation applied. Different data mining classification workloads are analyzed with the proposed anonymization procedure for any side effects incurred. It is shown empirically that classification accuracy obtained for most of the datasets outperforms the results obtained with original dataset.},
journal = {Cluster Computing},
month = dec,
pages = {3515–3525},
numpages = {11},
keywords = {Privacy, Evolutionary partitioning, Decomposition, Data mining, Classification, Anonymization}
}

@inproceedings{10.5555/3305890.3305916,
author = {Ma, Fan and Meng, Deyu and Xie, Qi and Li, Zina and Dong, Xuanyi},
title = {Self-paced co-training},
year = {2017},
publisher = {JMLR.org},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two different views and exchanges labels of unlabeled instances in an iterative way. During co-training process, labels of unlabeled instances in the training pool are very likely to be false especially in the initial training rounds, while the standard co-training algorithm utilizes a "draw without replacement" manner and does not remove these false labeled instances from training. This issue not only tends to degenerate its performance but also hampers its fundamental theory. Besides, there is no optimization model to explain what objective a co-training process optimizes. To these issues, in this study we design a new co-training algorithm named self-paced co-training (SPaCo) with a "draw with replacement" learning mode. The rationality of SPaCo can be proved under theoretical assumptions utilized in traditional co-training research, and furthermore, the algorithm exactly complies with the alternative optimization process for an optimization model of self-paced curriculum learning, which can be finely explained in robust learning manner. Experimental results substantiate the superiority of the proposed method as compared with current state-of-the-art co-training methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2275–2284},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{10.1007/s10827-021-00801-9,
title = {30th Annual Computational Neuroscience Meeting: CNS*2021–Meeting Abstracts},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {49},
number = {Suppl 1},
issn = {0929-5313},
url = {https://doi.org/10.1007/s10827-021-00801-9},
doi = {10.1007/s10827-021-00801-9},
journal = {J. Comput. Neurosci.},
month = dec,
pages = {3–208},
numpages = {206}
}

@article{10.4018/IJEIS.2019040104,
author = {Sbai, Hanae and El Faquih, Loubna and Fredj, Mounia},
title = {A Novel Tool for Configurable Process Evolution and Service Derivation},
year = {2019},
issue_date = {Apr 2019},
publisher = {IGI Global},
address = {USA},
volume = {15},
number = {2},
issn = {1548-1115},
url = {https://doi.org/10.4018/IJEIS.2019040104},
doi = {10.4018/IJEIS.2019040104},
abstract = {In recent years, variability management in business processes is considered a key of reuse. Research works in this field focused mainly on variability modeling and resolution; whereas, evolution has been somehow neglected. In fact, new business requirements may occur, and business processes must evolve in order to meet the new needs. Furthermore, the evolution at business layer represented by configurable processes impact the IT layer represented by services. In this case, it is necessary to synchronize the changes between these two layers. In other words, the alignment of configurable processes and configurable services must occur to maintain an integrated view of an organization. This can be reached by the concept of service-based configurable processes. The study of existing tools in this domain shows the lack of solutions integrating both the evolution management, and the change propagation with respect to the variability. This article aims to represent the CPMEv, a novel tool for evolution management of service-based configurable processes.},
journal = {Int. J. Enterp. Inf. Syst.},
month = apr,
pages = {58–75},
numpages = {18},
keywords = {Variability, Evolution, Configurable Services, Change Propagation, Alignment}
}

@article{10.1145/2581376,
author = {Behjati, Razieh and Nejati, Shiva and Briand, Lionel C.},
title = {Architecture-Level Configuration of Large-Scale Embedded Software Systems},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2581376},
doi = {10.1145/2581376},
abstract = {Configuration in the domain of Integrated Control Systems (ICS) is largely manual, laborious, and error prone. In this article, we propose a model-based configuration approach that provides automation support for reducing configuration effort and the likelihood of configuration errors in the ICS domain. We ground our approach on component-based specifications of ICS families. We then develop a configuration algorithm using constraint satisfaction techniques over finite domains to generate products that are consistent with respect to their ICS family specifications. We reason about the termination and consistency of our configuration algorithm analytically. We evaluate the effectiveness of our configuration approach by applying it to a real subsea oil production system. Specifically, we have rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {25},
numpages = {43},
keywords = {product configuration, formal specification, constraint satisfaction techniques, consistent configuration, UML/OCL, Model-based product-line engineering}
}

@inproceedings{10.1145/2591062.2591151,
author = {Jing, Xiao-Yuan and Zhang, Zhi-Wu and Ying, Shi and Wang, Feng and Zhu, Yang-Ping},
title = {Software defect prediction based on collaborative representation classification},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591151},
doi = {10.1145/2591062.2591151},
abstract = {In recent years, machine learning techniques have been successfully applied into software defect prediction. Although they can yield reasonably good prediction results, there still exists much room for improvement on the aspect of prediction accuracy. Sparse representation is one of the most advanced machine learning techniques. It performs well with respect to signal compression and classification, but suffers from its time-consuming sparse coding. Compared with sparse representation, collaborative representation classification (CRC) can yield significantly lower computational complexity and competitive classification performance in pattern recognition domains. To achieve better defect prediction results, we introduce the CRC technique in this paper and propose a CRC based software defect prediction (CSDP) approach. We first design a CRC based learner to build a prediction model, whose computational burden is low. Then, we design a CRC based predictor to classify whether the query software modules are defective or defective-free. Experimental results on the widely used NASA datasets demonstrate the effectiveness and efficiency of the proposed approach.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {632–633},
numpages = {2},
keywords = {Software defect prediction, Prediction model, Machine learning, Collaborative representation classification},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@article{10.1007/s00766-003-0166-0,
author = {Thompson, Jeffrey M. and Heimdahl, Mats P.},
title = {Structuring product family requirements for n-dimensional and hierarchical product lines},
year = {2003},
issue_date = {February  2003},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-003-0166-0},
doi = {10.1007/s00766-003-0166-0},
abstract = {The software product-line approach (for software product families) is one of the success stories of software reuse. When applied, it can result in cost savings and increases in productivity. In addition, in safety-critical systems the approach has the potential for reuse of analysis and testing results, which can lead to a safer system. Nevertheless, there are times when it seems like a product family approach should work when, in fact, there are difficulties in properly defining the boundaries of the product family. In this paper, we draw on our experiences in applying the software product-line approach to a family of mobile robots, a family of flight guidance systems, and a family of cardiac pacemakers, as well as case studies done by others to (1) illustrate how domain structure can currently limit applicability of product-line approaches to certain domains and (2) demonstrate our progress towards a solution using a set-theoretic approach to reason about domains of what we call n-dimensional and hierarchical product families.},
journal = {Requir. Eng.},
month = feb,
pages = {42–54},
numpages = {13},
keywords = {Requirements structuring, Requirements reuse, Product line modelling, Product line engineering, Domain Engineering}
}

@inproceedings{10.1007/978-3-030-87007-2_26,
author = {Szamosv\"{o}lgyi, Zsolt J\'{a}nos and V\'{a}radi, Endre Tam\'{a}s and T\'{o}th, Zolt\'{a}n and J\'{a}sz, Judit and Ferenc, Rudolf},
title = {Assessing Ensemble Learning Techniques in Bug Prediction},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_26},
doi = {10.1007/978-3-030-87007-2_26},
abstract = {The application of ensemble learning techniques is continuously increasing, since they have proven to be superior over traditional machine learning techniques in various domains. These algorithms could be employed for bug prediction purposes as well. Existing studies investigated the performance of ensemble learning techniques only for PROMISE and the NASA MDP public datasets; however, it is important to evaluate the ensemble learning techniques on additional public datasets in order to test the generalizability of the techniques. We investigated the performance of the two most widely-used ensemble learning techniques AdaBoost and Bagging on the Unified Bug Dataset, which encapsulates 3 class level public bug datasets in a uniformed format with a common set of software product metrics used as predictors. Additionally, we investigated the effect of using 3 different resampling techniques on the dataset. Finally, we studied the performance of using Decision Tree and Na\"{\i}ve Bayes as the weak learners in the ensemble learning. We also fine tuned the parameters of the weak learners to have the best possible end results.We experienced that AdaBoost with Decision Tree weak learner outperformed other configurations. We could achieve 54.61% F-measure value (81.96% Accuracy, 50.92% Precision, 58.90% Recall) with the configuration of 300 estimators and 0.05 learning rate. Based on the needs, one can apply RUS resampling to get a recall value up&nbsp;to 75.14% (of course losing precision at the same time).},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {368–381},
numpages = {14},
keywords = {Unified bug dataset, Resampling, Bug prediction, AdaBoost},
location = {Cagliari, Italy}
}

@inproceedings{10.1007/978-3-319-65340-2_52,
author = {Cabalar, Pedro and Mart\'{\i}n, Rodrigo},
title = {haspie - A Musical Harmonisation Tool Based on ASP},
year = {2017},
isbn = {978-3-319-65339-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-65340-2_52},
doi = {10.1007/978-3-319-65340-2_52},
abstract = {In this paper we describe a musical harmonisation and composition assistant based on Answer Set Programming (ASP). The tool takes scores in MusicXML format and annotates them with a preferred harmonisation. If specified, it is also able to complete intentionally blank sections and create new parts of the score that fit with the proposed harmonisation. Both the harmonisation and the completion of blank parts can be seen as constraint satisfaction problems that are encoded in ASP. Although the tool is a preliminary prototype still being improved, its basic functionality already helps to illustrate the appropriateness of ASP for musical knowledge representation, which provides a high degree of flexibility thanks to its relational, declarative orientation and an efficient computation of preferred solutions.},
booktitle = {Progress in Artificial Intelligence: 18th EPIA Conference on Artificial Intelligence, EPIA 2017, Porto, Portugal, September 5-8, 2017, Proceedings},
pages = {637–642},
numpages = {6},
keywords = {Answer Set, Harmonic Orientation, MusicXML Format, Blank Part, Blank Section},
location = {Porto, Portugal}
}

@article{10.1016/j.neucom.2019.08.002,
author = {Li, Zhenglai and Tang, Chang and Chen, Jiajia and Wan, Cheng and Yan, Weiqing and Liu, Xinwang},
title = {Diversity and consistency learning guided spectral embedding for multi-view clustering},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {370},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.08.002},
doi = {10.1016/j.neucom.2019.08.002},
journal = {Neurocomput.},
month = dec,
pages = {128–139},
numpages = {12},
keywords = {99-00, 00-01, Diversity and consistency learning, Spectral embedding, Multi-view clustering}
}

@inproceedings{10.1109/BotSE.2019.00016,
author = {Paikari, Elahe and Choi, JaeEun and Kim, SeonKyu and Baek, Sooyoung and Kim, MyeongSoo and Lee, SeungEon and Han, ChaeYeon and Kim, YoungJae and Ahn, KaHye and Cheong, Chan and van der Hoek, Andr\'{e}},
title = {A chatbot for conflict detection and resolution},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/BotSE.2019.00016},
doi = {10.1109/BotSE.2019.00016},
abstract = {We present Sayme, a chatbot that we are developing to address the detection and resolution of potential code conflicts that may arise in parallel software development. Sayme is designed to operate both proactively, informing developers when they engage in activities that create conflicting changes, and reactively, responding to user inquiries regarding the state of different developers' work and how it may overlap. We introduce our motivation for developing Sayme, present its design and features, and offer an outlook at our future work.},
booktitle = {Proceedings of the 1st International Workshop on Bots in Software Engineering},
pages = {29–33},
numpages = {5},
keywords = {configuration management, collaborative development, code conflicts, chatbots, awareness},
location = {Montreal, Quebec, Canada},
series = {BotSE '19}
}

@article{10.1016/j.specom.2017.04.002,
title = {Fourier model based features for analysis and classification of out-of-breath speech},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {90},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2017.04.002},
doi = {10.1016/j.specom.2017.04.002},
abstract = {A new stressed speech database, named out-of-breath speech (OBS) database, is created, which contains three classes, out-of-breath speech, low out-of-breath speech and normal speech.Four features are proposed using mutual information (MI) on the Fourier parameters for analysis and classification of different classes of OBS database.For multi-class classification, support vector machine (SVM) classifier is used with binary cascade approach.Recognition results show that the proposed features have higher potential to classify the out-of-breath speech, compared to the breathiness, MFCC and TEO-CB-Auto-Env features. This paper presents a new method of feature extraction using Fourier model for analysis of out-of-breath speech. The proposed feature is evaluated using mutual information (MI) on the difference and ratio values of the Fourier parameters, amplitude and frequency. The difference and ratio are calculated between two contiguous values of the Fourier parameters. To analyze the out-of-breath speech, a new stressed speech database, named out-of-breath speech (OBS) database, is created. The database contains three classes of speech, out-of-breath speech, low out-of-breath speech and normal speech. The effectiveness of the proposed features is evaluated with the statistical analysis. The proposed features not only differentiate the normal speech and the out-of-breath speech, but also can discriminate different breath emission levels of speech. Hidden Markov model (HMM) and support vector machine (SVM) are used to evaluate the performance of the proposed features using the OBS database. For multi-class classification problem, SVM classifier is used with binary cascade approach. The performance of the proposed features is compared with the breathiness feature, the mel frequency cepstral coefficient (MFCC) feature and the Teager energy operator (TEO) based critical band TEO autocorrelation envelope (TEO-CB-Auto-Env) feature. The proposed feature outperforms the breathiness feature, the MFCC feature and the TEO-CB-Auto-Env feature.},
journal = {Speech Commun.},
month = jun,
pages = {1–14},
numpages = {14}
}

@inproceedings{10.1007/978-3-030-98682-7_17,
author = {Hasselbring, Arne and Baude, Andreas},
title = {Soccer Field Boundary Detection Using Convolutional Neural Networks},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_17},
doi = {10.1007/978-3-030-98682-7_17},
abstract = {Detecting the field boundary is often one of the first steps in the vision pipeline of soccer robots. Conventional methods make use of a (possibly adaptive) green classifier, selection of boundary points and possibly model fitting. We present an approach to predict the coordinates of the field boundary column-wise in the image using a convolutional neural network. This is combined with a method to let the network predict the uncertainty of its output, which allows to fit a line model in which columns are weighted according to the network’s confidence. Experiments show that the resulting models are accurate enough in different lighting conditions as well as real-time capable. Code and data are available online (, ).},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {202–213},
numpages = {12},
location = {Sydney, NSW, Australia}
}

@article{10.1002/smr.2163,
author = {Chatzimparmpas, Angelos and Bibi, Stamatia},
title = {Maintenance process modeling and dynamic estimations based on Bayesian networks and association rules},
year = {2019},
issue_date = {September 2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {31},
number = {9},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2163},
doi = {10.1002/smr.2163},
abstract = {Managing the maintenance process and estimating accurately the effort and duration required for a new release is considered to be a crucial task as it affects successful software project survival and progress over time. In this study, we propose the combination of two well‐known machine learning (ML) techniques, Bayesian networks (BNs), and association rules (ARs) for modeling the maintenance process by identifying the relationships among the internal and external quality metrics related to a particular project release to both the maintainability of the project and the maintenance process indicators (ie, effort and duration). We also exploit Bayesian inference, to test the effect of certain changes in internal and external project factors to the maintainability of a project. We evaluate our approach through a case study on 957 releases of five open source JavaScript applications. The results show that the maintainability of a release, the changes observed between subsequent releases, and the time required between two releases can be accurately predicted from size, complexity, and activity metrics. The proposed combined approach achieves higher accuracy when evaluated against the BN model accuracy.},
journal = {J. Softw. Evol. Process},
month = oct,
numpages = {25},
keywords = {source code quality, software quality, maintenance, maintainability, JavaScript, developers' activity}
}

@inproceedings{10.1145/2897845.2897856,
author = {Meng, Guozhu and Xue, Yinxing and Mahinthan, Chandramohan and Narayanan, Annamalai and Liu, Yang and Zhang, Jie and Chen, Tieming},
title = {Mystique: Evolving Android Malware for Auditing Anti-Malware Tools},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897856},
doi = {10.1145/2897845.2897856},
abstract = {In the arms race of attackers and defenders, the defense is usually more challenging than the attack due to the unpredicted vulnerabilities and newly emerging attacks every day. Currently, most of existing malware detection solutions are individually proposed to address certain types of attacks or certain evasion techniques. Thus, it is desired to conduct a systematic investigation and evaluation of anti-malware solutions and tools based on different attacks and evasion techniques. In this paper, we first propose a meta model for Android malware to capture the common attack features and evasion features in the malware. Based on this model, we develop a framework, MYSTIQUE, to automatically generate malware covering four attack features and two evasion features, by adopting the software product line engineering approach. With the help of MYSTIQUE, we conduct experiments to 1) understand Android malware and the associated attack features as well as evasion techniques; 2) evaluate and compare the 57 off-the-shelf anti-malware tools, 9 academic solutions and 4 App market vetting processes in terms of accuracy in detecting attack features and capability in addressing evasion. Last but not least, we provide a benchmark of Android malware with proper labeling of contained attack and evasion features.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {365–376},
numpages = {12},
keywords = {malware generation, evolutionary algorithm, defense capability, android feature model},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@article{10.1016/j.jpdc.2019.08.008,
author = {Mart\'{\i}nez, Daniel and Brewer, Wesley and Strelzoff, Andrew and Wilson, Andrew and Wade, Daniel},
title = {Rotorcraft virtual sensors via deep regression},
year = {2020},
issue_date = {Jan 2020},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.08.008},
doi = {10.1016/j.jpdc.2019.08.008},
journal = {J. Parallel Distrib. Comput.},
month = jan,
pages = {114–126},
numpages = {13},
keywords = {Evolutionary Optimization, Deep Neural Networks, High Performance Computing, Deep Learning, Virtual Sensors}
}

@inproceedings{10.5555/2900929.2901066,
author = {Fisher, Douglas H. and Dilkina, Bistra and Eaton, Eric and Gomes, Carla},
title = {Incorporating computational sustainability into AI education through a freely-available, collectively-composed supplementary lab text},
year = {2012},
publisher = {AAAI Press},
abstract = {We introduce a laboratory text on environmental and societal sustainability applications that can be a supplemental resource for any undergraduate AI course. The lab text, entitled Artificial Intelligence for Computational Sustainability: A Lab Companion, is brand new and incomplete; freely available through Wikibooks; and open to community additions of projects, assignments, and explanatory material on AI for sustainability. The project adds to existing educational efforts of the computational sustainability community, encouraging the flow of knowledge from research to education and public outreach. Besides summarizing the laboratory book, this paper touches on its implications for integration of research and education, for communicating science to the public, and other broader impacts.},
booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
pages = {2369–2370},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {AAAI'12}
}

@inproceedings{10.1007/978-3-030-98682-7_13,
author = {Antonioni, Emanuele and Suriani, Vincenzo and Solimando, Filippo and Nardi, Daniele and Bloisi, Domenico D.},
title = {Learning from the Crowd: Improving the Decision Making Process in Robot Soccer Using the Audience Noise},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_13},
doi = {10.1007/978-3-030-98682-7_13},
abstract = {Fan input and support is an important component in many individual and team sports, ranging from athletics to basketball. Audience interaction provides a consistent impact on the athletes’ performance. The analysis of the crowd noise can provide a global indication on the ongoing game situation, less conditioned by subjective factors that can influence a single fan. In this work, we exploit the collective intelligence of the audience of a robot soccer match to improve the performance of the robot players. In particular, audio features extracted from the crowd noiseare used in a Reinforcement Learning process to possibly modify the game strategy. The effectiveness of the proposed approach is demonstrated by experiments on registered crowd noise samples from several past RoboCup SPL matches.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {153–164},
numpages = {12},
keywords = {Sound recognition, RoboCup SPL, Crowd noise interpretation},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3320269.3384742,
author = {Hendler, Danny and Kels, Shay and Rubin, Amir},
title = {AMSI-Based Detection of Malicious PowerShell Code Using Contextual Embeddings},
year = {2020},
isbn = {9781450367509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320269.3384742},
doi = {10.1145/3320269.3384742},
abstract = {PowerShell is a command-line shell, supporting a scripting language. It is widely used in organizations for configuration management and task automation but is also increasingly used for launching cyber attacks against organizations, mainly because it is pre-installed on Windows machines and exposes strong functionality that may be leveraged by attackers. This makes the problem of detecting malicious PowerShell code both urgent and challenging. Microsoft's Antimalware Scan Interface (AMSI), built into Windows 10, allows defending systems to scan all the code passed to scripting engines such as PowerShell prior to its execution. In this work, we conduct the first study of malicious PowerShell code detection using the information made available by AMSI. We present several novel deep-learning based detectors of malicious PowerShell code that employ pretrained contextual embeddings of words from the PowerShell "language". A contextual word embedding is able to project semantically-similar words to proximate vectors in the embedding space. A known problem in the cybersecurity domain is that labeled data is relatively scarce, in comparison with unlabeled data, making it difficult to devise effective supervised detection of malicious activity of many types. This is also the case with PowerShell code. Our work shows that this problem can be mitigated by learning a pretrained contextual embedding based on unlabeled data. We trained and evaluated our models using real-world data, collected using AMSI. The contextual embedding was learnt using a large corpus of unlabeled PowerShell scripts and modules collected from public repositories. Our performance analysis establishes that the use of unlabeled data for the embedding significantly improved the performance of our detectors. Our best-performing model uses an architecture that enables the processing of textual signals from both the character and token levels and obtains a true-positive rate of nearly 90% while maintaining a low false-positive rate of less than 0.1%.},
booktitle = {Proceedings of the 15th ACM Asia Conference on Computer and Communications Security},
pages = {679–693},
numpages = {15},
keywords = {powershell, neural networks, cybersecurity, contextual embedding},
location = {Taipei, Taiwan},
series = {ASIA CCS '20}
}

@article{10.1007/s10618-016-0475-9,
author = {Garcia, Lu\'{\i}s P. and Lorena, Ana C. and Matwin, Stan and Carvalho, Andr\'{e} C.},
title = {Ensembles of label noise filters: a ranking approach},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {5},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-016-0475-9},
doi = {10.1007/s10618-016-0475-9},
abstract = {Label noise can be a major problem in classification tasks, since most machine learning algorithms rely on data labels in their inductive process. Thereupon, various techniques for label noise identification have been investigated in the literature. The bias of each technique defines how suitable it is for each dataset. Besides, while some techniques identify a large number of examples as noisy and have a high false positive rate, others are very restrictive and therefore not able to identify all noisy examples. This paper investigates how label noise detection can be improved by using an ensemble of noise filtering techniques. These filters, individual and ensembles, are experimentally compared. Another concern in this paper is the computational cost of ensembles, once, for a particular dataset, an individual technique can have the same predictive performance as an ensemble. In this case the individual technique should be preferred. To deal with this situation, this study also proposes the use of meta-learning to recommend, for a new dataset, the best filter. An extensive experimental evaluation of the use of individual filters, ensemble filters and meta-learning was performed using public datasets with imputed label noise. The results show that ensembles of noise filters can improve noise filtering performance and that a recommendation system based on meta-learning can successfully recommend the best filtering technique for new datasets. A case study using a real dataset from the ecological niche modeling domain is also presented and evaluated, with the results validated by an expert.},
journal = {Data Min. Knowl. Discov.},
month = sep,
pages = {1192–1216},
numpages = {25},
keywords = {Recommendation system, Noise ranking, Noise filters, Label noise, Ensemble filters}
}

@article{10.1016/j.jss.2008.08.026,
author = {Lago, Patricia and Muccini, Henry and van Vliet, Hans},
title = {A scoped approach to traceability management},
year = {2009},
issue_date = {January, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2008.08.026},
doi = {10.1016/j.jss.2008.08.026},
abstract = {Traceability is the ability to describe and follow the life of a software artifact and a means for modeling the relations between software artifacts in an explicit way. Traceability has been successfully applied in many software engineering communities and has recently been adopted to document the transition among requirements, architecture and implementation. We present an approach to customize traceability to the situation at hand. Instead of automating tracing, or representing all possible traces, we scope the traces to be maintained to the activities stakeholders must carry out. We define core traceability paths, consisting of essential traceability links required to support the activities. We illustrate the approach through two examples: product derivation in software product lines, and release planning in software process management. By using a running software product line example, we explain why the core traceability paths identified are needed when navigating from feature to structural models and from family to product level and backward between models used in software product derivation. A feasibility study in release planning carried out in an industrial setting further illustrates the use of core traceability paths during production and measures the increase in performance of the development processes supported by our approach. These examples show that our approach can be successfully used to support both product and process traceability in a pragmatic yet efficient way.},
journal = {J. Syst. Softw.},
month = jan,
pages = {168–182},
numpages = {15},
keywords = {Traceability paths, Traceability issues, Software product line, Software process management}
}

@inproceedings{10.1145/3447545.3451177,
author = {Canales, Felipe and Hecht, Geoffrey and Bergel, Alexandre},
title = {Optimization of Java Virtual Machine Flags using Feature Model and Genetic Algorithm},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451177},
doi = {10.1145/3447545.3451177},
abstract = {Optimizing the Java Virtual Machine (JVM) options in order to get the best performance out of a program for production is a challenging and time-consuming task. HotSpot, the Oracle's open-source Java VM implementation offers more than 500 options, called flags, that can be used to tune the JVM's compiler, garbage collector (GC), heap size and much more. In addition to being numerous, these flags are sometimes poorly documented and create a need of benchmarking to ensure that the flags and their associated values deliver the best performance and stability for a particular program to execute.Auto-tuning approaches have already been proposed in order to mitigate this burden. However, in spite of increasingly sophisticated search techniques allowing for powerful optimizations, these approaches take little account of the underlying complexities of JVM flags. Indeed, dependencies and incompatibilities between flags are non-trivial to express, which if not taken into account may lead to invalid or spurious flag configurations that should not be considered by the auto-tuner.In this paper, we propose a novel model, inspired by the feature model used in Software Product Line, which takes the complexity of JVM's flags into account. We then demonstrate the usefulness of this model, using it as an input of a Genetic Algorithm (GA) to optimize the execution times of DaCapo Benchmarks.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {183–186},
numpages = {4},
keywords = {optimization, java virtual machine, genetic algorithm, feature model, auto-tuning},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.1007/s11042-019-7251-y,
author = {Mei, Jianhan and Wu, Ziming and Chen, Xiang and Qiao, Yu and Ding, Henghui and Jiang, Xudong},
title = {DeepDeblur: text image recovery from blur to sharp},
year = {2019},
issue_date = {Jul 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {13},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7251-y},
doi = {10.1007/s11042-019-7251-y},
abstract = {Digital images could be degraded by a variety of blur during the image acquisition (i.e. relative motion of cameras, electronic noise, capturing defocus, and so on). Blurring images can be computationally modeled as the result of a convolution process with the corresponding blur kernel and thus, image deblurring can be regarded as a deconvolution operation. In this paper, we explore to deblur images by approximating blind deconvolutions using a deep neural network. Different deep neural network structures are investigated to evaluate their deblurring capabilities, which contributes to the optimal design of a network architecture. It is found that shallow and narrow networks are not capable of handling complex motion blur. We thus, present a deep network with 20 layers to cope with text image blur. In addition, a novel network structure with Sequential Highway Connections (SHC) is leveraged to gain superior convergence. The experiment results demonstrate the state-of-the-art performance of the proposed framework with the higher visual quality of the delurred images.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {18869–18885},
numpages = {17},
keywords = {Text Deblurring, Short connection, Convolutional Neural Network (CNN), Blind deconvolution}
}

@article{10.1016/j.scico.2017.10.013,
author = {Castro, Thiago and Lanna, Andr and Alves, Vander and Teixeira, Leopoldo and Apel, Sven and Schobbens, Pierre-Yves},
title = {All roads lead to Rome},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2017.10.013},
doi = {10.1016/j.scico.2017.10.013},
abstract = {The formalization of seven strategies for product-line reliability analysis.The first feature-family-product-based strategy for product-line model checking.A general principle for lifting analyses to product lines using ADDs.Proofs that the formalized strategies commute.All strategies proven sound with respect to single-product reliability analysis. Software product line engineering is a means to systematically manage variability and commonality in software systems, enabling the automated synthesis of related programs (products) from a set of reusable assets. However, the number of products in a software product line may grow exponentially with the number of features, so it is practically infeasible to quality-check each of these products in isolation. There is a number of variability-aware approaches to product-line analysis that adapt single-product analysis techniques to cope with variability in an efficient way. Such approaches can be classified along three analysis dimensions (product-based, family-based, and feature-based), but, particularly in the context of reliability analysis, there is no theory comprising both (a) a formal specification of the three dimensions and resulting analysis strategies and (b) proof that such analyses are equivalent to one another. The lack of such a theory hinders formal reasoning on the relationship between the analysis dimensions and derived analysis techniques. We formalize seven approaches to reliability analysis of product lines, including the first instance of a feature-family-product-based analysis in the literature. We prove the formalized analysis strategies to be sound with respect to the probabilistic approach to reliability analysis of a single product. Furthermore, we present a commuting diagram of intermediate analysis steps, which relates different strategies and enables the reuse of soundness proofs between them.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {116–160},
numpages = {45},
keywords = {Verification, Software product lines, Reliability analysis, Product-line analysis, Model checking}
}

@article{10.1016/j.specom.2012.01.002,
author = {Zelinka, Petr and Sigmund, Milan and Schimmel, Jiri},
title = {Impact of vocal effort variability on automatic speech recognition},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {6},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2012.01.002},
doi = {10.1016/j.specom.2012.01.002},
abstract = {The impact of changes in a speaker's vocal effort on the performance of automatic speech recognition has largely been overlooked by researchers and virtually no speech resources exist for the development and testing of speech recognizers at all vocal effort levels. This study deals with speech properties in the whole range of vocal modes - whispering, soft speech, normal speech, loud speech, and shouting. Fundamental acoustic and phonetic changes are documented. The impact of vocal effort variability on the performance of an isolated-word recognizer is shown and effective means of improving the system's robustness are tested. The proposed multiple model framework approach reaches a 50% relative reduction of word error rate compared to the baseline system. A new specialized speech database, BUT-VE1, is presented, which contains speech recordings of 13 speakers at 5 vocal effort levels with manual phonetic segmentation and sound pressure level calibration.},
journal = {Speech Commun.},
month = jul,
pages = {732–742},
numpages = {11},
keywords = {Vocal effort level, Robust speech recognition, Machine learning}
}

@inproceedings{10.1007/978-3-030-58539-6_16,
author = {Majumdar, Arjun and Shrivastava, Ayush and Lee, Stefan and Anderson, Peter and Parikh, Devi and Batra, Dhruv},
title = {Improving Vision-and-Language Navigation with Image-Text Pairs from the Web},
year = {2020},
isbn = {978-3-030-58538-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58539-6_16},
doi = {10.1007/978-3-030-58539-6_16},
abstract = {Following a navigation instruction such as ‘Walk down the stairs and stop at the brown sofa’ requires embodied AI agents to ground referenced scene elements referenced (e.g. ‘stairs’) to visual content in the environment (pixels corresponding to ‘stairs’). We ask the following question – can we leverage abundant ‘disembodied’ web-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn the visual groundings that improve performance on a relatively data-starved embodied perception task (Vision-and-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction (‘...stop at the brown sofa’) and a trajectory of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN – outperforming prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful – with their combination resulting in further gains.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI},
pages = {259–274},
numpages = {16},
keywords = {Embodied AI, Transfer learning, Vision-and-language navigation},
location = {Glasgow, United Kingdom}
}

@article{10.1016/S0004-3702(02)00193-5,
author = {Gerevini, Alfonso and Renz, Jochen},
title = {Combining topological and size information for spatial reasoning},
year = {2002},
issue_date = {May 2002},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {137},
number = {1–2},
issn = {0004-3702},
url = {https://doi.org/10.1016/S0004-3702(02)00193-5},
doi = {10.1016/S0004-3702(02)00193-5},
abstract = {Information about the size of spatial regions is often easily accessible and, when combined with other types of spatial information, it can be practically very useful. In this paper we introduce four classes of qualitative and metric size constraints, and we study their integration with the Region Connection Calculus RCC-8, a well-known approach to qualitative spatial reasoning with topological relations. We propose a new path-consistency algorithm for combining RCC-8 relations and qualitative size relations. The algorithm is complete for deciding satisfiability of an input set of topological constraints over one of the three maximal tractable subclasses of RCC-8 containing all the basic relations. Moreover, its time complexity is cubic and is the same as the complexity of the best-known method for deciding satisfiability when only these topological relations are considered. We also provide results on finding a consistent scenario in cubic time for these combined classes. Regarding metric size constraints, we first study their combination with RCC-8 and we show that deciding satisfiability for the combined sets of constraints is NP-hard, even when only the RCC-8 basic relations are used. Then we introduce RCC-7, a subalgebra of RCC-8 that can be used for applications where spatial regions cannot partially overlap. We show that reasoning with the seven RCC-7 basic relations and the universal relation is intractable, but that reasoning with the RCC-7 basic relations combined with metric size information is tractable. Finally, we give a polynomial algorithm for the latter case and a backtracking algorithm for the general case.},
journal = {Artif. Intell.},
month = may,
pages = {1–42},
numpages = {42},
keywords = {spatial reasoning, region connection calculus, constraint-based reasoning, constraint satisfaction, computational complexity}
}

@article{10.1016/j.artint.2016.03.002,
author = {Kaldeli, Eirini and Lazovik, Alexander and Aiello, Marco},
title = {Domain-independent planning for services in uncertain and dynamic environments},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {236},
number = {C},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2016.03.002},
doi = {10.1016/j.artint.2016.03.002},
abstract = {Research in automated planning provides novel insights into service composition and contributes towards the provision of automatic compositions which adapt to changing user needs and environmental conditions. Most of the existing planning approaches to aggregating services, however, suffer from one or more of the following limitations: they are not domain-independent, cannot efficiently deal with numeric-valued variables, especially sensing outcomes or operator inputs, and they disregard recovery from runtime contingencies due to erroneous service behavior or exogenous events that interfere with plan execution. We present the RuGPlanner, which models the planning task as a Constraint Satisfaction Problem. In order to address the requirements put forward by service domains, the RuGPlanner is endowed with a number of special features. These include a knowledge-level representation to model uncertainty about the initial state and the outcome of sensing actions, and efficient handling of numeric-valued variables, inputs to actions or observational effects. In addition, it generates plans with a high level of parallelism, it supports a rich declarative language for expressing extended goals, and allows for continual plan revision to deal with sensing outputs, failures, long response times or timeouts, as well as the activities of external agents. The proposed planning framework is evaluated based on a number of scenarios to demonstrate its feasibility and efficiency in several planning domains and execution circumstances which reflect concerns from different service environments.},
journal = {Artif. Intell.},
month = jul,
pages = {30–64},
numpages = {35},
keywords = {Web service composition, AI planning}
}

@article{10.1016/j.patcog.2011.09.011,
author = {Rasmussen, Peter M. and Hansen, Lars K. and Madsen, Kristoffer H. and Churchill, Nathan W. and Strother, Stephen C.},
title = {Model sparsity and brain pattern interpretation of classification models in neuroimaging},
year = {2012},
issue_date = {June, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2011.09.011},
doi = {10.1016/j.patcog.2011.09.011},
abstract = {Interest is increasing in applying discriminative multivariate analysis techniques to the analysis of functional neuroimaging data. Model interpretation is of great importance in the neuroimaging context, and is conventionally based on a 'brain map' derived from the classification model. In this study we focus on the relative influence of model regularization parameter choices on both the model generalization, the reliability of the spatial patterns extracted from the classification model, and the ability of the resulting model to identify relevant brain networks defining the underlying neural encoding of the experiment. For a support vector machine, logistic regression and Fisher's discriminant analysis we demonstrate that selection of model regularization parameters has a strong but consistent impact on the generalizability and both the reproducibility and interpretable sparsity of the models for both @?"2 and @?"1 regularization. Importantly, we illustrate a trade-off between model spatial reproducibility and prediction accuracy. We show that known parts of brain networks can be overlooked in pursuing maximization of classification accuracy alone with either @?"2 and/or @?"1 regularization. This supports the view that the quality of spatial patterns extracted from models cannot be assessed purely by focusing on prediction accuracy. Our results instead suggest that model regularization parameters must be carefully selected, so that the model and its visualization enhance our ability to interpret the brain.},
journal = {Pattern Recogn.},
month = jun,
pages = {2085–2100},
numpages = {16},
keywords = {Sparsity, Regularization, Pattern analysis, Neuroimaging, NPAIRS resampling, Model interpretation, Machine learning, Kernel methods, Classification}
}

@inproceedings{10.1145/1960275.1960283,
author = {Schaefer, Ina and Bettini, Lorenzo and Damiani, Ferruccio},
title = {Compositional type-checking for delta-oriented programming},
year = {2011},
isbn = {9781450306058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1960275.1960283},
doi = {10.1145/1960275.1960283},
abstract = {Delta-oriented programming is a compositional approach to flexibly implementing software product lines. A product line is represented by a code base and a product line declaration. The code base consists of a set of delta modules specifying modifications to object-oriented programs. The product line declaration provides the connection of the delta modules with the product features. This separation increases the reusability of delta modules. In this paper, we provide a foundation for compositional type checking of delta-oriented product lines of Java programs by presenting a minimal core calculus for delta-oriented programming. The calculus is equipped with a constraint-based type system that allows analyzing each delta module in isolation, such that that also the results of the analysis can be reused. By combining the analysis results for the delta modules with the product line declaration it is possible to establish that all the products of the product line are well-typed according to the Java type system.},
booktitle = {Proceedings of the Tenth International Conference on Aspect-Oriented Software Development},
pages = {43–56},
numpages = {14},
keywords = {type system, software product line, java},
location = {Porto de Galinhas, Brazil},
series = {AOSD '11}
}

@inproceedings{10.1145/3451421.3451427,
author = {Liu, Xiaoli and Li, Jiali and Cao, Peng},
title = {SP-MTFL: A self paced multi-task feature learning method for cognitive performance predicting of Alzheimer's disease},
year = {2021},
isbn = {9781450389686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3451421.3451427},
doi = {10.1145/3451421.3451427},
abstract = {Machine learning algorithms and multivariate data analysis methods have been widely utilized in the field of Alzheimer's disease (AD) research in recent years. Predicting cognitive performance of subjects from neuroimage measures and identifying relevant imaging biomarkers are important research topics in the study of Alzheimer's disease. Multi-task based feature learning (MTFL) have been widely studied to select a discriminative feature subset from MRI features, and improve the performance by incorporating inherent correlations among multiple clinical cognitive measures. Inspired by the fact that humans often learn from easy concepts to hard ones in the cognitive process, we propose a self-paced multi-task feature learning framework that attempts to learn the tasks by simultaneously taking into consideration the complexities of both tasks and instances per task in this study. Experimental results on ADNI are provided, and the comparison results demonstrate the effectiveness of our approach and show that our approach outperforms the state-of-the-art methods.},
booktitle = {The Fourth International Symposium on Image Computing and Digital Medicine},
pages = {23–27},
numpages = {5},
keywords = {regression, multi-task learning, Self-paced learning, Machine learning, Alzheimer's disease},
location = {Shenyang, China},
series = {ISICDM 2020}
}

@article{10.1007/s10772-017-9429-x,
author = {Phu, Vo Ngoc and Tran, Vo Thi and Chau, Vo Thi and Dat, Nguyen Duy and Duy, Khanh Ly},
title = {A decision tree using ID3 algorithm for English semantic analysis},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-017-9429-x},
doi = {10.1007/s10772-017-9429-x},
abstract = {Natural language processing has been studied for many years, and it has been applied to many researches and commercial applications. A new model is proposed in this paper, and is used in the English document-level emotional classification. In this survey, we proposed a new model by using an ID3 algorithm of a decision tree to classify semantics (positive, negative, and neutral) for the English documents. The semantic classification of our model is based on many rules which are generated by applying the ID3 algorithm to 115,000 English sentences of our English training data set. We test our new model on the English testing data set including 25,000 English documents, and achieve 63.6% accuracy of sentiment classification results.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {593–613},
numpages = {21},
keywords = {id3, Sentiment classification, ID3 algorithm, English sentiment classification, English document opinion mining, Decision tree}
}

@article{10.1023/A:1024416916577,
author = {P\v{e}chou\v{c}ek, Michal},
title = {Decision Planning Knowledge Representation Framework: A Case-Study},
year = {2003},
issue_date = {September 2003},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {39},
number = {1–2},
issn = {1012-2443},
url = {https://doi.org/10.1023/A:1024416916577},
doi = {10.1023/A:1024416916577},
abstract = {This paper discusses experiences and perspectives of utilisation of declarative knowledge structures as a convenient knowledge base medium in configuration expert systems. Although many successful systems have been developed, these are often difficult to maintain and to generalize in rapidly changing domains. In this paper we address the problem of building intelligent knowledge based systems with emphasis on their maintainability. Firstly, several industrial applications of proof planning, a theorem proving technique, will be described and their advantages and flaws will be discussed. This discussion is followed by the theoretical foundation of decision planning knowledge representation framework that, based on proof planning, facilitates separate administration of inference problem solving knowledge and the domain theory axioms. Machine learning methods for maintaining the inference knowledge to be up-to-date with permanently changing domain theory are commented and evaluated.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = sep,
pages = {147–174},
numpages = {28},
keywords = {theorem proving, multi-agent systems, machine learning, industrial configuration, expert systems}
}

@article{10.1504/ijguc.2021.119573,
author = {Ji, Hongbo and Wang, Mingyue and Sun, Mingwei and Liu, Qiang},
title = {Neural network classifier based on genetic algorithm image segmentation of subject robot optimisation system},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {12},
number = {4},
issn = {1741-847X},
url = {https://doi.org/10.1504/ijguc.2021.119573},
doi = {10.1504/ijguc.2021.119573},
abstract = {Robot optimisation system is a kind of complex, nonlinear, strong coupling system with serious uncertainty. The effect of image segmentation has become an important index to judge the merits of many algorithms. The purpose of this study is to explore the effect of neural network based on genetic algorithm on image segmentation in the optimisation system of classifier subject robot. The method used in this study is to calculate the pre trained VGGl6 NET model as the pre training model through the framework of genetic algorithm. The resolution of the training picture used is 640 * 480, the learning rate is 10−5, the value of batch size is l, the number of iterations is set to 12,000 and then the trained model is used to detect the image. The results show that the average error of group B of SNN trained by BP algorithm is 11.62%, the SNN trained by SGA has reduced the result to 9.75% and the error reduced to 7.75% by the genetic algorithm in this study. Moreover, genetic algorithm is better in feature point extraction, and the detection rate reaches 94.62%, which is higher than 77.53% and 88.74% of other methods. The missing rate of this study is only 3.04%, far lower than 12.49% and 7.36%. The conclusion is that our genetic algorithm has obvious advantages, small error, high efficiency and applicability. The neural network based on genetic algorithm in this study has a certain value in image segmentation technology.},
journal = {Int. J. Grid Util. Comput.},
month = jan,
pages = {369–379},
numpages = {10},
keywords = {feature point extraction, image segmentation, robot optimisation system, neural network classifier, genetic algorithm}
}

@article{10.1145/3345314,
author = {Wang, Qingyong and Zhou, Yun and Ding, Weiping and Zhang, Zhiguo and Muhammad, Khan and Cao, Zehong},
title = {Random Forest with Self-Paced Bootstrap Learning in Lung Cancer Prognosis},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3345314},
doi = {10.1145/3345314},
abstract = {Training gene expression data with supervised learning approaches can provide an alarm sign for early treatment of lung cancer to decrease death rates. However, the samples of gene features involve lots of noises in a realistic environment. In this study, we present a random forest with self-paced learning bootstrap for improvement of lung cancer classification and prognosis based on gene expression data. To be specific, we propose an ensemble learning with random forest approach to improving the model classification performance by selecting multi-classifiers. Then, we investigate the sampling strategy by gradually embedding from high- to low-quality samples by self-paced learning. The experimental results based on five public lung cancer datasets show that our proposed method could select significant genes exactly, which improves classification performance compared to that of existing approaches. We believe that our proposed method has the potential to assist doctors in gene selections and lung cancer prognosis.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {34},
numpages = {12},
keywords = {self-paced learning, random forest, classification, bootstrap, Lung cancer}
}

@article{10.1007/s00034-021-01674-0,
author = {Naiemi, Fatemeh and Ghods, Vahid and Khalesi, Hassan},
title = {MOSTL: An Accurate Multi-Oriented Scene Text Localization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {40},
number = {9},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-021-01674-0},
doi = {10.1007/s00034-021-01674-0},
abstract = {Automatic text localization in natural environments is the main element of many applications including self-driving cars, identifying vehicles, and providing scene information to visually impaired people. However, text in the natural and irregular scene has different degrees in orientations, shapes, and colors that make it difficult to detect. In this paper, an accurate multi-oriented scene text localization (MOSTL) is presented to obtain high efficiency of detecting text-based on convolutional neural networks. In the proposed method, an improved ReLU layer (i.ReLU) and an improved inception layer (i.inception) were introduced. Firstly, the proposed structure is used to extract low-level visual features. Then, an extra layer has been used to improve the feature extraction. The i.ReLU and i.inception layers have improved valuable information in text detection. The i.ReLU layers cause to extract some low-level features appropriately. The i.inception layers (specially 3 \texttimes{} 3 convolutions) can obtain broadly varying-sized text more effectively than a linear chain of convolution layer (without inception layers). The output of i.ReLU layers and i.inception layers was fed to an extra layer, which enables MOSTL to detect multi-oriented even curved and vertical texts. We conducted text detection experiments on well-known databases including ICDAR 2019, ICDAR 2017, ICDAR 2015, ICDAR 2003, and MSRA-TD500. MOSTL results yielded performance improvement remarkably.},
journal = {Circuits Syst. Signal Process.},
month = sep,
pages = {4452–4473},
numpages = {22},
keywords = {Curved text, Improved ReLU layer, Improved inception layer, Convolutional neural network, Multi-oriented, Object detection, Scene text localization}
}

@inproceedings{10.1145/3297156.3297211,
author = {Kim, Jae-Hong and Choi, Yang-Seo and Na, Jung-Chan},
title = {Cybersecurity Vulnerability Scanner for Digital Nuclear Power Plant Instrumentation and Control Systems},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297211},
doi = {10.1145/3297156.3297211},
abstract = {Nuclear Power Plant (NPP) Instrumentation and Control (I&amp;C) systems have been introduced for operational safety and accident prevention at nuclear facilities. Though the efficiency of these I&amp;C systems has increased through digitalization, digitalization poses a threat owing to cybersecurity issues that might occur. In the event of a cybersecurity incident affecting the NPP I&amp;C system, the stability of nuclear facilities can be threatened, and power supply could be interrupted. Therefore, various governments have established regulatory guidelines to improve the security of NPP I&amp;C systems; these guidelines stipulate periodic checking of the I&amp;C systems for cybersecurity vulnerabilities. Considering this, a vulnerability scanner for the nuclear environment is required. Therefore, in this study, we propose a cybersecurity vulnerability checking system to reduce the impact of such vulnerabilities as well as vulnerability checks on the NPP I&amp;C system to ensure compliance with the automatic check regulatory guidelines. In summary, based on our experimental results, we observed that the network traffic generated by our proposed vulnerability scanner generated lesser network traffic than the existing active network scanning method as well as reduced the overhead on the object to be scanned. In addition, our proposed approach enabled compliance checks for certain automatically scanned elements of the NPP I&amp;C systems during vulnerability scans; these automatically-scanned elements are defined in the regulatory guidelines.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {463–467},
numpages = {5},
keywords = {Vulnerability Scanner, RS-015, R.G 5.71, NPP Cyber Security},
location = {Shenzhen, China},
series = {CSAI '18}
}

@article{10.1023/A:1011096320004,
author = {Miguel, I. and Shen, Q.},
title = {Solution Techniques for Constraint Satisfaction Problems: Advanced Approaches},
year = {2001},
issue_date = {June 2001},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {4},
issn = {0269-2821},
url = {https://doi.org/10.1023/A:1011096320004},
doi = {10.1023/A:1011096320004},
abstract = {Conventional techniques for the constraint satisfaction problem (CSP)

have had considerable success in their applications. However,

there are many areas in which the performance of the basic approaches

may be improved. These include heuristic ordering of certain tasks

performed by the CSP solver, hybrids which combine compatible solution

techniques and graph based methods which exploit the structure of the

constraint graph representation of a CSP. Also, conventional

constraint satisfaction techniques only address problems with hard

constraints (i.e. each of which are completely satisfied or completely

violated, and all of which must be satisfied by a valid

solution). Many real applications require a more flexible approach

which relaxes somewhat these rigid requirements. To address these

issues various approaches have been developed. This paper attempts a

systematic review of them.},
journal = {Artif. Intell. Rev.},
month = jun,
pages = {269–293},
numpages = {25},
keywords = {ill-defined problems, hybrids, heuristics, graph-based methods, flexible constraint satisfaction, constraint satisfaction problems}
}

@inproceedings{10.1007/978-3-030-32047-8_26,
author = {Khoshmanesh, Seyedehzahra and Lutz, Robyn R.},
title = {Leveraging Feature Similarity for Earlier Detection of Unwanted Feature Interactions in Evolving Software Product Lines},
year = {2019},
isbn = {978-3-030-32046-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32047-8_26},
doi = {10.1007/978-3-030-32047-8_26},
abstract = {Software product lines enable reuse of shared software across a family of products. As new products are built in the product line, new features are added. The features are units of functionality that provide services to users. Unwanted feature interactions, wherein one feature interferes with another feature’s operation, is a significant problem, especially as large software product lines evolve. Detecting feature interactions is a time-consuming and difficult task for developers. Moreover, feature interactions are often only discovered during testing, at which point costly re-work is needed. This paper proposes a similarity-based method to identify unwanted feature interactions much earlier in the development process. It uses knowledge of prior feature interactions stored with the software product line’s feature model to help find unwanted interactions between a new feature and existing features. The paper describes the framework and algorithms used to detect the feature interactions using three path similarity measures and evaluates the approach on a real-world, evolving software product line. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.},
booktitle = {Similarity Search and Applications: 12th International Conference, SISAP 2019, Newark, NJ, USA, October 2–4, 2019, Proceedings},
pages = {293–307},
numpages = {15},
keywords = {Feature interaction, Similarity measures, Software product lines},
location = {Newark, NJ, USA}
}

@article{10.1155/2015/196098,
author = {Yang, Jinfeng and Xiao, Yong and Wang, Jiabing and Ma, Qianli and Shen, Yanhua},
title = {A fast clustering algorithm for data with a few labeled instances},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-5265},
url = {https://doi.org/10.1155/2015/196098},
doi = {10.1155/2015/196098},
abstract = {The diameter of a cluster is the maximum intracluster distance between pairs of instances within the same cluster, and the split of a cluster is the minimum distance between instances within the cluster and instances outside the cluster. Given a few labeled instances, this paper includes two aspects. First, we present a simple and fast clustering algorithm with the following property: if the ratio of the minimum split to the maximum diameter (RSD) of the optimal solution is greater than one, the algorithm returns optimal solutions for three clustering criteria. Second, we study the metric learning problem: learn a distance metric to make the RSD as large as possible. Compared with existing metric learning algorithms, one of our metric learning algorithms is computationally efficient: it is a linear programming model rather than a semidefinite programming model used by most of existing algorithms. We demonstrate empirically that the supervision and the learned metric can improve the clustering quality.},
journal = {Intell. Neuroscience},
month = jan,
articleno = {21},
numpages = {1}
}

@article{10.1016/j.ins.2019.02.051,
author = {Ros, Fr\'{e}d\'{e}ric and Guillaume, Serge},
title = {         Munec: a mutual neighbor-based clustering algorithm},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {486},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.02.051},
doi = {10.1016/j.ins.2019.02.051},
journal = {Inf. Sci.},
month = jun,
pages = {148–170},
numpages = {23},
keywords = {Clustering, Distance, Density, Single link, Mutual neighbors}
}

@inproceedings{10.5555/3172077.3172256,
author = {Ren, Yazhou and Zhao, Peng and Sheng, Yongpan and Yao, Dezhong and Xu, Zenglin},
title = {Robust softmax regression for multi-class classification with self-paced learning},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Softmax regression, a generalization of Logistic regression (LR) in the setting of multi-class classification, has been widely used in many machine learning applications. However, the performance of softmax regression is extremely sensitive to the presence of noisy data and outliers. To address this issue, we propose a model of robust softmax regression (RoSR) originated from the self-paced learning (SPL) paradigm for multi-class classification. Concretely, RoSR equipped with the soft weighting scheme is able to evaluate the importance of each data instance. Then, data instances participate in the classification problem according to their weights. In this way, the influence of noisy data and outliers (which are typically with small weights) can be significantly reduced. However, standard SPL may suffer from the imbalanced class influence problem, where some classes may have little influence in the training process if their instances are not sensitive to the loss. To alleviate this problem, we design two novel soft weighting schemes that assign weights and select instances locally for each class. Experimental results demonstrate the effectiveness of the proposed methods.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2641–2647},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1109/ICSE43902.2021.00094,
author = {Dey, Tapajit and Karnauch, Andrey and Mockus, Audris},
title = {Representation of Developer Expertise in Open Source Software},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00094},
doi = {10.1109/ICSE43902.2021.00094},
abstract = {Background: Accurate representation of developer expertise has always been an important research problem. While a number of studies proposed novel methods of representing expertise within individual projects, these methods are difficult to apply at an ecosystem level. However, with the focus of software development shifting from monolithic to modular, a method of representing developers' expertise in the context of the entire OSS development becomes necessary when, for example, a project tries to find new maintainers and look for developers with relevant skills. Aim: We aim to address this knowledge gap by proposing and constructing the Skill Space where each API, developer, and project is represented and postulate how the topology of this space should reflect what developers know (and projects need). Method: we use the World of Code infrastructure to extract the complete set of APIs in the files changed by open source developers and, based on that data, employ Doc2Vec embeddings for vector representations of APIs, developers, and projects. We then evaluate if these embeddings reflect the postulated topology of the Skill Space by predicting what new APIs/projects developers use/join, and whether or not their pull requests get accepted. We also check how the developers' representations in the Skill Space align with their self-reported API expertise. Result: Our results suggest that the proposed embeddings in the Skill Space appear to satisfy the postulated topology and we hope that such representations may aid in the construction of signals that increase trust (and efficiency) of open source ecosystems at large and may aid investigations of other phenomena related to developer proficiency and learning.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {995–1007},
numpages = {13},
keywords = {World of Code, Vector Embedding, Skill Space, Project embedding, Open Source, Machine Learning, Expertise, Doc2Vec, Developer embedding, Developer Expertise, API embedding, API},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1109/IRI.2018.00047,
author = {Xu, Ling and Wang, Bei and Liu, Ling and Zhou, Mo and Liao, Shengping and Yan, Meng},
title = {Misclassification Cost-Sensitive Software Defect Prediction},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IRI.2018.00047},
doi = {10.1109/IRI.2018.00047},
abstract = {Software defect prediction helps developers focus on defective modules for efficient software quality assurance. A common goal shared by existing software defect prediction methods is to attain low classification error rates. These proposals suffer from two practical problems: (i) Most of the prediction methods rely on a large number of labeled training data. However, collecting labeled data is a difficult and expensive task. It is hard to obtain classification labels over new software projects or existing projects without historical defect data. (ii) Software defect datasets are highly imbalanced. In many real-world applications, the misclassification cost of defective modules is generally several times higher than that of non-defective ones. In this paper, we present a misclassification Cost-sensitive approach to Software Defect Prediction (CSDP). The CSDP approach is novel in two aspects: First, CSDP addresses the problem of unlabeled software detect datasets by combining an unsupervised sampling method with a domain specific misclassification cost model. This preprocessing step selectively samples a small percentage of modules through estimating their classification labels. Second, CSDP builds a cost-sensitive support vector machine model to predict defect-proneness of the rest of modules with both overall classification error rate and domain specific misclassification cost as quality metrics. CSDP is evaluated on four NASA projects. Experimental results highlight three interesting observations: (1) CSDP achieves higher Normalized Expected Cost of Misclassification (NECM) compared with state-of-art supervised learning models under imbalanced training data with limited labeling. (2) CSDP outperforms state-of-art semi-supervised learning methods, which disregards classification costs, especially in recall rate. (3) CSDP enhanced through unsupervised sampling as a preprocessing step prior to training and prediction outperforms the baseline CSDP without the sampling process.},
booktitle = {2018 IEEE International Conference on Information Reuse and Integration (IRI)},
pages = {256–263},
numpages = {8},
location = {Salt Lake City, UT, USA}
}

@inproceedings{10.1007/978-3-030-89370-5_19,
author = {Luo, Chao and Bi, Sheng and Dong, Min and Nie, Hongxu},
title = {RGB-D Based Visual Navigation Using Direction Estimation Module},
year = {2021},
isbn = {978-3-030-89369-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89370-5_19},
doi = {10.1007/978-3-030-89370-5_19},
abstract = {Target-driven visual navigation without mapping works to solve navigation problems that given a target object, mobile robots can navigate to the target object. Recently, visual navigation has been researched and improved largely by learning-based methods. However, their methods lack depth information and spatial perception, using only single RGB images. To overcome these problems, two methods are presented in this paper. Firstly, we encode visual features of objects by dynamic graph convolutional network and extract 3D spatial features for objects by 3D geometry, a high level visual feature for agent to easily understand object relationship. Secondly, as human beings, they solve this problem in two steps, first exploring a new environment to find the target object and second planning a path to arrive. Inspired by the way of humans navigation, we propose direction estimation module (DEM) based on RGB-D images. DEM provides direction estimation of the target object to our learning model by a wheel odometry. Given a target object, first stage, our agent explores an unseen scene to detect the target object. Second stage, when detected the target object, we can estimate current location of the target object by 3D geometry, after that, each step of the agent, DEM will estimate new location of target object, and give direction information of the target object from a first-view image. It can guide our agent to navigate to the target object. Our experiment results outperforms the result of state of the art method in the artificial environment AI2-Thor.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III},
pages = {252–264},
numpages = {13},
keywords = {Reinforcement learning, Direction estimation module, Mobile robot, Visual navigation},
location = {Hanoi, Vietnam}
}

@article{10.1007/s11390-019-1960-6,
author = {Alqmase, Mohammed and Alshayeb, Mohammad and Ghouti, Lahouari},
title = {Threshold Extraction Framework for Software Metrics},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1960-6},
doi = {10.1007/s11390-019-1960-6},
abstract = {Software metrics are used to measure different attributes of software. To practically measure software attributes using these metrics, metric thresholds are needed. Many researchers attempted to identify these thresholds based on personal experiences. However, the resulted experience-based thresholds cannot be generalized due to the variability in personal experiences and the subjectivity of opinions. The goal of this paper is to propose an automated clustering framework based on the expectation maximization (EM) algorithm where clusters are generated using a simplified 3-metric set (LOC, LCOM, and CBO). Given these clusters, different threshold levels for software metrics are systematically determined such that each threshold reflects a specific level of software quality. The proposed framework comprises two major steps: the clustering step where the software quality historical dataset is decomposed into a fixed set of clusters using the EM algorithm, and the threshold extraction step where thresholds, specific to each software metric in the resulting clusters, are estimated using statistical measures such as the mean (μ) and the standard deviation (σ) of each software metric in each cluster. The paper’s findings highlight the capability of EM-based clustering, using a minimum metric set, to group software quality datasets according to different quality levels.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1063–1078},
numpages = {16},
keywords = {empirical study, expectation maximization, metric threshold}
}

@article{10.1007/s00500-015-2004-y,
author = {Qin, Jindong and Liu, Xinwang and Pedrycz, Witold},
title = {A multiple attribute interval type-2 fuzzy group decision making and its application to supplier selection with extended LINMAP method},
year = {2017},
issue_date = {June      2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {12},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-2004-y},
doi = {10.1007/s00500-015-2004-y},
abstract = {Supplier selection is a key issue in supply chain management, which directly impacts the manufacturer's performance. The problem can be viewed as a multiple attribute group decision making (MAGDM) that concerns many conflicting evaluation attributes, both being of qualitative and quantitative nature. Due to the increasing complexity and uncertainty of socio-economic environment, some evaluations of attributes are not adequately represented by numerical assessments and type-1 fuzzy sets. In this paper, we develop some linear programming models with the aid of multidimensional analysis of preference (LINMAP) method to solve interval type-2 fuzzy MAGDM problems, in which the information about attribute weights is incompletely known, and all pairwise comparison judgments over alternatives are represented by IT2FSs. First, we introduce a new distance measure based on the centroid interval between the IT2FSs. Then, we construct the linear programming model to determine the interval type-2 fuzzy positive ideal solution (IT2PIS) and corresponding attributes weight vector. Based on it, an extended LINMAP method to solve MAGDM problem under IT2FSs environment is developed. Finally, a supplier selection example is provided to demonstrate the usefulness of the proposed method.},
journal = {Soft Comput.},
month = jun,
pages = {3207–3226},
numpages = {20},
keywords = {Supplier selection, Multiple attribute group decision making (MAGDM), Linear programming techniques for multidimensional analysis of preference (LINMAP) method, Interval type-2 fuzzy sets (IT2FSs)}
}

@inproceedings{10.1007/978-3-030-87199-4_50,
author = {Sedlar, Sara and Alimi, Abib and Papadopoulo, Th\'{e}odore and Deriche, Rachid and Deslauriers-Gauthier, Samuel},
title = {A Spherical Convolutional Neural Network for White Matter Structure Imaging via dMRI},
year = {2021},
isbn = {978-3-030-87198-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87199-4_50},
doi = {10.1007/978-3-030-87199-4_50},
abstract = {Diffusion Magnetic Resonance Imaging (dMRI) is a powerful non-invasive and in-vivo imaging modality for probing brain white matter structure. Convolutional neural networks (CNNs) have been shown to be a powerful tool for many computer vision problems where the signals are acquired on a regular grid and where translational invariance is important. However, as we are considering dMRI signals that are acquired on a sphere, rotational invariance, rather than translational, is desired. In this work, we propose a spherical CNN model with fully spectral domain convolutional and non-linear layers. It provides rotational invariance and is adapted to the real nature of dMRI signals and uniform random distribution of sampling points. The proposed model is positively evaluated on the problem of estimation of neurite orientation dispersion and density imaging (NODDI) parameters on the data from Human Connectome Project (HCP).},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part III},
pages = {529–539},
numpages = {11},
keywords = {White matter micro-structures, Diffusion MRI, Spherical CNN},
location = {Strasbourg, France}
}

@article{10.1016/j.jss.2019.110424,
author = {Gavidia-Calderon, Carlos and Sarro, Federica and Harman, Mark and Barr, Earl T.},
title = {Game-theoretic analysis of development practices: Challenges and opportunities},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110424},
doi = {10.1016/j.jss.2019.110424},
journal = {J. Syst. Softw.},
month = jan,
numpages = {9},
keywords = {Software engineering practices, Technical debt, Empirical analysis, Game theory}
}

@article{10.1016/j.jss.2019.03.027,
author = {Xu, Zhou and Li, Shuai and Luo, Xiapu and Liu, Jin and Zhang, Tao and Tang, Yutian and Xu, Jun and Yuan, Peipei and Keung, Jacky},
title = {TSTSS: A two-stage training subset selection framework for cross version defect prediction},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.027},
doi = {10.1016/j.jss.2019.03.027},
journal = {J. Syst. Softw.},
month = aug,
pages = {59–78},
numpages = {20},
keywords = {99-00, 00–01, Weighted extreme learning machine, Training subset selection, Spare modeling, Cross version defect prediction}
}

@inproceedings{10.1145/1109128.1109132,
author = {van der Storm, Tijs},
title = {Continuous release and upgrade of component-based software},
year = {2005},
isbn = {1595933107},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1109128.1109132},
doi = {10.1145/1109128.1109132},
abstract = {We show how under certain assumptions, the release and delivery of software updates can be automated in the context of component-based systems. These updates allow features or fixes to be delivered to users more quickly. Furthermore, user feedback is more accurate, thus enabling quicker response to defects encountered in the field.Based on a formal product model we extend the process of continuous integration to enable the agile and automatic release of software components component. From such releases traceable and incremental updates are derived.We have validated our solution with a prototype tool that computes and delivers updates for a component-based software system developed at CWI.},
booktitle = {Proceedings of the 12th International Workshop on Software Configuration Management},
pages = {43–57},
numpages = {15},
location = {Lisbon, Portugal},
series = {SCM '05}
}

@article{10.1016/j.patrec.2021.08.011,
author = {Mehta, Nancy and Murala, Subrahmanyam},
title = {MSAR-Net: Multi-scale attention based light-weight image super-resolution},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.08.011},
doi = {10.1016/j.patrec.2021.08.011},
journal = {Pattern Recogn. Lett.},
month = nov,
pages = {215–221},
numpages = {7},
keywords = {65D17, 65D05, 41A10, 41A05, Image super-resolution, Up and down-sampling projection block, Multi-scale attention residual block}
}

@inproceedings{10.1007/978-3-030-69532-3_29,
author = {Priisalu, Maria and Paduraru, Ciprian and Pirinen, Aleksis and Sminchisescu, Cristian},
title = {Semantic Synthesis of Pedestrian Locomotion},
year = {2020},
isbn = {978-3-030-69531-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-69532-3_29},
doi = {10.1007/978-3-030-69532-3_29},
abstract = {We present a model for generating 3d articulated pedestrian locomotion in urban scenarios, with synthesis capabilities informed by the 3d scene semantics and geometry. We reformulate pedestrian trajectory forecasting as a structured reinforcement learning (RL) problem. This allows us to naturally combine prior knowledge on collision avoidance, 3d human motion capture and the motion of pedestrians as observed e.g. in Cityscapes, Waymo or simulation environments like Carla. Our proposed RL-based model allows pedestrians to accelerate and slow down to avoid imminent danger (e.g. cars), while obeying human dynamics learnt from in-lab motion capture datasets. Specifically, we propose a hierarchical model consisting of a semantic trajectory policy network that provides a distribution over possible movements, and a human locomotion network that generates 3d human poses in each step. The RL-formulation allows the model to learn even from states that are seldom exhibited in the dataset, utilizing all of the available prior and scene information. Extensive evaluations using both real and simulated data illustrate that the proposed model is on par with recent models such as S-GAN, ST-GAT and S-STGCNN in pedestrian forecasting, while outperforming these in collision avoidance. We also show that our model can be used to plan goal reaching trajectories in urban scenes with dynamic actors.},
booktitle = {Computer Vision – ACCV 2020: 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 – December 4, 2020, Revised Selected Papers, Part II},
pages = {470–487},
numpages = {18},
location = {Kyoto, Japan}
}

@inproceedings{10.5555/1563601.1563678,
author = {Scheuer, Oliver and Zinn, Claus},
title = {How did the e-learning session go? The Student Inspector},
year = {2007},
isbn = {9781586037642},
publisher = {IOS Press},
address = {NLD},
abstract = {Good teachers know their students, and exploit this knowledge to adapt or optimise their instruction. Traditional teachers know their students because they interact with them face-to-face in classroom or one-to-one tutoring sessions. In these settings, they can build student models, i.e., by exploiting the multi-faceted nature of human-human communication. In distance-learning contexts, teacher and student have to cope with the lack of such direct interaction, and this must have detrimental effects for both teacher and student. In a past study we have analysed teacher requirements for tracking student actions in computer-mediated settings. Given the results of this study, we have devised and implemented a tool that allows teachers to keep track of their learners' interaction in e-learning systems. We present the tool's functionality and user interfaces, and an evaluation of its usability.},
booktitle = {Proceedings of the 2007 Conference on Artificial Intelligence in Education: Building Technology Rich Learning Contexts That Work},
pages = {487–494},
numpages = {8},
keywords = {Tools And Techniques For Effective Evaluation Of Cognitive, Methods, Meta-Cognitive And Affective Issues, Log Data Analysis, Distance-Learning Contexts}
}

@inproceedings{10.1145/3180465.3180469,
author = {Nadig, Deepak and Ramamurthy, Byrav and Bockelman, Brian and Swanson, David},
title = {Identifying Anomalies in GridFTP transfers for Data-Intensive Science through Application-Awareness},
year = {2018},
isbn = {9781450356350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180465.3180469},
doi = {10.1145/3180465.3180469},
abstract = {Network anomaly detection systems can be used to identify anomalous transfers or threats, which, when undetected, can trigger large-scale malicious events. Data-intensive science projects rely on high-throughput computing and high-speed networking resources for data analysis and processing. In this paper, we propose an anomaly detection framework and architecture for identifying anomalies in GridFTP transfers. Application-awareness plays an important role in our proposed architecture and is used to communicate GridFTP application metadata to the machine learning and anomaly detection system. We demonstrate the effectiveness of our architecture by evaluating the framework with a real-world, large-scale dataset of GridFTP transfers. Preliminary results show that our framework can be used to develop novel anomaly detection services with diverse feature sets for distributed and data-intensive projects.},
booktitle = {Proceedings of the 2018 ACM International Workshop on Security in Software Defined Networks &amp; Network Function Virtualization},
pages = {7–12},
numpages = {6},
keywords = {software defined networks., gridftp, application-awareness, anomaly detection},
location = {Tempe, AZ, USA},
series = {SDN-NFV Sec'18}
}

@inproceedings{10.1007/978-3-030-58571-6_2,
author = {Du, Heming and Yu, Xin and Zheng, Liang},
title = {Learning Object Relation Graph and Tentative Policy for Visual Navigation},
year = {2020},
isbn = {978-3-030-58570-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58571-6_2},
doi = {10.1007/978-3-030-58571-6_2},
abstract = {Target-driven visual navigation aims at navigating an agent towards a given target based on the observation of the agent. In this task, it is critical to learn informative visual representation and robust navigation policy. Aiming to improve these two components, this paper proposes three complementary techniques, object relation graph (ORG), trial-driven imitation learning (IL), and a memory-augmented tentative policy network (TPN). ORG improves visual representation learning by integrating object relationships, including category closeness and spatial correlations, e.g., a TV usually co-occurs with a remote spatially. Both Trial-driven IL and TPN underlie robust navigation policy, instructing the agent to escape from deadlock states, such as looping or being stuck. Specifically, trial-driven IL is a type of supervision used in policy network training, while TPN, mimicking the IL supervision in unseen environment, is applied in testing. Experiment in the artificial environment AI2-Thor validates that each of the techniques is effective. When combined, the techniques bring significantly improvement over baseline methods in navigation effectiveness and efficiency in unseen environments. We report 22.8% and 23.5% increase in success rate and Success weighted by Path Length (SPL), respectively. The code is available at .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VII},
pages = {19–34},
numpages = {16},
keywords = {Visual navigation, Tentative policy learning, Imitation learning, Graph},
location = {Glasgow, United Kingdom}
}

@article{10.1007/s11219-021-09553-2,
author = {Wu, Jie and Wu , Yingbo and Niu, Nan and Zhou, Min},
title = {MHCPDP: multi-source heterogeneous cross-project defect prediction via multi-source transfer learning and autoencoder},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09553-2},
doi = {10.1007/s11219-021-09553-2},
abstract = {Heterogeneous cross-project defect prediction (HCPDP) is aimed at building a defect prediction model for the target project by reusing datasets from source projects, where the source project datasets and target project dataset have different features. Most existing HCPDP methods only remove redundant or unrelated features without exploring the underlying features of cross-project datasets. Additionally, when the&nbsp;transfer learning method is used in HCPDP, these methods ignore the negative effect of transfer learning. In this paper, we propose a novel HCPDP method called multi-source heterogeneous cross-project defect prediction (MHCPDP). To reduce the gap between the target datasets and the source datasets, MHCPDP uses the autoencoder to extract the intermediate features from the original datasets instead of simply removing redundant and unrelated features and adopts a modified autoencoder algorithm to make instance selection for eliminating irrelevant instances from the source domain datasets. Furthermore, by incorporating multiple source projects to increase the number of source datasets, MHCPDP develops a multi-source transfer learning algorithm to reduce the impact of negative transfers and upgrade the performance of the classifier. We comprehensively evaluate MHCPDP on five open source datasets; our experimental results show that MHCPDP not only has significant improvement in two performance metrics but also overcomes the shortcomings of the conventional HCPDP methods.},
journal = {Software Quality Journal},
month = jun,
pages = {405–430},
numpages = {26},
keywords = {Modified autoencoder, Multi-source transfer learning, Heterogeneous cross-project defect prediction, Autoencoder}
}

@article{10.1016/j.eswa.2019.03.031,
author = {Ros, Fr\'{e}d\'{e}ric and Guillaume, Serge},
title = {A hierarchical clustering algorithm and an improvement of the single linkage criterion to deal with noise},
year = {2019},
issue_date = {Aug 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.03.031},
doi = {10.1016/j.eswa.2019.03.031},
journal = {Expert Syst. Appl.},
month = aug,
pages = {96–108},
numpages = {13},
keywords = {Density, Dissimilarity, Agglomerative}
}

@phdthesis{10.5555/AAI28499518,
author = {Tang, Yiming and Shankar, Subash and Raja, Anita and Bagherzadeh, Mehdi},
advisor = {Raffi, Khatchadourian,},
title = {Towards Automated Software Evolution of Data-Intensive Applications},
year = {2021},
isbn = {9798515256678},
publisher = {City University of New York},
address = {USA},
abstract = {Recent years have witnessed an explosion of work on Big Data. Data-intensive applications analyze and produce large volumes of data typically terabyte and petabyte in size. Many techniques for facilitating data processing are integrated into data-intensive applications. API is a software interface that allows two applications to communicate with each other. Streaming APIs are widely used in today's Object-Oriented programming development that can support parallel processing. In this dissertation, an approach that automatically suggests stream code run in parallel or sequentially is proposed. However, using streams efficiently and properly needs many subtle considerations. The use and misuse patterns for stream codes are proposed in this dissertation. Modern software, especially for highly transactional software systems, generates vast logging information every day. The huge amount of information prevents developers from receiving useful information effectively. Log-level could be used to filter run-time information. This dissertation proposes an automated evolution approach for alleviating logging information overload by rejuvenating log levels according to developers' interests. Machine Learning (ML) systems are pervasive in today's software society. They are always complex and can process large volumes of data. Due to the complexity of ML systems, they are prone to classic technical debt issues, but how ML systems evolve is still a puzzling problem. This dissertation introduces ML-specific refactoring and technical debt for solving this problem.},
note = {AAI28499518}
}

@inproceedings{10.1007/978-3-030-39306-9_3,
author = {Capizzi, Antonio and Distefano, Salvatore and Ara\'{u}jo, Luiz J. P. and Mazzara, Manuel and Ahmad, Muhammad and Bobrov, Evgeny},
title = {Anomaly Detection in DevOps Toolchain},
year = {2019},
isbn = {978-3-030-39305-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-39306-9_3},
doi = {10.1007/978-3-030-39306-9_3},
abstract = {The tools employed in the DevOps Toolchain generates a large quantity of data that is typically ignored or inspected only on particular occasions, at most. However, the analysis of such data could enable the extraction of useful information about the status and evolution of the project. For example, metrics like the “lines of code added since the last release” or “failures detected in the staging environment” are good indicators for predicting potential risks in the incoming release. In order to prevent problems appearing in later stages of production, an anomaly detection system can operate in the staging environment to compare the current incoming release with previous ones according to predefined metrics. The analysis is conducted before going into production to identify anomalies which should be addressed by human operators that address false-positive and negatives that can appear. In this paper, we describe a prototypical implementation of the aforementioned idea in the form of a “proof of concept”. The current study effectively demonstrates the feasibility of the approach for a set of implemented functionalities.},
booktitle = {Software Engineering Aspects of Continuous Development and New Paradigms of Software Production and Deployment: Second International Workshop, DEVOPS 2019, Ch\^{a}teau de Villebrumier, France, May 6–8, 2019, Revised Selected Papers},
pages = {37–51},
numpages = {15},
location = {Villebrumier, France}
}

@inproceedings{10.1145/3377812.3381399,
author = {Abbas, Muhammad},
title = {Variability aware requirements reuse analysis},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381399},
doi = {10.1145/3377812.3381399},
abstract = {Problem: The goal of a software product line is to aid quick and quality delivery of software products, sharing common features. Effectively achieving the above-mentioned goals requires reuse analysis of the product line features. Existing requirements reuse analysis approaches are not focused on recommending product line features, that can be reused to realize new customer requirements. Hypothesis: Given that the customer requirements are linked to product line features' description satisfying them: then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customer requirements can be evaluated against existing customer requirements and reuse of product line features can be recommended. Contributions: We treated the problem of feature reuse analysis as a text classification problem at the requirements-level. We use Natural Language Processing and clustering to recommend reuse of features based on similarities and historic reuse information. The recommendations can be used to realize new customer requirements.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {190–193},
numpages = {4},
keywords = {variability, software reuse, similarities, requirements, product line},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1007/978-3-030-87013-3_4,
author = {Gupta, Himanshu and Gulanikar, Abhiram Anand and Kumar, Lov and Neti, Lalita Bhanu Murthy},
title = {Empirical Analysis on Effectiveness of&nbsp;NLP Methods for Predicting Code Smell},
year = {2021},
isbn = {978-3-030-87012-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87013-3_4},
doi = {10.1007/978-3-030-87013-3_4},
abstract = {A code smell is a surface indicator of an inherent problem in the system, most often due to deviation from standard coding practices on the developer’s part during the development phase. Studies observe that code smells made the code more susceptible to call for modifications and corrections than code that did not contain code smells. Restructuring the code at the early stage of development saves the exponentially increasing amount of effort it would require to address the issues stemming from the presence of these code smells. Instead of using traditional features to detect code smells, we use user comments (given on the packages’ repositories) to manually construct features to predict code smells. We use three Extreme learning machine kernels over 629 packages to identify eight code smells by leveraging feature engineering aspects and using sampling techniques. Our findings indicate that the radial basis functional kernel performs best out of the three kernel methods with a mean accuracy of 98.52.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part IX},
pages = {43–53},
numpages = {11},
keywords = {Radial basis kernel, Natural language processing, Extreme learning machine},
location = {Cagliari, Italy}
}

@article{10.5555/2051237.2051253,
author = {Gra\c{c}a, Jo\~{a}o V. and Ganchev, Kuzman and Coheur, Lu\'{\i}sa and Pereira, Fernando and Taskar, Ben},
title = {Controlling complexity in part-of-speech induction},
year = {2011},
issue_date = {May 2011},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {41},
number = {2},
issn = {1076-9757},
abstract = {We consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. The standard maximum-likelihood hidden Markov model for this task performs poorly, because of its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning objective to control its capacity via parametric and non-parametric constraints. Our approach enforces word-category association sparsity, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our experiments on five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve significant improvements compared with previous methods for the same task.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {527–551},
numpages = {25}
}

@article{10.1145/1183236.1183254,
author = {Blank, Douglas},
title = {Robots make computer science personal},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183254},
doi = {10.1145/1183236.1183254},
abstract = {They also make it more hands-on, real, practical, and immediate, inspiring a new generation of scientists' deep interest in the field.},
journal = {Commun. ACM},
month = dec,
pages = {25–27},
numpages = {3}
}

@inproceedings{10.1007/978-3-030-69532-3_4,
author = {Huang, Bowen and Zhou, Jinjia and Yan, Xiao and Jing, Ming’e and Wan, Rentao and Fan, Yibo},
title = {CS-MCNet: A Video Compressive Sensing Reconstruction Network with Interpretable Motion Compensation},
year = {2020},
isbn = {978-3-030-69531-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-69532-3_4},
doi = {10.1007/978-3-030-69532-3_4},
abstract = {In this paper, a deep neural network with interpretable motion compensation called CS-MCNet is proposed to realize high-quality and real-time decoding of video compressive sensing. Firstly, explicit multi-hypothesis motion compensation is applied in our network to extract correlation information of adjacent frames (as shown in Fig.&nbsp;1), which improves the recover performance. And then, a residual module further narrows down the gap between reconstruction result and original signal. The overall architecture is interpretable by using algorithm unrolling, which brings the benefits of being able to transfer prior knowledge about the conventional algorithms. As a result, a PSNR of 22&nbsp;dB can be achieved at 64x compression ratio, which is about 4% to 9% better than state-of-the-art methods. In addition, due to the feed-forward architecture, the reconstruction can be processed by our network in real time and up&nbsp;to three orders of magnitude faster than traditional iterative methods.},
booktitle = {Computer Vision – ACCV 2020: 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 – December 4, 2020, Revised Selected Papers, Part II},
pages = {54–67},
numpages = {14},
location = {Kyoto, Japan}
}

@article{10.1504/IJBRA.2018.092685,
title = {Subspace module extraction from MI-based co-expression network},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {14},
number = {3},
issn = {1744-5485},
url = {https://doi.org/10.1504/IJBRA.2018.092685},
doi = {10.1504/IJBRA.2018.092685},
abstract = {Most of the existing methods in literature have used proximity measures in the construction of co-expression networks CEN consisting of functional gene modules. This work describes the construction of co-expression network using mutual information MI as a proximity measure with non-linear correlation. The network modules are extracted that are defined over a subset of samples. This method has been tested on several publicly available datasets and the subspace network modules obtained have been validated in terms of both internal and external measures.},
journal = {Int. J. Bioinformatics Res. Appl.},
month = jan,
pages = {207–234},
numpages = {28}
}

@inproceedings{10.5555/3540261.3542303,
author = {Hahn, Meera and Chaplot, Devendra and Tulsiani, Shubham and Mukadam, Mustafa and Rehg, James M. and Gupta, Abhinav},
title = {No RL, no simulation: learning to navigate without navigating},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most prior methods for learning navigation policies require access to simulation environments, as they need online policy interaction and rely on ground-truth maps for rewards. However, building simulators is expensive (requires manual effort for each and every scene) and creates challenges in transferring learned policies to robotic platforms in the real-world, due to the sim-to-real domain gap. In this paper, we pose a simple question: Do we really need active interaction, ground-truth maps or even reinforcement-learning (RL) in order to solve the image-goal navigation task? We propose a self-supervised approach to learn to navigate from only passive videos of roaming. Our approach, No RL, No Simulator (NRNS), is simple and scalable, yet highly effective. NRNS outperforms RL-based formulations by a significant margin. We present NRNS as a strong baseline for any future image-based navigation tasks that use RL or Simulation.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2042},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.5555/3495724.3496497,
author = {Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {Self-paced deep reinforcement learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Curriculum reinforcement learning (CRL) improves the learning speed and stability of an agent by exposing it to a tailored series of tasks throughout learning. Despite empirical successes, an open question in CRL is how to automatically generate a curriculum for a given reinforcement learning (RL) agent, avoiding manual design. In this paper, we propose an answer by interpreting the curriculum generation as an inference problem, where distributions over tasks are progressively learned to approach the target task. This approach leads to an automatic curriculum generation, whose pace is controlled by the agent, with solid theoretical motivation and easily integrated with deep RL algorithms. In the conducted experiments, the curricula generated with the proposed algorithm significantly improve learning performance across several environments and deep RL algorithms, matching or outperforming state-of-the-art existing CRL algorithms.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {773},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Software product lines, SPL Conqueror, Non-functional properties, Measurement and optimization, Feature-oriented software development}
}

@article{10.1007/s11280-018-0622-x,
author = {Wen, Guoqiu and Zhu, Yonghua and Cai, Zhiguo and Zheng, Wei},
title = {Self-tuning clustering for high-dimensional data},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-018-0622-x},
doi = {10.1007/s11280-018-0622-x},
abstract = {Spectral clustering is an important component of clustering method, via tightly relying on the affinity matrix. However, conventional spectral clustering methods 1). equally treat each data point, so that easily affected by the outliers; 2). are sensitive to the initialization; 3). need to specify the number of cluster. To conquer these problems, we have proposed a novel spectral clustering algorithm, via employing an affinity matrix learning to learn an intrinsic affinity matrix, using the local PCA to resolve the intersections; and further taking advantage of a robust clustering that is insensitive to initialization to automatically generate clusters without an input of number of cluster. Experimental results on both artificial and real high-dimensional datasets have exhibited our proposed method outperforms the clustering methods under comparison in term of four clustering metrics.},
journal = {World Wide Web},
month = nov,
pages = {1563–1573},
numpages = {11},
keywords = {Spectral clustering, Multi-manifold clustering, Local PCA, High-dimensional data}
}

@inproceedings{10.23919/ICCAS50221.2020.9268247,
author = {Yoo, Hwiyeon and Kim, Nuri and Park, Jeongho and Oh, Songhwai},
title = {Path-Following Navigation Network Using Sparse Visual Memory},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.23919/ICCAS50221.2020.9268247},
doi = {10.23919/ICCAS50221.2020.9268247},
abstract = {Following a demonstration path without observing exact location of an agent is a challenging navigation problem. Especially, considering the probabilistic transition function of the agent makes the problem hard to solve with an exact action decision, so learning-based approaches have been used to solve this task. For example, a previous method by Kumar and Gupta et al., robust path following network (RPF), is a neural-network-based method using visual memories of the demonstration. Although the RPF shows good performances on the path-following task, it does not consider the efficiency of the visual memory since it requires the entire visual memory of the demonstration. In this paper, we propose a path-following network using sparse memory of the demonstration path that can deal with various sparsity of the visual memory. For each time step, the proposed network makes soft attention on the sparse memory to control the agent. We test the proposed model on the Habitat simulator using MatterPort3D dataset with various sparsity of memory. The experimental results show that the proposed method achieves 81.9% of success rate and 73.7% of SPL on a model with 0.8 memory sparsity, and also the results of the models with other memory sparsity achieve reasonable performances compare to the baseline methods.},
booktitle = {2020 20th International Conference on Control, Automation and Systems (ICCAS)},
pages = {883–886},
numpages = {4},
location = {Busan, Korea (South)}
}

@article{10.1016/0004-3702(92)90007-K,
author = {Minton, Steven and Johnston, Mark D. and Philips, Andrew B. and Laird, Philip},
title = {Minimizing conflicts: a heuristic repair method for constraint satisfaction and scheduling problems},
year = {1992},
issue_date = {Dec. 1992},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {58},
number = {1–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(92)90007-K},
doi = {10.1016/0004-3702(92)90007-K},
journal = {Artif. Intell.},
month = dec,
pages = {161–205},
numpages = {45}
}

@inproceedings{10.1007/978-3-030-48077-6_3,
author = {Claris\'{o}, Robert and Cabot, Jordi},
title = {Diverse Scenario Exploration in Model Finders Using Graph Kernels and Clustering},
year = {2020},
isbn = {978-3-030-48076-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-48077-6_3},
doi = {10.1007/978-3-030-48077-6_3},
abstract = {Complex software systems can be described using modeling notations such as UML/OCL or Alloy. Then, some correctness properties of these systems can be checked using model finders, which compute sample scenarios either fulfilling the desired properties or illustrating potential faults. Such scenarios allow designers to validate, verify and test the system under development.Nevertheless, when asked to produce several scenarios, model finders tend to produce similar solutions. This lack of diversity impairs their effectiveness as testing or validation assets. To solve this problem, we propose the use of graph kernels, a family of methods for computing the (dis)similarity among pairs of graphs. With this metric, it is possible to cluster scenarios effectively, improving the usability of model finders and making testing and validation more efficient.},
booktitle = {Rigorous State-Based Methods: 7th International Conference, ABZ 2020, Ulm, Germany, May 27–29, 2020, Proceedings},
pages = {27–43},
numpages = {17},
keywords = {Diversity, Clustering, Graph kernels, Testing, Verification and validation, Model-driven engineering},
location = {Ulm, Germany}
}

@inproceedings{10.5555/3505326.3505356,
author = {Ravari, Yaser Norouzzadeh and Spronck, Pieter and Sifa, Rafet and Drachen, Anders},
title = {Predicting victory in a hybrid online competitive game: the case of Destiny},
year = {2017},
isbn = {978-1-57735-791-9},
publisher = {AAAI Press},
abstract = {Competitive multi-player game play is a common feature in major commercial titles, and has formed the foundation for esports. In this paper, the question whether it is possible to predict match outcomes in First Person Shooter-type multiplayer competitive games with mixed genres is addressed. The case employed is Destiny, which forms a hybrid title combining Massively Multi-player Online Role-Playing game features and First-Person Shooter games. Destiny provides the opportunity to investigate prediction of the match outcome, as well as the influence of performance metrics on the match results in a hybrid multi-player major commercial title. Two groups of models are presented for predicting match results: One group predicts match results for each individual game mode and the other group predicts match results in general, without considering specific game modes. Models achieve a performance between 63% and 99% in terms of average precision, with a higher performance recorded for the models trained on specific multi-player game modes, of which Destiny has several. We also analyzed performance metrics and their influence for each model. The results show that many key shooter performance metrics such as Kill/Death ratio are relevant across game modes, but also that some performance metrics are mainly important for specific competitive game modes. The results indicate that reliable match prediction is possible in FPS-type esports games.},
booktitle = {Proceedings of the Thirteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
articleno = {30},
numpages = {7},
location = {Little Cottonwood Canyon, Utah, USA},
series = {AIIDE'17}
}

@article{10.1007/s11192-019-03307-5,
author = {Tattershall, E. and Nenadic, G. and Stevens, R. D.},
title = {Detecting bursty terms in computer science research},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {122},
number = {1},
issn = {0138-9130},
url = {https://doi.org/10.1007/s11192-019-03307-5},
doi = {10.1007/s11192-019-03307-5},
abstract = {Research topics rise and fall in popularity over time, some more swiftly than others. The fastest rising topics are typically called bursts; for example “deep learning”, “internet of things” and “big data”. Being able to automatically detect and track bursty terms in the literature could give insight into how scientific thought evolves over time. In this paper, we take a trend detection algorithm from stock market analysis and apply it to over 30&nbsp;years of computer science research abstracts, treating the prevalence of each term in the dataset like the price of a stock. Unlike previous work in this domain, we use the free text of abstracts and titles, resulting in a finer-grained analysis. We report a list of bursty terms, and then use historical data to build a classifier to predict whether they will rise or fall in popularity in the future, obtaining accuracy in the region of 80%. The proposed methodology can be applied to any time-ordered collection of text to yield past and present bursty terms and predict their probable fate.},
journal = {Scientometrics},
month = jan,
pages = {681–699},
numpages = {19},
keywords = {MACD, DBLP, Machine learning, Term life cycles, Bibliometrics, Computer science}
}

@inproceedings{10.1007/978-3-030-98682-7_11,
author = {Blumenkamp, Jan and Baude, Andreas and Laue, Tim},
title = {Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_11},
doi = {10.1007/978-3-030-98682-7_11},
abstract = {Deep learning approaches have become the standard solution to many problems in computer vision and robotics, but obtaining sufficient training data in high enough quality is challenging, as human labor is error prone, time consuming, and expensive. Solutions based on simulation have become more popular in recent years, but the gap between simulation and reality is still a major issue. In this paper, we introduce a novel method for augmenting synthetic image data through unsupervised image-to-image translation by applying the style of real world images to simulated images with open source frameworks. The generated dataset is combined with conventional augmentation methods and is then applied to a neural network model running in real-time on autonomous soccer robots. Our evaluation shows a significant improvement compared to models trained on images generated entirely in simulation.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {127–139},
numpages = {13},
location = {Sydney, NSW, Australia}
}

@article{10.1145/3433949,
author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
title = {The (Im)possibility of fairness: different value systems require different mechanisms for fair decision making},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3433949},
doi = {10.1145/3433949},
abstract = {What does it mean to be fair?},
journal = {Commun. ACM},
month = mar,
pages = {136–143},
numpages = {8}
}

@article{10.1016/j.eswa.2016.01.035,
author = {Xu, Jingxin and Denman, Simon and Fookes, Clinton and Sridharan, Sridha},
title = {Detecting rare events using Kullback-Leibler divergence},
year = {2016},
issue_date = {July 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {54},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.01.035},
doi = {10.1016/j.eswa.2016.01.035},
abstract = {We present a weakly supervised approach for rare event detection.Coarse annotation, denoting only roughly when an event occurs is needed.The approach leverages the rare nature of the target events to its advantage.We demonstrate the proposed approach on the popular MIT traffic dataset.State-of-the-art performance is shown, alongside being real-time capable. Video surveillance infrastructure has been widely installed in public places for security purposes. However, live video feeds are typically monitored by human staff, making the detection of important events as they occur difficult. As such, an expert system that can automatically detect events of interest in surveillance footage is highly desirable. Although a number of approaches have been proposed, they have significant limitations: supervised approaches, which can detect a specific event, ideally require a large number of samples with the event spatially and temporally localised; while unsupervised approaches, which do not require this demanding annotation, can only detect whether an event is abnormal and not specific event types. To overcome these problems, we formulate a weakly-supervised approach using Kullback-Leibler (KL) divergence to detect rare events. The proposed approach leverages the sparse nature of the target events to its advantage, and we show that this data imbalance guarantees the existence of a decision boundary to separate samples that contain the target event from those that do not. This trait, combined with the coarse annotation used by weakly supervised learning (that only indicates approximately when an event occurs), greatly reduces the annotation burden while retaining the ability to detect specific events. Furthermore, the proposed classifier requires only a decision threshold, simplifying its use compared to other weakly supervised approaches. We show that the proposed approach outperforms state-of-the-art methods on a popular real-world traffic surveillance dataset, while preserving real time performance.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {13–28},
numpages = {16},
keywords = {Weakly supervised learning, Kullback-Leibler divergence, Event detection, Anomaly detection}
}

@article{10.1016/j.jbi.2008.12.012,
author = {Saha, Sujan Kumar and Sarkar, Sudeshna and Mitra, Pabitra},
title = {Feature selection techniques for maximum entropy based biomedical named entity recognition},
year = {2009},
issue_date = {October, 2009},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {42},
number = {5},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2008.12.012},
doi = {10.1016/j.jbi.2008.12.012},
abstract = {Named entity recognition is an extremely important and fundamental task of biomedical text mining. Biomedical named entities include mentions of proteins, genes, DNA, RNA, etc which often have complex structures, but it is challenging to identify and classify such entities. Machine learning methods like CRF, MEMM and SVM have been widely used for learning to recognize such entities from an annotated corpus. The identification of appropriate feature templates and the selection of the important feature values play a very important role in the success of these methods. In this paper, we provide a study on word clustering and selection based feature reduction approaches for named entity recognition using a maximum entropy classifier. The identification and selection of features are largely done automatically without using domain knowledge. The performance of the system is found to be superior to existing systems which do not use domain knowledge.},
journal = {J. of Biomedical Informatics},
month = oct,
pages = {905–911},
numpages = {7},
keywords = {Maximum entropy classifier, Machine learning, Feature selection, Feature reduction, Biomedical named entity recognition}
}

@inproceedings{10.1145/3377812.3382153,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {FeatureNET: diversity-driven generation of deep learning models},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382153},
doi = {10.1145/3377812.3382153},
abstract = {We present FeatureNET, an open-source Neural Architecture Search (NAS) tool1 that generates diverse sets of Deep Learning (DL) models. FeatureNET relies on a meta-model of deep neural networks, consisting of generic configurable entities. Then, it uses tools developed in the context of software product lines to generate diverse (maximize the differences between the generated) DL models. The models are translated to Keras and can be integrated into typical machine learning pipelines. FeatureNET allows researchers to generate seamlessly a large variety of models. Thereby, it helps choosing appropriate DL models and performing experiments with diverse models (mitigating potential threats to validity). As a NAS method, FeatureNET successfully generates models performing equally well with handcrafted models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {41–44},
numpages = {4},
keywords = {neural architecture search, configuration search, NAS, AutoML},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.5555/1308171.1308193,
author = {Gruler, Alexander and Harhurin, Alexander and Hartmann, Judith},
title = {Development and Configuration of Service-based Product Lines},
year = {2007},
isbn = {0769528880},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Increasing complexity due to the multitude of different functions and their interactions as well as a rising number of different product variants are just some of the challenges that must be faced during the development of multi-functional system families. Addressing this trend we present an approach combining model-based development with product line techniques aiming at a consistent description of a software product family as well as supporting the configuration of its variants. We integrate the concept of variability in our framework [7] which only supported the representation of single software systems on subsequent abstraction levels so far. For the configuration of a concrete product we extend this framework by a feature-based model which allows to configure and derive single systems from a system family model. Furthermore, we explain how the complexity due to the possibly huge amount of configuration decisions can be handled by means of a staged configuration process.},
booktitle = {Proceedings of the 11th International Software Product Line Conference},
pages = {107–116},
numpages = {10},
series = {SPLC '07}
}

@inproceedings{10.1145/1352135.1352185,
author = {Rao, T. M. and Mitra, Sandeep},
title = {An early software engineering approach to teaching cs1, cs2 and ai},
year = {2008},
isbn = {9781595937995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1352135.1352185},
doi = {10.1145/1352135.1352185},
abstract = {We propose the use of a new design-first approach called Problem Stereotypes and Solution Frameworks, for teaching CS1 and CS2. A problem stereotype is a category of problems that can be solved using similar techniques. A solution framework is a typical solution to a problem, parts of which can be reused to solve other problems of this stereotype. Students are introduced to a stereotype through a selection of related problems, and common features among these are identified. Homework problems are selected from the same stereotype, with students expected to follow the "recipe" provided by the given examples to generate their own solutions. Using this approach reduces the stress level for beginner students, and prevents them falling prey to the "CS is HARD" myth. We present the results of our experience with this approach in two introductory classes and an upper-division Artificial Intelligence (AI) class at SUNY Brockport.},
booktitle = {Proceedings of the 39th SIGCSE Technical Symposium on Computer Science Education},
pages = {143–147},
numpages = {5},
keywords = {teaching, stereotype, software engineering, puzzle solving, game playing, framework, cs1/2, artificial intelligence},
location = {Portland, OR, USA},
series = {SIGCSE '08}
}

@inproceedings{10.1145/2070821.2070824,
author = {Menzies, Tim and Bird, Christian and Zimmermann, Thomas and Schulte, Wolfram and Kocaganeli, Ekrem},
title = {The inductive software engineering manifesto: principles for industrial data mining},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070824},
doi = {10.1145/2070821.2070824},
abstract = {The practices of industrial and academic data mining are very different. These differences have significant implications for (a) how we manage industrial data mining projects; (b) the direction of academic studies in data mining; and (c) training programs for engineers who seek to use data miners in an industrial setting.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {19–26},
numpages = {8},
keywords = {industry, inductive engineering},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@article{10.1007/s10766-016-0417-6,
author = {Allombert, V. and Gava, F. and Tesson, J.},
title = {Multi-ML: Programming Multi-BSP Algorithms in ML},
year = {2017},
issue_date = {April     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {2},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-016-0417-6},
doi = {10.1007/s10766-016-0417-6},
abstract = {bsp is a bridging model between abstract execution and concrete parallel systems. Structure and abstraction brought by bsp allow to have portable parallel programs with scalable performance predictions, without dealing with low-level details of architectures. In the past, we designed bsml for programming bsp algorithms in ml. However, the simplicity of the bsp model does not fit the complexity of today's hierarchical architectures such as clusters of machines with multiple multi-core processors. The multi-bsp model is an extension of the bsp model which brings a tree-based view of nested components of hierarchical architectures. To program multi-bsp algorithms in ml, we propose the multi-ml language as an extension of bsml where a specific kind of recursion is used to go through a hierarchy of computing nodes. We define a formal semantics of the language and present preliminary experiments which show performance improvements with respect to bsml.},
journal = {Int. J. Parallel Program.},
month = apr,
pages = {340–361},
numpages = {22},
keywords = {multi-bsp, ml, bsp, Parallel programming}
}

@article{10.1109/TPAMI.2020.2972281,
author = {Wang, Xin and Huang, Qiuyuan and Celikyilmaz, Asli and Gao, Jianfeng and Shen, Dinghan and Wang, Yuan-Fang and Wang, William Yang and Zhang, Lei},
title = {Vision-Language Navigation Policy Learning and Adaptation},
year = {2021},
issue_date = {Dec. 2021},
publisher = {IEEE Computer Society},
address = {USA},
volume = {43},
number = {12},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2020.2972281},
doi = {10.1109/TPAMI.2020.2972281},
abstract = {Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms baseline methods by 10 percent on Success Rate weighted by Path Length (SPL) and achieves the state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore and adapt to unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7 to 11.7 percent).},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = dec,
pages = {4205–4216},
numpages = {12}
}

@inbook{10.5555/3454287.3454621,
author = {Hwang, Gunpil and Kim, Seohyeon and Bae, Hyeon-Min},
title = {Bat-G net: bat-inspired high-resolution 3D image reconstruction using ultrasonic echoes},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, a bat-inspired high-resolution ultrasound 3D imaging system is presented. Live bats demonstrate that the properly used ultrasound can be used to perceive 3D space. With this in mind, a neural network referred to as a Bat-G network is implemented to reconstruct the 3D representation of target objects from the hyperbolic FM (HFM) chirped ultrasonic echoes. The Bat-G network consists of an encoder emulating a bat's central auditory pathway, and a 3D graphical visualization decoder. For the acquisition of the ultrasound data, a custom-made Bat-I sensor module is used. The Bat-G network shows the uniform 3D reconstruction results and achieves precision, recall, and F1-score of 0.896, 0.899, and 0.895, respectively. The experimental results demonstrate the implementation feasibility of a high-resolution non-optical sound-based imaging system being used by live bats. The project web page (https://sites.google.com/view/batgnet) contains additional content summarizing our research.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {334},
numpages = {12}
}

@article{10.1016/j.asoc.2016.04.040,
author = {Gul\'{z}, Muhammet and Celik\'{z}, Erkan and Aydin\'{z}, Nezir and Taskin Gumus\'{z}, Alev and Guneri\'{z}, Ali Fuat},
title = {A state of the art literature review of VIKOR and its fuzzy extensions on applications},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.04.040},
doi = {10.1016/j.asoc.2016.04.040},
abstract = {Display Omitted A review of VIKOR and its fuzzy extensions on applications is presented.The systematic classification covers 13 different application areas.It provides an insight for researchers and practitioners on VIKOR applications. Multi criteria decision making (MCDM) is one of the research areas of operations research and management science which has widely studied by researchers and practitioners. It finds a compromise solution for evaluating and ranking alternatives from the best to the worst under conflicting criteria with respect to decision maker(s) preferences. In a compromise approach, the VlseKriterijumska Optimizacija I Kompromisno Resenje (VIKOR; that means multi-criteria optimization and compromise solution) continues to be applied satisfactorily across different application areas. This paper conducts a state-of-the-art literature review to categorize, analyze and interpret the current research on VIKOR applications. It also discusses the extensions of VIKOR applied in fuzzy environments. A total of 343 papers are classified into 13 different application areas and a number of sub-application areas. Furthermore, all papers are also categorized with respect to publication year, published journal, country of origin, application type (real case study vs empirical study), and version of fuzzy sets used. This comprehensive literature review provides an insight for researchers and practitioners on VIKOR applications in terms of showing current state and potential areas for future attempts to be focused in the future.},
journal = {Appl. Soft Comput.},
month = sep,
pages = {60–89},
numpages = {30},
keywords = {VIKOR, State-of-the-art review, Multi criteria decision making, Fuzzy sets}
}

@inproceedings{10.5555/3491440.3492028,
author = {Chen, Cheng and Luo, Luo and Zhang, Weinan and Yu, Yong and Lian, Yijiang},
title = {Efficient and robust high-dimensional linear contextual bandits},
year = {2021},
isbn = {9780999241165},
abstract = {The linear contextual bandits is a sequential decision-making problem where an agent decides among sequential actions given their corresponding contexts. Since large-scale data sets become more and more common, we study the linear contextual bandits in high-dimensional situations. Recent works focus on employing matrix sketching methods to accelerating contextual bandits. However, the matrix approximation error will bring additional terms to the regret bound. In this paper we first propose a novel matrix sketching method which is called Spectral Compensation Frequent Directions (SCFD). Then we propose an efficient approach for contextual bandits by adopting SCFD to approximate the covariance matrices. By maintaining and manipulating sketched matrices, our method only needs O(md) space and O(md) update time in each round, where d is the dimensionality of the data and m is the sketching size. Theoretical analysis reveals that our method has better regret bounds than previous methods in high-dimensional cases. Experimental results demonstrate the effectiveness of our algorithm and verify our theoretical guarantees.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {588},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@inproceedings{10.1145/3297156.3297243,
author = {Volzhaninov, D. A. and Lookin, O. N. and Antsygin, I. N. and Khokhlova, A. D.},
title = {Design and Programming of the Micromanipulator Network to Study Single Cardiac Cell Mechanics},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297243},
doi = {10.1145/3297156.3297243},
abstract = {Computer-controlled biomechanical experiments on single cells require high accuracy and synchronicity of signals and stability of experimental set-ups. In this paper, we propose a digital micromanipulator network consisting of two micromanipulators to study single cardiac cell mechanics. The micromanipulation system using Ethernet for this network has a large positioning range (20 mm), covering the experimental bath with the cells and nanometer movement resolution (5 nm) for precise experiments. The software for the micromanipulator network was developed in the LabVIEW to provide sufficient synchronicity of micromanipulator movement. The testing of seven consistent steps of two micromanipulators showed that movement was synchronous and maximum loss of synchrony of coordinates was 170 nm. We believe that studies on the changes in single cardiac cell length and force under various mechanical loads can be carried out using the developed network.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {455–458},
numpages = {4},
keywords = {Single cardiac cell, Micromanipulator network, LabVIEW, Ethernet, Biomechanics},
location = {Shenzhen, China},
series = {CSAI '18}
}

@article{10.1504/ijcat.2020.110428,
author = {Bai, Xue and Zhou, Hua and Yang, Hongji and Wang, Dong},
title = {Connecting historical changes for cross-version software defect prediction},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {63},
number = {4},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2020.110428},
doi = {10.1504/ijcat.2020.110428},
abstract = {In the whole software life cycle, software defects are inevitable and increase the cost of software development and evolution. Cross-Version Software Defect Prediction (CVSDP) aims at learning the defect patterns from the historical data of previous software versions to distinguish buggy software modules from clean ones. In CVSDP, metrics are intrinsic properties associated with the external manifestation of defects. However, traditional software defect measures ignore the sequential information of changes during software evolution process which may play a crucial role in CVSDP. Therefore, researchers tried to connect traditional metrics across versions as a new kind of evolution metrics. This study proposes a new way to connect historical sequence of metrics based on change sequence named HCSM and designs a novel deep learning algorithm GDNN as a classifier to process it. Compared to the traditional metrics approaches and other relevant approaches, the proposed approach fits in projects with stable and orderly defect control trend.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {371–383},
numpages = {12},
keywords = {gate recurrent unit, deep neural networks, DNN, deep learning, historical change sequences, software metrics, cross-version defect prediction, software testing}
}

@article{10.1155/2019/2384706,
author = {Yang, Xingguang and Yu, Huiqun and Fan, Guisheng and Shi, Kai and Chen, Liqiong and Tramontana, Emiliano},
title = {Local versus Global Models for Just-In-Time Software Defect Prediction},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1058-9244},
url = {https://doi.org/10.1155/2019/2384706},
doi = {10.1155/2019/2384706},
abstract = {Just-in-time software defect prediction (JIT-SDP) is an active topic in software defect prediction, which aims to identify defect-inducing changes. Recently, some studies have found that the variability of defect data sets can affect the performance of defect predictors. By using local models, it can help improve the performance of prediction models. However, previous studies have focused on module-level defect prediction. Whether local models are still valid in the context of JIT-SDP is an important issue. To this end, we compare the performance of local and global models through a large-scale empirical study based on six open-source projects with 227417 changes. The experiment considers three evaluation scenarios of cross-validation, cross-project-validation, and timewise-cross-validation. To build local models, the experiment uses the k-medoids to divide the training set into several homogeneous regions. In addition, logistic regression and effort-aware linear regression (EALR) are used to build classification models and effort-aware prediction models, respectively. The empirical results show that local models perform worse than global models in the classification performance. However, local models have significantly better effort-aware prediction performance than global models in the cross-validation and cross-project-validation scenarios. Particularly, when the number of clusters k is set to 2, local models can obtain optimal effort-aware prediction performance. Therefore, local models are promising for effort-aware JIT-SDP.},
journal = {Sci. Program.},
month = jan,
numpages = {13}
}

@inproceedings{10.5555/3491440.3491754,
author = {Han, Zhongyi and Gui, Xian-Jin and Cui, Chaoran and Yin, Yilong},
title = {Towards accurate and robust domain adaptation under noisy environments},
year = {2021},
isbn = {9780999241165},
abstract = {In non-stationary environments, learning machines usually confront the domain adaptation scenario where the data distribution does change over time. Previous domain adaptation works have achieved great success in theory and practice. However, they always lose robustness in noisy environments where the labels and features of examples from the source domain become corrupted. In this paper, we report our attempt towards achieving accurate noise-robust domain adaptation. We first give a theoretical analysis that reveals how harmful noises influence unsupervised domain adaptation. To eliminate the effect of label noise, we propose an offline curriculum learning for minimizing a newly-defined empirical source risk. To reduce the impact of feature noise, we propose a proxy distribution based margin discrepancy. We seamlessly transform our methods into an adversarial network that performs efficient joint optimization for them, successfully mitigating the negative influence from both data corruption and distribution shift. A series of empirical studies show that our algorithm remarkably outperforms state of the art, over 10% accuracy improvements in some domain adaptation tasks under noisy environments.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {314},
numpages = {8},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@inproceedings{10.1007/978-3-030-58539-6_2,
author = {Chen, Changan and Jain, Unnat and Schissler, Carl and Gari, Sebastia Vicenc Amengual and Al-Halah, Ziad and Ithapu, Vamsi Krishna and Robinson, Philip and Grauman, Kristen},
title = {SoundSpaces: Audio-Visual Navigation in&nbsp;3D&nbsp;Environments},
year = {2020},
isbn = {978-3-030-58538-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58539-6_2},
doi = {10.1007/978-3-030-58539-6_2},
abstract = {Moving around in the world is naturally a multisensory experience, but today’s embodied agents are deaf—restricted to solely their visual perception of the environment. We introduce audio-visual navigation for complex, acoustically and visually realistic 3D environments. By both seeing and hearing, the agent must learn to navigate to a sounding object. We propose a multi-modal deep reinforcement learning approach to train navigation policies end-to-end from a stream of egocentric audio-visual observations, allowing the agent to (1) discover elements of the geometry of the physical space indicated by the reverberating audio and (2) detect and follow sound-emitting targets. We further introduce SoundSpaces: a first-of-its-kind dataset of audio renderings based on geometrical acoustic simulations for two sets of publicly available 3D environments (Matterport3D and Replica), and we instrument Habitat to support the new sensor, making it possible to insert arbitrary sound sources in an array of real-world scanned environments. Our results show that audio greatly benefits embodied visual navigation in 3D spaces, and our work lays groundwork for new research in embodied AI with audio-visual perception. Project: .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI},
pages = {17–36},
numpages = {20},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1109/ICSE43902.2021.00040,
author = {Lin, Jinfeng and Liu, Yalin and Zeng, Qingkai and Jiang, Meng and Cleland-Huang, Jane},
title = {Traceability Transformed: Generating more Accurate Links with Pre-Trained BERT Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00040},
doi = {10.1109/ICSE43902.2021.00040},
abstract = {Software traceability establishes and leverages associations between diverse development artifacts. Researchers have proposed the use of deep learning trace models to link natural language artifacts, such as requirements and issue descriptions, to source code; however, their effectiveness has been restricted by availability of labeled data and efficiency at runtime. In this study, we propose a novel framework called Trace BERT (T-BERT) to generate trace links between source code and natural language artifacts. To address data sparsity, we leverage a three-step training strategy to enable trace models to transfer knowledge from a closely related Software Engineering challenge, which has a rich dataset, to produce trace links with much higher accuracy than has previously been achieved. We then apply the T-BERT framework to recover links between issues and commits in Open Source Projects. We comparatively evaluated accuracy and efficiency of three BERT architectures. Results show that a Single-BERT architecture generated the most accurate links, while a Siamese-BERT architecture produced comparable results with significantly less execution time. Furthermore, by learning and transferring knowledge, all three models in the framework outperform classical IR trace models. On the three evaluated real-word OSS projects, the best T-BERT stably outperformed the VSM model with average improvements of 60.31% measured using Mean Average Precision (MAP). RNN severely underper-formed on these projects due to insufficient training data, while T-BERT overcame this problem by using pretrained language models and transfer learning.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {324–335},
numpages = {12},
keywords = {language models, deep learning, Software traceability},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.5555/3491440.3492078,
author = {Wang, Xintong and Wellman, Michael P.},
title = {Market manipulation: an adversarial learning framework for detection and evasion},
year = {2021},
isbn = {9780999241165},
abstract = {We propose an adversarial learning framework to capture the evolving game between a regulator who develops tools to detect market manipulation and a manipulator who obfuscates actions to evade detection. The model includes three main parts: (1) a generator that learns to adapt original manipulation order streams to resemble trading patterns of a normal trader while preserving the manipulation intent; (2) a discriminator that differentiates the adversarially adapted manipulation order streams from normal trading activities; and (3) an agent-based simulator that evaluates the manipulation effect of adapted outputs. We conduct experiments on simulated order streams associated with a manipulator and a market-making agent respectively. We show examples of adapted manipulation order streams that mimic a specified market maker's quoting patterns and appear qualitatively different from the original manipulation strategy we implemented in the simulator. These results demonstrate the possibility of automatically generating a diverse set of (unseen) manipulation strategies that can facilitate the training of more robust detection algorithms.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {638},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@inproceedings{10.1145/2046684.2046686,
author = {Bannur, Sushma Nagesh and Saul, Lawrence K. and Savage, Stefan},
title = {Judging a site by its content: learning the textual, structural, and visual features of malicious web pages},
year = {2011},
isbn = {9781450310031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046684.2046686},
doi = {10.1145/2046684.2046686},
abstract = {The physical world is rife with cues that allow us to distinguish between safe and unsafe situations. By contrast, the Internet offers a much more ambiguous environment; hence many users are unable to distinguish a scam from a legitimate Web page. To help address this problem, we explore how to train classifiers that can automatically identify malicious Web pages based on clues from their textual content, structural tags, page links, visual appearance, and URLs. Using a contemporary labeled data feed from a large Web mail provider, we extract such features and demonstrate how they can be used to improve classification accuracy over previous, more constrained approaches. In particular, by analyzing the full content of individual Web pages, we more than halve the error rate obtained by a comparably trained classifier that only extracts features from URLs. By training classifiers on different sets of features, we are further able to assess the strength of clues provided by these different sources of information.},
booktitle = {Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence},
pages = {1–10},
numpages = {10},
keywords = {web security, machine learning, blacklisting},
location = {Chicago, Illinois, USA},
series = {AISec '11}
}

@article{10.3233/IDT-130182,
author = {Valverde, Raul and Saade, Raafat George and Talla, Malleswara},
title = {ITIL-based IT service support process reengineering},
year = {2014},
issue_date = {April 2014},
publisher = {IOS Press},
address = {NLD},
volume = {8},
number = {2},
issn = {1872-4981},
url = {https://doi.org/10.3233/IDT-130182},
doi = {10.3233/IDT-130182},
abstract = {The Information Technology Infrastructure Library ITIL supports best practices, reengineering activities and IT service support processes. ITIL framework only provides recommendations, and companies need to utilize this framework to improve their IT service support processes and establish best practices. This study provides a methodology on how to apply the ITIL framework for evaluating the IT service support processes, its reengineering and alignment to best practices, and subsequent integration into a decision support system framework. A case study approach was used to identify a set of Key Performance Indicators KPI which were monitored by a decision support system DSS for triggering on-going reengineering of IT service support processes. This paper focuses on the implementation of the ITIL guidelines at the operational level, improvement of the service desk, and incident, problem, change, release, and configuration management. It also presents the implementation of the ITIL guidelines at the tactical level for the improvement of the service level, capacity, IT service continuity, service availability, and security management. We conclude by providing recommendations for future research.},
journal = {Int. Dec. Tech.},
month = apr,
pages = {111–130},
numpages = {20},
keywords = {Key Performance Indicators, Information Technology Infrastructure Library, Decision Support System}
}

@article{10.1016/j.specom.2019.09.003,
author = {Shirzhiyan, Zahra and Shamsi, Elham and Jafarpisheh, Amir Salar and Jafari, Amir Homayoun},
title = {Objective classification of auditory brainstem responses to consonant-vowel syllables using local discriminant bases},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {114},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.09.003},
doi = {10.1016/j.specom.2019.09.003},
journal = {Speech Commun.},
month = nov,
pages = {36–48},
numpages = {13},
keywords = {Local discriminant bases, Speech encoding, Speech ABR}
}

@article{10.1016/j.artint.2007.05.008,
author = {Denundefinedux, Thierry},
title = {Conjunctive and disjunctive combination of belief functions induced by nondistinct bodies of evidence},
year = {2008},
issue_date = {February, 2008},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {172},
number = {2–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2007.05.008},
doi = {10.1016/j.artint.2007.05.008},
abstract = {Dempster's rule plays a central role in the theory of belief functions. However, it assumes the combined bodies of evidence to be distinct, an assumption which is not always verified in practice. In this paper, a new operator, the cautious rule of combination, is introduced. This operator is commutative, associative and idempotent. This latter property makes it suitable to combine belief functions induced by reliable, but possibly overlapping bodies of evidence. A dual operator, the bold disjunctive rule, is also introduced. This operator is also commutative, associative and idempotent, and can be used to combine belief functions issues from possibly overlapping and unreliable sources. Finally, the cautious and bold rules are shown to be particular members of infinite families of conjunctive and disjunctive combination rules based on triangular norms and conorms.},
journal = {Artif. Intell.},
month = feb,
pages = {234–264},
numpages = {31},
keywords = {Transferable belief model, Information fusion, Idempotence, Evidence theory, Distinct evidence, Dempster--Shafer theory}
}

@inproceedings{10.5555/2898607.2898658,
author = {Li, Jason Jingshi and Renz, Jochen},
title = {In defense of large qualitative calculi},
year = {2010},
publisher = {AAAI Press},
abstract = {The next challenge in qualitative spatial and temporal reasoning is to develop calculi that deal with different aspects of space and time. One approach to achieve this is to combine existing calculi that cover the different aspects. This, however, can lead to calculi that have a very large number of relations and it is a matter of ongoing discussions within the research community whether such large calculi are too large to be useful. In this paper we develop a procedure for reasoning about some of the largest known calculi, the Rectangle Algebra and the Block Algebra with about 10661 relations. We demonstrate that reasoning over these calculi is possible and can be done efficiently in many cases. This is a clear indication that one of the main goals of the field can be achieved: highly expressive spatial and temporal representations that support efficient reasoning.},
booktitle = {Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence},
pages = {315–320},
numpages = {6},
location = {Atlanta, Georgia},
series = {AAAI'10}
}

@inproceedings{10.1109/ICSE43902.2021.00060,
author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {A Context-based Automated Approach for Method Name Consistency Checking and Suggestion},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00060},
doi = {10.1109/ICSE43902.2021.00060},
abstract = {Misleading method names in software projects can confuse developers, which may lead to software defects and affect code understandability. In this paper, we present DeepName, a context-based, deep learning approach to detect method name inconsistencies and suggest a proper name for a method. The key departure point is the philosophy of "Show Me Your Friends, I'll Tell You Who You Are". Unlike the state-of-the-art approaches, in addition to the method's body, we also consider the interactions of the current method under study with the other ones including the caller and callee methods, and the sibling methods in the same enclosing class. The sequences of sub-tokens in the program entities' names in the contexts are extracted and used as the input for an RNN-based encoder-decoder to produce the representations for the current method. We modify that RNN model to integrate the copy mechanism and our newly developed component, called the non-copy mechanism, to emphasize on the possibility of a certain sub-token not to be copied to follow the current sub-token in the currently generated method name.We conducted several experiments to evaluate DEEPNAME on large datasets with +14M methods. For consistency checking, DeepName improves the state-of-the-art approach by 2.1%, 19.6%, and 11.9% relatively in recall, precision, and F-score, respectively. For name suggestion, DeepName improves relatively over the state-of-the-art approaches in precision (1.8%-30.5%), recall (8.8%-46.1%), and F-score (5.2%-38.2%). To assess DEEPNAME's usefulness, we detected inconsistent methods and suggested new method names in active projects. Among 50 pull requests, 12 were merged into the main branch. In total, in 30/50 cases, the team members agree that our suggested method names are more meaningful than the current names.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {574–586},
numpages = {13},
keywords = {Naturalness of Software, Inconsistent Method Name Checking, Entity Name Suggestion, Deep Learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1109/ASE.2009.11,
author = {Gr\"{u}nbacher, Paul and Rabiser, Rick and Dhungana, Deepak and Lehofer, Martin},
title = {Model-Based Customization and Deployment of Eclipse-Based Tools: Industrial Experiences},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.11},
doi = {10.1109/ASE.2009.11},
abstract = {Developers of software engineering tools are facing high expectations regarding capabilities and usability. Users expect tools tailored to their specific needs and integrated in their working environment. This increases tools' complexity and complicates their customization and deployment despite available mechanisms for adaptability and extensibility. A main challenge lies in understanding and managing the dependencies between different technical mechanisms for realizing tool variability. We report on industrial experiences of applying a model-based and tool-supported product line approach for the customization and deployment of two Eclipse-based tools. We illustrate challenges of customizing these tools to different development contexts: In the first case study we developed variability models of a product line tool suite used by an industry partner and utilized these models for tool customization and deployment. In the second case study we applied the same approach to a maintenance and setup tool of our industry partner. Our experiences suggest to design software tools as product lines; to formally describe the tools' variability in models; and to provide end-user capabilities for customizing and deploying the tools.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {247–256},
numpages = {10},
keywords = {product line engineering, industrial experience, end-user customization, deployment, Elicpse-based tools},
series = {ASE '09}
}

@inproceedings{10.1007/978-3-319-42061-5_1,
author = {Babur, \"{O}nder and Cleophas, Loek and Brand, Mark},
title = {Hierarchical Clustering of Metamodels for Comparative Analysis and Visualization},
year = {2016},
isbn = {9783319420608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-42061-5_1},
doi = {10.1007/978-3-319-42061-5_1},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models or metamodels. A good example is the comparison and merging of metamodel variants into a common metamodel in domain model recovery. Although there are many sophisticated techniques to process the input dataset, little attention has been given to the initial data analysis, visualization and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. In this paper we present a generic approach for metamodel comparison, analysis and visualization as an exploratory first step for domain model recovery. We propose representing metamodels in a vector space model, and applying hierarchical clustering techniques to compare and visualize them as a tree structure. We demonstrate our approach on two Ecore datasets: a collection of 50 state machine metamodels extracted from GitHub as top search results; and $$sim $$~100 metamodels from 16 different domains, obtained from AtlanMod Metamodel Zoo.},
booktitle = {Proceedings of the 12th European Conference on Modelling Foundations and Applications - Volume 9764},
pages = {3–18},
numpages = {16},
keywords = {Vector space model, R, Model-Driven Engineering, Model comparison, Hierarchical clustering}
}

@article{10.5555/3220755.3220875,
author = {Tarus, John K. and Niu, Zhendong and Mustafa, Ghulam},
title = {Knowledge-based recommendation: a review of ontology-based recommender systems for e-learning},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {1},
issn = {0269-2821},
abstract = {Recommender systems in e-learning domain play an important role in assisting the learners to find useful and relevant learning materials that meet their learning needs. Personalized intelligent agents and recommender systems have been widely accepted as solutions towards overcoming information retrieval challenges by learners arising from information overload. Use of ontology for knowledge representation in knowledge-based recommender systems for e-learning has become an interesting research area. In knowledge-based recommendation for e-learning resources, ontology is used to represent knowledge about the learner and learning resources. Although a number of review studies have been carried out in the area of recommender systems, there are still gaps and deficiencies in the comprehensive literature review and survey in the specific area of ontology-based recommendation for e-learning. In this paper, we present a review of literature on ontology-based recommenders for e-learning. First, we analyze and classify the journal papers that were published from 2005 to 2014 in the field of ontology-based recommendation for e-learning. Secondly, we categorize the different recommendation techniques used by ontology-based e-learning recommenders. Thirdly, we categorize the knowledge representation technique, ontology type and ontology representation language used by ontology-based recommender systems, as well as types of learning resources recommended by e-learning recommenders. Lastly, we discuss the future trends of this recommendation approach in the context of e-learning. This study shows that use of ontology for knowledge representation in e-learning recommender systems can improve the quality of recommendations. It was also evident that hybridization of knowledge-based recommendation with other recommendation techniques can enhance the effectiveness of e-learning recommenders.},
journal = {Artif. Intell. Rev.},
month = jun,
pages = {21–48},
numpages = {28},
keywords = {Recommender systems, Recommendation, Ontology, Knowledge-based, E-learning}
}

@article{10.1016/j.neucom.2015.07.152,
author = {Liu, Weifeng and Liu, Hongli and Tao, Dapeng},
title = {Hessian regularization by patch alignment framework},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {204},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.07.152},
doi = {10.1016/j.neucom.2015.07.152},
abstract = {In recent years, semi-supervised learning has played a key part in large-scale image management, where usually only a few images are labeled. To address this problem, many representative works have been reported, including transductive SVM, universum SVM, co-training and graph-based methods. The prominent method is the patch alignment framework, which unifies the traditional spectral analysis methods. In this paper, we propose Hessian regression based on the patch alignment framework. In particular, we construct a Hessian using the patch alignment framework and apply it to regression problems. To the best of our knowledge, there is no report on Hessian construction from the patch alignment viewpoint. Compared with the traditional Laplacian regularization, Hessian can better match the data and then leverage the performance. To validate the effectiveness of the proposed method, we conduct human face recognition experiments on a celebrity face dataset. The experimental results demonstrate the superiority of the proposed solution in human face classification.},
journal = {Neurocomput.},
month = sep,
pages = {183–188},
numpages = {6},
keywords = {Semi-supervised learning, Patch alignment, Least squares, Hessian}
}

@inproceedings{10.5555/3495724.3496081,
author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
title = {Object goal navigation using goal-oriented semantic exploration},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, 'Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allows us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {357},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@article{10.1016/j.dsp.2021.103106,
author = {Zhang, Hai and Xie, Qiangqiang and Lu, Bei and Gai, Shan},
title = {Dual attention residual group networks for single image deraining},
year = {2021},
issue_date = {Sep 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103106},
doi = {10.1016/j.dsp.2021.103106},
journal = {Digit. Signal Process.},
month = sep,
numpages = {11},
keywords = {Single image rain removal, Channel attention, Residual groups, Spatial attention}
}

@inproceedings{10.1145/3196494.3196511,
author = {Hendler, Danny and Kels, Shay and Rubin, Amir},
title = {Detecting Malicious PowerShell Commands using Deep Neural Networks},
year = {2018},
isbn = {9781450355766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196494.3196511},
doi = {10.1145/3196494.3196511},
abstract = {Microsoft's PowerShell is a command-line shell and scripting language that is installed by default on Windows machines. Based on Microsoft's .NET framework, it includes an interface that allows programmers to access operating system services. While PowerShell can be configured by administrators for restricting access and reducing vulnerabilities, these restrictions can be bypassed. Moreover, PowerShell commands can be easily generated dynamically, executed from memory, encoded and obfuscated, thus making the logging and forensic analysis of code executed by PowerShell challenging. For all these reasons, PowerShell is increasingly used by cybercriminals as part of their attacks' tool chain, mainly for downloading malicious contents and for lateral movement. Indeed, a recent comprehensive technical report by Symantec dedicated to PowerShell's abuse by cybercrimials [52] reported on a sharp increase in the number of malicious PowerShell samples they received and in the number of penetration tools and frameworks that use PowerShell. This highlights the urgent need of developing effective methods for detecting malicious PowerShell commands. In this work, we address this challenge by implementing several novel detectors of malicious PowerShell commands and evaluating their performance. We implemented both "traditional" natural language processing (NLP) based detectors and detectors based on character-level convolutional neural networks (CNNs). Detectors' performance was evaluated using a large real-world dataset. Our evaluation results show that, although our detectors (and especially the traditional NLP-based ones) individually yield high performance, an ensemble detector that combines an NLP-based classifier with a CNN-based classifier provides the best performance, since the latter classifier is able to detect malicious commands that succeed in evading the former. Our analysis of these evasive commands reveals that some obfuscation patterns automatically detected by the CNN classifier are intrinsically difficult to detect using the NLP techniques we applied. Our detectors provide high recall values while maintaining a very low false positive rate, making us cautiously optimistic that they can be of practical value.},
booktitle = {Proceedings of the 2018 on Asia Conference on Computer and Communications Security},
pages = {187–197},
numpages = {11},
keywords = {powershell, neural networks, natural language processing, malware detection, deep learning},
location = {Incheon, Republic of Korea},
series = {ASIACCS '18}
}

@inproceedings{10.5555/3507788.3507804,
author = {Ria and Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Process-metrics trends analysis for evaluating file-level error proneness},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Assessing the likelihood of a source code file being buggy or healthy in upcoming commits given its past behavior and its interaction with other files, has been an area where the software engineering community has paid significant attention over the years. Early efforts aimed on associating software metrics with maintainability indexes, while more recent efforts focused on the use of machine learning for classifying a software module as error prone or not. In most approaches to date, this analysis is primarily based on source code metrics or on information extracted from the system's source code, and to a lesser extend on information that relates to process metrics. In this paper, we propose a process-metrics based method for predicting the behavior of a file, based both on its GitHub commits and its interdependences with other co-committed files. More specifically, for each file and for each commit a file participates in, we compute a dependency score this file has with its other co-committed files. This score is appropriately amplified if the file is participating in a bug-fixing commit, or decayed over time if it does not. By examining, over several open source systems, the trend of that dependency score for every file as a product of time, for files whose outcome is known and that are used as gold standard, we report statistics which shed light on estimating the likelihood of whether these trends can predict the future behavior of a file or not.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {113–122},
numpages = {10},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {software product line, service-oriented architecture, service selection, optimization, feature-oriented development},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/1321631.1321741,
author = {Gawley, Rachel},
title = {Automating the identification of variability realisation techniques from feature models},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321741},
doi = {10.1145/1321631.1321741},
abstract = {In Software Product Line Engineering (SPLE), feature modelling is frequently used to model commonalities and variabilities within a domain. A feature model captures an abstract view of a product line and it can serve as a starting point for software design and component implementation. Handling variability exposed within the feature model is an important problem in this context, and in this paper, we describe ongoing research aimed at automating the identification of variability realisation techniques from feature models},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {555–558},
numpages = {4},
keywords = {variability realisation techniques, software product lines, feature models, design patterns},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@inproceedings{10.5555/3504035.3504989,
author = {Ieva, Carlo and Gotlieb, Arnaud and Kaci, Souhila and Lazaar, Nadjib},
title = {Discovering program topoi through clustering},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Understanding source code of large open-source software projects is very challenging when there is only little documentation. New developers face the task of classifying a huge number of files and functions without any help. This paper documents a novel approach to this problem, called FEAT, that automatically extracts topoi from source code by using hierarchical agglomerative clustering. Program topoi summarize the main capabilities of a software system by presenting to developers clustered lists of functions together with an index of their relevant words. The clustering method used in FEAT exploits a new hybrid distance which combines both textual and structural elements automatically extracted from source code and comments. The experimental evaluation of FEAT shows that this approach is suitable to understand open-source software projects of size approaching 2,000 functions and 150 files, which opens the door for its deployment in the open-source community.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {954},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{10.1007/978-3-030-98682-7_6,
author = {Bestmann, Marc and Engelke, Timon and Fiedler, Niklas and G\"{u}ldenstein, Jasper and Gutsche, Jan and Hagge, Jonas and Vahl, Florian},
title = {TORSO-21 Dataset: Typical Objects in&nbsp;RoboCup Soccer 2021},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_6},
doi = {10.1007/978-3-030-98682-7_6},
abstract = {We present a dataset specifically designed to be used as a benchmark to compare vision systems in the RoboCup Humanoid Soccer domain. The dataset is composed of a collection of images taken in various real-world locations as well as a collection of simulated images. It enables comparing vision approaches with a meaningful and expressive metric. The contributions of this paper consist of providing a comprehensive and annotated dataset, an overview of the recent approaches to vision in RoboCup, methods to generate vision training data in a simulated environment, and an approach to increase the variety of a dataset by automatically selecting a diverse set of images from a larger pool. Additionally, we provide a baseline of YOLOv4 and YOLOv4-tiny on this dataset.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {65–77},
numpages = {13},
keywords = {Deep learning, Vision dataset, Computer vision},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1007/978-3-030-64694-3_9,
author = {Hammad, Muhammad and Babur, \"{O}nder and Abdul Basit, Hamid and Brand, Mark van den},
title = {DeepClone: Modeling Clones to Generate Code Predictions},
year = {2020},
isbn = {978-3-030-64693-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64694-3_9},
doi = {10.1007/978-3-030-64694-3_9},
abstract = {Programmers often reuse code from source code repositories to reduce the development effort. Code clones are candidates for reuse in exploratory or rapid development, as they represent often repeated functionality in software systems. To facilitate code clone reuse, we propose DeepClone, a novel approach utilizing a deep learning algorithm for modeling code clones to predict the next set of tokens (possibly a complete clone method body) based on the code written so far. The predicted tokens require minimal customization to fit the context. DeepClone applies natural language processing techniques to learn from a large code corpus, and generates code tokens using the model learned. We have quantitatively evaluated our solution to assess (1) our model’s quality and its accuracy in&nbsp;token prediction, and (2) its performance and effectiveness in clone method prediction. We also discuss various application scenarios for our approach.},
booktitle = {Reuse in Emerging Software Engineering Practices: 19th International Conference on Software and Systems Reuse, ICSR 2020, Hammamet, Tunisia, December 2–4, 2020, Proceedings},
pages = {135–151},
numpages = {17},
keywords = {Code prediction, Code clone, Deep learning, Language modeling},
location = {Hammamet, Tunisia}
}

@article{10.1016/j.neucom.2019.05.009,
author = {Song, Shaoyue and Yu, Hongkai and Miao, Zhenjiang and Guo, Dazhou and Ke, Wei and Ma, Cong and Wang, Song},
title = {An easy-to-hard learning strategy for within-image co-saliency detection},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {358},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.05.009},
doi = {10.1016/j.neucom.2019.05.009},
journal = {Neurocomput.},
month = sep,
pages = {166–176},
numpages = {11},
keywords = {Multiple instance learning, Easy-to-hard learning, Within-image co-saliency}
}

@article{10.1016/j.sigpro.2020.107466,
author = {Zhu, Qi and Xu, Xiangyu and Yuan, Ning and Zhang, Zheng and Guan, Donghai and Huang, Sheng-Jun and Zhang, Daoqiang},
title = {Latent correlation embedded discriminative multi-modal data fusion},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {171},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2020.107466},
doi = {10.1016/j.sigpro.2020.107466},
journal = {Signal Process.},
month = jun,
numpages = {11},
keywords = {Sparse representation, Self-paced learning, Classification, Multi-modal data fusion}
}

@article{10.1016/j.scico.2012.06.007,
author = {Cetina, Carlos and Giner, Pau and Fons, Joan and Pelechano, Vicente},
title = {Prototyping Dynamic Software Product Lines to evaluate run-time reconfigurations},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.06.007},
doi = {10.1016/j.scico.2012.06.007},
abstract = {Dynamic Software Product Lines (DSPL) encompass systems that are capable of modifying their own behavior with respect to changes in their operating environment by using run-time reconfigurations. A failure in these reconfigurations can directly impact the user experience since the reconfigurations are performed when the system is already under the users control. In this work, we prototype a Smart Hotel DSPL to evaluate the reliability-based risk of the DSPL reconfigurations, specifically, the probability of malfunctioning (Availability) and the consequences of malfunctioning (Severity). This DSPL prototype was performed with the participation of human subjects by means of a Smart Hotel case study which was deployed with real devices. Moreover, we successfully identified and addressed two challenges associated with the involvement of human subjects in DSPL prototyping: enabling participants to (1) trigger the run-time reconfigurations and to (2) understand the effects of the reconfigurations. The evaluation of the case study reveals positive results regarding both Availability and Severity. However, the participant feedback highlights issues with recovering from a failed reconfiguration or a reconfiguration triggered by mistake. To address these issues, we discuss some guidelines learned in the case study. Finally, although the results achieved by the DSPL may be considered satisfactory for its particular domain, DSPL engineers must provide users with more control over the reconfigurations or the users will not be comfortable with DSPLs.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {2399–2413},
numpages = {15},
keywords = {Variability modeling, Smart Hotel, Dynamic Software Product Line}
}

@inproceedings{10.1007/978-3-030-78230-6_1,
author = {Bj\o{}rner, Nikolaj and Levatich, Maxwell and Lopes, Nuno P. and Rybalchenko, Andrey and Vuppalapati, Chandrasekar},
title = {Supercharging Plant Configurations Using Z3},
year = {2021},
isbn = {978-3-030-78229-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78230-6_1},
doi = {10.1007/978-3-030-78230-6_1},
abstract = {We describe our experiences using Z3 for synthesizing and optimizing next generation plant configurations for a car manufacturing company (The views expressed in this writing are our own. They make no representation on behalf of others). Our approach leverages unique capabilities of Z3: a combination of specialized solvers for finite domain bit-vectors and uninterpreted functions, and a programmable extension that we call constraints as code. To optimize plant configurations using Z3, we identify useful formalisms from Satisfiability Modulo Theories solvers and integrate solving capabilities for the resulting non-trivial optimization problems.},
booktitle = {Integration of Constraint Programming, Artificial Intelligence, and Operations Research: 18th International Conference, CPAIOR 2021, Vienna, Austria, July 5–8, 2021, Proceedings},
pages = {1–25},
numpages = {25},
location = {Vienna, Austria}
}

@inproceedings{10.1145/3168365.3168372,
author = {Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc and Galindo, Jos\'{e} A. and Martinez, Jabier and Ziadi, Tewfik},
title = {VaryLATEX: Learning Paper Variants That Meet Constraints},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168372},
doi = {10.1145/3168365.3168372},
abstract = {How to submit a research paper, a technical report, a grant proposal, or a curriculum vitae that respect imposed constraints such as formatting instructions and page limits? It is a challenging task, especially when coping with time pressure. In this work, we present VaryLATEX, a solution based on variability, constraint programming, and machine learning techniques for documents written in LATEX to meet constraints and deliver on time. Users simply have to annotate LATEX source files with variability information, e.g., (de)activating portions of text, tuning figures' sizes, or tweaking line spacing. Then, a fully automated procedure learns constraints among Boolean and numerical values for avoiding non-acceptable paper variants, and finally, users can further configure their papers (e.g., aesthetic considerations) or pick a (random) paper variant that meets constraints, e.g., page limits. We describe our implementation and report the results of two experiences with VaryLATEX.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {83–88},
numpages = {6},
keywords = {variability modelling, technical writing, machine learning, generators, constraint programming, LATEX},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1007/978-3-030-58542-6_19,
author = {Wang, Hanqing and Wang, Wenguan and Shu, Tianmin and Liang, Wei and Shen, Jianbing},
title = {Active Visual Information Gathering for Vision-Language Navigation},
year = {2020},
isbn = {978-3-030-58541-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58542-6_19},
doi = {10.1007/978-3-030-58542-6_19},
abstract = {Vision-language navigation (VLN) is the task of entailing an agent to carry out navigational instructions inside photo-realistic environments. One of the key challenges in VLN is how to conduct a robust navigation by mitigating the uncertainty caused by ambiguous instructions and insufficient observation of the environment. Agents trained by current approaches typically suffer from this and would consequently struggle to avoid random and inefficient actions at every step. In contrast, when humans face such a challenge, they can still maintain robust navigation by actively exploring the surroundings to gather more information and thus make more confident navigation decisions. This work draws inspiration from human navigation behavior and endows an agent with an active information gathering ability for a more intelligent vision-language navigation policy. To achieve this, we propose an end-to-end framework for learning an exploration policy that decides i) when and where to explore, ii) what information is worth gathering during exploration, and iii) how to adjust the navigation decision after the exploration. The experimental results show promising exploration strategies emerged from training, which leads to significant boost in navigation performance. On the R2R challenge leaderboard, our agent gets promising results all three VLN settings, i.e., single run, pre-exploration, and beam search.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXII},
pages = {307–322},
numpages = {16},
keywords = {Vision-language navigation, Active exploration},
location = {Glasgow, United Kingdom}
}

@article{10.1016/0004-3702(92)90010-U,
author = {Zweben, Monte and Davis, Eugene and Daun, Brian and Drascher, Ellen and Deale, Michael and Eskey, Megan},
title = {Learning to improve constraint-based scheduling},
year = {1992},
issue_date = {Dec. 1992},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {58},
number = {1–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(92)90010-U},
doi = {10.1016/0004-3702(92)90010-U},
journal = {Artif. Intell.},
month = dec,
pages = {271–296},
numpages = {26}
}

@article{10.1016/j.jss.2009.10.011,
author = {Sun, Chang-ai and Rossing, Rowan and Sinnema, Marco and Bulanov, Pavel and Aiello, Marco},
title = {Modeling and managing the variability of Web service-based systems},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.10.011},
doi = {10.1016/j.jss.2009.10.011},
abstract = {Web service-based systems are built orchestrating loosely coupled, standardized, and internetworked programs. If on the one hand, Web services address the interoperability issues of modern information systems, on the other hand, they enable the development of software systems on the basis of reuse, greatly limiting the necessity for reimplementation. Techniques and methodologies to gain the maximum from this emerging computing paradigm are in great need. In particular, a way to explicitly model and manage variability would greatly facilitate the creation and customization of Web service-based systems. By variability we mean the ability of a software system to be extended, changed, customized or configured for use in a specific context. We present a framework and related tool suite for modeling and managing the variability of Web service-based systems for design and run-time, respectively. It is an extension of the COVAMOF framework for the variability management of software product families, which was developed at the University of Groningen. Among the novelties and advantages of the approach are the full modeling of variability via UML diagrams, the run-time support, and the low involvement of the user. All of which leads to a great deal of automation in the management of all kinds of variability.},
journal = {J. Syst. Softw.},
month = mar,
pages = {502–516},
numpages = {15},
keywords = {Web services, Variability modeling, Variability management, Service engineering}
}

@inproceedings{10.1145/2973839.2973852,
author = {Santos, Ismayle S. and Rocha, Lincoln S. and Neto, Pedro A. Santos and Andrade, Rossana M. C.},
title = {Model Verification of Dynamic Software Product Lines},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973852},
doi = {10.1145/2973839.2973852},
abstract = {Dynamic Software Product Lines (DSPLs) extend the concept of Software Product Lines enabling adaptation at runtime according to context changes. Such dynamic behavior is typically designed using adaptation rules, context-triggered actions responsible for features activation and deactivation at runtime. The erroneous specification and the interleaving of adaptation rules (i.e., the parallel execution of adaptation rules) can lead DSPL to reach an undesired (improperly or defective) product configuration at runtime. Thus, in order to improve the reliability of DSPL behavior, design faults must be rigorously identified and eliminated in the early stages of DSPL development. In this paper, we address this issue introducing Dynamic Feature Transition Systems (DFTSs) that allow the modeling and formal verification of the DSPLs adaptive behavior. These transition systems are derived from the adaptation rules and a Context Kripke Structure, which is a context evolution model. Furthermore, we formally define five properties that can be used to identify existing design faults in DSPL design. Aiming to assess the feasibility of our approach, a feasibility study was conducted using two DSPLs, Mobile Visit Guides and Car. In both cases, design faults were automatically detected indicating that our formalism can help in the detection of design faults in the DSPLs adaptive behavior.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {113–122},
numpages = {10},
keywords = {Software Verification, Software Reliability, Model Checking, Dynamic Software Product Line},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@inproceedings{10.1145/3180155.3180219,
author = {Bhatia, Sahil and Kohli, Pushmeet and Singh, Rishabh},
title = {Neuro-symbolic program corrector for introductory programming assignments},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180219},
doi = {10.1145/3180155.3180219},
abstract = {Automatic correction of programs is a challenging problem with numerous real world applications in security, verification, and education. One application that is becoming increasingly important is the correction of student submissions in online courses for providing feedback. Most existing program repair techniques analyze Abstract Syntax Trees (ASTs) of programs, which are unfortunately unavailable for programs with syntax errors. In this paper, we propose a novel Neuro-symbolic approach that combines neural networks with constraint-based reasoning. Specifically, our method first uses a Recurrent Neural Network (RNN) to perform syntax repairs for the buggy programs; subsequently, the resulting syntactically-fixed programs are repaired using constraint-based techniques to ensure functional correctness. The RNNs are trained using a corpus of syntactically correct submissions for a given programming assignment, and are then queried to fix syntax errors in an incorrect programming submission by replacing or inserting the predicted tokens at the error location. We evaluate our technique on a dataset comprising of over 14,500 student submissions with syntax errors. Our method is able to repair syntax errors in 60% (8689) of submissions, and finds functionally correct repairs for 23.8% (3455) submissions.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {60–70},
numpages = {11},
keywords = {automated feedback generation, neural guided search, neural program correction},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2970276.2975938,
author = {Babur, \"{O}nder},
title = {Statistical analysis of large sets of models},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2975938},
doi = {10.1145/2970276.2975938},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models, e.g. for comparing and merging of model variants into a common domain model. Despite many sophisticated techniques for model comparison, little attention has been given to the initial data analysis and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. We would like to develop a generic approach for model comparison and analysis for large datasets; using techniques from information retrieval, natural language processing and machine learning. We are implementing our approach as an open framework and have so far evaluated it on public datasets involving domain analysis, repository management and model searching scenarios.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {888–891},
numpages = {4},
keywords = {vector space model, model comparison, clustering, Model-driven engineering},
location = {Singapore, Singapore},
series = {ASE '16}
}

@article{10.1016/j.jss.2014.10.037,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Galindo, Jos\'{e} A. and Parejo, Jos\'{e} A. and Benavides, David and Segura, Sergio and Egyed, Alexander},
title = {An assessment of search-based techniques for reverse engineering feature models},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.10.037},
doi = {10.1016/j.jss.2014.10.037},
abstract = {HighlightsSearch based techniques perform well for reverse engineering feature models.Different algorithms and objectives favour precision and recall differently.The F1 objective function provides a trade-off between precision and recall. Successful software evolves from a single system by adding and changing functionality to keep up with users' demands and to cater to their similar and different requirements. Nowadays it is a common practice to offer a system in many variants such as community, professional, or academic editions. Each variant provides different functionality described in terms of features. Software Product Line Engineering (SPLE) is an effective software development paradigm for this scenario. At the core of SPLE is variability modelling whose goal is to represent the combinations of features that distinguish the system variants using feature models, the de facto standard for such task. As SPLE practices are becoming more pervasive, reverse engineering feature models from the feature descriptions of each individual variant has become an active research subject. In this paper we evaluated, for this reverse engineering task, three standard search based techniques (evolutionary algorithms, hill climbing, and random search) with two objective functions on 74 SPLs. We compared their performance using precision and recall, and found a clear trade-off between these two metrics which we further reified into a third objective function based on Fβ, an information retrieval measure, that showed a clear performance improvement. We believe that this work sheds light on the great potential of search-based techniques for SPLE tasks.},
journal = {J. Syst. Softw.},
month = may,
pages = {353–369},
numpages = {17},
keywords = {Search Based Software Engineering, Reverse engineering, Feature model}
}

@article{10.5555/3291125.3309615,
author = {Lamprier, Sylvain and Gisselbrecht, Thibault and Gallinari, Patrick},
title = {Profile-based bandit with unknown profiles},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {Stochastic bandits have been widely studied since decades. A very large panel of settings have been introduced, some of them for the inclusion of some structure between actions. If actions are associated with feature vectors that underlie their usefulness, the discovery of a mapping parameter between such proffles and rewards can help the exploration process of the bandit strategies. This is the setting studied in this paper, but in our case the action profiles (constant feature vectors) are unknown beforehand. Instead, the agent is only given sample vectors, with mean centered on the true profiles, for a subset of actions at each step of the process. In this new bandit instance, policies have thus to deal with a doubled uncertainty, both on the profile estimators and the reward mapping parameters learned so far. We propose a new algorithm, called SampLinUCB, specifically designed for this case. Theoretical convergence guarantees are given for this strategy, according to various profile samples delivery scenarios. Finally, experiments are conducted on both artificial data and a task of focused data capture from online social networks. Obtained results demonstrate the relevance of the approach in various settings.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2060–2099},
numpages = {40},
keywords = {upper confidence bounds, stochastic linear bandits, profile-based exploration}
}

@inproceedings{10.1145/3379597.3387456,
author = {El Zarif, Omar and Da Costa, Daniel Alencar and Hassan, Safwat and Zou, Ying},
title = {On the Relationship between User Churn and Software Issues},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387456},
doi = {10.1145/3379597.3387456},
abstract = {The satisfaction of users is only part of the success of a software product, since a strong competition can easily detract users from a software product/service. User churn is the jargon used to denote when a user changes from a product/service to the one offered by the competition. In this study, we empirically investigate the relationship between the issues that are present in a software product and user churn. For this purpose, we investigate a new dataset provided by the alternativeto.net platform. Alternativeto.net has a unique feature that allows users to recommend alternatives for a specific software product, which signals the intention to switch from one software product to another. Through our empirical study, we observe that (i) the intention to change software is tightly associated to the issues that are present in these software; (ii) we can predict the rate of potential churn using machine learning models; (iii) the longer the issue takes to be fixed, the higher the chances of user churn; and (iv) issues within more general software modules are more likely to be associated with user churn. Our study can provide more insights on the prioritization of issues that need to be fixed to proactively minimize the chances of user churn.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {339–349},
numpages = {11},
keywords = {users churn, software issues, software alternatives, deep learning},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1007/978-3-030-64793-3_2,
author = {Shi, Zheyuan Ryan and Procaccia, Ariel D. and Chan, Kevin S. and Venkatesan, Sridhar and Ben-Asher, Noam and Leslie, Nandi O. and Kamhoua, Charles and Fang, Fei},
title = {Learning and Planning in the Feature Deception Problem},
year = {2020},
isbn = {978-3-030-64792-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64793-3_2},
doi = {10.1007/978-3-030-64793-3_2},
abstract = {Today’s high-stakes adversarial interactions feature attackers who constantly breach the ever-improving security measures. Deception mitigates the defender’s loss by misleading the attacker to make suboptimal decisions. In order to formally reason about deception, we introduce the feature deception problem (FDP), a domain-independent model and present a learning and planning framework for finding the optimal deception strategy, taking into account the adversary’s preferences which are initially unknown to the defender. We make the following contributions. (1) We show that we can uniformly learn the adversary’s preferences using data from a modest number of deception strategies. (2) We propose an approximation algorithm for finding the optimal deception strategy given the learned preferences and show that the problem is NP-hard. (3) We perform extensive experiments to validate our methods and results. In addition, we provide a case study of the credit bureau network to illustrate how FDP implements deception on a real-world problem.},
booktitle = {Decision and Game Theory for Security: 11th International Conference, GameSec 2020, College Park, MD, USA, October 28–30, 2020, Proceedings},
pages = {23–44},
numpages = {22},
location = {College Park, MD, USA}
}

@inproceedings{10.1145/1014052.1014087,
author = {Steyvers, Mark and Smyth, Padhraic and Rosen-Zvi, Michal and Griffiths, Thomas},
title = {Probabilistic author-topic models for information discovery},
year = {2004},
isbn = {1581138881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1014052.1014087},
doi = {10.1145/1014052.1014087},
abstract = {We propose a new unsupervised learning technique for extracting information from large text collections. We model documents as if they were generated by a two-stage stochastic process. Each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic. The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm. We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library, and learn a model with 300 topics. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between 1990 and 2002, parsing of abstracts by topics and authors and detection of unusual papers by specific authors. An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer.},
booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {306–315},
numpages = {10},
keywords = {unsupervised learning, text modeling, Gibbs sampling},
location = {Seattle, WA, USA},
series = {KDD '04}
}

@article{10.1016/j.procs.2019.01.042,
author = {abdelali, Zakrani and Mustapha, Hain and Abdelwahed, Namir},
title = {Investigating the use of random forest in software effort estimation},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {148},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.01.042},
doi = {10.1016/j.procs.2019.01.042},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {343–352},
numpages = {10},
keywords = {accuracy evaluation, regression trees, random forest, Software effort estimation}
}

@inproceedings{10.5555/3297863.3297936,
author = {Hanna, Josiah P. and Stone, Peter},
title = {Grounded action transformation for robot learning in simulation},
year = {2017},
publisher = {AAAI Press},
abstract = {Robot learning in simulation is a promising alternative to the prohibitive sample cost of learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the physical robot. This paper proposes a new algorithm for learning in simulation - Grounded Action Transformation - and applies it to learning of humanoid bipedal locomotion. Our approach results in a 43.27% improvement in forward walk velocity compared to a state-of-the art hand-coded walk.1},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {4931–4932},
numpages = {2},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.1016/j.jbi.2017.04.015,
author = {Miller, Timothy and Dligach, Dmitriy and Bethard, Steven and Lin, Chen and Savova, Guergana},
title = {Towards generalizable entity-centric clinical coreference resolution},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {69},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2017.04.015},
doi = {10.1016/j.jbi.2017.04.015},
abstract = {Display Omitted Coreference resolution is important for extracting information from clinical documents.We implement and evaluate a model that tracks entities across a document.We explore generalizability of some important coreference features. ObjectiveThis work investigates the problem of clinical coreference resolution in a model that explicitly tracks entities, and aims to measure the performance of that model in both traditional in-domain train/test splits and cross-domain experiments that measure the generalizability of learned models. MethodsThe two methods we compare are a baseline mention-pair coreference system that operates over pairs of mentions with best-first conflict resolution and a mention-synchronous system that incrementally builds coreference chains. We develop new features that incorporate distributional semantics, discourse features, and entity attributes. We use two new coreference datasets with similar annotation guidelines the THYME colon cancer dataset and the DeepPhe breast cancer dataset. ResultsThe mention-synchronous system performs similarly on in-domain data but performs much better on new data. Part of speech tag features prove superior in feature generalizability experiments over other word representations. Our methods show generalization improvement but there is still a performance gap when testing in new domains. DiscussionGeneralizability of clinical NLP systems is important and under-studied, so future work should attempt to perform cross-domain and cross-institution evaluations and explicitly develop features and training regimens that favor generalizability. A performance-optimized version of the mention-synchronous system will be included in the open source Apache cTAKES software.},
journal = {J. of Biomedical Informatics},
month = may,
pages = {251–258},
numpages = {8},
keywords = {Portability, Machine learning, Generalizability, Coreference, Clinical NLP}
}

@inproceedings{10.1145/3469096.3469872,
author = {Yang, Eugene and Lewis, David D. and Frieder, Ophir},
title = {On minimizing cost in legal document review workflows},
year = {2021},
isbn = {9781450385961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469096.3469872},
doi = {10.1145/3469096.3469872},
abstract = {Technology-assisted review (TAR) refers to human-in-the-loop machine learning workflows for document review in legal discovery and other high recall review tasks. Attorneys and legal technologists have debated whether review should be a single iterative process (one-phase TAR workflows) or whether model training and review should be separate (two-phase TAR workflows), with implications for the choice of active learning algorithm. The relative cost of manual labeling for different purposes (training vs. review) and of different documents (positive vs. negative examples) is a key and neglected factor in this debate. Using a novel cost dynamics analysis, we show analytically and empirically that these relative costs strongly impact whether a one-phase or two-phase workflow minimizes cost. We also show how category prevalence, classification task difficulty, and collection size impact the optimal choice not only of workflow type, but of active learning method and stopping point.},
booktitle = {Proceedings of the 21st ACM Symposium on Document Engineering},
articleno = {30},
numpages = {10},
keywords = {total recall, high-recall retrieval, cost modeling, active learning},
location = {Limerick, Ireland},
series = {DocEng '21}
}

@article{10.1007/s11263-019-01278-x,
author = {Ramasinghe, Sameera and Khan, Salman and Barnes, Nick and Gould, Stephen},
title = {Representation Learning on Unit Ball with 3D Roto-translational Equivariance},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {128},
number = {6},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-019-01278-x},
doi = {10.1007/s11263-019-01278-x},
abstract = {Convolution is an integral operation that defines how the shape of one function is modified by another function. This powerful concept forms the basis of hierarchical feature learning in deep neural networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces—such as a sphere (S2) or a unit ball (B3)—entails unique challenges. In this work, we propose a novel ‘volumetric convolution’ operation that can effectively model and convolve arbitrary functions in B3. We develop a theoretical framework for volumetric convolution based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer in deep networks. By construction, our formulation leads to the derivation of a novel formula to measure the symmetry of a function in B3 around an arbitrary axis, that is useful in function analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on one viable use case i.e., 3D object recognition.},
journal = {Int. J. Comput. Vision},
month = jun,
pages = {1612–1634},
numpages = {23},
keywords = {Deep learning, Zernike polynomials, Volumetric convolution, 3D moments, Convolution neural networks}
}

@inproceedings{10.1007/11908029_95,
author = {Ichihashi, Hidetomo and Honda, Katsuhiro and Notsu, Akira},
title = {Postsupervised hard c-means classifier},
year = {2006},
isbn = {3540476938},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11908029_95},
doi = {10.1007/11908029_95},
abstract = {Miyamoto et al. derived a hard clustering algorithms by defuzzifying a generalized entropy-based fuzzy c-means in which covariance matrices are introduced as decision variables. We apply the hard c-means (HCM) clustering algorithms to a postsupervised classifier to improve resubstitution error rate by choosing best clustering results from local minima of an objective function. Due to the nature of the prototype based classifier, the error rates can easily be improved by increasing the number of clusters with the cost of computer memory and CPU speed. But, with the HCM classifier, the resubstitution error rate along with the data set compression ratio is improved on several benchmark data sets by using a small number of clusters for each class.},
booktitle = {Proceedings of the 5th International Conference on Rough Sets and Current Trends in Computing},
pages = {918–927},
numpages = {10},
location = {Kobe, Japan},
series = {RSCTC'06}
}

@article{10.1016/j.future.2018.06.045,
author = {Ravandi, Babak and Papapanagiotou, Ioannis},
title = {A self-organized resource provisioning for cloud block storage},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.06.045},
doi = {10.1016/j.future.2018.06.045},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {765–776},
numpages = {12},
keywords = {Machine learning, Cloud computing, Resource provisioning, Quality of service, Self-organization, Cloud storage}
}

@article{10.1016/j.dsp.2018.12.007,
author = {Yue, Guanghui and Hou, Chunping and Yan, Weiqing and Choi, Lark Kwon and Zhou, Tianwei and Hou, Yonghong},
title = {Blind quality assessment for screen content images via convolutional neural network},
year = {2019},
issue_date = {Aug 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {91},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2018.12.007},
doi = {10.1016/j.dsp.2018.12.007},
journal = {Digit. Signal Process.},
month = aug,
pages = {21–30},
numpages = {10},
keywords = {Convolutional neural network (CNN), Image quality assessment (IQA), Blind/no reference (NR), Screen content image (SCI)}
}

@inproceedings{10.1007/978-3-642-30829-1_6,
author = {Khosravi, Ramtin and Sabouri, Hamideh},
title = {Using coordinated actors to model families of distributed systems},
year = {2012},
isbn = {9783642308284},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30829-1_6},
doi = {10.1007/978-3-642-30829-1_6},
abstract = {Software product line engineering enables strategic reuse in development of families of related products. In a component-based approach to product line development, components capture functionalities appearing in one or more products in the family and different assemblies of components yield to various products or configurations. In this approach, an interaction model which effectively factors out the logic handling variability from the functionality of the system greatly enhances the reusability of components. We study the problem of variability modeling for a family of distributed systems expressed in actor model. We define a special type of actors called coordinators whose behavior is described as Reo circuits with the aim of encapsulating the variability logic. We have the benefits of Reo language for expressing coordination logic, while modeling the entire system as an actor-based distributed model. We have applied this model to a case study extracted from an industrial software family in the domain of interactive TV.},
booktitle = {Proceedings of the 14th International Conference on Coordination Models and Languages},
pages = {74–88},
numpages = {15},
location = {Stockholm, Sweden},
series = {COORDINATION'12}
}

@inproceedings{10.1145/2903220.2903234,
author = {Omrani, Mohamed Amine and Naanaa, Wady},
title = {A constrained molecular graph generation with imposed and forbidden fragments},
year = {2016},
isbn = {9781450337342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2903220.2903234},
doi = {10.1145/2903220.2903234},
abstract = {In analytical chemistry, the molecular structure generation problem consists in finding all molecular structure that are consistent with a dataset derived from different kind of spectra. The automation of this task is an important research area which has been investigated in many systems known under the name of structure elucidation systems. In this paper, we encoded this real-world problem as a constraint satisfaction problem (CSP) in order to take advantage of the flurry of efficient solution algorithms available within the constraint programming (CP) framework. The CSP encoding allowed to take into account structural constraints like inter-atom distances, imposed and forbidden fragments.},
booktitle = {Proceedings of the 9th Hellenic Conference on Artificial Intelligence},
articleno = {4},
numpages = {5},
keywords = {NMR Spectroscopy, Molecular Structure Elucidation, Molecular Graph Generation, Molecular Fragment, Fragments Assembly, Forbidden Fragment, Constraint Satisfaction Problem},
location = {Thessaloniki, Greece},
series = {SETN '16}
}

@article{10.1016/j.infsof.2019.06.012,
author = {Balera, Juliana Marino and Santiago J\'{u}nior, Valdivino Alexandre de},
title = {A systematic mapping addressing Hyper-Heuristics within Search-based Software Testing},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.06.012},
doi = {10.1016/j.infsof.2019.06.012},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {176–189},
numpages = {14},
keywords = {Meta-heuristics, Genetic Algorithms, Evolutionary Algorithms, Systematic Mapping, Hyper-heuristics, Search-based Software Testing}
}

@article{10.1016/j.comcom.2019.07.002,
author = {Sotelo Monge, Marco Antonio and Maestre Vidal, Jorge and Mart\'{\i}nez P\'{e}rez, Gregorio},
title = {Detection of economic denial of sustainability (EDoS) threats in self-organizing networks},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {145},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2019.07.002},
doi = {10.1016/j.comcom.2019.07.002},
journal = {Comput. Commun.},
month = sep,
pages = {284–308},
numpages = {25},
keywords = {Self-organizing networks, Network function virtualization, Intrusion detection, Information security, Economic denial of sustainability, Cloud computing}
}

@inproceedings{10.1109/ICSE-NIER.2019.00028,
author = {Trubiani, Catia and Apel, Sven},
title = {PLUS: performance learning for uncertainty of software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00028},
doi = {10.1109/ICSE-NIER.2019.00028},
abstract = {Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software performance engineering domain: How to model the variable configuration options (i.e., software and hardware resources) and their intrinsic uncertainties (e.g., resource demand, processor speed) to represent the performance characteristics of software systems? This way, software engineers are exposed to a quantitative evaluation of their systems that supports them in the task of identifying performance critical configurations along with their uncertainties.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {77–80},
numpages = {4},
keywords = {uncertainty, machine learning},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1007/978-3-030-55583-2_29,
author = {Schwalbe, Gesina and Knie, Bernhard and S\"{a}mann, Timo and Dobberphul, Timo and Gauerhof, Lydia and Raafatnia, Shervin and Rocco, Vittorio},
title = {Structuring the Safety Argumentation for Deep Neural Network Based Perception in Automotive Applications},
year = {2020},
isbn = {978-3-030-55582-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-55583-2_29},
doi = {10.1007/978-3-030-55583-2_29},
abstract = {Deep neural networks (DNNs) are widely considered as a key technology for perception in high and full driving automation. However, their safety assessment remains challenging, as they exhibit specific insufficiencies: black-box nature, simple performance issues, incorrect internal logic, and instability. These are not sufficiently considered in existing standards on safety argumentation. In this paper, we systematically establish and break down safety requirements to argue the sufficient absence of risk arising from such insufficiencies. We furthermore argue why diverse evidence is highly relevant for a safety argument involving DNNs, and classify available sources of evidence. Together, this yields a generic approach and template to thoroughly respect DNN specifics within a safety argumentation structure. Its applicability is shown by providing examples of methods and measures following an example use case based on pedestrian detection.},
booktitle = {Computer Safety, Reliability, and Security. SAFECOMP 2020 Workshops: DECSoS 2020, DepDevOps 2020, USDAI 2020, and WAISE 2020, Lisbon, Portugal, September 15, 2020, Proceedings},
pages = {383–394},
numpages = {12},
keywords = {Deep neural networks, Safety case, Automated driving},
location = {Lisbon, Portugal}
}

@inproceedings{10.5555/1158337.1158678,
author = {Czarnecki, Krzysztof and Peter Kim, Chang Hwan and Kalleberg, Karl Trygve},
title = {Feature Models are Views on Ontologies},
year = {2006},
isbn = {0769525997},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Feature modeling has been proposed as an approach for describing variable requirements for software product lines. In this paper, we explore the relationship between feature models and ontologies. First, we examine how previous extensions to basic feature modeling move it closer to richer formalisms for specifying ontologies such as MOF and OWL. Then, we explore the idea of feature models as views on ontologies. Based on that idea, we propose two approaches for the combined use of feature models and ontologies: view derivation and view integration. Finally, we give some ideas about tool support for these approaches.},
booktitle = {Proceedings of the 10th International on Software Product Line Conference},
pages = {41–51},
numpages = {11},
series = {SPLC '06}
}

@article{10.1016/j.patcog.2015.02.027,
author = {Shen, Jialie and Deng, Robert H. and Cheng, Zhiyong and Nie, Liqiang and Yan, Shuicheng},
title = {On robust image spam filtering via comprehensive visual modeling},
year = {2015},
issue_date = {October 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {48},
number = {10},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2015.02.027},
doi = {10.1016/j.patcog.2015.02.027},
abstract = {The Internet has brought about fundamental changes in the way peoples generate and exchange media information. Over the last decade, unsolicited message images (image spams) have become one of the most serious problems for Internet service providers (ISPs), business firms and general end users. In this paper, we report a novel system called RoBoTs (Robust BoosTrap based spam detector) to support accurate and robust image spam filtering. The system is developed based on multiple visual properties extracted from different levels of granularity, aiming to capture more discriminative contents for effective spam image identification. In addition, a resampling based learning framework is developed to effectively integrate random forest and linear discriminative analysis (LDA) to generate comprehensive signature of spam images. It can facilitate more accurate and robust spam classification process with very limited amount of initial training examples. Using three public available test collections, the proposed system is empirically compared with the state-of-the-art techniques. Our results demonstrate its significantly higher performance from different perspectives. Author-HighlightsWe develop a novel scheme to model contents of spam image and compute comprehensive signatures.A hybrid framework is developed to detect spam images effectively.Our approach achieves substantial performance improvement on spam detection in terms of effectiveness and robustness.},
journal = {Pattern Recogn.},
month = oct,
pages = {3227–3238},
numpages = {12},
keywords = {Spam, Security, Experimentation, Algorithm}
}

@article{10.1016/j.compag.2019.105023,
author = {Moon, Taewon and Hong, Seojung and Choi, Ha Young and Jung, Dae Ho and Chang, Se Hong and Son, Jung Eek},
title = {Interpolation of greenhouse environment data using multilayer perceptron},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {166},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2019.105023},
doi = {10.1016/j.compag.2019.105023},
journal = {Comput. Electron. Agric.},
month = nov,
numpages = {8},
keywords = {Spline, Random forest, Multivariate regression, Linear, Data loss}
}

@article{10.1016/j.eswa.2012.03.061,
author = {Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S. and Garc\'{\i}a-Torres, M.},
title = {Fast feature selection aimed at high-dimensional data via hybrid-sequential-ranked searches},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {12},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.03.061},
doi = {10.1016/j.eswa.2012.03.061},
abstract = {We address the feature subset selection problem for classification tasks. We examine the performance of two hybrid strategies that directly search on a ranked list of features and compare them with two widely used algorithms, the fast correlation based filter (FCBF) and sequential forward selection (SFS). The proposed hybrid approaches provide the possibility of efficiently applying any subset evaluator, with a wrapper model included, to large and high-dimensional domains. The experiments performed show that our two strategies are competitive and can select a small subset of features without degrading the classification error or the advantages of the strategies under study.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {11094–11102},
numpages = {9},
keywords = {Feature selection, Feature ranking, Data mining, Classification}
}

@article{10.1007/s10462-013-9404-0,
author = {Sarmah, Achyanta Kumar and Hazarika, Shyamanta M. and Sinha, Smriti Kumar},
title = {Formal concept analysis: current trends and directions},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {44},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-013-9404-0},
doi = {10.1007/s10462-013-9404-0},
abstract = {Formalization of human thinking helps in fostering the process of learning by giving an explicit representation to human thoughts. Formal Concept Analysis (FCA) finds it's core here. It considers a "concept" as a formal unit of human thought. A concept is represented as a set of inter related objects called the extent and the set of the properties of these objects, called the intent. Making use of the mathematical principles of Lattice Theory and Map Theory of Abstract Algebra, a set of tools and algorithms have been developed in FCA. These helps us to analyze and represent any context as a relation between it's extent and intent. Concepts drawn from the subsets of the extent and intent can be organized in the form of a lattice giving a subsumption hierarchy. Such concept lattices could be maintained by different operations on the lattice like scaling, pruning, navigating etc. A host of applications and software have been developed over the years which serves the usage of FCA tools and processes for specific purposes in various fields. This paper reviews the theoretical foundation, research and applications of FCA in different areas. The paper projects current trends in FCA and concludes with a discussion on open issues and limitations of FCA.},
journal = {Artif. Intell. Rev.},
month = jun,
pages = {47–86},
numpages = {40},
keywords = {Formal context, Formal concept, FCA, Concept}
}

@inproceedings{10.5555/3297863.3297884,
author = {Wang, Yi and Lee, Joohyung and Kim, Doo Soon},
title = {A logic based approach to answering questions about alternatives in DIY domains},
year = {2017},
publisher = {AAAI Press},
abstract = {Many question answering systems have primarily focused on factoid questions. These systems require the answers to be explicitly stored in a knowledge base (KB) but due to this requirement, they fail to answer many questions for which the answers cannot be pre-formulated. This paper presents a question answering system which aims at answering non-factoid questions in the DIY domain using logic-based reasoning. Specifically, the system uses Answer Set Programming to derive an answer by combining various types of knowledge such as domain and commonsense knowledge. We showcase the system by answering one specific type of questions — questions about alternatives. The evaluation result shows that our logic-based reasoning together with the KB (constructed from texts using Information Extraction) significantly improves the user experience.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {4753–4758},
numpages = {6},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@article{10.1016/j.infsof.2013.05.002,
author = {Rodriguez, Daniel and Ruiz, Roberto and Riquelme, Jose C. and Harrison, Rachel},
title = {A study of subgroup discovery approaches for defect prediction},
year = {2013},
issue_date = {October, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.002},
doi = {10.1016/j.infsof.2013.05.002},
abstract = {Context: Although many papers have been published on software defect prediction techniques, machine learning approaches have yet to be fully explored. Objective: In this paper we suggest using a descriptive approach for defect prediction rather than the precise classification techniques that are usually adopted. This allows us to characterise defective modules with simple rules that can easily be applied by practitioners and deliver a practical (or engineering) approach rather than a highly accurate result. Method: We describe two well-known subgroup discovery algorithms, the SD algorithm and the CN2-SD algorithm to obtain rules that identify defect prone modules. The empirical work is performed with publicly available datasets from the Promise repository and object-oriented metrics from an Eclipse repository related to defect prediction. Subgroup discovery algorithms mitigate against characteristics of datasets that hinder the applicability of classification algorithms and so remove the need for preprocessing techniques. Results: The results show that the generated rules can be used to guide testing effort in order to improve the quality of software development projects. Such rules can indicate metrics, their threshold values and relationships between metrics of defective modules. Conclusions: The induced rules are simple to use and easy to understand as they provide a description rather than a complete classification of the whole dataset. Thus this paper represents an engineering approach to defect prediction, i.e., an approach which is useful in practice, easily understandable and can be applied by practitioners.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1810–1822},
numpages = {13},
keywords = {Subgroup discovery, Rules, Imbalanced datasets, Defect prediction}
}

@inproceedings{10.1007/978-3-030-44429-7_10,
author = {Franch, Xavier and Seyff, Norbert and Oriol, Marc and Fricker, Samuel and Groher, Iris and Vierhauser, Michael and Wimmer, Manuel},
title = {Towards Integrating Data-Driven Requirements Engineering into the Software Development Process: A Vision Paper},
year = {2020},
isbn = {978-3-030-44428-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-44429-7_10},
doi = {10.1007/978-3-030-44429-7_10},
abstract = {[Context and motivation] Modern software engineering processes have shifted from traditional upfront requirements engineering (RE) to a more continuous way of conducting RE, particularly including data-driven approaches. [Question/problem] However, current research on data-driven RE focuses more on leveraging certain techniques such as natural language processing or machine learning than on making the concept fit for facilitating its use in the entire software development process. [Principal ideas/results] In this paper, we propose a research agenda composed of six distinct research directions. These include a data-driven RE infrastructure, embracing data heterogeneity, context-aware adaptation, data analysis and decision support, privacy and confidentiality, and finally process integration. Each of these directions addresses challenges that impede the broader use of data-driven RE. [Contribution] For researchers, our research agenda provides topics relevant to investigate. For practitioners, overcoming the underlying challenges with the help of the proposed research will allow to adopt a data-driven RE approach and facilitate its seamless integration into modern software engineering. For users, the proposed research will enable the transparency, control, and security needed to trust software systems and software providers.},
booktitle = {Requirements Engineering: Foundation for Software Quality: 26th International Working Conference, REFSQ 2020, Pisa, Italy, March 24–27, 2020, Proceedings},
pages = {135–142},
numpages = {8},
keywords = {Model-driven Engineering, Requirements monitoring, Feedback gathering, Data-driven requirements engineering},
location = {Pisa, Italy}
}

@article{10.1007/s10916-015-0219-1,
author = {Ayd\i{}n, Serap and Tunga, M. Alper and Yetkin, Sinan},
title = {Mutual Information Analysis of Sleep EEG in Detecting Psycho-Physiological Insomnia},
year = {2015},
issue_date = {May       2015},
publisher = {Plenum Press},
address = {USA},
volume = {39},
number = {5},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-015-0219-1},
doi = {10.1007/s10916-015-0219-1},
abstract = {The primary goal of this study is to state the clear changes in functional brain connectivity during all night sleep in psycho-physiological insomnia (PPI). The secondary goal is to investigate the usefulness of Mutual Information (MI) analysis in estimating cortical sleep EEG arousals for detection of PPI. For these purposes, healthy controls and patients were compared to each other with respect to both linear (Pearson correlation coefficient and coherence) and nonlinear quantifiers (MI) in addition to phase locking quantification for six sleep stages (stage.1---4, rem, wake) by means of interhemispheric dependency between two central sleep EEG derivations. In test, each connectivity estimation calculated for each couple of epoches (C3-A2 and C4-A1) was identified by the vector norm of estimation. Then, patients and controls were classified by using 10 different types of data mining classifiers for five error criteria such as accuracy, root mean squared error, sensitivity, specificity and precision. High performance in a classification through a measure will validate high contribution of that measure to detecting PPI. The MI was found to be the best method in detecting PPI. In particular, the patients had lower MI, higher PCC for all sleep stages. In other words, the lower sleep EEG synchronization suffering from PPI was observed. These results probably stand for the loss of neurons that then contribute to less complex dynamical processing within the neural networks in sleep disorders an the functional central brain connectivity is nonlinear during night sleep. In conclusion, the level of cortical hemispheric connectivity is strongly associated with sleep disorder. Thus, cortical communication quantified in all existence sleep stages might be a potential marker for sleep disorder induced by PPI.},
journal = {J. Med. Syst.},
month = may,
pages = {1–10},
numpages = {10},
keywords = {Sleep EEG, Mutual information, Data mining, Classification, Brain connectivity}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature model extraction from large collections of informal product descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Product Lines, Feature Models, Domain Analysis},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1016/j.image.2019.04.017,
author = {Wu, Hehe and Wang, Anhong and Liang, Jie and Li, Suyue and Li, Peihao},
title = {DCSN-Cast: Deep compressed sensing network for wireless video multicast},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0923-5965},
url = {https://doi.org/10.1016/j.image.2019.04.017},
doi = {10.1016/j.image.2019.04.017},
journal = {Image Commun.},
month = aug,
pages = {56–67},
numpages = {12},
keywords = {Deep residual network, Compressed sensing, Fully connected network, DCSFCN-cast, DCSRN-cast, DCSN-cast}
}

@inbook{10.5555/3454287.3454321,
author = {Anderson, Peter and Shrivastava, Ayush and Parikh, Devi and Batra, Dhruv and Lee, Stefan},
title = {Chasing ghosts: instruction following as bayesian state tracking},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A visually-grounded navigation instruction can be interpreted as a sequence of expected observations and actions an agent following the correct trajectory would encounter and perform. Based on this intuition, we formulate the problem of finding the goal location in Vision-and-Language Navigation (VLN) [1] within the framework of Bayesian state tracking – learning observation and motion models conditioned on these expectable events. Together with a mapper that constructs a semantic spatial map on-the-fly during navigation, we formulate an end-to-end differentiable Bayes filter and train it to identify the goal by predicting the most likely trajectory through the map according to the instructions. The resulting navigation policy constitutes a new approach to instruction following that explicitly models a probability distribution over states, encoding strong geometric and algorithmic priors while enabling greater explainability. Our experiments show that our approach outperforms a strong LingUNet [2] baseline when predicting the goal location on the map. On the full VLN task, i.e., navigating to the goal location, our approach achieves promising results with less reliance on navigation constraints.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {34},
numpages = {11}
}

@inproceedings{10.1145/3324884.3416640,
author = {Yin, Likang and Filkov, Vladimir},
title = {Team discussions and dynamics during DevOps tool adoptions in OSS projects},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416640},
doi = {10.1145/3324884.3416640},
abstract = {In Open Source Software (OSS) projects, pre-built tools dominate DevOps-oriented pipelines. In practice, a multitude of configuration management, cloud-based continuous integration, and automated deployment tools exist, and often more than one for each task. Tools are adopted (and given up) by OSS projects regularly. Prior work has shown that some tool adoptions are preceded by discussions, and that tool adoptions can result in benefits to the project. But important questions remain: how do teams decide to adopt a tool? What is discussed before the adoption and for how long? And, what team characteristics are determinant of the adoption?In this paper, we employ a large-scale empirical study in order to characterize the team discussions and to discern the teamlevel determinants of tool adoption into OSS projects' development pipelines. Guided by theories of team and individual motivations and dynamics, we perform exploratory data analyses, do deep-dive case studies, and develop regression models to learn the determinants of adoption and discussion length, and the direction of their effect on the adoption. From data of commit and comment traces of large-scale GitHub projects, our models find that prior exposure to a tool and member involvement are positively associated with the tool adoption, while longer discussions and the number of newer team members associate negatively. These results can provide guidance beyond the technical appropriateness for the timeliness of tool adoptions in diverse programmer teams.Our data and code is available at https://github.com/lkyin/tool_adoptions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {697–708},
numpages = {12},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1007/978-3-030-31726-3_42,
author = {Li, Juanjuan and Jing, Xiao-Yuan and Wu, Fei and Sun, Ying and Yang, Yongguang},
title = {A Cost-Sensitive Shared Hidden Layer Autoencoder for Cross-Project Defect Prediction},
year = {2019},
isbn = {978-3-030-31725-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-31726-3_42},
doi = {10.1007/978-3-030-31726-3_42},
abstract = {Cross-project defect prediction means training a classifier model using the historical data of the other source project, and then testing whether the target project instance is defective or not. Since source and target projects have different data distributions, and data distribution difference will degrade the performance of classifier. Furthermore, the class imbalance of datasets increases the difficulty of classification. Therefore, a cost-sensitive shared hidden layer autoencoder (CSSHLA) method is proposed. CSSHLA learns a common feature representation between source and target projects by shared hidden layer autoencoder, and makes the different data distributions more similar. To solve the class imbalance problem, CSSHLA introduces a cost-sensitive factor to assign different importance weights to different instances. Experiments on 10 projects of PROMISE dataset show that CSSHLA improves the performance of cross-project defect prediction compared with baselines.},
booktitle = {Pattern Recognition and Computer Vision: Second Chinese Conference, PRCV 2019, Xi’an, China, November 8–11, 2019, Proceedings, Part III},
pages = {491–502},
numpages = {12},
keywords = {Cross-project software defect prediction, Cost-sensitive learning, Shared hidden layer autoencoder},
location = {Xi'an, China}
}

@article{10.1016/j.asoc.2015.03.045,
author = {Fahad, Labiba Gillani and Rajarajan, Muttukrishnan},
title = {Integration of discriminative and generative models for activity recognition in smart homes},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {37},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.03.045},
doi = {10.1016/j.asoc.2015.03.045},
abstract = {Graphical abstractDisplay Omitted HighlightsA hybrid activity recognition approach that combines DM with PE using SVM.DM is suitable for imbalanced number of activity instances.PE has better generalization ability in activity recognition.Evaluation on five smart home datasets validates an improved performance of the approach. Activity recognition in smart homes enables the remote monitoring of elderly and patients. In healthcare systems, reliability of a recognition model is of high importance. Limited amount of training data and imbalanced number of activity instances result in over-fitting thus making recognition models inconsistent. In this paper, we propose an activity recognition approach that integrates the distance minimization (DM) and probability estimation (PE) approaches to improve the reliability of recognitions. DM uses distances of instances from the mean representation of each activity class for label assignment. DM is useful in avoiding decision biasing towards the activity class with majority instances; however, DM can result in over-fitting. PE on the other hand has good generalization abilities. PE measures the probability of correct assignments from the obtained distances, while it requires a large amount of data for training. We apply data oversampling to improve the representation of classes with less number of instances. Support vector machine (SVM) is applied to combine the outputs of both DM and PE, since SVM performs better with imbalanced data and further improves the generalization ability of the approach. The proposed approach is evaluated using five publicly available smart home datasets. The results demonstrate better performance of the proposed approach compared to the state-of-the-art activity recognition approaches.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {992–1001},
numpages = {10},
keywords = {Support vector machine, Smart homes, Probability estimation, Pervasive healthcare, Distance minimization, Activity recognition}
}

@article{10.1016/j.scico.2013.06.009,
author = {Sabouri, Hamideh and Khosravi, Ramtin},
title = {Reducing the verification cost of evolving product families using static analysis techniques},
year = {2014},
issue_date = {April, 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {83},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2013.06.009},
doi = {10.1016/j.scico.2013.06.009},
abstract = {Software product line engineering enables proactive reuse among a set of related products through explicit modeling of commonalities and differences among them. Software product lines are intended to be used in a long period of time. As a result, they evolve over time, due to the changes in the requirements. Having several individual products in a software family, verification of the entire family may take a considerable effort. In this paper we aim to decrease this cost by reducing the number of verified products using static analysis techniques. Furthermore, to reduce model checking costs after product line evolution, we restrict the number of products that should be re-verified by reusing the previous verification result. All proposed techniques are based on static analysis of the product family model with respect to the property and can be automated. To show the effectiveness of these techniques we apply them on a set of case studies and present the results.},
journal = {Sci. Comput. Program.},
month = apr,
pages = {35–55},
numpages = {21},
keywords = {Static analysis, Software product lines, Reduction techniques, Program slicing, Model checking}
}

@inproceedings{10.1145/1865875.1865876,
author = {Azevedo, Sofia and Machado, Ricardo J. and Bragan\c{c}a, Alexandre and Ribeiro, Hugo},
title = {Support for variability in use case modeling with refinement},
year = {2010},
isbn = {9781450301237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1865875.1865876},
doi = {10.1145/1865875.1865876},
abstract = {The development of software product lines with model-driven approaches involves dealing with diverse modeling artifacts such as use case diagrams, component diagrams, class diagrams, activity diagrams, sequence diagrams and others. In this paper we focus on use cases for product line development and we analyze them from the perspective of variability. In that context we explore the UML (Unified Modeling Language) «extend» relationship. We also explore the functional refinement of use cases with «extend» relationships between them. This work allows understanding the activities of use case modeling with support for variability and of use case modeling with functional refinement when variability is present.},
booktitle = {Proceedings of the 7th International Workshop on Model-Based Methodologies for Pervasive and Embedded Software},
pages = {1–8},
numpages = {8},
keywords = {variability, use case, specialization, software product line, refinement, option, alternative, «extend»},
location = {Antwerpen, Belgium},
series = {MOMPES '10}
}

@article{10.1145/1183236.1183264,
author = {Batory, Don and Benavides, David and Ruiz-Cortes, Antonio},
title = {Automated analysis of feature models: challenges ahead},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183264},
doi = {10.1145/1183236.1183264},
journal = {Commun. ACM},
month = dec,
pages = {45–47},
numpages = {3}
}

@article{10.15388/21-INFOR454,
author = {Sayago-Heredia, Jaime and P\'{e}rez-Castillo, Ricardo and Piattini, Mario},
title = {A Systematic Mapping Study on Analysis of Code Repositories},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {32},
number = {3},
issn = {0868-4952},
url = {https://doi.org/10.15388/21-INFOR454},
doi = {10.15388/21-INFOR454},
abstract = {Code repositories contain valuable information, which can be extracted, processed and synthesized into valuable information. It enabled developers to improve maintenance, increase code quality and understand software evolution, among other insights. Certain research has been made during the last years in this field. This paper presents a systematic mapping study to find, evaluate and investigate the mechanisms, methods and techniques used for the analysis of information from code repositories that allow the understanding of the evolution of software. Through this mapping study, we have identified the main information used as input for the analysis of code repositories (commit data and source code), as well as the most common methods and techniques of analysis (empirical/experimental and automatic). We believe the conducted research is useful for developers working on software development projects and seeking to improve maintenance and understand the evolution of software through the use and analysis of code repositories.},
journal = {Informatica},
month = jan,
pages = {619–660},
numpages = {42},
keywords = {systematic mapping study, GitHub, code repository, repository mining, code repository analysis}
}

@article{10.1016/0004-3702(92)90013-N,
author = {Yang, Qiang},
title = {A theory of conflict resolution in planning},
year = {1992},
issue_date = {Dec. 1992},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {58},
number = {1–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(92)90013-N},
doi = {10.1016/0004-3702(92)90013-N},
journal = {Artif. Intell.},
month = dec,
pages = {361–392},
numpages = {32}
}

@article{10.1007/s10458-006-9002-5,
author = {Valckenaers, Paul and Sauter, John and Sierra, Carles and Rodriguez-Aguilar, Juan Antonio},
title = {Applications and environments for multi-agent systems},
year = {2007},
issue_date = {February  2007},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {1},
issn = {1387-2532},
url = {https://doi.org/10.1007/s10458-006-9002-5},
doi = {10.1007/s10458-006-9002-5},
abstract = {This paper addresses multi agent system (MAS) environments from an application perspective. It presents a structured view on environment-centric MAS applications. This comprises three base configurations, which MAS applications may apply directly or combine into a composite configuration. For each configuration, the paper presents key issues, requirements and opportunities (e.g. time management issues, real-world augmentation opportunities and state snapshot requirements). Thus, the paper delineates what environment technology may implement to serve MAS applications. Sample applications illustrate the configurations. Next, electronic institutions provide an example of an environment technology, addressing norms and laws in an agent society, already achieving some maturity. In comparison, application-domain specific environment technologies still are in embryonic stages.},
journal = {Autonomous Agents and Multi-Agent Systems},
month = feb,
pages = {61–85},
numpages = {25},
keywords = {Multi-agent system, Multi-agent applications, Environments for multi-agent systems}
}

@article{10.1007/s10515-019-00253-7,
author = {Angerer, Florian and Grimmer, Andreas and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Change impact analysis for maintenance and evolution of variable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00253-7},
doi = {10.1007/s10515-019-00253-7},
abstract = {Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This article presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining the possibly impacted source code elements when changing the source code of a product family. The approach also supports engineers, who are adapting the code of specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanisms, it is more precise than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. We report evaluation results on the benefit and performance of the approach using industrial product lines.},
journal = {Automated Software Engg.},
month = jun,
pages = {417–461},
numpages = {45},
keywords = {Variability, Program analysis, Maintenance, Change impact analysis}
}

@article{10.1016/j.ijar.2021.07.015,
author = {Bodewes, Tjebbe and Scutari, Marco},
title = {Learning Bayesian networks from incomplete data with the node-average likelihood},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {138},
number = {C},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2021.07.015},
doi = {10.1016/j.ijar.2021.07.015},
journal = {Int. J. Approx. Reasoning},
month = nov,
pages = {145–160},
numpages = {16},
keywords = {Incomplete data, Score-based structure learning, Bayesian networks}
}

@article{10.1016/j.future.2015.05.017,
title = {Allocating resources for customizable multi-tenant applications in clouds using dynamic feature placement},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {53},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.05.017},
doi = {10.1016/j.future.2015.05.017},
abstract = {Multi-tenancy, where multiple end users make use of the same application instance, is often used in clouds to reduce hosting costs. A disadvantage of multi-tenancy is however that it makes it difficult to create customizable applications, as all end users use the same application instance. In this article, we describe an approach for the development and management of highly customizable multi-tenant cloud applications. We apply software product line engineering techniques to cloud applications, and use an approach where applications are composed of multiple interacting components, referred to as application features. Using this approach, multiple features can be shared between different applications. Allocating resources for these feature-based applications is complex, as relations between components must be taken into account, and is referred to as the feature placement problem.In this article, we describe dynamic feature placement algorithms that minimize migrations between subsequent invocations, and evaluate them in dynamic scenarios where applications are added and removed throughout the evaluation scenario. We find that the developed algorithm achieves a low cost, while resulting in few resource migrations. In our evaluations, we observe that adding migration-awareness to the management algorithms reduces the number of instance migrations by more than 77 % and reduces the load moved between instances by more than 96 % when compared to a static management approach. Despite this reduction in number of migrations, a cost that is on average less than 3 % more than the optimal cost is achieved. We model customizable SaaS applications using feature modeling.A dynamic, migration-aware management approach is presented.Two ILP-based algorithms and a heuristic algorithm are compared.The dynamic algorithms reduce migrations and remain within 3% of the optimal cost.},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {63–76},
numpages = {14}
}

@article{10.5555/1752610.1752668,
author = {Lombardi, Michele and Milano, Michela},
title = {Allocation and scheduling of Conditional Task Graphs},
year = {2010},
issue_date = {May, 2010},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {174},
number = {7–8},
issn = {0004-3702},
abstract = {We propose an original, complete and efficient approach to the allocation and scheduling of Conditional Task Graphs (CTGs). In CTGs, nodes represent activities, some of them are branches and are labeled with a condition, arcs rooted in branch nodes are labeled with condition outcomes and a corresponding probability. A task is executed at run time if the condition outcomes that label the arcs in the path to the task hold at schedule execution time; this can be captured off-line by adopting a stochastic model. Tasks need for their execution either unary or cumulative resources and some tasks can be executed on alternative resources. The solution to the problem is a single assignment of a resource and of a start time to each task so that the allocation and schedule is feasible in each scenario and the expected value of a given objective function is optimized. For this problem we need to extend traditional constraint-based scheduling techniques in two directions: (i) compute the probability of sets of scenarios in polynomial time, in order to get the expected value of the objective function; (ii) define conditional constraints that ensure feasibility in all scenarios. We show the application of this framework on problems with objective functions depending either on the allocation of resources to tasks or on the scheduling part. Also, we present the conditional extension to the timetable global constraint. Experimental results show the effectiveness of the approach on a set of benchmarks taken from the field of embedded system design. Comparing our solver with a scenario based solver proposed in the literature, we show the advantages of our approach both in terms of execution time and solution quality.},
journal = {Artif. Intell.},
month = may,
pages = {500–529},
numpages = {30},
keywords = {Scenarios, Probabilistic reasoning, Optimization, Constraint Programming, Conditional constraints, Conditional Task Graphs}
}

@article{10.5555/3122009.3176858,
author = {Fenn, Shannon and Moscato, Pablo},
title = {Target curricula via selection of minimum feature sets: a case study in Boolean networks},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider the effect of introducing a curriculum of targets when training Boolean models on supervised Multi Label Classification (MLC) problems. In particular, we consider how to order targets in the absence of prior knowledge, and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models.We show that hierarchical dependencies between targets can be exploited by enforcing an appropriate curriculum using hierarchical loss functions. On several multi-output circuit-inference problems with known target difficulties, Feedforward Boolean Networks (FBNs) trained with such a loss function achieve significantly lower out-of-sample error, up to 10% in some cases. This improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curricula. We also demonstrate the same improvements on three real-world models and two Gene Regulatory Network (GRN) inference problems.We posit a simple a-priori method for identifying an appropriate target order and estimating the strength of target relationships in Boolean MLCs. These methods use intrinsic dimension as a proxy for target difficulty, which is estimated using optimal solutions to a combinatorial optimisation problem known as the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same generalisation gains can be achieved without providing any knowledge of target difficulty.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {4070–4095},
numpages = {26},
keywords = {target curriculum, multi-label classification, k-feature Set, Boolean betworks}
}

@article{10.1016/0004-3702(92)90009-M,
author = {Dechter, Rina and Pearl, Judea},
title = {Structure identification in relational data},
year = {1992},
issue_date = {Dec. 1992},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {58},
number = {1–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(92)90009-M},
doi = {10.1016/0004-3702(92)90009-M},
journal = {Artif. Intell.},
month = dec,
pages = {237–270},
numpages = {34}
}

@article{10.1007/s10489-018-01399-9,
author = {Abolpour Mofrad, Asieh and Yazidi, Anis and Lewi Hammer, Hugo},
title = {On solving the SPL problem using the concept of probability flux},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {7},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-01399-9},
doi = {10.1007/s10489-018-01399-9},
abstract = {The Stochastic Point Location (SPL) problem Oommen is a fundamental learning problem that has recently found a lot of research attention. SPL can be summarized as searching for an unknown point in an interval under faulty feedback. The search is performed via a Learning Mechanism (LM) (algorithm) that interacts with a stochastic Environment which in turn informs it about the direction of the search. Since the Environment is stochastic, the guidance for directions could be faulty. The first solution to the SPL problem, which was pioneered two decades ago by Oommen, relies on discretizing the search interval and performing a controlled random walk on it. The state of the random walk at each step is considered to be the estimation of the point location. The convergence of the latter simplistic estimation strategy is proved for an infinite resolution, i.e., infinite memory. However, this strategy yields rather poor accuracy for low discretization resolutions. In this paper, we present two major contributions to the SPL problem. First, we demonstrate that the estimation of the point location can significantly be improved by resorting to the concept of mutual probability flux between neighboring states along the line. Second, we are able to accurately track the position of the optimal point and simultaneously show a method by which we can estimate the error probability characterizing the Environment. Interestingly, learning this error probability of the Environment takes place in tandem with the unknown location estimation. We present and analyze several experiments discussing the weaknesses and strengths of the different methods.},
journal = {Applied Intelligence},
month = jul,
pages = {2699–2722},
numpages = {24},
keywords = {Stochastic Point Location (SPL), Stochastic Learning Weak Estimation (SLWE), Mutual probability flux, Last Transition-based Estimation Solution (LTES), Flux-based Estimation Solution (FES), Estimating environment effectiveness}
}

@phdthesis{10.5555/AAI28713793,
author = {Palaparthi, Anil Kumar Reddy and A., Weiss, Jeffrey and D., Rabbitt, Richard and II, Dorval, Alan D., and M, Barkmeier-Kraemer, Julie},
advisor = {R, Titze, Ingo},
title = {Computational Motor Learning and Control of the Vocal Source for Voice Production},
year = {2021},
isbn = {9798780644439},
publisher = {The University of Utah},
abstract = {Voice production is a motor skill and requires the coordinated function of many brain regions, namely the brainstem, cerebellum, basal ganglia, diencephalon, and cerebral hemispheres. The vocal system can be subdivided into three major components: lungs, larynx, and vocal tract. Lung pressure drives the airflow in the trachea towards the larynx. The airflow causes the vocal folds in the larynx (vocal source) to oscillate under certain prephonatory conditions, generating audible pulses of airflow into the vocal tract. The vocal tract filters these pulses and radiates the sound into the air. The intrinsic laryngeal muscles play a significant role in voice production. They position the glottis, the space between the vocal folds in several pre-phonatory positions that facilitate vocal fold vibration. The resulting glottal flow is the vocal source. This project aims to develop a control system that controls the vocal source based on four acoustic and four somatosensory features. Nonlinear control theory and artificial neural networks were used to develop the controllers. A voice simulator with a biomechanical model of the vocal system, LeTalker, was used to model the voice production mechanism. In Aim 1, interrelationships between the intrinsic laryngeal muscles and lung pressure in producing various acoustic and somatosensory features during phonation were obtained. In Aim 2, feedforward and feedback controllers based on acoustic and somatosensory features to control the vocal source were developed. In Aim 3, the controllers' sensitivity and performance were assessed using perturbation analysis. The results demonstrated that the control system was able to generate the lung pressure and muscle activations such that the four acoustic and four somatosensory targets were reached with high accuracy. It was observed that for most of the test cases, the control system produces lung pressure and muscle activations that result in phonation that is within ±15 Hz of the targeted fo and ±2 dB of the targeted SPL. The three studies conducted in this dissertation can be a stepping stone to simulate and study various motor disorders related to voice.},
note = {AAI28713793}
}

@article{10.1002/smr.2341,
author = {Wang, Siwen and Wang, Linhui and Qu, Yang and Chen, Rong and Guo, Shikai},
title = {Toward more accurate developer recommendation via inference of development activities from interaction with bug repair process},
year = {2021},
issue_date = {May 2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {33},
number = {5},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2341},
doi = {10.1002/smr.2341},
abstract = {Software projects usually receive a large number of submitted bug reports every day. Manually triaging the bug reports is often time‐consuming and error‐prone; thus, it is necessary to automatically assign the bug reports to the suitable developers for bug repair, with the help of bug tracking systems. Aiming to reducing the time consumption and mismatch of bug report assignments, we present a developer recommendation model for bug repair based on weighted recurrent neural network, namely, DTPM, which contains two parts: One obtains multisource semantic information of bug reports and fuses them into high‐dimensional semantic feature vectors, and the other combines a penalty matrix into a single hidden layer neural network to obtain more reasonable developer recommendations. We conduct experiments on five datasets of open bug repositories (NetBeans, OpenOffice, GCC, Mozilla, and Eclipse), and the experimental results show that DTPM can achieve better performance than state‐of‐the‐art models LDA_KL, LDA_KL, LDA_SVM, DERTOM, DREX, and DeepTriage.},
journal = {J. Softw. Evol. Process},
month = apr,
numpages = {20},
keywords = {penalty matrix, neural network, bug triaging}
}

@article{10.1016/j.ins.2021.06.013,
author = {Yang, Guoli and Kang, Yuanji and Zhu, Xianqiang and Zhu, Cheng and Xiao, Gaoxi},
title = {Info2vec: An aggregative representation method in multi-layer and heterogeneous networks},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {574},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.06.013},
doi = {10.1016/j.ins.2021.06.013},
journal = {Inf. Sci.},
month = oct,
pages = {444–460},
numpages = {17},
keywords = {Cyberspace, Representation learning, Multi-layer networks}
}

@inproceedings{10.1007/978-3-030-98682-7_5,
author = {Fernandes, Roberto and Rodrigues, Walber M. and Barros, Edna},
title = {Dataset and&nbsp;Benchmarking of&nbsp;Real-Time Embedded Object Detection for&nbsp;RoboCup SSL},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_5},
doi = {10.1007/978-3-030-98682-7_5},
abstract = {When producing a model to object detection in a specific context, the first obstacle is to have a dataset labeling the desired classes. In RoboCup, some leagues already have more than one dataset to train and evaluate a model. However, in the Small Size League (SSL), there is not such dataset available yet. This paper presents an open-source dataset to be used as a benchmark for real-time object detection in SSL. This work also presented a pipeline to train, deploy, and evaluate Convolutional Neural Networks (CNNs) models in a low-power embedded system. This pipeline is used to evaluate the proposed dataset with state-of-art optimized models. In this dataset, the MobileNet SSD v1 achieves 44.88% AP (68.81% AP50) at 94 Frames Per Second (FPS), while running on an SSL robot.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {53–64},
numpages = {12},
keywords = {Object detection, Deep learning, Benchmark, Dataset},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.5555/3172077.3172239,
author = {Murugesan, Keerthiram and Carbonell, aime},
title = {Self-paced multitask learning with shared knowledge},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {This paper introduces self-paced task selection to multitask learning, where instances from more closely related tasks are selected in a progression of easier-to-harder tasks, to emulate an effective human education strategy, but applied to multitask machine learning. We develop the mathematical foundation for the approach based on iterative selection of the most appropriate task, learning the task parameters, and updating the shared knowledge, optimizing a new bi-convex loss function. This proposed method applies quite generally, including to multitask feature learning, multitask learning with alternating structure optimization, etc. Results show that in each of the above formulations self-paced (easier-to-harder) task selection outperforms the baseline version of these methods in all the experiments.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2522–2528},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.1016/j.asoc.2015.03.052,
author = {Fasanghari, Mehdi and Amalnick, Mohsen Sadegh and Taghipour Anvari, Reza and Razmi, Jafar},
title = {A novel credibility-based group decision making method for Enterprise Architecture scenario analysis using Data Envelopment Analysis},
year = {2015},
issue_date = {July 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {32},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.03.052},
doi = {10.1016/j.asoc.2015.03.052},
abstract = {Alignment of IT and business.Enterprise Architecture (EA) analysis.p-Robust group DEA.Fuzzy credibility constrained programming DEA. Analysis and selection of Enterprise Architecture (EA) scenarios is a difficult and complex decision making process directly effecting the long-term business strategies realization. This complexity is associated with contradictory objectives and significant uncertainties involved in analysis process. Although a large body of intuitive and analytical models for EA analysis has evolved over the last few years, none of them leads to an efficient and optimized ranking in fuzzy environments. Moreover, it is necessary to simultaneously employ some complementary methods to reflect the ambiguity and vagueness as the main sources of uncertainty. This paper incorporates the concept of Data Envelopment Analysis (DEA) model into EA scenario analysis through a group analysis under uncertain conditions. To resolve the vagueness and ambiguity of the EA analysis, fuzzy credibility constrained programming and p-robustness technique are applied, respectively. Not only is the proposed DEA model linear, robust, and flexible in aggregating experts' opinion in a group decision making process, but it also is successful in discrimination power improvement - a major shortcoming associated with classic DEA model. The proposed model provides useful solutions to support decision making process for large-scale Information Technology (IT) development planning.},
journal = {Appl. Soft Comput.},
month = jul,
pages = {347–368},
numpages = {22},
keywords = {p-Robustness, Group decision making, Fuzzy credibility constrained programming, Enterprise Architecture, Data Envelopment Analysis, COBIT}
}

@inproceedings{10.1145/2739482.2764681,
author = {Martinez, Jabier and Rossi, Gabriele and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} Fran\c{c}ois D. Assise and Klein, Jacques and Le Traon, Yves},
title = {Estimating and Predicting Average Likability on Computer-Generated Artwork Variants},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764681},
doi = {10.1145/2739482.2764681},
abstract = {Computer assisted human creativity encodes human design decisions in algorithms allowing machines to produce artwork variants. Based on this automated production, one can leverage collective understanding of beauty to rank computer-generated artworks according to their average likability. We present the use of Software Product Line techniques for computer-generated art systems as a case study on leveraging the feedback of human perception within the boundaries of a variability model. Since it is not feasible to get feedback for all variants because of a combinatorial explosion of possible configurations, we propose an approach that is developed in two phases: 1) the creation of a data set using an interactive genetic algorithm and 2) the application of a data mining technique on this dataset to create a ranking enriched with confidence metrics.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1431–1432},
numpages = {2},
keywords = {software product lines, media arts, gentic algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1016/0004-3702(92)90003-G,
author = {Mackworth, Alan K.},
title = {The logic of constraint satisfaction},
year = {1992},
issue_date = {Dec. 1992},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {58},
number = {1–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(92)90003-G},
doi = {10.1016/0004-3702(92)90003-G},
journal = {Artif. Intell.},
month = dec,
pages = {3–20},
numpages = {18}
}

@inproceedings{10.1145/3338906.3342508,
author = {Radavelli, Marco},
title = {Using software testing to repair models},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342508},
doi = {10.1145/3338906.3342508},
abstract = {Software testing is an important phase in the software development process, aiming at locating faults in artifacts, and achieve some confidence that the software behaves according to specification. There exists many software testing techniques applied to debugging, fault-localization, and repair of code, however, to the best of our knowledge, the application of software testing to locating faults in models and automatically repair them, is still an open issue. We present a project that investigates the use of software testing methods to automatically repair model artifacts, to support engineers in maintaining them consistent with the implementation and specification. We describe the research approach, the structure of the devised test-driven repair processes, present results in the cases of combinatorial models and feature models, and finally discuss future work of applying testing to repair models for other scenarios, such as timed automata.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1253–1255},
numpages = {3},
keywords = {timed automata, software testing, software product lines, search-based software engineering, mutation, model repair, CIT},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1007/978-3-030-58604-1_7,
author = {Krantz, Jacob and Wijmans, Erik and Majumdar, Arjun and Batra, Dhruv and Lee, Stefan},
title = {Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments},
year = {2020},
isbn = {978-3-030-58603-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58604-1_7},
doi = {10.1007/978-3-030-58604-1_7},
abstract = {We develop a language-guided navigation task set in a continuous 3D environment where agents must execute low-level actions to follow natural language navigation directions. By being situated in continuous environments, this setting lifts a number of assumptions implicit in prior work that represents environments as a sparse graph of panoramas with edges corresponding to navigability. Specifically, our setting drops the presumptions of known environment topologies, short-range oracle navigation, and perfect agent localization. To contextualize this new task, we develop models that mirror many of the advances made in prior settings as well as single-modality baselines. While some transfer, we find significantly lower absolute performance in the continuous setting – suggesting that performance in prior ‘navigation-graph’ settings may be inflated by the strong implicit assumptions. Code at 
.},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII},
pages = {104–120},
numpages = {17},
keywords = {Embodied agents, Vision-and-Language Navigation},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00126,
author = {Heum\"{u}ller, Robert},
title = {Learning to boost the efficiency of modern code review},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00126},
doi = {10.1109/ICSE-Companion52605.2021.00126},
abstract = {Modern Code Review (MCR) is a standard in all kinds of organizations that develop software. MCR pays for itself through perceived and proven benefits in quality assurance and knowledge transfer. However, the time invest in MCR is generally substantial. The goal of this thesis is to boost the efficiency of MCR by developing AI techniques that can partially replace or assist human reviewers. The envisioned techniques distinguish from existing MCR-related AI models in that we interpret these challenges as graph-learning problems. This should allow us to use state-of-science algorithms from that domain to learn coding and reviewing standards directly from existing projects. The required training data will be mined from online repositories and the experiments will be designed to use standard, quantitative evaluation metrics. This research proposal defines the motivation, research-questions, and solution components for the thesis, and gives an overview of the relevant related work.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {275–277},
numpages = {3},
keywords = {modern code review, deep learning, automated software engineering},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@article{10.1016/0004-3702(92)90004-H,
author = {Freuder, Eugene C. and Wallace, Richard J.},
title = {Partial constraint satisfaction},
year = {1992},
issue_date = {Dec. 1992},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {58},
number = {1–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(92)90004-H},
doi = {10.1016/0004-3702(92)90004-H},
journal = {Artif. Intell.},
month = dec,
pages = {21–70},
numpages = {50}
}

@article{10.1016/j.specom.2021.06.001,
author = {Akbarzadeh, Sara and Lee, Sungmin and Chen, Fei and Tan, Chin-Tuan},
title = {The effect of speech and noise levels on the quality perceived by cochlear implant and normal hearing listeners},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2021.06.001},
doi = {10.1016/j.specom.2021.06.001},
journal = {Speech Commun.},
month = sep,
pages = {106–113},
numpages = {8},
keywords = {SNR, NR, SPL, CI, NH, Sound quality perception, Noise level, Speech level, Cochlear implant}
}

@article{10.1016/0004-3702(92)90005-I,
author = {Hyv\"{o}nen, Eero},
title = {Constraint reasoning based on interval arithmetic: the tolerance propagation approach},
year = {1992},
issue_date = {Dec. 1992},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {58},
number = {1–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(92)90005-I},
doi = {10.1016/0004-3702(92)90005-I},
journal = {Artif. Intell.},
month = dec,
pages = {71–112},
numpages = {42}
}

@inproceedings{10.1109/IROS51168.2021.9636743,
author = {Yokoyama, Naoki and Ha, Sehoon and Batra, Dhruv},
title = {Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IROS51168.2021.9636743},
doi = {10.1109/IROS51168.2021.9636743},
abstract = {We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent’s dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exempli-fies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in a real robot to navigate an apartment, and show that they can generalize in a zero-shot manner. A video summary is available here: https://youtu.be/QOQ56XVIYVE},
booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {1562–1569},
numpages = {8},
location = {Prague, Czech Republic}
}

@inproceedings{10.1145/3440749.3442599,
author = {Kostromin, Roman and Feoktistov, Alexander},
title = {Agent-Based DevOps of Software and Hardware Resources for Digital Twins of Infrastructural Objects},
year = {2021},
isbn = {9781450388863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440749.3442599},
doi = {10.1145/3440749.3442599},
abstract = {Nowadays, the problem of automating the software delivery processes and resource configuring in computing environments with parallel and distributed architectures to ensure the functioning of digital twins is highly relevant. A large spectrum of the well-known tools for software configuration management is available. However, most of them do not solve problems emerging in the considered subject domain fully. To this end, we propose a new framework for implementing preparing, debugging, delivering, testing, deploying, and configuring virtualized software of digital twins. Currently, we are developing such twins for analyzing the operation of environmentally friendly equipment of infrastructural objects located in the Baikal natural territory. The proposed framework is based on the integrated applying methods and tools of knowledge engineering, multi-agent technologies, conceptual and service-oriented programming, and administration of hybrid distributed computing systems. These systems can include both personal resources and resources of public access centers, grid systems, and cloud or fog platforms. We carried out the practical experiments with the framework prototype. The obtained results have shown that significant speedup in performing the aforementioned processes can be achieved.},
booktitle = {Proceedings of the 4th International Conference on Future Networks and Distributed Systems},
articleno = {8},
numpages = {6},
keywords = {multi-agent system, infrastructural object, digital twin, DevOps, Baikal natural territory},
location = {St.Petersburg, Russian Federation},
series = {ICFNDS '20}
}

@article{10.1016/0004-3702(92)90011-L,
author = {van Beek, Peter},
title = {Reasoning about qualitative temporal information},
year = {1992},
issue_date = {Dec. 1992},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {58},
number = {1–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(92)90011-L},
doi = {10.1016/0004-3702(92)90011-L},
journal = {Artif. Intell.},
month = dec,
pages = {297–326},
numpages = {30},
keywords = {temporal reasoning, point algebra, interval algebra, constraint satisfaction}
}

@article{10.1145/3236386.3237207,
author = {Limoncelli, Thomas A.},
title = {GitOps: A Path to More Self-service IT: IaC + PR = GitOps},
year = {2018},
issue_date = {May-June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1542-7730},
url = {https://doi.org/10.1145/3236386.3237207},
doi = {10.1145/3236386.3237207},
abstract = {GitOps lowers the bar for creating self-service versions of common IT processes, making it easier to meet the return in the ROI calculation. GitOps not only achieves this, but also encourages desired behaviors in IT systems: better testing, reduction of bus factor, reduced wait time, more infrastructure logic being handled programmatically with IaC, and directing time away from manual toil toward creating and maintaining automation.},
journal = {Queue},
month = jun,
pages = {13–26},
numpages = {14}
}

@article{10.1155/2020/8509821,
author = {Li, Hui and Qu, Yang and Guo, Shikai and Gao, Guofeng and Chen, Rong and Chen, Guo and Mobayen, Saleh},
title = {Surprise Bug Report Prediction Utilizing Optimized Integration with Imbalanced Learning Strategy},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1076-2787},
url = {https://doi.org/10.1155/2020/8509821},
doi = {10.1155/2020/8509821},
abstract = {In software projects, a large number of bugs are usually reported to bug repositories. Due to the limited budge and work force, the developers often may not have enough time and ability to inspect all the reported bugs, and thus they often focus on inspecting and repairing the highly impacting bugs. Among the high-impact bugs, surprise bugs are reported to be a fatal threat to the software systems, though they only account for a small proportion. Therefore, the identification of surprise bugs becomes an important work in practices. In recent years, some methods have been proposed by the researchers to identify surprise bugs. Unfortunately, the performance of these methods in identifying surprise bugs is still not satisfied for the software projects. The main reason is that surprise bugs only occupy a small percentage of all the bugs, and it is difficult to identify these surprise bugs from the imbalanced distribution. In order to overcome the imbalanced category distribution of the bugs, a method based on machine learning to predict surprise bugs is presented in this paper. This method takes into account the textual features of the bug reports and employs an imbalanced learning strategy to balance the datasets of the bug reports. Then these datasets after balancing are used to train three selected classifiers which are built by three different classification algorithms and predict the datasets with unknown type. In particular, an ensemble method named optimization integration is proposed to generate a unique and best result, according to the results produced by the three classifiers. This ensemble method is able to adjust the ability of the classifier to detect different categories based on the characteristics of different projects and integrate the advantages of three classifiers. The experiments performed on the datasets from 4 software projects show that this method performs better than the previous methods in terms of detecting surprise bugs.},
journal = {Complex.},
month = jan,
numpages = {14}
}

@inproceedings{10.1109/CEC48606.2020.9185809,
author = {Malhotra, Ruchika and Lata, Kusum},
title = {Improving Software Maintainability Predictions using Data Oversampling and Hybridized Techniques},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC48606.2020.9185809},
doi = {10.1109/CEC48606.2020.9185809},
abstract = {Software systems being developed in today’s era are inherently large and complex. Maintaining these software is a big challenge before the software industry. Since software maintenance demands a high cost, this activity becomes even more challenging. In order to reduce the maintenance cost, it becomes crucial to know the maintainability of software modules/ classes in the initial stages of software development. Many efforts have been made to identify the maintainability of modules in the initial development stages in which prediction models have a lot of roles. Prediction models are trained from past historical data and should consist of an adequate number of instances of low maintainability and high maintainability class/modules. But this is not usually the case, and because of this, we are not able to train the prediction models properly. This situation shows data imbalance. In this direction, in this paper, we will handle the imbalanced data problem so that prediction models can be properly trained. We apply four oversampling techniques in this paper and train the prediction models for maintainability by hybridized techniques. The results of the paper advocate the effectiveness of examined oversampling techniques along with the hybridized classification techniques to develop competent maintainability prediction models.},
booktitle = {2020 IEEE Congress on Evolutionary Computation (CEC)},
pages = {1–7},
numpages = {7},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.artmed.2006.06.003,
author = {Botros, Andrew and van Dijk, Bas and Killian, Matthijs},
title = {AutoNRTTM: An automated system that measures ECAP thresholds with the Nucleus® FreedomTM cochlear implant via machine intelligence},
year = {2007},
issue_date = {May, 2007},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {40},
number = {1},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2006.06.003},
doi = {10.1016/j.artmed.2006.06.003},
abstract = {Objective: AutoNRT(TM) is an automated system that measures electrically evoked compound action potential (ECAP) thresholds from the auditory nerve with the Nucleus^(R) Freedom(TM) cochlear implant. ECAP thresholds along the electrode array are useful in objectively fitting cochlear implant systems for individual use. This paper provides the first detailed description of the AutoNRT algorithm and its expert systems, and reports the clinical success of AutoNRT to date. Methods: AutoNRT determines thresholds by visual detection, using two decision tree expert systems that automatically recognise ECAPs. The expert systems are guided by a dataset of 5393 neural response measurements. The algorithm approaches threshold from lower stimulus levels, ensuring recipient safety during postoperative measurements. Intraoperative measurements use the same algorithm but proceed faster by beginning at stimulus levels much closer to threshold. When searching for ECAPs, AutoNRT uses a highly specific expert system (specificity of 99% during training, 96% during testing; sensitivity of 91% during training, 89% during testing). Once ECAPs are established, AutoNRT uses an unbiased expert system to determine an accurate threshold. Throughout the execution of the algorithm, recording parameters (such as implant amplifier gain) are automatically optimised when needed. Results: In a study that included 29 intraoperative and 29 postoperative subjects (a total of 418 electrodes), AutoNRT determined a threshold in 93% of cases where a human expert also determined a threshold. When compared to the median threshold of multiple human observers on 77 randomly selected electrodes, AutoNRT performed as accurately as the 'average' clinician. Conclusions: AutoNRT has demonstrated a high success rate and a level of performance that is comparable with human experts. It has been used in many clinics worldwide throughout the clinical trial and commercial launch of Nucleus Custom Sound(TM) Suite, significantly streamlining the clinical procedures associated with cochlear implant use.},
journal = {Artif. Intell. Med.},
month = may,
pages = {15–28},
numpages = {14},
keywords = {Threshold estimation, Pattern recognition, Neural response telemetry, Machine learning, Electrically evoked compound action potential, Decision trees, Cochlear implants, Automated systems}
}

@inproceedings{10.1007/978-3-030-64148-1_10,
author = {Aho, Timo and Sievi-Korte, Outi and Kilamo, Terhi and Yaman, Sezin and Mikkonen, Tommi},
title = {Demystifying Data Science Projects: A Look on the People and Process of Data Science Today},
year = {2020},
isbn = {978-3-030-64147-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64148-1_10},
doi = {10.1007/978-3-030-64148-1_10},
abstract = {Processes and practices used in data science projects have been reshaping especially over the last decade. These are different from their software engineering counterparts. However, to a large extent, data science relies on software, and, once taken to use, the results of a data science project are often embedded in software context. Hence, seeking synergy between software engineering and data science might open promising avenues. However, while there are various studies on data science workflows and data science project teams, there have been no attempts to combine these two very interlinked aspects. Furthermore, existing studies usually focus on practices within one company. Our study will fill these gaps with a multi-company case study, concentrating both on the roles found in data science project teams as well as the process. In this paper, we have studied a number of practicing data scientists to understand a typical process flow for a data science project. In addition, we studied the involved roles and the teamwork that would take place in the data context. Our analysis revealed three main elements of data science projects: Experimentation, Development Approach, and Multi-disciplinary team(work). These key concepts are further broken down to 13 different sub-themes in&nbsp;total. The found themes pinpoint critical elements and challenges found in data science projects, which are still often done in an ad-hoc fashion. Finally, we compare the results with modern software development to analyse how good a match there is.},
booktitle = {Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings},
pages = {153–167},
numpages = {15},
keywords = {Case study, Prototyping, Software process, Data engineering, Data science},
location = {Turin, Italy}
}

@inproceedings{10.1109/SEmotion52567.2021.00008,
title = {Keynote},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEmotion52567.2021.00008},
doi = {10.1109/SEmotion52567.2021.00008},
abstract = {In 2003 Dave et al. have coined the term “opinion mining” to refer to “processing a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good)”. Nine years later, in 2012 Brooks and Swigger have applied sentiment analysis in the context of software engineering. Today another nine years have passed and it is time to look back: what have we achieved as a research community and where should we go next?},
booktitle = {2021 IEEE/ACM Sixth International Workshop on Emotion Awareness in Software Engineering (SEmotion)},
pages = {ix},
location = {Madrid, Spain}
}

@article{10.1145/3472291,
author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
title = {A Survey of Deep Active Learning},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3472291},
doi = {10.1145/3472291},
abstract = {Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due.It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {180},
numpages = {40},
keywords = {deep active learning, active learning, Deep learning}
}

@article{10.1504/IJAISC.2013.056838,
author = {Paulraj, M. P. and Andrew, Allan Melvin},
title = {Classification of interior noise comfort level of Proton model cars using feedforward neural network},
year = {2013},
issue_date = {September 2013},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {3},
number = {4},
issn = {1755-4950},
url = {https://doi.org/10.1504/IJAISC.2013.056838},
doi = {10.1504/IJAISC.2013.056838},
abstract = {In this research, a Proton model cars noise comfort level classification system has been developed to detect the noise comfort level in cars using artificial neural network. This research focuses on developing a database consisting of car sound samples measured from different Proton make models in stationary and moving state. In the stationary condition, the sound pressure level is measured at 1,300 RPM, 2,000 RPM and 3,000 RPM while in moving condition, the sound is recorded using dB Orchestra while the car is moving at constant speed from 30 km/h up to 110 km/h. Subjective test is conducted to find the jury's evaluation for the specific sound sample. The feature set is then feed to the neural network model to classify the comfort level. The spectral power feature gives the highest classification accuracy of 88.42%.},
journal = {Int. J. Artif. Intell. Soft Comput.},
month = sep,
pages = {344–359},
numpages = {16}
}

@article{10.1016/0004-3702(92)90012-M,
author = {Kramer, Glenn A.},
title = {A geometric constraint engine},
year = {1992},
issue_date = {Dec. 1992},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {58},
number = {1–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(92)90012-M},
doi = {10.1016/0004-3702(92)90012-M},
journal = {Artif. Intell.},
month = dec,
pages = {327–360},
numpages = {34}
}

@inproceedings{10.1007/978-3-319-47157-0_11,
author = {Wang, Yan and Wu, Xi and Ma, Guangkai and Ma, Zongqing and Fu, Ying and Zhou, Jiliu},
title = {Patch-Based Hippocampus Segmentation Using a Local Subspace Learning Method},
year = {2016},
isbn = {978-3-319-47156-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-47157-0_11},
doi = {10.1007/978-3-319-47157-0_11},
abstract = {Patch-based segmentation methods utilizing multiple atlases have been widely studied for alleviating some misalignments when registering atlases to the target image. However, weights assigned to the fused labels are typically computed based on predefined features (e.g. simple patch intensities), thus being not necessarily optimal. Due to lack of discriminating features for different regions of an anatomical structure, the original feature space defined by image intensities may limit the segmentation accuracy. To address these problems, we propose a novel local subspace learning based patch-wise label propagation method to estimate a voxel-wise segmentation of the target image. Specifically, multi-scale patch intensities and texture features are first extracted from the image patch in order to acquire the abundant appearance information. Then, margin fisher analysis (MFA) is applied to neighboring samples of each voxel to be segmented from the aligned atlases, in order to extract discriminant features. This process can enhance discrimination of features for different local regions in the anatomical structure. Finally, based on extracted discriminant features, the k-nearest neighbor (kNN) classifier is used to determine the final label for the target voxel. Moreover, for the patch-wise label propagation, we first translate label patches into several discrete class labels by using the k-means clustering method, and then apply MFA to ensure that samples with similar label patches achieve a higher similarity and those with dissimilar label patches achieve a lower similarity. To demonstrate segmentation performance, we comprehensively evaluated the proposed method on the ADNI dataset for hippocampus segmentation. Experimental results show that the proposed method outperforms several conventional multi-atlas based segmentation methods.},
booktitle = {Machine Learning in Medical Imaging: 7th International Workshop, MLMI 2016, Held in Conjunction with MICCAI 2016, Athens, Greece, October 17, 2016, Proceedings},
pages = {86–94},
numpages = {9},
keywords = {Target Image, Image Patch, Label Propagation, Spatial Neighborhood, Deformable Image Registration},
location = {Athens, Greece}
}

@inproceedings{10.5555/1251203.1251207,
author = {Feamster, Nick and Balakrishnan, Hari},
title = {Detecting BGP configuration faults with static analysis},
year = {2005},
publisher = {USENIX Association},
address = {USA},
abstract = {The Internet is composed of many independent autonomous systems (ASes) that exchange reachability information to destinations using the Border Gateway Protocol (BGP). Network operators in each AS configure BGP routers to control the routes that are learned, selected, and announced to other routers. Faults in BGP configuration can cause forwarding loops, packet loss, and unintended paths between hosts, each of which constitutes a failure of the Internet routing infrastructure.This paper describes the design and implementation of rcc, the router configuration checker, a tool that finds faults in BGP configurations using static analysis. rcc detects faults by checking constraints that are based on a high-level correctness specification. rcc detects two broad classes of faults: route validity faults, where routers may learn routes that do not correspond to usable paths, and path visibility faults, where routers may fail to learn routes for paths that exist in the network. rcc enables network operators to test and debug configurations before deploying them in an operational network, improving on the status quo where most faults are detected only during operation. rcc has been downloaded by more than sixty-five network operators to date, some of whom have shared their configurations with us. We analyze network-wide configurations from 17 different ASes to detect a wide variety of faults and use these findings to motivate improvements to the Internet routing infrastructure.},
booktitle = {Proceedings of the 2nd Conference on Symposium on Networked Systems Design &amp; Implementation - Volume 2},
pages = {43–56},
numpages = {14},
series = {NSDI'05}
}

@inproceedings{10.1109/WI-IAT.2010.25,
author = {Jaroucheh, Zakwan and Liu, Xiaodong and Smith, Sally},
title = {Mapping Features to Context Information: Supporting Context Variability for Context-Aware Pervasive Applications},
year = {2010},
isbn = {9780769541914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2010.25},
doi = {10.1109/WI-IAT.2010.25},
abstract = {Context-aware computing is widely accepted as a promising paradigm to enable seamless computing. Several middlewares and ontology-based models for describing context information have been developed in order to support context-aware applications. However, the context variability, which refers to the possibility to infer or interpret different context information from different perspectives, has been neglected in the existing context modeling approaches. This paper presents an approach for context-aware software development based on a flexible product line based context model which significantly enhances reusability of context information by providing context variability constructs to satisfy different application needs.},
booktitle = {Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {611–614},
numpages = {4},
keywords = {software product line, pervasive applications, feature model, context-awareness, context variability},
series = {WI-IAT '10}
}

@phdthesis{10.5555/AAI28022406,
author = {Do, Quoc Anh and Bradshaw, Gary and Crumpton, Joseph and Wang, Shaowei},
advisor = {Tanmay, Bhowmik,},
title = {Towards Generation of Creative Software Requirements},
year = {2020},
isbn = {9798664728873},
publisher = {Mississippi State University},
address = {USA},
abstract = {Increasingly competitive software industry, where multiple systems serve the same application domain and compete for customers, favors software with creative features. To promote software creativity, research has proposed multi-day workshops with experienced facilitators, and semi-automated tools to provide a limited support for creative thinking. Such approach is either time consuming and demands substantial involvement from analysts with creative abilities, or useful only for existing large-scale software with a rich issue tracking system. In this dissertation, we present different approaches leveraging advanced natural language processing and machine learning techniques to provided automated support for generating creative software requirements with minimal human intervention. A controlled experiment is conducted to assess the effectiveness of our automated framework compared to the traditional brainstorming technique. The results demonstrate our framework's ability to generate creative features for a wide range of stakeholders and provoke innovative thinking among developers with various experience levels.},
note = {AAI28022406}
}

@article{10.1016/j.pmcj.2019.02.001,
author = {Lee, Hyungu and Hwang, Jung Yeon and Lee, Shincheol and Kim, Dong In and Lee, Sung-Hoon and Lee, Jaehwan and Shin, Ji Sun},
title = {A parameterized model to select discriminating features on keystroke dynamics authentication on smartphones},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1574-1192},
url = {https://doi.org/10.1016/j.pmcj.2019.02.001},
doi = {10.1016/j.pmcj.2019.02.001},
journal = {Pervasive Mob. Comput.},
month = mar,
pages = {45–57},
numpages = {13},
keywords = {Machine learning, IoT, Smartphones, Edge devices, Keystroke dynamics authentication}
}

@inproceedings{10.1145/2491411.2491440,
author = {Blincoe, Kelly and Valetto, Giuseppe and Damian, Daniela},
title = {Do all task dependencies require coordination? the role of task properties in identifying critical coordination needs in software projects},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491440},
doi = {10.1145/2491411.2491440},
abstract = {Several methods exist to detect the coordination needs within software teams. Evidence exists that developers’ awareness about coordination needs improves work performance. Distinguishing with certainty between critical and trivial coordination needs and identifying and prioritizing which specific tasks a pair of developers should coordinate about remains an open problem. We investigate what work dependencies should be considered when establishing coordination needs within a development team. We use our conceptualization of work dependencies named Proximity and leverage machine learning techniques to analyze what additional task properties are indicative of coordination needs. In a case study of the Mylyn project, we were able to identify from all potential coordination requirements a subset of 17% that are most critical. We define critical coordination requirements as those that can cause the most disruption to task duration when left unmanaged. These results imply that coordination awareness tools could be enhanced to make developers aware of only the coordination needs that can bring about the highest performance benefit.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {213–223},
numpages = {11},
keywords = {Task Dependencies, Proximity, Machine Learning, Coordination Requirements, Collaborative Software Development, Awareness},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1109/MSR.2017.18,
author = {Ghotra, Baljinder and Mcintosh, Shane and Hassan, Ahmed E.},
title = {A large-scale study of the impact of feature selection techniques on defect classification models},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.18},
doi = {10.1109/MSR.2017.18},
abstract = {The performance of a defect classification model depends on the features that are used to train it. Feature redundancy, correlation, and irrelevance can hinder the performance of a classification model. To mitigate this risk, researchers often use feature selection techniques, which transform or select a subset of the features in order to improve the performance of a classification model. Recent studies compare the impact of different feature selection techniques on the performance of defect classification models. However, these studies compare a limited number of classification techniques and have arrived at contradictory conclusions about the impact of feature selection techniques. To address this limitation, we study 30 feature selection techniques (11 filter-based ranking techniques, six filter-based subset techniques, 12 wrapper-based subset techniques, and a no feature selection configuration) and 21 classification techniques when applied to 18 datasets from the NASA and PROMISE corpora. Our results show that a correlation-based filter-subset feature selection technique with a BestFirst search method outperforms other feature selection techniques across the studied datasets (it outperforms in 70%--87% of the PROMISE-NASA data sets) and across the studied classification techniques (it outperforms for 90% of the techniques). Hence, we recommend the application of such a selection technique when building defect classification models.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {146–157},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@article{10.1145/3280848,
author = {Pereira, Fernando Magno Quint\~{a}o and Leobas, Guilherme Vieira and Gamati\'{e}, Abdoulaye},
title = {Static Prediction of Silent Stores},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3280848},
doi = {10.1145/3280848},
abstract = {A store operation is called “silent” if it writes in memory a value that is already there. The ability to detect silent stores is important, because they might indicate performance bugs, might enable code optimizations, and might reveal opportunities of automatic parallelization, for instance. Silent stores are traditionally detected via profiling tools. In this article, we depart from this methodology and instead explore the following question: is it possible to predict silentness by analyzing the syntax of programs? The process of building an answer to this question is interesting in itself, given the stochastic nature of silent stores, which depend on data and coding style. To build such an answer, we have developed a methodology to classify store operations in terms of syntactic features of programs. Based on such features, we develop different kinds of predictors, some of which go much beyond what any trivial approach could achieve. To illustrate how static prediction can be employed in practice, we use it to optimize programs running on nonvolatile memory systems.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {44},
numpages = {26},
keywords = {static analysis, nonvolatile memory, machine learning, code optimization, Silent stores}
}

@inproceedings{10.1145/3417113.3422155,
author = {Rahman, Akond and Bhuiyan, Farzana Ahamed},
title = {A vision to mitigate bioinformatics software development challenges},
year = {2021},
isbn = {9781450381284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417113.3422155},
doi = {10.1145/3417113.3422155},
abstract = {Developers construct bioinformatics software to automate crucial analysis and research related to biological science. However, challenges while developing bioinformatics software can prohibit advancement in biological science research. Through a human-centric systematic analysis, we can identify challenges related to bioinformatics software development and envision future research directions. From our qualitative analysis with 221 Stack Overflow questions, we identify six categories of challenges: file operations, searching genetic entities, defect resolution, configuration management, sequence alignment, and translation of genetic information. To mitigate the identified challenges we envision three research directions that require synergies between bioinformatics and automated software engineering: (i) automated configuration recommendation using optimization algorithms, (ii) automated and comprehensive defect categorization, and (iii) intelligent task assistance with active and reinforcement learning.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {57–60},
numpages = {4},
keywords = {stack overflow, empirical study, challenge, bioinformatics},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1109/TCBB.2007.1014,
author = {Bontempi, Gianluca},
title = {A Blocking Strategy to Improve Gene Selection for Classification of Gene Expression Data},
year = {2007},
issue_date = {April 2007},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {4},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2007.1014},
doi = {10.1109/TCBB.2007.1014},
abstract = {Because of high dimensionality, machine learning algorithms typically rely on feature selection techniques in order to perform effective classification in microarray gene expression data sets. However, the large number of features compared to the number of samples makes the task of feature selection computationally hard and prone to errors. This paper interprets feature selection as a task of stochastic optimization, where the goal is to select among an exponential number of alternative gene subsets the one expected to return the highest generalization in classification. Blocking is an experimental design strategy which produces similar experimental conditions to compare alternative stochastic configurations in order to be confident that observed differences in accuracy are due to actual differences rather than to fluctuations and noise effects. We propose an original blocking strategy for improving feature selection which aggregates in a paired way the validation outcomes of several learning algorithms to assess a gene subset and compare it to others. This is a novelty with respect to conventional wrappers, which commonly adopt a sole learning algorithm to evaluate the relevance of a given set of variables. The rationale of the approach is that, by increasing the amount of experimental conditions under which we validate a feature subset, we can lessen the problems related to the scarcity of samples and consequently come up with a better selection. The paper shows that the blocking strategy significantly improves the performance of a conventional forward selection for a set of 16 publicly available cancer expression data sets. The experiments involve six different classifiers and show that improvements take place independent of the classification algorithm used after the selection step. Two further validations based on available biological annotation support the claim that blocking strategies in feature selection may improve the accuracy and the quality of the solution. The first validation is based on retrieving PubMEd abstracts associated to the selected genes and matching them to regular expressions describing the biological phenomenon underlying the expression data sets. The biological validation that follows is based on the use of the Bioconductor package GoStats in order to perform Gene Ontology statistical analysis.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = apr,
pages = {293–300},
numpages = {8},
keywords = {machine learning, feature evaluation and selection., data mining, Bioinformatics (genome or protein) databases}
}

@inproceedings{10.5555/2499406.2499419,
author = {Hagen, Sebastian and da Costa Cordeiro, Weverton Luis and Gaspary, Luciano Paschoal and Granville, Lisandro Zambenedetti and Seibold, Michael and Kemper, Alfons},
title = {Planning in the large: efficient generation of IT change plans on large infrastructures},
year = {2012},
isbn = {9781450322102},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {Change Management, a core process of the Information Technology Infrastructure Library (ITIL), is concerned with the management of changes to networks and services to minimize costly disruptions on the business. As part of Change Management, IT changes need to be planned. Previous approaches to automatically generate IT change plans struggle, in terms of scalability, to properly deal with large Configuration Management Databases (CMDBs). To enable IT change planning in the large, in this paper we discuss and analyze optimizations for refinement-based IT change planning over object-oriented CMDBs. Our optimizations reduce the runtime complexity of several key operations part of refinement-based IT change planning algorithms. A sensitivity analysis shows that our optimizations outperform SHOP2 - the winner of a previous comparison among IT change planners - in terms of runtime complexity for several important characteristics of IT changes and CMDBs. A cloud deployment case study of a Three-tier application and a virtual network configuration case study demonstrate the feasibility of our approach and confirm the results from the sensitivity analysis: IT change planning has evolved from planning in the small to planning in the large.},
booktitle = {Proceedings of the 8th International Conference on Network and Service Management},
pages = {108–116},
numpages = {9},
keywords = {automated planning, artificial intelligence, ITIL change management, IT change plan generation},
location = {Las Vegas, Nevada},
series = {CNSM '12}
}

@article{10.1007/s11227-016-1770-3,
author = {Rom\'{a}n Mu\~{n}oz, Fernando and Garc\'{\i}a Villalba, Luis Javier},
title = {An algorithm to find relationships between web vulnerabilities},
year = {2018},
issue_date = {Mar 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {74},
number = {3},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-016-1770-3},
doi = {10.1007/s11227-016-1770-3},
abstract = {Over the past years, there has been a high increase in web sites using cloud computing. Like usual web site, those web applications can have most of the common web vulnerabilities, like SQL injection or cross-site scripting. Therefore, cloud computing has become more attractive to cyber criminals. Besides, in many cases it is necessary to comply with regulations like PCI DSS or standards like ISO/IEC 27001. To face those threats and requirements it is a common task to analyze web applications to detect and correct their vulnerabilities. The most used tools to analyze web applications are automatic scanners. But it is difficult to comparatively decide which scanner is best or at least is best suited to detect a particular vulnerability. To evaluate scanner capabilities some evaluation criteria have been defined. Often a web vulnerability classification is also used to evaluate scanners, but current web vulnerability classifications do not usually include all vulnerabilities. To face evaluation criteria which are not up-to-date and to have the fullest possible classification, in this paper a new method to map web vulnerability classifications is proposed. The result will be the vulnerabilities an automatic scanner has to detect. As classifications change over time, this new method could be executed when the existing classifications change or when new classifications are developed. The vulnerabilities described this way can also be seen as a web vulnerability classification that includes all vulnerabilities in the classifications taken into account.},
journal = {J. Supercomput.},
month = mar,
pages = {1061–1089},
numpages = {29},
keywords = {Web vulnerabilities, Vulnerability classifications, Cyber-security, Cloud computing vulnerabilities}
}

@inproceedings{10.1007/978-3-030-29551-6_44,
author = {Lin, Jiping and Zhou, Yu and Kang, Junhao},
title = {Low-Sampling Imagery Data Recovery by Deep Learning Inference and Iterative Approach},
year = {2019},
isbn = {978-3-030-29550-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29551-6_44},
doi = {10.1007/978-3-030-29551-6_44},
abstract = {Block-based compressed sensing (CS) recovery aims to reconstruct the high quality image from only a small number of observations in a block-wise manner. However, when the sampling rate is very low and the existence of additive noise, there are usually some block artifacts and detail blurs which degrades the reconstructed quality. In this paper, we propose an efficient method which takes both the advantages of deep learning (DL) framework and iterative approaches. First, a deep multi-layer perceptron (DMLP) is constructed to obtain the initial reconstructed image. Then, an efficient iterative approach is applied to keep the consistence and smoothness between the adjacent blocks. The proposed method demonstrates its efficacy on benchmark datasets.},
booktitle = {Knowledge Science, Engineering and Management: 12th International Conference, KSEM 2019, Athens, Greece, August 28–30, 2019, Proceedings, Part I},
pages = {488–493},
numpages = {6},
keywords = {Iterative approach, Deep learning, Compressed sensing},
location = {Athens, Greece}
}

@article{10.1016/0004-3702(92)90006-J,
author = {Van Hentenryck, Pascal and Simonis, Helmut and Dincbas, Mehmet},
title = {Constraint satisfaction using constraint logic programming},
year = {1992},
issue_date = {Dec. 1992},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {58},
number = {1–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(92)90006-J},
doi = {10.1016/0004-3702(92)90006-J},
journal = {Artif. Intell.},
month = dec,
pages = {113–159},
numpages = {47}
}

@article{10.1007/s11263-007-0095-3,
author = {Leibe, Bastian and Leonardis, Ale\v{s} and Schiele, Bernt},
title = {Robust Object Detection with Interleaved Categorization and Segmentation},
year = {2008},
issue_date = {May       2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {1–3},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-007-0095-3},
doi = {10.1007/s11263-007-0095-3},
abstract = {This paper presents a novel method for detecting and localizing objects of a visual category in cluttered real-world scenes. Our approach considers object categorization and figure-ground segmentation as two interleaved processes that closely collaborate towards a common goal. As shown in our work, the tight coupling between those two processes allows them to benefit from each other and improve the combined performance.

The core part of our approach is a highly flexible learned representation for object shape that can combine the information observed on different training examples in a probabilistic extension of the Generalized Hough Transform. The resulting approach can detect categorical objects in novel images and automatically infer a probabilistic segmentation from the recognition result. This segmentation is then in turn used to again improve recognition by allowing the system to focus its efforts on object pixels and to discard misleading influences from the background. Moreover, the information from where in the image a hypothesis draws its support is employed in an MDL based hypothesis verification stage to resolve ambiguities between overlapping hypotheses and factor out the effects of partial occlusion.

An extensive evaluation on several large data sets shows that the proposed system is applicable to a range of different object categories, including both rigid and articulated objects. In addition, its flexible representation allows it to achieve competitive object detection performance already from training sets that are between one and two orders of magnitude smaller than those used in comparable systems.},
journal = {Int. J. Comput. Vision},
month = may,
pages = {259–289},
numpages = {31},
keywords = {Segmentation, Object detection, Object categorization, MDL, Hypothesis selection, Hough transform, Clustering}
}

@article{10.1016/0004-3702(92)90008-L,
author = {Cooper, Paul R. and Swain, Michael J.},
title = {Arc consistency: parallelism and domain dependence},
year = {1992},
issue_date = {Dec. 1992},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {58},
number = {1–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(92)90008-L},
doi = {10.1016/0004-3702(92)90008-L},
journal = {Artif. Intell.},
month = dec,
pages = {207–235},
numpages = {29}
}

@article{10.1016/j.cad.2004.09.022,
author = {Fuxin, Freddy},
title = {Configurable product views based on geometry user requirements},
year = {2005},
issue_date = {August, 2005},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {37},
number = {9},
issn = {0010-4485},
url = {https://doi.org/10.1016/j.cad.2004.09.022},
doi = {10.1016/j.cad.2004.09.022},
abstract = {This paper describes an approach at Volvo Truck Corporation where geometry users' requirements are utilised to define configurable product views. The paper is derived from a research framework on geometry management. The objective of the research framework is to improve reuse of geometry by providing relevant geometry-based product information, so-called geometric product views, through application of configuration management. In order to provide relevant product views, specific fundamental product information must exist. This type of information will vary depending on the different types of product views. Several examples on product view are presented. The product views are realised by a developed configuration management system, an example of a configured product view is presented as proof of concept. The configuration management system is currently under industrial implementation.},
journal = {Comput. Aided Des.},
month = aug,
pages = {957–966},
numpages = {10},
keywords = {Digital product development, Digital mock-ups, Configuration management}
}

@inproceedings{10.1007/978-3-030-79382-1_24,
author = {Munoz, Daniel-Jesus and Gurov, Dilian and Pinto, Monica and Fuentes, Lidia},
title = {Category Theory Framework for Variability Models with Non-functional Requirements},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_24},
doi = {10.1007/978-3-030-79382-1_24},
abstract = {In Software Product Line (SPL) engineering one uses Variability Models (VMs) as input to automated reasoners to generate optimal products according to certain Quality Attributes (QAs). Variability models, however, and more specifically those including numerical features (i.e., NVMs), do not natively support QAs, and consequently, neither do automated reasoners commonly used for variability resolution. However, those satisfiability and optimisation problems have been covered and refined in other relational models such as databases.Category Theory (CT) is an abstract mathematical theory typically used to capture the common aspects of seemingly dissimilar algebraic structures. We propose a unified relational modelling framework subsuming the structured objects of VMs and QAs and their relationships into algebraic categories. This abstraction allows a combination of automated reasoners over different domains to analyse SPLs. The solutions’ optimisation can now be natively performed by a combination of automated theorem proving, hashing, balanced-trees and chasing algorithms. We validate this approach by means of the edge computing SPL tool HADAS.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {397–413},
numpages = {17},
keywords = {Category theory, Quality attribute, Non-functional requirement, Feature, Numerical variability model},
location = {Melbourne, VIC, Australia}
}

@article{10.1007/s11219-021-09564-z,
author = {Amit, Idan and Feitelson, Dror G.},
title = {Corrective commit probability: a measure of the effort invested in bug fixing},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09564-z},
doi = {10.1007/s11219-021-09564-z},
abstract = {The effort invested in software development should ideally be devoted to the implementation of new features. But some of the effort is invariably also invested in corrective maintenance, that is in fixing bugs. Not much is known about what fraction of software development work is devoted to bug fixing, and what factors affect this fraction. We suggest the Corrective Commit Probability (CCP), which measures the probability that a commit reflects corrective maintenance, as an estimate of the relative effort invested in fixing bugs. We identify corrective commits by applying a linguistic model to the commit messages, achieving an accuracy of 93%, higher than any previously reported model. We compute the CCP of all large active GitHub projects (7,557 projects with 200+ commits in 2019). This leads to the creation of an investment scale, suggesting that the bottom 10% of projects spend less than 6% of their total effort on bug fixing, while the top 10% of projects spend at least 39% of their effort on bug fixing — more than 6 times more. Being a process metric, CCP is conditionally independent of source code metrics, enabling their evaluation and investigation. Analysis of project attributes shows that lower CCP (that is, lower relative investment in bug fixing) is associated with smaller files, lower coupling, use of languages like JavaScript and C# as opposed to PHP and C++, fewer code smells, lower project age, better perceived quality, fewer developers, lower developer churn, better onboarding, and better productivity.},
journal = {Software Quality Journal},
month = dec,
pages = {817–861},
numpages = {45},
keywords = {Process metric, Effort estimate, Corrective commits, Corrective maintenance}
}

@article{10.1016/j.infsof.2019.03.014,
author = {Yadav, Asmita and Singh, Sandeep Kumar and Suri, Jasjit S.},
title = {Ranking of software developers based on expertise score for bug triaging},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.014},
doi = {10.1016/j.infsof.2019.03.014},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1–17},
numpages = {17},
keywords = {Cmv, BNu, TD t, VD s, PRD r, TSq, PTp, PRo, SRn, WBFSm, DNl, RULk, EMj, REPi, COMh, SVMg, NBf, Pe, CLSd, ASc, DOBb, SUMa, Software metrics, Software process, Open source software (OSS), Developer contribution assessment, Bug tossing, Bug reports, Bug assignment, Developer expertise, Bug triaging, Bug repository}
}

@inproceedings{10.1145/1411204.1411255,
author = {Dolstra, Eelco and L\"{o}h, Andres},
title = {NixOS: a purely functional Linux distribution},
year = {2008},
isbn = {9781595939197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1411204.1411255},
doi = {10.1145/1411204.1411255},
abstract = {Existing package and system configuration management tools suffer from an imperative model, where system administration actions such as upgrading packages or changes to system configuration files are stateful: they destructively update the state of the system. This leads to many problems, such as the inability to roll back changes easily, to run multiple versions of a package side-by-side, to reproduce a configuration deterministically on another machine, or to reliably upgrade a system. In this paper we show that we can overcome these problems by moving to a purely functional system configuration model. This means that all static parts of a system (such as software packages, configuration files and system startup scripts) are built by pure functions and are immutable, stored in a way analogously to a heap in a purely function language. We have implemented this model in NixOS, a non-trivial Linux distribution that uses the Nix package manager to build the entire system configuration from a purely functional specification.},
booktitle = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming},
pages = {367–378},
numpages = {12},
keywords = {system configuration management, software deployment, purely functional language, purely functional deployment model, package management, nix, NixOS},
location = {Victoria, BC, Canada},
series = {ICFP '08}
}

@article{10.1016/j.inffus.2013.10.010,
author = {Smirnov, Alexander and Levashova, Tatiana and Shilov, Nikolay},
title = {Patterns for context-based knowledge fusion in decision support systems},
year = {2015},
issue_date = {January, 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {21},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2013.10.010},
doi = {10.1016/j.inffus.2013.10.010},
abstract = {The here presented research focuses on the context-based knowledge fusion patterns. Patterns are discovered based on an analysis and investigation of knowledge fusion processes in a context aware decision support system at the operational stage of the system functioning. At this stage the context-based knowledge fusion processes are manifested around the context. The patterns are generalized in regard to the following three aspects: (1) the effects that the knowledge fusion processes produce in the system; (2) the preservation of internal structures for the context and multiple sources the information/knowledge is fused from; and (3) the preservation of multiple sources and the context autonomies. At that, seven knowledge fusion patterns have been discovered: simple fusion, extension, instantiated fusion, configured fusion, adaptation, flat fusion, and historical fusion.},
journal = {Inf. Fusion},
month = jan,
pages = {114–129},
numpages = {16},
keywords = {Ontology-based context, Knowledge fusion patterns, Context aware decision support}
}

@inproceedings{10.1007/978-3-030-26142-9_9,
author = {Wang, Yunyun and Zhao, Dan and Li, Yun and Chen, Kejia and Xue, Hui},
title = {The Most Related Knowledge First: A Progressive Domain Adaptation Method},
year = {2019},
isbn = {978-3-030-26141-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26142-9_9},
doi = {10.1007/978-3-030-26142-9_9},
abstract = {In domain adaptation, how to select and transfer related knowledge is critical for learning. Inspired by the fact that human usually transfer from the more related experience to the less related one, in this paper, we propose a novel progressive domain adaptation (PDA) model, which attempts to transfer source knowledge by considering the transfer order based on relevance. Specifically, PDA transfers source instances iteratively from the most related ones to the least related ones, until all related source instances have been adopted. It is an iterative learning process, source instances adopted in each iteration are determined by a gradually annealed weight such that the later iteration will introduce more source instances. Further, a reverse classification performance is used to set the termination of iteration. Experiments on real datasets demonstrate the competiveness of PDA compared with the state-of-arts.},
booktitle = {Trends and Applications in Knowledge Discovery and Data Mining: PAKDD 2019 Workshops, BDM, DLKT, LDRC, PAISI, WeL, Macau, China, April 14–17, 2019, Revised Selected Papers},
pages = {90–102},
numpages = {13},
keywords = {Reverse classification, Iteration, Progressive transfer, Domain adaptation},
location = {Macau, China}
}

@inproceedings{10.5555/3299905.3299978,
author = {Safavi, Saeid and Wang, Wenwu and Plumbley, Mark and Choobbasti, Ali Janalizadeh and Fazekas, George},
title = {Predicting the Perceived Level of Reverberation using Features from Nonlinear Auditory Model},
year = {2018},
publisher = {FRUCT Oy},
address = {Helsinki, Uusimaa, FIN},
abstract = {Perceptual measurements have typically been recognized as the most reliable measurements in assessing perceived levels of reverberation. In this paper, a combination of blind RT60 estimation method and a binaural, nonlinear auditory model is employed to derive signal-based measures (features) that are then utilized in predicting the perceived level of rever- beration. Such measures lack the excess of effort necessary for calculating perceptual measures; not to mention the variations in either stimuli or assessors that may cause such measures to be statistically insigni?cant. As a result, the automatic extraction of objective measurements that can be applied to predict the perceived level of reverberation become of vital signi?cance. Consequently, this work is aimed at discovering measurements such as clarity, reverberance, and RT60 which can automatically be derived directly from audio data. These measurements along with labels from human listening tests are then forwarded to a machine learning system seeking to build a model to estimate the perceived level of reverberation, which is labeled by an expert, autonomously. The data has been labeled by an expert human listener for a unilateral set of ?les from arbitrary audio source types. By examining the results, it can be observed that the automatically extracted features can aid in estimating the perceptual rates.},
booktitle = {Proceedings of the 23rd Conference of Open Innovations Association FRUCT},
articleno = {73},
numpages = {5},
keywords = {machine learning, Reverberation, Human experiments, Feature extraction, Audio signal processing},
location = {Bologna, Italy},
series = {FRUCT'23}
}

@inproceedings{10.5555/1860967.1860968,
title = {Front Matter},
year = {2010},
isbn = {9781607506058},
publisher = {IOS Press},
address = {NLD},
abstract = {I am delighted to have the honour to introduce the Proceedings of the Nineteenth European Conference on Artificial Intelligence (ECAI-2010), including the proceedings of the Sixth Conference on Prestigious Applications of Intelligent Systems (PAIS-2010).Artificial Intelligence (AI) is a central topic in contemporary computer science and informatics. The fruits of fifty years of AI research have benefited application domains as disparate as industrial systems control and medicine. The milestone events in AI research are increasingly regarded as milestones in human scientific and technological development: from the first chess playing program to defeat a reigning world champion under standard chess tournament rules, to the first robot to autonomously traverse 150 miles of rough terrain. Techniques, results, and concepts developed under the banner of AI research have proved to be of fundamental importance in areas such as economics, philosophy, linguistics, psychology, and logical analysis. And of course, AI remains a topic of perennial fascination in popular culture.Initiated in 1974, the biennial European Conference on Artificial Intelligence (ECAI) is Europe's premier archival venue for presenting scientific results in AI. Organised by the European Coordinating Committee for AI (ECCAI), the ECAI conference provides an opportunity for researchers to present and hear about the very best research in contemporary AI. As well as a full programme of technical papers, ECAI-2010 features the Prestigious Applications of Intelligent Systems conference (PAIS), the Starting AI Researcher Symposium (STAIRS), and an extensive programme of workshops, tutorials, and invited speakers.Some 607 papers were submitted to ECAI-2010, with the largest areas of submission being Knowledge Representation &amp; Reasoning, Multi-Agent Systems, and Machine Learning. After review by the international programme committee, 135 full papers were accepted for presentation, with further papers being accepted as short papers/posters. Overall, the acceptance rate for full papers was approximately 22%.Acknowledgments: Many thanks to Gerd Brewka for his continual encouragement and sound advice. Dave Shield at Liverpool maintained the web submission site, and without his support, my life would have been much, much, much more difficult over the past six months.Michael Wooldridge, University of Liverpool, UK, June 2010},
booktitle = {Proceedings of the 2010 Conference on ECAI 2010: 19th European Conference on Artificial Intelligence},
pages = {i–xxx}
}

@article{10.5555/3122009.3242055,
author = {Patrascu, Andrei and Necoara, Ion},
title = {Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {A popular approach for solving stochastic optimization problems is the stochastic gradient descent (SGD) method. Although the SGD iteration is computationally cheap and its practical performance may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate choice of parameters. To avoid some of the drawbacks of SGD, stochastic proximal point (SPP) algorithms have been recently considered. We introduce a new variant of the SPP method for solving stochastic convex problems subject to (in)finite intersection of constraints satisfying a linear regularity condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex Lipschitz continuous objective functions, we prove nonasymptotic convergence rates in terms of the expected value function gap of order O(1/k1/2), where k is the iteration counter. We also derive better nonasymptotic convergence rates in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order O(1/k). Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real problems.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {7204–7245},
numpages = {42},
keywords = {stochastic proximal point, stochastic convex optimization, rates of convergence, nonasymptotic convergence analysis, intersection of convex constraints}
}

@inproceedings{10.1007/978-3-030-98682-7_9,
author = {Antonioni, Emanuele and Riccio, Francesco and Nardi, Daniele},
title = {Improving Sample Efficiency in&nbsp;Behavior Learning by&nbsp;Using Sub-optimal Planners for&nbsp;Robots},
year = {2021},
isbn = {978-3-030-98681-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98682-7_9},
doi = {10.1007/978-3-030-98682-7_9},
abstract = {The design and implementation of behaviors for robots operating in dynamic and complex environments are becoming mandatory in nowadays applications. Reinforcement learning is consistently showing remarkable results in learning effective action policies and in achieving super-human performance in various tasks – without exploiting prior knowledge. However, in robotics, the use of purely learning-based techniques is still subject to strong limitations. Foremost, sample efficiency. Such techniques, in fact, are known to require large training datasets, and long training sessions, in order to develop effective action policies. Hence in this paper, to alleviate such constraint, and to allow learning in such robotic scenarios, we introduce SErP (Sample Efficient robot Policies), an iterative algorithm to improve the sample-efficiency of learning algorithms. SErP exploits a sub-optimal planner (here implemented with a monitor-replanning algorithm) to lead the exploration of the learning agent through its initial iterations. Intuitively, SErP exploits the planner as an expert in order to enable focused exploration and to avoid portions of the search space that are not effective to solve the task of the robot. Finally, to confirm our insights and to show the improvements that SErP carries with, we report the results obtained in two different robotic scenarios: (1) a cartpole scenario and (2) a soccer-robots scenario within the RoboCup@Soccer SPL environment.},
booktitle = {RoboCup 2021: Robot World Cup XXIV},
pages = {103–114},
numpages = {12},
keywords = {Decision-making, Reinforcement learning, Automated planning},
location = {Sydney, NSW, Australia}
}

@article{10.1016/j.dss.2012.10.005,
author = {Van Valkenhoef, Gert and Tervonen, Tommi and Zwinkels, Tijs and De Brock, Bert and Hillege, Hans},
title = {ADDIS: A decision support system for evidence-based medicine},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {2},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2012.10.005},
doi = {10.1016/j.dss.2012.10.005},
abstract = {Clinical trials are the main source of information for the efficacy and safety evaluation of medical treatments. Although they are of pivotal importance in evidence-based medicine, there is a lack of usable information systems providing data-analysis and decision support capabilities for aggregate clinical trial results. This is partly caused by unavailability (i) of trial data in a structured format suitable for re-analysis, and (ii) of a complete data model for aggregate level results. In this paper, we develop a unifying data model that enables the development of evidence-based decision support in the absence of a complete data model. We describe the supported decision processes and show how these are implemented in the open source ADDIS software. ADDIS enables semi-automated construction of meta-analyses, network meta-analyses and benefit-risk decision models, and provides visualization of all results.},
journal = {Decis. Support Syst.},
month = may,
pages = {459–475},
numpages = {17},
keywords = {ePRO, eLab, eCRF, caBIG, WHO, UMLS, TDM, SmPC, SPL, SNOMEDCT, SMAA, SEND, SDTM, RIM, QRD, PhRMA, PRM, PMDA, PIM, OWL, ODM, OCRe, OBX, NIHUS, NDA, NCI, MedDRA, MeSH, MCDA, LAB, JAMA, ICTRP, ICMJE, ICD, HSDB, HL7, GUI, GCP, FDAAA, FDA, Evidence-based medicine, Evidence synthesis, EPAR, EMA, EHR, EDC, EBM, EAV, Decision analysis, Data model, DSS, DOI, DIS, DED, DB, Clinical trial, CTMS, CTIS, CRO, CRF, CPOE, CHMP, CDMS, CDISC, CDASH, BRIDG, ATC, ANSI, AMIA, ADaM, ADR, ADE}
}

@inproceedings{10.1145/3379597.3387496,
author = {Spinellis, Diomidis and Kotti, Zoe and Mockus, Audris},
title = {A Dataset for GitHub Repository Deduplication},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387496},
doi = {10.1145/3379597.3387496},
abstract = {GitHub projects can be easily replicated through the site's fork process or through a Git clone-push sequence. This is a problem for empirical software engineering, because it can lead to skewed results or mistrained machine learning models. We provide a dataset of 10.6 million GitHub projects that are copies of others, and link each record with the project's ultimate parent. The ultimate parents were derived from a ranking along six metrics. The related projects were calculated as the connected components of an 18.2 million node and 12 million edge denoised graph created by directing edges to ultimate parents. The graph was created by filtering out more than 30 hand-picked and 2.3 million pattern-matched clumping projects. Projects that introduced unwanted clumping were identified by repeatedly visualizing shortest path distances between unrelated important projects. Our dataset identified 30 thousand duplicate projects in an existing popular reference dataset of 1.8 million projects. An evaluation of our dataset against another created independently with different methods found a significant overlap, but also differences attributed to the operational definition of what projects are considered as related.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {523–527},
numpages = {5},
keywords = {project clone, fork, dataset, GitHub, Deduplication},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.5555/646863.707945,
author = {Mitra, Debasis},
title = {Interactive Modeling for Batch Simulation of Engineering Systems: A Constraint Satisfaction Problem},
year = {2001},
isbn = {3540422196},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We have presented here an algorithm CPRAO for doing constraint processing using relational algebraic operators. Van Beek and others have investigated such constraint processing in the relational algebraic framework, for quite some time, producing some unique results. Apart from providing new theoretical angles, the framework also provides the opportunity to use the existing efficient implementations of relational database management systems as the underlying data structures for any relevant algorithm. Our algorithm here enhances that framework. The algorithm is quite general in its current form. Weak heuristics (like forward checking) developed within the CSP area could be plugged in this algorithm. The algorithm is developed here is targeted toward a component-oriented modeling problem that we are currently working on, namely, the problem of interactive modeling for batch-simulation of engineering systems (IMBSES). However, it could be adopted for many other CSP problems as well. The article here discusses the algorithm and many aspects of the problem we are addressing.},
booktitle = {Proceedings of the 14th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems: Engineering of Intelligent Systems},
pages = {602–611},
numpages = {10},
keywords = {planning and scheduling, modeling for simulation, interactive planning, intelligent software engineering, intelligent interfaces, constraint-based reasoning, component-oriented programming, AI application to design},
series = {IEA/AIE '01}
}

@inproceedings{10.1145/3001867.3001874,
author = {Queiroz, Rodrigo and Berger, Thorsten and Czarnecki, Krzysztof},
title = {Towards predicting feature defects in software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001874},
doi = {10.1145/3001867.3001874},
abstract = {Defect-prediction techniques can enhance the quality assurance activities for software systems. For instance, they can be used to predict bugs in source files or functions. In the context of a software product line, such techniques could ideally be used for predicting defects in features or combinations of features, which would allow developers to focus quality assurance on the error-prone ones. In this preliminary case study, we investigate how defect prediction models can be used to identify defective features using machine-learning techniques. We adapt process metrics and evaluate and compare three classifiers using an open-source product line. Our results show that the technique can be effective. Our best scenario achieves an accuracy of 73 % for accurately predicting features as defective or clean using a Naive Bayes classifier. Based on the results we discuss directions for future work.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {58–62},
numpages = {5},
keywords = {software product lines, features, defect prediction},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1016/j.eswa.2014.04.046,
author = {Chin, Kwai-Sang and Fu, Chao},
title = {Integrated evidential reasoning approach in the presence of cardinal and ordinal preferences and its applications in software selection},
year = {2014},
issue_date = {November, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {15},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.04.046},
doi = {10.1016/j.eswa.2014.04.046},
abstract = {A combination of cardinal and ordinal preferences in multiple-attribute decision making (MADM) demonstrates more reliability and flexibility compared with sole cardinal or ordinal preferences derived from a decision maker. This situation occurs particularly when the knowledge and experience of the decision maker, as well as the data regarding specific alternatives on certain attributes, are insufficient or incomplete. This paper proposes an integrated evidential reasoning (IER) approach to analyze uncertain MADM problems in the presence of cardinal and ordinal preferences. The decision maker provides complete or incomplete cardinal and ordinal preferences of each alternative on each attribute. Ordinal preferences are expressed as unknown distributed assessment vectors and integrated with cardinal preferences to form aggregated preferences of alternatives. Three optimization models considering cardinal and ordinal preferences are constructed to determine the minimum and maximum minimal satisfaction of alternatives, simultaneous maximum minimal satisfaction of alternatives, and simultaneous minimum minimal satisfaction of alternatives. The minimax regret rule, the maximax rule, and the maximin rule are employed respectively in the three models to generate three kinds of value functions of alternatives, which are aggregated to find solutions. The attribute weights in the three models can be precise or imprecise (i.e., characterized by six types of constraints). The IER approach is used to select the optimum software for product lifecycle management of a famous Chinese automobile manufacturing enterprise.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {6718–6727},
numpages = {10},
keywords = {Multiple-attribute decision making, Integrated decision, Evidential reasoning approach, Decision analysis, Cardinal and ordinal preferences}
}

@inproceedings{10.1145/3167132.3167162,
author = {Hielscher, Tommy and V\"{o}lzke, Henry and Papapetrou, Panagiotis and Spiliopoulou, Myra},
title = {Discovering, selecting and exploiting feature sequence records of study participants for the classification of epidemiological data on hepatic steatosis},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167162},
doi = {10.1145/3167132.3167162},
abstract = {In longitudinal epidemiological studies, participants undergo repeated medical examinations and are thus represented by a potentially large number of short examination outcome sequences. Some of those sequences may contain important information in various forms, such as patterns, with respect to the disease under study, while others may be on features of little relevance to the outcome. In this work, we propose a framework for Discovery, Selection and Exploitation (DiSelEx) of longitudinal epidemiological data, aiming to identify informative patterns among these sequences. DiSelEx combines sequence clustering with supervised learning to identify sequence groups that contribute to class separation. Newly derived and old features are evaluated and selected according to their redundancy and informativeness regarding the target variable. The selected feature set is then used to learn a classification model on the study data. We evaluate DiSelEx on cohort participants for the disorder "hepatic steatosis" and report on the impact on predictive performance when using sequential data in comparison to utilizing only the basic classifier.1},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {6–13},
numpages = {8},
keywords = {time-series clustering, patient similarity, medical data mining, hepatic steatosis, feature selection, epidemiological studies, classification},
location = {Pau, France},
series = {SAC '18}
}

@article{10.1007/s00500-021-06096-3,
author = {Pandey, Sushant Kumar and Tripathi, Anil Kumar},
title = {An empirical study toward dealing with noise and class imbalance issues in software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06096-3},
doi = {10.1007/s00500-021-06096-3},
abstract = {The quality of the defect datasets is a critical issue in the domain of software defect prediction (SDP). These datasets are obtained through the mining of software repositories. Recent studies claim over the quality of the defect dataset. It is because of inconsistency between bug/clean fix keyword in fault reports and the corresponding link in the change management logs. Class Imbalance (CI) problem is also a big challenging issue in SDP models. The defect prediction method trained using noisy and imbalanced data leads to inconsistent and unsatisfactory results. Combined analysis over noisy instances and CI problem needs to be required. To the best of our knowledge, there are insufficient studies that have been done over such aspects. In this paper, we deal with the impact of noise and CI problem on five baseline SDP models; we manually added the various noise level (0–80%) and identified its impact on the performance of those SDP models. Moreover, we further provide guidelines for the possible range of tolerable noise for baseline models. We have also suggested the SDP model, which has the highest noise tolerable ability and outperforms over other classical methods. The True Positive Rate (TPR) and False Positive Rate (FPR) values of the baseline models reduce between 20–30% after adding 10–40% noisy instances. Similarly, the ROC (Receiver Operating Characteristics) values of SDP models reduce to 40–50%. The suggested model leads to avoid noise between 40–60% as compared to other traditional models.},
journal = {Soft Comput.},
month = nov,
pages = {13465–13492},
numpages = {28},
keywords = {Fault proneness, Software metrics, Machine learning, Noisy instance, Class imbalance, Software fault prediction, Software testing}
}

@inproceedings{10.1145/3292500.3330990,
author = {Chen, Yu-Chia and Bijral, Avleen S. and Ferres, Juan Lavista},
title = {On Dynamic Network Models and Application to Causal Impact},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330990},
doi = {10.1145/3292500.3330990},
abstract = {Dynamic extensions of Stochastic block model (SBM) are of importance in several fields that generate temporal interaction data. These models, besides producing compact and interpretable network representations, can be useful in applications such as link prediction or network forecasting. In this paper we present a conditional pseudo-likelihood based extension to dynamic SBM that can be efficiently estimated by optimizing a regularized objective. Our formulation leads to a highly scalable approach that can handle very large networks, even with millions of nodes. We also extend our formalism to causal impact for networks that allows us to quantify the impact of external events on a time dependent sequence of networks. We support our work with extensive results on both synthetic and real networks.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1194–1204},
numpages = {11},
keywords = {stochastic block model, dynamic networks, clustering, causal impact},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1145/3039207,
author = {Hirzel, Martin and Schneider, Scott and Gedik, Bu\u{g}ra},
title = {SPL: An Extensible Language for Distributed Stream Processing},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/3039207},
doi = {10.1145/3039207},
abstract = {Big data is revolutionizing how all sectors of our economy do business, including telecommunication, transportation, medical, and finance. Big data comes in two flavors: data at rest and data in motion. Processing data in motion is stream processing. Stream processing for big data analytics often requires scale that can only be delivered by a distributed system, exploiting parallelism on many hosts and many cores. One such distributed stream processing system is IBM Streams. Early customer experience with IBM Streams uncovered that another core requirement is extensibility, since customers want to build high-performance domain-specific operators for use in their streaming applications. Based on these two core requirements of distribution and extensibility, we designed and implemented the Streams Processing Language (SPL). This article describes SPL with an emphasis on the language design, distributed runtime, and extensibility mechanism. SPL is now the gateway for the IBM Streams platform, used by our customers for stream processing in a broad range of application domains.},
journal = {ACM Trans. Program. Lang. Syst.},
month = mar,
articleno = {5},
numpages = {39},
keywords = {Stream processing}
}

@inproceedings{10.1109/ASE.2019.00120,
author = {Reuling, Dennis and Kelter, Udo and Ruland, Sebastian and Lochau, Malte},
title = {SiMPOSE: configurable N-way program merging strategies for superimposition-based analysis of variant-rich software},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00120},
doi = {10.1109/ASE.2019.00120},
abstract = {Modern software often exists in many different, yet similar versions and/or variants, usually derived from a common code base (e.g., via clone-and-own). In the context of product-line engineering, family-based analysis has shown very promising potential for improving efficiency in applying quality-assurance techniques to variant-rich software, as compared to a variant-by-variant approach. Unfortunately, these strategies rely on a product-line representation superimposing all program variants in a syntactically well-formed, semantically sound and variant-preserving manner, which is manually hard to obtain in practice. We demonstrate the SiMPOSE methodology for automatically generating superimpositions of N given program versions and/or variants facilitating family-based analysis of variant-rich software. SiMPOSE is based on a novel N-way model-merging technique operating at the level of control-flow automata (CFA) representations of C programs. CFAs constitute a unified program abstraction utilized by many recent software-analysis tools. We illustrate different merging strategies supported by SiMPOSE, namely variant-by-variant, N-way merging, incremental 2-way merging, and partition-based N/2-way merging, and demonstrate how SiMPOSE can be used to systematically compare their impact on efficiency and effectiveness of family-based unit-test generation. The SiMPOSE tool, the demonstration of its usage as well as related artifacts and documentation can be found at http://pi.informatik.uni-siegen.de/projects/variance/simpose.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1134–1137},
numpages = {4},
keywords = {software testing, program merging, model merging, family-based analyses},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1016/j.datak.2010.01.002,
author = {Reinhartz-Berger, Iris},
title = {Towards automatization of domain modeling},
year = {2010},
issue_date = {May, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {69},
number = {5},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2010.01.002},
doi = {10.1016/j.datak.2010.01.002},
abstract = {A domain model, which captures the common knowledge and the possible variability allowed among applications in a domain, may assist in the creation of other valid applications in that domain. However, to create such domain models is not a trivial task: it requires expertise in the domain, reaching a very high level of abstraction, and providing flexible, yet formal, artifacts. In this paper an approach, called Semi-automated Domain Modeling (SDM), to create draft domain models from applications in those domains, is presented. SDM takes a repository of application models in a domain and matches, merges, and generalizes them into sound draft domain models that include the commonality and variability allowed in these domains. The similarity of the different elements is measured, with consideration of syntactic, semantic, and structural aspects. Unlike ontology and schema integration, these models capture both structural and behavioral aspects of the domain. Running SDM on small repositories of project management applications and scheduling systems, we found that the approach may provide reasonable draft domain models, whose comprehensibility, correctness, completeness, and consistency levels are satisfactory.},
journal = {Data Knowl. Eng.},
month = may,
pages = {491–515},
numpages = {25},
keywords = {UML, Product line engineering, Metamodeling, Domain engineering, Domain analysis, DSL}
}

@inproceedings{10.1145/3486949.3486965,
author = {ElBatanony, Ahmed and Succi, Giancarlo},
title = {Towards the no-code era: a vision and plan for the future of software development},
year = {2021},
isbn = {9781450391252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486949.3486965},
doi = {10.1145/3486949.3486965},
abstract = {This paper provides a highly opinionated and biased vision and a two-stage plan with guidelines to reach a new era of software development, where anyone can create software without bothering to write code. Moreover, this paper explores in depth the first of these stages, which consists of creating a no-code tool based on six principles: configuration driven development, APIs, open-source, cross-platform, cloud computing, and design systems. An examination of each principle is presented and a case is made for why such a combination of principles would lay the foundation for future development efforts. Possible enquiries are addressed and a path is laid out for future works.},
booktitle = {Proceedings of the 1st ACM SIGPLAN International Workshop on Beyond Code: No Code},
pages = {29–35},
numpages = {7},
keywords = {Open-source, No-code, Design Systems, Cross-platform, Configuration Driven Development, Cloud Computing, API},
location = {Chicago, IL, USA},
series = {BCNC 2021}
}

@article{10.1016/j.patcog.2019.106972,
author = {Dong, Ganggang and Liu, Hongwei and Kuang, Gangyao and Chanussot, Jocelyn},
title = {Target recognition in SAR images via sparse representation in the frequency domain},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {96},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.106972},
doi = {10.1016/j.patcog.2019.106972},
journal = {Pattern Recogn.},
month = dec,
numpages = {10},
keywords = {Target recognition, Transformed domain, Sparse representation}
}

@article{10.1145/3169795,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Lau, Jey Han and Abebe, Ermyas and Ruan, Wenjie},
title = {Duplicate Detection in Programming Question Answering Communities},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3169795},
doi = {10.1145/3169795},
abstract = {Community-based Question Answering (CQA) websites are attracting increasing numbers of users and contributors in recent years. However, duplicate questions frequently occur in CQA websites and are currently manually identified by the moderators. Automatic duplicate detection, on one hand, alleviates this laborious effort for moderators before taking close actions, and, on the other hand, helps question issuers quickly find answers. A number of studies have looked into related problems, but very limited works target Duplicate Detection in Programming CQA (PCQA), a branch of CQA that is dedicated to programmers. Existing works framed the task as a supervised learning problem on the question pairs and relied on only textual features. Moreover, the issue of selecting candidate duplicates from large volumes of historical questions is often un-addressed. To tackle these issues, we model duplicate detection as a two-stage “ranking-classification” problem over question pairs. In the first stage, we rank the historical questions according to their similarities to the newly issued question and select the top ranked ones as candidates to reduce the search space. In the second stage, we develop novel features that capture both textual similarity and latent semantics on question pairs, leveraging techniques in deep learning and information retrieval literature. Experiments on real-world questions about multiple programming languages demonstrate that our method works very well; in some cases, up to 25% improvement compared to the state-of-the-art benchmarks.},
journal = {ACM Trans. Internet Technol.},
month = apr,
articleno = {37},
numpages = {21},
keywords = {question quality, latent semantics, classification, association rules, Community-based question answering}
}

@inproceedings{10.5555/3540261.3542304,
author = {Zhang, Jiangning and Xu, Chao and Li, Jian and Chen, Wenzhou and Wang, Yabiao and Tai, Ying and Chen, Shuo and Wang, Chengjie and Huang, Feiyue and Liu, Yong},
title = {Analogous to evolutionary algorithm: designing a unified sequence model},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Inspired by biological evolution, we explain the rationality of Vision Transformer by analogy with the proven practical Evolutionary Algorithm (EA) and derive that both of them have consistent mathematical representation. Analogous to the dynamic local population in EA, we improve the existing transformer structure and propose a more efficient EAT model, and design task-related heads to deal with different tasks more flexibly. Moreover, we introduce the spatial-filling curve into the current vision transformer to sequence image data into a uniform sequential format. Thus we can design a unified EAT framework to address multi-modal tasks, separating the network architecture from the data format adaptation. Our approach achieves state-of-the-art results on the ImageNet classification task compared with recent vision transformer works while having smaller parameters and greater throughput. We further conduct multi-modal tasks to demonstrate the superiority of the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves the rank-1 by +3.7 points over the baseline on the CSS dataset.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2043},
numpages = {15},
series = {NIPS '21}
}

@article{10.1016/j.infsof.2021.106668,
author = {Tong, Yao and Zhang, Xiaofang},
title = {Crowdsourced test report prioritization considering bug severity},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106668},
doi = {10.1016/j.infsof.2021.106668},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {15},
keywords = {Textual description, Bug severity, Prioritization, Test report processing, Crowdsourced testing}
}

@inproceedings{10.5555/645456.654379,
author = {Cain, James Westland and McCrindle, Rachel Jane},
title = {Making Movies: Watching Software Evolve through Visualisation},
year = {2001},
isbn = {3540422331},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper introduces an innovative visualisation technique for exposing the software defects that develop as a software project evolves. The application of this technique to a large-scale industrial software project is described together with the way in which the technique was modified to enable integration with the software configuration management process. The paper shows how a number of forces acting on the project can be equated to changes in the visualisations and how this can be used as a measure of the quality of the software.},
booktitle = {Proceedings of the International Conference on Computational Science-Part II},
pages = {738–750},
numpages = {13},
keywords = {software visualisation, software evolution, software configuration management, large scale software engineering},
series = {ICCS '01}
}

@article{10.1007/s11277-020-07481-1,
author = {Mohajer, Amin and Bavaghar, Maryam and Farrokhi, Hamid},
title = {RETRACTED ARTICLE: Reliability and Mobility Load Balancing in Next Generation Self-organized Networks: Using Stochastic Learning Automata: Reliability and Mobility Load Balancing in Next Generation…},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {114},
number = {3},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-020-07481-1},
doi = {10.1007/s11277-020-07481-1},
abstract = {Self-organizing networking (SON) is an automation technology designed to make the planning, configuration, management, optimization and healing of mobile radio access networks simpler and faster. Most current self-organization networking functions apply rule-based recommended systems to control network resources which seem too complicated and time-consuming to design in practical conditions. This research proposes a cognitive cellular network empowered by an efficient self-organization networking approach which enables SON functions to separately learn and find the best configuration setting. An effective learning approach is proposed for the functions of the cognitive cellular network, which exhibits how the framework is mapped to SON functions. One of the main functions applied in this framework is mobility load balancing. In this paper, a novel Stochastic Learning Automata has been suggested as the load balancing function in which approximately the same quality level is provided for each subscriber. This framework can also be effectively extended to cloud-based systems, where adaptive approaches are needed due to unpredictability of total accessible resources, considering cooperative nature of cloud environments. The results demonstrate that the function of mobility robustness optimization not only learns to optimize HO performance, but also it learns how to distribute excess load throughout the network. The experimental results demonstrate that the proposed scheme minimizes the number of unsatisfied subscribers (Nus) by moving some of the edge users served by overloaded cells towards one or more adjacent target cells. This solution can also guarantee a more balanced network using cell load sharing approach in addition to increase cell throughput outperform the current schemes.},
journal = {Wirel. Pers. Commun.},
month = oct,
pages = {2389–2415},
numpages = {27},
keywords = {Self-organization networks, Reliability, Robustness optimization, mobility load balancing, Cognitive cellular networks, Learning automata}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00109,
author = {Dey, Tapajit and Karnauch, Andrey and Mockus, Audris},
title = {Replication package for representation of developer expertise in open source software},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00109},
doi = {10.1109/ICSE-Companion52605.2021.00109},
abstract = {This describes the artifact associated with the article "Representation of Developer Expertise in Open Source Software" at the International Conference on Software Engineering 2021. The aim of the original paper was to define a feasible representation of a developer's expertise in specific focus areas of software development by gauging their fluency with different sets of APIs. The artifact is made available through Zenodo under the CC-BY-4.0 license at https://doi.org/10.5281/zenodo.4457107. The README file has detailed instructions on how to replicate the results presented in the original paper. The artifact includes the input dataset (with the developers' names and email addresses replaced by their corresponding SHA1 digest values to protect privacy) and all the associated scripts. The trained Doc2Vec models are also included in the artifact. These models can be used to obtain the Skill Space representations of developers, projects, and APIs without having to re-train the model.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {236–237},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3387904.3389251,
author = {Aung, Thazin Win Win and Huo, Huan and Sui, Yulei},
title = {A Literature Review of Automatic Traceability Links Recovery for Software Change Impact Analysis},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389251},
doi = {10.1145/3387904.3389251},
abstract = {In large-scale software development projects, change impact analysis (CIA) plays an important role in controlling software design evolution. Identifying and accessing the effects of software changes using traceability links between various software artifacts is a common practice during the software development cycle. Recently, research in automated traceability-link recovery has received broad attention in the software maintenance community to reduce the manual maintenance cost of trace links by developers. In this study, we conducted a systematic literature review related to automatic traceability link recovery approaches with a focus on CIA. We identified 33 relevant studies and investigated the following aspects of CIA: traceability approaches, CIA sets, degrees of evaluation, trace direction and methods for recovering traceability link between artifacts of different types. Our review indicated that few traceability studies focused on designing and testing impact analysis sets, presumably due to the scarcity of datasets. Based on the findings, we urge further industrial case studies. Finally, we suggest developing traceability tools to support fully automatic traceability approaches, such as machine learning and deep learning.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {14–24},
numpages = {11},
keywords = {traceability, natural language processing, change impact analysis},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@article{10.5555/2784062.2784495,
author = {Luo, Xudong and Jennings, Nicholas R. and Shadbolt, Nigel and Leung, Ho-fung and Lee, Jimmy Ho-man},
title = {A fuzzy constraint based model for bilateral, multi-issue negotiations in semi-competitive environments},
year = {2003},
issue_date = {August 2003},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {148},
number = {1},
issn = {0004-3702},
abstract = {This paper develops a fuzzy constraint based model for bilateral multi-issue negotiation in trading environments. In particular, we are concerned with the principled negotiation approach in which agents seek to strike a fair deal for both parties, but which, nevertheless, maximises their own payoff. Thus, there are elements of both competition and cooperation in the negotiation (hence semi-competitive environments). One of the key intuitions of the approach is that there is often more than one option that can satisfy the interests of both parties. So, if the opponent cannot accept an offer then the proponent should endeavour to find an alternative that is equally acceptable to it, but more acceptable to the opponent. That is, the agent should make a trade-off. Only if such a trade-off is not possible should the agent make a concession. Against this background, our model ensures the agents reach a deal that is fair (Pareto-optimal) for both parties if such a solution exists. Moreover, this is achieved by minimising the amount of private information that is revealed. The model uses prioritised fuzzy constraints to represent trade-offs between the different possible values of the negotiation issues and to indicate how concessions should be made when they are necessary. Also by using constraints to express negotiation proposals, the model can cover the negotiation space more efficiently since each exchange covers a region rather than a single point (which is what most existing models deal with). In addition, by incorporating the notion of a reward into our negotiation model, the agents can sometimes reach agreements that would not otherwise be possible.},
journal = {Artif. Intell.},
month = aug,
pages = {53–102},
numpages = {50},
keywords = {Software agents, Fuzzy constraint satisfaction, E-business, Automated negotiation}
}

@article{10.1016/j.patrec.2021.06.029,
author = {Chaudhary, Sachin and Dudhane, Akshay and Patil, Prashant W. and Murala, Subrahmanyam and Talbar, Sanjay},
title = {Motion estimation in hazy videos},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {150},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.06.029},
doi = {10.1016/j.patrec.2021.06.029},
journal = {Pattern Recogn. Lett.},
month = oct,
pages = {130–138},
numpages = {9},
keywords = {Motion estimation, Scene understanding, 65D17, 65D05, 41A10, 41A05}
}

@article{10.1007/s10664-017-9580-7,
author = {Przyby\l{}ek, Adam},
title = {An empirical study on the impact of AspectJ on software evolvability},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9580-7},
doi = {10.1007/s10664-017-9580-7},
abstract = {Since its inception in 1996, aspect-oriented programming (AOP) has been believed to reduce the effort required to maintain software systems by replacing cross-cutting code with aspects. However, little convincing empirical evidence exists to support this claim, while several studies suggest that AOP brings new obstacles to maintainability. This paper discusses two controlled experiments conducted to evaluate the impact of AspectJ (the most mature and popular aspect-oriented programming language) versus Java on software evolvability. We consider evolvability as the ease with which a software system can be updated to fulfill new requirements. Since a minor language was compared to the mainstream, the experiments were designed so as to anticipate that the participants were much more experienced in one of the treatments. The first experiment was performed on 35 student subjects who were asked to comprehend either Java or AspectJ implementation of the same system, and perform the corresponding comprehension tasks. Participants of both groups achieved a high rate of correct answers without a statistically significant difference between the groups. Nevertheless, the Java group significantly outperformed the AspectJ group with respect to the average completion time. In the second experiment, 24 student subjects were asked to implement (in a non-invasive way) two extension scenarios to the system that they had already known. Each subject evolved either the Java version using Java or the AspectJ version using AspectJ. We found out that a typical AspectJ programmer needs significantly fewer atomic changes to implement the change scenarios than a typical Java programmer, but we did not observe a significant difference in completion time. The overall result indicates that AspectJ has a different effect on two sub-characteristics of the evolvability: understandability and changeability. While AspectJ decreases the former, it improves one aspect of the latter.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2018–2050},
numpages = {33},
keywords = {Understandability, Separation of concerns, Maintainability, Controlled experiment, Aspect-oriented programming, AOP}
}

@article{10.1016/j.artmed.2012.03.004,
author = {Mariam, Mai and Delb, Wolfgang and Schick, Bernhard and Strauss, Daniel J.},
title = {Feasibility of an objective electrophysiological loudness scaling: A kernel-based novelty detection approach},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {55},
number = {3},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2012.03.004},
doi = {10.1016/j.artmed.2012.03.004},
abstract = {Objective: The objective of our research is to structure a foundation for an electrophysiological loudness scaling measurement, in particular to estimate an uncomfortable loudness (UCL) level by using the hybrid wavelet-kernel novelty detection (HWND). Methods and materials: Late auditory evoked potentials (LAEPs) were obtained from 20 normal hearing adults. These LAEPs were stimulated by 4 intensity levels (60 decibel (dB) sound pressure level (SPL), 70dB SPL, 80dB SPL, and 90dB SPL). We have extracted the habituation correlates in LAEPs by using HWND. For this, we employed a lattice structure-based wavelet frame decompositions for feature extraction combined with a kernel-based novelty detector. Results: The group results showed that the habituation correlates degrees, i.e., relative changes within the sweep sequences, were significantly different among 60dB SPL, 70dB SPL, 80dB SPL, and 90dB SPL stimulation level, independently from the intensity related amplitude information in the averaged LAEPs. At these particular intensities, 60% of the subjects show the correlation between the novelty measures and the stimulation levels resembles a loudness scaling function, in reverse. In this paper, we have found a correlation in between the novelty measures and loudness perception as well. We have found that high ranges of loudness levels such as loud, upper level and too loud show generally 4.88% of novelty measures and comfortable ranges of loudness levels, i.e., soft, comfortable but soft, comfortable loud and comfortable but loud are generally have 12.29% of novelty measures. Additionally, we demonstrated that our sweep-to-sweep basis of post processing scheme is reliable for habituation extraction and offers an advantage of reducing experimental time as the proposed scheme need less than 20% of single sweeps in comparison to the amount that are commonly used in arithmetical average for a meaningful result. Conclusions: We assessed the feasibility of habituation correlates for an objective loudness scaling. With respect to this first feasibility study, the presented results are promising when using the described signal processing and machine learning methodology. For the group results, the novelty measures approach is able to discriminate 60dB, 70dB, 80dB and 90dB stimulated sweeps. In addition, a correlation between the novelty measures and the subjective loudness scaling is observed. However, more loudness perception and frequency specific experiments need to be conducted to determine the UCL novelty measures threshold as well as clinically oriented studies are necessary to evaluate whether this approach might be used in the objective hearing instrument fitting procedures.},
journal = {Artif. Intell. Med.},
month = jul,
pages = {185–195},
numpages = {11},
keywords = {Uncomfortable loudness level, Loudness scaling, Kernel machines, Habituation, Event-related potentials, Adapted filter banks}
}

@article{10.1007/s00500-014-1364-z,
author = {Baladhandapani, Arunadevi and Nachimuthu, Deepa Subramaniam},
title = {Evolutionary learning of spiking neural networks towards quantification of 3D MRI brain tumor tissues},
year = {2015},
issue_date = {July      2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {7},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-014-1364-z},
doi = {10.1007/s00500-014-1364-z},
abstract = {This paper presents a new classification technique for 3D MR images, based on a third-generation network of spiking neurons. Implementation of multi-dimensional co-occurrence matrices for the identification of pathological tumor tissue and normal brain tissue features are assessed. The results show the ability of spiking classifier with iterative training using genetic algorithm to automatically and simultaneously recover tissue-specific structural patterns and achieve segmentation of tumor part. The spiking network classifier has been validated and tested for various real-time and Harvard benchmark datasets, where appreciable performance in terms of mean square error, accuracy and computational time is obtained. The spiking network employed Izhikevich neurons as nodes in a multi-layered structure. The classifier has been compared with computational power of multi-layer neural networks with sigmoidal neurons. The results on misclassified tumors are analyzed and suggestions for future work are discussed.},
journal = {Soft Comput.},
month = jul,
pages = {1803–1816},
numpages = {14},
keywords = {Spiking neural networks, Multi-dimensional co-occurrence matrices, Izhikevich neurons, Genetic algorithm, 3D Magnetic resonance imaging}
}

@article{10.5555/2598944.2599210,
author = {Maldonado, Sebasti\'{a}n and L\'{o}pez, Julio},
title = {Alternative second-order cone programming formulations for support vector classification},
year = {2014},
issue_date = {June, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {268},
issn = {0020-0255},
abstract = {This paper presents two novel second-order cone programming (SOCP) formulations that determine a linear predictor using Support Vector Machines (SVMs). Inspired by the soft-margin SVM formulation, our first approach (@x-SOCP-SVM) proposes a relaxation of the conic constraints via a slack variable, penalizing it in the objective function. The second formulation (r-SOCP-SVM) is based on the LP-SVM formulation principle: the bound of the VC dimension is loosened properly using the l"~-norm, and the margin is directly maximized. The proposed methods have several advantages: The first approach constructs a flexible classifier, extending the benefits of the soft-margin SVM formulation to second-order cones. The second method obtains comparable results to the SOCP-SVM formulation with less computational effort, since one conic restriction is eliminated. Experiments on well-known benchmark datasets from the UCI Repository demonstrate that our approach accomplishes the best classification performance compared to the traditional SOCP-SVM formulation, LP-SVM, and to standard linear SVM.},
journal = {Inf. Sci.},
month = jun,
pages = {328–341},
numpages = {14},
keywords = {Support Vector Machine, Second-order cone programming, Linear programming SVM}
}

@inproceedings{10.5555/2432523.2432528,
author = {Hewson, John A. and Anderson, Paul and Gordon, Andrew D.},
title = {A declarative approach to automated configuration},
year = {2012},
publisher = {USENIX Association},
address = {USA},
abstract = {System administrators increasingly use declarative, object-oriented languages to configure their systems. Extending such systems with automated analysis and decision making is an area of active research. We introduce ConfSolve, an object-oriented declarative configuration language, in which logical constraints over a system can be specified. Verification, impact analysis or even the generation of valid configurations can then be performed, by translation to a Constraint Satisfaction Problem (CSP), which is solved with an off-the-shelf solver. We present a full definition of our language and its compilation process, and show that our implementation outperforms previous work utilising an SMT solver, while adding new features such as optimisation.},
booktitle = {Proceedings of the 26th International Conference on Large Installation System Administration: Strategies, Tools, and Techniques},
pages = {51–66},
numpages = {16},
keywords = {constraints, configuration management, automation},
location = {San Diego, CA},
series = {lisa'12}
}

@article{10.1016/j.patcog.2008.09.015,
author = {Liu, Manhua and Jiang, Xudong and Kot, Alex C.},
title = {A multi-prototype clustering algorithm},
year = {2009},
issue_date = {May, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {42},
number = {5},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.09.015},
doi = {10.1016/j.patcog.2008.09.015},
abstract = {Clustering is an important unsupervised learning technique widely used to discover the inherent structure of a given data set. Some existing clustering algorithms uses single prototype to represent each cluster, which may not adequately model the clusters of arbitrary shape and size and hence limit the clustering performance on complex data structure. This paper proposes a clustering algorithm to represent one cluster by multiple prototypes. The squared-error clustering is used to produce a number of prototypes to locate the regions of high density because of its low computational cost and yet good performance. A separation measure is proposed to evaluate how well two prototypes are separated. Multiple prototypes with small separations are grouped into a given number of clusters in the agglomerative method. New prototypes are iteratively added to improve the poor cluster separations. As a result, the proposed algorithm can discover the clusters of complex structure with robustness to initial settings. Experimental results on both synthetic and real data sets demonstrate the effectiveness of the proposed clustering algorithm.},
journal = {Pattern Recogn.},
month = may,
pages = {689–698},
numpages = {10},
keywords = {Squared-error clustering, Separation measure, Data clustering, Cluster prototype}
}

@article{10.1155/2020/7917021,
author = {Zhang, Cheng and He, Dan and Zhang, Qingchen},
title = {A Deep Multiscale Fusion Method via Low-Rank Sparse Decomposition for Object Saliency Detection Based on Urban Data in Optical Remote Sensing Images},
year = {2020},
issue_date = {2020},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2020},
issn = {1530-8669},
url = {https://doi.org/10.1155/2020/7917021},
doi = {10.1155/2020/7917021},
abstract = {The urban data provides a wealth of information that can support the life and work for people. In this work, we research the object saliency detection in optical remote sensing images, which is conducive to the interpretation of urban scenes. Saliency detection selects the regions with important information in the remote sensing images, which severely imitates the human visual system. It plays a powerful role in other image processing. It has successfully made great achievements in change detection, object tracking, temperature reversal, and other tasks. The traditional method has some disadvantages such as poor robustness and high computational complexity. Therefore, this paper proposes a deep multiscale fusion method via low-rank sparse decomposition for object saliency detection in optical remote sensing images. First, we execute multiscale segmentation for remote sensing images. Then, we calculate the saliency value, and the proposal region is generated. The superpixel blocks of the remaining proposal regions of the segmentation map are input into the convolutional neural network. By extracting the depth feature, the saliency value is calculated and the proposal regions are updated. The feature transformation matrix is obtained based on the gradient descent method, and the high-level semantic prior knowledge is obtained by using the convolutional neural network. The process is iterated continuously to obtain the saliency map at each scale. The low-rank sparse decomposition of the transformed matrix is carried out by robust principal component analysis. Finally, the weight cellular automata method is utilized to fuse the multiscale saliency graphs and the saliency map calculated according to the sparse noise obtained by decomposition. Meanwhile, the object priors knowledge can filter most of the background information, reduce unnecessary depth feature extraction, and meaningfully improve the saliency detection rate. The experiment results show that the proposed method can effectively improve the detection effect compared to other deep learning methods.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {14}
}

@article{10.1016/j.csl.2012.01.008,
author = {Li, Ming and Han, Kyu J. and Narayanan, Shrikanth},
title = {Automatic speaker age and gender recognition using acoustic and prosodic level information fusion},
year = {2013},
issue_date = {January, 2013},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {27},
number = {1},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2012.01.008},
doi = {10.1016/j.csl.2012.01.008},
abstract = {The paper presents a novel automatic speaker age and gender identification approach which combines seven different methods at both acoustic and prosodic levels to improve the baseline performance. The three baseline subsystems are (1) Gaussian mixture model (GMM) based on mel-frequency cepstral coefficient (MFCC) features, (2) Support vector machine (SVM) based on GMM mean supervectors and (3) SVM based on 450-dimensional utterance level features including acoustic, prosodic and voice quality information. In addition, we propose four subsystems: (1) SVM based on UBM weight posterior probability supervectors using the Bhattacharyya probability product kernel, (2) Sparse representation based on UBM weight posterior probability supervectors, (3) SVM based on GMM maximum likelihood linear regression (MLLR) matrix supervectors and (4) SVM based on the polynomial expansion coefficients of the syllable level prosodic feature contours in voiced speech segments. Contours of pitch, time domain energy, frequency domain harmonic structure energy and formant for each syllable (segmented using energy information in the voiced speech segment) are considered for analysis in subsystem (4). The proposed four subsystems have been demonstrated to be effective and able to achieve competitive results in classifying different age and gender groups. To further improve the overall classification performance, weighted summation based fusion of these seven subsystems at the score level is demonstrated. Experiment results are reported on the development and test set of the 2010 Interspeech Paralinguistic Challenge aGender database. Compared to the SVM baseline system (3), which is the baseline system suggested by the challenge committee, the proposed fusion system achieves 5.6% absolute improvement in unweighted accuracy for the age task and 4.2% for the gender task on the development set. On the final test set, we obtain 3.1% and 3.8% absolute improvement, respectively.},
journal = {Comput. Speech Lang.},
month = jan,
pages = {151–167},
numpages = {17},
keywords = {UBM weight posterior probability supervectors, Sparse representation, Score level fusion, SVM, Prosodic features, Polynomial expansion, Pitch, Maximum likelihood linear regression, Harmonic structure, Gender recognition, GMM, Formant, Age recognition}
}

@article{10.1016/j.infsof.2021.106679,
author = {Eyal&nbsp;Salman, Hamzeh},
title = {Feature-based insight for forks in social coding platforms},
year = {2021},
issue_date = {Dec 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {140},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106679},
doi = {10.1016/j.infsof.2021.106679},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {14},
keywords = {Reuse, Repositories, Source code, Feature, FCA, Forks insight}
}

@inproceedings{10.1007/978-3-030-66823-5_24,
author = {Campari, Tommaso and Eccher, Paolo and Serafini, Luciano and Ballan, Lamberto},
title = {Exploiting Scene-Specific Features for Object Goal Navigation},
year = {2020},
isbn = {978-3-030-66822-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66823-5_24},
doi = {10.1007/978-3-030-66823-5_24},
abstract = {Can the intrinsic relation between an object and the room in which it is usually located help agents in the Visual Navigation Task? We study this question in the context of Object Navigation, a problem in which an agent has to reach an object of a specific class while moving in a complex domestic environment. In this paper, we introduce a new reduced dataset that speeds up the training of navigation models, a notoriously complex task. Our proposed dataset permits the training of models that do not exploit online-built maps in reasonable times even without the use of huge computational resources. Therefore, this reduced dataset guarantees a significant benchmark and it can be used to identify promising models that could be then tried on bigger and more challenging datasets. Subsequently, we propose the SMTSC model, an attention-based model capable of exploiting the correlation between scenes and objects contained in them, highlighting quantitatively how the idea is correct.},
booktitle = {Computer Vision – ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part IV},
pages = {406–421},
numpages = {16},
keywords = {Reinforcement Learning, ObjectGoal Navigation, Visual Navigation},
location = {Glasgow, United Kingdom}
}

@article{10.1016/j.jss.2013.06.034,
author = {Alf\'{e}rez, G. H. and Pelechano, V. and Mazo, R. and Salinesi, C. and Diaz, D.},
title = {Dynamic adaptation of service compositions with variability models},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.034},
doi = {10.1016/j.jss.2013.06.034},
abstract = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
journal = {J. Syst. Softw.},
month = may,
pages = {24–47},
numpages = {24},
keywords = {Web service composition, Verification, Variability, Models at runtime, Dynamic software product line, Dynamic adaptation, Constraint programming, Autonomic computing}
}

@inproceedings{10.1007/978-3-030-32248-9_54,
author = {Han, Shuo and Carass, Aaron and Prince, Jerry L.},
title = {Hierarchical Parcellation of the Cerebellum},
year = {2019},
isbn = {978-3-030-32247-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32248-9_54},
doi = {10.1007/978-3-030-32248-9_54},
abstract = {Parcellation of the cerebellum in an MR image has been used to study regional associations with both motion and cognitive functions. Despite the fact that the division of the cerebellum is defined hierarchically—i.e., the cerebellum can be divided into lobes and the lobes can be further divided into lobules—previous automatic methods to parcellate the cerebellum do not utilize this information. In this work, we propose a method based on convolutional neural networks&nbsp;(CNNs) to explicitly incorporate the hierarchical organization of the cerebellum. The network is constructed in a tree structure with each node representing a cerebellar region and having child nodes that further subdivide the region into finer substructures. Thus, our CNN is aware of the hierarchical organization of the cerebellum. Furthermore, by selecting tree nodes to represent the hierarchical properties of a given training sample, our network can be trained with heterogeneous training data that are labeled to different hierarchical depths. The proposed method was compared with a state-of-the-art cerebellum parcellation network. Our approach shows promising results as a first parcellation method to take the cerebellar hierarchical organization into consideration.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part III},
pages = {484–491},
numpages = {8},
location = {Shenzhen, China}
}

@inproceedings{10.1007/978-3-031-14135-5_11,
author = {Munir, Sohail and Ali, Hamid and Qureshi, Jahangeer},
title = {Log Attention – Assessing Software Releases with Attention-Based Log Anomaly Detection},
year = {2021},
isbn = {978-3-031-14134-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-14135-5_11},
doi = {10.1007/978-3-031-14135-5_11},
abstract = {A Software Engineering Manager (EM) has to cater to the demand for higher reliability and resilience in Production while simultaneously addressing the evolution of software architecture from monolithic applications to multi-cloud distributed microservices. Pre-release functional testing is no longer sufficient to eliminate faults as more and more issues are generated at runtime, which is challenging to diagnose due to complex inter-service dependencies and dynamic late binding of services. Bugs in Production are known to propagate across software components and become critical as they go undetected.This paper introduces LogAttention, a methodology based on analysis of runtime logs that provides actionable insights to the EM to identify faults and preempt failure in Production. LogAttention is a Log Anomaly Detection (LAD) technique that uses Attention-based Transformer Models to identify Anomalous Log Messages. LogAttention assigns a quality score to the software release in Production and presents remarkable logs to the EM to analyze, predict, and preempt failure. This paper presents empirical evidence showing that LogAttention outperforms existing LAD techniques to identify anomalous log messages and ensure that the detected log anomalies are reliable indicators of the health of a software release.},
booktitle = {Service-Oriented Computing – ICSOC 2021 Workshops:  AIOps, STRAPS, AI-PA and Satellite Events, Dubai, United Arab Emirates, November 22–25, 2021, Proceedings},
pages = {139–150},
numpages = {12},
keywords = {Software release quality, Log attention, Log anomaly detection, Log analysis, Attention models, AIOps}
}

@article{10.1016/j.ic.2020.104638,
author = {Sioutis, Michael and Paparrizou, Anastasia and Janhunen, Tomi},
title = {On neighbourhood singleton-style consistencies for qualitative spatial and temporal reasoning},
year = {2021},
issue_date = {Oct 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {280},
number = {C},
issn = {0890-5401},
url = {https://doi.org/10.1016/j.ic.2020.104638},
doi = {10.1016/j.ic.2020.104638},
journal = {Inf. Comput.},
month = oct,
numpages = {17},
keywords = {Minimal labeling problem, Neighbourhood, Singleton-style consistencies, Spatial and temporal reasoning, Qualitative constraints}
}

@inproceedings{10.5555/3386691.3386706,
author = {Lu, Sidi and Luo, Bing and Patel, Tirthak and Yao, Yongtao and Tiwari, Devesh and Shi, Weisong},
title = {Making disk failure predictions SMARTer!},
year = {2020},
isbn = {9781939133120},
publisher = {USENIX Association},
address = {USA},
abstract = {Disk drives are one of the most commonly replaced hardware components and continue to pose challenges for accurate failure prediction. In this work, we present analysis and findings from one of the largest disk failure prediction studies covering a total of 380,000 hard drives over a period of two months across 64 sites of a large leading data center operator. Our proposed machine learning based models predict disk failures with 0.95 F-measure and 0.95 Matthews correlation coefficient (MCC) for 10-days prediction horizon on average.},
booktitle = {Proceedings of the 18th USENIX Conference on File and Storage Technologies},
pages = {151–168},
numpages = {18},
location = {Santa Clara, CA, USA},
series = {FAST'20}
}

@inproceedings{10.1145/3240508.3240648,
author = {Zheng, Xiaoju and Zha, Zheng-Jun and Zhuang, Liansheng},
title = {A Feature-Adaptive Semi-Supervised Framework for Co-saliency Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240648},
doi = {10.1145/3240508.3240648},
abstract = {Co-saliency detection, which refers to the discovery of common salient foreground regions in a group of relevant images, has attracted increasing attention due to its widespread applications in many vision tasks. Existing methods assemble features from multiple views toward a comprehensive representation, however overlook the efficacy disparity among various features in detecting co-saliency. This paper proposes a novel feature-adaptive semi-supervised (FASS) framework for co-saliency detection, which seamlessly integrates multi-view feature learning, graph structure optimization and co-saliency prediction in a unified solution. In particular, the FASS exploits the efficacy disparity of multi-view features at both view and element levels by a joint formulation of view-wise feature weighting and element-wise feature selection, leading to an effective representation robust to feature noise and redundancy as well as adaptive to the task at hand. It predicts co-saliency map by optimizing co-saliency label prorogation over a graph of both labeled and unlabeled image regions. The graph structure is optimized jointly with feature learning and co-saliency prediction to precisely characterize underlying correlation among regions. The FASS is thus able to produce satisfactory co-saliency map based on the effective exploration of multi-view features as well as inter-region correlation. Extensive experiments on three benchmark datasets, i.e., iCoseg, Cosal2015 and MSRC, have demonstrated that the proposed FASS outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {959–966},
numpages = {8},
keywords = {semi-supervised learning, multi-view feature, graph optimization, co-saliency detection},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1007/978-3-030-26061-3_55,
author = {Yu, Jianguo and Markov, Konstantin and Karpov, Alexey},
title = {Speaking Style Based Apparent Personality Recognition},
year = {2019},
isbn = {978-3-030-26060-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26061-3_55},
doi = {10.1007/978-3-030-26061-3_55},
abstract = {In this study, we investigate the problem of apparent personality recognition using person’s voice, or more precisely, the way he or she speaks. Based on the style transfer idea in deep neural net image processing, we developed a system capable of speaking style extraction from recorded speech utterances, which then uses this information to estimate the so called Big-Five personality traits. The latent speaking style space is represented by the Gram matrix of convoluted acoustic features. We used a database with labels of personality traits perceived by other people (first impression). The experimental results showed that the proposed system achieves state of the art results for the task of audio based apparent personality recognition.},
booktitle = {Speech and Computer: 21st International Conference, SPECOM 2019, Istanbul, Turkey, August 20–25, 2019, Proceedings},
pages = {540–548},
numpages = {9},
keywords = {Computational Paralinguistics, Speaking style representation, First impression prediction, Automatic Apparent Personality Recognition},
location = {Istanbul, Turkey}
}

@article{10.1109/TFUZZ.2019.2899809,
author = {Chen, Rong and Guo, Shi-Kai and Wang, Xi-Zhao and Zhang, Tian-Lun},
title = {Fusion of Multi-RSMOTE With Fuzzy Integral to Classify Bug Reports With an Imbalanced Distribution},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {1063-6706},
url = {https://doi.org/10.1109/TFUZZ.2019.2899809},
doi = {10.1109/TFUZZ.2019.2899809},
abstract = {With the help of automated classification, &lt;italic&gt;severe&lt;/italic&gt; bugs can be rapidly identified so that the latent damage to software projects can be minimized. However, bug report datasets commonly suffer from disproportionate number of category samples. When presented with the situation of class imbalance, most standard classification learning approaches fail to properly learn the distributive characteristics of the samples and tend to result in unfavorable performance to predict class label. In this case, imbalanced learning becomes critical to advance classification algorithms. In this paper, we propose an improved synthetic minority oversampling technique to avoid the degraded performance caused by class imbalance in bug report datasets. Moreover, to lessen the chance of occasionalities in random sampling process, we propose a repeated sampling technique to train different, but related classifiers. Finally, an ensemble algorithm based on Choquet fuzzy integral is employed to combine the wisdom of crowds and make better decisions. We conduct comprehensive experiments on several bug report datasets from real-world bug repositories. The results demonstrate that the proposed method boosts the classification performance across the classes of the data. Specifically, compared with various ensemble learning techniques, the Choquet fuzzy integral achieves outstanding results on integrating multiple random oversampling techniques.},
journal = {Trans. Fuz Sys.},
month = dec,
pages = {2406–2420},
numpages = {15}
}

@article{10.1007/s11390-019-1953-5,
author = {Xi, Sheng-Qu and Yao, Yuan and Xiao, Xu-Sheng and Xu, Feng and Lv, Jian},
title = {Bug Triaging Based on Tossing Sequence Modeling},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1953-5},
doi = {10.1007/s11390-019-1953-5},
abstract = {Bug triaging, which routes the bug reports to potential fixers, is an integral step in software development and maintenance. To make bug triaging more efficient, many researchers propose to adopt machine learning and information retrieval techniques to identify some suitable fixers for a given bug report. However, none of the existing proposals simultaneously take into account the following three aspects that matter for the efficiency of bug triaging: 1) the textual content in the bug reports, 2) the metadata in the bug reports, and 3) the tossing sequence of the bug reports. To simultaneously make use of the above three aspects, we propose iTriage which first adopts a sequence-to-sequence model to jointly learn the features of textual content and tossing sequence, and then uses a classification model to integrate the features from textual content, metadata, and tossing sequence. Evaluation results on three different open-source projects show that the proposed approach has significantly improved the accuracy of bug triaging compared with the state-of-the-art approaches.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {942–956},
numpages = {15},
keywords = {software repository minings, tossing sequence, bug triaging}
}

@article{10.1007/s10472-019-09645-7,
author = {Liu, Xudong and Truszczynski, Miroslaw},
title = {Voting-based ensemble learning for partial lexicographic preference forests over combinatorial domains},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {87},
number = {1–2},
issn = {1012-2443},
url = {https://doi.org/10.1007/s10472-019-09645-7},
doi = {10.1007/s10472-019-09645-7},
abstract = {We study preference representation models based on partial lexicographic preference trees (PLP-trees). We propose to represent preference relations as forests of small PLP-trees (PLP-forests), and to use voting rules to aggregate orders represented by the individual trees into a single order to be taken as a model of the agent’s preference relation. We show that when learned from examples, PLP-forests have better accuracy than single PLP-trees. We also show that the choice of a voting rule does not have a major effect on the aggregated order, thus rendering the problem of selecting the “right” rule less critical. Next, for the proposed PLP-forest preference models, we develop methods to compute optimal and near-optimal outcomes, the tasks that appear difficult for some other common preference models. Lastly, we compare our models with those based on decision trees, which brings up questions for future research.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = oct,
pages = {137–155},
numpages = {19},
keywords = {Maximum satisfiability, Voting theory, Computational complexity theory, Social choice theory, Preference modeling and reasoning, Preference learning, Lexicographic preference models}
}

@inproceedings{10.5555/3540261.3540707,
author = {Chen, Shizhe and Guhur, Pierre-Louis and Schmid, Cordelia and Laptev, Ivan},
title = {History aware multimodal transformer for vision-and-language navigation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with fine-grained instructions (R2R, RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {446},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.1145/2007052.2007088,
author = {Jeet, Kawal and Dhir, Renu},
title = {A model for estimating efforts required to make changes in a software development project},
year = {2011},
isbn = {9781450306355},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2007052.2007088},
doi = {10.1145/2007052.2007088},
abstract = {Research on software quality is as old as software project management. As in other engineering and science disciplines, one approach to understand and control this issue is the use of models. These, quality models have become a well-accepted means to describe and manage software quality. Statistical techniques like Bayesians Networks are used to access and predict software quality by using these quality models. But they are not very accurate. These models lack clarity and operation. In this paper, we propose to develop software model that uses a fuzzy inference approach to access and predict software quality. This model indicates the impact of implementation, quality assurance and analysis on maintenance (an important factor for measuring quality) and the result is studied by the impact on indicator like average efforts required to maintain a project.},
booktitle = {Proceedings of the International Conference on Advances in Computing and Artificial Intelligence},
pages = {175–178},
numpages = {4},
keywords = {sugeno system, software project management, quality model, maintainability, fuzzy inference system, average change effort},
location = {Rajpura/Punjab, India},
series = {ACAI '11}
}

@inproceedings{10.1145/3324884.3416540,
author = {St\"{o}ckle, Patrick and Grobauer, Bernd and Pretschner, Alexander},
title = {Automated implementation of windows-related security-configuration guides},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416540},
doi = {10.1145/3324884.3416540},
abstract = {Hardening is the process of configuring IT systems to ensure the security of the systems' components and data they process or store. The complexity of contemporary IT infrastructures, however, renders manual security hardening and maintenance a daunting task.In many organizations, security-configuration guides expressed in the SCAP (Security Content Automation Protocol) are used as a basis for hardening, but these guides by themselves provide no means for automatically implementing the required configurations.In this paper, we propose an approach to automatically extract the relevant information from publicly available security-configuration guides for Windows operating systems using natural language processing. In a second step, the extracted information is verified using the information of available settings stored in the Windows Administrative Template files, in which the majority of Windows configuration settings is defined.We show that our implementation of this approach can extract and implement 83% of the rules without any manual effort and 96% with minimal manual effort. Furthermore, we conduct a study with 12 state-of-the-art guides consisting of 2014 rules with automatic checks and show that our tooling can implement at least 97% of them correctly. We have thus significantly reduced the effort of securing systems based on existing security-configuration guides.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {598–610},
numpages = {13},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@phdthesis{10.5555/AAI28414631,
author = {Gatling, Teia C. and Blackburn, Timothy},
advisor = {Oluwatomi, Adetunji, and Amirhossein, Etemadi,},
title = {Applying Documentation Metrics in Cross Version Defect Prediction Modeling},
year = {2021},
isbn = {9798708754837},
publisher = {The George Washington University},
abstract = {Software documentation such as documented Application Programming Interface (API) and comments embedded with the software code aid in faster debugging of software defects. The existence of this documentation is used as a measurement of software quality. Over the last 40 years, a series of object-oriented metrics-based defect prediction models have been successfully developed. However, documentation metrics in combination with object-oriented metrics in software defect prediction modeling has not been explored in predicting defects. By leveraging a publicly available GitHub dataset that contains both documentation and object-oriented metrics and applying the cross version defect prediction approach, this research determined documentation metrics impact on defects across a project and developed a predictive model using these metric in combination with baseline metrics to improve model performance. As a result, Boosting ensemble method returned improved model performance when combining the documentation metrics with commonly used object-oriented code metrics. Likewise, the Random Forest returned an improved model when using a feature subset. Random Forest using a subset of metrics provided the most promising results with F-Measure performance improvement of 8.9 percent. The results of this research highlight quantitatively the impact documentation metrics have on software defect prediction and that model performance can improve when identifying a subset of metrics. The results also demonstrate the use of data from three previous versions versus solely using the latest version, the models perform within an average of five percentage points of each other. This knowledge can be leveraged by managers to enhance the application of documentation throughout the lifecycle of software.},
note = {AAI28414631}
}

@inproceedings{10.1109/ICSE43902.2021.00145,
author = {He, Xincheng and Xu, Lei and Zhang, Xiangyu and Hao, Rui and Feng, Yang and Xu, Baowen},
title = {PyART: Python API Recommendation in Real-Time},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00145},
doi = {10.1109/ICSE43902.2021.00145},
abstract = {API recommendation in real-time is challenging for dynamic languages like Python. Many existing API recommendation techniques are highly effective, but they mainly support static languages. A few Python IDEs provide API recommendation functionalities based on type inference and training on a large corpus of Python libraries and third-party libraries. As such, they may fail to recommend or make poor recommendations when type information is missing or target APIs are project-specific. In this paper, we propose a novel approach, PyART, to recommending APIs for Python programs in real-time. It features a light-weight analysis to derive so-called optimistic data-flow, which is neither sound nor complete, but simulates the local data-flow information humans can derive. It extracts three kinds of features: data-flow, token similarity, and token co-occurrence, in the context of the program point where a recommendation is solicited. A predictive model is trained on these features using the Random Forest algorithm. Evaluation on 8 popular Python projects demonstrates that PyART can provide effective API recommendations. When historic commits can be leveraged, which is the target scenario of a state-of-the-art tool ARIREC, our average top-1 accuracy is over 50% and average top-10 accuracy over 70%, outperforming APIREC and Intellicode (i.e., the recommendation component in Visual Studio) by 28.48%-39.05% for top-1 accuracy and 24.41%-30.49% for top-10 accuracy. In other applications such as when historic comments are not available and cross-project recommendation, PyART also shows better overall performance. The time to make a recommendation is less than a second on average, satisfying the real-time requirement.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1634–1645},
numpages = {12},
keywords = {real-time recommendation, data flow analysis, context analysis, Python, API recommendation},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1007/978-3-030-30796-7_14,
author = {Kubitza, Dennis Oliver and B\"{o}ckmann, Matthias and Graux, Damien},
title = {SemanGit: A Linked Dataset from git},
year = {2019},
isbn = {978-3-030-30795-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30796-7_14},
doi = {10.1007/978-3-030-30796-7_14},
abstract = {The growing interest in free and open-source software which occurred over the last decades has accelerated the usage of versioning systems to help developers collaborating together in the same projects. As a consequence, specific tools such as git and specialized open-source on-line platforms gained importance. In this study, we introduce and share SemanGit which provides a resource at the crossroads of both Semantic Web and git web-based version control systems. SemanGit is actually the first collection of linked data extracted from GitHub based on a git ontology we designed and extended to include specific GitHub features. In this article, we present the dataset, describe the extraction process according to the ontology, show some promising analyses of the data and outline how SemanGit could be linked with external datasets or enriched with new sources to allow for more complex analyses.Resource type: DatasetWebsite:Permanent URL:},
booktitle = {The Semantic Web – ISWC 2019: 18th International Semantic Web Conference, Auckland, New Zealand, October 26–30, 2019, Proceedings, Part II},
pages = {215–228},
numpages = {14},
location = {Auckland, New Zealand}
}

@article{10.1504/IJAOSE.2016.080887,
author = {Nunes, Ingrid and Faccin, Jo\~{a}o Guilherme},
title = {Modelling and implementing modularised BDI agents with capability relationships},
year = {2016},
issue_date = {January 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {5},
number = {2/3},
issn = {1746-1375},
url = {https://doi.org/10.1504/IJAOSE.2016.080887},
doi = {10.1504/IJAOSE.2016.080887},
abstract = {The BDI model is the foundation for one of the most widely used architectures to develop autonomous agents. Such model provides the concepts of beliefs, desires and intentions, which comprise the internal agent structure. Although much work has been done to support BDI agent development, there is lack of approaches that focus on modularisation of intra-agent software components. Given that agents often present a complex behaviour and, consequently, complex design and implementation, modularisation is a key to make the development of large-scale enterprise applications feasible. In this paper, we extend the concept of capability, which emerged to model BDI agent modules, by adding different types of relationships between them, namely association, composition and generalisation. Such relationships allow the development of BDI agent building blocks that can be combined so as to form agents, while hiding capability information as needed. Moreover, we present a modelling tool and implementation of the proposed relationships to not only provide the conceptual foundation of our approach but also enable its practical use. We show the effectiveness of our approach by refactoring an existing software product line implemented with BDI agents using our capability relationships.},
journal = {Int. J. Agent-Oriented Softw. Eng.},
month = jan,
pages = {203–231},
numpages = {29},
keywords = {multi-agent systems, modularised BDI agents, modularisation, modelling, capability relationships, belief desire intention, autonomous agents, agent-oriented programming, agent-based systems, BDI4JADE, BDI architecture}
}

@inproceedings{10.1145/3019612.3019712,
author = {Terenziani, Paolo and Andolina, Antonella},
title = {Probabilistic quantitative temporal reasoning},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019712},
doi = {10.1145/3019612.3019712},
abstract = {Temporal reasoning, in the form of propagation of temporal constraints, is an important topic in Artificial Intelligence. The current literature in the area is moving from the treatment of "crisp" temporal constraints to fuzzy or probabilistic constraints, to account for different forms of uncertainty andor preferences. However, despite the huge amount of work in the area, the spectrum of possible solutions has not been fully explored. In particular, no probabilistic approach coping with quantitative temporal constraints has been proposed yet. We overcome such a limitation of the current literature by proposing the first approach providing (i) a probabilistic extension to quantitative constraints, supporting the possibility of expressing alternative distances between time points, and of associating a probability to each alternative, and (ii) a framework for the propagation of such temporal constraints.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {965–970},
numpages = {6},
keywords = {temporal reasoning, quantitative temporal constraints, probabilities},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/1809085.1809089,
author = {Loques, Orlando and Sztajnberg, Alexandre},
title = {Adaptation issues in software architectures of remote health care systems},
year = {2010},
isbn = {9781605589732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1809085.1809089},
doi = {10.1145/1809085.1809089},
abstract = {In this text we identify relevant adaptation issues concerning the software architecture of a system for intelligent health monitoring of a person at home. Our solution integrates medical knowledge, patient's physiological and behavioral data, and environmental conditions. The designed software architecture includes modules for context management, alarm generation, reasoning and learning. A fuzzy logic model, and rules based on medical recommendations, helps analyzing and identifying critical situations of the patient. In this scenario, ubiquitous computing has an important role allowing the non-intrusive monitoring of several relevant context variables. Considering this infrastructure, we argue that configuration management and context management are key mechanisms to support the adaptation requirements of the target class of applications. Finally, we describe our approach towards tackling the related architecture adaptation issues.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Software Engineering in Health Care},
pages = {24–28},
numpages = {5},
keywords = {software architecture adaptation and configuration, software architecture, remote health care, remote assisted living, health care, experimentation, context management, configuration management},
location = {Cape Town, South Africa},
series = {SEHC '10}
}

@article{10.1016/j.neucom.2015.04.087,
author = {Liu, Weifeng and Li, Yang and Tao, Dapeng and Wang, Yanjiang},
title = {A general framework for co-training and its applications},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.04.087},
doi = {10.1016/j.neucom.2015.04.087},
abstract = {Co-training is one of the major semi-supervised learning paradigms in which two classifiers are alternately trained on two distinct views and they teach each other by adding the predictions of unlabeled data to the training set of the other view. Co-training can achieve promising performance, especially when there is only a small number of labeled data. Hence, co-training has received considerable attention, and many variant co-training algorithms have been developed. It is essential and informative to provide a systematic framework for a better understanding of the common properties and differences in these algorithms. In this paper, we propose a general framework for co-training according to the diverse learners constructed in co-training. Specifically, we provide three types of co-training implementations, including co-training on multiple views, co-training on multiple classifiers, and co-training on multiple manifolds. Finally, comprehensive experiments of different methods are conducted on the UCF-iPhone dataset for human action recognition and the USAA dataset for social activity recognition. The experimental results demonstrate the effectiveness of the proposed solutions.},
journal = {Neurocomput.},
month = nov,
pages = {112–121},
numpages = {10},
keywords = {Social activity recognition, Semi-supervised learning, Multi-view, Human action recognition, Co-training}
}

@article{10.1504/ijbidm.2021.114469,
author = {Pinto, Ricardo A.M. and Bernardini, Flavia and Marcacini, Ricardo M.},
title = {PPM-HC: a method for helping project portfolio management based on topic hierarchy learnings},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {3},
issn = {1743-8195},
url = {https://doi.org/10.1504/ijbidm.2021.114469},
doi = {10.1504/ijbidm.2021.114469},
abstract = {The projects categorisation is a crucial step in the project portfolio management (PPM). Categorising projects allows the organisation to identify categories with a lack or excess of projects, according to its strategic objectives. In this work, we present a new method for project portfolio management based on hierarchical clustering (PPM-HC) to organise the projects at several levels of abstraction. In the PPM-HC, similar projects are allocated to the same clusters and subclusters. PPM-HC automatically learns an understandable topic hierarchy from the project portfolio dataset, thereby facilitating the (human) task of exploring, analysing and prioritising the projects of the organisation. We also proposed a card sorting-based technique which allows the evaluation of the projects categorisation using an intuitive visual map. We carried out an experimental evaluation based on a benchmark dataset and we also presented a real-world case study. The results show that the proposed PPM-HC method is promising.},
journal = {Int. J. Bus. Intell. Data Min.},
month = jan,
pages = {364–382},
numpages = {18},
keywords = {hierarchical clustering, topic hierarchy learning, projects categorisation, PPM, project portfolio management}
}

@inproceedings{10.1145/2007052.2007067,
author = {Raj, Gaurav},
title = {An efficient broker cloud management system},
year = {2011},
isbn = {9781450306355},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2007052.2007067},
doi = {10.1145/2007052.2007067},
abstract = {In Cloud Computing Architecture, Brokers are responsible for providing applications to end user from Clouds. To manage Broker Cloud Communication, an efficient management infrastructure is required which also take care for cost and time to get application services. In this paper, an Efficient Broker Cloud Management (BCM), a system which works in Broker Cloud Communication Paradigm (BCCP) has been proposed to find a communication link with minimum cost of link uses between broker and cloud. An algorithm, Optimum Route Cost Finder (ORCF) has been proposed for finding optimum route between broker and cloud on the behalf of cost factor. Different VM Scheduling and VMM Allocation policies are analyzed in here which help in executing Cloudlets over VMs to select best policy for BCM System. Furthermore, this paper provided the analysis of processing cost and total execution cost on the bases of Hops Count, Bandwidth, Network Delay or Combined Approach. By using Bandwidth-Delay Product (BDP), It also analyze the link capacity. ORCF help in analyzing the processing cost and link uses cost of executing whole task and identify the total execution cost for broker.},
booktitle = {Proceedings of the International Conference on Advances in Computing and Artificial Intelligence},
pages = {72–76},
numpages = {5},
location = {Rajpura/Punjab, India},
series = {ACAI '11}
}

@article{10.1007/s11042-020-09907-1,
author = {Zhong, Yuanhong and Zhang, Jing and Zhou, Zhaokun and Cheng, Xinyu and Huang, Guan and Li, Qiang},
title = {Recovery of image and video based on compressive sensing via tensor approximation and Spatio-temporal correlation},
year = {2021},
issue_date = {Feb 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {5},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09907-1},
doi = {10.1007/s11042-020-09907-1},
abstract = {In recent years, block-based compressive sensing (BCS) has been extensively studied because it can reduce computational complexity and data storage by dividing the image into smaller patches, but the performance of the reconstruction algorithm is not satisfactory. In this paper, a new reconstruction model for image and video is proposed. The model makes full use of spatio-temporal correlation and utilizes low-rank tensor approximation to improve the quality of the reconstructed image and video. For image recovery, the proposed model obtains a low-rank approximation of a tensor formed by non-local similar patches, and improves the reconstruction quality from a spatial perspective by combining non-local similarity and low-rank property. For video recovery, the reconstruction process is divided into two phases. In the first phase, each frame of the video sequence is regarded as an independent image to be reconstructed by taking advantage of spatial property. The second phase performs tensor approximation through searching similar patches within frames near the target frame, to achieve reconstruction by putting the spatio-temporal correlation into full play. The resulting model is solved by an efficient Alternating Direction Method of Multipliers (ADMM) algorithm. A series of experiments show that the quality of the proposed model is comparable to the current state-of-the-art recovery methods.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {7433–7450},
numpages = {18},
keywords = {Spatio-temporal correlation, High order singular value decomposition (HOSVD), Low-rank tensor approximation, Image and video recovery, Block-based compressive sensing}
}

@inproceedings{10.5555/1620113.1620134,
author = {Van Hoeve, Willem-Jan and Gomes, Carla P. and Selman, Bart and Lombardi, Michele},
title = {Optimal multi-agent scheduling with constraint programming},
year = {2007},
isbn = {9781577353232},
publisher = {AAAI Press},
abstract = {We consider the problem of computing optimal schedules in multi-agent systems. In these problems, actions of one agent can influence the actions of other agents, while the objective is to maximize the total 'quality' of the schedule. More specifically, we focus on multi-agent scheduling problems with time windows, hard and soft precedence relations, and a nonlinear objective function. We show how we can model and efficiently solve these problems with constraint programming technology. Elements of our proposed method include constraint-based reasoning, search strategies, problem decomposition, scheduling algorithms, and a linear programming relaxation. We present experimental results on realistic problem instances to display the different elements of the solution process.},
booktitle = {Proceedings of the 19th National Conference on Innovative Applications of Artificial Intelligence - Volume 2},
pages = {1813–1818},
numpages = {6},
location = {Vancouver, British Columbia, Canada},
series = {IAAI'07}
}

@inproceedings{10.1007/978-3-642-27872-3_21,
author = {Kumar, K. M. Anil and Suresha, Suresha},
title = {Detection of web users' opinion from normal and short opinionated words},
year = {2010},
isbn = {9783642278716},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-27872-3_21},
doi = {10.1007/978-3-642-27872-3_21},
abstract = {In this paper we present an approach to identify opinion of web users from an opinionated text and to classify web user's opinion into positive or negative. Web users document their opinion in opinionated sites, shopping sites, personal pages etc., to express and share their opinion with other web users. The opinion expressed by web users may be on diverse topics such as politics, sports, products, movies etc. These opinions will be very useful to others such as, leaders of political parties, selection committees of various sports, business analysts and other stake holders of products, directors and producers of movies as well as to the other concerned web users. Today web users express their opinion using normal words and short words. These short words, such as gud for good, grt8 for great etc., are very popular and are used by a large number of web users to document their opinion. We use semantic based approach to find users opinion from both normal and short words. Our approach first detects subjective phrases and uses these phrases along with intensifiers and diminishers to obtain semantic orientation scores. The semantic orientation score of these phrases is used to identify user's opinion from an opinionated text. Our approach provides better results than the other approaches on different data sets.},
booktitle = {Proceedings of the Second International Conference on Data Engineering and Management},
pages = {139–145},
numpages = {7},
keywords = {sentiment analysis, opinion mining, artificial intelligence},
location = {Tiruchirappalli, India},
series = {ICDEM'10}
}

@article{10.1007/s10664-021-10010-8,
author = {Zhou, Cheng and Li, Bin and Sun, Xiaobing and Bo, Lili},
title = {Why and what happened? Aiding bug comprehension with automated category and causal link identification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10010-8},
doi = {10.1007/s10664-021-10010-8},
abstract = {When a new bug report is assigned to developers, they first need to understand what the bug report expresses (what) and why this bug occurs (why). To do so, developers usually explore different bug related data sources to investigate whether there are historical bugs with similar symptoms and causes related to the bug at hand. Automatic bug classification with respect to what and why information of bugs would enable developers to narrow down their search of bug resources and improve the bug fixing productivity. To achieve this goal, we propose an approach, BugClass, which applies a deep neural network classification approach based on Hierarchical Attention Networks (HAN) to automatically classify the bugs into different what and why categories by exploiting the bug repository and commit repository. Then, we explore the causal link relationship between what and why categories to further improve the accuracy of the bug classification. Experimental results demonstrate that BugClass is effective to classify the given bug reports into what and why categories, and can be also effectively used for identifying the why category for new bugs based on the causal link relations.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {36},
keywords = {Hierarchical attention network, Causal link, Bug classification, Bug comprehension}
}

@inproceedings{10.5555/3540261.3541282,
author = {Zhang, Jiwen and Wei, Zhongyu and Fan, Jianqing and Peng, Jiajie},
title = {Curriculum learning for vision-and-language navigation},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Vision-and-Language Navigation (VLN) is a task where an agent navigates in an embodied indoor environment under human instructions. Previous works ignore the distribution of sample difficulty and we argue that this potentially degrade their agent performance. To tackle this issue, we propose a novel curriculum-based training paradigm for VLN tasks that can balance human prior knowledge and agent learning progress about training samples. We develop the principle of curriculum design and re-arrange the benchmark Room-to-Room (R2R) dataset to make it suitable for curriculum training. Experiments show that our method is model-agnostic and can significantly improve the performance, the generalizability, and the training efficiency of current state-of-the-art navigation agents without increasing model complexity.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1021},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.1007/978-3-642-14052-5_9,
author = {Autexier, Serge and Dietrich, Dominik},
title = {A tactic language for declarative proofs},
year = {2010},
isbn = {3642140513},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-14052-5_9},
doi = {10.1007/978-3-642-14052-5_9},
abstract = {Influenced by the success of the Mizar system many declarative proof languages have been developed in the theorem prover community, as declarative proofs are more readable, easier to modify and to maintain than their procedural counterparts. However, despite their advantages, many users still prefer the procedural style of proof, because procedural proofs are faster to write. In this paper we show how to define a declarative tactic language on top of a declarative proof language. The language comes along with a rich facility to declaratively specify conditions on proof states in the form of sequent patterns, as well as ellipses (dot notation) to provide a limited form of iteration. As declarative tactics are specified using the declarative proof language, they offer the same advantages as declarative proof languages. At the same time, they also produce declarative justifications in the form of a declarative proof script and can thus be seen as an attempt to reduce the gap between procedural and declarative proofs.},
booktitle = {Proceedings of the First International Conference on Interactive Theorem Proving},
pages = {99–114},
numpages = {16},
location = {Edinburgh, UK},
series = {ITP'10}
}

@article{10.1007/s10844-017-0479-y,
author = {Cai, Yuanyuan and Zhang, Qingchuan and Lu, Wei and Che, Xiaoping},
title = {A hybrid approach for measuring semantic similarity based on IC-weighted path distance in WordNet},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {1},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-017-0479-y},
doi = {10.1007/s10844-017-0479-y},
abstract = {As a valuable tool for text understanding, semantic similarity measurement enables discriminative semantic-based applications in the fields of natural language processing, information retrieval, computational linguistics and artificial intelligence. Most of the existing studies have used structured taxonomies such as WordNet to explore the lexical semantic relationship, however, the improvement of computation accuracy is still a challenge for them. To address this problem, in this paper, we propose a hybrid WordNet-based approach CSSM-ICSP to measuring concept semantic similarity, which leverage the information content(IC) of concepts to weight the shortest path distance between concepts. To improve the performance of IC computation, we also develop a novel model of the intrinsic IC of concepts, where a variety of semantic properties involved in the structure of WordNet are taken into consideration. In addition, we summarize and classify the technical characteristics of previous WordNet-based approaches, as well as evaluate our approach against these approaches on various benchmarks. The experimental results of the proposed approaches are more correlated with human judgment of similarity in term of the correlation coefficient, which indicates that our IC model and similarity detection approach are comparable or even better for semantic similarity measurement as compared to others.},
journal = {J. Intell. Inf. Syst.},
month = aug,
pages = {23–47},
numpages = {25},
keywords = {WordNet, Intrinsic information content, Edge distance, Concept semantic similarity}
}

@article{10.1007/s00180-012-0381-6,
author = {Schmidt, Miriam and Palm, G\"{u}nther and Schwenker, Friedhelm},
title = {Spectral graph features for the classification of graphs and graph sequences},
year = {2014},
issue_date = {February  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1–2},
issn = {0943-4062},
url = {https://doi.org/10.1007/s00180-012-0381-6},
doi = {10.1007/s00180-012-0381-6},
abstract = {In this paper, the classification power of the eigenvalues of six graph-associated matrices is investigated. Each matrix contains a certain type of geometric/ spatial information, which may be important for the classification process. The performances of the different feature types is evaluated on two data sets: first a benchmark data set for optical character recognition, where the extracted eigenvalues were utilized as feature vectors for multi-class classification using support vector machines. Classification results are presented for all six feature types, as well as for classifier combinations at decision level. For the decision level combination, probabilistic output support vector machines have been applied, with a performance up to 92.4 %. To investigate the power of the spectra for time dependent tasks, too, a second data set was investigated, consisting of human activities in video streams. To model the time dependency, hidden Markov models were utilized and the classification rate reached 98.3 %.},
journal = {Comput. Stat.},
month = feb,
pages = {65–80},
numpages = {16},
keywords = {Spectrum, Optical character recognition, Human activity recognition, Graph-associated matrices, Graph classification}
}

@article{10.1007/s00521-020-05110-3,
author = {Bashab, Abeer and Ibrahim, Ashraf Osman and AbedElgabar, Eltayeb E. and Ismail, Mohd Arfian and Elsafi, Abubakar and Ahmed, Ali and Abraham, Ajith},
title = {A systematic mapping study on solving university timetabling problems using meta-heuristic algorithms},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {23},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05110-3},
doi = {10.1007/s00521-020-05110-3},
abstract = {Since university timetabling is commonly classified as a combinatorial optimisation problem, researchers tend to use optimisation approaches to reach the optimal timetable solution. Meta-heuristic algorithms have been presented as effective solutions as proven on their leverage over the last decade. Extensive literature studies have been published until today. However, a comprehensive systematic overview is missing. Therefore, this mapping study aimed to provide an organised view of the current state of the field and comprehensive awareness of the meta-heuristic approaches, by conducting meta-heuristic for solving university timetabling problems. In addition, the mapping study tried to highlight the intensity of publications over the last years, spotting the current trends and directions in the field of solving university timetabling problems, as well as having the work to provide guidance for future research by indicating the gaps and open questions to be fulfilled. Primary studies on mapping study that have been published in the last decade from 2009 until the first quarter of 2020, which consist of 131 publications, were selected as a benchmark for future research to solve university timetabling problems using meta-heuristic algorithms. The majority of the articles based on the publication type are hybrid methods (32%), in which the distribution of meta-heuristic algorithms the hybrid algorithms represent the higher application (31%). Likewise, the majority of the research is solution proposals (66%). The result of this study confirmed the efficiency and intensive application of the meta-heuristic algorithms in solving university timetabling problems, specifically the hybrid algorithms. A new trend of meta-heuristic algorithms such as grey wolf optimiser, cat swarm optimisation algorithm, Elitist self-adaptive step-size search and others with high expectations for reliable and satisfying results can be proposed to fill this gap.},
journal = {Neural Comput. Appl.},
month = dec,
pages = {17397–17432},
numpages = {36},
keywords = {Optimisation, Meta-heuristic algorithm, University timetabling, Systematic mapping review}
}

@inproceedings{10.1109/CEC48606.2020.9185776,
author = {Mendon\c{c}a, Willian D.F. and Assun\c{c}\~{a}o, Wesley K.G. and Estanislau, Lucas V. and Vergilio, Silvia R. and Garcia, Alessandro},
title = {Towards a Microservices-Based Product Line with Multi-Objective Evolutionary Algorithms},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CEC48606.2020.9185776},
doi = {10.1109/CEC48606.2020.9185776},
abstract = {Microservices are small and independently deployable services. They can be developed on different platforms and communicate via lightweight protocols, what makes them highly interoperable. The interoperability between microservices, as well as their reuse and customization needs make this kind of systems adequate to constitute a Software Product Line. However, there is no automatic approach to support the designing of Microservices-Based Product Lines (MBPLs). To move towards the development of MBPLs, this work presents an approach, named MOEA4MBPL, to extract Feature Models (FMs) from a set of microservices-based systems. These FMs intent to leverage interoperability, enabling the practitioners to reason about reuse and/or customization of functionalities. The proposed approach is based on multi-objective evolutionary algorithms, optimizing three objectives, namely precision and recall of products denoted by an FM, and conformance with existing dependencies between microservices. MOEA4MBPL was evaluated with six microservices-based systems, using the algorithms NSGA-II and SPEA2. Our approach was capable of finding FMs with good trade-off values of precision and recall, satisfying all dependencies among the microservices. SPEA2 found better fronts of solutions than NSGA-II, but the latter always executed faster and could find single solutions closer to an ideal solution than the former.},
booktitle = {2020 IEEE Congress on Evolutionary Computation (CEC)},
pages = {1–8},
numpages = {8},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.5555/3016387.3016586,
author = {Stone, Peter},
title = {What's hot at RoboCup (extended abstract)},
year = {2016},
publisher = {AAAI Press},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4346–4347},
numpages = {2},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@article{10.1016/j.cogsys.2019.09.021,
author = {Vityaev, Evgenii},
title = {Consciousness as a logically consistent and prognostic model of reality},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {C},
issn = {1389-0417},
url = {https://doi.org/10.1016/j.cogsys.2019.09.021},
doi = {10.1016/j.cogsys.2019.09.021},
journal = {Cogn. Syst. Res.},
month = jan,
pages = {231–246},
numpages = {16},
keywords = {Concepts, Integrated information, Natural concepts, Natural classification, Categorization, Clustering}
}

@article{10.1007/s11280-019-00766-x,
author = {Hu, Rongyao and Zhu, Xiaofeng and Zhu, Yonghua and Gan, Jiangzhang},
title = {Robust SVM with adaptive graph learning},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-019-00766-x},
doi = {10.1007/s11280-019-00766-x},
abstract = {Support Vector Machine (SVM) has been widely applied in real application due to its efficient performance in the classification task so that a large number of SVM methods have been proposed. In this paper, we present a novel SVM method by taking the dynamic graph learning and the self-paced learning into account. To do this, we propose utilizing self-paced learning to assign important samples with large weights, learning a transformation matrix for conducting feature selection to remove redundant features, and learning a graph matrix from the low-dimensional data of original data to preserve the data structure. As a consequence, both the important samples and the useful features are used to select support vectors in the SVM framework. Experimental analysis on four synthetic and sixteen benchmark data sets demonstrated that our method outperformed state-of-the-art methods in terms of both binary classification and multi-class classification tasks.},
journal = {World Wide Web},
month = may,
pages = {1945–1968},
numpages = {24},
keywords = {SVM, Graph learning, Feature selection, Self-paced learning}
}

@inbook{10.1145/3191315.3191317,
author = {Maier, David and Tekle, K. Tuncay and Kifer, Michael and Warren, David S.},
title = {Datalog: concepts, history, and outlook},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191317},
abstract = {This chapter is a survey of the history and the main concepts of Datalog.We begin with an introduction to the language and its use for database definition and querying. We then look back at the threads from logic languages, databases, artificial intelligence, and expert systems that led to the emergence of Datalog and reminiscence about the origin of the name. We consider the interaction of recursion with other common data language features, such as negation and aggregation, and look at other extensions, such as constraints, updates, and object-oriented features.We provide an overview of the main approaches to Datalog evaluation and their variants, then recount some early implementations of Datalog and of similar deductive database systems.We speculate on the reasons for the decline in the interest in the language in the 1990s and the causes for its later resurgence in a number of application areas.We conclude with several examples of current systems based on or supporting Datalog and briefly examine the performance of some of them.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {3–100},
numpages = {98}
}

@article{10.1145/765568.765569,
author = {Zhang, Yuanlin and Yap, Roland H. C.},
title = {P. van Beek and R. Dechter's theorem on constraint looseness and local consistency},
year = {2003},
issue_date = {May 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0004-5411},
url = {https://doi.org/10.1145/765568.765569},
doi = {10.1145/765568.765569},
journal = {J. ACM},
month = may,
pages = {277–279},
numpages = {3},
keywords = {relations, local consistency, constraint satisfaction problems, constraint networks, Constraint-based reasoning}
}

@inproceedings{10.5555/1558109.1558283,
author = {Nunes, Ingrid and Kulesza, Uir\'{a} and Nunes, Camila and Lucena, Carlos J. P.},
title = {A domain engineering process for developing multi-agent systems product lines},
year = {2009},
isbn = {9780981738178},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent Systems Product Lines (MAS-PLs) have emerged to integrate two promising trends of software engineering: agent-oriented software engineering and software product lines. In this paper, we propose a domain engineering process to develop MAS-PLs, built on top of agent-oriented and software product line approaches.},
booktitle = {Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems - Volume 2},
pages = {1339–1340},
numpages = {2},
keywords = {software product lines, process, multi-agent systems, domain engineering, agent-oriented software engineering},
location = {Budapest, Hungary},
series = {AAMAS '09}
}

@inproceedings{10.5555/3020419.3020482,
author = {Weng, Paul},
title = {Axiomatic foundations for a class of generalized expected utility: algebraic expected utility},
year = {2006},
isbn = {0974903922},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {In this paper, we provide two axiomatizations of algebraic expected utility, which is a particular generalized expected utility, in a von Neumann-Morgenstern setting, i.e. uncertainty representation is supposed to be given and here to be described by a plausibility measure valued on a semiring, which could be partially ordered. We show that axioms identical to those for expected utility entail that preferences are represented by an algebraic expected utility. This algebraic approach allows many previous propositions (expected utility, binary possibilistic utility,...) to be unified in a same general framework and proves that the obtained utility enjoys the same nice features as expected utility: linearity, dynamic consistency, autoduality of the underlying uncertainty representation, autoduality of the decision criterion and possibility of modeling decision maker's attitude toward uncertainty.},
booktitle = {Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence},
pages = {520–527},
numpages = {8},
location = {Cambridge, MA, USA},
series = {UAI'06}
}

@article{10.5555/2591248.2591263,
author = {Androutsopoulos, Ion and Lampouras, Gerasimos and Galanis, Dimitrios},
title = {Generating natural language descriptions from OWL ontologies: the natural OWL system},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {We present Naturalowl, a natural language generation system that produces texts describing individuals or classes of owl ontologies. Unlike simpler owl verbalizers, which typically express a single axiom at a time in controlled, often not entirely fluent natural language primarily for the benefit of domain experts, we aim to generate fluent and coherent multi-sentence texts for end-users. With a system like Naturalowl, one can publish information in owl on the Web, along with automatically produced corresponding texts in multiple languages, making the information accessible not only to computer programs and domain experts, but also end-users. We discuss the processing stages of Naturalowl, the optional domain-dependent linguistic resources that the system can use at each stage, and why they are useful. We also present trials showing that when the domain-dependent linguistic resources are available, Naturalowl produces significantly better texts compared to a simpler verbalizer, and that the resources can be created with relatively light effort.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {671–715},
numpages = {45}
}

@article{10.1016/j.jss.2011.06.026,
author = {Guo, Jianmei and White, Jules and Wang, Guangxin and Li, Jian and Wang, Yinglin},
title = {A genetic algorithm for optimized feature selection with resource constraints in software product lines},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.06.026},
doi = {10.1016/j.jss.2011.06.026},
abstract = {Abstract: Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86-97% of the optimality of other automated feature selection algorithms and in 45-99% less time than existing exact and heuristic feature selection techniques.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2208–2221},
numpages = {14},
keywords = {Software product lines, Product derivation, Optimization, Genetic algorithm, Feature models, Configuration}
}

@inproceedings{10.5555/3367243.3367420,
author = {Kim, Yejin and Kim, Kwangseob and Park, Chanyoung and Yu, Hwanjo},
title = {Sequential and diverse recommendation with long tail},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Sequential recommendation is a task that learns a temporal dynamic of a user behaviour in sequential data and predicts items that a user would like afterward. However, diversity has been rarely emphasized in the context of sequential recommendation. Sequential and diverse recommendation must learn temporal preference on diverse items as well as on general items. Thus, we propose a sequential and diverse recommendation model that predicts a ranked list containing general items and also diverse items without compromising significant accuracy. To learn temporal preference on diverse items as well as on general items, we cluster and relocate consumed long tail items to make a pseudo ground truth for diverse items and learn the preference on long tail using recurrent neural network, which enables us to directly learn a ranking function. Extensive online and offline experiments deployed on a commercial platform demonstrate that our models significantly increase diversity while preserving accuracy compared to the state-of-the-art sequential recommendation model, and consequently our models improve user satisfaction.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {2740–2746},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

@inproceedings{10.1145/3338286.3344387,
author = {Lee, Hao-Ping and Dingler, Tilman and Lin, Chih-Heng and Chen, Kuan-Yin and Chung, Yu-Lin and Chen, Chia-Yu and Chang, Yung-Ju},
title = {Predicting Smartphone Users' General Responsiveness to IM Contacts Based on IM Behavior},
year = {2019},
isbn = {9781450368254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338286.3344387},
doi = {10.1145/3338286.3344387},
abstract = {History of conversations through instant messaging (IM) contains abundant information about the communication patterns of the dyad, including conversation partners' mutual responsiveness to messages. We have, however, not seen many examinations of using such information in modeling mobile users' responsiveness in IM communication. In this paper, we present an in-the-wild study, in which we leverage participants' IM messaging logs to build models predicting their general responsiveness. Our models based on data from 33 IM user achieved an accuracy of up to 71% (AUROC). In particular, we show that 90-day IM-communication patterns, in general, outperformed their 14-day equivalent in our prediction models, indicating better coherence between long-term IM patterns with their general communication experience.},
booktitle = {Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {40},
numpages = {6},
keywords = {mobile receptivity, machine learning, Mobile notifications, ESM},
location = {Taipei, Taiwan},
series = {MobileHCI '19}
}

@inproceedings{10.1145/3178876.3186000,
author = {Ernst, Patrick and Siu, Amy and Weikum, Gerhard},
title = {HighLife: Higher-arity Fact Harvesting},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186000},
doi = {10.1145/3178876.3186000},
abstract = {Text-based knowledge extraction methods for populating knowledge bases have focused on binary facts: relationships between two entities. However, in advanced domains such as health, it is often crucial to consider ternary and higher-arity relations. An example is to capture which drug is used for which disease at which dosage (e.g. 2.5 mg/day) for which kinds of patients (e.g., children vs. adults). In this work, we present an approach to harvest higher-arity facts from textual sources. Our method is distantly supervised by seed facts, and uses the fact-pattern duality principle to gather fact candidates with high recall. For high precision, we devise a constraint-based reasoning method to eliminate false candidates. A major novelty is in coping with the difficulty that higher-arity facts are often expressed only partially in texts and strewn across multiple sources. For example, one sentence may refer to a drug, a disease and a group of patients, whereas another sentence talks about the drug, its dosage and the target group without mentioning the disease. Our methods cope well with such partially observed facts, at both pattern-learning and constraint-reasoning stages. Experiments with health-related documents and with news articles demonstrate the viability of our method.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {1013–1022},
numpages = {10},
keywords = {tree pattern learning, text-based knowledge harvesting, partial facts, knowledge graphs, knowledge base construction, higher-arity relation extraction, health, distant supervision},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.1016/j.engappai.2009.03.001,
author = {Borangiu, Theodor and Gilbert, Pascal and Ivanescu, Nick-Andrei and Rosu, Andrei},
title = {An implementing framework for holonic manufacturing control with multiple robot-vision stations},
year = {2009},
issue_date = {June, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {22},
number = {4–5},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.03.001},
doi = {10.1016/j.engappai.2009.03.001},
abstract = {The paper describes a holonic control architecture and implementing issues for agile job shop assembly with networked intelligent robots, based on the dynamic simulation of material processing and transportation. The holarchy was defined considering the PROSA reference architecture relative to which in-line vision-based quality control was added by help of feature-based descriptions of the material flow. Two solutions for production planning are proposed: a knowledge-based algorithm using production rules, and an OO resolved scheduling rate planner (RSRP) based on variable-timing simulation. Failure- and recovery-management are developed as generic scenarios embedding the CNP mechanism into production self-rescheduling. Aggregate Order Holon execution is realized by OPC-based PLC software integration and event-driven product transportation. The holonic control of multiple networked robot-vision stations also features tolerance to station computer- (IBM PC-type), station controller- (robot controller), quality control- (machine vision) and communication- (LAN) failure. Fault tolerance and high availability at shop-floor level are provided due to the multiple physical communication capabilities of the robot controllers, to their multiple-axis multitasking operating capability, and to hardware redundancy of single points of failure (SPOF). Implementing solutions and experiments are reported for a 6-station robot-vision assembly cell with twin-track closed-loop pallet transportation system and product-racking RD/WR devices. Future developments will consider manufacturing integration at enterprise level.},
journal = {Eng. Appl. Artif. Intell.},
month = jun,
pages = {505–521},
numpages = {17},
keywords = {Semi-heterarchical control, Robotics, Real-time vision, Holonic manufacturing, Applied AI}
}

@article{10.5555/2910557.2910565,
author = {Mossakowski, Till and Moratz, Reinhard},
title = {Relations between spatial calculi about directions and orientations},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {Qualitative spatial descriptions characterize essential properties of spatial objects or configurations by relying on relative comparisons rather than measuring. Typically, in qualitative approaches only relatively coarse distinctions between configurations are made. Qualitative spatial knowledge can be used to represent incomplete and underdetermined knowledge in a systematic way. This is especially useful if the task is to describe features of classes of configurations rather than individual configurations.Although reasoning with them is generally NP-hard (even ∃IR-complete), relative directions are important because they play a key role in human spatial descriptions and there are several approaches how to represent them using qualitative methods. In these approaches directions between spatial locations can be expressed as constraints over infinite domains, e.g. the Euclidean plane. The theory of relation algebras has been successfully applied to this field. Viewing relation algebras as universal algebras and applying and modifying standard tools from universal algebra in this work, we (re)define notions of qualitative constraint calculus, of homomorphism between calculi, and of quotient of calculi. Based on this method we derive important properties for spatial calculi from corresponding properties of related calculi. From a conceptual point of view these formal mappings between calculi are a means to translate between different granularities.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {277–308},
numpages = {32}
}

