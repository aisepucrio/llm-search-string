@inproceedings{10.1145/3336294.3336304,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Software Product Line Engineering: A Practical Experience},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336304},
doi = {10.1145/3336294.3336304},
abstract = {The lack of mature tool support is one of the main reasons that make the industry to be reluctant to adopt Software Product Line (SPL) approaches. A number of systematic literature reviews exist that identify the main characteristics offered by existing tools and the SPL phases in which they can be applied. However, these reviews do not really help to understand if those tools are offering what is really needed to apply SPLs to complex projects. These studies are mainly based on information extracted from the tool documentation or published papers. In this paper, we follow a different approach, in which we firstly identify those characteristics that are currently essential for the development of an SPL, and secondly analyze whether the tools provide or not support for those characteristics. We focus on those tools that satisfy certain selection criteria (e.g., they can be downloaded and are ready to be used). The paper presents a state of practice with the availability and usability of the existing tools for SPL, and defines different roadmaps that allow carrying out a complete SPL process with the existing tool support.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {164–176},
numpages = {13},
keywords = {spl in practice, state of practice, tool support, tooling roadmap},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.infsof.2020.106389,
author = {Chac\'{o}n-Luna, Ana Eva and Guti\'{e}rrez, Antonio Manuel and Galindo, Jos\'{e} A. and Benavides, David},
title = {Empirical software product line engineering: A systematic literature review},
year = {2020},
issue_date = {Dec 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {128},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106389},
doi = {10.1016/j.infsof.2020.106389},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {22},
keywords = {Software product lines, Empirical strategies, Case study, Experiment, Systematic literature review}
}

@article{10.1007/s11227-021-03627-5,
author = {Kiani, Azaz Ahmed and Hafeez, Yaser and Imran, Muhammad and Ali, Sadia},
title = {A dynamic variability management approach working with agile product line engineering practices for reusing features},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {8},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-021-03627-5},
doi = {10.1007/s11227-021-03627-5},
abstract = {Agile software development (ASD) and software product line (SPL) have shown significant benefits for software engineering processes and practices. Although both methodologies promise similar benefits, they are based on different foundations. SPL encourages systematic reuse that exploits the commonalities of various products belonging to a common domain and manages their variations systematically. In contrast, ASD stresses a flexible and rapid development of products using iterative and incremental approaches. ASD encourages active involvement of customers and their frequent feedback. Both ASD and SPL require alternatives to extend agile methods for several reasons such as (1) to manage reusability and variability across the products of any domain, (2) to avoid the risk of developing core assets that will become obsolete and not used in future projects, and (3) to meet the requirements of changing markets. This motivates the researchers for the integration of ASD and SPL approaches. As a result, an innovative approach called agile product line engineering (APLE) by integrating SPL and ASD has been introduced. The principal aim of APLE is to maximize the benefits of ASD and SPL and address the shortcomings of both. However, combining both is a major challenge. Researchers have proposed a few approaches that try to put APLE into practice, but none of the existing approaches cover all APLE features needed. This paper proposes a new dynamic variability approach for APLE that uses APLE practices for reusing features. The proposed approach (PA) is based on the agile method Scrum and the reactive approach of SPL. In this approach, reusable core assets respond reactively to customer requirements. The PA constructs and develops the SPL architecture iteratively and incrementally. It provides the benefits of reusability and maintainability of SPLs while keeping the delivery-focused approach from agile methods. We conducted a quantitative survey of software companies applying the APLE to assess the performance of the PA and hypotheses of empirical study. Findings of empirical evaluation provide evidence on integrating ASD and SPL and the application of APLE into practices.},
journal = {J. Supercomput.},
month = aug,
pages = {8391–8432},
numpages = {42},
keywords = {Software product line, Agile software development, Agile software product line, Agile product line engineering}
}

@inproceedings{10.1145/2593882.2593888,
author = {Metzger, Andreas and Pohl, Klaus},
title = {Software product line engineering and variability management: achievements and challenges},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593888},
doi = {10.1145/2593882.2593888},
abstract = {Software product line engineering has proven to empower organizations to develop a diversity of similar software-intensive systems (applications) at lower cost, in shorter time, and with higher quality when compared with the development of single systems. Over the last decade the software product line engineering research community has grown significantly. It has produced impressive research results both in terms of quality as well as quantity. We identified over 600 relevant research and experience papers published within the last seven years in established conferences and journals. We briefly summarize the major research achievements of these past seven years. We structure this research summary along a standardized software product line framework. Further, we outline current and future research challenges anticipated from major trends in software engineering and technology.},
booktitle = {Future of Software Engineering Proceedings},
pages = {70–84},
numpages = {15},
keywords = {Software product lines, design, quality assurance, requirements engineering, variability management, variability modeling},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/3233027.3233045,
author = {Becker, Martin and Zhang, Bo},
title = {How do our neighbours do product line engineering? a comparison of hardware and software product line engineering approaches from an industrial perspective},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233045},
doi = {10.1145/3233027.3233045},
abstract = {Product line engineering (PLE) approaches have been followed in industry for hardware and software solutions for more than three decades now. However, the different engineering disciplines (e.g. mechanics, electrics, software) have developed and evolved their approaches within their own realms, which is fine as long as there is no need for integrated approaches. Driven by the increasing complexity of systems, there is a rising need for interdisciplinary systems engineering these days. Companies engineering cyber-physical systems and their components have to integrate product line engineering approaches across the involved engineering disciplines to enable a global optimization of portfolio, solution structures, and assets along their lifecycle. From a bird's-eye view, there is noticeable commonality but also variety in the approaches followed for PLE in the different engineering disciplines, which renders the integration of approaches a non-trivial endeavour. In order to foster the development of integrated PLE approaches, this paper explores, maps, and compares PLE approaches in the field of hardware and software engineering. Furthermore, the paper identifies integration opportunities and challenges. As the paper targets industrial practitioners, it mainly provides references to respective industrial events and material and does not fully cover related work in the respective research communities.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {190–195},
numpages = {6},
keywords = {SPLC, academia, industry, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.infsof.2018.01.016,
author = {Soares, Larissa Rocha and Schobbens, Pierre-Yves and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Feature interaction in software product line engineering: A systematic mapping study},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.01.016},
doi = {10.1016/j.infsof.2018.01.016},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {44–58},
numpages = {15},
keywords = {Feature interaction, Software product lines, Systematic mapping}
}

@inproceedings{10.1145/3106195.3106224,
author = {Tizzei, Leonardo P. and Nery, Marcelo and Segura, Vin\'{\i}cius C. V. B. and Cerqueira, Renato F. G.},
title = {Using Microservices and Software Product Line Engineering to Support Reuse of Evolving Multi-tenant SaaS},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106224},
doi = {10.1145/3106195.3106224},
abstract = {In order to achieve economies of scale, a Software as a Service (SaaS) should be configurable, multi-tenant efficient, and scalable. But building SaaS with these characteristics comes at a price of having more complex services. Some works in the literature integrate software product line engineering and service-oriented architecture to tackle the complexity of building multi-tenant SaaS. Most of these works focused on centralized approaches that rely on middleware or platforms, but they do not investigate the use of decentralized architectural style. Microservices architecture is an architectural style that relies on small, decentralized, and autonomous services that work together. Thus, this paper investigates the integrated use of microservices architecture and software produt line techniques to develop multi-tenant SaaS. We conducted an empirical study that analyzes the behavior of software reuse during the evolution of a multi-tenant SaaS. This empirical study showed an average software reuse of 62% of lines of code among tenants. We also provide lessons we learned during the the re-engineering and maintenance of such multi-tenant SaaS.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {205–214},
numpages = {10},
keywords = {Microservices, Multi-tenancy, Service-oriented Architectures, Software Evolution, Software Reuse},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3461001.3473060,
author = {Sch\"{a}fer, Andreas and Becker, Martin and Andres, Markus and Kistenfeger, Tim and Rohlf, Florian},
title = {Variability realization in model-based system engineering using software product line techniques: an industrial perspective},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473060},
doi = {10.1145/3461001.3473060},
abstract = {Efficiently handling system variants is rising of importance in industry and challenges the application of model-based systems engineering.This paper reveals the increasing industrial demand of guidance and decision support on how to handle variants and variability within SysML and UML models. While a substantial amount of variability realization approaches has already been published on source code level, there is little guidance for practitioners on system model level. Hence, there is major uncertainty in dealing with system changes or concurrent system modeling of related system. Due to a poor modularization and variability realization these model variants are ending up in interwoven and complex system models.In this paper, we aim to raise awareness of the need for appropriate guidance and decision support, identify important contextual factors of MBSE that influence variability realization, and derive well known variability mechanisms used in software coding for their applicability in system modeling.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {25–34},
numpages = {10},
keywords = {SysML, UML, decision support, model-based systems engineering, system and software product line engineering, variability mechanism, variability realization, variant management},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414942,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Kr\"{u}ger, Jacob and Mendon\c{c}a, Willian D. F.},
title = {Variability management meets microservices: six challenges of re-engineering microservice-based webshops},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414942},
doi = {10.1145/3382025.3414942},
abstract = {A microservice implements a small unit of functionality that it provides through a network using lightweight protocols. So, microservices can be combined to fulfill tasks and implement features of a larger software system---resembling a variability mechanism in the context of a software product line (SPL). Microservices and SPLs have similar goals, namely facilitating reuse and customizing, but they are usually employed in different contexts. Any developer who has access to the network can provide a microservice for any task, while SPLs are usually intended to implement features of a specific domain. Due to their different concepts, using microservices to implement an SPL or adopting SPL practices (e.g., variability management) for microservices is a challenging cross-area research problem. However, both techniques can complement each other, and thus tackling this problem promises benefits for organizations that employ either technique. In this paper, we reason on the importance of advancing in this direction, and sketch six concrete challenges to initiate research, namely (1) feature identification, (2) variability modeling, (3) variable microservice architectures, (4) interchangeability, (5) deep customization, and (6) re-engineering an SPL. We intend these challenges to serve as a starting point for future research in this cross-area research direction---avoiding that the concepts of one area are reinvented in the other.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {22},
numpages = {6},
keywords = {cloud computing, microservices, re-engineering, software product line, variability management},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2811681.2811703,
author = {Tan, Lei and Lin, Yuqing},
title = {An Aspect-Oriented Feature Modelling Framework for Software Product Line Engineering},
year = {2015},
isbn = {9781450337960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811681.2811703},
doi = {10.1145/2811681.2811703},
abstract = {Software Product Line Engineering (SPLE) is a software development paradigm that focusing on systematic software assets reuse. SPLE treats software products in the same application domains as a product family and developing various of assets could be reused in the product family. Feature modelling is a critical activity of SPLE, which developing the requirement model for product families and providing guidance for individual product implementation. In this paper, we discuss several drawbacks of current feature modelling and propose a solution which adopting aspect-oriented development ideas and approaches. The proposed framework is intended to better manage complex feature relationships, and enhance quality-aware feature modelling. We include a case study of a real-life experience to demonstrate the proposed approach.},
booktitle = {Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference},
pages = {111–115},
numpages = {5},
keywords = {aspectoriented, feature modelling, software product line engineering},
location = {Adelaide, SA, Australia},
series = {ASWEC ' 15 Vol. II}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {SBSE, SPL, genetic programming, program synthesis},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1109/SPLC.2011.55,
author = {Dao, Tung M. and Lee, Hyesun and Kang, Kyo C.},
title = {Problem Frames-Based Approach to Achieving Quality Attributes in Software Product Line Engineering},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.55},
doi = {10.1109/SPLC.2011.55},
abstract = {In software product line engineering (SPLE), commonality and variability across products of a product line domain are captured typically by a feature model. Reusable components are then developed from features. However, mapping features to components remains a complex task requiring a systematic way of exploring and analyzing various concerns arising from inadequate/insufficient domain assumptions. Essentially, those concerns prevent SPLE from achieving various quality attributes. This paper proposes a problem frames-based approach to addressing this problem. An elevator product line example is used to demonstrate the feasibility of the approach.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {175–180},
numpages = {6},
keywords = {Feature Models, Goal Models, Problem Frames, Quality Attributes},
series = {SPLC '11}
}

@inproceedings{10.1145/3307630.3342418,
author = {Rinc\'{o}n, Luisa and Mazo, Ra\'{u}l and Salinesi, Camille},
title = {Analyzing the Convenience of Adopting a Product Line Engineering Approach: An Industrial Qualitative Evaluation},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342418},
doi = {10.1145/3307630.3342418},
abstract = {Engineering Software Product Lines may be a strategy to reduce costs and efforts for developing software and increasing business productivity. However, it cannot be considered as a "silver bullet" that applies to all types of organizations. Companies must consider pros and cons to determine sound reasons and justify its adoption. In previous work, we proposed the APPLIES evaluation framework to help decision-makers find arguments that may justify (or not) adopting a product line engineering approach. This paper presents our experience using this framework in a mid-sized software development company with more than 25 years of experience but without previous experience in product line engineering. This industrial experience, conducted as a qualitative empirical evaluation, helped us to evaluate to what extent APPLIES is practical to be used in a real environment and to gather ideas from real potential users to improve the framework.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {90–97},
numpages = {8},
keywords = {empirical evaluation, product line adoption, product line engineering, qualitative evaluation},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/978-3-319-35122-3_2,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Automated Composition of Service Mashups Through Software Product Line Engineering},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_2},
doi = {10.1007/978-3-319-35122-3_2},
abstract = {The growing number of online resources, including data and services, has motivated both researchers and practitioners to provide methods and tools for non-expert end-users to create desirable applications by putting these resources together leading to the so called mashups. In this paper, we focus on a class of mashups referred to as service mashups. A service mashup is built from existing services such that the developed service mashup offers added-value through new functionalities. We propose an approach which adopts concepts from software product line engineering and automated AI planning to support the automated composition of service mashups. One of the advantages of our work is that it allows non-experts to build and optimize desired mashups with little knowledge of service composition. We report on the results of the experimentation that we have performed which support the practicality and scalability of our proposed work.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {20–38},
numpages = {19},
keywords = {Automated composition, Feature model, Planning, Service mashups, Software product lines, Workflow optimization},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/2491627.2499880,
author = {Clarke, Dave and Schaefer, Ina and ter Beek, Maurice H. and Apel, Sven and Atlee, Joanne M.},
title = {Formal methods and analysis in software product line engineering: 4th edition of FMSPLE workshop series},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2499880},
doi = {10.1145/2491627.2499880},
abstract = {FMSPLE 2013 is the fourth edition of the FMSPLE workshop series aimed at connecting researchers and practitioners interested in raising the efficiency and the effectiveness of software product line engineering through the application of innovative analysis approaches and formal methods.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {266–267},
numpages = {2},
keywords = {evolution, formal methods, semantics, software product lines, testing, variability, verification},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1109/APSEC.2014.94,
author = {Tan, Lei and Lin, Yuqing and Liu, Li},
title = {Quality Ranking of Features in Software Product Line Engineering},
year = {2014},
isbn = {9781479974269},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2014.94},
doi = {10.1109/APSEC.2014.94},
abstract = {Software Product Line Engineering (SPLE) is a systematic software reuse approach that developing a set of similar software products as a family. All the visible characters of the products in a product family are represented as features and their relationships are modelled in a feature model. During application engineering, desired features are selected from the feature model in a configuration process based on the requirements. In this process, the quality of final product should be considered as early as possible which requires identifying and ranking associated features' contributions to related quality attributes before configuring member products. In this paper, we propose a ranking approach to address the issues in current quality based feature ranking approaches, we also include a case study to illustrate our approach at the end.},
booktitle = {Proceedings of the 2014 21st Asia-Pacific Software Engineering Conference - Volume 02},
pages = {57–62},
numpages = {6},
series = {APSEC '14}
}

@inproceedings{10.1145/3106195.3106218,
author = {Krueger, Charles and Clements, Paul},
title = {Enterprise Feature Ontology for Feature-based Product Line Engineering and Operations},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106218},
doi = {10.1145/3106195.3106218},
abstract = {Feature trees have been the standard data structure for representing product diversity in feature-based systems and software product line engineering (PLE). For basic product lines of modest size or complexity, one or several modular feature trees can be sufficient for managing the and resolving the variation present across the engineering assets in the systems engineering 'V' --- from requirements, to design, through implementation, verification, validation, documentation, and more --- in the software, mechanical, and electrical disciplines. However, enterprises seeking to adopt PLE at all levels of their organization, including areas such as product marketing, portfolio planning, manufacturing, supply chain, product sales, product service and maintenance, Internet-of-Things, resource planning, and much more are finding that thousands of nonengineering users need different views and interaction scenarios with a feature diversity representation. This paper describes a feature ontology (a specification of the meaning of terms in the feature modeling realm) that is suitable for managing the feature-based product line engineering and operations in the largest and most complex product line organizations. This ontology is based on layers of abstraction that each incrementally constrain the complexity and combinatorics and targets specific roles in the organization for greater degrees of efficiency, precision, and automation across an entire business enterprise.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {227–236},
numpages = {10},
keywords = {PLE factory, Product line engineering, bill-of-features, enterprise feature ontology, feature modeling, feature profiles, feature-based product line engineering, product configurator, product portfolio, software product lines, variation points},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.infsof.2013.05.006,
author = {Mohabbati, Bardia and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and M\"{u}ller, Hausi A.},
title = {Combining service-orientation and software product line engineering: A systematic mapping study},
year = {2013},
issue_date = {November, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.006},
doi = {10.1016/j.infsof.2013.05.006},
abstract = {Context: Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective: This study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE. Method: We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000-2011 and classified them with respect to research focus, types of research and contribution. Result: The mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches. In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion: Our study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1845–1859},
numpages = {15},
keywords = {Service-oriented architecture, Software product lines, Systematic mapping}
}

@inproceedings{10.1145/3106195.3106205,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Configurations of Functional Quality Attributes},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106205},
doi = {10.1145/3106195.3106205},
abstract = {Functional quality attributes (FQAs) are those quality attributes that, to be satisfied, require the incorporation of additional functionality into the application architecture. By adding an FQA (e.g., security) we can improve the quality of the final product, but there is also an increase in energy consumption. This paper proposes a solution to help the software architect to generate configurations of FQAs whilst keeping the energy consumed by the application as low as possible. For this, a usage model is defined for each FQA, taking into account the variables that affect the energy consumption, and that the values of these variables change according to the part of the application where the FQA is required. We extend a Software Product Line that models a family of FQAs to incorporate the variability of the usage model and the existing frameworks that implement FQAs. We generate the most eco-efficient configuration of FQAs by selecting the framework with the most suitable characteristics according to the requirements of the application.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {79–83},
numpages = {5},
keywords = {Energy Consumption, FQA, Quality Attributes, SPL, Variability},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3483899.3483902,
author = {Marchezan, Luciano and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Carbonell, Jo\~{a}o and Rodrigues, Elder and Bernardino, Maicon and Basso, F\'{a}bio},
title = {SPLReePlan - Automated Support for Software Product Line Reengineering Planning},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483902},
doi = {10.1145/3483899.3483902},
abstract = {The extractive adoption of Software Product Lines (SPL) relies on the reuse of the already developed systems, employing a reengineering process. However, due to the diversity of options found in the daily practice of SPL development, rigorous planning of scenarios is critical to perform SPL reengineering. This diversity is the result of different organizational aspects, such as team experience and product portfolio. Hence, a proper planning process must consider technical and organizational aspects, however, most existing studies in the field do not take into account organizational aspects of the companies. In this work, we present SPLReePlan, an automated framework to aid the SPL reengineering planning taking into account technical and organizational aspects. Our framework is supported by a web-based tool, ready to be used in the industry. To investigate how flexible is SPLReePlan to support the SPL reengineering planning in diverse situations, we extracted eight different scenarios from the SPL literature, which are used as input for the evaluation of SPLReePlan. The results indicate that SPLReePlan can be satisfactorily customized to a variety of scenarios with different artifacts, feature retrieval techniques, and reengineering activities. As a contribution, we discuss the lessons learned within the evaluation, and present challenges that were faced, being a source of information for tool builders or motivating new studies.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {automated support, reengineering process, software product lines, variability management},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1145/2362536.2362576,
author = {ter Beek, Maurice H. and Becker, Martin and Classen, Andreas and Roos-Frantz, Fabricia and Schaefer, Ina and Wong, Peter Y. H.},
title = {Formal methods and analysis in software product line engineering: 3rd edition of FMSPLE workshop series},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362576},
doi = {10.1145/2362536.2362576},
abstract = {FMSPLE 2012 is the third edition of the FMSPLE workshop series, traditionally affiliated with SPLC, which aims to connect researchers and practitioners interested in raising the efficiency and the effectiveness of SPLE through the application of innovative analysis approaches and formal methods.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {286–287},
numpages = {2},
keywords = {evolution, formal methods, semantics, software product lines, testing, variability, verification},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.dam.2019.06.008,
author = {Carbonnel, Jessie and Bertet, Karell and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {FCA for software product line representation: Mixing configuration and feature relationships in a unique canonical representation},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {273},
number = {C},
issn = {0166-218X},
url = {https://doi.org/10.1016/j.dam.2019.06.008},
doi = {10.1016/j.dam.2019.06.008},
journal = {Discrete Appl. Math.},
month = feb,
pages = {43–64},
numpages = {22},
keywords = {Software product line, Feature model, Formal concept analysis, Concept lattice}
}

@article{10.1007/s11334-011-0159-y,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {An architecture process maturity model of software product line engineering},
year = {2011},
issue_date = {September 2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {7},
number = {3},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-011-0159-y},
doi = {10.1007/s11334-011-0159-y},
abstract = {Software architecture has been a key research area in the software engineering community due to its significant role in creating high-quality software. The trend of developing product lines rather than single products has made the software product line a viable option in the industry. Software product line architecture (SPLA) is regarded as one of the crucial components in the product lines, since all of the resulting products share this common architecture. The increased popularity of software product lines demands a process maturity evaluation methodology. Consequently, this paper presents an architecture process maturity model for software product line engineering to evaluate the current maturity of the product line architecture development process in an organization. Assessment questionnaires and a rating methodology comprise the framework of this model. The objective of the questionnaires is to collect information about the SPLA development process. Thus, in general this work contributes towards the establishment of a comprehensive and unified strategy for the process maturity evaluation of software product line engineering. Furthermore, we conducted two case studies and reported the assessment results, which show the maturity of the architecture development process in two organizations.},
journal = {Innov. Syst. Softw. Eng.},
month = sep,
pages = {191–207},
numpages = {17},
keywords = {Application engineering, Domain engineering, Process assessment, Software architecture, Software product line}
}

@inproceedings{10.1145/3382026.3431248,
author = {Ferreira, Thiago Nascimento and Vergilio, Silvia Regina and Kessentini, Mauroane},
title = {Many-objective Search-based Selection of Software Product Line Test Products with Nautilus},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431248},
doi = {10.1145/3382026.3431248},
abstract = {The Variability Testing of Software Product Lines (VTSPL) concerns the selection of the most representative products to be tested according to specific goals. Works in the literature use a great variety of objectives and distinct algorithms. However, they neither address all the objectives at the same time nor offer an automatic tool to support this task. To this end, this work introduces Nautilus/VTSPL, a tool to address the VTSPL problem, created by instantiating Nautilus Framework. Nautilus/VTSPL allows the tester to experiment and configure different objectives and categories of many-objective algorithms. The tool also offers support to visualization of the generated solutions, easing the decision-making process.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–4},
numpages = {4},
keywords = {many-objective algorithms, product line testing, sbse},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3109729.3109744,
author = {Munoz, Daniel-Jesus},
title = {Achieving energy efficiency using a Software Product Line Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109744},
doi = {10.1145/3109729.3109744},
abstract = {Green computing and energy-aware software engineering are trend approaches that try to address the development of applications respectful with the environment. To reduce the energy consumption of an application the developer needs: (i) to identify what are the concerns that will impact more in the energy consumption; (ii) to model the variability of alternative designs and implementations of each concern; (iii) to store and compare the experimentation results related with the energy and time consumption of concerns; (iv) to find out what is the most eco-efficient solution for each concern. HADAS addresses these issues by modelling the variability of energy consuming concerns for different energy contexts. It connects the variability model with a repository that stores energy measurements, providing a Software Product Line (SPL) service, helping developers to reason and find out what are the most eco-friendly configurations. We have an initial implementation of the HADAS toolkit using Clafer. We have tested our implementation with several case studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {131–138},
numpages = {8},
keywords = {Clafer, Energy Efficiency, Metrics, Optimisation, Repository, Software Product Line, Variability},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/1982185.1982336,
author = {Asadi, Mohsen and Bagheri, Ebrahim and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Mohabbati, Bardia},
title = {Goal-driven software product line engineering},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982336},
doi = {10.1145/1982185.1982336},
abstract = {Feature Models encapsulate functionalities and quality properties of a product family. The employment of feature models for managing variability and commonality of large-scale product families raises an important question: on what basis should the features of a product family be selected for a target software application, which is going to be derived from the product family. Thus, the selection of the most suitable features for a specific application requires the understanding of its stakeholders' intentions and also the relationship between their intentions and the available software features. To address this important issue, we adopt a standard goal-oriented requirements engineering framework, i.e., the i* framework, for identifying stakeholders' intentions and propose an approach for explicitly mapping and bridging between the features of a product family and the goals and objectives of the stakeholders. We propose a novel approach to automatically preconfigure a given feature model based on the objectives of the target product stakeholders. Also, our approach is able to elucidate the rationale behind the selection of the most important features of a family for a target application.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {691–698},
numpages = {8},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.5555/1885639.1885680,
author = {Lutz, Robyn and Weiss, David and Krishnan, Sandeep and Yang, Jingwei},
title = {Software product line engineering for long-lived, sustainable systems},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The design and operation of long-lived, sustainable systems (LSS) are hampered by limited support for change over time and limited preservation of system knowledge. The solution we propose is to adopt software product-line engineering (SPLE) techniques for use in single, critical systems with requirements for sustainability. We describe how four categories of change in a LSS can be usefully handled as variabilities in a software product line. We illustrate our argument with examples of changes from the Voyager spacecraft.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {430–434},
numpages = {5},
keywords = {commonality/variability analysis, long-lived system, software product line, sustainable system, variability},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.jss.2015.11.005,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {An automatic process for weaving functional quality attributes using a software product line approach},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {112},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.11.005},
doi = {10.1016/j.jss.2015.11.005},
abstract = {We define a family of FQAs ready to be reused in many software architectures.We define an Aspect-Oriented SPL to inject customized FQAs into the applications.Two different implementations of the SPL are provided and compared.Modelling FQAs separately from the applications increases reusability.The final architectures exhibit a high degree of separation of concerns. Some quality attributes can be modelled using software components, and are normally known as Functional Quality Attributes (FQAs). Applications may require different FQAs, and each FQA (e.g., security) can be composed of many concerns (e.g., access control or authentication). They normally have dependencies between them and crosscut the system architecture. The goal of the work presented here is to provide the means for software architects to focus only on application functionality, without having to worry about FQAs. The idea is to model FQAs separately from application functionality following a Software Product Line (SPL) approach. By combining SPL and aspect-oriented mechanisms, we will define a generic process to model and automatically inject FQAs into the application without breaking the base architecture. We will provide and compare two implementations of our generic approach using different variability and architecture description languages: (i) feature models and an aspect-oriented architecture description language; and (ii) the Common Variability Language (CVL) and a MOF-compliant language (e.g., UML). We also discuss the benefits and limitations of our approach. Modelling FQAs separately from the base application has many advantages (e.g., reusability, less coupled components, high cohesive architectures).},
journal = {J. Syst. Softw.},
month = feb,
pages = {78–95},
numpages = {18},
keywords = {Quality attributes, Software product lines, Weaving}
}

@inproceedings{10.1145/2420942.2420948,
author = {Gonz\'{a}lez-Huerta, Javier and Insfran, Emilio and Abrah\~{a}o, Silvia and McGregor, John D.},
title = {Non-functional requirements in model-driven software product line engineering},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420948},
doi = {10.1145/2420942.2420948},
abstract = {Developing variant-rich software systems through the application of the software product line approach requires the management of a wide set of requirements. However, in most cases, the focus of those requirements is limited to the functional requirements. The non-functional requirements are often informally defined and their management does not provide traceability mechanisms for their validation. In this paper, we present a multimodel approach that allows the explicit representation of non-functional requirements for software product lines both at domain engineering, and application engineering levels. The multimodel allows the representation of different viewpoints of a software product line, including the non-functional requirements and the relationships that these non-functional requirements might have with features and functionalities. The feasibility of this approach is illustrated through a specific example from the automotive domain.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {6},
numpages = {6},
keywords = {model driven engineering, non-functional requirements, software product lines},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {Software product lines, product-line analysis}
}

@inproceedings{10.1145/2701319.2701326,
author = {Soares, Larissa Rocha and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Non-Functional Properties in Software Product Lines: A Reuse Approach},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701326},
doi = {10.1145/2701319.2701326},
abstract = {Software Product Line Engineering (SPLE) emerges for software organizations interested in customized products at reasonable costs. Based on the selection of features, stakeholders can derive programs satisfying a range of functional properties and non-functional ones. The explicit definition of Non-Functional Properties (NFP) during software configuration has been considered a challenging task. Dealing with them is not well established yet, neither in theory nor in practice. In this sense, we present a framework to specify NFP for SPLE and we also propose a reuse approach that promotes the reuse of NFP values during the product configuration. We discuss the results of a case study aimed to evaluate the applicability of the proposed work.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {67–74},
numpages = {8},
keywords = {Empirical Software Engineering, Quality Attributes, Software Product Line},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/3233027.3233028,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {A study and comparison of industrial vs. academic software product line research published at SPLC},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233028},
doi = {10.1145/3233027.3233028},
abstract = {The study presented in this paper aims to provide evidence for the hypothesis that software product line research has been changing and that the works in industry and academia have diverged over time. We analysed a subset (140) of all (593) papers published at the Software Product Line Conference (SPLC) until 2017. The subset was randomly selected to cover all years as well as types of papers. We assessed the research type of the papers (academic or industry), the kind of evaluation (application example, empirical, etc.), and the application domain. Also, we assessed which product line life-cycle phases, development practices, and topics the papers address. We present an analysis of the topics covered by academic vs. industry research and discuss the evolution of these topics and their relation over the years. We also discuss implications for researchers and practitioners. We conclude that even though several topics have received more attention than others, academic and industry research on software product lines are actually rather in line with each other.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {14–24},
numpages = {11},
keywords = {SPLC, academia, industry, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1007/978-3-642-33678-2_30,
author = {Braga, Rosana T. Vaccare and Trindade Junior, Onofre and Castelo Branco, Kalinka Regina and Neris, Luciano De Oliveira and Lee, Jaejoon},
title = {Adapting a software product line engineering process for certifying safety critical embedded systems},
year = {2012},
isbn = {9783642336775},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33678-2_30},
doi = {10.1007/978-3-642-33678-2_30},
abstract = {Software Product Line Engineering (SPLE) is a software development paradigm that aims at reducing the development effort and shorting time-to-market through systematic software reuse. While this paradigm has been successfully applied for the development of embedded systems in various domains, new challenges have emerged from the development of safety critical systems that require certification against a specific standard. Existing SPLE approaches do not explicitly consider the various certification standards or levels that products should satisfy. In this paper, we focus on several practical issues involved in the SPLE process, establishing an infrastructure of a product line engineering for certified products. A metamodel is proposed to capture the entities involved in SPL certification and the relationships among them. ProLiCES, which is a model-driven process for the development of SPLs, was modified to serve as an example of our approach, in the context of the UAV (Unmanned Aerial Vehicle) domain.},
booktitle = {Proceedings of the 31st International Conference on Computer Safety, Reliability, and Security},
pages = {352–363},
numpages = {12},
keywords = {development process, safety-critical embedded systems, software certification},
location = {Magdeburg, Germany},
series = {SAFECOMP'12}
}

@inproceedings{10.1145/2491627.2493906,
author = {Bashari, Mahdi and Bagheri, Ebrahim},
title = {Engineering self-adaptive systems and dynamic software product line},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2493906},
doi = {10.1145/2491627.2493906},
abstract = {Self-adaptive systems are a class of software applications, which are able to dynamically transform their internal structure and hence their behavior in response to internal or external stimuli. The transformation may provide the basis for new functionalities or improve or maintain non-functional properties in order to match the application better to its operational requirements and standards. Software Product Line Engineering has rich methods and techniques in variability modeling and management which is one of the main issues in developing self-adaptive systems. Dynamic software product lines (DSPL) have been proposed to exploit the knowledge acquired in SPLE to develop self-adaptive software systems.In this tutorial, we portray the problem of developing self-adaptive systems. Then we investigate how the idea of dynamic software product line could help to deal with the challenges that we face in developing efficient self-adaptive software. We also offer insight into the different approaches that use dynamic software product line engineering for developing self-adaptive systems focusing on practical approaches by showing how the approaches are applied to real case studies and also methods for evaluating these approaches. This tutorial also discuss how DSPL could be used some relevant areas to self-adaptive systems and challenges which still exist in the area.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {285},
numpages = {1},
keywords = {dynamic software product line, self-adaptive systems},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2364412.2364445,
author = {Martinez, Jabier and Thurimella, Anil Kumar},
title = {Collaboration and source code driven bottom-up product line engineering},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364445},
doi = {10.1145/2364412.2364445},
abstract = {Companies that develop similar software systems often transition from single-system development to software product line development. In this transition, reusable assets are identified and incrementally created over a period of time. Bottom-up Software Product Line Engineering approaches aid stakeholders to identify variability from the legacy artifacts. One of these artifacts is the legacy source code. In this paper, we contribute the Collaboration and Source Code Driven Bottom-up approach, with two main enhancements. We apply clone detection and architecture reengineering techniques for identifying variability from the legacy artifacts. These techniques which have been traditionally used for maintaining software are now used for identifying variability and analyze code coupling and cohesion from the legacy code. Our second enhancement is improving stakeholder collaboration by guiding the domain experts in order to decide on variability. In particular, we apply Questions, Options and Criteria technique for capturing rationale and supporting collaboration.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {196–200},
numpages = {5},
keywords = {architecture reengineering, clone detection, knowledge management, rationale, software product line engineering, variability modeling},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1109/APSEC.2008.45,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Saake, Gunter},
title = {Measuring Non-Functional Properties in Software Product Line for Product Derivation},
year = {2008},
isbn = {9780769534466},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2008.45},
doi = {10.1109/APSEC.2008.45},
abstract = {A software product line (SPL) enables stakeholders to derive different software products for a domain while providing a high degree of reuse of their code units. Software products are derived in a configuration process by composing different code units. The configuration process becomes complex if SPLs contain hundreds of features. In many cases, a stakeholder is not only interested in functional but also in non-functional properties of a desired product. Because SPLs can be used in different application scenarios alternative implementations of already existing functionality are developed to meet special non-functional requirements, like restricted binary size and performance guarantees. To enable these complex configurations we discuss and present techniques to measure non-functional properties of software modules and use these values to compute SPL configurations optimized to the users needs.},
booktitle = {Proceedings of the 2008 15th Asia-Pacific Software Engineering Conference},
pages = {187–194},
numpages = {8},
keywords = {Non-functional Properties, Product Derivation, Software Product Lines},
series = {APSEC '08}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1109/SPLC.2011.44,
author = {Nolan, Andy J. and Abrahao, Silvia and Clements, Paul and McGregor, John D. and Cohen, Sholom},
title = {Towards the Integration of Quality Attributes into a Software Product Line Cost Model},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.44},
doi = {10.1109/SPLC.2011.44},
abstract = {A good estimation tool offers a "model" of a project and is usually used to estimate cost and schedule, but it can also be used to help make trade decisions that affect cost and schedule as well as to estimate risks and opportunities. It was evident that Rolls-Royce needed a cost model to underpin decisions when they launched a Software Product Line initiative. The first generation cost model was based on COCOMO II, which represents the software product as a single size measure (Source Lines of Code) but makes limited use of the architecture or any characteristics of the product being developed. The next generation of the cost model, currently under development, is intended to account for the quality attributes of the core assets and the resulting products in order to estimate their impact on cost and net-benefit to the business. The objective of this paper is to describe our current efforts to integrate key quality attributes into the SPL cost model. We describe the quality attributes selected, the reason for their selection and the benefits we expect to obtain after integrating them into the model.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {203–212},
numpages = {10},
keywords = {Cost Estimation, Industrial Experiences, Quality Attributes, Safety-Critical Software, Software Product Lines},
series = {SPLC '11}
}

@inproceedings{10.1145/3307630.3342385,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {HADAS: Analysing Quality Attributes of Software Configurations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342385},
doi = {10.1145/3307630.3342385},
abstract = {Software Product Lines (SPLs) are highly configurable systems. Automatic analyses of SPLs rely on solvers to navigate complex dependencies among features and find legal solutions. Variability analysis tools are complex due to the diversity of products and domain-specific knowledge. On that, while there are experimental studies that analyse quality attributes, the knowledge is not easily accessible for developers, and its appliance is not trivial. Aiming to allow the industry to quality-explore SPL design spaces, we developed the HADAS assistant that: (1) models systems and collects quality attributes metrics in a cloud repository, and (2) reasons about it helping developers with quality attributes requirements.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {NFQA, attribute, model, numerical, software product line, variability},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Variability management, Software product lines, Multiple-case study, Challenges}
}

@article{10.1504/ijaip.2021.116357,
author = {Rajesh, Sudha and Sekar, A. Chandra},
title = {Evaluation of quality attributes of software design patterns using association rules},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {19},
number = {3–4},
issn = {1755-0386},
url = {https://doi.org/10.1504/ijaip.2021.116357},
doi = {10.1504/ijaip.2021.116357},
abstract = {For the past decades, there were many analysing methods used which in turn to analyse only the views of a single stakeholder. The analysing progression facilitates us to guarantee the quality of overall design. By doing so there were many limitations that lead to critical situations in the development process. This work proposes to collect all the stakeholders' decisions in a single structural view, which expose the centric-view decision in an architectural design. It also used to deal with the disagreement in vision by examining it, with the premium software quality characteristic. The quality attributes are evaluated by association rules to improve the excellence of the software design. Association rules are implemented using R tool to get the support, confidence, lift and count values of each attribute along with its metric. The best quality attributes are chosen after managing the stakeholders' conflicts, since all the systems are designed due to the stakeholders' concerns. The best design patterns can be evaluated along with the distinguished attributes. These patterns are evaluated by fixing the basic principles of patterns based on their excellence.},
journal = {Int. J. Adv. Intell. Paradigms},
month = jan,
pages = {314–327},
numpages = {13},
keywords = {association rules, confidence, design patterns, support, quality attributes}
}

@inproceedings{10.1145/2491627.2491647,
author = {Murashkin, Alexandr and Antkiewicz, Micha\l{} and Rayside, Derek and Czarnecki, Krzysztof},
title = {Visualization and exploration of optimal variants in product line engineering},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491647},
doi = {10.1145/2491627.2491647},
abstract = {The decision-making process in Product Line Engineering (PLE) is often concerned with variant qualities such as cost, battery life, or security. Pareto-optimal variants, with respect to a set of objectives such as minimizing a variant's cost while maximizing battery life and security, are variants in which no single quality can be improved without sacrificing other qualities. We propose a novel method and a tool for visualization and exploration of a multi-dimensional space of optimal variants (i.e., a Pareto front). The visualization method is an integrated, interactive, and synchronized set of complementary views onto a Pareto front specifically designed to support PLE scenarios, including: understanding differences among variants and their positioning with respect to quality dimensions; solving trade-offs; selecting the most desirable variants; and understanding the impact of changes during product line evolution on a variant's qualities. We present an initial experimental evaluation showing that the visualization method is a good basis for supporting these PLE scenarios.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {111–115},
numpages = {5},
keywords = {ClaferMoo, ClaferMoo visualizer, clafer, exploration, feature modeling, optimal variant, pareto front, product line engineering, visualization},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2364412.2364451,
author = {Machado, Ivan do Carmo},
title = {Towards a reasoning framework for software product line testing},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364451},
doi = {10.1145/2364412.2364451},
abstract = {Testing can still be considered a bottleneck for software product line engineering. The variability implemented in the source artifacts increases its complexity. Due to its key role for product line quality, testing requires cost-effective practices, such as techniques for test selection should be produced to enable companies to experience the substantial production cost savings. In this paper, we present the outline of a Ph.D. research aimed at developing a reasoning framework to improve SPL testing practices. Based on multiple sources of evidence, the framework intends to provide testers with an automated reasoner for determining which techniques may be suitable for a given variability implementation mechanism, and how these should be employed in order to makes testing in a SPL a more effective and efficient practice. We plan to perform empirical evaluations in order to assess the proposal effectiveness.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {229–232},
numpages = {4},
keywords = {fault models, software product lines, software testing, variability management},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1109/APSEC.2010.25,
author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
title = {Quality Attributes Assessment for Feature-Based Product Configuration in Software Product Line},
year = {2010},
isbn = {9780769542669},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2010.25},
doi = {10.1109/APSEC.2010.25},
abstract = {Product configuration based on a feature model in software product lines is the process of selecting the desired features based on customers’ requirements. In most cases, application engineers focus on the functionalities of the target product during product configuration process whereas the quality attributes are handled until the final product is produced. However, it is costly to fix the problem if the quality attributes have not been considered in the product configuration stage. The key issue of assessing a quality attribute of a product configuration is to measure the impact on a quality attribute made by the set of functional variable features selected in a configuration. Current existing approaches have several limitations, such as no quantitative measurements provided or requiring existing valid products and heavy human effort for the assessment. To overcome theses limitations, we propose an Analytic Hierarchical Process (AHP) based approach to estimate the relative importance of each functional variable feature on a quality attribute. Based on the relative importance value of each functional variable feature on a quality attribute, the level of quality attributes of a product configuration in software product lines can be assessed. An illustrative example based on the Computer Aided Dispatch (CAD) software product line is presented to demonstrate how the proposed approach works.},
booktitle = {Proceedings of the 2010 Asia Pacific Software Engineering Conference},
pages = {137–146},
numpages = {10},
keywords = {Analytic Hierarchical Process (AHP), product configuration, quality attributes assessment},
series = {APSEC '10}
}

@inproceedings{10.1145/2648511.2648537,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina and Gimenes, Itana M. S. and Oizumi, Willian Nalepa},
title = {A search-based approach for software product line design},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648537},
doi = {10.1145/2648511.2648537},
abstract = {The Product Line Architecture (PLA) can be improved by taking into account key factors such as feature modularization, and by continuously evaluating its design according to metrics. Search-Based Software Engineering (SBSE) principles can be used to support an informed-design of PLAs. However, existing search-based design works address only traditional software design not considering intrinsic Software Product Line aspects. This paper presents MOA4PLA, a search-based approach to support the PLA design. It gives a multi-objective treatment to the design problem based on specific PLA metrics. A metamodel to represent the PLA and a novel search operator to improve feature modularization are proposed. Results point out that the application of MOA4PLA leads to PLA designs with well modularized features, contributing to improve features reusability and extensibility. It raises a set of solutions with different design trade-offs that can be used to improve the PLA design.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {237–241},
numpages = {5},
keywords = {multi-objective algorithms, searchbased PLA design, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2019136.2019172,
author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
title = {Using knowledge-based systems to manage quality attributes in software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019172},
doi = {10.1145/2019136.2019172},
abstract = {Product configuration in a feature model in software product line engineering is a process, in which the desired features are selected based on the customers' functional requirements and non-functional requirements. The functional requirements of the target product can be satisfied by including the proper functional features. However, there is no such a straightforward way to realize the non-functional requirements and quality attributes of the target product. In our early work, we have developed a quantitative based method to assess the quality attributes for a configured product. However, this approach cannot adequately represent the inter-relationships among quality attributes which play an important role in product configuration process. We supplement our previous work by introducing a quality attribute knowledge base (QA_KB) to represent the inter-relationships among different quality attributes in a SPL. Furthermore, we develop algorithms for configuring a product based on customers' quality requirements. We also use a case study to illustrate our approach.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {32},
numpages = {7},
keywords = {feature model, non-functional requirements, product configuration, quality attributes, software product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2019136.2019182,
author = {McGregor, John D. and Monteith, J. Yates and Zhang, Jie},
title = {Quantifying value in software product line design},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019182},
doi = {10.1145/2019136.2019182},
abstract = {A software product line is a strategic investment for an organization. Besides the initial decision to use a product line approach other strategic decisions are made, including which variations to accommodate. In this paper we present an adaptation of an equation for computing option values. The equation can be used to understand the economic impact of adding a variation point to the product line architecture. The equation was exercised on multiple sets of hypothetical data and and produced the expected changes from one data set to another. In the future the equation will be validated with data from real projects. We describe some practical sources of values for the parameters of the equation.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {40},
numpages = {7},
keywords = {software engineering, strategic software design},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2892664.2892686,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Towards the dynamic reconfiguration of quality attributes},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892686},
doi = {10.1145/2892664.2892686},
abstract = {There are some Quality Attributes (QAs) whose variability is addressed through functional variability in the software architecture. Separately modelling the variability of these QAs from the variability of the base functionality of the application has many advantages (e.g., a better reusability), and facilitates the reconfiguration of the QA variants at runtime. Many factors may vary the QA functionality: variations in the user preferences and usage needs; variations in the non-functional QAs; variations in resources, hardware, or even in the functionality of the base application, that directly affect the product's QAs. In this paper, we aim to elicit the relationships and dependencies between the functionalities required to satisfy the QAs and all those factors that can provoke a reconfiguration of the software architecture at runtime. We follow an approach in which the variability of the QAs is modelled separately from the base application functionality, and propose a dynamic approach to reconfigure the software architecture based on those reconfiguration criteria.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {131–136},
numpages = {6},
keywords = {Quality attributes, SPL, reconfiguration, software architecture, variability},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@inproceedings{10.1145/3167132.3167353,
author = {Pereira, Juliana Alves and Martinez, Jabier and Gurudu, Hari Kumar and Krieter, Sebastian and Saake, Gunter},
title = {Visual guidance for product line configuration using recommendations and non-functional properties},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167353},
doi = {10.1145/3167132.3167353},
abstract = {Software Product Lines (SPLs) are a mature approach for the derivation of a family of products using systematic reuse. Different combinations of predefined features enable tailoring the product to fit the needs of each customer. These needs are related to functional properties of the system (optional features) as well as non-functional properties (e.g., performance or cost of the final product). In industrial scenarios, the configuration process of a final product is complex and the tool support is usually limited to check functional properties interdependencies. In addition, the importance of nonfunctional properties as relevant drivers during configuration has been overlooked. Thus, there is a lack of holistic paradigms integrating recommendation systems and visualizations that can help the decision makers. In this paper, we propose and evaluate an interrelated set of visualizations for the configuration process filling these gaps. We integrate them as part of the FeatureIDE tool and we evaluate its effectiveness, scalability, and performance.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2058–2065},
numpages = {8},
keywords = {configuration, feature model, recommendation systems, software product lines, visualization},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1109/SPLC.2011.20,
author = {Siegmund, Norbert and Rosenmuller, Marko and Kastner, Christian and Giarrusso, Paolo G. and Apel, Sven and Kolesnikov, Sergiy S.},
title = {Scalable Prediction of Non-functional Properties in Software Product Lines},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.20},
doi = {10.1109/SPLC.2011.20},
abstract = {A software product line is a family of related software products, typically, generated from a set of common assets. Users can select features to derive a product that fulfills their needs. Often, users expect a product to have specific non-functional properties, such as a small footprint or a minimum response time. Because a product line can contain millions of products, it is usually not feasible to generate and measure non-functional properties for each possible product of a product line. Hence, we propose an approach to predict a product's non-functional properties, based on the product's feature selection. To this end, we generate and measure a small set of products, and by comparing the measurements, we approximate each feature's non-functional properties. By aggregating the approximations of selected features, we predict the product's properties. Our technique is independent of the implementation approach and language. We show how already little domain knowledge can improve predictions and discuss trade-offs regarding accuracy and the required number of measurements. Although our approach is in general applicable for quantifiable non-functional properties, we evaluate it for the non-functional property footprint. With nine case studies, we demonstrate that our approach usually predicts the footprint with an accuracy of 98% and an accuracy of over 99% if feature interactions are known.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {160–169},
numpages = {10},
keywords = {SPL Conqueror, measurement, non-functional properties, predicition, software product lines},
series = {SPLC '11}
}

@inproceedings{10.1145/2499777.2500715,
author = {Ishida, Yuzo},
title = {Scalable variability management for enterprise applications with data model driven development},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500715},
doi = {10.1145/2499777.2500715},
abstract = {Unlike embedded systems, some of enterprise systems are evolved over the decades. The predictability of requirements is a key to success in building reusable assets however it is very hard to predict future business context changes, which are driving factors of requirements. Thus, both functional and context variability must be managed in order to satisfy ever-changing requirements. Scalability does matter for enterprise systems in two aspects. One aspect comes from data volume. Once data become big, it is difficult to maintain performance requirements without de-normalizing database schema. Since database de-normalization is driven by non-functional properties, a model driven approach is not feasible if the model cannot express such properties. Another aspect comes from the unpredictability of future functional requirements. A functional decomposition of enterprise systems usually introduces ever-increasing complexity among systems' interactions due to cross-cutting requirements across functional systems. This paper reflects our empirical studies in data intensive large enterprise systems such as retail and telecommunication industries with industry independent application framework to separate functional and non-functional concerns. Our variability management technique is based on database schema modeling, which can be evolved incrementally in scaling an enterprise system with both data and functional aspects.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {90–93},
numpages = {4},
keywords = {core assets, higher-order simple predicate logic, quality attributes, relational algebra, type theory},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.5220/0007955905140521,
author = {Derras, M. and Deruelle, L. and Douin, J.-M. and Levy, N. and Losavio, F. and Mahamane, R. and Reiner, V.},
title = {Approach for Variability Management of Legal Rights in Human Resources Software Product Lines},
year = {2019},
isbn = {9789897583797},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0007955905140521},
doi = {10.5220/0007955905140521},
abstract = {This work concerns software product lines (SPL); it comes from the experience gained collaborating with Berger-Levrault, a French society leader in Human Resources systems. This enterprise serves many French and European territorial communities. They had a variability problem associated to the differences of applicable legal rights in different countries or territories, and this activity was performed manually at a high cost. On the other hand, functionalities were common and mandatory and did not vary much. The crucial issue in SPL development and practice is to manage the correct selection of variants. However, no standard methods have been developed yet, and industry builds SPL using on-the-market or in-house techniques and methods, aware of the benefits a product line can provide; nevertheless, this development must return the investment, and this is not always the case. In this work an approach to variability management in case of legal rights applicability to different entities is proposed. This architecture-centric and quality-based approach uses a reference architecture that has been built with a bottom-up strategy. Variability is incorporated to the reference architecture at abstract level considering non-functional properties. A production plan to reduce the gap between abstraction and implementation levels is defined.},
booktitle = {Proceedings of the 14th International Conference on Software Technologies},
pages = {514–521},
numpages = {8},
keywords = {Human Resources, Legal Rights., Software Product Lines (SPL), Variability Management},
location = {Prague, Czech Republic},
series = {ICSOFT 2019}
}

@inproceedings{10.1145/3461001.3471147,
author = {Kenner, Andy and May, Richard and Kr\"{u}ger, Jacob and Saake, Gunter and Leich, Thomas},
title = {Safety, security, and configurable software systems: a systematic mapping study},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471147},
doi = {10.1145/3461001.3471147},
abstract = {Safety and security are important properties of any software system, particularly in safety-critical domains, such as embedded, automotive, or cyber-physical systems. Moreover, particularly those domains also employ highly-configurable systems to customize variants, for example, to different customer requirements or regulations. Unfortunately, we are missing an overview understanding of what research has been conducted on the intersection of safety and security with configurable systems. To address this gap, we conducted a systematic mapping study based on an automated search, covering ten years (2011--2020) and 65 relevant (out of 367) publications. We classified each publication based on established security and safety concerns (e.g., CIA triad) as well as the connection to configurable systems (e.g., ensuring security of such a system). In the end, we found that considerably more research has been conducted on safety concerns, but both properties seem under-explored in the context of configurable systems. Moreover, existing research focuses on two directions: Ensuring safety and security properties in product-line engineering; and applying product-line techniques to ensure safety and security properties. Our mapping study provides an overview of the current state-of-the-art as well as open issues, helping practitioners identify existing solutions and researchers define directions for future research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {148–159},
numpages = {12},
keywords = {configurable systems, mapping study, safety, security, software product line engineering},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1016/j.infsof.2012.07.020,
author = {Siegmund, Norbert and Rosenm\"{u}Ller, Marko and K\"{a}Stner, Christian and Giarrusso, Paolo G. and Apel, Sven and Kolesnikov, Sergiy S.},
title = {Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.020},
doi = {10.1016/j.infsof.2012.07.020},
abstract = {Context: A software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product. Objective: Our overall goal is to derive optimal products with respect to non-functional requirements by showing customers which features must be selected. Method: We propose an approach to predict a product's non-functional properties based on the product's feature selection. We aggregate the influence of each selected feature on a non-functional property to predict a product's properties. We generate and measure a small set of products and, by comparing measurements, we approximate each feature's influence on the non-functional property in question. As a research method, we conducted controlled experiments and evaluated prediction accuracy for the non-functional properties footprint and main-memory consumption. But, in principle, our approach is applicable for all quantifiable non-functional properties. Results: With nine software product lines, we demonstrate that our approach predicts the footprint with an average accuracy of 94%, and an accuracy of over 99% on average if feature interactions are known. In a further series of experiments, we predicted main memory consumption of six customizable programs and achieved an accuracy of 89% on average. Conclusion: Our experiments suggest that, with only few measurements, it is possible to accurately predict non-functional properties of products of a product line. Furthermore, we show how already little domain knowledge can improve predictions and discuss trade-offs between accuracy and required number of measurements. With this technique, we provide a basis for many reasoning and product-derivation approaches.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {491–507},
numpages = {17},
keywords = {Measurement, Non-functional properties, Prediction, SPL Conqueror, Software product lines}
}

@inproceedings{10.1145/2499777.2500720,
author = {Nakagawa, Elisa Yumi and Oquendo, Flavio},
title = {Perspectives and challenges of reference architectures in multi software product line},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500720},
doi = {10.1145/2499777.2500720},
abstract = {Multi Software Product Line (MSPL or simply Multi Product Line - MPL) has recently emerged as a new approach to develop software systems, mainly those large, complex ones. This approach can be investigated to the development of Systems-of-Systems (SoS), i.e., a new class of software systems that are resulted by the integration of several operationally independent systems. In another perspective, a special type of architecture that contains knowledge about a specific domain has been increasingly investigated, resulting in the research area of Reference Architecture. In this context, the main motivation of this paper is that, in spite of the positive impact of this type of architecture on reuse and productivity, the use of the knowledge contained in existing reference architectures in order to develop MPL, specially to develop SoS, has not been widely explored yet. In this scenario, the main objective of this paper is to present a set of perspectives and challenges of research to use reference architectures in the context of MPL. For this, we have based on our previous experience as well as the literature of SPL, SoS, and reference architecture. As result, we have observed that reference architectures together with MPL seems to be a quite promising research topic. To conclude, we also intend this paper can identify new required research in this context, contributing to improve reuse and productivity in MPL.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {100–103},
numpages = {4},
keywords = {multi product line, reference architecture, system-of-systems},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2851613.2851977,
author = {Kim, Jin Hyun and Legay, Axel and Traonouez, Louis-Marie and Acher, Mathieu and Kang, Sungwon},
title = {A formal modeling and analysis framework for software product line of preemptive real-time systems},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851977},
doi = {10.1145/2851613.2851977},
abstract = {This paper presents a formal analysis framework to analyze a family of platform products w.r.t. real-time properties. First, we propose an extension of the widely-used feature model, called Property Feature Model (PFM), that distinguishes features and properties explicitly Second, we present formal behavioral models of components of a real-time scheduling unit such that all real-time scheduling units implied by a PFM are automatically composed to be analyzed against the properties given by the PFM. We apply our approach to the verification of the schedulability of a family of scheduling units using the symbolic and statistical model checkers of Uppaal.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1562–1565},
numpages = {4},
keywords = {model checking, platform-constrained, scheduling systems, software product line engineering},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3307630.3342384,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {MetricHaven: More than 23,000 Metrics for Measuring Quality Attributes of Software Product Lines},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342384},
doi = {10.1145/3307630.3342384},
abstract = {Variability-aware metrics are designed to measure qualitative aspects of software product lines. As we identified in a prior SLR [6], there exist already many metrics that address code or variability separately, while the combination of both has been less researched. MetricHaven fills this gap, as it extensively supports combining information from code files and variability models. Further, we also enable the combination of well established single system metrics with novel variability-aware metrics, going beyond existing variability-aware metrics. Our tool supports most prominent single system and variability-aware code metrics. We provide configuration support for already implemented metrics, resulting in 23,342 metric variations. Further, we present an abstract syntax tree developed for MetricHaven, that allows the realization of additional code metrics.Tool: https://github.com/KernelHaven/MetricHavenVideo: https://youtu.be/vPEmD5Sr6gM},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {25–28},
numpages = {4},
keywords = {SPL, feature models, implementation, metrics, software product lines, variability models},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2364412.2364459,
author = {Lee, Hyesun and Yang, Jin-seok and Kang, Kyo C.},
title = {VULCAN: architecture-model-based workbench for product line engineering},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364459},
doi = {10.1145/2364412.2364459},
abstract = {Adaptability and reusability are important quality attributes for software targeted for global market due to diverse market needs, ever increasing number of features, rapidly changing technologies, and various laws/standards of different countries. In response to these requirements, software development organizations are interested in product line engineering and searching for support tools. However, most of the existing tools for supporting product line engineering focus only on providing mechanisms for instantiating products without adequately supporting development of software assets that are adaptable and reusable.To address this problem, we provide a CASE tool, called VULCAN, that provides architecture models/patterns that are adaptable/reusable and also supports mechanisms for instantiating products from assets. We have applied VULCAN to various product lines including glucose management systems and elevator control systems, and we could experience that maintainability of the assets has improved substantially because a large portion of the assets are specifications rather than low-level code and product-specific code is generated from the specifications.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {260–264},
numpages = {5},
keywords = {architecture-model-based, feature-oriented, software product line},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1109/SEAA.2014.48,
author = {Soares, Larissa Rocha and Potena, Pasqualina and Machado, Ivan do Carmo and Crnkovic, Ivica and Almeida, Eduardo Santana de},
title = {Analysis of Non-functional Properties in Software Product Lines: A Systematic Review},
year = {2014},
isbn = {9781479957958},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SEAA.2014.48},
doi = {10.1109/SEAA.2014.48},
abstract = {Software Product Lines (SPL) approach has been widely developed in academia and successfully applied in industry. Based on the selection of features, stakeholders can efficiently derive tailor-made programs satisfying different requirements. While SPL was very successful at building products based on identified features, achievements and preservation of many nonfunctional properties (NFPs) remain challenging. A knowledge how to deal with NFPs is still not fully obtained. In this paper, we present a systematic literature review of NFPs analysis for SPL products, focusing on runtime NFPs. The goal of the paper is twofold: (i) to present an holistic overview of SPL approaches that have been reported regarding the analysis of runtime NFPs, and (ii) to categorize NFPs treated in the scientific literature regarding development of SPLs. We analyzed 36 research papers, and identified that system performance attributes are typically the most considered. The results also aid future research studies in NFPs analysis by providing an unbiased view of the body of empirical evidence and by guiding future research directions.},
booktitle = {Proceedings of the 2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications},
pages = {328–335},
numpages = {8},
keywords = {Non-functional Properties, Product Derivation, Software Product Lines, Systematic Literature Review},
series = {SEAA '14}
}

@article{10.4018/ijkss.2014100102,
author = {Tian, Kun},
title = {Adding More Agility to Software Product Line Methods: A Feasibility Study on Its Customization Using Agile Practices},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100102},
doi = {10.4018/ijkss.2014100102},
abstract = {Software Product Line Methods SPLMs have been continuously gaining attention, especially in practice, for on one hand, they address diverse market needs while controlling costs by planned systematic reuse in core assets development domain engineering, and on another hand, they reduce products' time-to-market, achieving a certain level of agility in product development application engineering. More cost-effective and agile as they are than traditional development methods for producing families of similar products, SPLMs still seem to be heavy weight in nature. In SPLMs, significant up-front commitments are involved in development of a flexible product platform, which will be modified into a range of products sharing common features. Agile Methods AMs share similar goals with SPLMs, e.g., on rapidly delivering high quality software that meets the changing needs of stakeholders. However, they appear to differ significantly practices. The purpose of this work is to compare Agile and Software Product line approaches from fundamental goals/principles, engineering, software quality assurance, sand project management perspectives, etc. The results of the study can be used to determine the feasibility of tailoring a software product line approach with Agile practices, resulting in a lighter-weight approach that provides mass customization, reduced time-to-market, and improved customer satisfaction.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {17–34},
numpages = {18},
keywords = {Agile Methods AMs, Customer Satisfaction, Software Product Line Methods SPLMs, Time-To-Market}
}

@inproceedings{10.1145/2695664.2695797,
author = {Tizzei, Leonardo P. and Azevedo, Leonardo G. and de Bayser, Maximilien and Cerqueira, Renato F. G.},
title = {Architecting cloud tools using software product line techniques: an exploratory study},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695797},
doi = {10.1145/2695664.2695797},
abstract = {Multitenant cloud computing tools are usually complex and have to manage variabilities to support customization. Software Product Line (SPL) techniques have been successfully applied in the industry to manage variability in complex systems. However, few works in the literature discuss the application of SPL techniques to architect industry cloud computing tools, resulting in a lack of support to cloud architects on how to apply such techniques. This work presents how software product line techniques can be applied for architecting cloud tools, and discusses the benefits, drawbacks, and some challenges of applying such techniques to develop a real industry cloud tool, named as Installation Service.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1441–1448},
numpages = {8},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/2797433.2797456,
author = {Galster, Matthias},
title = {Architecting for Variability in Quality Attributes of Software Systems},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797456},
doi = {10.1145/2797433.2797456},
abstract = {Variability in software systems is usually concerned with variability in features and functionality. However, variability also occurs in quality attributes (e.g., performance, security) and quality attribute requirements (for example, a performance requirement may state that a system must respond to a user request within 0.1 seconds). We discuss what variability in quality attributes is, including several scenarios in which variability in quality attributes can occur. We then discuss the state of research and what we know about variability in quality attributes, including some existing research to address the challenge of identifying, implementing and managing variability in quality attributes. Finally, we discuss potential directions for future research.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {23},
numpages = {4},
keywords = {Variability, quality attributes, software architecture},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@article{10.1016/j.infsof.2012.09.007,
author = {Guana, Victor and Correal, Dario},
title = {Improving software product line configuration: A quality attribute-driven approach},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.09.007},
doi = {10.1016/j.infsof.2012.09.007},
abstract = {Context: During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product's intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task. Objective: Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets. Method: We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product's functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years. Results: Through our SPL experiment, we established that our approach improved in nearly 98% the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools. Conclusion: We can conclude that our domain-specific modeling approach significantly improves the software architect's decision making when selecting the most suitable combinations of reusable components in the context of a SPL.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {541–562},
numpages = {22},
keywords = {Domain specific modeling, Model driven - software product lines, Quality evaluation, Sensitivity points, Software architecture, Variability management}
}

@inproceedings{10.1145/1868688.1868690,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Apel, Sven},
title = {Automating energy optimization with features},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868690},
doi = {10.1145/1868688.1868690},
abstract = {Mobile devices such as cell phones and notebooks rely on battery power supply. For these systems, optimizing the power consumption is important to increase the system's lifetime. However, this is hard to achieve because energy-saving functions often depend on the hardware, and operating systems. The diversity of hardware components and operating systems makes the implementation time consuming and difficult. We propose an approach to automate energy optimization of programs by implementing energy-saving functionality as modular, separate implementation units (e.g., feature modules or aspects). These units are bundled as energy features into an energy-optimization feature library. Based on aspect-oriented and feature-oriented programming, we discuss different techniques to compose the source code of a client program and the implementation units of the energy features.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {2–9},
numpages = {8},
keywords = {energy consumption, feature-oriented programming, software product lines},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@article{10.1016/j.jss.2017.01.026,
author = {Arvanitou, Elvira Maria and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander and Galster, Matthias and Avgeriou, Paris},
title = {A mapping study on design-time quality attributes and metrics},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {127},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.01.026},
doi = {10.1016/j.jss.2017.01.026},
abstract = {Support to the quality attribute (QA) &amp; metric selection process.Maintainability is the most studied QA for most domains and development phases.Quality attributes are usually assessed through a correlation to a single metric.Metrics are validated in empirical settings and may lack theoretical validity. Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently examined high-level quality attribute, regardless of the application domain or the development phase, (c) assessment of quality attributes is usually performed by a single metric, rather than a combination of multiple metrics, and (d) metrics are mostly validated in an empirical setting. These outcomes are interpreted and discussed based on related work, offering useful implications to both researchers and practitioners.},
journal = {J. Syst. Softw.},
month = may,
pages = {52–77},
numpages = {26},
keywords = {Design-time quality attributes, Mapping study, Measurement, Software quality}
}

@inproceedings{10.1145/2851613.2851964,
author = {Cool, Benjamin and Knieke, Christoph and Rausch, Andreas and Schindler, Mirco and Strasser, Arthur and Vogel, Martin and Brox, Oliver and Jauns-Seyfried, Stefanie},
title = {From product architectures to a managed automotive software product line architecture},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851964},
doi = {10.1145/2851613.2851964},
abstract = {To keep the software development for vehicles cost efficient, software components are reused for different variants as well as for succeeding generations. Furthermore, cost reductions are achieved by software sharing between the Original Equipment Manufacturer (OEM) and the suppliers. However, as a consequence of the blackboxed view caused by software sharing, no common detailed software product line architecture specification for the Electronic Control Unit (ECU) software exists, as it would be required for analyzing the quality of the product line architecture, planning changes on the product line architecture, checking the compliance between the product architecture and the product line architecture, and therefore, avoiding architecture erosion. Thus, after several product generations, software erosion is growing steadily, resulting in an increasing effort of reusing software components, and planning of further development. Here, we propose an approach for repairing an eroded software consisting of a set of product architectures by applying strategies for recovery and discovery of the product line architecture. Furthermore, we give a methodology for a long-term manageable, plannable, and reuseable software product line architecture for automotive software systems.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1350–1353},
numpages = {4},
keywords = {architecture evolution, architecture quality measures, automotive, software erosion, software product lines},
location = {Pisa, Italy},
series = {SAC '16}
}

@article{10.1007/s00766-013-0165-8,
author = {Bagheri, Ebrahim and Ensan, Faezeh},
title = {Dynamic decision models for staged software product line configuration},
year = {2014},
issue_date = {June      2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {2},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0165-8},
doi = {10.1007/s00766-013-0165-8},
abstract = {Software product line engineering practices offer desirable characteristics such as rapid product development, reduced time-to-market, and more affordable development costs as a result of systematic representation of the variabilities of a domain of discourse that leads to methodical reuse of software assets. The development lifecycle of a product line consists of two main phases: domain engineering, which deals with the understanding and formally modeling of the target domain, and application engineering that is concerned with the configuration of a product line into one concrete product based on the preferences and requirements of the stakeholders. The work presented in this paper focuses on the application engineering phase and builds both the theoretical and technological tools to assist the stakeholders in (a) understanding the complex interactions of the features of a product line; (b) eliciting the utility of each feature for the stakeholders and hence exposing the stakeholders' otherwise implicit preferences in a way that they can more easily make decisions; and (c) dynamically building a decision model through interaction with the stakeholders and by considering the structural characteristics of software product line feature models, which will guide the stakeholders through the product configuration process. Initial exploratory empirical experiments that we have performed show that our proposed approach for helping stakeholders understand their feature preferences and its associated staged feature model configuration process is able to positively impact the quality of the end results of the application engineering process within the context of the limited number of participants. In addition, it has been observed that the offered tooling support is able to ease the staged feature model configuration process.},
journal = {Requir. Eng.},
month = jun,
pages = {187–212},
numpages = {26},
keywords = {Feature models, Software product lines, Stakeholder preferences, Utility elicitation}
}

@inproceedings{10.1109/SEAA.2013.20,
author = {Horcas, Jos\'{e} Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Variability and Dependency Modeling of Quality Attributes},
year = {2013},
isbn = {9780769550916},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SEAA.2013.20},
doi = {10.1109/SEAA.2013.20},
abstract = {Functional Quality Attributes (FQAs) are quality attributes that have strong functional implications and so can be easily modeled by software components. Thus, we use an aspect-oriented software product line approach, to model the commonalities and variabilities of FQAs from the early stages of the software development. However, FQAs cannot be modeled in isolation since they usually have dependencies and interactions between them. In this paper we focus on identifying and modeling the dependencies among different FQAs. These dependencies are automatically incorporated into the final software architecture of the system under development, even when the software architect may be unaware of them.},
booktitle = {Proceedings of the 2013 39th Euromicro Conference on Software Engineering and Advanced Applications},
pages = {185–188},
numpages = {4},
keywords = {AO-ADL, dependencies, feature models, quality attributes},
series = {SEAA '13}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {Software engineering, computer-aided software engineering, software variability}
}

@inproceedings{10.1145/2993412.3004847,
author = {Anvaari, Mohsen and S\o{}rensen, Carl-Fredrik and Zimmermann, Olaf},
title = {Associating architectural issues with quality attributes: a survey on expert agreement},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3004847},
doi = {10.1145/2993412.3004847},
abstract = {The architectural decision-making process is a complex and crucial endeavor in companies that develop large and distributed software systems. In this process, choosing and evaluating a solution for each architectural issue depends on decision drivers. The drivers are mainly the business factors (e.g., cost, time-to-market, etc.) and software quality attributes (e.g., security, adaptability, etc.). This paper examines whether there is agreement among experts in associating (i.e., relating) architectural issues with relevant quality attributes. We conducted a survey with 37 experts from several industrial domains who, at least once a month, make one or more architectural decisions. The results show there is poor agreement among these experts in identifying and scoring relevant quality attributes for each architectural issue. Poor agreement implies that the associating task is subjective, and that experts inconsistently define and interpret the relevance of various quality attributes for a given architectural issue that may hurt the sustainability of their architectural decisions. This paper suggests that practitioners in their decision-making should employ approaches that are more systematic. The approaches should be supported by methods and tools designed to diminish the biases of intuitive, experience-based approaches of associating architectural issues with quality attributes.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {11},
numpages = {7},
keywords = {architectural decision, architectural synthesis, inter-rater agreement, software quality attribute, survey},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Controlled experimentation, Feature model, Maintainability, Quality attributes, Software prediction model, Software product line, Structural complexity}
}

@inproceedings{10.1145/2896982.2896988,
author = {Cu, Cuong and Zheng, Yongjie},
title = {Architecture-centric derivation of products in a software product line},
year = {2016},
isbn = {9781450341646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896982.2896988},
doi = {10.1145/2896982.2896988},
abstract = {It is essential to architecture-centric product line development that product line architecture can be used to drive activities specific to product line development, such as product derivation. This requires a mechanism that can automatically derive the architecture and code of a product instance from the customization of product line architecture. In this paper, we analyze the insufficiency of two existing solutions in this area and present an architecture-centric approach that meets the requirement. The approach can support product line differences in platforms and functions, and generate both product line code and product code. It is based on a product line implementation mechanism that combines a code generation and separation pattern with an architecture-based code annotation technique. We have implemented the approach, and finished a preliminary evaluation with a chat application.},
booktitle = {Proceedings of the 8th International Workshop on Modeling in Software Engineering},
pages = {27–33},
numpages = {7},
keywords = {architecture-centric development, product line architecture, software architecture},
location = {Austin, Texas},
series = {MiSE '16}
}

@article{10.1016/j.infsof.2012.07.017,
author = {Ghezzi, Carlo and Molzam Sharifloo, Amir},
title = {Model-based verification of quantitative non-functional properties for software product lines},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.017},
doi = {10.1016/j.infsof.2012.07.017},
abstract = {Evaluating quality attributes of a design model in the early stages of development can significantly reduce the cost and risks of developing a low quality product. To make this possible, software designers should be able to predict quality attributes by reasoning on a model of the system under development. Although there exists a variety of quality-driven analysis techniques for software systems, only a few work address software product lines. This paper describes how probabilistic model checking techniques and tools can be used to verify non-functional properties of different configurations of a software product line. We propose a model-based approach that enables software engineers to assess their design solutions for software product lines in the early stages of development. Furthermore, we discuss how the analysis time can be surprisingly reduced by applying parametric model checking instead of classic model checking. The results show that the parametric approach is able to substantially alleviate the verification time and effort required to analyze non-functional properties of software product lines.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {508–524},
numpages = {17},
keywords = {Non-functional requirements, Parametric verification, Probabilistic model checking, Quality analysis, Software product lines}
}

@inproceedings{10.1109/SPLC.2011.33,
author = {Ghezzi, Carlo and Sharifloo, Amir Molzam},
title = {Verifying Non-functional Properties of Software Product Lines: Towards an Efficient Approach Using Parametric Model Checking},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.33},
doi = {10.1109/SPLC.2011.33},
abstract = {In this paper, we describe how probabilistic model checking techniques and tools can be used to verify non-functional properties of different configurations of a software product line. We propose a model-based approach that enables software engineers to assess their design solutions in the early stages of development. Furthermore, we discuss how verification time can surprisingly be reduced by applying parametric model checking instead of classic model checking, and show that the approach can be effective in practice.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {170–174},
numpages = {5},
keywords = {Non-Functional Requirements, Probabilistic Model Checking, Software Product Lines},
series = {SPLC '11}
}

@inproceedings{10.1109/APSEC.2010.26,
author = {Sincero, Julio and Schroder-Preikschat, Wolfgang and Spinczyk, Olaf},
title = {Approaching Non-functional Properties of Software Product Lines: Learning from Products},
year = {2010},
isbn = {9780769542669},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2010.26},
doi = {10.1109/APSEC.2010.26},
abstract = {Approaching the configuration of non-functional properties (NFPs) in traditional software systems is not an easy task, addressing the configuration of these properties in software product lines (SPLs) imposes even further challenges. Therefore, we have devised the Feedback Approach, which extends the traditional SPL development techniques in order to improve the configuration of NFPs. In this work we present the general guidelines of our approach and also we show the feasibility of the idea by presenting a case study using the Linux Kernel.},
booktitle = {Proceedings of the 2010 Asia Pacific Software Engineering Conference},
pages = {147–155},
numpages = {9},
keywords = {Linux, Non-Functional Properties, Software Product Lines},
series = {APSEC '10}
}

@article{10.1016/j.infsof.2015.09.004,
author = {Souza Neto, Pl\'{a}cido A. and Vargas-Solar, Genoveva and da Costa, Umberto Souza and Musicante, Martin A.},
title = {Designing service-based applications in the presence of non-functional properties},
year = {2016},
issue_date = {January 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {69},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.09.004},
doi = {10.1016/j.infsof.2015.09.004},
abstract = {ContextThe development of distributed software systems has become an important problem for the software engineering community. Service-based applications are a common solution for this kind of systems. Services provide a uniform mechanism for discovering, integrating and using these resources. In the development of service based applications not only the functionality of services and compositions should be considered, but also conditions in which the system operates. These conditions are called non-functional requirements (NFR). The conformance of applications to NFR is crucial to deliver software that meets the expectations of its users. ObjectiveThis paper presents the results of a systematic mapping carried out to analyze how NFR have been addressed in the development of service-based applications in the last years, according to different points of view. MethodOur analysis applies the systematic mapping approach. It focuses on the analysis of publications organized by categories called facets, which are combined to answer specific research questions. The facets compose a classification schema which is part of the contribution and results. ResultsThis paper presents our findings on how NFR have been supported in the development of service-based applications by proposing a classification scheme consisting in five facets: (i) programming paradigm (object/service oriented); (ii) contribution (methodology, system, middleware); (iii) software process phase; (iv) technique or mathematical model used for expressing NFR; and (v) the types of NFR addressed by the papers, based on the classification proposed by the ISO/IEC 9126 specification. The results of our systematic mapping are presented as bubble charts that provide a quantitative analysis to show the frequencies of publications for each facet. The paper also proposes a qualitative analysis based on these plots. This analysis discusses how NFR (quality properties) have been addressed in the design and development of service-based applications, including methodologies, languages and tools devised to support different phases of the software process. ConclusionThis systematic mapping showed that NFR are not fully considered in all software engineering phases for building service based applications. The study also let us conclude that work has been done for providing models and languages for expressing NFR and associated middleware for enforcing them at run time. An important finding is that NFR are not fully considered along all software engineering phases and this opens room for proposing methodologies that fully model NFR. The data collected by our work and used for this systematic mapping are available in https://github.com/placidoneto/systematic-mapping_service-based-app_nfr.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {84–105},
numpages = {22},
keywords = {Non-functional requirements, Service-based software process, Systematic mapping}
}

@inproceedings{10.1007/978-3-662-43652-3_34,
author = {Zulkoski, Ed and Kleynhans, Chris and Yee, Ming-Ho and Rayside, Derek and Czarnecki, Krzysztof},
title = {Optimizing Alloy for Multi-objective Software Product Line Configuration},
year = {2014},
isbn = {9783662436516},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-43652-3_34},
doi = {10.1007/978-3-662-43652-3_34},
abstract = {Software product line SPL engineering involves the modeling, analysis, and configuration of variability-rich systems. We improve the performance of the multi-objective optimization of SPLs in Alloy by several orders of magnitude with two techniques.First, we rewrite the model to remove binary relations that map to integers, which enables removing most of the integer atoms from the universe. SPL models often require using large bitwidths, hence the number of integer atoms in the universe can be orders of magnitude more than the other atoms. In our approach, the tuples for these integer-valued relations are computed outside the sat solver before returning the solution to the user. Second, we add a checkpointing facility to Kodkod, which allows the multi-objective optimization algorithm to reuse previously computed internal sat solver state, after backtracking.Together these result in orders of magnitude improvement in using Alloy as a multi-objective optimization tool for software product lines.},
booktitle = {Proceedings of the 4th International Conference on Abstract State Machines, Alloy, B, TLA, VDM, and Z - Volume 8477},
pages = {328–333},
numpages = {6},
keywords = {Alloy, Kodkod, Multi-objective Optimization, Product Lines},
location = {Toulouse, France},
series = {ABZ 2014}
}

@inproceedings{10.5555/1753235.1753258,
author = {Ganesan, Dharmalingam and Lindvall, Mikael and Ackermann, Chris and McComas, David and Bartholomew, Maureen},
title = {Verifying architectural design rules of the flight software product line},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {This paper presents experiences of verifying architectural design rules of the NASA Core Flight Software (CFS) product line implementation. The goal is to check whether the implementation is consistent with the CFS' architectural rules derived from the developer's guide. The results indicate that consistency checking helps a) identifying architecturally significant deviations that were eluded during code reviews, b) clarifying the design rules to the team, and c) assessing the overall implementation quality. Furthermore, it helps connecting business goals to architectural principles, and to the implementation. This paper is the first step in the definition of a method for analyzing and evaluating product line implementations from an architecture-centric perspective.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {161–170},
numpages = {10},
keywords = {architectural rules, business goals, flight software, implemented architecture},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.scico.2012.05.003,
author = {Laguna, Miguel A. and Crespo, Yania},
title = {A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {8},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.05.003},
doi = {10.1016/j.scico.2012.05.003},
abstract = {Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities.},
journal = {Sci. Comput. Program.},
month = aug,
pages = {1010–1034},
numpages = {25},
keywords = {Evolution, Legacy system, Reengineering, Refactoring, Software product line}
}

@article{10.1016/j.advengsoft.2014.01.011,
author = {Rossel, Pedro O. and Bastarrica, Mar\'{\i}a Cecilia and Hitschfeld-Kahler, Nancy and D\'{\i}az, Violeta and Medina, Mario},
title = {Domain modeling as a basis for building a meshing tool software product line},
year = {2014},
issue_date = {April, 2014},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {70},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2014.01.011},
doi = {10.1016/j.advengsoft.2014.01.011},
abstract = {Meshing tools are highly complex software for generating and managing geometrical discretizations. Due to their complexity, they have generally been developed by end users - physicists, forest engineers, mechanical engineers - with ad hoc methodologies and not by applying well established software engineering practices. Different meshing tools have been developed over the years, making them a good application domain for Software Product Lines (SPLs). This paper proposes building a domain model that captures the different domain characteristics such as features, goals, scenarios and a lexicon, and the relationships among them. The model is partly specified using a formal language. The domain model captures product commonalities and variabilities as well as the particular characteristics of different SPL products. The paper presents a rigorous process for building the domain model, where specific roles, activities and artifacts are identified. This process also clearly establishes consistency and completeness conditions. The usefulness of the model and the process are validated by using them to generate a software product line of Tree Stem Deformation (TSD) meshing tools. We also present Meshing Tool Generator, a software that follows the SPL approach for generating meshing tools belonging to the TSD SPL. We show how an end user can easily generate three different TSD meshing tools using Meshing Tool Generator.},
journal = {Adv. Eng. Softw.},
month = apr,
pages = {77–89},
numpages = {13},
keywords = {Code generator, Domain analysis, Domain model, Meshing tools, Software product line, Tree stem deformation}
}

@article{10.1016/j.infsof.2007.10.013,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {The software product line architecture: An empirical investigation of key process activities},
year = {2008},
issue_date = {October, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {50},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.10.013},
doi = {10.1016/j.infsof.2007.10.013},
abstract = {Software architecture has been a key area of concern in software industry due to its profound impact on the productivity and quality of software products. This is even more crucial in case of software product line, because it deals with the development of a line of products sharing common architecture and having controlled variability. The main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting a comprehensive empirical investigation covering a broad range of organizations currently involved in the business of software product lines. This is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge. The results of this investigation provide empirical evidence that software product line architecture process activities play a significant role in successfully developing and managing a software product line.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1098–1113},
numpages = {16},
keywords = {Domain engineering, Empirical study, Software architecture, Software engineering, Software product line}
}

@article{10.1016/j.infsof.2011.01.001,
author = {Peng, Xin and Yu, Yijun and Zhao, Wenyun},
title = {Analyzing evolution of variability in a software product line: From contexts and requirements to features},
year = {2011},
issue_date = {July, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.01.001},
doi = {10.1016/j.infsof.2011.01.001},
abstract = {Context: In the long run, features of a software product line (SPL) evolve with respect to changes in stakeholder requirements and system contexts. Neither domain engineering nor requirements engineering handles such co-evolution of requirements and contexts explicitly, making it especially hard to reason about the impact of co-changes in complex scenarios. Objective: In this paper, we propose a problem-oriented and value-based analysis method for variability evolution analysis. The method takes into account both kinds of changes (requirements and contexts) during the life of an evolving software product line. Method: The proposed method extends the core requirements engineering ontology with the notions to represent variability-intensive problem decomposition and evolution. On the basis of problemorientation, the analysis method identifies candidate changes, detects influenced features, and evaluates their contributions to the value of the SPL. Results and Conclusion: The process of applying the analysis method is illustrated using a concrete case study of an evolving enterprise software system, which has confirmed that tracing back to requirements and contextual changes is an effective way to understand the evolution of variability in the software product line.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {707–721},
numpages = {15},
keywords = {Context, Evolution, Feature, Requirements, Software product line, Variability}
}

@article{10.1007/s10270-015-0471-3,
author = {Bonif\'{a}cio, Rodrigo and Borba, Paulo and Ferraz, Cristiano and Accioly, Paola},
title = {Empirical assessment of two approaches for specifying software product line use case scenarios},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0471-3},
doi = {10.1007/s10270-015-0471-3},
abstract = {Modularity benefits, including the independent maintenance and comprehension of individual modules, have been widely advocated. However, empirical assessments to investigate those benefits have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). In this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the benefits of modularity in the context of two approaches for specifying product line use case scenarios: PLUSS and MSVCM. The first uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario specifications. After evaluating these approaches through the specifications of several systems, we find out that MSVCM reduces feature scattering and improves scenario cohesion. These results suggest that evolving a product line specification using MSVCM requires only localized changes. On the other hand, the results of six experiments reveal that MSVCM requires more time to derive the product line specifications and, contrasting with the modularity results, reduces the time to evolve a product line specification only when the subjects have been well trained and are used to the task of evolving product line specifications.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {97–123},
numpages = {27},
keywords = {Experimentation in software engineering, Requirements engineering, Software modularity, Software product lines, Usage scenarios}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1109/ICIS.2012.43,
author = {Ryu, Duksan and Lee, Dan and Baik, Jongmoon},
title = {Designing an Architecture of SNS Platform by Applying a Product Line Engineering Approach},
year = {2012},
isbn = {9780769546940},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICIS.2012.43},
doi = {10.1109/ICIS.2012.43},
abstract = {The demand of new Social Networking Service (SNS) is high because the SNSs have been popular these days. In order to deliver various SNSs as early as possible, software product line (SPL) approach can be useful. By using the state of the practices of SPL, this paper shows how to manage commonalities and variabilities of SNS. Specifically, to make an architecture design, presented practices include: understanding relevant domains, requirements engineering, architecture definition. The strengths and weaknesses of Face book architecture are evaluated with the Architecture Tradeoff Analysis Method (ATAM). As a result of applying a framework for SPL practice, layered view and component-based view are illustrated along with variabilities represented by Product Line UML-based Software Engineering (PLUS) and Orthogonal Variability Model (OVM). Based on the analysis of requirements of SNS, additional services such as file sharing and instant messaging are represented as optional components. In case of Face book, three key quality attributes, i.e., availability, scalability, and privacy are analyzed by using quality attribute utility tree. We identified that Face book employs client-server architecture. Through ATAM, Peer-to-Peer (P2P) approach promoting privacy is explained.},
booktitle = {Proceedings of the 2012 IEEE/ACIS 11th International Conference on Computer and Information Science},
pages = {559–564},
numpages = {6},
keywords = {architecture design, architecture evaluation, social networking service, software product line},
series = {ICIS '12}
}

@inproceedings{10.1145/2602458.2602460,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Injecting quality attributes into software architectures with the common variability language},
year = {2014},
isbn = {9781450325776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602458.2602460},
doi = {10.1145/2602458.2602460},
abstract = {Quality attributes that add new behavior to the functional software architecture are known as functional quality attributes (FQAs). These FQAs are applied to pieces of software from small components to entire systems, usually crosscutting some of them. Due to this crosscutting nature, modeling them separately from the base application has many advantages (e.g. reusability, less coupled architectures). However, different applications may require different configurations of an FQA (e.g. different levels of security), so we need a language that: (i) easily expresses the variability of the FQAs at the architectural level; and that (ii) also facilitates the automatic generation of architectural configurations with custom-made FQAs. In this sense, the Common Variability Language (CVL) is extremely suited for use at the architectural level, not requiring the use of a particular architectural language to model base functional requirements. In this paper we propose a method based on CVL to: (i) model separately and generate FQAs customized to the application requirements; (ii) automatically inject customized FQA components into the architecture of the applications. We quantitatively evaluate our approach and discuss its benefits with a case study.},
booktitle = {Proceedings of the 17th International ACM Sigsoft Symposium on Component-Based Software Engineering},
pages = {35–44},
numpages = {10},
keywords = {cvl, quality attributes, spl, variability, weaving},
location = {Marcq-en-Bareul, France},
series = {CBSE '14}
}

@article{10.4018/ijismd.2014070103,
author = {Lotz, Alex and Ingl\'{e}s-Romero, Juan F. and Stampfer, Dennis and Lutz, Matthias and Vicente-Chicote, Cristina and Schlegel, Christian},
title = {Towards a Stepwise Variability Management Process for Complex Systems: A Robotics Perspective},
year = {2014},
issue_date = {July 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {3},
issn = {1947-8186},
url = {https://doi.org/10.4018/ijismd.2014070103},
doi = {10.4018/ijismd.2014070103},
abstract = {Complex systems are executed in environments with a huge number of potential situations and contingencies, therefore a mechanism is required to express dynamic variability at design-time that can be efficiently resolved in the application at run-time based on the then available information. We present an approach for dynamic variability modeling and its exploitation at run-time. It supports different developer roles and allows the separation of two different kinds of dynamic variability at design-time: (i) variability related to the system operation, and (ii) variability associated with QoS. The former provides robustness to contingencies, maintaining a high success rate in task fulfillment. The latter focuses on the quality of the application execution (defined in terms of non-functional properties like safety or task efficiency) under changing situations and limited resources. The authors also discuss different alternatives for the run-time integration of the two variability management mechanisms, and show real-world robotic examples to illustrate them.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = jul,
pages = {55–74},
numpages = {20},
keywords = {Modeling Run-Time Variability, Service Robotics, SmartTCL, VML, Variability Management}
}

@article{10.1016/j.jss.2013.12.038,
author = {Capilla, Rafael and Bosch, Jan and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio and Hinchey, Mike},
title = {An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.12.038},
doi = {10.1016/j.jss.2013.12.038},
abstract = {Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures.},
journal = {J. Syst. Softw.},
month = may,
pages = {3–23},
numpages = {21},
keywords = {Dynamic Software Product Lines, Dynamic variability, Feature models, Software architecture}
}

@inproceedings{10.1145/2304676.2304679,
author = {Klatt, Benjamin and K\"{u}ster, Martin},
title = {Respecting component architecture to migrate product copies to a software product line},
year = {2012},
isbn = {9781450313483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304676.2304679},
doi = {10.1145/2304676.2304679},
abstract = {Software product lines (SPL) are a well-known concept to efficiently develop product variants. However, migrating existing, customised product copies to a product line is still an open issue due to the required comprehension of differences among products and SPL design decisions. Most existing SPL approaches are focused on forward engineering. Only few aim to handle SPL evolution, but even those lack support of variability reverse engineering, which is necessary for migrating product copies to a product line. In this paper, we present how component architecture information can be used to enhance a variabilty reverse engineering process to target this challenge and show the relevance of component architecture in the individual requirements on the resulting SPL. We further provide an illustrating example to show how the concept is applied.},
booktitle = {Proceedings of the 17th International Doctoral Symposium on Components and Architecture},
pages = {7–12},
numpages = {6},
keywords = {component architecture, reverse engineering, software product line},
location = {Bertinoro, Italy},
series = {WCOP '12}
}

@article{10.1007/s11219-011-9156-5,
author = {Roos-Frantz, Fabricia and Benavides, David and Ruiz-Cort\'{e}s, Antonio and Heuer, Andr\'{e} and Lauenroth, Kim},
title = {Quality-aware analysis in product line engineering with the orthogonal variability model},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9156-5},
doi = {10.1007/s11219-011-9156-5},
abstract = {Software product line engineering is about producing a set of similar products in a certain domain. A variability model documents the variability amongst products in a product line. The specification of variability can be extended with quality information, such as measurable quality attributes (e.g., CPU and memory consumption) and constraints on these attributes (e.g., memory consumption should be in a range of values). However, the wrong use of constraints may cause anomalies in the specification which must be detected (e.g., the model could represent no products). Furthermore, based on such quality information, it is possible to carry out quality-aware analyses, i.e., the product line engineer may want to verify whether it is possible to build a product that satisfies a desired quality. The challenge for quality-aware specification and analysis is threefold. First, there should be a way to specify quality information in variability models. Second, it should be possible to detect anomalies in the variability specification associated with quality information. Third, there should be mechanisms to verify the variability model to extract useful information, such as the possibility to build a product that fulfils certain quality conditions (e.g., is there any product that requires less than 512 MB of memory?). In this article, we present an approach for quality-aware analysis in software product lines using the orthogonal variability model (OVM) to represent variability. We propose to map variability represented in the OVM associated with quality information to a constraint satisfaction problem and to use an off-the-shelf constraint programming solver to automatically perform the verification task. To illustrate our approach, we use a product line in the automotive domain which is an example that was created in a national project by a leading car company. We have developed a prototype tool named FaMa-OVM, which works as a proof of concepts. We were able to identify void models, dead and false optional elements, and check whether the product line example satisfies quality conditions.},
journal = {Software Quality Journal},
month = sep,
pages = {519–565},
numpages = {47},
keywords = {Automated analysis, Orthogonal variability model, Quality modelling, Quality-aware analysis, Software product lines}
}

@inproceedings{10.1109/ICWS.2015.20,
author = {Gamez, Nadia and El Haddad, Joyce and Fuentes, Lidia},
title = {SPL-TQSSS: A Software Product Line Approach for Stateful Service Selection},
year = {2015},
isbn = {9781467372725},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICWS.2015.20},
doi = {10.1109/ICWS.2015.20},
abstract = {An important problem in Web services composition process is optimal selection of services meeting the user functional requirements (tasks of a workflow) and ensuring a reliable execution of the composition. Therefore, non-functional properties of services such as their transactional behavior as well as their Quality of Service (QoS) must be considered. In this context, a challenging objective is to assist users in integrating on the fly the operations of services to realize their required tasks by further meeting their transactional and QoS preferences. Towards this purpose, we present SPLTQSSS, a Software Product Line based approach for Stateful (conversation-based) Service Selection problem with Transactional and QoS support. SPL-TQSSS considers the set of functionally-equivalent services as part of a service family by modeling their internal operations using Feature Models. Then, SPL-TQSSS chooses the best services, from the service families matching with every task of the workflow, which fit with the user transactional preference and satisfy QoS constraints.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Web Services},
pages = {73–80},
numpages = {8},
keywords = {Feature Model, QoS, Service Selection, Software Product Line, Transactional, Variability},
series = {ICWS '15}
}

@inproceedings{10.1145/3461001.3461660,
author = {Michelon, Gabriela Karoline and Obermann, David and Assun\c{c}\~{a}o, Wesley K. G. and Linsbauer, Lukas and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Managing systems evolving in space and time: four challenges for maintenance, evolution and composition of variants},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3461660},
doi = {10.1145/3461001.3461660},
abstract = {Software companies need to provide a large set of features satisfying functional and non-functional requirements of diverse customers, thereby leading to variability in space. Feature location techniques have been proposed to support software maintenance and evolution in space. However, so far only one feature location technique also analyses the evolution in time of system variants, which is required for feature enhancements and bug fixing. Specifically, existing tools for managing a set of systems over time do not offer proper support for keeping track of feature revisions, updating existing variants, and creating new product configurations based on feature revisions. This paper presents four challenges concerning such capabilities for feature (revision) location and composition of new product configurations based on feature/s (revisions). We also provide a benchmark containing a ground truth and support for computing metrics. We hope that this will motivate researchers to provide and evaluate tool-supported approaches aiming at managing systems evolving in space and time. Further, we do not limit the evaluation of techniques to only this benchmark: we introduce and provide instructions on how to use a benchmark extractor for generating ground truth data for other systems. We expect that the feature (revision) location techniques maximize information retrieval in terms of precision, recall, and F-score, while keeping execution time and memory consumption low.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {75–80},
numpages = {6},
keywords = {benchmark extractor, feature location, feature revision, repository mining, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2430502.2430511,
author = {Kolesnikov, Sergiy S. and Apel, Sven and Siegmund, Norbert and Sobernig, Stefan and K\"{a}stner, Christian and Senkaya, Semah},
title = {Predicting quality attributes of software product lines using software and network measures and sampling},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430511},
doi = {10.1145/2430502.2430511},
abstract = {Software product-line engineering aims at developing families of related products that share common assets to provide customers with tailor-made products. Customers are often interested not only in particular functionalities (i.e., features), but also in non-functional quality attributes, such as performance, reliability, and footprint. Measuring quality attributes of all products of a product line usually does not scale. In this research-in-progress report, we propose a systematic approach aiming at efficient and scalable prediction of quality attributes of products. To this end, we establish predictors for certain categories of quality attributes (e.g., a predictor for high memory consumption) based on software and network measures, and receiver operating characteristic analysis. We use these predictors to guide a sampling process that takes the assets of a product line as input and determines the products that fall into the category denoted by the given predictor (e.g., products with high memory consumption). We propose to use predictors to make the process of finding "acceptable" products more efficient. We discuss and compare several strategies to incorporate predictors in the sampling process.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {5},
keywords = {metrics, prediction, quality attributes, sampling, software product lines},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.5555/2337223.2337302,
author = {Cordy, Maxime and Classen, Andreas and Perrouin, Gilles and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Simulation-based abstractions for software product-line model checking},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) engineering is a software engineering paradigm that exploits the commonality between similar software products to reduce life cycle costs and time-to-market. Many SPLs are critical and would benefit from efficient verification through model checking. Model checking SPLs is more difficult than for single systems, since the number of different products is potentially huge. In previous work, we introduced Featured Transition Systems (FTS), a formal, compact representation of SPL behaviour, and provided efficient algorithms to verify FTS. Yet, we still face the state explosion problem, like any model checking-based verification. Model abstraction is the most relevant answer to state explosion. In this paper, we define a novel simulation relation for FTS and provide an algorithm to compute it. We extend well-known simulation preservation properties to FTS and thus lay the theoretical foundations for abstraction-based model checking of SPLs. We evaluate our approach by comparing the cost of FTS-based simulation and abstraction with respect to product-by-product methods. Our results show that FTS are a solid foundation for simulation-based model checking of SPL.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {672–682},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1007/978-3-642-12107-4_8,
author = {Alf\'{e}rez, Mauricio and Santos, Jo\~{a}o and Moreira, Ana and Garcia, Alessandro and Kulesza, Uir\'{a} and Ara\'{u}jo, Jo\~{a}o and Amaral, Vasco},
title = {Multi-view composition language for software product line requirements},
year = {2009},
isbn = {3642121063},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12107-4_8},
doi = {10.1007/978-3-642-12107-4_8},
abstract = {Composition of requirements models in Software Product Line (SPL) development enables stakeholders to derive the requirements of target software products and, very important, to reason about them. Given the growing complexity of SPL development and the various stakeholders involved, their requirements are often specified from heterogeneous, partial views. However, existing requirements composition languages are very limited to generate specific requirements views for SPL products. They do not provide specialized composition rules for referencing and composing elements in recurring requirements models, such as use cases and activity models. This paper presents a multi-view composition language for SPL requirements, the Variability Modeling Language for Requirements (VML4RE). This language describes how requirements elements expressed in different models should be composed to generate a specific SPL product. The use of VML4RE is illustrated with UML-based requirements models defined for a home automation SPL case study. The language is evaluated with additional case studies from different application domains, such as mobile phones and sales management.},
booktitle = {Proceedings of the Second International Conference on Software Language Engineering},
pages = {103–122},
numpages = {20},
keywords = {cmposition languages, requirements engineering, requirements reuse, software product lines, variability management},
location = {Denver, CO},
series = {SLE'09}
}

@article{10.1016/j.infsof.2012.08.010,
author = {Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
title = {Variability in quality attributes of service-based software systems: A systematic literature review},
year = {2013},
issue_date = {February, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.08.010},
doi = {10.1016/j.infsof.2012.08.010},
abstract = {Context: Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective: We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method: A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results: Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions: The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {320–343},
numpages = {24},
keywords = {Quality attributes, Service-based systems, Systematic literature review, Variability}
}

@inproceedings{10.1109/SBCARS.2010.13,
author = {Oliveira Junior, Edson A. and Maldonado, Jose C. and Gimenes, Itana M. S.},
title = {Empirical Validation of Complexity and Extensibility Metrics for Software Product Line Architectures},
year = {2010},
isbn = {9780769542591},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SBCARS.2010.13},
doi = {10.1109/SBCARS.2010.13},
abstract = {The software product line (PL) architecture (PLA) is one of the most important PL core assets as it is the abstraction of the products that can be generated, and it represents similarities and variabilities of a PL. Its quality attributes analysis and evaluation can serve as a basis for analyzing the managerial and economical values of a PL. We proposed metrics for PLA complexity and extensibility quality attributes. This paper is concerned with the empirical validation of such metrics. As a result of the experimental work we can conclude that the metrics are relevant indicators of complexity and extensibility of PLA by presenting their correlation analysis.},
booktitle = {Proceedings of the 2010 Fourth Brazilian Symposium on Software Components, Architectures and Reuse},
pages = {31–40},
numpages = {10},
keywords = {complexity, empirical validation, extensibility, metrics, product line architecture, software product line},
series = {SBCARS '10}
}

@inproceedings{10.1145/2031759.2031768,
author = {Lence, Ram\'{o}n and Fuentes, Lidia and Pinto, M\'{o}nica},
title = {Quality attributes and variability in AO-ADL software architectures},
year = {2011},
isbn = {9781450306188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2031759.2031768},
doi = {10.1145/2031759.2031768},
abstract = {The quality attributes of a system are determined, to a large extend, by the decisions taken early on in the development process, noticeably affecting the specification of its software architecture. This is especially true for attributes such as security, usability, context awareness, etc., that have strong functional implications -- i.e. they require the incorporation of Specific functionality to the application architecture in order to satisfy them. Our approach models functional quality attributes considering that: (1) they are complex enough so as to be modeled by a large set of related concerns and the compositions among them. For instance, security includes authentication, access control, privacy, encryption, auditing, etc; (2) the same quality attributes are required by several applications, and thus should be modeled as separate, ready-to-use (re)usable architectural solutions that final applications can incorporate without "being previously prepared" for it; and (3) not all the concerns that are part of a quality attribute need to be instantiated for a particular application (e.g. only the authentication and access control concerns of security are required). In order to consider all the above requirements, in this paper we present a software product line approach that permits modeling the variability of quality attributes using feature models, and generating different configurations of their software architecture depending on the particular concerns required by each application.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture: Companion Volume},
articleno = {7},
numpages = {10},
keywords = {AO-ADL, Hydra, VML, architectural templates, quality attributes, variability},
location = {Essen, Germany},
series = {ECSA '11}
}

@article{10.1007/s10270-020-00839-w,
author = {Pol’la, Matias and Buccella, Agustina and Cechich, Alejandra},
title = {Analysis of variability models: a systematic literature review},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00839-w},
doi = {10.1007/s10270-020-00839-w},
abstract = {Dealing with variability, during Software Product Line Engineering (SPLE), means trying to allow software engineers to develop a set of similar applications based on a manageable range of variable functionalities according to expert users’ needs. Particularly, variability management (VM) is an activity that allows flexibility and a high level of reuse during software development. In the last years, we have witnessed a proliferation of methods, techniques and supporting tools for VM in general, and for its analysis in particular. More precisely, a specific field has emerged, named (automated) variability analysis, focusing on verifying variability models across the SPLE’s phases. In this paper, we introduce a systematic literature review of existing proposals (as primary studies) focused on analyzing variability models. We define a classification framework, which is composed of 20 sub-characteristics addressing general aspects, such as scope and validation, as well as model-specific aspects, such as variability primitives, reasoner type. The framework allows to look at the analysis of variability models during its whole life cycle—from design to derivation—according to the activities involved during an SPL development. Also, the framework helps us answer three research questions defined for showing the state of the art and drawing challenges for the near future. Among the more interesting challenges, we can highlight the needs of more applications in industry, the existence of more mature tools, and the needs of providing more semantics in the way of variability primitives for identifying inconsistencies in the models.},
journal = {Softw. Syst. Model.},
month = aug,
pages = {1043–1077},
numpages = {35},
keywords = {Variability analysis, Software Product Line, Variability management, Supporting tools}
}

@article{10.1016/j.infsof.2010.12.006,
author = {Chen, Lianping and Ali Babar, Muhammad},
title = {A systematic review of evaluation of variability management approaches in software product lines},
year = {2011},
issue_date = {April, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.12.006},
doi = {10.1016/j.infsof.2010.12.006},
abstract = {ContextVariability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated. ObjectiveThe objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches. MethodWe carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007. ResultsWe selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects. ConclusionThe findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {344–362},
numpages = {19},
keywords = {Empirical studies, Software product line, Systematic literature reviews, Variability management}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Feature-oriented software development, Measurement and optimization, Non-functional properties, SPL Conqueror, Software product lines}
}

@inproceedings{10.1145/2245276.2231956,
author = {Horikoshi, Hisayuki and Nakagawa, Hiroyuki and Tahara, Yasuyuki and Ohsuga, Akihiko},
title = {Dynamic reconfiguration in self-adaptive systems considering non-functional properties},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231956},
doi = {10.1145/2245276.2231956},
abstract = {Self-adaptive systems have recently been receiving much attention because of their ability to cope with the changes of environment, failures, and unanticipated events. These systems need an adaptation mechanism, which automatically computes the possible configurations, and decides the most appropriate configuration to fit the environment. In particular, the satisfaction of non-functional requirements must be considered when selecting the best reconfiguration. However, there are trade-off problems among non-functional requirements. Moreover, the adaptation mechanisms are typically developed separately from the components to be implemented, and it complicates the construction of such systems. We propose (1) a feature-oriented analysis technique, which can identify adaptation points, and calculate the contribution to non-functional goals of the configuration; (2) a component specification model, which extends an architectural description language for self-adaptation; (3) a reconfiguration framework aimed to reduce the complexity of the reconfiguration and generate the best configuration at run-time. We evaluate the feasibility of our framework by four different scenarios, and show that our framework reduces the complexity of the reconfiguration, and solves the trade-off problem among non-functional requirements.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1144–1150},
numpages = {7},
keywords = {architecture description language, dynamic reconfiguration, feature-oriented analysis, self-adaptive systems, software architecture},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/2430502.2430516,
author = {Lanceloti, Leandro A. and Maldonado, Jos\'{e} C. and Gimenes, Itana M. S. and Oliveira, Edson A.},
title = {SMartyParser: a XMI parser for UML-based software product line variability models},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430516},
doi = {10.1145/2430502.2430516},
abstract = {Variability management is an important issue for the software-intensive systems domain. Such an issue is essential for the success of software product line (SPL) adoption strategies. Although it is a well-discussed subject in the SPL community, there is a lack of tool support for environments that handle UML-based SPL variabilities, as several variability management approaches take UML as a basis, specially its profiling mechanism. Such environments might handle variabilities for several reasons, such as, evaluating SPLs, defining and applying metrics based on a SPL modeling, and automating the product generation. Therefore, this paper presents the SMartyParser, a parser for processing UML-based SPL models. Such models can be obtained, in the XMI format, from every UML specification-compliant tool. Such a parser provides several services to make it easier the handling of variability data in a particular SPL environment/tool. SMartyParser was built by taking the Open Core framework as a basis for processing XMI files. A parser use example is presented by taking into account the SPL Arcade Game Maker UML models.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {5},
keywords = {UML, XMI, parser, software product line, stereotype, variability management},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1109/SERA.2010.17,
author = {Tanhaei, Mohammad and Moaven, Shahrouz and Habibi, Jafar and Ahmadi, Hamed},
title = {Toward a Business Model for Software Product Line Architecture},
year = {2010},
isbn = {9780769540757},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SERA.2010.17},
doi = {10.1109/SERA.2010.17},
abstract = {Nowadays, software product line is an approach to reduce costs of software development, decrease time to market, and increase capabilities of reuse in designing and exploiting software development processes. Moreover, other quality attributes of the project domain should be considered to enhance quality of the product. Meanwhile, taking advantage of software product line makes developers capable of estimating development costs and time to market in a more realistic way. However, old approaches to estimate cost of development and foresee time to market are not suitable enough for software product line. In this paper, some important business parameters and a way to calculate cost and time to market in a product line are presented. Changing components among time, portion of the change in a specific product and organization issues are observed in the estimation function.},
booktitle = {Proceedings of the 2010 Eighth ACIS International Conference on Software Engineering Research, Management and Applications},
pages = {50–56},
numpages = {7},
series = {SERA '10}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {artificial intelligence, configuration, feature model, planning techniques, software product line engineering},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.5555/1887899.1887951,
author = {Lopez-Herrejon, Roberto E.},
title = {On the need of safe software product line architectures},
year = {2010},
isbn = {3642151132},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A Software Product Line (SPL) is a family of related software systems distinguished by the different sets of features each system provides. Over the last decade, the substantial benefits of SPL practices have been extensively documented and corroborated both in academia and industry. Several architecture methods have been proposed that employ different artifacts for expressing the components of a SPL, their properties and relationships. Of crucial importance for any SPL architecture method is to guarantee that the variability, for instance as expressed in feature models, is not only preserved but also kept consistent across all artifacts used. In this research challenge paper we argue that Safe Composition - the guarantee that all programs of a product line are type safe - can be leveraged to address this guarantee for structural properties of SPL architectures and the challenges that that entails.},
booktitle = {Proceedings of the 4th European Conference on Software Architecture},
pages = {493–496},
numpages = {4},
location = {Copenhagen, Denmark},
series = {ECSA'10}
}

@inproceedings{10.1145/3307630.3342705,
author = {Krieter, Sebastian},
title = {Enabling Efficient Automated Configuration Generation and Management},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342705},
doi = {10.1145/3307630.3342705},
abstract = {Creating and managing valid configurations is one of the main tasks in software product line engineering. Due to the often complex constraints from a feature model, some kind of automated configuration generation is required to facilitate the configuration process for users and developers. For instance, decision propagation can be applied to support users in configuring a product from a software product line (SPL) with less manual effort and error potential, leading to a semi-automatic configuration process. Furthermore, fully-automatic configuration processes, such as random sampling or t-wise interaction sampling can be employed to test or to optimize an SPL. However, current techniques for automated configuration generation still do not scale well to SPLs with large and complex feature models. Within our thesis, we identify current challenges regarding the efficiency and effectiveness of the semi- and fully-automatic configuration process and aim to address these challenges by introducing novel techniques and improving current ones. Our preliminary results show already show promising progress for both, the semi- and fully-automatic configuration process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {215–221},
numpages = {7},
keywords = {configurable system, decision propagation, software product lines, t-wise sampling, uniform random sampling},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s11219-011-9146-7,
author = {Montagud, Sonia and Abrah\~{a}o, Silvia and Insfran, Emilio},
title = {A systematic review of quality attributes and measures for software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9146-7},
doi = {10.1007/s11219-011-9146-7},
abstract = {It is widely accepted that software measures provide an appropriate mechanism for understanding, monitoring, controlling, and predicting the quality of software development projects. In software product lines (SPL), quality is even more important than in a single software product since, owing to systematic reuse, a fault or an inadequate design decision could be propagated to several products in the family. Over the last few years, a great number of quality attributes and measures for assessing the quality of SPL have been reported in literature. However, no studies summarizing the current knowledge about them exist. This paper presents a systematic literature review with the objective of identifying and interpreting all the available studies from 1996 to 2010 that present quality attributes and/or measures for SPL. These attributes and measures have been classified using a set of criteria that includes the life cycle phase in which the measures are applied; the corresponding quality characteristics; their support for specific SPL characteristics (e.g., variability, compositionality); the procedure used to validate the measures, etc. We found 165 measures related to 97 different quality attributes. The results of the review indicated that 92% of the measures evaluate attributes that are related to maintainability. In addition, 67% of the measures are used during the design phase of Domain Engineering, and 56% are applied to evaluate the product line architecture. However, only 25% of them have been empirically validated. In conclusion, the results provide a global vision of the state of the research within this area in order to help researchers in detecting weaknesses, directing research efforts, and identifying new research lines. In particular, there is a need for new measures with which to evaluate both the quality of the artifacts produced during the entire SPL life cycle and other quality characteristics. There is also a need for more validation (both theoretical and empirical) of existing measures. In addition, our results may be useful as a reference guide for practitioners to assist them in the selection or the adaptation of existing measures for evaluating their software product lines.},
journal = {Software Quality Journal},
month = sep,
pages = {425–486},
numpages = {62},
keywords = {Measures, Quality, Quality attributes, Software product lines, Systematic literature review}
}

@article{10.1016/j.infsof.2006.08.008,
author = {Her, Jin Sun and Kim, Ji Hyeok and Oh, Sang Hun and Rhew, Sung Yul and Kim, Soo Dong},
title = {A framework for evaluating reusability of core asset in product line engineering},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.008},
doi = {10.1016/j.infsof.2006.08.008},
abstract = {Product line engineering (PLE) is a new effective approach to software reuse, where applications are generated by instantiating a core asset which is a large-grained reuse unit. Hence, a core asset is a key element of PLE, and therefore the reusability of the core asset largely determines the success of PLE projects. However, current quality models to evaluate reusability do not adequately address the unique characteristics of core assets in PLE. This paper proposes a comprehensive framework for evaluating the reusability of core assets. We first identify the key characteristics of core assets, and derive a set of quality attributes that characterizes the reusability of core assets. Then, we define metrics for each quality attribute and finally present practical guidelines for applying the evaluation framework in PLE projects. Using the proposed framework, the reusability of core assets can be more effectively and precisely evaluated.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {740–760},
numpages = {21},
keywords = {Core asset, Metric, Product line engineering, Quality model, Reusability}
}

@article{10.1007/s10664-015-9414-4,
author = {Mkaouer, Mohamed Wiem and Kessentini, Marouane and Bechikh, Slim and O\'{z} Cinne\'{z}Ide, Mel and Deb, Kalyanmoy},
title = {On the use of many quality attributes for software refactoring: a many-objective search-based software engineering approach},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9414-4},
doi = {10.1007/s10664-015-9414-4},
abstract = {Search-based software engineering (SBSE) solutions are still not scalable enough to handle high-dimensional objectives space. The majority of existing work treats software engineering problems from a single or bi-objective point of view, where the main goal is to maximize or minimize one or two objectives. However, most software engineering problems are naturally complex in which many conflicting objectives need to be optimized. Software refactoring is one of these problems involving finding a compromise between several quality attributes to improve the quality of the system while preserving the behavior. To this end, we propose a novel representation of the refactoring problem as a many-objective one where every quality attribute to improve is considered as an independent objective to be optimized. In our approach based on the recent NSGA-III algorithm, the refactoring solutions are evaluated using a set of 8 distinct objectives. We evaluated this approach on one industrial project and seven open source systems. We compared our findings to: several other many-objective techniques (IBEA, MOEA/D, GrEA, and DBEA-Eps), an existing multi-objective approach a mono-objective technique and an existing refactoring technique not based on heuristic search. Statistical analysis of our experiments over 31 runs shows the efficiency of our approach.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {2503–2545},
numpages = {43},
keywords = {Many-objective optimization, Refactoring, Search-based software engineering, Software quality}
}

@inproceedings{10.1145/3106195.3106212,
author = {Marimuthu, C. and Chandrasekaran, K.},
title = {Systematic Studies in Software Product Lines: A Tertiary Study},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106212},
doi = {10.1145/3106195.3106212},
abstract = {Software product lines are widely used in the software industries to increase the re-usability and to decrease maintenance cost. On the other hand, systematic reviews are widely used in the software engineering research community to provide the overview of the research field and practitioners guidelines. Researchers have conducted many systematic studies on the different aspects of SPLs. To the best of our knowledge, till now there is no tertiary study conducted on systematic studies of SPL related research topics. In this paper, we aim at conducting a systematic mapping study of existing systematic studies to report the overview of the findings for researchers and practitioners. We performed snowballing and automated search to find out the relevant systematic studies. As a result, we analyzed 60 relevant studies to answer 5 research questions. The main focus of this tertiary study is to highlight the research topics, type of published reviews, active researchers and publication forums. Additionally, we highlight some of the limitations of the systematic studies. The important finding of this study is that the research field is well matured as the systematic studies covered a wide range of research topics. Another important finding is that many studies provided information for practitioners as well as researchers which is a notable improvement in the systematic reviews. However, many studies failed to assess the quality of the primary studies which is the major limitation of the existing systematic studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {143–152},
numpages = {10},
keywords = {software product line, systematic review, tertiary study},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3236405.3236426,
author = {Belarbi, Maouaheb},
title = {A methodological framework to enable the generation of code from DSML in SPL},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236426},
doi = {10.1145/3236405.3236426},
abstract = {Software Product Line has acquired a significant momentum at the end of the 1990ies since it allows the production of variable software systems corresponding to the same domain portfolio. The effectiveness of the derivation process depends on how well variability is defined and implemented which is a crucial topic area that was addressed among two essential trends: On the one hand, starting from Domain Specific Modelling Language to express domain requirements and automate the code generation with Model-Driven Engineering techniques and on the second hand, exploiting the soar of variability mechanisms.In this context, the current research presents a method that unifies the two aforementioned approaches to cover the overall strategies by defining a framework that allows a better code generation in terms of documentation, maintainability, rapidity,etc. The starting point is the usage of the Domain Specific Modelling Language to represent the stakeholders requirements. Then, the resulting meta-model will be converted into one our several Feature Diagrams on which variability mechanisms can be applied to generate all the family products.A preliminary experiment has been undertaken to design the methodology of the proposed software factory in a meta-model. The validation task was evaluated with an academic use case called HandiWeb developed to facilitate handicap persons access to the internet. The first results allow us to put the hand on the key challenges that must be resolved by the proposed methodology.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {64–71},
numpages = {8},
keywords = {DSML, SPL, methodology, software factory, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.infsof.2012.04.009,
author = {Engstr\"{o}M, Emelie and Runeson, Per},
title = {Test overlay in an emerging software product line - An industrial case study},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.04.009},
doi = {10.1016/j.infsof.2012.04.009},
abstract = {Context: In large software organizations with a product line development approach, system test planning and scope selection is a complex task. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of redundant testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests. Aims: This study assesses the amount and type of overlaid manual testing across feature, integration and system test in such context, it explores the causes of potential redundancy and elaborates on how to provide decision support in terms of visualization for the purpose of avoiding redundancy. Method: An in-depth case study was launched including both qualitative and quantitative observations. Results: A high degree of test overlay is identified originating from distributed test responsibilities, poor documentation and structure of test cases, parallel work and insufficient delta analysis. The amount of test overlay depends on which level of abstraction is studied. Conclusions: Avoiding redundancy requires tool support, e.g. visualization of test design coverage, test execution progress, priorities of coverage items as well as visualized priorities of variants to support test case selection.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {581–594},
numpages = {14},
keywords = {Case study, Efficiency, Overlay, Product-line, Redundancy, Software testing}
}

@inproceedings{10.1145/3307630.3342416,
author = {Rodriguez, Germania and P\'{e}rez, Jennifer and Benavides, David},
title = {Accessibility Variability Model: The UTPL MOOC Case Study},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342416},
doi = {10.1145/3307630.3342416},
abstract = {Several approaches to define Variability Models (VM) of non-functional requirements or quality attributes have been proposed. However, these approaches have focused on specific quality attributes rather than more general non-functional aspects established by standards such as ISO/IEC 25010 for software evaluation and quality. Thus, developing specific software products by selecting features and at the same time measuring the level of compliance with a standard/guideline is a challenge. In this work, we present the definition of an accessibility VM based on the web content accessibility guides (WCAG) 2.1 W3C recommendation, to obtain a quantitative measure to improve or construct specific SPL products that require to be accessibility-aware. This paper is specially focused on illustrating the experience of measuring the accessibility in a software product line (SPL) in order to check if it is viable measuring products and recommending improvements in terms of features before addressing the construction of accessibility-aware products. The adoption of the VM accessibility has been putted into practice through a pilot case study, the MOOC (Massive Open Online Course) initiative of the Universidad T\'{e}cnica Particular de Loja. The conduction of this pilot case study has allowed us to illustrate how it is possible to model and measure the accessibility in SPL using accessibility VM, as well as to recommend accessibility configuration improvements for the construction of new or updated MOOC platforms.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {114–121},
numpages = {8},
keywords = {reusability, software and its engineering, software creation and management, software development techniques, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382026.3431247,
author = {Meixner, Kristof},
title = {Integrating Variability Modeling of Products, Processes, and Resources in Cyber-Physical Production Systems Engineering},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431247},
doi = {10.1145/3382026.3431247},
abstract = {The Industry 4.0 initiative envisions the flexible and optimized production of customized products on Cyber-Physical Production Systems (CPPSs) that consist of subsystems coordinated to conduct complex production processes. Hence, accurate CPPS modeling requires integrating the modeling of variability for Product-Process-Resource (PPR) aspects. Yet, current variability modeling approaches treat structural and behavioral variability separately, leading to inaccurate CPPS production models that impede CPPS engineering and optimization. This paper proposes a PhD project for integrated variability modeling of PPR aspects to improve the accuracy of production models with variability for CPPS engineers and production optimizers. The research project follows the Design Science approach aiming for the iterative design and evaluation of (a) a framework to categorize currently incomplete and scattered models and methods for PPR variability modeling as a foundation for an integrated model; and (b) a modeling approach for more accurate integrated PPR variability modeling. The planned research will provide the Software Product Line (SPL) and CPPS engineering research communities with (a) novel models, methods, and insights on integrated PPR variability modeling, (b) open data from CPPS engineering use cases for common modeling, and (c) empirical data from field studies for shared analysis and evaluation.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {96–103},
numpages = {8},
keywords = {Cyber-Physical Production System, Product-Process-Resource, Variability Modelling},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342403,
author = {Berger, Thorsten and Collet, Philippe},
title = {Usage Scenarios for a Common Feature Modeling Language},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342403},
doi = {10.1145/3307630.3342403},
abstract = {Feature models are recognized as a de facto standard for variability modeling. Presented almost three decades ago, dozens of different variations and extensions to the original feature-modeling notation have been proposed, together with hundreds of variability management techniques building upon feature models. Unfortunately, despite several attempts to establish a unified language, there is still no emerging consensus on a feature-modeling language that is both intuitive and simple, but also expressive enough to cover a range of important usage scenarios. There is not even a documented and commonly agreed set of such scenarios.Following an initiative among product-line engineering researchers in September 2018, we present 14 usage scenarios together with examples and requirements detailing each scenario. The scenario descriptions are the result of a systematic process, where members of the initiative authored original descriptions, which received feedback via a survey, and which we then refined and extended based on the survey results, reviewers' comments, and our own expertise. We also report the relevance of supporting each usage scenario for the language, as perceived by the initiative's members, prioritizing each scenario. We present a roadmap to build and implement a first version of the envisaged common language.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {174–181},
numpages = {8},
keywords = {feature models, software product lines, unified language},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/S0164-1212(02)00081-X,
author = {Lutz, Robyn R. and Gannod, Gerald C.},
title = {Analysis of a software product line architecture: an experience report},
year = {2003},
issue_date = {15 June 2003},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {66},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/S0164-1212(02)00081-X},
doi = {10.1016/S0164-1212(02)00081-X},
abstract = {This paper describes experiences with the architectural specification and tool-assisted architectural analysis of a mission-critical, high-performance software product line. The approach used defines a "good" product line architecture in terms of those quality attributes required by the particular product line under development. Architectures are analyzed against several criteria by both manual and tool-supported methods. The approach described in this paper provides a structured analysis of an existing product line architecture using (1) architecture recovery and specification, (2) architecture evaluation, and (3) model checking of behavior to determine the level of robustness and fault tolerance at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain the approach and describe lessons learned.},
journal = {J. Syst. Softw.},
month = jun,
pages = {253–267},
numpages = {15}
}

@inproceedings{10.1145/2934466.2962728,
author = {Santos, Alcemir Rodrigues and Machado, Ivan do Carmo and de Almeida, Eduardo Santana},
title = {RiPLE-HC: visual support for features scattering and interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2962728},
doi = {10.1145/2934466.2962728},
abstract = {With the ever increasing popularity of JavaScript in different domains to build bigger and more complex software systems, variability management may be deemed as an affordable strategy. In this sense, Software Product Lines (SPL) engineering is one of the most successful paradigms to accomplish the necessary modularity and systematic reuse of code artifacts for that purpose. In previous work, we present tool support to hybrid composition of JavaScript-based product lines, called RiPLE-HC, which we now extend to incorporate a means to deal with feature interactions and feature annotation scattering in a more smooth way. The proposed tool support may provide practitioners with an easy-to-use approach to implement crosscutting features by increasing the awareness of the developers about the features implementation.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {320–323},
numpages = {4},
keywords = {eclipse plugin, feature scattering visualization, featureide, javascript, software product line engineering},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3382025.3414943,
author = {Th\"{u}m, Thomas},
title = {A BDD for Linux? the knowledge compilation challenge for variability},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414943},
doi = {10.1145/3382025.3414943},
abstract = {What is the number of valid configurations for Linux? How to generate uniform random samples for Linux? Can we create a binary decision diagram for Linux? It seems that the product-line community tries hard to answer such questions for Linux and other configurable systems. However, attempts are often not published due to the publication bias (i.e., unsuccessful attempts are not published). As a consequence, researchers keep trying by potentially spending redundant effort. The goal of this challenge is to guide research on these computationally complex problems and to foster the exchange between researchers and practitioners.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {16},
numpages = {6},
keywords = {artificial intelligence, binary decision diagrams, configurable system, decision models, feature models, knownledge compilation, product configuration, satisfiability solving, software configuration, software product line},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3233027.3236395,
author = {Pereira, Juliana Alves and Maciel, Lucas and Noronha, Thiago F. and Figueiredo, Eduardo},
title = {Heuristic and exact algorithms for product configuration in software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236395},
doi = {10.1145/3233027.3236395},
abstract = {The Software Product Line (SPL) configuration field is an active area of research and has attracted both practitioners and researchers attention in the last years. A key part of an SPL configuration is a feature model that represents features and their dependencies (i.e., SPL configuration rules). This model can be extended by adding Non-Functional Properties (NFPs) as feature attributes resulting in Extended Feature Models (EFMs). Configuring products from an EFM requires considering the configuration rules of the model and satisfying the product functional and non-functional requirements. Although the configuration of a product arising from EFMs may reduce the space of valid configurations, selecting the most appropriate set of features is still an overwhelming task due to many factors including technical limitations and diversity of contexts. Consequently, configuring large and complex SPLs by using configurators is often beyond the users' capabilities of identifying valid combinations of features that match their (non-functional) requirements. To overcome this limitation, several approaches have modeled the product configuration task as a combinatorial optimization problem and proposed constraint programming algorithms to automatically derive a configuration. Although these approaches do not require any user intervention to guarantee the optimality of the generated configuration, due to the NP-hard computational complexity of finding an optimal variant, exact approaches have inefficient exponential time. Thus, to improve scalability and performance issues, we introduced the adoption of a greedy heuristic algorithm and a biased random-key genetic algorithm (BRKGA). Our experiment results show that our proposed heuristics found optimal solutions for all instances where those are known. For the instances where optimal solutions are not known, the greedy heuristic outperformed the best solution obtained by a one-hour run of the exact algorithm by up to 67.89%. Although the BRKGA heuristic slightly outperformed the greedy heuristic, it has shown larger running times (especially on the largest instances). Therefore, to ensure a good user experience and enable a very fast configuration task, we extended a state-of-the-art configurator with the proposed greedy heuristic approach.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {247},
numpages = {1},
keywords = {configuration optimization, search-based software engineering, software product line configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2491627.2491631,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: a case study in the telecommunication domain},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491631},
doi = {10.1145/2491627.2491631},
abstract = {In the research on software product lines, product variants typically differ by their functionality, and quality attributes are more or less similar across products. To accumulate empirical evidence, this paper presents a descriptive case study of performance variability in a software product line of mobile network base stations. The goal is to study the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The results highlight that the evolution of customer needs motivates performance variability; performance variability can be realized either with software or hardware variability strategy, with the latter often being prevailing; and the software strategy can be kept focused by downgrading performance.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {32–41},
numpages = {10},
keywords = {architecture, case study, software product line, variability},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/1383559.1383571,
author = {Tawhid, Rasha and Petriu, Dorina C.},
title = {Towards automatic derivation of a product performance model from a UML software product line model},
year = {2008},
isbn = {9781595938732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1383559.1383571},
doi = {10.1145/1383559.1383571},
abstract = {Software Product Line (SPL) engineering is a software development approach that takes advantage of the commonality and variability between products from a family, and supports the generation of specific products by reusing a set of core family assets. This paper proposes a UML model transformation approach for software product lines to derive a performance model for a specific product. The input to the proposed technique, the "source model", is a UML model of a SPL with performance annotations, which uses two separate profiles: a "product line" profile from literature for specifying the commonality and variability between products, and the MARTE profile recently standardized by OMG for performance annotations. The source model is generic and therefore its performance annotations must be parameterized. The proposed derivation of a performance model for a concrete product requires two steps: a) the transformation of a SPL model to a UML model with performance annotations for a given product, and b) the transformation of the outcome of the first step into a performance model. This paper focuses on the first step, whereas the second step will use the PUMA transformation approach of annotated UML models to performance models, developed in previous work. The output of the first step, named "target model", is a UML model with MARTE annotations, where the variability expressed in the SPL model has been analyzed and bound to a specific product, and the generic performance annotations have been bound to concrete values for the product. The proposed technique is illustrated with an e-commerce case study.},
booktitle = {Proceedings of the 7th International Workshop on Software and Performance},
pages = {91–102},
numpages = {12},
keywords = {marte, model transformation, software performance engineering, software product line, uml},
location = {Princeton, NJ, USA},
series = {WOSP '08}
}

@inproceedings{10.1145/3109729.3109749,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Extending the Common Variability Language (CVL) Engine: A practical tool},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109749},
doi = {10.1145/3109729.3109749},
abstract = {The Common Variability Language (CVL) has become a reference in the specification and resolution of variability in the last few years. Despite the multiple advantages of CVL (orthogonal variability, architecture variability resolution, MOF-compliant, standard proposed,...), several approaches require extending and/or modifying the CVL approach in different ways in order to fulfill the industrial needs for variability modeling in Software Product Lines. However, the community lacks a tool that would enable proposed extensions and the integration of novel approaches to be put into practice. Existing tools that provide support for CVL are incomplete or are mainly focused on the variability model's editor, instead of executing the resolution of the variability over the base models. Moreover, there is no API that allows direct interaction with the CVL engine to extend or use it in an independent application. In this paper, we identify the extension points of the CVL approach with the goal of making the CVL engine more flexible, and to help software architects in the task of resolving the variability of their products. The practical tool presented here is a working implementation of the CVL engine, that can be extended through a proposed API.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {32–37},
numpages = {6},
keywords = {CVL, Software Product Line, Variability},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1109/HICSS.2009.472,
title = {Towards Tool Support for the Configuration of Non-Functional Properties in SPLs},
year = {2009},
isbn = {9780769534503},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2009.472},
doi = {10.1109/HICSS.2009.472},
abstract = {The configuration of NFPs (non-functional properties) is a crucial problem in the development of software-intensive systems. Most of the approaches currently available tackle this problem during software design. However, at this stage, NFPs cannot be properly predicted. As a solution for this problem we present the new extensions of the Feedback approach which aims at improving the configuration of NFPs in SPLs. We introduce our set of tools that are used to support the approach and show how to use them by applying it to the well-known SPL (The Graph Product Line) that was suggested as a platform for evaluating SPL technologies.},
booktitle = {Proceedings of the 42nd Hawaii International Conference on System Sciences},
pages = {1–7},
numpages = {7},
series = {HICSS '09}
}

@inproceedings{10.1145/2420942.2420944,
author = {Olaechea, Rafael and Stewart, Steven and Czarnecki, Krzysztof and Rayside, Derek},
title = {Modelling and multi-objective optimization of quality attributes in variability-rich software},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420944},
doi = {10.1145/2420942.2420944},
abstract = {Variability-rich software, such as software product lines, offers optional and alternative features to accommodate varying needs of users. Designers of variability-rich software face the challenge of reasoning about the impact of selecting such features on the quality attributes of the resulting software variant. Attributed feature models have been proposed to model such features and their impact on quality attributes, but existing variability modelling languages and tools have limited or no support for such models and the complex multi-objective optimization problem that arises. This paper presents ClaferMoo, a language and tool that addresses these shortcomings. ClaferMoo uses type inheritance to modularize the attribution of features in feature models and allows specifying multiple optimization goals. We evaluate an implementation of the language on a set of attributed feature models from the literature, showing that the optimization infrastructure can handle small-scale feature models with about a dozen features within seconds.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {2},
numpages = {6},
keywords = {multi-objective optimization, software product lines},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/3461001.3471142,
author = {Gu\'{e}gain, \'{E}douard and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {On reducing the energy consumption of software product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471142},
doi = {10.1145/3461001.3471142},
abstract = {Along the last decade, several studies considered green software design as a key development concern to improve the energy efficiency of software. Yet, few techniques address this concern for Software Product Lines (SPL). In this paper, we therefore introduce two approaches to measure and reduce the energy consumption of a SPL by analyzing a limited set of products sampled from this SPL. While the first approach relies on the analysis of individual feature consumptions, the second one takes feature interactions into account to better mitigate energy consumption of resulting products.Our experimental results on a real-world SPL indicate that both approaches succeed to produce significant energy improvements on a large number of products, while consumption data was modeled from a small set of sampled products. Furthermore, we show that taking feature interactions into account leads to more products improved with higher energy savings per product.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {89–99},
numpages = {11},
keywords = {consumption, energy, measurement, mitigation, software product lines},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473944,
author = {Ballesteros, Joaqu\'{\i}n and Fuentes, Lidia},
title = {Transfer learning for multiobjective optimization algorithms supporting dynamic software product lines},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473944},
doi = {10.1145/3461002.3473944},
abstract = {Dynamic Software Product Lines (DSPLs) are a well-accepted approach for self-adapting Cyber-Physical Systems (CPSs) at run-time. The DSPL approaches make decisions supported by performance models, which capture system features' contribution to one or more optimization goals. Combining performance models with Multi-Objectives Evolutionary Algorithms (MOEAs) as decision-making mechanisms is common in DSPLs. However, MOEAs algorithms start solving the optimization problem from a randomly selected population, not finding good configurations fast enough after a context change, requiring too many resources so scarce in CPSs. Also, the DSPL engineer must deal with the hardware and software particularities of the target platform in each CPS deployment. And although each system instantiation has to solve a similar optimization problem of the DSPL, it does not take advantage of experiences gained in similar CPS. Transfer learning aims at improving the efficiency of systems by sharing the previously acquired knowledge and applying it to similar systems. In this work, we analyze the benefits of transfer learning in the context of DSPL and MOEAs testing on 8 feature models with synthetic performance models. Results are good enough, showing that transfer learning solutions dominate up to 71% of the non-transfer learning ones for similar DSPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {51–59},
numpages = {9},
keywords = {cyber-physical systems, dynamic software product lines, multiobjective optimization algorithms, self-adaptation, transfer learning},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3109729.3109751,
author = {Krieter, Sebastian and Pinnecke, Marcus and Kr\"{u}ger, Jacob and Sprey, Joshua and Sontag, Christopher and Th\"{u}m, Thomas and Leich, Thomas and Saake, Gunter},
title = {FeatureIDE: Empowering Third-Party Developers},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109751},
doi = {10.1145/3109729.3109751},
abstract = {FeatureIDE is a popular open-source tool for modeling, implementing, configuring, and analyzing software product lines. However, FeatureIDE's initial design was lacking mechanisms that facilitate extension and reuse of core implementations. In current releases, we improve these traits by providing a modular concept for core data structures and functionalities. As a result, we are facilitating the usage of external implementations for feature models and file formats within FeatureIDE. Additionally, we provide a Java library containing FeatureIDE's core functionalities, including feature modeling and configuration. This allows developers to use these functionalities in their own tools without relying on external dependencies, such as the Eclipse framework.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {42–45},
numpages = {4},
keywords = {Software product line, configuration, feature modeling, feature-oriented software development},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.5555/1885639.1885642,
author = {Bagheri, Ebrahim and Di Noia, Tommaso and Ragone, Azzurra and Gasevic, Dragan},
title = {Configuring software product line feature models based on Stakeholders' soft and hard requirements},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature modeling is a technique for capturing commonality and variability. Feature models symbolize a representation of the possible application configuration space, and can be customized based on specific domain requirements and stakeholder goals. Most feature model configuration processes neglect the need to have a holistic approach towards the integration and satisfaction of the stakeholder's soft and hard constraints, and the application-domain integrity constraints. In this paper, we will show how the structure and constraints of a feature model can be modeled uniformly through Propositional Logic extended with concrete domains, called P(N). Furthermore, we formalize the representation of soft constraints in fuzzy P(N) and explain how semi-automated feature model configuration is performed. The model configuration derivation process that we propose respects the soundness and completeness properties.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {16–31},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/2499777.2500725,
author = {Varshosaz, Mahsa and Khosravi, Ramtin},
title = {Discrete time Markov chain families: modeling and verification of probabilistic software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500725},
doi = {10.1145/2499777.2500725},
abstract = {Software product line engineering (SPLE) enables systematic reuse in development of a family of related software systems by explicitly defining commonalities and variabilities among the individual products in the family. Nowadays, SPLE is used in a variety of complex domains such as avionics and automotive. As such domains include safety critical systems which exhibit probabilistic behavior, there is a major need for modeling and verification approaches dealing with probabilistic aspects of systems in the presence of variabilities. In this paper, we introduce a mathematical model, Discrete Time Markov Chain Family (DTMCF), which compactly represents the probabilistic behavior of all the products in the product line. We also provide a probabilistic model checking method to verify DTMCFs against Probabilistic Computation Tree Logic (PCTL) properties. This way, instead of verifying each product individually, the whole family is model checked at once, resulting in the set of products satisfying the desired property. This reduces the required cost for model checking by eliminating redundant processing caused by the commonalities among the products.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {34–41},
numpages = {8},
keywords = {probabilistic model checking, software product line, variable discrete time Markov chains},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3106195.3106215,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-healing in Service Mashups Through Feature Adaptation},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106215},
doi = {10.1145/3106195.3106215},
abstract = {The composition of the functionality of multiple services into a single unique service mashup has received wide interest in the recent years. Given the distributed nature of these mashups where the constituent services can be located on different servers, it is possible that a change in the functionality or availability of a constituent service result in the failure of the service mashup. In this paper, we propose a novel method based on the Software Product Line Engineering (SPLE) paradigm which is able to find an alternate valid service mashup which has maximum possible number of original service mashup features in order to mitigate a service failure when complete recovery is not possible. This method also has an advantage that it can recover or mitigate the failure automatically without requiring the user to specify any adaptation rule or strategy. We show the practicality of our proposed approach through extensive experiments.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {94–103},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.5555/1762146.1762166,
author = {D'Alberto, Paolo and P\"{u}schel, Markus and Franchetti, Franz},
title = {performance/energy optimization of dsp transforms on the XScale processor},
year = {2007},
isbn = {9783540693376},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The XScale processor family provides user-controllable independent configuration of CPU, bus, and memory frequencies. This feature introduces another handle for the code optimization with respect to energy consumption or runtime performance. We quantify the effect of frequency configurations on both performance and energy for three signal processing transforms: the discrete Fourier transform (DFT), finite impulse response (FIR) filters, and the Walsh-Hadamard Transform (WHT).To do this, we use SPIRAL, a program generation and optimization system for signal processing transforms. For a given transform to be implemented, SPIRAL searches over different algorithms to find the best match to the given platform with respect to the chosen performance metric (usually runtime). In this paper we use SPIRAL to generate implementations for different frequency configurations and optimize for runtime and physically measured energy consumption. In doing so we show that first, each transform achieves best performance/energy consumption for a different system configuration; second, the best code depends on the chosen configuration, problem size and algorithm; third, the fastest implementation is not always the most energy efficient; fourth, we introduce dynamic (i.e., during execution) reconfiguration in order to further improve performance/energy. Finally, we benchmark SPIRAL generated code against Intel's vendor library routines. We show competitive results as well as 20% performance improvements or energy reduction for selected transforms and problem sizes.},
booktitle = {Proceedings of the 2nd International Conference on High Performance Embedded Architectures and Compilers},
pages = {201–214},
numpages = {14},
location = {Ghent, Belgium},
series = {HiPEAC'07}
}

@inproceedings{10.1007/11767718_28,
author = {Chang, Soo Ho and Kim, Soo Dong and Rhew, Sung Yul},
title = {A variability-centric approach to instantiating core assets in product line engineering},
year = {2006},
isbn = {3540346821},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11767718_28},
doi = {10.1007/11767718_28},
abstract = {As a key activity in product line engineering (PLE), instantiation is a task to generate target applications by resolving variability embedded in core assets. However, instantiation is often conducted in manual and ad-hoc fashion, largely replying on domain knowledge and experience. Hence, it can easily lead to technical problems in precisely specifying decision model consisting of product-specific variation points and variants, and in handling inter-variant conflicts/dependency. To overcome this difficulty, it is desirable to develop a systematic process which includes a set of systematic activities, detailed instructions, and concrete specification of artifacts. In this paper, we first propose a meta-model of a core asset to specify its key elements. Then, we represent a comprehensive process that defines key instantiation activities, representations of artifacts, and work instructions. With the proposed process, one can instantiate core assets more effectively and systematically.},
booktitle = {Proceedings of the 7th International Conference on Product-Focused Software Process Improvement},
pages = {334–347},
numpages = {14},
location = {Amsterdam, The Netherlands},
series = {PROFES'06}
}

@inproceedings{10.1145/3307630.3342411,
author = {Meixner, Kristof and Rabiser, Rick and Biffl, Stefan},
title = {Towards Modeling Variability of Products, Processes and Resources in Cyber-Physical Production Systems Engineering},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342411},
doi = {10.1145/3307630.3342411},
abstract = {Planning and developing Cyber-Physical Production Systems (CPPS) are multi-disciplinary engineering activities that rely on effective and efficient knowledge exchange for better collaboration between engineers of different disciplines. The Product-Process-Resource (PPR) approach allows modeling products produced by industrial processes using specific production resources. In practice, a CPPS manufactures a portfolio of product type variants, i.e., a product line. Therefore, engineers need to create and maintain several PPR models to cover PPR variants and their evolving versions. In this paper, we detail a representative use case, identify challenges for using Variability Modeling (VM) methods to describe and manage PPR variants, and present a first solution approach based on cooperation with domain experts at an industry partner, a system integrator of automation for high-performance CPPS. We conclude that integrating basic variability concepts into PPR models is a promising first step and describe our further research plans to support PPR VM in CPPS.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {49–56},
numpages = {8},
keywords = {cyber-physical production system, product-process-resource, variability modelling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {feature identification, requirement documents, reverse engineering, software product lines, variability extraction},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414962,
author = {Chrszon, Philipp and Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha},
title = {From features to roles},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414962},
doi = {10.1145/3382025.3414962},
abstract = {The detection of interactions is a challenging task present in almost all stages of software development. In feature-oriented system design, this task is mainly investigated for interactions of features within a single system, detected by their emergent behaviors. We propose a formalism to describe interactions in hierarchies of feature-oriented systems (hierarchical interactions) and the actual situations where features interact (active interplays). Based on the observation that such interactions are also crucial in role-based systems, we introduce a compositional modeling framework based on concepts and notions of roles, comprising role-based automata (RBAs). To describe RBAs, we present a modeling language that is close to the input language of the probabilistic model checker Prism. To exemplify the use of RBAs, we implemented a tool that translates RBA models into Prism and thus enables the formal analysis of functional and non-functional properties including system dynamics, contextual changes, and interactions. We carry out two case studies as a proof of concept of such analyses: First, a peer-to-peer protocol case study illustrates how undesired hierarchical interactions can be discovered automatically. Second, a case study on a self-adaptive production cell demonstrates how undesired interactions influence quality-of-service measures such as reliability and throughput.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {19},
numpages = {11},
keywords = {feature-oriented systems, formal methods, roles, verification},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1109/APSEC.2004.12,
author = {Kim, Soo Dong and Chang, Soo Ho and Chang, Chee Won},
title = {A Systematic Method to Instantiate Core Assets in Product Line Engineering},
year = {2004},
isbn = {0769522459},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2004.12},
doi = {10.1109/APSEC.2004.12},
abstract = {Product line engineering (PLE) is one of the recent and effective reuse approaches, and it consists of two processes; framework engineering and application engineering. Framework engineering is to model and realize a core asset which represents common functionality and quality attributes in a target domain, and application engineering is to generate a target product by instantiating the core asset. Hence, a core asset is a key element of PLE, and the quality of core assets largely determines the overall quality of products. Although numerous PLE methodologies have been introduced, it is still unclear what should be the elements of a core asset and how the core asset should be instantiated for each application. That is, the overall process and instructions of instantiating core assets have not been studied enough. In this paper, we first define a meta-model of core assets, propose a process of instantiation, and define its instructions on how to carry out each activity in practice. We believe that the proposed meta-model and the process framework can effectively be used in developing core asset and applications in practice.},
booktitle = {Proceedings of the 11th Asia-Pacific Software Engineering Conference},
pages = {92–98},
numpages = {7},
series = {APSEC '04}
}

@inproceedings{10.1145/3307630.3342414,
author = {Th\"{u}m, Thomas and Teixeira, Leopoldo and Schmid, Klaus and Walkingshaw, Eric and Mukelabai, Mukelabai and Varshosaz, Mahsa and Botterweck, Goetz and Schaefer, Ina and Kehrer, Timo},
title = {Towards Efficient Analysis of Variation in Time and Space},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342414},
doi = {10.1145/3307630.3342414},
abstract = {Variation is central to today's software development. There are two fundamental dimensions to variation: Variation in time refers to the fact that software exists in numerous revisions that typically replace each other (i.e., a newer version supersedes an older one). Variation in space refers to differences among variants that are designed to coexist in parallel. There are numerous analyses to cope with variation in space (i.e., product-line analyses) and others that cope with variation in time (i.e., regression analyses). The goal of this work is to discuss to which extent product-line analyses can be applied to revisions and, conversely, where regression analyses can be applied to variants. In addition, we discuss challenges related to the combination of product-line and regression analyses. The overall goal is to increase the efficiency of analyses by exploiting the inherent commonality between variants and revisions.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
keywords = {product-line analysis, regression analysis, software configuration management, software evolution, software product lines, software variation, variability management, variability-aware analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336302,
author = {Str\"{u}ber, Daniel and Mukelabai, Mukelabai and Kr\"{u}ger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {benchmark, product lines, software evolution, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414963,
author = {Creff, Stephen and Noir, J\'{e}r\^{o}me Le and Lenormand, Eric and Madel\'{e}nat, S\'{e}bastien},
title = {Towards facilities for modeling and synthesis of architectures for resource allocation problem in systems engineering},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414963},
doi = {10.1145/3382025.3414963},
abstract = {Exploring architectural design space is often beyond human capacity and makes architectural design a difficult task. Model-based systems engineering must include assistance to the system designer in identifying candidate architectures to subsequently analyze tradeoffs. Unfortunately, existing languages and approaches do not incorporate this concern, generally favoring solution analysis over exploring a set of candidate architectures.In this paper, we explore the advantages of designing and configuring the variability problem to solve one of the problems of exploring (synthesizing) candidate architectures in systems engineering: the resource allocation problem. More specifically, this work reports on the use of the Clafer modeling language and its gateway to the CSP Choco Solver, on an industrial case study of heterogeneous hardware resource allocation (GPP-GPGPU-FPGA).Based on experiments on the modeling in Clafer, and the impact of its translation into the constraint programming paradigm (performance studies), discussions highlight some issues concerning facilities for modeling and synthesis of architectures and recommendations are proposed towards the use of this variability approach.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {32},
numpages = {11},
keywords = {allocation problem, architecture synthesis, constraint solving, empirical study, variability modeling},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {configurable systems, feature models, monte carlo tree search, software product lines, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2934466.2934473,
author = {Olaechea, Rafael and Fahrenberg, Uli and Atlee, Joanne M. and Legay, Axel},
title = {Long-term average cost in featured transition systems},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934473},
doi = {10.1145/2934466.2934473},
abstract = {A software product line is a family of software products that share a common set of mandatory features and whose individual products are differentiated by their variable (optional or alternative) features. Family-based analysis of software product lines takes as input a single model of a complete product line and analyzes all its products at the same time. As the number of products in a software product line may be large, this is generally preferable to analyzing each product on its own. Family-based analysis, however, requires that standard algorithms be adapted to accomodate variability.In this paper we adapt the standard algorithm for computing limit average cost of a weighted transition system to software product lines. Limit average is a useful and popular measure for the long-term average behavior of a quality attribute such as performance or energy consumption, but has hitherto not been available for family-based analysis of software product lines. Our algorithm operates on weighted featured transition systems, at a symbolic level, and computes limit average cost for all products in a software product line at the same time. We have implemented the algorithm and evaluated it on several examples.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {109–118},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.compind.2006.09.004,
author = {Boucher, Xavier and Bonjour, Eric and Grabot, Bernard},
title = {Formalisation and use of competencies for industrial performance optimisation: A survey},
year = {2007},
issue_date = {February, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {58},
number = {2},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2006.09.004},
doi = {10.1016/j.compind.2006.09.004},
abstract = {For many years, industrial performance has been implicitly considered as deriving from the optimisation of technological and material resources (machines, inventories, etc.), made possible by centralized organisations. The topical requirements for reactive and flexible industrial systems have progressively reintroduced the human workforce as the main source of industrial performance. Making this paradigm operational requires the identification and careful formalisation of the link between human resource and industrial performance, through concepts like skills, competencies or know-how. This paper provides a general survey of the formalisation and integration of competence-oriented concepts within enterprise information systems and decision systems, aiming at providing new methods and tools for performance management.},
journal = {Comput. Ind.},
month = feb,
pages = {98–117},
numpages = {20},
keywords = {Competence model, Enterprise modelling, Information and decision systems, Performance}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Exploit, Feature Model, Variability Model, Vulnerability, Vulnerability Analysis and Management},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2377816.2377821,
author = {Garcia-Alonso, Jose and Olmeda, Javier Berrocal and Murillo, Juan Manuel},
title = {Architectural variability management in multi-layer web applications through feature models},
year = {2012},
isbn = {9781450313094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377816.2377821},
doi = {10.1145/2377816.2377821},
abstract = {The development of large web applications has focused on the use of increasingly complex architectures based on the layer architectural pattern and different development frame-works. Many techniques have been proposed to deal with this increasing complexity, mostly in the field of model-based development which abstracts the architects and designers from the architectural and technological complexities. However, these techniques do not take into account the great variability of these architectures, and therefore limit the architectural options available for their users. We here describe a feature model that captures the architectural and technological variability of multilayer applications. Using this feature model as the core of a model-driven development process, we are able to incorporate architectural and technological variability into the model-based development of multilayer applications. This approach keeps complexity under control whilst flexibility on choosing technologies is not penalized},
booktitle = {Proceedings of the 4th International Workshop on Feature-Oriented Software Development},
pages = {29–36},
numpages = {8},
keywords = {design patterns, development frameworks, feature model, model-driven development, multilayer architectures},
location = {Dresden, Germany},
series = {FOSD '12}
}

@inproceedings{10.1145/2499777.2500719,
author = {Schr\"{o}ter, Reimar and Siegmund, Norbert and Th\"{u}m, Thomas},
title = {Towards modular analysis of multi product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500719},
doi = {10.1145/2499777.2500719},
abstract = {Software product-line engineering enables efficient development of tailor-made software by means of reusable artifacts. As practitioners increasingly develop software systems as product lines, there is a growing potential to reuse product lines in other product lines, which we refer to as multi product line. We identify challenges when developing multi product lines and propose interfaces for different levels of abstraction ranging from variability modeling to functional and non-functional properties. We argue that these interfaces ease the reuse of product lines and identify research questions that need to be solved toward modular analysis of multi product lines.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {96–99},
numpages = {4},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3382025.3414965,
author = {Young, Jeffrey M. and Walkingshaw, Eric and Th\"{u}m, Thomas},
title = {Variational satisfiability solving},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414965},
doi = {10.1145/3382025.3414965},
abstract = {Incremental satisfiability (SAT) solving is an extension of classic SAT solving that allows users to efficiently solve a set of related SAT problems by identifying and exploiting shared terms. However, using incremental solvers effectively is hard since performance is sensitive to a problem's structure and the order sub-terms are fed to the solver, and the burden to track results is placed on the end user. For analyses that generate sets of related SAT problems, such as those in software product lines, incremental SAT solvers are either not used at all, used but not explicitly stated so in the literature, or used but suffer from the aforementioned usability problems. This paper translates the ordering problem to an encoding problem and automates the use of incremental SAT solving. We introduce variational SAT solving, which differs from incremental SAT solving by accepting all related problems as a single variational input and returning all results as a single variational output. Our central idea is to make explicit the operations of incremental SAT solving, thereby encoding differences between related SAT problems as local points of variation. Our approach automates the interaction with the incremental solver and enables methods to automatically optimize sharing of the input. To evaluate our methods we construct a prototype variational SAT solver and perform an empirical analysis on two real-world datasets that applied incremental solvers to software evolution scenarios. We show, assuming a variational input, that the prototype solver scales better for these problems than naive incremental solving while also removing the need to track individual results.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {18},
numpages = {12},
keywords = {choice calculus, satisfiability solving, software product lines, variation},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.5555/1105634.1105651,
author = {de Oliveira, Edson Alves and Gimenes, Itana M. S. and Huzita, Elisa Hatsue Moriya and Maldonado, Jos\'{e} Carlos},
title = {A variability management process for software product lines},
year = {2005},
publisher = {IBM Press},
abstract = {The software product line approach (PL) promotes the generation of specific products from a set of core assets for a given domain. This approach is applicable to domains in which products have well-defined commonalities and variation points. Variability management is concerned with the management of the differences between products throughout the PL lifecycle. This paper presents a UML-based process for variability management that allows identification, representation and delimitation of variabilities as well as identification of mechanisms for variability implementation. The process is illustrated with excerpts of a case study carried out within the context of an existing PL for the Workflow Management System (WfMS) domain. The case study was carried out based on the experimental software engineering concepts. The results have shown that the proposed process has made explicit a higher number of variabilities than does the existing PL process, and it offers better support for variability tracing.},
booktitle = {Proceedings of the 2005 Conference of the Centre for Advanced Studies on Collaborative Research},
pages = {225–241},
numpages = {17},
location = {Toranto, Ontario, Canada},
series = {CASCON '05}
}

@inproceedings{10.1145/2934466.2934474,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Purposeful performance variability in software product lines: a comparison of two case studies},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934474},
doi = {10.1145/2934466.2934474},
abstract = {Within software product lines, customers may have different quality needs. To produce products with purposefully different quality attributes, several challenges must be addressed. First, one must be able to distinguish product quality attributes to the customers in a meaningful way. Second, one must create the desired quality attribute differences during product-line architecture design and derivation. To study how performance is varied purposefully in software product lines, we conducted a comparison and re-analysis of two industrial case studies in the telecommunication and mobile game domains. The results show that performance variants must be communicated to the customer in a way that links to customer value and her role. When performance or its adaptation are crucial for the customer, performance differences must be explicitly "designed in" with software or hardware means. Due to the emergent nature of performance, it is important to test performance and manage how other variability affects performance.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {144–153},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3307630.3342404,
author = {Th\"{u}m, Thomas and Seidl, Christoph and Schaefer, Ina},
title = {On Language Levels for Feature Modeling Notations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342404},
doi = {10.1145/3307630.3342404},
abstract = {Configuration is a key enabling technology for the engineering of systems and software as wells as physical goods. A selection of configuration options (aka. features) is often enough to automatically generate a product tailored to the needs of a customer. It is common that not all combinations of features are possible in a given domain. Feature modeling is the de-facto standard for specifying features and their valid combinations. However, a pivotal hurdle for practitioners, researchers, and teachers in applying feature modeling is that there are hundreds of tools and languages available. While there have been first attempts to define a standard feature modeling language, they still struggle with finding an appropriate level of expressiveness. If the expressiveness is too high, the language will not be adopted, as it is too much effort to support all language constructs. If the expressiveness is too low, the language will not be adopted, as many interesting domains cannot be modeled in such a language. Towards a standard feature modeling notation, we propose the use of language levels with different expressiveness each and discuss criteria to be used to define such language levels. We aim to raise the awareness on the expressiveness and eventually contribute to a standard feature modeling notation.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {158–161},
numpages = {4},
keywords = {automated analysis, expressiveness, feature model, language design, product lines, variability modeling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2019136.2019168,
author = {Nakagawa, Elisa Yumi and Antonino, Pablo Oliveira and Becker, Martin},
title = {Exploring the use of reference architectures in the development of product line artifacts},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019168},
doi = {10.1145/2019136.2019168},
abstract = {Software Product Line (SPL) has arisen as an approach for developing a family of software-intensive systems at lower costs, within shorter time, and with higher quality. In particular, SPL is supported by a product line architecture (sometimes also referred to as reference architecture) that captures the architectures of a product family. In another context, a special type of architecture that contains knowledge about a specific domain has been increasingly investigated, resulting in the Reference Architecture research area. In spite of the positive impact of this type of architecture on reuse and productivity, the use of existing domain-specific reference architectures as basis of SPL has not been widely explored. The main contribution of this paper is to present how and when elements contained in existing reference architectures could contribute to the building of SPL artifacts during development of an SPL. We have observed that, in fact, reference architectures could make an important contribution to improving reuse and productivity, which are also important concerns in SPL.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {28},
numpages = {8},
keywords = {SPL design method, reference architecture, software product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1007/11946441_74,
author = {Bonelli, Andreas and Franchetti, Franz and Lorenz, Juergen and P\"{u}schel, Markus and Ueberhuber, Christoph W.},
title = {Automatic performance optimization of the discrete fourier transform on distributed memory computers},
year = {2006},
isbn = {3540680675},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11946441_74},
doi = {10.1007/11946441_74},
abstract = {This paper introduces a formal framework for automatically generating performance optimized implementations of the discrete Fourier transform (DFT) for distributed memory computers. The framework is implemented as part of the program generation and optimization system Spiral. DFT algorithms are represented as mathematical formulas in Spiral's internal language SPL. Using a tagging mechanism and formula rewriting, we extend Spiral to automatically generate parallelized formulas. Using the same mechanism, we enable the generation of rescaling DFT algorithms, which redistribute the data in intermediate steps to fewer processors to reduce communication overhead. It is a novel feature of these methods that the redistribution steps are merged with the communication steps of the algorithm to avoid additional communication overhead. Among the possible alternative algorithms, Spiral's search mechanism now determines the fastest for a given platform, effectively generating adapted code without human intervention. Experiments with DFT MPI programs generated by Spiral show performance gains of up to 30% due to rescaling. Further, our generated programs compare favorably with Fftw-MPI 2.1.5.},
booktitle = {Proceedings of the 4th International Conference on Parallel and Distributed Processing and Applications},
pages = {818–832},
numpages = {15},
location = {Sorrento, Italy},
series = {ISPA'06}
}

@inproceedings{10.5555/998675.999419,
author = {Matinlassi, Mari},
title = {Comparison of Software Product Line Architecture Design Methods: COPA, FAST, FORM, KobrA and QADA},
year = {2004},
isbn = {0769521630},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Product line architectures (PLAs) have been undercontinuous attention in the software research communityduring the past few years. Although several methods havebeen established to create PLAs there are not availablestudies comparing PLA methods. Five methods are knownto answer the needs of software product lines: COPA,FAST, FORM, KobrA and QADA. In this paper, anevaluation framework is introduced for comparing PLAdesign methods. The framework considers the methodsfrom the points of view of method context, user, structureand validation. Comparison revealed distinguishableideologies between the methods. Therefore, methods donot overlap even though they all are PLA design methods.All the methods have been validated on various domains.The most common domains are telecommunicationinfrastructure and information domains. Some of themethods apply software standards; at least OMG\'{y}s MDAfor method structures, UML for language and IEEE Std-1471-2000 for viewpoint definitions.},
booktitle = {Proceedings of the 26th International Conference on Software Engineering},
pages = {127–136},
numpages = {10},
series = {ICSE '04}
}

@inproceedings{10.1145/3382025.3414945,
author = {G\"{o}ttmann, Hendrik and Luthmann, Lars and Lochau, Malte and Sch\"{u}rr, Andy},
title = {Real-time-aware reconfiguration decisions for dynamic software product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414945},
doi = {10.1145/3382025.3414945},
abstract = {Dynamic Software Product Lines (DSPL) have recently shown promising potentials as integrated engineering methodology for (self-)adaptive software systems. Based on the software-configuration principles of software product lines, DSPL additionally foster reconfiguration capabilities to continuously adapt software products to ever-changing environmental contexts. However, in most recent works concerned with finding near-optimal reconfiguration decisions, real-time aspects of reconfiguration processes are usually out of scope. In this paper, we present a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Those real-time aware DSPL specifications are internally translated into timed automata, a well-founded formalism for real-time behaviors. This representation allows for formally reasoning about consistency and worst-case/best-case execution-time behaviors of sequences of reconfiguration decisions. The technique is implemented in a prototype tool and experimentally evaluated with respect to a set of case studies1.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {13},
numpages = {11},
keywords = {dynamic software product lines, reconfiguration decisions, timed automata},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2499777.2500710,
author = {Gabillon, Yoann and Biri, Nicolas and Otjacques, Beno\^{\i}t},
title = {Methodology to integrate multi-context UI variations into a feature model},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500710},
doi = {10.1145/2499777.2500710},
abstract = {Software product line (SPL) paradigm aims to explore commonalities and variabilities in a set of applications for developing an efficient derivation of products. One of the most common ways to model variability in this paradigm is to use a Feature Model. However, variability in SPL is often limited to functional features. The User Interface (UI) variations are modeled as entire UIs and thus these variations are not reusable and inspectable. Research in the Human Computer Interaction (HCI) field has proven the importance of variability for non functional, purely UI centric features. The HCI community has proposed several levels of abstraction for multi-context UI design. Indeed, new variations can be introduced at each abstraction level. UI designers are used to them and they usually introduce variability at each step of the UI definition without using SPL. To build usable softwares that take into account UI, we propose to merge functional concerns and UI concerns, providing a methodology to integrate variability of both aspects into a single Feature Model.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {74–81},
numpages = {8},
keywords = {abstraction levels, feature model, multi-context, software product line, usability, user interface, variability},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.5555/645882.672260,
author = {Voget, Stefan and Becker, Martin},
title = {Establishing a Software Product Line in an Immature Domain},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Often product lines are applied to "stable domains" (i.e., a set of common features is identifiable in advance and the evolution of the domain is manageable during the lifetime of the product line). These prerequisites are not always given. But there may be market pressure that requires developing products with systematic and preplanned reuse in a domain that is difficult to grasp. In such a case the product line approach also offers a set of methods that helps to overcome the risks of an immature domain.In this paper we discuss some risks in context of immature domains. For some challenges we present approaches to manage them. The considerations are substantiated by experiences in the domain of entertainment and infotainment systems in an automotive context. The development is deeply influenced by technological changes (e.g., Internet, MP3-player, UMTS) that challenge the successful deployment of product line technology.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {60–67},
numpages = {8},
series = {SPLC 2}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {asset health, extensibility, industrial analytics, interoperability, knowledge, performance, reusability, software product line},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {recommender systems, runtime decision-making, self-configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791099,
author = {Filho, Jo\~{a}o Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
title = {Assessing product line derivation operators applied to Java source code: an empirical study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791099},
doi = {10.1145/2791060.2791099},
abstract = {Product Derivation is a key activity in Software Product Line Engineering. During this process, derivation operators modify or create core assets (e.g., model elements, source code instructions, components) by adding, removing or substituting them according to a given configuration. The result is a derived product that generally needs to conform to a programming or modeling language. Some operators lead to invalid products when applied to certain assets, some others do not; knowing this in advance can help to better use them, however this is challenging, specially if we consider assets expressed in extensive and complex languages such as Java. In this paper, we empirically answer the following question: which product line operators, applied to which program elements, can synthesize variants of programs that are incorrect, correct or perhaps even conforming to test suites? We implement source code transformations, based on the derivation operators of the Common Variability Language. We automatically synthesize more than 370,000 program variants from a set of 8 real large Java projects (up to 85,000 lines of code), obtaining an extensive panorama of the sanity of the operations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {36–45},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2499777.2500711,
author = {Ciolfi Felice, Marianela and Filho, Joao Bosco Ferreira and Acher, Mathieu and Blouin, Arnaud and Barais, Olivier},
title = {Interactive visualisation of products in online configurators: a case study for variability modelling technologies},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500711},
doi = {10.1145/2499777.2500711},
abstract = {Numerous companies develop interactive environments to assist users in customising sales products through the selection of configuration options. A visual representation of these products is an important factor in terms of user experience. However, an analysis of 100+ existing configurators highlights that not all provide visual representations of configured products. One of the current challenges is the trade-off developers face between either the memory consuming use of pregenerated images of all the combinations of options, or rendering products on the fly, which is non trivial to implement efficiently. We believe that a new approach to associate product configurations to visual representations is needed to compose and render them dynamically. In this paper we present a formal statement of the problem and a model-driven perspective for addressing it as well as our ongoing work and further challenges.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {82–85},
numpages = {4},
keywords = {configurator, software product line, user interface, variability modelling},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3233027.3233035,
author = {Varshosaz, Mahsa and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Runge, Tobias and Mousavi, Mohammad Reza and Schaefer, Ina},
title = {A classification of product sampling for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233035},
doi = {10.1145/3233027.3233035},
abstract = {The analysis of software product lines is challenging due to the potentially large number of products, which grow exponentially in terms of the number of features. Product sampling is a technique used to avoid exhaustive testing, which is often infeasible. In this paper, we propose a classification for product sampling techniques and classify the existing literature accordingly. We distinguish the important characteristics of such approaches based on the information used for sampling, the kind of algorithm, and the achieved coverage criteria. Furthermore, we give an overview on existing tools and evaluations of product sampling techniques. We share our insights on the state-of-the-art of product sampling and discuss potential future work.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {1–13},
numpages = {13},
keywords = {domain models, feature interaction, sampling algorithms, software product lines, testing},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2362536.2362567,
author = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
title = {Developing platforms for multiple software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362567},
doi = {10.1145/2362536.2362567},
abstract = {Many approaches to software product line engineering have been founded on the development of a single product line platform. However as customer requirements change and new products are added to the product line, software producers recognize that the platform cannot be "stretched" indefinitely and a significant problem is striking a balance between development efficiency by increasing platform commonality and customer dissatisfaction from products with additional undesirable features and properties.One alternative is to develop multiple product lines (MPLs). However the challenge remains about what to include in a multiple product line platform. Drawing upon industrial experience of working with 4 companies, this paper explores the characteristics of the contexts in which MPLs are a viable alternative development strategy and then proposes a framework of approaches to platform development.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {220–228},
numpages = {9},
keywords = {industrial experience, multiple product lines, software reuse},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2362536.2362546,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {A systematically conducted literature review: quality attribute variability in software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362546},
doi = {10.1145/2362536.2362546},
abstract = {Typically, products in a software product line differ by their functionality, and quality attributes are not intentionally varied. Why, how, and which quality attributes to vary has remained an open issue. A systematically conducted literature review on quality attribute variability is presented, where primary studies are selected by reading all content of full studies in Software Product Line Conference. The results indicate that the success of feature modeling influences the proposed approaches, different approaches suit specific quality attributes differently, and empirical evidence on industrial quality variability is lacking.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {41–45},
numpages = {5},
keywords = {quality attribute, systematic literature review, variability},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {product and production configuration, product line of factories, smart factory, smart product, smart production},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3233036,
author = {Hamza, Mostafa and Walker, Robert J. and Elaasar, Maged},
title = {CIAhelper: towards change impact analysis in delta-oriented software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233036},
doi = {10.1145/3233027.3233036},
abstract = {Change is inevitable for software systems to deal with the evolving environment surrounding them, and applying changes requires careful design and implementation not to break existing functionalities. Evolution in software product lines (SPLs) is more complex compared to evolution for individual products: a change applied to a single feature might affect all the products in the whole product family. In this paper we present an approach for change impact analysis in delta-oriented programming (DOP), an existing language aimed at supporting SPLs. We propose the CIAHelper tool to identify dependencies within a DOP program, by analyzing the semantics of both the code artifacts and variability models to construct a directed dependency graph. We also consider how the source code history could be used to enhance the recall of detecting the affected artifacts given a change proposal. We evaluate our approach by means of five case studies on two different DOP SPLs.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {31–42},
numpages = {12},
keywords = {change impact analysis, code assets, delta-oriented programming, feature model, variability model},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1007/s10664-004-6190-y,
author = {Svahnberg, Mikael and Wohlin, Claes},
title = {An Investigation of a Method for Identifying a Software Architecture Candidate with Respect to Quality Attributes},
year = {2005},
issue_date = {April     2005},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {10},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-004-6190-y},
doi = {10.1007/s10664-004-6190-y},
abstract = {To sustain the qualities of a software system during evolution, and to adapt the quality attributes as the requirements evolve, it is necessary to have a clear software architecture that is understood by all developers and to which all changes to the system adheres. This software architecture can be created beforehand, but must also be updated to reflect changes in the domain, and hence the requirements of the software. The choice of which software architecture to use is typically based on informal decisions. There exist, to the best of our knowledge, little factual knowledge of which quality attributes are supported or obstructed by different architecture approaches. In this paper we present an empirical study of a method that enables quantification of the perceived support different software architectures give for different quality attributes. This in turn enables an informed decision of which architecture candidate best fit the mixture of quality attributes required by a system being designed.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {149–181},
numpages = {33},
keywords = {Software architectures, analytic hierarchy process, quality attributes}
}

@inproceedings{10.1145/2934466.2934492,
author = {Groher, Iris and Weinreich, Rainer and Buchgeher, Georg and Schossleitner, Robert},
title = {Reusable architecture variants for customer-specific automation solutions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934492},
doi = {10.1145/2934466.2934492},
abstract = {Manufacturing execution systems (MES) are key elements of industrial automation systems. MES can be deployed at different levels of scale from a single site or plant to a company with globally distributed production sites all over the world. Establishing or extending an MES is a complex process, which requires taking the already existing software and system architecture into account in addition to the desired MES features. We developed an approach and an associated tool to support the process of creating offers for customer-specific MES solutions based on a vendor-specific automation platform. We define architecture variants for selecting a specific MES feature set and for supporting different MES expansion stages. Additionally, we provide an architecture modeling approach to explore the integration with existing software and system infrastructures. The approach has been applied at the STIWA Group, a vendor of MES for industrial production lines.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {242–251},
numpages = {10},
keywords = {architecture variants, automation platform, customer-specific offer, feature set, manufacturing execution system (MES)},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2934466.2946045,
author = {Noir, J\'{e}rome Le and Madel\'{e}nat, S\'{e}bastien and Gailliard, Gr\'{e}gory and Labreuche, Christophe and Acher, Mathieu and Barais, Olivier and Constant, Olivier},
title = {A decision-making process for exploring architectural variants in systems engineering},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946045},
doi = {10.1145/2934466.2946045},
abstract = {In systems engineering, practitioners shall explore numerous architectural alternatives until choosing the most adequate variant. The decision-making process is most of the time a manual, time-consuming, and error-prone activity. The exploration and justification of architectural solutions is ad-hoc and mainly consists in a series of tries and errors on the modeling assets. In this paper, we report on an industrial case study in which we apply variability modeling techniques to automate the assessment and comparison of several candidate architectures (variants). We first describe how we can use a model-based approach such as the Common Variability Language (CVL) to specify the architectural variability. We show that the selection of an architectural variant is a multi-criteria decision problem in which there are numerous interactions (veto, favor, complementary) between criteria.We present a tooled process for exploring architectural variants integrating both CVL and the MYRIAD method for assessing and comparing variants based on an explicit preference model coming from the elicitation of stakeholders' concerns. This solution allows understanding differences among variants and their satisfactions with respect to criteria. Beyond variant selection automation improvement, this experiment results highlight that the approach improves rationality in the assessment and provides decision arguments when selecting the preferred variants.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {277–286},
numpages = {10},
keywords = {architecture, decision-making, design exploration, model-driven engineering, multi-criteria decision analysis, systems engineering},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2647908.2655972,
author = {Meinicke, Jens and Th\"{u}m, Thomas and Schr\"{o}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
title = {An overview on analysis tools for software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655972},
doi = {10.1145/2647908.2655972},
abstract = {A software product line is a set of different software products that share commonalities. For a selection of features, specialized products of one domain can be generated automatically from domain artifacts. However, analyses of software product lines need to handle a large number of products that can be exponential in the number of features. In the last decade, many approaches have been proposed to analyze software product lines efficiently. For some of these approaches tool support is available. Based on a recent survey on analysis for software product lines, we provide a first overview on such tools. While our discussion is limited to analysis tools, we provide an accompanying website covering further tools for product-line development. We compare tools according to their analysis and implementation strategy to identify underrepresented areas. In addition, we want to ease the reuse of existing tools for researchers and students, and to simplify research transfer to practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {94–101},
numpages = {8},
keywords = {code metrics, model checking, non-functional properties, sampling, software product lines, static analysis, testing, theorem proving, tool support, type checking},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3474624.3474626,
author = {Bettin, Giovanna and OliveiraJr, Edson},
title = {SMartyPerspective: a perspective-based inspection technique for software product lines},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3474626},
doi = {10.1145/3474624.3474626},
abstract = {Software Product Line (SPL) is an approach for reusing software artifacts for a specific domain. To improve products quality, verification and validation activities for SPL artifacts are necessary, thus defects are not propagated to derived products. The lack of techniques to exploit the inherited SPL reuse characteristics, mainly in early phases, provides an opportunity to investigate how to improve SPL quality. Perspective-Based Reading (PBR) has proven to be a feasible inspection technique, as it considers different scenarios and perspectives of reviewers of software artifacts. Therefore, in this paper, we specify and evaluate SMartyPerspective, a PBR technique to inspect UML-based SPL diagrams (use case, class, sequence, and component) and feature diagrams. SMartyPerspective comprises Domain Engineering perspectives: Product Manager, Domain Requirements Engineer, Domain Architect, Domain Developer, and Domain Asset Manager. We evaluated it by carrying out a TAM-based qualitative study with 19 participants with experience in SPL and software inspections. We also used coding to analyze the open questions. Obtained results provide initial evidence SMartyPerspective is feasible for inspecting its supported diagrams.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {90–94},
numpages = {5},
keywords = {Defects, Perspective-Based Reading, SMarty, SPL Inspections, Software Product Line, TAM, UML},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/2019136.2019152,
author = {Kozuka, Nobuaki and Ishida, Yuzo},
title = {Building a product line architecture for variant-rich enterprise applications using a data-oriented approach},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019152},
doi = {10.1145/2019136.2019152},
abstract = {IT industry in Japan has grown by providing specific made-to-order enterprise applications for various industries. Most of enterprise applications are built upon relational database management system (RDBMS), which takes the responsibility of keeping data integrity and data manipulation. However, data explosion in recent years especially in retail and telecommunication industries makes IT industry difficult to satisfy quality attributes such as scalability, availability and data consistency with traditional development techniques. From the beginning of this century, NRI has built and refined product line architecture as a primary core asset for such data intensive industries, which have very rich variations in functional and nonfunctional requirements of their enterprise applications. This paper summarizes key criteria to build such an architecture based on our ten years experience in developing dozens of mission critical IT systems as product families for those industries.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {14},
numpages = {6},
keywords = {core asset development, data intensiveness, data oriented approach, enterprise applications, product line architecture, quality attributes, relational database management system},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2019136.2019158,
author = {Guana, Victor and Correal, Dario},
title = {Variability quality evaluation on component-based software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019158},
doi = {10.1145/2019136.2019158},
abstract = {Quality assurance and evaluation in Model Driven Software Product Lines (MD-SPLs) are pivotal points for the growing and solidification of the generative software factories. They are framed as one of the future fact methodologies for the construction of software systems. Although several approximations address the problem of generative environments, software product line scope expression, and core asset definition, not many of them try to solve, as a fundamental step, the automation of the quality attribute evaluation in the MD-SPL development cycle. This paper presents a model-driven engineering method and a tool for the quality evaluation of product line configurations through a cross architectural view analysis.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {19},
numpages = {8},
keywords = {domain specific modeling, model composition, model-driven software product line, quality attribute, sensitivity point},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3493244.3493250,
author = {Wolfart, Daniele and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Martinez, Jabier},
title = {Variability Debt: Characterization, Causes and Consequences},
year = {2021},
isbn = {9781450395533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493244.3493250},
doi = {10.1145/3493244.3493250},
abstract = {Variability is an inherent property of software systems to create families of products dealing with needs of different customers and environments. However, some practices to manage variability may incur technical debt. For example, the use of opportunistic reuse strategies, e.g., clone-and-own, harms maintenance and evolution activities; or deciding to abandon variability management and deriving a single product with all the features might threaten system usability. These examples are common problems found in practice but, to the best of or knowledge, not properly investigated from the perspective of technical debt. To expand the knowledge on the research and practice of technical debt in the perspective of variability management, we report results of this phenomenon, which we defined as variability debt. Our work is based on 52 industrial case studies that report problems observed in the use of opportunistic reuse. The results show that variability debt is caused by business, operational and technical aspects; leads to complex maintenance, creates difficulties to customize and create new products, misuse of human resources, usability problems; and impacts artifacts along the whole life-cycle. Although some of these issues are investigated in the field of systematic variability management, e.g., software product lines, our contribution is to present them from a technical debt perspective to enrich and create synergies between the two fields. As additional contribution, we present a catalog of variability debts in the light of technical debts found in the literature.},
booktitle = {Proceedings of the XX Brazilian Symposium on Software Quality},
articleno = {17},
numpages = {10},
keywords = {Software Product Lines, Technical Debt, Variability Debt, Variability management},
location = {Virtual Event, Brazil},
series = {SBQS '21}
}

@inproceedings{10.1145/3336294.3336297,
author = {Munoz, Daniel-Jesus and Oh, Jeho and Pinto, M\'{o}nica and Fuentes, Lidia and Batory, Don},
title = {Uniform Random Sampling Product Configurations of Feature Models That Have Numerical Features},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336297},
doi = {10.1145/3336294.3336297},
abstract = {Analyses of Software Product Lines (SPLs) rely on automated solvers to navigate complex dependencies among features and find legal configurations. Often these analyses do not support numerical features with constraints because propositional formulas use only Boolean variables. Some automated solvers can represent numerical features natively, but are limited in their ability to count and Uniform Random Sample (URS) configurations, which are key operations to derive unbiased statistics on configuration spaces.Bit-blasting is a technique to encode numerical constraints as propositional formulas. We use bit-blasting to encode Boolean and numerical constraints so that we can exploit existing #SAT solvers to count and URS configurations. Compared to state-of-art Satisfiability Modulo Theory and Constraint Programming solvers, our approach has two advantages: 1) faster and more scalable configuration counting and 2) reliable URS of SPL configurations. We also show that our work can be used to extend prior SAT-based SPL analyses to support numerical features and constraints.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {289–301},
numpages = {13},
keywords = {bit-blasting, feature model, model counting, numerical features, propositional formula, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2019136.2019187,
author = {Abbas, Nadeem},
title = {Towards autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019187},
doi = {10.1145/2019136.2019187},
abstract = {We envision an Autonomic Software Product Line (ASPL). The ASPL is a dynamic software product line that supports self adaptable products. We plan to use reflective architecture to model and develop ASPL. To evaluate the approach, we have implemented three autonomic product lines which show promising results. The ASPL approach is at initial stages, and require additional work. We plan to exploit online learning to realize more dynamic software product lines to cope with the problem of product line evolution. We propose on-line knowledge sharing among products in a product line to achieve continuous improvement of quality in product line products.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {44},
numpages = {8},
keywords = {knowledge, on-line learning, self-adaptation},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based test case selection of cyber-physical system product lines for simulation-based validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {cyber-physical system product lines, search-based software engineering, test case selection},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791075,
author = {Fang, Miao and Leyh, Georg and Doerr, Joerg and Elsner, Christoph and Zhao, Jingjing},
title = {Towards model-based derivation of systems in the industrial automation domain},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791075},
doi = {10.1145/2791060.2791075},
abstract = {Many systems in the industrial automation domain include information systems. They manage manufacturing processes and control numerous distributed hardware and software components. In current practice, the development and reuse of such systems is costly and time-consuming, due to the variability of systems' topology and processes. Up to now, product line approaches for systematic modeling and management of variability have not been well established for such complex domains.In this paper, we present a model-based approach to support the derivation of systems in the target domain. The proposed architecture of the derivation infrastructure enables feature-, topology- and process configuration to be integrated into the multi-staged derivation process. We have developed a prototype to prove feasibility and improvement of derivation efficiency. We report the evaluation results that we collected through semi-structured interviews from domain stakeholders. The results show high potential to improve derivation efficiency by adopting the approach in practice. Finally, we report the lessons learned that raise the opportunities and challenges for future research.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {283–292},
numpages = {10},
keywords = {derivation, model-based engineering, product line, variability modeling},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1007/s10664-014-9353-5,
author = {Asadi, Mohsen and Soltani, Samaneh and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
title = {The effects of visualization and interaction techniques on feature model configuration},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9353-5},
doi = {10.1007/s10664-014-9353-5},
abstract = {A Software Product Line is a set of software systems of a domain, which share some common features but also have significant variability. A feature model is a variability modeling artifact which represents differences among software products with respect to variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by selecting features in the feature model (referred to as the configuration process) and by instantiating the reference model. However, feature model configuration can be a cumbersome task because: 1) feature models may consist of a large number of features, which are hard to comprehend and maintain; and 2) many factors including technical limitations, implementation costs, stakeholders' requirements and expectations must be considered in the configuration process. Recognizing these issues, a significant amount of research efforts has been dedicated to different aspects of feature model configuration such as automating the configuration process. Several approaches have been proposed to alleviate the feature model configuration challenges through applying visualization and interaction techniques. However, there have been limited empirical insights available into the impact of visualization and interaction techniques on the feature model configuration process. In this paper, we present a set of visualization and interaction interventions for representing and configuring feature models, which are then empirically validated to measure the impact of the proposed interventions. An empirical study was conducted by following the principles of control experiments in software engineering and by applying the well-known software quality standard ISO 9126 to operationalize the variables investigated in the experiment. The results of the empirical study revealed that the employed visualization and interaction interventions significantly improved completion time of comprehension and changing of the feature model configuration. Additionally, according to results, the proposed interventions are easy-to-use and easy-to-learn for the participants.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1706–1743},
numpages = {38},
keywords = {Controlled experiment, Software product line engineering, Tools}
}

@inproceedings{10.1145/2362536.2362570,
author = {Braga, Rosana T. V. and Trindade, Onofre and Branco, Kalinka R. L. J. Castelo and Lee, Jaejoon},
title = {Incorporating certification in feature modelling of an unmanned aerial vehicle product line},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362570},
doi = {10.1145/2362536.2362570},
abstract = {Safety critical systems (e.g., an avionics control system for safe flight) are often required to achieve certification under pre-established standards (e.g., DO-178B for software considerations in airborne systems and equipment certification). We have been working with our industrial partner for the last three years to develop product line assets for their avionics software product line (SPL) and, recently, we encountered two major challenges regarding certification. Firstly, an individual product must be certified, but each may require a different certification level: there might be variations in the certification requirements according to specific system usage contexts. Secondly, certification involves not only product but also process, as standards such as DO-178B also assess the quality of the development process. In this paper, we propose to include a certification view during feature modelling to provide a better understanding of the relationships between features and a certification level required for each product. The experience of introducing certification into the design model of an Unmanned Aerial Vehicle (UAV) SPL is presented to illustrate some key ideas. We also describe the lessons we have learned from this experience.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {249–258},
numpages = {10},
keywords = {critical software development, feature modelling, software certification, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3302333.3302336,
author = {Galster, Matthias},
title = {Variability-intensive Software Systems: Product Lines and Beyond},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302336},
doi = {10.1145/3302333.3302336},
abstract = {Product line engineering emerged from the software reuse and generative programming movements of the 70s. However, in today's competitive and fast-paced markets where users expect software to adapt to their specific needs, most modern software-intensive products and services are variability-intensive, regardless of whether they are part of a product line or not. In this talk, we explore how building variability-intensive software systems influences the various software product lifecycle stages. Furthermore, we look beyond variability in functional features: We explore the role of quality attributes in the design of variability-intensive software systems and discuss how to address the challenge of identifying and managing variability in quality attributes. Finally, based on the example of software product line research, we explore how research on variability has been changing over time and if research trends in industry and academia have diverged.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {3},
numpages = {1},
keywords = {Variability-intensive software systems, product lines and families, research and practice, software quality attributes},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/2791060.2791102,
author = {Mu\~{n}oz-Fern\'{a}ndez, Juan C. and Tamura, Gabriel and Raicu, Irina and Mazo, Ra\'{u}l and Salinesi, Camille},
title = {REFAS: a PLE approach for simulation of self-adaptive systems requirements},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791102},
doi = {10.1145/2791060.2791102},
abstract = {Model simulation has demonstrated its usefulness in evaluation and decision-making for improving preliminary versions of artefacts before production. Particularly, one of the main goals of simulation is to verify model properties based on data collected from its execution. In this paper, we present the simulation capabilities of our REFAS framework for specifying requirements models for dynamic software products lines and self-adaptive systems. The simulation is controlled by a feedback loop and a reasoning engine that operates on the functional and non-functional requirements. The paper contribution is threefold. First, REFAS allows developers to evaluate and improve requirements models through their simulation capabilities. Second, REFAS provides rich feedback in its interactive simulations for the human modeller to make informed decisions to improve her model. Third, REFAS automates the generation of simulation scenarios required to verify the model adequacy and correctness. We evaluate our contribution by comparing the application of REFAS to a case study used in other approaches.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {121–125},
numpages = {5},
keywords = {MAPE-K loops, dynamic adaptation, dynamic software product lines, requirements engineering, simulation},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2491627.2491630,
author = {Linsbauer, Lukas and Lopez-Herrejon, E. Roberto and Egyed, Alexander},
title = {Recovering traceability between features and code in product variants},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491630},
doi = {10.1145/2491627.2491630},
abstract = {Many companies offer a palette of similar software products though they do not necessarily have a Software Product Line (SPL). Rather, they start building and selling individual products which they then adapt, customize and extend for different customers. As the number of product variants increases, these companies then face the severe problem of having to maintain them all. Software Product Lines can be helpful here - not so much as a platform for creating new products but as a means of maintaining the existing ones with their shared features. Here, an important first step is to determine where features are implemented in the source code and in what product variants. To this end, this paper presents a novel technique for deriving the traceability between features and code in product variants by matching code overlaps and feature overlaps. This is a difficult problem because a feature's implementation not only covers its basic functionality (which does not change across product variants) but may include code that deals with feature interaction issues and thus changes depending on the combination of features present in a product variant. We empirically evaluated the approach on three non-trivial case studies of different sizes and domains and found that our approach correctly identifies feature to code traces except for code that traces to multiple disjunctive features, a rare case involving less than 1% of the code.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {131–140},
numpages = {10},
keywords = {features, product variants, traceability},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2648511.2648516,
author = {Reinhartz-Berger, Iris and Figl, Kathrin},
title = {Comprehensibility of orthogonal variability modeling languages: the cases of CVL and OVM},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648516},
doi = {10.1145/2648511.2648516},
abstract = {As the complexity and variety of systems and software products have increased, the ability to manage their variability effectively and efficiently became crucial. To this end, variability can be specified either as an integral part of the development artifacts or in a separate orthogonal variability model. Lately, orthogonal variability models attract a lot of attention due to the fact that they do not require changing the complexity of the development artifacts and can be used in conjunction with different development artifacts. Despite this attention and to the best of our knowledge, no empirical study examined the comprehensibility of orthogonal variability models.In this work, we conducted an exploratory experiment to examine potential comprehension problems in two common orthogonal variability modeling languages, namely, Common Variability Language (CVL) and Orthogonal Variability Model (OVM). We examined the comprehensibility of the variability models and their relations to the development artifacts for novice users. To measure comprehensibility we used comprehension score (i.e., percentage of correct solution), time spent to complete tasks, and participants' perception of difficulty of different model constructs. The results showed high comprehensibility of the variability models, but low comprehensibility of the relations between the variability models and the development artifacts. Although the comprehensibility of CVL and OVM was similar in terms of comprehension score and time spent to complete tasks, novice users perceived OVM as more difficult to comprehend.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {42–51},
numpages = {10},
keywords = {CVL, OVM, empirical study, model comprehension, variability analysis},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2647908.2655977,
author = {El Yamany, Ahmed Eid and Shaheen, Mohamed and Sayyad, Abdel Salam},
title = {OPTI-SELECT: an interactive tool for user-in-the-loop feature selection in software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655977},
doi = {10.1145/2647908.2655977},
abstract = {Opti-Select is an Interactive Multi-objective feature analysis and optimization tool for software product lines configuration and feature models optimization based on an innovative UIL (User-In-the-loop) idea. In this tool, the experience of system analysts and stakeholders are merged with optimization techniques and algorithms.Opti-Select interactive tool is an integrated set of techniques providing step by step feature model and attribute configuration, selecting and excluding features, solution set optimization, and user interaction utilities that can all together reach satisfactory set of solutions that fits stakeholder preferences.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {126–129},
numpages = {4},
keywords = {Pareto front visualization, exploration, feature modeling, feature models, features, modeling, multi-objective optimization, optimal feature selection, optimal variant, product line engineering, search-based software engineering, software product lines, user-in-the-loop (UIL)},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2648511.2648521,
author = {Olaechea, Rafael and Rayside, Derek and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Comparison of exact and approximate multi-objective optimization for software product lines},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648521},
doi = {10.1145/2648511.2648521},
abstract = {Software product lines (SPLs) allow stakeholders to manage product variants in a systematical way and derive variants by selecting features. Finding a desirable variant is often difficult, due to the huge configuration space and usually conflicting objectives (e.g., lower cost and higher performance). This scenario can be characterized as a multi-objective optimization problem applied to SPLs. We address the problem using an exact and an approximate algorithm and compare their accuracy, time consumption, scalability, parameter setting requirements on five case studies with increasing complexity. Our empirical results show that (1) it is feasible to use exact techniques for small SPL multi-objective optimization problems, and (2) approximate methods can be used for large problems but require substantial effort to find the best parameter setting for acceptable approximation which can be ameliorated with known good parameter ranges. Finally, we discuss the tradeoff between accuracy and time consumption when using exact and approximate techniques for SPL multi-objective optimization and guide stakeholders to choose one or the other in practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {92–101},
numpages = {10},
keywords = {multi-objective optimization, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1007/s11219-013-9197-z,
author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
title = {Quality attribute modeling and quality aware product configuration in software product lines},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9197-z},
doi = {10.1007/s11219-013-9197-z},
abstract = {In software product line engineering, the customers mostly concentrate on the functionalities of the target product during product configuration. The quality attributes of a target product, such as security and performance, are often assessed until the final product is generated. However, it might be very costly to fix the problem if it is found that the generated product cannot satisfy the customers' quality requirements. Although the quality of a generated product will be affected by all the life cycles of product development, feature-based product configuration is the first stage where the estimation or prediction of the quality attributes should be considered. As we know, the key issue of predicting the quality attributes for a product configured from feature models is to measure the interdependencies between functional features and quality attributes. The current existing approaches have several limitations on this issue, such as requiring real products for the measurement or involving domain experts' efforts. To overcome these limitations, we propose a systematic approach of modeling quality attributes in feature models based on domain experts' judgments using the analytic hierarchical process (AHP) and conducting quality aware product configuration based on the captured quality knowledge. Domain experts' judgments are adapted to avoid generating the real products for quality evaluation, and AHP is used to reduce domain experts' efforts involved in the judgments. A prototype tool is developed to implement the concepts of the proposed approach, and a formal evaluation is carried out based on a large-scale case study.},
journal = {Software Quality Journal},
month = sep,
pages = {365–401},
numpages = {37},
keywords = {Analytic hierarchical process (AHP), Feature model, Non-functional requirement (NFR) framework, Product configuration, Quality attributes assessment, Software product line}
}

@inproceedings{10.1145/2362536.2362560,
author = {Lettner, Daniela and Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Supporting end users with business calculations in product configuration},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362560},
doi = {10.1145/2362536.2362560},
abstract = {Business calculations like break-even, return on investment, or cost are essential in many domains to support decision making while configuring products. For instance, customers and sales people need to estimate and compare the business value of different product variants. Some product line approaches provide initial support, e.g., by defining quality attributes in relation to features. However, an approach that allows domain engineers to easily define business calculations together with variability models is still lacking. In product configuration, calculation results need to be instantly presented to end users after making configuration choices. Further, due to the often high number of calculations, the presentation of calculation results to end users can be challenging. These challenges cannot be addressed by integrating off-the-shelf applications performing the calculations with product line tools. We thus present an approach based on dedicated calculation models that are related to variability models. Our approach seamlessly integrates business calculations with product configuration and provides support for formatting calculations and calculation results. We use the DOPLER tool suite to deploy calculations together with variability models to end users in product configuration. We evaluate the expressiveness and practical relevance of the approach by investigating the development of business calculations for 15 product lines from the domain of industrial automation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {171–180},
numpages = {10},
keywords = {business calculations, product configuration, variability models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {Software product line, configuration management, critical path analysis, product derivation, weighted approach}
}

@inproceedings{10.5555/1753235.1753255,
author = {Sun, Hongyu and Lutz, Robyn R. and Basu, Samik},
title = {Product-line-based requirements customization for web service compositions},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Customizing web services according to users' individual functional and non-functional requirements has become increasingly difficult as the number of users increases. This paper introduces a new way to customize and verify composite web services by incorporating a software product-line engineering approach into web-service composition. The approach uses a partitioning similar to that between domain engineering and application engineering in the product-line context. It specifies the options that the user can select and constructs the resulting web-service compositions. By first creating a web-service composition search space that satisfies the common requirements and then querying the search space as the user selects values for the parameters of variation, we provide a more efficient way to customize web services. A decision model, illustrated with examples from an emergency-response application, is created to interact with the customers and ensure the consistency of their specifications. The capability to reuse the composition search space may also help improve the quality and reliability of the composite services and reduce the cost of re-verifying the same compositions.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {141–150},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1007/978-3-642-31095-9_12,
author = {Ghaddar, Ali and Tamzalit, Dalila and Assaf, Ali and Bitar, Abdalla},
title = {Variability as a service: outsourcing variability management in multi-tenant saas applications},
year = {2012},
isbn = {9783642310942},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31095-9_12},
doi = {10.1007/978-3-642-31095-9_12},
abstract = {In order to reduce the overall application expenses and time to market, SaaS (Software as a Service) providers tend to outsource several parts of their IT resources to other services providers. Such outsourcing helps SaaS providers in reducing costs and concentrating on their core competences: software domain expertises, business-processes modeling, implementation technologies and frameworks etc. However, when a SaaS provider offers a single application instance for multiple customers following the multi-tenant model, these customers (or tenants) requirements may differ, generating an important variability management concern. We believe that variability management should also be outsourced and considered as a service. The novelty of our work is to introduce the new concept of Variability as a Service (VaaS) model. It induces the appearance of VaaS providers. The objective is to relieve the SaaS providers looking forward to adopt such attractive multi-tenant solution, from developing a completely new and expensive variability solution beforehand. We present in this paper the first stage of our work: the VaaS meta-model and the VariaS component.},
booktitle = {Proceedings of the 24th International Conference on Advanced Information Systems Engineering},
pages = {175–189},
numpages = {15},
keywords = {SaaS, multi-tenant, variability},
location = {Gda\'{n}sk, Poland},
series = {CAiSE'12}
}

@inproceedings{10.1145/3218585.3218670,
author = {Martins, Luana Almeida and Parreira, Paulo Afonso and Freire, Andr\'{e} Pimenta and Costa, Heitor},
title = {Exploratory Study on the Use of Software Product Lines in the Development of Quality Assistive Technology Software},
year = {2018},
isbn = {9781450364676},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3218585.3218670},
doi = {10.1145/3218585.3218670},
abstract = {The use of Software Product Line for the development of Assistive Technologies has not been widely explored yet. However, some studies point to the viability of using this approach to develop Assistive Technology software. Through this approach, important limiting factors to use Assistive Technologies can be overcome. These factors are related to the acquisition costs and difficulty to find products corresponding to specific and varying user needs. Considering that Software Product Line approach provides mass customization of software products, the specific needs of each user can be more easily satisfied by software developers. Furthermore, the reuse of code artifacts to development provides a fall in the acquisition cost of these software products. We present in this paper a literature review that aims to investigate how this approach has been applied to the development of Assistive Technology software. Also, we present some quality factors that should be considered to develop Assistive Technologies using Software Product Lines. Thus, the main findings of the review are grouped in order to find the main gaps to be explored in future work.},
booktitle = {Proceedings of the 8th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {262–269},
numpages = {8},
keywords = {Assistive Technology, Software Product Line, Software Quality},
location = {Thessaloniki, Greece},
series = {DSAI '18}
}

@inproceedings{10.1145/2499777.2500718,
author = {Abbas, Nadeem and Andersson, Jesper},
title = {Architectural reasoning for dynamic software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500718},
doi = {10.1145/2499777.2500718},
abstract = {Software quality is critical in today's software systems. A challenge is the trade-off situation architects face in the design process. Designers often have two or more alternatives, which must be compared and put into context before a decision is made. The challenge becomes even more complex for dynamic software product lines, where domain designers have to take runtime variations into consideration as well. To address the problem we propose extensions to an architectural reasoning framework with constructs/artifacts to define and model a domain's scope and dynamic variability. The extended reasoning framework encapsulates knowledge to understand and reason about domain quality behavior and self-adaptation as a primary variability mechanism. The framework is demonstrated for a self-configuration property, self-upgradability on an educational product-line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {117–124},
numpages = {8},
keywords = {adaptation, architectural reasoning, software product lines, variability},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2491627.2491635,
author = {Henard, Christopher and Papadakis, Mike and Perrouin, Gilles and Klein, Jacques and Traon, Yves Le},
title = {Multi-objective test generation for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491635},
doi = {10.1145/2491627.2491635},
abstract = {Software Products Lines (SPLs) are families of products sharing common assets representing code or functionalities of a software product. These assets are represented as features, usually organized into Feature Models (FMs) from which the user can configure software products. Generally, few features are sufficient to allow configuring millions of software products. As a result, selecting the products matching given testing objectives is a difficult problem.The testing process usually involves multiple and potentially conflicting testing objectives to fulfill, e.g. maximizing the number of optional features to test while at the same time both minimizing the number of products and minimizing the cost of testing them. However, most approaches for generating products usually target a single objective, like testing the maximum amount of feature interactions. While focusing on one objective may be sufficient in certain cases, this practice does not reflect real-life testing situations.The present paper proposes a genetic algorithm to handle multiple conflicting objectives in test generation for SPLs. Experiments conducted on FMs of different sizes demonstrate the effectiveness, feasibility and practicality of the introduced approach.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {62–71},
numpages = {10},
keywords = {feature models, genetic algorithms, multi-objective optimization, software product lines, test generation},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2019136.2019159,
author = {Otsuka, Jun and Kawarabata, Kouichi and Iwasaki, Takashi and Uchiba, Makoto and Nakanishi, Tsuneo and Hisazumi, Kenji},
title = {Small inexpensive core asset construction for large gainful product line development: developing a communication system firmware product line},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019159},
doi = {10.1145/2019136.2019159},
abstract = {Product line development of communication system firmware with more than 2,000 features was performed in a large-scale project that involved more than 300 engineers (at a maximum) across four distributed sites. However, since intense demands to reduce development costs and time made it prohibitive to construct core assets for all those identified features, the project screened a limited number of the features, for which core assets were constructed, and then performed partial application of product line engineering. Nevertheless, when compared with previously engineered derivative developments, when the second product of the product line was released, it was clear that the project had achieved significant improvements in quality, as well as reductions in development costs and time requirements. Automatic code generation also contributed to those improvements.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {20},
numpages = {5},
keywords = {case study, core assets, feature modeling, product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.asoc.2016.07.040,
author = {Xue, Yinxing and Zhong, Jinghui and Tan, Tian Huat and Liu, Yang and Cai, Wentong and Chen, Manman and Sun, Jun},
title = {IBED},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.040},
doi = {10.1016/j.asoc.2016.07.040},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose to combine IBEA and DE for the optimal feature selection in SPLE.We propose a feedback-directed method into EAs to improve the correctness of results.Our IBED with the seeding method has significantly shortened the search time.In most cases, IBED finds more unique and non-dominated solutions than IBEA. Software configuration, which aims to customize the software for different users (e.g., Linux kernel configuration), is an important and complicated task. In software product line engineering (SPLE), feature oriented domain analysis is adopted and feature model is used to guide the configuration of new product variants. In SPLE, product configuration is an optimal feature selection problem, which needs to find a set of features that have no conflicts and meanwhile achieve multiple design objectives (e.g., minimizing cost and maximizing the number of features). In previous studies, several multi-objective evolutionary algorithms (MOEAs) were used for the optimal feature selection problem and indicator-based evolutionary algorithm (IBEA) was proven to be the best MOEA for this problem. However, IBEA still suffers from the issues of correctness and diversity of found solutions. In this paper, we propose a dual-population evolutionary algorithm, named IBED, to achieve both correctness and diversity of solutions. In IBED, two populations are individually evolved with two different types of evolutionary operators, i.e., IBEA operators and differential evolution (DE) operators. Furthermore, we propose two enhancement techniques for existing MOEAs, namely the feedback-directed mechanism to fast find the correct solutions (e.g., solutions that satisfy the feature model constraints) and the preprocessing method to reduce the search space. Our empirical results have shown that IBED with the enhancement techniques can outperform several state-of-the-art MOEAs on most case studies in terms of correctness and diversity of found solutions.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1215–1231},
numpages = {17},
keywords = {Differential evolutionary algorithm (DE), Indicator-based evolutionary algorithm (IBEA), Optimal feature selection, Software product line engineering}
}

@inproceedings{10.5555/776816.776947,
author = {Knauber, Peter and Bosch, Jan},
title = {ICSE workshop on: Software Variability Management},
year = {2003},
isbn = {076951877X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {During recent years, the amount of variability that has to be supported by a software artifact is growing considerably and its management is developing as a main challenge during development, usage, and evolution of software artifacts. Successful management of variability in software artifacts leads to better customizable software products that are in turn likely to result in higher market success.The aim of this workshop is to study software variability management both from a 'problems' and from a 'solutions' perspective by bringing together people from industrial practice and from applied research in academia to present and discuss their respective experience.Issues to be addressed include, but are not limited to, technological, process, and organizational aspects as well as notation, assessment, design, and evolution aspects.},
booktitle = {Proceedings of the 25th International Conference on Software Engineering},
pages = {779–780},
numpages = {2},
keywords = {software adaptation, software configuration, software customization, software variability, variability management},
location = {Portland, Oregon},
series = {ICSE '03}
}

@inproceedings{10.1145/2019136.2019150,
author = {Serajzadeh, Hadi and Shams, Fereidoon},
title = {The application of swarm intelligence in service-oriented product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019150},
doi = {10.1145/2019136.2019150},
abstract = {Changing markets and environments has made the ability to rapidly adapt to these changes a necessity in software systems. However the costs of changing and adapting systems to new requirements still remains an unsolved issue. In this context service-oriented software product lines were introduced with the aim to combine the reusability of software product line with the flexibility of service-oriented architecture. Although this approach helps build flexible software systems with high levels of reuse, certain issues are raised. The main issue is the complexity that a service-oriented product line will face. Developing systems from internal and external assets, taking into consideration the variety and number of these assets, can cause problems in deciding which asset is best suited for the system. To help solve these issues we propose the use of approaches based on artificial intelligence. In this paper we show how swarm intelligence can be used in service-oriented product lines to reduce complexity and find optimal solutions for the development of software systems. We also present an example of the application of swarm intelligence in finding the optimal product for a service-oriented product line.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {12},
numpages = {7},
keywords = {optimization, service-oriented product line, swarm intelligence},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2791060.2791082,
author = {Hotz, Lothar and Wang, Yibo and Riebisch, Matthias and G\"{o}tz, Olaf and Lackhove, Josef},
title = {Evaluation across multiple views for variable automation systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791082},
doi = {10.1145/2791060.2791082},
abstract = {Automation systems in industry are often software-intensive systems consisting of software and hardware components. During their development several engineers of different disciplines are involved, such as mechanical, electrical and software engineering. Each engineer focuses on specific system aspects to be developed. To enable an efficient development, product lines especially with feature models for variability modeling are promising technologies. In order to reduce the complexity of both feature models and development process, views on feature models can be applied. The use of views for filtering purposes constitutes an established method. However, views also enable further options missing in current approaches, such as evaluations regarding requirements, including non-functional ones. This paper presents an approach for evaluation across multiple views to enable collaborative development for developers who focus on different system aspects. We validate our approach by applying it in an industrial project for the planning of flying saws.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {311–315},
numpages = {5},
keywords = {automation systems, configuration, consistency check, feature model, multi-criteria evaluation, product lines},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2491627.2491651,
author = {Nakagawa, Elisa Yumi and Becker, Martin and Maldonado, Jos\'{e} Carlos},
title = {Towards a process to design product line architectures based on reference architectures},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491651},
doi = {10.1145/2491627.2491651},
abstract = {Software Product Line (SPL) has arisen as an approach for developing a family of software-intensive systems at lower costs, within shorter time, and with higher quality. In particular, SPL is supported by a product line architecture (sometimes also referred to as reference architecture) that captures the architectures of a product family. From another perspective, a special type of architecture that contains knowledge about a specific domain has been increasingly investigated, resulting in the research area of Reference Architecture. In spite of the positive impact of this type of architecture on reuse and productivity, the use of the knowledge contained in existing reference architectures in order to develop SPL has not been widely explored yet. The main contribution of this paper is to present a process, named ProSA-RA2PLA, that systematizes the use of reference architectures for building product line architectures. To illustrate the application of this process, we have built a product line architecture for an SPL of software testing tools using a reference architecture of that domain. Based on initial results, we have observed that benefits can be achieved, mainly regarding improvement in reuse and productivity to develop SPL.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {157–161},
numpages = {5},
keywords = {knowledge sharing, reference architecture},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2791060.2791096,
author = {F\'{e}derle, \'{E}dipo Luis and do Nascimento Ferreira, Thiago and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {OPLA-tool: a support tool for search-based product line architecture design},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791096},
doi = {10.1145/2791060.2791096},
abstract = {The Product Line Architecture (PLA) design is a complex task, influenced by many factors such as feature modularization and PLA extensibility, which are usually evaluated according to different metrics. Hence, the PLA design is an optimization problem and problems like that have been successfully solved in the Search-Based Software Engineering (SBSE) area, by using metaheuristics such as Genetic Algorithm. Considering this fact, this paper introduces a tool named OPLA-Tool, conceived to provide computer support to a search-based approach for PLA design. OPLA-Tool implements all the steps necessary to use multi-objective optimization algorithms, including PLA transformations and visualization through a graphical interface. OPLA-Tool receives as input a PLA at the class diagram level, and produces a set of good alternative diagrams in terms of cohesion, feature modularization and reduction of crosscutting concerns.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {370–373},
numpages = {4},
keywords = {multi-objective evolutionary algorithms, product line architecture design, search-based software engineering},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1007/s00766-014-0203-1,
author = {D\'{\i}az, Jessica and P\'{e}rez, Jennifer and Garbajosa, Juan},
title = {A model for tracing variability from features to product-line architectures: a case study in smart grids},
year = {2015},
issue_date = {September 2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0203-1},
doi = {10.1007/s00766-014-0203-1},
abstract = {In current software systems with highly volatile requirements, traceability plays a key role to maintain the consistency between requirements and code. Traceability between artifacts involved in the development of software product line (SPL) is still more critical because it is necessary to guarantee that the selection of variants that realize the different SPL products meet the requirements. Current SPL traceability mechanisms trace from variability in features to variations in the configuration of product-line architecture (PLA) in terms of adding and removing components. However, it is not always possible to materialize the variable features of a SPL through adding or removing components, since sometimes they are materialized inside components, i.e., in part of their functionality: a class, a service, and/or an interface. Additionally, variations that happen inside components may crosscut several components of architecture. These kinds of variations are still challenging and their traceability is not currently well supported. Therefore, it is not possible to guarantee that those SPL products with these kinds of variations meet the requirements. This paper presents a solution for tracing variability from features to PLA by taking these kinds of variations into account. This solution is based on models and traceability between models in order to automate SPL configuration by selecting the variants and realizing the product application. The FPLA modeling framework supports this solution which has been deployed in a software factory. Validation has consisted in putting the solution into practice to develop a product line of power metering management applications for smart grids.},
journal = {Requir. Eng.},
month = sep,
pages = {323–343},
numpages = {21},
keywords = {Product-line architecture, Software product line engineering, Traceability modeling, Variability}
}

@inproceedings{10.1145/2648511.2648525,
author = {Stein, Jacob and Nunes, Ingrid and Cirilo, Elder},
title = {Preference-based feature model configuration with multiple stakeholders},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648525},
doi = {10.1145/2648511.2648525},
abstract = {Feature model configuration is known to be a hard, error-prone and time-consuming activity. This activity gets even more complicated when it involves multiple stakeholders in the configuration process. Research work has proposed approaches to aid multi-stakeholder feature model configuration, but they rely on systematic processes that constraint decisions of some of the stakeholders. In this paper, we propose a novel approach to improve the multi-stakeholder configuration process, considering stakeholders' preferences expressed through both hard and soft constraints. Based on such preferences, we recommend different product configurations using different strategies from the social choice theory. We conducted an empirical study to evaluate the effectiveness of our strategies with respect to individual stakeholder satisfaction and fairness among all stakeholders. Results indicate that particular strategies perform best with respect to these aspects.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {132–141},
numpages = {10},
keywords = {feature model configuration, preferences, social choice},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1002/smr.1568,
author = {Belategi, Lorea and Sagardui, Goiuria and Etxeberria, Leire and Azanza, Maider},
title = {Embedded software product lines: domain and application engineering model‐based analysis processes},
year = {2014},
issue_date = {April 2014},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {26},
number = {4},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.1568},
doi = {10.1002/smr.1568},
abstract = {Nowadays, embedded systems are gaining importance. At the same time, the development of their software is increasing its complexity, having to deal with quality, cost, and time‐to‐market issues among others. With stringent quality requirements such as performance, early verification and validation become critical in these systems. In this regard, advanced development paradigms such as model‐driven engineering and software product line engineering bring considerable benefits to the development and validation of embedded system software. However, these benefits come at the cost of increasing process complexity. This work presents a process based on UML and MARTE for the analysis of embedded model‐driven product lines. It specifies the tasks, the involved roles, and the workproducts that form the process and how it is integrated in the more general development process. Existing tools that support the tasks to be performed in the process are also described. A classification of such tools and a study of traceability among them are provided, allowing engineering teams to choose the most adequate chain of tools to support the process. Copyright © 2012 John Wiley &amp; Sons, Ltd.Embedded systems are becoming ubiquitous, and software running on them is fundamental for them to function. At the same time, the development of their software is increasing its complexity, dealing with cost, time to market, and quality, among others. With stringent quality requirements such as performance, early verification and validation of their software is essential for assuring software quality. In this setting, this work presents a process that supports model‐based analysis of an embedded software product line to assure temporal requirements.  


image
image},
journal = {J. Softw. Evol. Process},
month = apr,
pages = {419–433},
numpages = {14},
keywords = {software product line, model‐based analysis process, model‐driven development, quality attributes, performance}
}

@inproceedings{10.1145/2362536.2362573,
author = {Bartholdt, J\"{o}rg and Becker, Detlef},
title = {Scope extension of an existing product line},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362573},
doi = {10.1145/2362536.2362573},
abstract = {At the beginning, creating a product line needs a well defined and narrow scope to meet short time to market demands. When established, there is a tendency to broaden the scope and to cover more domains and products.We have undergone a scope extension of our medical diagnostic platform that was implemented while the platform and (existing) products were evolving. In this paper, we list best practices for the migration process and how to come to a sustainable solution without cannibalizing the existing platform and products.In particular, we describe our way of identification beneficial sub-domains using C/V analysis and give an example scenario with alignments in order to increase commonality. We explain the maturity considerations for deciding on reuse of existing implementations and a carve-out strategy to split existing assets into common modules and product-line specific extensions. Furthermore, we describe our best practices for making the scope extension sustainable in a long term, using various types of governance means. We briefly complement these experiences with further insights gained during execution of this endeavor.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {275–282},
numpages = {8},
keywords = {C/V analysis, governance, hierarchical product-line, scope extension},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2362536.2362556,
author = {Nunes, Camila and Garcia, Alessandro and Lucena, Carlos and Lee, Jaejoon},
title = {History-sensitive heuristics for recovery of features in code of evolving program families},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362556},
doi = {10.1145/2362536.2362556},
abstract = {A program family might degenerate due to unplanned changes in its implementation, thus hindering the maintenance of family members. This degeneration is often induced by feature code that is changed individually in each member without considering other family members. Hence, as a program family evolves over time, it might no longer be possible to distinguish between common and variable features. One of the imminent activities to address this problem is the history-sensitive recovery of program family's features in the code. This recovery process encompasses the analysis of the evolution history of each family member in order to classify the implementation elements according to their variability nature. In this context, this paper proposes history-sensitive heuristics for the recovery of features in code of degenerate program families. Once the analysis of the family history is carried out, the feature elements are structured as Java project packages; they are intended to separate those elements in terms of their variability degree. The proposed heuristics are supported by a prototype tool called RecFeat. We evaluated the accuracy of the heuristics in the context of 33 versions of 2 industry program families. They presented encouraging results regarding recall measures that ranged from 85% to 100%; whereas the precision measures ranged from 71% to 99%.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {136–145},
numpages = {10},
keywords = {feature recovery, heuristics, program families, software evolution},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3377930.3390215,
author = {Silva, Diego Fernandes da and Okada, Luiz Fernando and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Enhancing search-based product line design with crossover operators},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390215},
doi = {10.1145/3377930.3390215},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA designing has been formulated as a multi-objective optimization problem and successfully solved by a state-of-the-art search-based approach. However, the majority of empirical studies optimize PLA designs without applying one of the fundamental genetic operators: the crossover. An operator for PLA design, named Feature-driven Crossover, was proposed in a previous study. In spite of the promising results, this operator occasionally generated incomplete solutions. To overcome these limitations, this paper aims to enhance the search-based PLA design optimization by improving the Feature-driven Crossover and introducing a novel crossover operator specific for PLA design. The proposed operators were evaluated in two well-studied PLA designs, using three experimental configurations of NSGA-II in comparison with a baseline that uses only mutation operators. Empirical results show the usefulness and efficiency of the presented operators on reaching consistent solutions. We also observed that the two operators complement each other, leading to PLA design solutions with better feature modularization than the baseline experiment.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1250–1258},
numpages = {9},
keywords = {multi-objective evolutionary algorithm, recombination operators, software architecture, software product line},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/2019136.2019178,
author = {Brataas, Gunnar and Jiang, Shanshan and Reichle, Roland and Geihs, Kurt},
title = {Performance property prediction supporting variability for adaptive mobile systems},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019178},
doi = {10.1145/2019136.2019178},
abstract = {A performance property prediction (PPP) method for component-based self-adaptive applications is presented. Such performance properties are required by an adaptation middleware for reasoning about adaptation activities. Our PPP method is based on the Structure and Performance (SP) framework, a conceptually simple, yet powerful performance modelling framework based on matrices. The main contribution of this paper are the integration of SP-based PPP into a comprehensive model- and variability-based adaptation framework for context-aware mobile applications. A meta model for the SP method is described. The framework is demonstrated using a practical example.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {37},
numpages = {8},
keywords = {autonomic computing, mobile systems},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1145/3361146,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Parejo, Jose Antonio and Segura, Sergio and Yao, Xin},
title = {Many-Objective Test Suite Generation for Software Product Lines},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3361146},
doi = {10.1145/3361146},
abstract = {A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {2},
numpages = {46},
keywords = {Software product line, multi-objective optimisation, test prioritisation, test selection}
}

@inproceedings{10.1145/2362536.2362568,
author = {Hofman, Peter and Stenzel, Tobias and Pohley, Thomas and Kircher, Michael and Bermann, Andreas},
title = {Domain specific feature modeling for software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362568},
doi = {10.1145/2362536.2362568},
abstract = {This paper summarizes our experience with introducing feature modeling into a product line for imaging and therapy systems in the Siemens Healthcare Sector. Determining and negotiating the scope in a product line that spans several business units with their own economic goals is challenging. Feature modeling offers a good way to do variability/commonality analysis for complex product lines. A precondition for feature modeling is the identification of all features supporting the product line. To identify these features, we developed a method for systematically deriving a feature model top down based on domain know-how. We call this method domain specific feature modeling. As the primary artifact to describe the problem space, a domain specific feature model additionally improves the requirement understanding for all stakeholders by considerably improving the scoping, traceability, testing, efficiency and transparency of planning activities and making the development efforts easier to estimate. In this paper, we share our experience with domain specific feature modeling in a large platform project and describe the lessons learned. We describe our general approach that can also be used for other domains.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {229–238},
numpages = {10},
keywords = {Scrum, agile, commonality analysis, domain specific feature model, feature dependency diagram, feature modeling, product line, variability analysis},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1007/11554844_7,
author = {Zhang, Weishan and Jarzabek, Stan},
title = {Reuse without compromising performance: industrial experience from RPG software product line for mobile devices},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_7},
doi = {10.1007/11554844_7},
abstract = {It is often believed that reusable solutions, being generic, must necessarily compromise performance. In this paper, we consider a family of Role-Playing Games (RPGs). We analyzed similarities and differences among four RPGs. By applying a reuse technique of XVCL, we built an RPG product line architecture (RPG-PLA) from which we could derive any of the four RPGs. We built into the RPG-PLA a number of performance optimization strategies that could benefit any of the four (and possibly other similar) RPGs. By comparing the original vs. the new RPGs derived from the RPG-PLA, we demonstrated that reuse allowed us to achieve improved performance, both speed and memory utilization, as compared to each game developed individually. At the same time, our solution facilitated rapid development of new games, for new mobile devices, as well as ease of evolving with new features the RPG-PLA and custom games already in use.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {57–69},
numpages = {13},
location = {Rennes, France},
series = {SPLC'05}
}

@inproceedings{10.1145/3350768.3351299,
author = {de Oliveira, Davi Cedraz S. and Bezerra, Carla I. M.},
title = {Development of the Maintainability Index for SPLs Feature Models Using Fuzzy Logic},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351299},
doi = {10.1145/3350768.3351299},
abstract = {The variability of the common features in an Software Product Line (SPL) can be managed by an feature model, an artifact that consist of a tree-shaped diagram, that describe the features identified in the products and the possible relationships between them. Guarantee the quality of the feature model may be essential to ensure that errors do not propagate across all products. The process of evaluating the quality of a product or artifact can be done using measures, which may reflect the characteristics, sub-characteristics or attributes of quality. However, the isolated values of each measure do not allow access to a whole quality of the feature model, since most of the measures cover several specific aspects that are not correlated. In this context, this paper proposes the aggregation of measures in order to evaluate the maintainability of the feature model in SPL. We aim to investigate how to aggregate these measures and access the respective sub-characteristics by means of a single aggregate value that has the same available information as a set of measures. For this, we have used the theory of Fuzzy Logic as a technique for aggregation of these measures. The new aggregate measure represents the maintainability index of a feature models (MIFM) was obtained. Moreover, to evaluate the MIFM, we applied it to a set of models. It was verified that the aggregate measure obtained allows to measure if a feature models has a high or low maintainability index, supporting the domain engineer in the evaluation of the maintenance of the feature model in a faster and more precise way.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {357–366},
numpages = {10},
keywords = {Feature Models, Fuzzy Logic, Measures, Quality Evaluation, Software Product Line},
location = {Salvador, Brazil},
series = {SBES '19}
}

@article{10.1007/s10664-014-9359-z,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: proposing theories from a case study},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9359-z},
doi = {10.1007/s10664-014-9359-z},
abstract = {In the software product line research, product variants typically differ by their functionality and quality attributes are not purposefully varied. The goal is to study purposeful performance variability in software product lines, in particular, the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The research method was a theory-building case study that was augmented with a systematic literature review. The case was a mobile network base station product line with capacity variability. The data collection, analysis and theorizing were conducted in several stages: the initial case study results were augmented with accounts from the literature. We constructed three theoretical models to explain and characterize performance variability in software product lines: the models aim to be generalizable beyond the single case. The results describe capacity variability in a base station product line. Thereafter, theoretical models of performance variability in software product lines in general are proposed. Performance variability is motivated by customer needs and characteristics, by trade-offs and by varying operating environment constraints. Performance variability can be realized by hardware or software means; moreover, the software can either realize performance differences in an emergent way through impacts from other variability or by utilizing purposeful varying design tactics. The results point out two differences compared with the prevailing literature. Firstly, when the customer needs and characteristics enable price differentiation, performance may be varied even with no trade-offs or production cost differences involved. Secondly, due to the dominance of feature modeling, the literature focuses on the impact management realization. However, performance variability can be realized through purposeful design tactics to downgrade the available software resources and by having more efficient hardware.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1623–1669},
numpages = {47},
keywords = {Case study, Software architecture, Software product line, Variability}
}

@article{10.1007/s00766-014-0201-3,
author = {Lung, Chung-Horng and Balasubramaniam, Balasangar and Selvarajah, Kamalachelva and Elankeswaran, Poopalasingham and Gopalasundaram, Umatharan},
title = {On building architecture-centric product line architecture},
year = {2015},
issue_date = {September 2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0201-3},
doi = {10.1007/s00766-014-0201-3},
abstract = {Software architects typically spend a great deal of time and effort exploring uncertainties, evaluating alternatives, and balancing the concerns of stakeholders. Selecting the best architecture to meet both the functional and non-functional requirements is a critical but difficult task, especially at the early stage of software development when there may be many uncertainties. For example, how will a technology match the operational or performance expectations in reality? This paper presents an approach to building architecture-centric product line. The main objective of the proposed approach is to support effective requirements validation and architectural prototyping for the application-level software. Architectural prototyping is practically essential to architecture design and evaluation. However, architectural prototyping practiced in the field mostly is not used to explore alternatives. Effective construction and evaluation of multiple architecture alternatives is one of the critically challenging tasks. The product line architecture advocated in this paper consists of multiple software architecture alternatives, from which the architect can select and rapidly generate a working application prototype. The paper presents a case study of developing a framework that is primarily built with robust architecture patterns in distributed and concurrent computing and includes variation mechanisms to support various applications even in different domains. The development process of the framework is an application of software product line engineering with an aim to effectively facilitate upfront requirements analysis for an application and rapid architectural prototyping to explore and evaluate architecture alternatives.},
journal = {Requir. Eng.},
month = sep,
pages = {301–321},
numpages = {21},
keywords = {Architectural prototyping, Architecture evaluation, Patterns, Requirements validation, Software performance, Software product line}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green software development and research with the HADAS toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {CVL, clafer, energy efficiency, metrics, optimisation, repository, software product line, variability},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.1007/978-3-642-25535-9_29,
author = {Mohabbati, Bardia and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Asadi, Mohsen and Bagheri, Ebrahim and Bo\v{s}kovi\'{c}, Marko},
title = {A quality aggregation model for service-oriented software product lines based on variability and composition patterns},
year = {2011},
isbn = {9783642255342},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25535-9_29},
doi = {10.1007/978-3-642-25535-9_29},
abstract = {Quality evaluation is a challenging task in monolithic software systems. It is even more complex when it comes to Service-Oriented Software Product Lines (SOSPL), as it needs to analyze the attributes of a family of SOA systems. In SOSPL, variability can be planned and managed at the architectural level to develop a software product with the same set of functionalities but different degrees of non-functional quality attribute satisfaction. Therefore, architectural quality evaluation becomes crucial due to the fact that it allows for the examination of whether or not the final product satisfies and guarantees all the ranges of quality requirements within the envisioned scope. This paper addresses the open research problem of aggregating QoS attribute ranges with respect to architectural variability. Previous solutions for quality aggregation do not consider architectural variability for composite services. Our approach introduces variability patterns that can possibly occur at the architectural level of an SOSPL. We propose an aggregation model for QoS computation which takes both variability and composition patterns into account.},
booktitle = {Proceedings of the 9th International Conference on Service-Oriented Computing},
pages = {436–451},
numpages = {16},
keywords = {QoS aggregation, feature modeling, non-functional properties, process family, service variability, service-oriented architecture (SOA), software product line (SPL), variability management},
location = {Paphos, Cyprus},
series = {ICSOC'11}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Automated reasoning, Configuration, Domain-specific languages, Feature modeling, Software product line engineering, Variability modeling, Video testing}
}

@inproceedings{10.5220/0004745201110118,
author = {Oliveira Jr., Edson and M. S. Gimenes, Itana},
title = {Empirical Validation of Product-line Architecture Extensibility Metrics},
year = {2014},
isbn = {9789897580284},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0004745201110118},
doi = {10.5220/0004745201110118},
abstract = {The software product line (PL) approach has been applied as a successful software reuse technique for specificdomains. The SPL architecture (PLA) is one of the most important SPL core assets as it is the abstraction ofthe products that can be generated, and it represents similarities and variabilities of a PL. Its quality attributesanalysis and evaluation can serve as a basis for analyzing the managerial and economical values of a PL. Thisanalysis can be quantitatively supported by metrics. Thus, we proposed metrics for the PLA extensibilityquality attribute. This paper is concerned with the empirical validation of such metrics. As a result of the experimentalwork we can provide evidence that the proposed metrics serve as relevant indicators of extensibilityof PLA by presenting a correlation analysis.},
booktitle = {Proceedings of the 16th International Conference on Enterprise Information Systems - Volume 2},
pages = {111–118},
numpages = {8},
keywords = {Empirical Validation, Extensibility, Metrics, Software Product Line Architecture, Variability Management.},
location = {Lisbon, Portugal},
series = {ICEIS 2014}
}

@inproceedings{10.5555/1753235.1753249,
author = {Montagud, Sonia and Abrah\~{a}o, Silvia},
title = {Gathering current knowledge about quality evaluation in software product lines},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Recently, a number of methods and techniques for assessing the quality of software product lines have been proposed. However, to the best of our knowledge, there is no study which summarizes all the existing evidence about them. This paper presents a systematic review that investigates what methods and techniques have been employed (in the last 10 years) to evaluate the quality of software product lines and how they were employed. A total of 39 research papers have been reviewed from an initial set of 1388 papers. The results show that 25% of the papers reported evaluations at the Design phase of the Domain Engineering phase. The most widely used mechanism for modeling quality attributes was extended feature models and the most evaluated artifact was the base architecture. In addition, the results of the review have identified several research gaps. Specifically, 77% of the papers employed case studies as a "proof of concept" whereas 23% of the papers did not perform any type of validation. Our results are particularly relevant in positioning new research activities and in the selection of quality evaluation methods or techniques that best fit a given purpose.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {91–100},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Configuration Process, Decision Process, Economic Value, Software Product Line, Utility Elicitation}
}

@article{10.1016/j.jss.2018.05.069,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-adaptation of service compositions through product line reconfiguration},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.05.069},
doi = {10.1016/j.jss.2018.05.069},
journal = {J. Syst. Softw.},
month = oct,
pages = {84–105},
numpages = {22},
keywords = {Service composition, Feature model, Software product lines, Self adaptation}
}

@article{10.1145/3088440,
author = {Acher, Mathieu and Lopez-Herrejon, Roberto E. and Rabiser, Rick},
title = {Teaching Software Product Lines: A Snapshot of Current Practices and Challenges},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
url = {https://doi.org/10.1145/3088440},
doi = {10.1145/3088440},
abstract = {Software Product Line (SPL) engineering has emerged to provide the means to efficiently model, produce, and maintain multiple similar software variants, exploiting their common properties, and managing their variabilities (differences). With over two decades of existence, the community of SPL researchers and practitioners is thriving, as can be attested by the extensive research output and the numerous successful industrial projects. Education has a key role to support the next generation of practitioners to build highly complex, variability-intensive systems. Yet, it is unclear how the concepts of variability and SPLs are taught, what are the possible missing gaps and difficulties faced, what are the benefits, and what is the material available. Also, it remains unclear whether scholars teach what is actually needed by industry. In this article, we report on three initiatives we have conducted with scholars, educators, industry practitioners, and students to further understand the connection between SPLs and education, that is, an online survey on teaching SPLs we performed with 35 scholars, another survey on learning SPLs we conducted with 25 students, as well as two workshops held at the International Software Product Line Conference in 2014 and 2015 with both researchers and industry practitioners participating. We build upon the two surveys and the workshops to derive recommendations for educators to continue improving the state of practice of teaching SPLs, aimed at both individual educators as well as the wider community.},
journal = {ACM Trans. Comput. Educ.},
month = oct,
articleno = {2},
numpages = {31},
keywords = {Software product lines, software engineering teaching, software product line teaching, variability modeling}
}

@inproceedings{10.1145/1987875.1987886,
author = {Belategi, Lorea and Sagardui, Goiuria and Etxeberria, Leire},
title = {Model based analysis process for embedded software product lines},
year = {2011},
isbn = {9781450307307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987875.1987886},
doi = {10.1145/1987875.1987886},
abstract = {Nowadays, embedded system development is increasing its complexity dealing with quality, cost and time-to-market among others. Quality attributes are an important issue to consider in embedded software development where time issues may be critical. Development paradigms such as Model Driven Development and Software Product Lines can be an adequate alternative to traditional software development and validation methods due to the characteristics of embedded systems. But for a proper validation and verification based on MARTE model analysis, all variability issues and critical quality attributes that take part in analysis must be properly modelled and managed. Therefore, a model analysis process for Model Driven Embedded Software Product Lines has been defined as some process lacks have been found.},
booktitle = {Proceedings of the 2011 International Conference on Software and Systems Process},
pages = {53–62},
numpages = {10},
keywords = {model based analysis process, model driven development, performance, quality attributes, schedulability, software product line},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSSP '11}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacit\'{u}a, Ricardo and Sep\'{u}lveda, Samuel and Mazo, Ra\'{u}l},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Feature model, Software product lines, Framework, Classification, Models}
}

@inproceedings{10.5555/1753235.1753263,
author = {Than Tun, Thein and Boucher, Quentin and Classen, Andreas and Hubaux, Arnaud and Heymans, Patrick},
title = {Relating requirements and feature configurations: a systematic approach},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {A feature model captures various possible configurations of products within a product family. When configuring a product, several features are selected and composed. Selecting features at the program level has a general limitation of not being able to relate the resulting configuration to its requirements. As a result, it is difficult to decide whether a given configuration of features is optimal. An optimal configuration satisfies all stakeholder requirements and quantitative constraints, while ensuring that there is no extraneous feature in it. In relating requirements and feature configurations, we use the description of the problem world context in which the software is designed to operate as the intermediate description between them. The advantage of our approach is that feature selection can be done at the requirements level, and an optimal program level configuration can be generated from the requirements selected. Our approach is illustrated with a real-life problem of configuring a satellite communication software. The use of an existing tool to support our approach is also discussed.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {201–210},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2737182.2737183,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {Representing and Configuring Security Variability in Software Product Lines},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737182.2737183},
doi = {10.1145/2737182.2737183},
abstract = {In a software product line, security may need to be varied. Consequently, security variability must be managed both from the customer and product line architecture point of view. We utilize design science to build an artifact and a generalized design theory for representing and configuring security and functional variability from the requirements to the architecture in a configurable software product line. An open source web shop product line, Magento, is used as a case example to instantiate and evaluate the contribution. The results indicate that security variability can be represented and distinguished as countermeasures; and that a configurator tool is able to find consistent products as stable models of answer set programs.},
booktitle = {Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {1–10},
numpages = {10},
keywords = {security, software architecture, software product line, variability},
location = {Montr\'{e}al, QC, Canada},
series = {QoSA '15}
}

@article{10.1016/j.jss.2008.07.046,
author = {Eriksson, Magnus and B\"{o}rstler, J\"{u}rgen and Borg, Kjell},
title = {Managing requirements specifications for product lines - An approach and industry case study},
year = {2009},
issue_date = {March, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2008.07.046},
doi = {10.1016/j.jss.2008.07.046},
abstract = {Software product line development has emerged as a leading approach for software reuse. This paper describes an approach to manage natural-language requirements specifications in a software product line context. Variability in such product line specifications is modeled and managed using a feature model. The proposed approach has been introduced in the Swedish defense industry. We present a multiple-case study covering two different product lines with in total eight product instances. These were compared to experiences from previous projects in the organization employing clone-and-own reuse. We conclude that the proposed product line approach performs better than clone-and-own reuse of requirements specifications in this particular industrial context.},
journal = {J. Syst. Softw.},
month = mar,
pages = {435–447},
numpages = {13},
keywords = {Feature model, Natural-language requirements specification, Software product line, Variability management}
}

@inproceedings{10.5555/1753235.1753270,
author = {Slegers, Walter J.},
title = {Building automotive product lines around managed interfaces},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {TomTom is extending its current business of portable navigation devices into the embedded automotive navigation domain. Portable navigation devices have a high pace of innovation and moderate diversity. Automotive devices traditionally have a slower pace of innovation and high diversity. Their integration in the vehicle needs to comply with formal and intrusive automotive requirements.How can both worlds be combined, offering an increased innovation and reduced lead time in the automotive domain? We introduced an architectural decoupling with explicit management of interfaces to support a product line approach with systematic reuse across business units enabling an increase of diversity in these different market segments.This paper describes business, architecture, organization, and process aspects of this approach with special attention to the architecture and the management of interfaces.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {257–264},
numpages = {8},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.cl.2016.09.004,
author = {M\'{e}ndez-Acu\~{n}a, David and Galindo, Jos\'{e} A. and Degueule, Thomas and Combemale, Beno\^{\i}t and Baudry, Beno\^{\i}t},
title = {Leveraging Software Product Lines Engineering in the development of external DSLs},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2016.09.004},
doi = {10.1016/j.cl.2016.09.004},
abstract = {The use of domain-specific languages (DSLs) has become a successful technique in the development of complex systems. Consequently, nowadays we can find a large variety of DSLs for diverse purposes. However, not all these DSLs are completely different; many of them share certain commonalities coming from similar modeling patterns - such as state machines or petri nets - used for several purposes. In this scenario, the challenge for language designers is to take advantage of the commonalities existing among similar DSLs by reusing, as much as possible, formerly defined language constructs. The objective is to leverage previous engineering efforts to minimize implementation from scratch. To this end, recent research in software language engineering proposes the use of product line engineering, thus introducing the notion of language product lines. Nowadays, there are several approaches that result useful in the construction of language product lines. In this article, we report on an effort for organizing the literature on language product line engineering. More precisely, we propose a definition for the life-cycle of language product lines, and we use it to analyze the capabilities of current approaches. In addition, we provide a mapping between each approach and the technological space it supports. HighlightsSurvey on the applicability of software product lines in the construction of DSLs.General life-cycle for language product lines.Mapping current approaches on language product lines and technological spaces.Research map in language product lines engineering.},
journal = {Comput. Lang. Syst. Struct.},
month = nov,
pages = {206–235},
numpages = {30},
keywords = {Domain-specific languages, Software Product Lines Engineering, Software language engineering, Variability management}
}

@article{10.1145/2853073.2853095,
author = {Alebrahim, Azadeh and Fa\ss{}bender, Stephan and Filipczyk, Martin and Goedicke, Michael and Heisel, Maritta and Zdun, Uwe},
title = {Variability for Qualities in Software Architecture},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853095},
doi = {10.1145/2853073.2853095},
abstract = {Variability is a key factor of most systems. While there are many works covering variability in functionality, there is a research gap regarding variability in software qualities. There is an obvious imbalance between the importance of variability in the context of quality attributes, and the intensity of research in this area. To improve this situation, the First International Workshop on VAri- ability for QUalIties in SofTware Architecture (VAQUITA) was held jointly with ECSA 2015 in Cavtat/Dubrovnik, Croatia as a one-day workshop. The goal of VAQUITA was to investigate and stimulate the discourse about the matter of variability, qualities, and software architectures. The workshop featured three research paper presentations, one keynote talk, and two working group discussions. In this workshop report, we summarize the keynote talk and the presented papers. Additionally, we present the results of the working group discussions},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {32–35},
numpages = {4},
keywords = {Software architecture, quality attributes, variability}
}

@inproceedings{10.5220/0006791403830391,
author = {Aouzal, Khadija and Hafiddi, Hatim and Dahchour, Mohamed},
title = {Handling Tenant-Specific Non-Functional Requirements through a Generic SLA},
year = {2018},
isbn = {9789897583001},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0006791403830391},
doi = {10.5220/0006791403830391},
abstract = {In a multi-tenant architecture of a Software as a Service (SaaS) application, one single instance is shared amongdifferent tenants. However, this architectural style supports only the commonalities among tenants and doesnot cope with the variations and the specific context of each tenant. These variations concern either functionalor non-functional properties. In this paper, we deal with non-functional variability in SaaS services in orderto support the different quality levels that a service may have. For that purpose, we propose an approachthat considers Service Level Agreements (SLAs) as Families in terms of Software Product Line Engineering.We define two metamodels: NFVariability metamodel and VariableSLA metamodel. The first one modelsand captures variability in quality attributes of services. The second one models a dynamic and variableSLA. Model-to-model transformations are performed to transform Feature Model (NFVariability metamodelinstance) to Generic SLA (VariableSLA instance) in order to dynamically deal with the tenant-specific nonfunctionalrequirements.},
booktitle = {Proceedings of the 13th International Conference on Evaluation of Novel Approaches to Software Engineering},
pages = {383–391},
numpages = {9},
keywords = {MDE, Non-Functional Variability, QoS Characteristics, SLA., SPLE, SaaS},
location = {Funchal, Madeira, Portugal},
series = {ENASE 2018}
}

@inproceedings{10.1145/2647908.2655981,
author = {Acher, Mathieu and Alf\'{e}rez, Mauricio and Galindo, Jos\'{e} A. and Romenteau, Pierre and Baudry, Benoit},
title = {ViViD: a variability-based tool for synthesizing video sequences},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655981},
doi = {10.1145/2647908.2655981},
abstract = {We present ViViD, a variability-based tool to synthesize variants of video sequences. ViViD is developed and used in the context of an industrial project involving consumers and providers of video processing algorithms. The goal is to synthesize synthetic video variants with a wide range of characteristics to then test the algorithms. We describe the key components of ViViD (1) a variability language and an environment to model what can vary within a video sequence; (2) a reasoning back-end to generate relevant testing configurations; (3) a video synthesizer in charge of producing variants of video sequences corresponding to configurations. We show how ViViD can synthesize realistic videos with different characteristics such as luminances, vehicles and persons that cover a diversity of testing scenarios.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {143–147},
numpages = {5},
keywords = {T-wise, combinatorial interaction testing, multimedia, prioritization, variability modeling, video generation},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/1621607.1621633,
author = {Sanen, Frans and Truyen, Eddy and Joosen, Wouter},
title = {Mapping problem-space to solution-space features: a feature interaction approach},
year = {2009},
isbn = {9781605584942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1621607.1621633},
doi = {10.1145/1621607.1621633},
abstract = {Mapping problem-space features into solution-space features is a fundamental configuration problem in software product line engineering. A configuration problem is defined as generating the most optimal combination of software features given a requirements specification and given a set of configuration rules. Current approaches however provide little support for expressing complex configuration rules between problem and solution space that support incomplete requirements specifications. In this paper, we propose an approach to model complex configuration rules based on a generalization of the concept of problem-solution feature interactions. These are interactions between solution-space features that only arise in specific problem contexts. The use of an existing tool to support our approach is also discussed: we use the DLV answer set solver to express a particular configuration problem as a logic program whose answer set corresponds to the optimal combinations of solution-space features. We motivate and illustrate our approach with a case study in the field of managing dynamic adaptations in distributed software, where the goal is to generate an optimal protocol for accommodating a given adaptation.},
booktitle = {Proceedings of the Eighth International Conference on Generative Programming and Component Engineering},
pages = {167–176},
numpages = {10},
keywords = {DLV, configuration knowledge, default logic, distributed runtime adaptation, problem-solution feature interactions, software product line engineering},
location = {Denver, Colorado, USA},
series = {GPCE '09}
}

@inproceedings{10.1145/2771783.2771808,
author = {Tan, Tian Huat and Xue, Yinxing and Chen, Manman and Sun, Jun and Liu, Yang and Dong, Jin Song},
title = {Optimizing selection of competing features via feedback-directed evolutionary algorithms},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771808},
doi = {10.1145/2771783.2771808},
abstract = {Software that support various groups of customers usually require complicated configurations to attain different functionalities. To model the configuration options, feature model is proposed to capture the commonalities and competing variabilities of the product variants in software family or Software Product Line (SPL). A key challenge for deriving a new product is to find a set of features that do not have inconsistencies or conflicts, yet optimize multiple objectives (e.g., minimizing cost and maximizing number of features), which are often competing with each other. Existing works have attempted to make use of evolutionary algorithms (EAs) to address this problem. In this work, we incorporated a novel feedback-directed mechanism into existing EAs. Our empirical results have shown that our method has improved noticeably over all unguided version of EAs on the optimal feature selection. In particular, for case studies in SPLOT and LVAT repositories, the feedback-directed Indicator-Based EA (IBEA) has increased the number of correct solutions found by 72.33% and 75%, compared to unguided IBEA. In addition, by leveraging a pre-computed solution, we have found 34 sound solutions for Linux X86, which contains 6888 features, in less than 40 seconds.},
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {246–256},
numpages = {11},
keywords = {SAT solvers, Software product line, evolutionary algorithms},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.5220/0004973404600465,
author = {Florencio da Silva, Jackson Raniel and Soares de Melo Filho, Aloisio and Cardoso Garcia, Vinicius},
title = {Toward a QoS Based Run-time Reconfiguration in Service-oriented Dynamic Software Product Lines},
year = {2014},
isbn = {9789897580284},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0004973404600465},
doi = {10.5220/0004973404600465},
abstract = {Ford invented the product line that makes possible to mass produce by reducing the delivery time and production costs. Regarding the software industry, this, roughly presents both a manufacturing and mass production that generates products that are denoted as individual software and standard software (Pohl et al., 2005): a clear influence of Fordism in the development paradigm of Software Product Lines (SPL). However, this development paradigm was not designed to support user requirements changes at run-time. Faced with this problem, the academy has developed and proposed the Dynamic Software Product Lines (DSPL) (Hallsteinsen et al., 2008) paradigm. Considering this scenario, we objective contribute to DSPL field presenting a new way of thinking which DSPL features should be connected at run-time to a product based on an analysis of quality attributes in service levels specified by the user. In order to validate the proposed approach we tested it on a context-aware DSPL. At the end of the exploratory validation we can observe the effectiveness of the proposed approach in the DSPL which it was applied. However, it is necessary to perform another studies in order to achieve statistical evidences of this effectiveness.},
booktitle = {Proceedings of the 16th International Conference on Enterprise Information Systems - Volume 2},
pages = {460–465},
numpages = {6},
keywords = {DSPL, Dynamic Software Product Line., SOA, SPL, Software Product Line},
location = {Lisbon, Portugal},
series = {ICEIS 2014}
}

@article{10.1016/j.jss.2018.07.054,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.054},
doi = {10.1016/j.jss.2018.07.054},
journal = {J. Syst. Softw.},
month = oct,
pages = {511–532},
numpages = {22},
keywords = {Extended product line, Product configuration, Systematic literature review}
}

@inproceedings{10.1109/SPLC.2008.37,
author = {Etxeberria, Leire and Sagardui, Goiuria},
title = {Variability Driven Quality Evaluation in Software Product Lines},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.37},
doi = {10.1109/SPLC.2008.37},
abstract = {Variability is a key aspect in software product lines. Functional variability has been largely studied as a way to obtain all the desired products for a line. Quality variability, less understood and more complex, has not received so much attention by researchers. However, different members of the line may require different levels of a quality attribute. The design phase is a good point to assure that quality attributes requirements are met within the product line so this means paying attention to software architecture evaluation during Domain Engineering. The quality evaluation in software product lines is much more complicated than in single-systems as products can require different quality levels and the product line can have variability on design that in turn affects quality. The evaluation of all the products of a line is very expensive. Thus, ways of reducing the evaluation efforts are necessary. Herein is presented a method for facilitating cost-effective quality evaluation of a product line taking into consideration variability on quality attributes.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {243–252},
numpages = {10},
keywords = {Quality attributes, evaluation, variability},
series = {SPLC '08}
}

@inproceedings{10.1145/2361999.2362028,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Modeling variability in product lines using domain quality attribute scenarios},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362028},
doi = {10.1145/2361999.2362028},
abstract = {The concept of variability is fundamental in software product lines and a successful implementation of a product line largely depends on how well domain requirements and their variability are specified, managed, and realized. While developing an educational software product line, we identified a lack of support to specify variability in quality concerns. To address this problem we propose an approach to model variability in quality concerns, which is an extension of quality attribute scenarios. In particular, we propose domain quality attribute scenarios, which extend standard quality attribute scenarios with additional information to support specification of variability and deriving product specific scenarios. We demonstrate the approach with scenarios for robustness and upgradability requirements in the educational software product line.},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {135–142},
numpages = {8},
keywords = {quality attributes, scenarios, software product lines, variability},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@article{10.4018/jismd.2012100101,
author = {Asadi, Mohsen and Mohabbati, Bardia and Ga\v{s}evic, Dragan and Bagheri, Ebrahim and Hatala, Marek},
title = {Developing Semantically-Enabled Families of Method-Oriented Architectures},
year = {2012},
issue_date = {October 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012100101},
doi = {10.4018/jismd.2012100101},
abstract = {Method Engineering ME aims to improve software development methods by creating and proposing adaptation frameworks whereby methods are created to provide suitable matches with the requirements of the organization and address project concerns and fit specific situations. Therefore, methods are defined and modularized into components stored in method repositories. The assembly of appropriate methods depends on the particularities of each project, and rapid method construction is inevitable in the reuse and management of existing methods. The ME discipline aims at providing engineering capability for optimizing, reusing, and ensuring flexibility and adaptability of methods; there are three key research challenges which can be observed in the literature: 1 the lack of standards and tooling support for defining, publishing, discovering, and retrieving methods which are only locally used by their providers without been largely adapted by other organizations; 2 dynamic adaptation and assembly of methods with respect to imposed continuous changes or evolutions of the project lifecycle; and 3 variability management in software methods in order to enable rapid and effective construction, assembly and adaptation of existing methods with respect to particular situations. The authors propose semantically-enabled families of method-oriented architecture by applying service-oriented product line engineering principles and employing Semantic Web technologies.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {1–26},
numpages = {26},
keywords = {Method Engineering, Method Oriented Architecture MOA, Semantic Web, Software Development, Software Product Line}
}

@article{10.1016/j.jss.2010.01.048,
author = {Lee, Jaejoon and Muthig, Dirk and Naab, Matthias},
title = {A feature-oriented approach for developing reusable product line assets of service-based systems},
year = {2010},
issue_date = {July, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.01.048},
doi = {10.1016/j.jss.2010.01.048},
abstract = {Service orientation (SO) is a relevant promising candidate for accommodating rapidly changing user needs and expectations. One of the goals of adopting SO is the improvement of reusability, however, the development of service-based system in practice has uncovered several challenging issues, such as how to identify reusable services, how to determine configurations of services that are relevant to users' current product configuration and context, and how to maintain service validity after configuration changes. In this paper, we propose a method that addresses these issues by adapting a feature-oriented product line engineering approach. The method is notable in that it guides developers to identify reusable services at the right level of granularity and to map users' context to relevant service configuration, and it also provides a means to check the validity of services at runtime in terms of invariants and pre/post-conditions of services. Moreover, we propose a heterogeneous style based architecture model for developing such systems.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1123–1136},
numpages = {14},
keywords = {Feature-oriented, Service-based systems, Software architecture, Software architecture styles, Software product line engineering}
}

@inproceedings{10.1145/2814251.2814263,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Th\"{u}m, Thomas},
title = {Using decision rules for solving conflicts in extended feature models},
year = {2015},
isbn = {9781450336864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814251.2814263},
doi = {10.1145/2814251.2814263},
abstract = {Software Product Line Engineering has introduced feature modeling as a domain analysis technique used to represent the variability of software products and decision-making scenarios. We present a model-based transformation approach to solve conflicts among configurations performed by different stakeholders on feature models. We propose the usage of a domain-specific language named CoCo to specify attributes as non-functional properties of features, and to describe business-related decision rules in terms of costs, time, and human resources. These specifications along with the stakeholders' configurations and the feature model are transformed into a constraint programming problem, on which decision rules are executed to find a non-conflicting set of solution configurations that are aligned to business objectives. We evaluate CoCo's compositionality and model complexity simplification while using a set of motivating decision scenarios.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {149–160},
numpages = {12},
keywords = {Domain engineering, conflicting configurations, constraint satisfaction problem, domain-specific language, extended feature model, model transformation chain},
location = {Pittsburgh, PA, USA},
series = {SLE 2015}
}

@article{10.1016/j.eswa.2015.02.020,
author = {Dermeval, Diego and Ten\'{o}rio, Thyago and Bittencourt, Ig Ibert and Silva, Alan and Isotani, Seiji and Ribeiro, M\'{a}rcio},
title = {Ontology-based feature modeling},
year = {2015},
issue_date = {July 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {11},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.02.020},
doi = {10.1016/j.eswa.2015.02.020},
abstract = {We compare two ontology-based feature modeling styles by conducting an experiment.The results show that ontology factor has statistical significance in all metrics.The results show that the ontology based on instances is more flexible.The results show that the ontology based on instances demands less time to change. A software product line (SPL) is a set of software systems that have a particular set of common features and that satisfy the needs of a particular market segment or mission. Feature modeling is one of the key activities involved in the design of SPLs. The feature diagram produced in this activity captures the commonalities and variabilities of SPLs. In some complex domains (e.g., ubiquitous computing, autonomic systems and context-aware computing), it is difficult to foresee all functionalities and variabilities a specific SPL may require. Thus, Dynamic Software Product Lines (DSPLs) bind variation points at runtime to adapt to fluctuations in user needs as well as to adapt to changes in the environment. In this context, relying on formal representations of feature models is important to allow them to be automatically analyzed during system execution. Among the mechanisms used for representing and analyzing feature models, description logic (DL) based approaches demand to be better investigated in DSPLs since it provides capabilities, such as automated inconsistency detection, reasoning efficiency, scalability and expressivity. Ontology is the most common way to represent feature models knowledge based on DL reasoners. Previous works conceived ontologies for feature modeling either based on OWL classes and properties or based on OWL individuals. However, considering change or evolution scenarios of feature models, we need to compare whether a class-based or an individual-based feature modeling style is recommended to describe feature models to support SPLs, and especially its capabilities to deal with changes in feature models, as required by DSPLs. In this paper, we conduct a controlled experiment to empirically compare two approaches based on each one of these modeling styles in several changing scenarios (e.g., add/remove mandatory feature, add/remove optional feature and so on). We measure time to perform changes, structural impact of changes (flexibility) and correctness for performing changes in our experiment. Our results indicate that using OWL individuals requires less time to change and is more flexible than using OWL classes and properties. These results provide insightful assumptions towards the definition of an approach relying on reasoning capabilities of ontologies that can effectively support products reconfiguration in the context of DSPL.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {4950–4964},
numpages = {15},
keywords = {Empirical software engineering, Feature modeling, Ontology, Software product line}
}

@inproceedings{10.1007/978-3-030-79382-1_24,
author = {Munoz, Daniel-Jesus and Gurov, Dilian and Pinto, Monica and Fuentes, Lidia},
title = {Category Theory Framework for Variability Models with Non-functional Requirements},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_24},
doi = {10.1007/978-3-030-79382-1_24},
abstract = {In Software Product Line (SPL) engineering one uses Variability Models (VMs) as input to automated reasoners to generate optimal products according to certain Quality Attributes (QAs). Variability models, however, and more specifically those including numerical features (i.e., NVMs), do not natively support QAs, and consequently, neither do automated reasoners commonly used for variability resolution. However, those satisfiability and optimisation problems have been covered and refined in other relational models such as databases.Category Theory (CT) is an abstract mathematical theory typically used to capture the common aspects of seemingly dissimilar algebraic structures. We propose a unified relational modelling framework subsuming the structured objects of VMs and QAs and their relationships into algebraic categories. This abstraction allows a combination of automated reasoners over different domains to analyse SPLs. The solutions’ optimisation can now be natively performed by a combination of automated theorem proving, hashing, balanced-trees and chasing algorithms. We validate this approach by means of the edge computing SPL tool HADAS.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {397–413},
numpages = {17},
keywords = {Numerical variability model, Feature, Non-functional requirement, Quality attribute, Category theory},
location = {Melbourne, VIC, Australia}
}

@article{10.1016/j.infsof.2013.02.007,
author = {Santos Rocha, Roberto dos and Fantinato, Marcelo},
title = {The use of software product lines for business process management},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.02.007},
doi = {10.1016/j.infsof.2013.02.007},
abstract = {ContextBusiness Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT. ObjectivePresenting the results of a study undertaken to seek and assess PL approaches for BPM through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic PL approaches for BPM. MethodA SLR was conducted with four research questions formulated to evaluate PL approaches for BPM. Results63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that PLs only partially address the BPM lifecycle since the last business process phase is not a current concern on the found approaches. ConclusionsThe found PL approaches for BPM only cover partially the BPM lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic PL proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that PL approaches for BPM are still at an early stage and gaining maturity.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1355–1373},
numpages = {19},
keywords = {BPM, Business process management, PL, Software product line}
}

@article{10.1016/j.jss.2019.01.057,
author = {Kr\"{u}ger, Jacob and Mukelabai, Mukelabai and Gu, Wanzi and Shen, Hui and Hebig, Regina and Berger, Thorsten},
title = {Where is my feature and what is it about? A case study on recovering feature facets},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.057},
doi = {10.1016/j.jss.2019.01.057},
journal = {J. Syst. Softw.},
month = jun,
pages = {239–253},
numpages = {15},
keywords = {Feature location, Marlin, Bitcoin-wallet, Case study, Feature facets, Software product line}
}

@article{10.1016/j.infsof.2017.01.012,
author = {Reinhartz-Berger, Iris and Figl, Kathrin and Haugen, ystein},
title = {Investigating styles in variability modeling},
year = {2017},
issue_date = {July 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {87},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.01.012},
doi = {10.1016/j.infsof.2017.01.012},
abstract = {ContextA common way to represent product lines is with variability modeling. Yet, there are different ways to extract and organize relevant characteristics of variability. Comprehensibility of these models and the ease of creating models are important for the efficiency of any variability management approach. ObjectiveThe goal of this paper is to investigate the comprehensibility of two common styles to organize variability into models hierarchical and constrained where the dependencies between choices are specified either through the hierarchy of the model or as cross-cutting constraints, respectively. MethodWe conducted a controlled experiment with a sample of 90 participants who were students with prior training in modeling. Each participant was provided with two variability models specified in Common Variability Language (CVL) and was asked to answer questions requiring interpretation of provided models. The models included 920 nodes and 819 edges and used the main variability elements. After answering the questions, the participants were asked to create a model based on a textual description. ResultsThe results indicate that the hierarchical modeling style was easier to comprehend from a subjective point of view, but there was also a significant interaction effect with the degree of dependency in the models, that influenced objective comprehension. With respect to model creation, we found that the use of a constrained modeling style resulted in higher correctness of variability models. ConclusionsPrior exposure to modeling style and the degree of dependency among elements in the model determine what modeling style a participant chose when creating the model from natural language descriptions. Participants tended to choose a hierarchical style for modeling situations with high dependency and a constrained style for situations with low dependency. Furthermore, the degree of dependency also influences the comprehension of the variability model.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {81–102},
numpages = {22},
keywords = {Cognitive aspects, Comprehensibility, Empirical research, Feature modeling, Hierarchical modeling, Product line engineering, Textual constraints, Variability modeling}
}

@article{10.1007/s10270-017-0610-0,
author = {Guo, Jianmei and Liang, Jia Hui and Shi, Kai and Yang, Dingyu and Zhang, Jingsong and Czarnecki, Krzysztof and Ganesh, Vijay and Yu, Huiqun},
title = {SMTIBEA: a hybrid multi-objective optimization algorithm for configuring large constrained software product lines},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0610-0},
doi = {10.1007/s10270-017-0610-0},
abstract = {A key challenge to software product line engineering is to explore a huge space of various products and to find optimal or near-optimal solutions that satisfy all predefined constraints and balance multiple often competing objectives. To address this challenge, we propose a hybrid multi-objective optimization algorithm called SMTIBEA that combines the indicator-based evolutionary algorithm (IBEA) with the satisfiability modulo theories (SMT) solving. We evaluated the proposed algorithm on five large, constrained, real-world SPLs. Compared to the state-of-the-art, our approach significantly extends the expressiveness of constraints and simultaneously achieves a comparable performance. Furthermore, we investigate the performance influence of the SMT solving on two evolutionary operators of the IBEA.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1447–1466},
numpages = {20},
keywords = {Constraint solving, Feature models, Multi-objective evolutionary algorithms, Search-based software engineering, Software product lines}
}

@inproceedings{10.1145/1878450.1878475,
author = {Lobato, Luanna Lopes and O'Leary, P\'{a}draig and de Almeida, Eduardo Santana and de Lemos Meira, S\'{\i}lvio Romero},
title = {The importance of documentation, design and reuse in risk management for SPL},
year = {2010},
isbn = {9781450304030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1878450.1878475},
doi = {10.1145/1878450.1878475},
abstract = {Software Product Lines (SPL) is a methodology focusing on systematic software reuse, multiple benefits have been reported as a result of this type of software development. However, establishing a SPL is not a simple task. It is a challenging activity raising many challenges for engineering and management. This research aims to manage the risks during SPL development to provide traceability among them. For this, it is important that the risks are documented and there is a common design related to them. As solution, we identified the strengths and weakness in SPL development and the importance in designing of communication for risk documentation.},
booktitle = {Proceedings of the 28th ACM International Conference on Design of Communication},
pages = {143–150},
numpages = {8},
keywords = {documentation, software product line, web products},
location = {S\~{a}o Carlos, S\~{a}o Paulo, Brazil},
series = {SIGDOC '10}
}

@inproceedings{10.1145/3180155.3180159,
author = {Krieter, Sebastian and Th\"{u}m, Thomas and Schulze, Sandro and Schr\"{o}ter, Reimar and Saake, Gunter},
title = {Propagating configuration decisions with modal implication graphs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180159},
doi = {10.1145/3180155.3180159},
abstract = {Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {898–909},
numpages = {12},
keywords = {configuration, decision propagation, software product line},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3168365.3168375,
author = {Bezerra, Carla I. M. and Andrade, Rossana M. C. and Monteiro, Jos\'{e} M. S. and Cedraz, Davi},
title = {Aggregating Measures using Fuzzy Logic for Evaluating Feature Models},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168375},
doi = {10.1145/3168365.3168375},
abstract = {In the context of Software Product Lines (SPLs), evaluating the quality of a feature model is essential to ensure that errors in the early stages do not spread throughout the SPL. One way to evaluate a feature model is to use measures. However, measures alone are not enough to characterize the feature model quality, because most of them cover specific aspects, such as the number of features. So, there is a need for methods to aggregate measures at the level of quality sub-characteristic or characteristic. In this paper, we aim to investigate how to aggregate measures that have been proposed to evaluate the quality of feature models in SPL. We have used the fuzzy logic theory in order to aggregate these measures. The new aggregated measures can be applied to evaluate different and complex aspects of a feature model, such as: size, stability, flexibility and dynamicity. Moreover, to evaluate the use of the new aggregate measures, we applied them in different feature models. Our findings suggest that aggregate measures can assist the domain engineer in evaluating the maintainability of feature models.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {35–42},
numpages = {8},
keywords = {Feature Models, Fuzzy Logic, Measures, Software Product Line},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.5555/2666064.2666077,
author = {Leitner, Andrea and Wei\ss{}, Reinhold and Kreiner, Christian},
title = {Optimizing problem space representations through domain multi-modeling},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {This work states that there is a need for an optimized problem space representation for heterogeneous domains. We identify two modeling paradigms widely used in practice: Domain-Specific Modeling (DSM) and Feature-Oriented Domain Modeling (FODM). Each modeling paradigm favors different domain characteristics. Especially the fact that software often is embedded either in a system or in a process and, therefore, is strongly influenced by its environment enforces the demand for a combined representation.We propose a concept for a multi-modeling approach based on existing technology. Multi-modeling means the combination of the two main modeling paradigms to represent a heterogeneous domain. The major benefit of the approach is the reduction of representation complexity by optimizing the representation of single subdomains. This will be shown on one representative case study from the automotive domain. Another advantage is the improved stakeholder communication because of familiar notations. A discussion of limitations shows potential for future work.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {49–52},
numpages = {4},
keywords = {automotive software product line, binding times, domain modeling, model-based development, software product line engineering, variant-rich component model},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@inproceedings{10.1145/1842752.1842813,
author = {Galv\~{a}o, Ism\^{e}nia and van den Broek, Pim and Ak\c{s}it, Mehmet},
title = {A model for variability design rationale in SPL},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842813},
doi = {10.1145/1842752.1842813},
abstract = {The management of variability in software product lines goes beyond the definition of variations, traceability and configurations. It involves a lot of assumptions about the variability and related models, which are made by the stakeholders all over the product line but almost never handled explicitly. In order to better manage the design with variability, we must consider the rationale behind its specification. In this paper we present a model for the specification of variability design rationale and its application to the modelling of architectural variability in software product lines.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {332–335},
numpages = {4},
keywords = {design rationale, software architecture, software product line, variability},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@article{10.1016/j.jss.2019.110422,
author = {Edded, Sabrine and Sassi, Sihem Ben and Mazo, Ra\'{u}l and Salinesi, Camille and Ghezala, Henda Ben},
title = {Collaborative configuration approaches in software product lines engineering: A systematic mapping study},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110422},
doi = {10.1016/j.jss.2019.110422},
journal = {J. Syst. Softw.},
month = dec,
numpages = {17},
keywords = {Product lines, Collaborative configuration, Systematic mapping study, Framework}
}

@article{10.1016/j.jss.2013.10.010,
author = {White, Jules and Galindo, Jos\'{e} A. and Saxena, Tripti and Dougherty, Brian and Benavides, David and Schmidt, Douglas C.},
title = {Evolving feature model configurations in software product lines},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {87},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.10.010},
doi = {10.1016/j.jss.2013.10.010},
abstract = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps.},
journal = {J. Syst. Softw.},
month = jan,
pages = {119–136},
numpages = {18},
keywords = {Feature model, Multi-step configuration, Software product line}
}

@inproceedings{10.1145/2892664.2892700,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia and Zschaler, Steffen},
title = {Towards contractual interfaces for reusable functional quality attribute operationalisations},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892700},
doi = {10.1145/2892664.2892700},
abstract = {The quality of a software system can be measured by the extent to which it possesses a desired combination of quality attributes (QAs). While some QAs are achieved implicitly through the interaction of various functional components of the system, others (e.g., security) can be encapsulated in dedicated software components. These QAs are known as functional quality attributes (FQAs). As applications may require different FQAs, and each FQA can be composed of many concerns (e.g., access control and authentication), integrating FQAs is very complex and requires dedicated expertise. Software architects are required to manually define FQA components, identify appropriate points in their architecture where to weave them, and verify that the composition of these FQA components with the other components is correct. This is a complex and error prone process. In our previous work we defined reusable FQAs by encapsulating them as aspectual architecture models that can be woven into a base architecture. So far, the joinpoints for weaving had to be identified manually. This made it difficult for software architects to verify that they have woven all the necessary FQAs into all the right places. In this paper, we address this problem by introducing a notion of contract for FQAs so that the correct application of an FQA (or one of its concerns) can be checked or, alternatively, appropriate binding points can be identified and proposed to the software architect automatically.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {201–205},
numpages = {5},
keywords = {Aspect-Orientation, Model-Driven Development, Quality Attributes, Weaving Patterns},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@inproceedings{10.1145/1985484.1985493,
author = {Chastek, Gary and Donohoe, Patrick and McGregor, John D.},
title = {Commonality and variability analysis for resource constrained organizations},
year = {2011},
isbn = {9781450305846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985484.1985493},
doi = {10.1145/1985484.1985493},
abstract = {This position paper describes our current work in adapting a software product line technique to the constraints of a development organization. We report on applying a commonality and variability analysis with an organization adopting a software product line approach while facing sever resource constraints because of current product development commitments. The immediate focus of the paper is on blending commonality and variability analysis into the organization's existing requirements development process. The longer-term goal of this work is to facilitate the transition to product lines in a minimally intrusive way. The paper describes how the approach was introduced and implemented, and summarizes the benefits achieved and the issues arising from the work to date.},
booktitle = {Proceedings of the 2nd International Workshop on Product Line Approaches in Software Engineering},
pages = {31–34},
numpages = {4},
keywords = {commonality and variability analysis, software product line},
location = {Waikiki, Honolulu, HI, USA},
series = {PLEASE '11}
}

@inproceedings{10.1007/978-3-030-65310-1_20,
author = {Metzger, Andreas and Quinton, Cl\'{e}ment and Mann, Zolt\'{a}n \'{A}d\'{a}m and Baresi, Luciano and Pohl, Klaus},
title = {Feature Model-Guided Online Reinforcement Learning for Self-Adaptive Services},
year = {2020},
isbn = {978-3-030-65309-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65310-1_20},
doi = {10.1007/978-3-030-65310-1_20},
abstract = {A self-adaptive service can maintain its QoS requirements in the presence of dynamic environment changes. To develop a self-adaptive service, service engineers have to create self-adaptation logic encoding when the service should execute which adaptation actions. However, developing self-adaptation logic may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. Online reinforcement learning addresses design time uncertainty by learning suitable adaptation actions through interactions with the environment at runtime. To learn more about its environment, reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens has an impact on the performance of the learning process. We focus on two problems related to how a service’s adaptation actions are explored: (1) Existing solutions randomly explore adaptation actions and thus may exhibit slow learning if there are many possible adaptation actions to choose from. (2) Existing solutions are unaware of service evolution, and thus may explore new adaptation actions introduced during such evolution rather late. We propose novel exploration strategies that use feature models (from software product line engineering) to guide exploration in the presence of many adaptation actions and in the presence of service evolution. Experimental results for a self-adaptive cloud management service indicate an average speed-up of the learning process of 58.8% in the presence of many adaptation actions, and of 61.3% in the presence of service evolution. The improved learning performance in turn led to an average QoS improvement of 7.8% and 23.7% respectively
.},
booktitle = {Service-Oriented Computing: 18th International Conference, ICSOC 2020, Dubai, United Arab Emirates, December 14–17, 2020, Proceedings},
pages = {269–286},
numpages = {18},
keywords = {Adaptation, Reinforcement learning, Feature model, Cloud service},
location = {Dubai, United Arab Emirates}
}

@inproceedings{10.1145/2556624.2556634,
author = {Lytra, Ioanna and Eichelberger, Holger and Tran, Huy and Leyh, Georg and Schmid, Klaus and Zdun, Uwe},
title = {On the interdependence and integration of variability and architectural decisions},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556634},
doi = {10.1145/2556624.2556634},
abstract = {In software product line engineering, the design of assets for reuse and the derivation of software products entails low-level and high-level decision making. In this process, two major types of decisions must be addressed: variability decisions, i.e., decisions made as part of variability management, and architectural decisions, i.e., fundamental decisions to be made during the design of the architecture of the product line or the products. In practice, variability decisions often overlap with or influence architectural decisions. For instance, resolving a variability may enable or prevent some architectural options. This inherent interdependence has not been explicitly and systematically targeted in the literature, and therefore, is mainly resolved in an ad hoc and informal manner today. In this paper, we discuss possible ways how variability and architectural decisions interact, as well as their management and integration in a systematic manner. We demonstrate the integration between the two types of decisions in a motivating case and leverage existing tools for implementing our proposal.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {19},
numpages = {8},
keywords = {architectural decisions, product derivation, software product lines, variability decisions},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1007/s10009-013-0298-6,
author = {Ferrari, Alessio and Spagnolo, Giorgio O. and Martelli, Giacomo and Menabeni, Simone},
title = {From commercial documents to system requirements: an approach for the engineering of novel CBTC solutions},
year = {2014},
issue_date = {November  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {6},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-013-0298-6},
doi = {10.1007/s10009-013-0298-6},
abstract = {Communications-based train control (CBTC) systems are the new frontier of automated train control and operation. Currently developed CBTC platforms are actually very complex systems including several functionalities, and every installed system, developed by a different company, varies in extent, scope, number, and even names of the implemented functionalities. International standards have emerged, but they remain at a quite abstract level, mostly setting terminology. This paper presents the results of an experience in defining a global model of CBTC, by mixing semi-formal modelling and product line engineering. The effort has been based on an in-depth market analysis, not limiting to particular aspects but considering as far as possible the whole picture. The paper also describes a methodology to derive novel CBTC products from the global model, and to define system requirements for the individual CBTC components. To this end, the proposed methodology employs scenario-based requirements elicitation aided with rapid prototyping. To enhance the quality of the requirements, these are written in a constrained natural language (CNL), and evaluated with natural language processing (NLP) techniques. The final goal is to go toward a formal representation of the requirements for CBTC systems. The overall approach is discussed, and the current experience with the implementation of the method is presented. In particular, we show how the presented methodology has been used in practice to derive a novel CBTC architecture.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = nov,
pages = {647–667},
numpages = {21},
keywords = {CBTC, CENELEC, Constrained natural language, Experience report, Formal methods, Product line engineering}
}

@inproceedings{10.1145/3194078.3194082,
author = {Pukhkaiev, Dmytro and G\"{o}tz, Sebastian},
title = {BRISE: energy-efficient benchmark reduction},
year = {2018},
isbn = {9781450357326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194078.3194082},
doi = {10.1145/3194078.3194082},
abstract = {A considerable portion of research activities in computer science heavily relies on the process of benchmarking, e.g., to evaluate a hypothesis in an empirical study. The goal is to reveal how a set of independent variables (factors) influences one or more dependent variables. With a vast number of factors or a high amount of factors' values (levels), this process becomes time- and energy-consuming. Current approaches to lower the benchmarking effort suffer from two deficiencies: (1) they focus on reducing the number of factors and, hence, are inapplicable to experiments with only two factors, but a vast number of levels and (2) being adopted from, e.g., combinatorial optimization they are designed for a different search space structure and, thus, can be very wasteful. This paper provides an approach for benchmark reduction, based on adaptive instance selection and multiple linear regression. We evaluate our approach using four empirical studies, which investigate the effect made by dynamic voltage and frequency scaling in combination with dynamic concurrency throttling on the energy consumption of a computing system (parallel compression, sorting, and encryption algorithms as well as database query processing). Our findings show the effectiveness of the approach. We can save 78% of benchmarking effort, while the result's quality decreases only by 3 pp, due to using only a near-optimal configuration.},
booktitle = {Proceedings of the 6th International Workshop on Green and Sustainable Software},
pages = {23–30},
numpages = {8},
keywords = {active learning, adaptive instance selection, benchmarking, fractional factorial design, non-functional properties},
location = {Gothenburg, Sweden},
series = {GREENS '18}
}

@article{10.1007/s10619-013-7130-x,
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
title = {Automatic optimization of stream programs via source program operator graph transformations},
year = {2013},
issue_date = {December  2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {4},
issn = {0926-8782},
url = {https://doi.org/10.1007/s10619-013-7130-x},
doi = {10.1007/s10619-013-7130-x},
abstract = {Distributed data stream processing is a data analysis paradigm where massive amounts of data produced by various sources are analyzed online within real-time constraints. Execution performance of a stream program/query executed on such middleware is largely dependent on the ability of the programmer to fine tune the program to match the topology of the stream processing system. However, manual fine tuning of a stream program is a very difficult, error prone process that demands huge amounts of programmer time and expertise which are expensive to obtain. We describe an automated process for stream program performance optimization that uses semantic preserving automatic code transformation to improve stream processing job performance. We first identify the structure of the input program and represent the program structure in a Directed Acyclic Graph. We transform the graph using the concepts of Tri-OP Transformation and Bi-Op Transformation. The resulting sample program space is pruned using both empirical as well as profiling information to obtain a ranked list of sample programs which have higher performance compared to their parent program. We successfully implemented this methodology on a prototype stream program performance optimization mechanism called Hirundo. The mechanism has been developed for optimizing SPADE programs which run on System S stream processing run-time. Using five real world applications (called VWAP, CDR, Twitter, Apnoea, and Bargain) we show the effectiveness of our approach. Hirundo was able to identify a 31.1 times higher performance version of the CDR application within seven minutes time on a cluster of 4 nodes.},
journal = {Distrib. Parallel Databases},
month = dec,
pages = {543–599},
numpages = {57},
keywords = {Automatic tuning, Code transformation, Data-intensive computing, Performance optimization, Stream processing}
}

@article{10.1016/j.infsof.2012.02.002,
author = {Holl, Gerald and Gr\"{u}nbacher, Paul and Rabiser, Rick},
title = {A systematic review and an expert survey on capabilities supporting multi product lines},
year = {2012},
issue_date = {August, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.02.002},
doi = {10.1016/j.infsof.2012.02.002},
abstract = {Context: Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented. Objective: The aims of this paper are thus to identify, describe, and classify existing approaches supporting MPLs and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs. Method: Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting MPLs from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry. Results: The paper discusses key research issues in MPLs and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized. Conclusions: We conclude that approaches supporting MPLs need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate MPL approaches.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {828–852},
numpages = {25},
keywords = {Large-scale systems, Multi product lines, Product line engineering, Systematic literature review}
}

@article{10.1016/j.procs.2017.08.206,
author = {Mani, Neel and Helfert, Markus and Pahl, Claus},
title = {A Domain-specific Rule Generation Using Model-Driven Architecture in Controlled Variability Model},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.08.206},
doi = {10.1016/j.procs.2017.08.206},
abstract = {The business environment changes rapidly and needs to adapt to the enterprise business systems must be considered for new types of requirements to accept changes in the business strategies and processes. This raises new challenges that the traditional development approaches cannot always provide a complete solution in an efficient way. However, most of the current proposals for automatic generation are not devised to cope with rapid integration of the changes in the business requirement of end user (stakeholders and customers) resource. Domain-specific Rules constitute a key element for domain specific enterprise application, allowing configuration of changes, and management of the domain constraint within a domain. In this paper, we propose an approach to the development of an automatic generation of the domain-specific rules by using variability feature model and ontology definition of domain model concepts coming from Software product line engineering and Model Driven Architecture. We provide a process approach to generate a domain-specific rule based on the end user requirement.},
journal = {Procedia Comput. Sci.},
month = sep,
pages = {2354–2362},
numpages = {9},
keywords = {Business Process Model, Domain-specific rules, Model Driven Architecture, Rule Generation, Variability Model}
}

@inproceedings{10.1145/3168365.3168377,
author = {Ananieva, Sofia and Klare, Heiko and Burger, Erik and Reussner, Ralf},
title = {Variants and Versions Management for Models with Integrated Consistency Preservation},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168377},
doi = {10.1145/3168365.3168377},
abstract = {Modern software systems are often developed and maintained by describing them in several modeling and programming languages. To reduce complexity and improve understandability of such systems, models represent specific views on the system. These views have semantic interrelations (e.g., by sharing common or dependent information) that need to be kept consistent during evolution of the system. Apart from that, modern systems need to run in many different contexts and be highly configurable to satisfy the demand for fully customizable products. Such variable systems often comprise various dependencies from which inconsistencies may arise. Combining solutions for consistency management with variants and versions management, however, comes with many challenges.In this research-in-progress paper, we introduce the VaVe approach which makes variants and versions management aware of automated consistency preservation in the context of multi-view modeling. We explain core features of the approach and reason about its benefits and limitations.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {3–10},
numpages = {8},
keywords = {Delta-Based Consistency Preservation, Software Product Lines, Variability Management},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3364641.3364656,
author = {Sousa, Amanda and Uch\^{o}a, Anderson and Fernandes, Eduardo and Bezerra, Carla I. M. and Monteiro, Jos\'{e} Maria and Andrade, Rossana M. C.},
title = {REM4DSPL: A Requirements Engineering Method for Dynamic Software Product Lines},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364656},
doi = {10.1145/3364641.3364656},
abstract = {Context: Dynamic Software Product Line (DSPL) is a set of software products capable of self-adapt and configure in runtime. DSPL products have common features (commonalities) and varying features (managed in runtime according to context changes). Objective: DSPL requirements engineering is challenging. Requirements engineers have to carefully plan self-adaptation while eliciting, modeling, and managing variability requirements. This paper introduces a method for DSPL requirements engineering. Method: We relied on empirically-derived activities of DSPL requirements engineering to build our method. We selected techniques and templates used in other domains such as SPL for refinement and incorporation into the method. We asked DSPL experts via a survey on the method applicability. Result: We introduced the Requirements Engineering Method for DSPL (REM4DSPL). Elicitation is guided by supervised discussions. Modeling relies on feature models. Variability Management is tool-assisted and validated via feature model inspection. DSPL experts agreed on the method applicability and suggested improvements. Conclusion: REM4DSPL relies on empirically-derived activities, techniques that have been successfully used by previous work, and templates adapted to the DSPL context. We expect our method to guide requirements engineers in practice.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {129–138},
numpages = {10},
keywords = {Dynamic Software Product Lines, Requirements Engineering},
location = {Fortaleza, Brazil},
series = {SBQS '19}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Evolutionary algorithm, Metaheuristics, Search based software engineering, Software product line, Systematic mapping study}
}

@inproceedings{10.5555/1753235.1753246,
author = {Hendrickson, Scott A. and Wang, Yang and van der Hoek, Andr\'{e} and Taylor, Richard N. and Kobsa, Alfred},
title = {Modeling PLA variation of privacy-enhancing personalized systems},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Privacy-enhancing personalized (PEP) systems address individual users' privacy preferences as well as privacy laws and regulations. Building such systems entails modeling two different domains: (a) privacy constraints as mandated by law, voluntary self-regulation, or users' individual privacy preferences, and modeled by legal professionals, and (b) software architectures as dictated by available software components and modeled by software architects. Both can evolve independently, e.g., as new laws go into effect or new components become available. In prior work, we proposed modeling PEP systems using a product line architecture (PLA). However, with an extensional PLA, these domain models became strongly entangled making it difficult to modify one without inadvertently affecting the other. This paper evaluates an approach towards modeling both domains within an intensional PLA. We find evidence that this results in a clearer separation between the two domain models, making each easier to evolve and maintain.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {71–80},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1109/ECBS.2008.14,
author = {Etxeberria, Leire and Sagardui, Goiuria},
title = {Evaluation of Quality Attribute Variability in Software Product Families},
year = {2008},
isbn = {9780769531410},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ECBS.2008.14},
doi = {10.1109/ECBS.2008.14},
abstract = {Software product family or line is a software engineering paradigm that systematizes reuse. In Software Product Line Engineering, two phases are distinguished: Domain Engineering which is in charge of developing a common infrastructure and assets and Application Engineering which makes use of those assets to generate the products. One of the key aspects of product lines is variability and its management. However, the main focus has been on functional variability and quality variability in software product lines has not received so much attention by researchers. In a product line different members of the line may require different levels of a quality requirement, for instance they could differ in terms of their availability, security, reliability, etc. Due to this variability, quality evaluation in software product lines is much more complicated that in single-systems. One alternative is to evaluate all the products of a line but it is very expensive and ways of reducing evaluation efforts are necessary. In this direction, the paper presents a method for facilitating cost-effective quality evaluation of a product line taking into consideration variability on quality attributes.},
booktitle = {Proceedings of the 15th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems},
pages = {255–264},
numpages = {10},
keywords = {Software product lines, evaluation, quality attributes, variability},
series = {ECBS '08}
}

@article{10.1016/j.entcs.2016.02.007,
author = {Zela Ruiz, Jael and Rubira, Cec\'{\i}lia M.},
title = {Quality of Service Conflict During Web Service Monitoring},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {321},
number = {C},
issn = {1571-0661},
url = {https://doi.org/10.1016/j.entcs.2016.02.007},
doi = {10.1016/j.entcs.2016.02.007},
abstract = {Web services have become one of the most used technologies in service-oriented systems. Its popularity is due to its property to adapt to any context. As a consequence of the increasing number of Web services on the Internet and its important role in many applications today, Web service quality has become a crucial requirement and demanded by service consumers. Terms of quality levels are written between service providers and service consumers to ensure a degree of quality. The use of monitoring tools to control service quality levels is very important. Quality attributes suffer variations in their values during runtime, this is produced by many factors such as a memory leak, deadlock, race data, inconsistent data, etc. However, sometimes monitoring tools can impact negatively affecting the quality of service when they are not properly used and configured, producing possible conflicts between quality attributes. This paper aims to show the impact of monitoring tools over service quality, two of the most important quality attributes - performance and accuracy - were chosen to be monitored. A case study is conducted to present and evaluate the relationship between performance and accuracy over a Web service. As a result, conflict is found between performance and accuracy, where performance was the most affected, because it presented a degradation in its quality level during monitoring.},
journal = {Electron. Notes Theor. Comput. Sci.},
month = mar,
pages = {113–127},
numpages = {15},
keywords = {Accuracy, Conflict, Monitoring Tools, Performance, Quality Attributes, Quality of Service, SOA, Web Services}
}

@inproceedings{10.1145/2797433.2797455,
author = {Alebrahim, Azadeh and Fa\ss{}bender, Stephan and Filipczyk, Martin and Goedicke, Michael and Heisel, Maritta and Zdun, Uwe},
title = {1st Workshop on VAriability for QUalIties in SofTware Architecture (VAQUITA): Workshop Introduction},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797455},
doi = {10.1145/2797433.2797455},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {22},
numpages = {2},
keywords = {Software architecture, quality attributes, variability},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1007/978-3-642-41533-3_24,
author = {Gonz\'{a}lez-Huerta, Javier and Insfr\'{a}n, Emilio and Abrah\~{a}o, Silvia},
title = {Defining and Validating a Multimodel Approach for Product Architecture Derivation and Improvement},
year = {2013},
isbn = {9783642415326},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41533-3_24},
doi = {10.1007/978-3-642-41533-3_24},
abstract = {Software architectures are the key to achieving the non-functional requirements NFRs in any software project. In software product line SPL development, it is crucial to identify whether the NFRs for a specific product can be attained with the built-in architectural variation mechanisms of the product line architecture, or whether additional architectural transformations are required. This paper presents a multimodel approach for quality-driven product architecture derivation and improvement QuaDAI. A controlled experiment is also presented with the objective of comparing the effectiveness, efficiency, perceived ease of use, intention to use and perceived usefulness with regard to participants using QuaDAI as opposed to the Architecture Tradeoff Analysis Method ATAM. The results show that QuaDAI is more efficient and perceived as easier to use than ATAM, from the perspective of novice software architecture evaluators. However, the other variables were not found to be statistically significant. Further replications are needed to obtain more conclusive results.},
booktitle = {Proceedings of the 16th International Conference on Model-Driven Engineering Languages and Systems - Volume 8107},
pages = {388–404},
numpages = {17},
keywords = {Architectural Patterns, Controlled Experiment, Model Transformations, Quality Attributes, Software Product Lines}
}

@inproceedings{10.5555/2666064.2666070,
author = {Sozen, Neset and Merlo, Ettore},
title = {Adapting software product lines for complex certifiable avionics software},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {In avionics, the size and complexity of software-intensive systems increased considerably during recent years. Besides the size and the complexity, certification constraints also had negative impact on the cost and schedule of avionics software projects. Model-Driven Development (MDD) and Software Product Lines Engineering (SPLE) offer an opportunity to improve the avionics software development process, reduce the cost and improve the time to market.Complexity of avionics software and certification constraints pose several challenges to SPLE adoption. Software Product Lines (SPL) framework must provide bi-directional traceability between requirements and low level software assets (e.g. code and test), facilitate production of certification deliverables, allow validation on the target platform and provide code coverage. Also, SPL offer a scheme to manage the complexity of avionics software systems through variability management tools.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {21–24},
numpages = {4},
keywords = {DO-178C, avionics software, certifiable software, model-driven development, software product line},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@inproceedings{10.1145/2000259.2000286,
author = {Cavalcanti, Ricardo de Oliveira and de Almeida, Eduardo Santana and Meira, Silvio R.L.},
title = {Extending the RiPLE-DE process with quality attribute variability realization},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000286},
doi = {10.1145/2000259.2000286},
abstract = {Software product lines engineering is a viable way to achieve the productivity gains desired by companies. Product line architecture must benefit from commonalities among products in the family and enable the variability among them. The aspect of variability in quality attributes has been neglected or ignored by most of the researchers as attention has been mainly put in functional variability. This paper describes an architecture and design process for software product lines that can properly deal with quality attribute variability. The proposed approach enhances the RiPLE-DE process for software product line engineering with activities and guidelines for quality attribute variability. An initial experimental study is presented to characterize and evaluate the proposed process enhancements.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {159–164},
numpages = {6},
keywords = {quality attribute variability, software architecture, software product lines (spl), software reuse},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1145/2851613.2851959,
author = {Noorian, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Quality-centric feature model configuration using goal models},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851959},
doi = {10.1145/2851613.2851959},
abstract = {In software product line engineering, a feature model represents the possible configuration space and can be customized based on the stakeholders' needs. Considering the complexity of feature models in addition to the diversity of the stake-holders' expectations, the configuration process is viewed as a complex optimization problem. In this paper, we propose a holistic approach for the configuration process that seeks to satisfy the stakeholders' requirements as well as the feature models' structural and integrity constraints. Here, we model stakeholders' functional and non-functional needs and their preferences using requirement engineering goal models. We formalize the structure of the feature model, the stake-holders' objectives, and their preferences in the form of an integer linear program to automatically perform feature selection.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1296–1299},
numpages = {4},
keywords = {configuration process, feature model, goal model},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.5555/2820656.2820661,
author = {Segura, Vin\'{\i}cius C. V. B. and Tizzei, Leonardo P. and de F. Ramirez, Jo\~{a}o Paulo and dos Santos, Marcelo N. and Azevedo, Leonardo G. and de G. Cerqueira, Renato F.},
title = {WISE-SPL: bringing multi-tenancy to the weather InSights environment system},
year = {2015},
publisher = {IEEE Press},
abstract = {Weather conditions affect many cities and companies. The WISE (Weather InSights Environment) system serves as a central place to gather and present weather related information for decision makers. It was initially developed to fit a single tenant. Due to a multi-tenant opportunity, WISE is evolving to be deployed on a Cloud environment to support on-demand computing resources and multiple clients. Software product line techniques were applied to model common and variable features of tenants. WISE-SPL enables the derivation of products for each client and also the deployment on Cloud infrastructure. The contribution of this work is a demonstration and discussion of benefits and limitations in applying SPL techniques, following a extractive approach, to build a multi-tenant Cloud application.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {7–10},
numpages = {4},
location = {Florence, Italy},
series = {PLEASE '15}
}

@inproceedings{10.1145/2465478.2465495,
author = {Klatt, Benjamin and K\"{u}ster, Martin},
title = {Improving product copy consolidation by architecture-aware difference analysis},
year = {2013},
isbn = {9781450321266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465478.2465495},
doi = {10.1145/2465478.2465495},
abstract = {Software product lines (SPL) are a well-known concept to efficiently develop product variants. However, migrating customised product copies to a product line is still a labour-intensive challenge due to the required comprehension of differences among the implementations and SPL design decisions. Most existing SPL approaches are focused on forward engineering. Only few aim to handle SPL evolution, but even those lack support of variability reverse engineering, which is necessary for migrating product copies to a product line. In this paper, we present our continued concept on using component architecture information to enhance a variability reverse engineering process. Including this information particularly improves the difference identification as well as the variation point analysis and -aggregation steps. We show how the concept can be applied by providing an illustrating example.},
booktitle = {Proceedings of the 9th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {117–122},
numpages = {6},
keywords = {component architecture, reverse engineering, software product line},
location = {Vancouver, British Columbia, Canada},
series = {QoSA '13}
}

@article{10.1016/j.jss.2019.02.028,
author = {Jakubovski Filho, Helson Luiz and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Preference based multi-objective algorithms applied to the variability testing of software product lines},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.028},
doi = {10.1016/j.jss.2019.02.028},
journal = {J. Syst. Softw.},
month = may,
pages = {194–209},
numpages = {16},
keywords = {Software product line testing, Search-Based software engineering, Preference-Based algorithms}
}

@inproceedings{10.1145/2188286.2188347,
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
title = {Hirundo: a mechanism for automated production of optimized data stream graphs},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188347},
doi = {10.1145/2188286.2188347},
abstract = {Stream programs have to be crafted carefully to maximize the performance gain that can be obtained from stream processing environments. Manual fine tuning of a stream program is a very difficult process which requires considerable amount of programmer time and expertise. In this paper we present Hirundo, which is a mechanism for automatically generating optimized stream programs that are tailored for the environment they run. Hirundo analyzes, identifies the structure of a stream program, and transforms it to many different sample programs with same semantics using the notions of Tri-Operator Transformation, Transformer Blocks, and Operator Blocks Fusion. Then it uses empirical optimization information to identify a small subset of generated sample programs that could deliver high performance. It runs the selected sample programs in the run-time environment for a short period of time to obtain their performance information. Hirundo utilizes these information to output a ranked list of optimized stream programs that are tailored for a particular run-time environment. Hirundo has been developed using Python as a prototype application for optimizing SPADE programs, which run on System S stream processing run-time. Using three example real world stream processing applications we demonstrate effectiveness of our approach, and discuss how well it generalizes for automatic stream program performance optimization.},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {335–346},
numpages = {12},
keywords = {data-intensive computing, fault tolerance, performance optimization, scalability, stream processing},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@inproceedings{10.1145/1147249.1147252,
author = {Kolb, Ronny and Muthig, Dirk},
title = {Making testing product lines more efficient by improving the testability of product line architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147252},
doi = {10.1145/1147249.1147252},
abstract = {Product line engineering is a recent approach to software development that has shown to enable organizations to achieve significant reductions in development and maintenance cost as well as time-to-market of increasingly complex software systems. Yet, the testing process has not kept up with these reductions and the relative cost for testing product lines is actually becoming higher than in traditional single system development. Also, testing often cannot keep pace with accelerated development in product line engineering due to technical and organizational issues. This paper advocates that testing of product lines can be made more efficient and effective by considering testability already during architectural design. It explores the relationship between testability and product line architecture and discusses the importance of high testability for reducing product line testing effort and achieving required coverage criteria. The paper also outlines a systematic approach that will support product line organizations in improving and evaluating testability of product lines at the architectural level.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {22–27},
numpages = {6},
keywords = {architecture, design, evaluation, software product line, testability, testing},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1007/978-3-642-33666-9_46,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel and Walawege, Suneth},
title = {A product line modeling and configuration methodology to support model-based testing: an industrial case study},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_46},
doi = {10.1007/978-3-642-33666-9_46},
abstract = {Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behavioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {726–742},
numpages = {17},
keywords = {UML state machine, aspect-oriented modeling, behavioral variability, model-based testing, product line engineering},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1145/1529282.1529388,
author = {Bure\v{s}, Tom\'{a}\v{s} and Hn\v{e}tynka, Petr and Malohlava, Michal},
title = {Using a product line for creating component systems},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529388},
doi = {10.1145/1529282.1529388},
abstract = {Component systems have become a wide-spread technology and found their place in several application domains. Each component system has its specifics and particularities that reflect its focus and the application domain it is intended for. Although important, the diversity of component systems leads to a number of problems including having different tools for each systems, unnecessary duplication of functionality and problems with integration when several domains are to be targeted. Based on categorization of component application domains, we propose a "meta-component system", which provides a software product line for creating custom component systems. We focus especially on the deployment and execution environment, which is where most diversities are found. We demonstrate the usage of the "meta-component system" and propose how it is to be realized by two core concepts of SOFA 2, namely connector generator and microcomponents.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {501–508},
numpages = {8},
keywords = {component systems, generative programming, product line engineering, runtime environment},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@inproceedings{10.1109/ECBS.2008.53,
author = {Thao, Cheng and Munson, Ethan V. and Nguyen, Tien N.},
title = {Software Configuration Management for Product Derivation in Software Product Families},
year = {2008},
isbn = {9780769531410},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ECBS.2008.53},
doi = {10.1109/ECBS.2008.53},
abstract = {A key process in software product line (SPL) engineering is product derivation, which is the process of building software products from a base set of core assets. During product derivation, the components in both core assets and derived software products are modified to meet needs for different functionality, platforms, quality attributes, etc. However, existing software configuration management (SCM) systems do not sufficiently support the derivation process in SPL. In this paper, we introduce a novel SCM system that is well-suited for product derivation in SPL. Our tool, MoSPL handles version management at the component level via its product versioning and data models. It explicitly manages logical constraints and derivation relations among components in both core assets and derived products, thus enabling the automatic propagation of changes in the core assets to their copies in derived products and vice versa. The system can also detect conflicting changes to different copies of components in software product lines.},
booktitle = {Proceedings of the 15th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems},
pages = {265–274},
numpages = {10},
keywords = {Product Derivation, Software Configuration Management, Software Product Line},
series = {ECBS '08}
}

@inproceedings{10.1145/1944892.1944913,
author = {Nguyen, Tuan and Colman, Alan and Talib, Muhammad Adeel and Han, Jun},
title = {Managing service variability: state of the art and open issues},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944913},
doi = {10.1145/1944892.1944913},
abstract = {In addition to inherited characteristics from software variability, service variability exposes two distinct characteristics that impose certain challenges in variability management. These characteristics are: i) Different types of variability and their inter-relationships; and ii) Dynamic and recursive variability communication among different stakeholders. This paper elaborates these distinct characteristics in detail with a case study. The challenges brought about by these distinct characteristics in managing variability also are highlighted. We present a review of related work in service variability management and briefly propose our ongoing approach to addressing these challenges.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {165–173},
numpages = {9},
keywords = {web services, variability management, variability communication, service variability, service oriented computing},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {multi-objective optimisation, multi-criteria decision analysis, design-space exploration, decision modelling},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@article{10.1007/s10270-015-0459-z,
author = {S\'{a}nchez, Ana B. and Segura, Sergio and Parejo, Jos\'{e} A. and Ruiz-Cort\'{e}s, Antonio},
title = {Variability testing in the wild: the Drupal case study},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0459-z},
doi = {10.1007/s10270-015-0459-z},
abstract = {Variability testing techniques search for effective and manageable test suites that lead to the rapid detection of faults in systems with high variability. Evaluating the effectiveness of these techniques in realistic settings is a must, but challenging due to the lack of variability-intensive systems with available code, automated tests and fault reports. In this article, we propose using the Drupal framework as a case study to evaluate variability testing techniques. First, we represent the framework variability using a feature model. Then, we report on extensive non-functional data extracted from the Drupal Git repository and the Drupal issue tracking system. Among other results, we identified 3392 faults in single features and 160 faults triggered by the interaction of up to four features in Drupal v7.23. We also found positive correlations relating the number of bugs in Drupal features to their size, cyclomatic complexity, number of changes and fault history. To show the feasibility of our work, we evaluated the effectiveness of non-functional data for test case prioritization in Drupal. Results show that non-functional attributes are effective at accelerating the detection of faults, outperforming related prioritization criteria as test case similarity.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {173–194},
numpages = {22},
keywords = {Variability-intensive systems, Variability testing, Test case selection, Test case prioritization, Non-functional properties, Automated testing}
}

@article{10.1016/j.cageo.2014.09.004,
author = {Buccella, Agustina and Cechich, Alejandra and Pol'la, Matias and Arias, Maximiliano and del Socorro Doldan, Maria and Morsan, Enrique},
title = {Marine ecology service reuse through taxonomy-oriented SPL development},
year = {2014},
issue_date = {December 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {73},
number = {C},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2014.09.004},
doi = {10.1016/j.cageo.2014.09.004},
abstract = {Nowadays, reusing software applications encourages researchers and industrials to collaborate in order to increase software quality and to reduce software development costs. However, effective reuse is not easy and only a limited portion of reusable models actually offers effective evidence regarding their appropriateness, usability and/or effectiveness. Focusing reuse on a particular domain, such as marine ecology, allows us to narrow the scope; and along with a systematic approach such as software product line development, helps us to potentially improving reuse. From our experiences developing a subdomain-oriented software product line (SPL for the marine ecology subdomain), in this paper we describe semantic resources created for assisting this development and thus promoting systematic software reuse. The main contributions of our work are focused on the definition of a standard conceptual model for marine ecology applications together with a set of services and guides which assist the process of product derivation. The services are structured in a service taxonomy (as a specialization of the ISO 19119 std) in which we create a new set of categories and services built over a conceptual model for marine ecology applications. We also define and exemplify a set of guides for composing the services of the taxonomy in order to fulfill different functionalities of particular systems in the subdomain. HighlightsSolutions for software reuse for GIS domains by using standard information.Domain-specific taxonomy for supporting the generation of software artifacts.Guides for using geographic services in order to fulfill different GIS functionalities of systems in the domain.Evaluation of the effectiveness of the taxonomy and guides when building an SPL and two derived products.Improvements on time and costs of new GIS products being developed.},
journal = {Comput. Geosci.},
month = dec,
pages = {108–121},
numpages = {14},
keywords = {Software reuse, ISO 19100 standards, Geographic information systems, Domain-specific taxonomies, Domain engineering}
}

@inproceedings{10.1145/1509239.1509259,
author = {Niu, Nan and Easterbrook, Steve},
title = {Concept analysis for product line requirements},
year = {2009},
isbn = {9781605584423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1509239.1509259},
doi = {10.1145/1509239.1509259},
abstract = {Traditional methods characterize a software product line's requirements using either functional or quality criteria. This appears to be inadequate to assess modularity, detect interferences, and analyze trade-offs. We take advantage of both symmetric and asymmetric views of aspects, and perform formal concept analysis to examine the functional and quality requirements of an evolving product line. The resulting concept lattice provides a rich notion which allows remarkable insights into the modularity and interactions of requirements. We formulate a number of problems that aspect-oriented product line requirements engineering should address, and present our solutions according to the concept lattice. We describe a case study applying our approach to analyze a mobile game product line's requirements, and review lessons learned.},
booktitle = {Proceedings of the 8th ACM International Conference on Aspect-Oriented Software Development},
pages = {137–148},
numpages = {12},
keywords = {quality attribute scenarios, product line engineering, functional requirements profiles, formal concept analysis},
location = {Charlottesville, Virginia, USA},
series = {AOSD '09}
}

@inproceedings{10.1007/978-3-662-45234-9_20,
author = {Collet, Philippe},
title = {Domain Specific Languages for Managing Feature Models: Advances and Challenges},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_20},
doi = {10.1007/978-3-662-45234-9_20},
abstract = {Managing multiple and complex feature models is a tedious and error-prone activity in software product line engineering. Despite many advances in formal methods and analysis techniques, the supporting tools and APIs are not easily usable together, nor unified. In this paper, we report on the development and evolution of the Familiar Domain-Specific Language DSL. Its toolset is dedicated to the large scale management of feature models through a good support for separating concerns, composing feature models and scripting manipulations. We overview various applications of Familiar and discuss both advantages and identified drawbacks. We then devise salient challenges to improve such DSL support in the near future.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {273–288},
numpages = {16}
}

@article{10.1145/1183236.1183260,
author = {Sugumaran, Vijayan and Park, Sooyong and Kang, Kyo C.},
title = {Introduction},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183260},
doi = {10.1145/1183236.1183260},
journal = {Commun. ACM},
month = dec,
pages = {28–32},
numpages = {5}
}

@article{10.1016/j.csi.2019.04.011,
author = {Barros-Justo, Jos\'{e} L. and Benitti, Fabiane B.V. and Matalonga, Santiago},
title = {Trends in software reuse research: A tertiary study},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2019.04.011},
doi = {10.1016/j.csi.2019.04.011},
journal = {Comput. Stand. Interfaces},
month = oct,
numpages = {18},
keywords = {Tertiary study, Systematic literature review, Trends in software reuse, Software reuse}
}

@inproceedings{10.5555/1885639.1885675,
author = {Clements, Paul and McGregor, John D. and Bass, Len},
title = {Eliciting and capturing business goals to inform a product line's business case and architecture},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Business goals constitute an important kind of knowledge for a software product line. They inform the product line's business case and they inform its architecture and quality attribute requirements. This paper establishes the connection between business goals and a product line's business case and architecture. It then presents a set of common business goal categories, gleaned from a systematic search of the business literature that can be used to elicit an organization's business goals from key stakeholders. Finally, it presents a well-defined method, which we have tried out in practice, for eliciting and capturing business goals and tying them to quality attribute requirements.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {393–405},
numpages = {13},
keywords = {software product line, quality attribute requirements, product line architecture, business goals, business goal scenario, business case, architecture},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/2556624.2556628,
author = {Lengauer, Philipp and Bitto, Verena and Angerer, Florian and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Where has all my memory gone? determining memory characteristics of product variants using virtual-machine-level monitoring},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556628},
doi = {10.1145/2556624.2556628},
abstract = {Non-functional properties such as memory footprint have recently gained importance in software product line research. However, determining the memory characteristics of individual features and product variants is extremely challenging. We present an approach that supports the monitoring of memory characteristics of individual features at the level of Java virtual machines. Our approach provides extensions to Java virtual machines to track memory allocations and deal-locations of individual features based on a feature-to-code mapping. The approach enables continuous monitoring at the level of features to detect anomalies such as memory leaks, excessive memory consumption, or abnormal garbage collection times in product variants. We provide an evaluation of our approach based on different product variants of the DesktopSearcher product line. Our experiment with different program inputs demonstrates the feasibility of our technique.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {8},
keywords = {monitoring, memory footprint, feature-oriented software development, Java},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1016/j.scico.2017.10.013,
author = {Castro, Thiago and Lanna, Andr and Alves, Vander and Teixeira, Leopoldo and Apel, Sven and Schobbens, Pierre-Yves},
title = {All roads lead to Rome},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2017.10.013},
doi = {10.1016/j.scico.2017.10.013},
abstract = {The formalization of seven strategies for product-line reliability analysis.The first feature-family-product-based strategy for product-line model checking.A general principle for lifting analyses to product lines using ADDs.Proofs that the formalized strategies commute.All strategies proven sound with respect to single-product reliability analysis. Software product line engineering is a means to systematically manage variability and commonality in software systems, enabling the automated synthesis of related programs (products) from a set of reusable assets. However, the number of products in a software product line may grow exponentially with the number of features, so it is practically infeasible to quality-check each of these products in isolation. There is a number of variability-aware approaches to product-line analysis that adapt single-product analysis techniques to cope with variability in an efficient way. Such approaches can be classified along three analysis dimensions (product-based, family-based, and feature-based), but, particularly in the context of reliability analysis, there is no theory comprising both (a) a formal specification of the three dimensions and resulting analysis strategies and (b) proof that such analyses are equivalent to one another. The lack of such a theory hinders formal reasoning on the relationship between the analysis dimensions and derived analysis techniques. We formalize seven approaches to reliability analysis of product lines, including the first instance of a feature-family-product-based analysis in the literature. We prove the formalized analysis strategies to be sound with respect to the probabilistic approach to reliability analysis of a single product. Furthermore, we present a commuting diagram of intermediate analysis steps, which relates different strategies and enables the reuse of soundness proofs between them.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {116–160},
numpages = {45},
keywords = {Verification, Software product lines, Reliability analysis, Product-line analysis, Model checking}
}

@article{10.1007/s10515-010-0076-6,
author = {Dhungana, Deepak and Gr\"{u}nbacher, Paul and Rabiser, Rick},
title = {The DOPLER meta-tool for decision-oriented variability modeling: a multiple case study},
year = {2011},
issue_date = {March     2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0076-6},
doi = {10.1007/s10515-010-0076-6},
abstract = {The variability of a product line is typically defined in models. However, many existing variability modeling approaches are rigid and don't allow sufficient domain-specific adaptations. We have thus been developing a flexible and extensible approach for defining product line variability models. Its main purposes are to guide stakeholders through product derivation and to automatically generate product configurations. Our approach is supported by the DOPLER (  D ecision-  O riented  P roduct  L ine  E ngineering for effective  R euse) meta-tool that allows modelers to specify the types of reusable assets, their attributes, and dependencies for their specific system and context. The aim of this paper is to investigate the suitability of our approach for different domains. More specifically, we explored two research questions regarding the implementation of variability and the utility of DOPLER for variability modeling in different domains. We conducted a multiple case study consisting of four cases in the domains of industrial automation systems and business software. In each of these case studies we analyzed variability implementation techniques. Experts from our industry partners then developed domain-specific meta-models, tool extensions, and variability models for their product lines using DOPLER. The four cases demonstrate the flexibility of the DOPLER approach and the extensibility and adaptability of the supporting meta tool.},
journal = {Automated Software Engg.},
month = mar,
pages = {77–114},
numpages = {38},
keywords = {Product line engineering, Meta-tools, Decision models}
}

@inproceedings{10.5555/2093889.2093921,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Ensan, Faezeh and Ga\v{s}evi\'{c}, Dragan and Mohabbati, Bardia},
title = {Bringing semantics to feature models with SAFMDL},
year = {2011},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of a target domain and through the development of comprehensive and variability-covering domain models. The domain models developed within the software product line development process need to cover all of the possible features and aspects of the target domain. In other words, the domain models often described using feature models should be elaborate representations of the feature space of that domain. In order to operationalize feature-based representations of a software application, appropriate implementation mechanisms need to be employed. In this paper, we propose a Semantic Web-oriented language, called Semantic Annotations for Feature Modeling Description Language (SAFMDL) that provides the means to semantically describe feature models. We will show that using SAFMDL along with Semantic Web Query techniques, we are able to bridge the gap between software product lines and SOA technology. Our proposed work allows software practitioners to use Semantic Web technology to quickly and rapidly develop new software products based on SOA technology from software product lines.},
booktitle = {Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {287–300},
numpages = {14},
location = {Toronto, Ontario, Canada},
series = {CASCON '11}
}

@inproceedings{10.5555/1885639.1885685,
author = {Medeiros, Fl\'{a}vio M. and de Almeida, Eduardo S. and Meira, Silvio R. L.},
title = {SOPLE-DE: an approach to design service-oriented product line architectures},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software reuse is crucial for enterprises interested in software quality and productivity gains. In this context, Software Product Line (SPL) and Service-Oriented Architecture (SOA) are two reuse strategies that share common goals and can be used together to increase reuse and produce service-oriented systems faster, cheaper and customizable to specific customers. In this sense, this work investigates the problem of designing software product lines using service-oriented architectures, and presents a systematic approach to design software product lines based on services. The proposed approach provides guidance to identify, design and document architectural components, services, service compositions and their associated flows. In addition, an initial experimental study performed with the intention of validating and refining the approach is also depicted demonstrating that the proposed solution can be viable.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {456–460},
numpages = {5},
keywords = {software product line (SPL), software architecture and software development processes, service-oriented architecture (SOA)},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/1629716.1629735,
author = {Asadi, Mohsen and Mohabbati, Bardia and Kaviani, Nima and Ga\v{s}evi\'{c}, Dragan and Bo\v{s}kovi\'{c}, Marko and Hatala, Marek},
title = {Model-driven development of families of Service-Oriented Architectures},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629735},
doi = {10.1145/1629716.1629735},
abstract = {The paradigms of Service Oriented Architecture (SOA) and Software Product Line Engineering (SPLE) facilitate the development of families of software-intensive products. Software Product Line practices can be leveraged to support the development of service-oriented applications to promote the reusability of assets throughout the iterative and incremental development of software product families. Such an approach enables various service oriented business processes and software products of the same family to be systematically created and integrated. In this paper, we advocate integration of software product line engineering with model driven engineering to enable a model driven specification of software services, capable of creating software products from a family of software services. Using the proposed method, we aim to provide a consistent view of a composed software system from a higher business administration perspective to lower levels of service implementation and deployment. We demonstrate how Model Driven Engineering (MDE) can help with injecting the set of required commonalities and variabilities of a software product from a high level business process design to the lower levels of service use.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {95–102},
numpages = {8},
keywords = {business process management, semantic web, service-oriented architectures, software product lines},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.4018/IJCAC.2019100105,
author = {Aouzal, Khadija and Hafiddi, Hatim and Dahchour, Mohamed},
title = {Policy-Driven Middleware for Multi-Tenant SaaS Services Configuration},
year = {2019},
issue_date = {Oct 2019},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {4},
issn = {2156-1834},
url = {https://doi.org/10.4018/IJCAC.2019100105},
doi = {10.4018/IJCAC.2019100105},
abstract = {The multi-tenancy architecture allows software-as-a-service applications to serve multiple tenants with a single instance. This is beneficial as it leverages economies of scale. However, it does not cope with the specificities of each tenant and their variability; notably, the variability induced in the required quality levels that differ from a tenant to another. Hence, sharing one single instance hampers the fulfillment of these quality levels for all the tenants and leads to service level agreement violations. In this context, this article proposes a policy-driven middleware that configures the service according to the non-functional requirements of the tenants. The adopted approach combines software product lines engineering and model driven engineering principles. It spans the quality attributes lifecycle, from documenting them to annotating the service components with them as policies, and it enables dynamic configuration according to service level agreements terms of the tenants.},
journal = {Int. J. Cloud Appl. Comput.},
month = oct,
pages = {86–106},
numpages = {21},
keywords = {SPLE, SLA, SaaS, Policy, Non-Functional Variability, Multi-Tenancy, MDE}
}

@inproceedings{10.1007/978-3-540-68073-4_16,
author = {Etxeberria, Leire and Sagardui, Goiuria},
title = {Quality Assessment in Software Product Lines},
year = {2008},
isbn = {9783540680628},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-68073-4_16},
doi = {10.1007/978-3-540-68073-4_16},
abstract = {In a software product line, quality assessment is especially important because an error or an inadequate design decision can be spread into a lot of products. Moreover, in a product line, different members of the line may require different quality attributes. In this paper, a method for quality aware software product line engineering that takes into account the variability of quality aspects and facilitates quality assessment is presented.},
booktitle = {Proceedings of the 10th International Conference on Software Reuse: High Confidence Software Reuse in Large Systems},
pages = {178–181},
numpages = {4},
location = {Beijing, China},
series = {ICSR '08}
}

@article{10.1007/s00766-013-0184-5,
author = {Alf\'{e}rez, Mauricio and Bonif\'{a}cio, Rodrigo and Teixeira, Leopoldo and Accioly, Paola and Kulesza, Uir\'{a} and Moreira, Ana and Ara\'{u}jo, Jo\~{a}o and Borba, Paulo},
title = {Evaluating scenario-based SPL requirements approaches: the case for modularity, stability and expressiveness},
year = {2014},
issue_date = {November  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0184-5},
doi = {10.1007/s00766-013-0184-5},
abstract = {Software product lines (SPL) provide support for productivity gains through systematic reuse. Among the various quality attributes supporting these goals, modularity,
 stability and expressiveness of feature specifications, their composition and configuration knowledge emerge as strategic values in modern software development paradigms. This paper presents a metric-based evaluation aiming at assessing how well the chosen qualities are supported by scenario-based SPL requirements
approaches. The selected approaches for this study span from type of notation (textual or graphical based), style to support variability (annotation or composition based), and specification expressiveness. They are compared using the metrics developed in a set of releases from an exemplar case study. Our major findings indicate that composition-based approaches have greater potential to support modularity and stability, and that quantification mechanisms simplify and increase expressiveness of configuration knowledge and composition specifications.},
journal = {Requir. Eng.},
month = nov,
pages = {355–376},
numpages = {22},
keywords = {Requirements specification, Software product lines, Use scenarios, Variability modeling}
}

@article{10.1007/s00766-013-0183-6,
author = {Lee, Jaejoon and Kang, Kyo C. and Sawyer, Pete and Lee, Hyesun},
title = {A holistic approach to feature modeling for product line requirements engineering},
year = {2014},
issue_date = {November  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0183-6},
doi = {10.1007/s00766-013-0183-6},
abstract = {Requirements engineering (RE) offers the means to discover, model, and manage the requirements of the products that comprise a product line, while software product line engineering (SPLE) offers the means of realizing the products' requirements from a common base of software assets. In practice, however, RE and SPLE have proven to be less complementary than they should. While some RE techniques, particularly goal modeling, support the exploration of alternative solutions, the appropriate solution is typically conditional on context and a large product line may have many product-defining contexts. Thus, scalability and traceability through into product line features are key challenges for RE. Feature modeling, by contrast, has been widely accepted as a way of modeling commonality and variability of products of a product line that may be very complex. In this paper, we propose a goal-driven feature modeling approach that separates a feature space in terms of problem space and solution space features, and establish explicit mappings between them. This approach contributes to reducing the inherent complexity of a mixed-view feature model, deriving key engineering drivers for developing core assets of a product line, and facilitating the quality-based product configuration.},
journal = {Requir. Eng.},
month = nov,
pages = {377–395},
numpages = {19},
keywords = {Product line requirements engineering, Goal modeling, Feature space, Feature modeling viewpoints, Feature modeling}
}

@article{10.1016/j.scico.2013.10.010,
author = {Sousa Ferreira, Gabriel Coutinho and Gaia, Felipe Nunes and Figueiredo, Eduardo and De Almeida Maia, Marcelo},
title = {On the use of feature-oriented programming for evolving software product lines - A comparative study},
year = {2014},
issue_date = {November, 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {93},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2013.10.010},
doi = {10.1016/j.scico.2013.10.010},
abstract = {Feature-oriented programming (FOP) is a programming technique based on composition mechanisms, called refinements. It is often assumed that feature-oriented programming is more suitable than other variability mechanisms for implementing Software Product Lines (SPLs). However, there is no empirical evidence to support this claim. In fact, recent research work found out that some composition mechanisms might degenerate the SPL modularity and stability. However, there is no study investigating these properties focusing on the FOP composition mechanisms. This paper presents quantitative and qualitative analysis of how feature modularity and change propagation behave in the context of two evolving SPLs, namely WebStore and MobileMedia. Quantitative data have been collected from the SPLs developed in three different variability mechanisms: FOP refinements, conditional compilation, and object-oriented design patterns. Our results suggest that FOP requires few changes in source code and a balanced number of added modules, providing better support than other techniques for non-intrusive insertions. Therefore, it adheres closer to the Open-Closed principle. Additionally, FOP seems to be more effective tackling modularity degeneration, by avoiding feature tangling and scattering in source code, than conditional compilation and design patterns. These results are based not only on the variability mechanism itself, but also on careful SPL design. However, the aforementioned results are weaker when the design needs to cope with crosscutting and fine-grained features.},
journal = {Sci. Comput. Program.},
month = nov,
pages = {65–85},
numpages = {21},
keywords = {Variability management, Software product lines, Feature-oriented programming, Design patterns, Conditional compilation}
}

@inproceedings{10.5555/1887899.1887907,
author = {Lung, Chung-Horng and Balasubramaniam, Balasangar and Selvarajah, Kamalachelva and Elankeswaran, Poopalasinkam and Gopalasundaram, Umatharan},
title = {Towards architecture-centric software generation},
year = {2010},
isbn = {3642151132},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Architecture-centric software generation has the potential to support flexible design and large-scale reuse. This paper describes the development of an architecture-centric framework that consists of multiple architecture alternatives, from which the architect can select and generate a working prototype in a top-down manner through a user interface rather than building it from scratch. The framework is primarily built with well-understood design patterns in distributed and concurrent computing. The development process involves extensive domain analysis, variability management, and bottom-up component engineering effort. The framework enables the architect or designer to effectively conduct upfront software architecture analysis and/or rapid architectural prototyping.},
booktitle = {Proceedings of the 4th European Conference on Software Architecture},
pages = {38–52},
numpages = {15},
keywords = {variability management, patterns, generative technique, domain analysis, concurrency, architecture-centric development},
location = {Copenhagen, Denmark},
series = {ECSA'10}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {software product line, service-oriented architecture, service selection, optimization, feature-oriented development},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@article{10.4018/ijeoe.2015010107,
author = {Roy, Adhit and Dutta, Susanta and Roy, Provas Kumar},
title = {Load Frequency Control of Interconnected Power System Using Teaching Learning Based Optimization},
year = {2015},
issue_date = {January 2015},
publisher = {IGI Global},
address = {USA},
volume = {4},
number = {1},
issn = {2160-9500},
url = {https://doi.org/10.4018/ijeoe.2015010107},
doi = {10.4018/ijeoe.2015010107},
abstract = {This paper presents the design and performance analysis of teaching learning based optimization TLBO algorithm based PID controller for load frequency control LFC of an interconnected power system. A two area reheat thermal system equipped with PID controllers which is widely used in literature is considered for the design and analysis purpose. The design objective is to improve the transient performance of the interconnected system. The power system dynamic performance is analyzed based on time response plots achieved with the implementation of designed optimal and sub-optimal LFC regulators in the wake of 1% load disturbance in one of the areas. The results of the TLBO optimized PID controllers on a two area reheat thermal system are compared with those of artificial bee colony ABC and differential evolution DE optimized PID controllers. The TLBO optimized controllers are found to be superior in terms of peak transient deviation, settling times, and dynamic oscillations.},
journal = {Int. J. Energy Optim. Eng.},
month = jan,
pages = {102–117},
numpages = {16},
keywords = {Transient Response Analysis, Teaching Learning Based Optimization, Settling Time, Maximum Overshoot, Load Frequency Control, Differential Evolution}
}

@inproceedings{10.1145/3338906.3338928,
author = {Shahin, Ramy and Chechik, Marsha and Salay, Rick},
title = {Lifting Datalog-based analyses to software product lines},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338928},
doi = {10.1145/3338906.3338928},
abstract = {Applying program analyses to Software Product Lines (SPLs) has been a fundamental research problem at the intersection of Product Line Engineering and software analysis. Different attempts have been made to ”lift” particular product-level analyses to run on the entire product line. In this paper, we tackle the class of Datalog-based analyses (e.g., pointer and taint analyses), study the theoretical aspects of lifting Datalog inference, and implement a lifted inference algorithm inside the Souffl\'{e} Datalog engine. We evaluate our implementation on a set of benchmark product lines. We show significant savings in processing time and fact database size (billions of times faster on one of the benchmarks) compared to brute-force analysis of each product individually.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {39–49},
numpages = {11},
keywords = {Souffl'{e}, Software Product Lines, Program Analysis, Pointer Analysis, Lifting, Doop, Datalog},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/2555523.2555556,
author = {Bagheri, Ebrahim and Ensan, Faezeh},
title = {Light-weight software product lines for small and medium-sized enterprises (SMEs)},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {Product line engineering practices promote the idea of systematic reuse of core assets and have been reported to decrease time-to-market and development costs for new products. However, our recent efforts to transfer our product line engineering knowledge to several of our small and medium-size enterprise industrial partner showed that there are challenges that need to be addressed before core product line engineering ideas can be deployed in SME context. These challenges include upfront investment costs, business traceability, levels of abstraction of functional features and semantic distinction between functional and non-functional software aspects. In order to address these challenges within the context of SMEs, we adopt and extend the behavior-driven development methodology in a way to not only offer agility in practice but also to equip software developers with the means to capture and manage software variability within the behavior-driven development process. We introduce the details of the extended methodology and discuss its advantages and disadvantages in detail.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {311–324},
numpages = {14},
location = {Ontario, Canada},
series = {CASCON '13}
}

@article{10.1007/s00766-003-0166-0,
author = {Thompson, Jeffrey M. and Heimdahl, Mats P.},
title = {Structuring product family requirements for n-dimensional and hierarchical product lines},
year = {2003},
issue_date = {February  2003},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-003-0166-0},
doi = {10.1007/s00766-003-0166-0},
abstract = {The software product-line approach (for software product families) is one of the success stories of software reuse. When applied, it can result in cost savings and increases in productivity. In addition, in safety-critical systems the approach has the potential for reuse of analysis and testing results, which can lead to a safer system. Nevertheless, there are times when it seems like a product family approach should work when, in fact, there are difficulties in properly defining the boundaries of the product family. In this paper, we draw on our experiences in applying the software product-line approach to a family of mobile robots, a family of flight guidance systems, and a family of cardiac pacemakers, as well as case studies done by others to (1) illustrate how domain structure can currently limit applicability of product-line approaches to certain domains and (2) demonstrate our progress towards a solution using a set-theoretic approach to reason about domains of what we call n-dimensional and hierarchical product families.},
journal = {Requir. Eng.},
month = feb,
pages = {42–54},
numpages = {13},
keywords = {Requirements structuring, Requirements reuse, Product line modelling, Product line engineering, Domain Engineering}
}

@inproceedings{10.5555/1885639.1885687,
author = {Belategi, Lorea and Sagardui, Goiuria and Etxeberria, Leire},
title = {MARTE mechanisms to model variability when analyzing embedded software product lines},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Nowadays, embedded systems development is increasing its complexity dealing with quality among others. Model Driven Development (MDD) and Software Product Line (SPL) can be adequate paradigms to traditional development and validation methods. MARTE (UML Profile for Modeling and Analysis of Real-Time and Embedded systems) profile facilitates model analysis thus ensuring quality achievement from models. SPL requires taking into account variability like functional, quality attributes, platform and allocation. Therefore, variability mechanisms of MARTE profile have been studied in order to perform embedded SPL model analysis.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {466–470},
numpages = {5},
keywords = {variability, software product lines, model analysis, embedded software, MARTE},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.jss.2009.10.011,
author = {Sun, Chang-ai and Rossing, Rowan and Sinnema, Marco and Bulanov, Pavel and Aiello, Marco},
title = {Modeling and managing the variability of Web service-based systems},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.10.011},
doi = {10.1016/j.jss.2009.10.011},
abstract = {Web service-based systems are built orchestrating loosely coupled, standardized, and internetworked programs. If on the one hand, Web services address the interoperability issues of modern information systems, on the other hand, they enable the development of software systems on the basis of reuse, greatly limiting the necessity for reimplementation. Techniques and methodologies to gain the maximum from this emerging computing paradigm are in great need. In particular, a way to explicitly model and manage variability would greatly facilitate the creation and customization of Web service-based systems. By variability we mean the ability of a software system to be extended, changed, customized or configured for use in a specific context. We present a framework and related tool suite for modeling and managing the variability of Web service-based systems for design and run-time, respectively. It is an extension of the COVAMOF framework for the variability management of software product families, which was developed at the University of Groningen. Among the novelties and advantages of the approach are the full modeling of variability via UML diagrams, the run-time support, and the low involvement of the user. All of which leads to a great deal of automation in the management of all kinds of variability.},
journal = {J. Syst. Softw.},
month = mar,
pages = {502–516},
numpages = {15},
keywords = {Web services, Variability modeling, Variability management, Service engineering}
}

@article{10.1145/1163514.1178645,
author = {Sinnema, Marco and van der Ven, Jan Salvador and Deelstra, Sybren},
title = {Using variability modeling principles to capture architectural knowledge},
year = {2006},
issue_date = {September 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1163514.1178645},
doi = {10.1145/1163514.1178645},
abstract = {In the field of software architectures, there is an emerging awareness of the importance of architectural decisions. In this view, the architecting process is explained as a decision process, while the design and eventually the software system are seen as the result of this decision process. However, the effects of different alternatives on the quality of the system often remain implicit. In the field of software product families, the same issues arise when configuring products. We propose to use the proven expertise from COVAMOF, a framework for managing variability, to solve the issues that arise when relating quality attributes to architectural decisions.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {5–es},
numpages = {6},
keywords = {quality attributes, architectural knowledge, architectural decisions}
}

@article{10.1007/s10009-012-0250-1,
author = {Wong, Peter Y. and Albert, Elvira and Muschevici, Radu and Proen\c{c}a, Jos\'{e} and Sch\"{a}fer, Jan and Schlatte, Rudolf},
title = {The ABS tool suite: modelling, executing and analysing distributed adaptable object-oriented systems},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-012-0250-1},
doi = {10.1007/s10009-012-0250-1},
abstract = {Modern software systems must support a high degree of variability to accommodate a wide range of requirements and operating conditions. This paper introduces the Abstract Behavioural Specification (ABS) language and tool suite, a comprehensive platform for developing and analysing highly adaptable distributed concurrent software systems. The ABS language has a hybrid functional and object- oriented core, and comes with extensions that support the development of systems that are adaptable to diversified requirements, yet capable to maintain a high level of trustworthiness. Using ABS, system variability is consistently traceable from the level of requirements engineering down to object behaviour. This facilitates temporal evolution, as changes to the required set of features of a system are automatically reflected by functional adaptation of the system's behaviour. The analysis capabilities of ABS stretch from debugging, observing and simulating to resource analysis of ABS models and help ensure that a system will remain dependable throughout its evolutionary lifetime. We report on the experience of using the ABS language and the ABS tool suite in an industrial case study.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {567–588},
numpages = {22},
keywords = {Variability, Tool support, Software product line, Formal modelling and analysis, Feature modelling, Concurrency}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Configurable systems, Machine learning, Software product lines, Systematic literature review}
}

@inproceedings{10.1007/978-3-642-39031-9_10,
author = {Silva, Eduardo and Medeiros, Ana Luisa and Cavalcante, Everton and Batista, Thais},
title = {A lightweight language for software product lines architecture description},
year = {2013},
isbn = {9783642390302},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39031-9_10},
doi = {10.1007/978-3-642-39031-9_10},
abstract = {The architecture description of a software product line (SPL) is essential to make it clear how the architecture realizes the feature model and to represent both the domain and application engineering architectural artefacts. However, most architecture description languages (ADLs) for SPL have limited support regarding variability management and they do not express the relationship between features and the architecture, besides the lack of tools for graphical and textual modelling and a non-clear separation between the domain and application engineering activities. In order to overcome these deficiencies, this paper presents LightPL-ACME, an ADL whose main goal is to be a simple, lightweight language for the SPL architecture description, and enable the association between the architectural specification and the artefacts involved in the SPL development process, including the relationship with the feature model and the representation of both domain and application engineering elements.},
booktitle = {Proceedings of the 7th European Conference on Software Architecture},
pages = {114–121},
numpages = {8},
keywords = {software product lines architectures, architecture description languages, LightPL-ACME, ACME},
location = {Montpellier, France},
series = {ECSA'13}
}

@inproceedings{10.5555/1885639.1885643,
author = {Lee, Kwanwoo and Kang, Kyo C.},
title = {Usage context as key driver for feature selection},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product derivation in software product line engineering starts with selection of variable features manifested in a feature model. Selection of variable features for a particular product, however, is not made arbitrarily. There are various factors affecting feature selection. We experienced that the usage context of a product is often the primary driver for feature selection. In this paper, we propose a model showing how product usage contexts are related to product features, and present a method for developing such a model during the domain engineering process and utilizing it to derive an optimal product configuration during the application engineering process. An elevator control software example is used to illustrate and validate the concept and the method.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {32–46},
numpages = {15},
keywords = {product usage contexts, product derivation, feature modeling, commonality and variability},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/1216262.1216265,
author = {Kabanda, Salah and Adigun, Mathew},
title = {Extending model driven architecture benefits to requirements engineering},
year = {2006},
isbn = {1595935673},
publisher = {South African Institute for Computer Scientists and Information Technologists},
address = {ZAF},
url = {https://doi.org/10.1145/1216262.1216265},
doi = {10.1145/1216262.1216265},
abstract = {This work focuses on developing a requirement engineering model (RSPL) based on a Model Driven Architecture (MDA) and Web-tier Application Framework (WAF), to support automatic and interactive requirements generation when creating families of systems. In realizing the model, two goals were targeted namely (i) to construct a RE model that support automatic transformation of domain features into actor-specific requirements; and (ii) to design and implement an interactive web based tool for requirements engineering. The result obtained is twofold: (i) adopting MDA during RE for a product line reduced costs and development time; (ii) tool implementation based on WAF ensured that support for different client types was possible. In conclusion, the study is a contribution to a recently advocated idea that requirements generation could be model-driven. The result shows that the idea is promising with respect to requirement reuse and improving communication barriers among members of a system development team.},
booktitle = {Proceedings of the 2006 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists on IT Research in Developing Countries},
pages = {22–30},
numpages = {9},
keywords = {model driven architecture, requirement specification model for product lines, software product line engineering, web-tier application framework},
location = {Somerset West, South Africa},
series = {SAICSIT '06}
}

@article{10.1007/s11219-011-9153-8,
author = {Mussbacher, Gunter and Ara\'{u}jo, Jo\~{a}o and Moreira, Ana and Amyot, Daniel},
title = {AoURN-based modeling and analysis of software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9153-8},
doi = {10.1007/s11219-011-9153-8},
abstract = {Software Product Line Engineering concerns itself with domain engineering and application engineering. During domain engineering, the whole product family is modeled with a preferred flavor of feature models and additional models as required (e.g., domain models or scenario-based models). During application engineering, the focus shifts toward a single family member and the configuration of the member's features. Recently, aspectual concepts have been employed to better encapsulate individual features of a Software Product Line (SPL), but the existing body of SPL work does not include a unified reasoning framework that integrates aspect-oriented feature description artifacts with the capability to reason about stakeholders' goals while taking feature interactions into consideration. Goal-oriented SPL approaches have been proposed, but do not provide analysis capabilities that help modelers meet the needs of the numerous stakeholders involved in an SPL while at the same time considering feature interactions. We present an aspect-oriented SPL approach for the requirements phase that allows modelers (a) to capture features, goals, and scenarios in a unified framework and (b) to reason about stakeholders' needs and perform trade-off analyses while considering undesirable interactions that are not obvious from the feature model. The approach is based on the Aspect-oriented User Requirements Notation (AoURN) and helps identify, prioritize, and choose products based on analysis results provided by AoURN editor and analysis tools. We apply the AoURN-based SPL framework to the Via Verde SPL to demonstrate the feasibility of this approach through the selection of a Via Verde product configuration that satisfies stakeholders' needs and results in a high-level, scenario-based specification that is free from undesirable feature interactions.},
journal = {Software Quality Journal},
month = sep,
pages = {645–687},
numpages = {43},
keywords = {User Requirements Notation, Software product lines, Scenario-based requirements engineering, Goal-based requirements engineering, Feature interactions, Aspect-oriented modeling}
}

@inproceedings{10.1145/2430502.2430510,
author = {Lee, Hyesun and Kang, Kyo Chul},
title = {A design feature-based approach to deriving program code from features: a step towards feature-oriented software development},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430510},
doi = {10.1145/2430502.2430510},
abstract = {Feature-oriented software development is a software development paradigm that uses "features" as the first class objects in designing program instead of objects as with the object-orientation. Most of researches on feature-oriented software development attempt to derive program code directly from a feature model, which presents several problems: there is no explicit attempt to embed required quality attributes into code; and features tend to cut across program units and it is difficult to derive program units from the features. To address these problems, a design-feature-based approach is proposed in this paper. Design feature model captures implementation-level design decisions explicitly. It bridges the abstraction gap between features (i.e., functionalities in abstraction) and program units (i.e., concrete implementation). Design features of a design feature model are identified based on required quality attributes. We demonstrated the feasibility of the proposed approach by providing a case study of an arcade game software product line. Initial lessons learned and research agenda are also introduced in the paper.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {5},
numpages = {6},
keywords = {quality attribute, feature-oriented software development, design feature model},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@article{10.1016/j.jss.2013.06.034,
author = {Alf\'{e}rez, G. H. and Pelechano, V. and Mazo, R. and Salinesi, C. and Diaz, D.},
title = {Dynamic adaptation of service compositions with variability models},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.034},
doi = {10.1016/j.jss.2013.06.034},
abstract = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
journal = {J. Syst. Softw.},
month = may,
pages = {24–47},
numpages = {24},
keywords = {Web service composition, Verification, Variability, Models at runtime, Dynamic software product line, Dynamic adaptation, Constraint programming, Autonomic computing}
}

@inproceedings{10.1145/1868433.1868445,
author = {Trujillo, Salvador and Perez, Antonio and Gonzalez, David and Hamid, Brahim},
title = {Towards the integration of advanced engineering paradigms into RCES: raising the issues for the safety-critical model-driven product-line case},
year = {2010},
isbn = {9781450303682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868433.1868445},
doi = {10.1145/1868433.1868445},
abstract = {The conception and design of Resource Constrained Embedded Systems is an inherently complex endeavor. In particular, non-functional requirements from security, dependability and variability are exacerbating this complexity. Recent times have seen a paradigm shift in terms of design through the combination of multiple software engineering paradigms together, namely, Model Driven Engineering and Software Product Line Engineering. Such paradigm shift is changing the way systems are developed nowadays, reducing development time significantly. Embedded systems are a case in point where a range of products for assorted domains such as energy, transportation, automotive, and so on are conceived as a family. However, most of the work so far has been focused on functional parts. The purpose of this talk is to foster some discussion during the workshop on the issues that need to be faced for these techniques to be applicable for Resource Constrained Embedded Systems for which security and dependability are primary requirements.},
booktitle = {Proceedings of the International Workshop on Security and Dependability for Resource Constrained Embedded Systems},
articleno = {9},
numpages = {4},
keywords = {software product lines, resource constrained embedded systems, model-driven development, dependability},
location = {Vienna, Austria},
series = {S&amp;D4RCES '10}
}

@inproceedings{10.1109/PESOS.2009.5068815,
author = {Mietzner, Ralph and Metzger, Andreas and Leymann, Frank and Pohl, Klaus},
title = {Variability modeling to support customization and deployment of multi-tenant-aware Software as a Service applications},
year = {2009},
isbn = {9781424437160},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PESOS.2009.5068815},
doi = {10.1109/PESOS.2009.5068815},
abstract = {More and more companies are offering their software by following the Software as a Service (SaaS) model. The promise of the SaaS model is to exploit economies of scale on the provider side by hosting multiple customers (or tenants) on the same hardware and software infrastructure. However, to attract a significant number of tenants, SaaS applications have to be customizable to fulfill the varying functional and quality requirements of individual tenants. In this paper, we describe how variability modeling techniques from software product line engineering can support SaaS providers in managing the variability of SaaS applications and their requirements. Specifically, we propose using explicit variability models to systematically derive customization and deployment information for individual SaaS tenants. We also demonstrate how variability models could be used to systematically consider information about already deployed SaaS applications for efficiently deploying SaaS applications for new tenants. We illustrate our approach by a running example for a meeting planning application.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Principles of Engineering Service Oriented Systems},
pages = {18–25},
numpages = {8},
series = {PESOS '09}
}

@article{10.1016/j.jss.2009.06.048,
author = {Khurum, Mahvish and Gorschek, Tony},
title = {A systematic review of domain analysis solutions for product lines},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.048},
doi = {10.1016/j.jss.2009.06.048},
abstract = {Domain analysis is crucial and central to software product line engineering (SPLE) as it is one of the main instruments to decide what to include in a product and how it should fit in to the overall software product line. For this reason many domain analysis solutions have been proposed both by researchers and industry practitioners. Domain analysis comprises various modeling and scoping activities. This paper presents a systematic review of all the domain analysis solutions presented until 2007. The goal of the review is to analyze the level of industrial application and/or empirical validation of the proposed solutions with the purpose of mapping maturity in terms of industrial application, as well as to what extent proposed solutions might have been evaluated in terms of usability and usefulness. The finding of this review indicates that, although many new domain analysis solutions for software product lines have been proposed over the years, the absence of qualitative and quantitative results from empirical application and/or validation makes it hard to evaluate the potential of proposed solutions with respect to their usability and/or usefulness for industry adoption. The detailed results of the systematic review can be used by individual researchers to see large gaps in research that give opportunities for future work, and from a general research perspective lessons can be learned from the absence of validation as well as from good examples presented. From an industry practitioner view, the results can be used to gauge as to what extent solutions have been applied and/or validated and in what manner, both valuable as input prior to industry adoption of a domain analysis solution.},
journal = {J. Syst. Softw.},
month = dec,
pages = {1982–2003},
numpages = {22},
keywords = {Usefulness, Usability, Systematic review, Empirical evidence, Domain scoping, Domain modeling, Domain analysis}
}

@inproceedings{10.1145/3241403.3241426,
author = {Plakidas, Konstantinos and Schall, Daniel and Zdun, Uwe},
title = {Model-based support for decision-making in architecture evolution of complex software systems},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241426},
doi = {10.1145/3241403.3241426},
abstract = {Design decision support for software architects in complex industrial software systems, such as software ecosystems and systems-of-systems, which feature extensive reuse of third-party solutions and a variety of deployment options, is still an open challenge. We describe three industrial use cases involving considerable re-architecting, where on-premises solutions were migrated to a cloud-based IoT platforms. Based on these use cases, we analyse the challenges and derive requirements for an architecture knowledge model supporting this process. The presented methodology builds upon existing approaches and proposes a model for the description of extant software applications and the management of domain knowledge. We demonstrate its use to support the evolution and/or composition of software applications in a migration scenario in a systematic and traceable manner.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {21},
numpages = {7},
keywords = {systems-of-systems composition, software variability management, software migration, software architecture evolution, model-based decision support},
location = {Madrid, Spain},
series = {ECSA '18}
}

@article{10.1016/j.jss.2007.06.002,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Industrial validation of COVAMOF},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.06.002},
doi = {10.1016/j.jss.2007.06.002},
abstract = {COVAMOF is a variability management framework for product families that was developed to reduce the number of iterations required during product derivation and to reduce the dependency on experts. In this paper, we present the results of an experiment with COVAMOF in industry. The results show that with COVAMOF, engineers that are not involved in the product family were now capable of deriving the products in 100% of the cases, compared to 29% of the cases without COVAMOF. For experts, the use of COVAMOF reduced the number of iterations by 42%, and the total derivation time by 38%.},
journal = {J. Syst. Softw.},
month = apr,
pages = {584–600},
numpages = {17},
keywords = {Software Variability Management, Product family engineering, Industrial validation}
}

@inproceedings{10.1145/1629716.1629738,
author = {Alf\'{e}rez, Mauricio and Moreira, Ana and Kulesza, Uir\'{a} and Ara\'{u}jo, Jo\~{a}o and Mateus, Ricardo and Amaral, Vasco},
title = {Detecting feature interactions in SPL requirements analysis models},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629738},
doi = {10.1145/1629716.1629738},
abstract = {The consequences of unwanted feature interactions in a Software Product Line (SPL) can range from minor problems to critical software failures. However, detecting feature interactions in reasonably complex model-based SPLs is a non-trivial task. This is due to the often large number of interdependent models that describe the SPL features and the lack of support for analyzing the relationships inside those models. We believe that the early detection of the points, where two or more features interact --- based on the models that describe the behavior of the features ---, is a starting point for the detection of conflicts and inconsistencies between features, and therefore, take an early corrective action.This vision paper foresees a process to find an initial set of points where it is likely to find potential feature interactions in model-based SPL requirements, by detecting: (i) dependency patterns between features using use case models; and (ii) overlapping between use case scenarios modeled using activity models.We focus on requirements models, which are special, since they do not contain many details about the structural components and the interactions between the higher-level abstraction modules of the system. Therefore, use cases and activity models are the means that help us to analyze the functionality of a complex system looking at it from a high level end-user view to anticipate the places where there are potential feature interactions. We illustrate the approach with a home automation SPL and then discuss about its applicability.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {117–123},
numpages = {7},
keywords = {feature interactions, software product lines requirements},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/3194133.3194143,
author = {Olaechea, Rafael and Atlee, Joanne and Legay, Axel and Fahrenberg, Uli},
title = {Trace checking for dynamic software product lines},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194143},
doi = {10.1145/3194133.3194143},
abstract = {A key objective of self-adaptive systems is to continue to provide optimal quality of service when the environment changes. A dynamic software product line (DSPL) can benefit from knowing how its various product variants would have performed (in terms of quality of service) with respect to the recent history of inputs. We propose a family-based analysis that simulates all the product variants of a DSPL simultaneously, at runtime, on recent environmental inputs to obtain an estimate of the quality of service that each one of the product variants would have had, provided it had been executing. We assessed the efficiency of our DSPL analysis compared to the efficiency of analyzing each product individually on three case studies. We obtained mixed results due to the explosion of quality-of-service values for the product variants of a DSPL. After introducing a simple data abstraction on the values of quality-of- service variables, our DSPL analysis is between 1.4 and 7.7 times faster than analyzing the products one at a time.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {69–75},
numpages = {7},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@article{10.1016/j.infsof.2010.03.014,
author = {Alves, Vander and Niu, Nan and Alves, Carina and Valen\c{c}a, George},
title = {Requirements engineering for software product lines: A systematic literature review},
year = {2010},
issue_date = {August, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.03.014},
doi = {10.1016/j.infsof.2010.03.014},
abstract = {Context: Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed. Objective: This paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement. Method: A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009. Results: The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy. Conclusions: Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {806–820},
numpages = {15},
keywords = {Systematic literature review, Software product lines, Requirements engineering}
}

@article{10.1016/j.jss.2017.11.004,
author = {Carvalho, Michelle Larissa Luciano and da Silva, Matheus Lessa Gonalves and Gomes, Gecynalda Soares da Silva and Santos, Alcemir Rodrigues and Machado, Ivan do Carmo and Souza, Magno Lu de Jesus and de Almeida, Eduardo Santana},
title = {On the implementation of dynamic software product lines},
year = {2018},
issue_date = {February 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.11.004},
doi = {10.1016/j.jss.2017.11.004},
abstract = {A set of criteria to characterize mechanisms suitable to implement dynamic variability.A characterization of thirteen DSPL-ready variability mechanisms.Empirical evaluation of OOP and AOP from the perspective of DSPL evolution.Evidence showing that AOP is a feasible strategy to implement DSPL projects. Dynamic Software Product Line (DSPL) engineering is a paradigm aimed at handling adaptations at runtime. An inherent challenge in DSPL engineering is to reduce the design complexity of adaptable software, particularly in terms of evolution. Existing research only recently started to investigate evolution in this field, but does not assess the impact of different implementations under software quality in evolutionary scenarios. This work presents a characterization of thirteen dynamic variability mechanisms. Based on such characterization, we implemented a DSPL using Object-oriented Programming (OOP) mechanisms. From this implementation, we evidenced that DSPL requires changes and extensions to design, in terms of functionality and adaptation capabilities. Since Aspect-oriented Programming (AOP) was well ranked according to characterization and some studies have demonstrated the likely synergies between AOP and DSPL, we decided to compare it with OOP. We empirically evaluated how OOP and AOP could affect source code quality from the viewpoint of an evolving DSPL. As a result, AOP yields better results in terms of size, SoC, cohesion, and coupling measures. Conversely, AOP provides lower change propagation impact. Although the packages in AOP were more susceptible to changes than in OOP, we could indicate that AOP may be a feasible strategy for DSPL implementation.},
journal = {J. Syst. Softw.},
month = feb,
pages = {74–100},
numpages = {27},
keywords = {Variability mechanisms, Software evolution, Evidence-based software engineering, Dynamic software product lines}
}

@inproceedings{10.1145/2897045.2897047,
author = {da Mota Silveira Neto, Paulo Anselmo and de Santana, Taijara Loiola and de Almeida, Eduardo Santana and Cavalcanti, Yguarata Cerqueira},
title = {RiSE events: a testbed for software product lines experimentation},
year = {2016},
isbn = {9781450341769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897045.2897047},
doi = {10.1145/2897045.2897047},
abstract = {Software Product Lines (SPL) demand mature software engineering, planning and reuse, adequate practices of management and development, and also the ability to deal with organizational issues and architectural complexity. Thus, it is important the development of new techniques, tools and methods to deal with SPL complexity required by the variability management. To address this issue, an SPL has been proposed, where the existing variability was implemented by applying conditional compilation. Moreover, no framework was used to develop it, allowing any researcher to use the SPL without losing time learning some framework. In this work, we implemented an SPL test bed containing 34 functional features has 26.457 lines of code, 1493 methods and 496 classes.},
booktitle = {Proceedings of the 1st International Workshop on Variability and Complexity in Software Design},
pages = {12–13},
numpages = {2},
keywords = {variability, test bed, software product lines, security and availability tacticts},
location = {Austin, Texas},
series = {VACE '16}
}

@inproceedings{10.1145/2897695.2897701,
author = {Abilio, Ramon and Vale, Gustavo and Figueiredo, Eduardo and Costa, Heitor},
title = {Metrics for feature-oriented programming},
year = {2016},
isbn = {9781450341776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897695.2897701},
doi = {10.1145/2897695.2897701},
abstract = {Feature-oriented programming (FOP) is a programming technique to implement software product lines based on composition mechanisms called refinements. A software product line is a set of software systems that share a common, managed set of features satisfying the specific needs of a particular market segment. The literature reports various software metrics for software product lines developed using object-oriented and aspect-oriented programming. However, after a literature review, we observed that we lack the definition of FOP-specific metrics. Based on this observation, this paper proposes a set of eight novel metrics for feature-oriented programming. These metrics were derived both from our experience in FOP and from existing software metrics. We demonstrate the applicability of the proposed metrics by applying them to a software product line.},
booktitle = {Proceedings of the 7th International Workshop on Emerging Trends in Software Metrics},
pages = {36–42},
numpages = {7},
keywords = {software quality, software product lines, software metrics, feature-oriented programming},
location = {Austin, Texas},
series = {WETSoM '16}
}

@inproceedings{10.1145/2973839.2973842,
author = {Lima, Crescencio and Chavez, Christina},
title = {A Systematic Review on Metamodels to Support Product Line Architecture Design},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973842},
doi = {10.1145/2973839.2973842},
abstract = {Product Line Architecture (PLA) design is a key activity for developing successful Software Product Line (SPL) projects. PLA design is a difficult task, mostly due to the complexity of the software systems that SPLs deal with, and their variabilities. Metamodels have been used to support the representation of assets that compose a PLA, SPL variability and the relationships among them. The goal of this study is to characterize the use of metamodeling on PLA design, aiming to identify the main characteristics of metamodels, the elements used for PLA and variability representation and trace the evolution of metamodels. We conducted a systematic literature review to identify the primary studies on the use of metamodels in PLA Design. Thirty-five studies that proposed metamodels to support PLA design were selected. The review main findings are: (i) it is difficult to identify the existence of research trends because the number of publication varies and metamodels lack standardization; (ii) several metamodels support feature representation; (iii) the majority of studies addressed variability representation with variation points in UML diagrams; and, (iv) five evolution lines that describe how metamodels evolved over the years were identified.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {13–22},
numpages = {10},
keywords = {Variability, Systematic Literature Review, Software Product Lines, Product Line Architecture, Metamodels},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@article{10.1016/j.jss.2019.06.003,
author = {Capilla, Rafael and Fuentes, Lidia and Lochau, Malte},
title = {Software variability in dynamic environments},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.003},
doi = {10.1016/j.jss.2019.06.003},
journal = {J. Syst. Softw.},
month = oct,
pages = {62–64},
numpages = {3}
}

@article{10.1016/j.infsof.2006.08.001,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Classifying variability modeling techniques},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.001},
doi = {10.1016/j.infsof.2006.08.001},
abstract = {Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {717–739},
numpages = {23},
keywords = {Variability modeling, Variability management, Software product family, Classification}
}

@inproceedings{10.1145/3297280.3297511,
author = {Allian, Ana Paula and Sena, Bruno and Nakagawa, Elisa Yumi},
title = {Evaluating variability at the software architecture level: an overview},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297511},
doi = {10.1145/3297280.3297511},
abstract = {Software architecture are designed for developing software systems needed for a diverse of business goals. Consequently, architecture has to deal with a significant amount of variability in functionality and quality attributes to create different products. Due to this variability, the evaluation in software architectures is much more complex, as different alternatives of systems might be developed leading to an expensive and time consuming task. Several methods and techniques have been proposed to evaluate product line architectures (PLAs) aiming to asses whether or not the architecture will lead to the desired quality attributes. However, there is little consensus on the existing evaluations methods is most suitable for evaluating variability in software architectures, instead of only considering PLAs. Understanding and explicitly evaluating variations in architectures is a cost-effective way of mitigating substantial risk to organizations and their software systems. Therefore, the main contribution of this research work is to present the state of the art about means for evaluating software architectures (including, PLAs, software architectures, reference and enterprise architectures) that contain variability information. We conducted a Systematic Mapping Study (SMS) to provide an overview and insight to practitioners about the most relevant techniques and methods developed for this evaluation. Results indicate that most evaluation techniques assess variability as a quality attribute in PLAs through scenario-based; however, little is known about their real effectiveness as most studies present gaps and lack of evaluation, which difficult the usage of such techniques in an industrial environment.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2354–2361},
numpages = {8},
keywords = {evaluation, software architecture, software variability, systematic mapping study},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1016/j.infsof.2011.11.009,
author = {Angelov, Samuil and Grefen, Paul and Greefhorst, Danny},
title = {A framework for analysis and design of software reference architectures},
year = {2012},
issue_date = {April, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.11.009},
doi = {10.1016/j.infsof.2011.11.009},
abstract = {Context: A software reference architecture is a generic architecture for a class of systems that is used as a foundation for the design of concrete architectures from this class. The generic nature of reference architectures leads to a less defined architecture design and application contexts, which makes the architecture goal definition and architecture design non-trivial steps, rooted in uncertainty. Objective: The paper presents a structured and comprehensive study on the congruence between context, goals, and design of software reference architectures. It proposes a tool for the design of congruent reference architectures and for the analysis of the level of congruence of existing reference architectures. Method: We define a framework for congruent reference architectures. The framework is based on state of the art results from literature and practice. We validate our framework and its quality as analytical tool by applying it for the analysis of 24 reference architectures. The conclusions from our analysis are compared to the opinions of experts on these reference architectures documented in literature and dedicated communication. Results: Our framework consists of a multi-dimensional classification space and of five types of reference architectures that are formed by combining specific values from the multi-dimensional classification space. Reference architectures that can be classified in one of these types have better chances to become a success. The validation of our framework confirms its quality as a tool for the analysis of the congruence of software reference architectures. Conclusion: This paper facilitates software architects and scientists in the inception, design, and application of congruent software reference architectures. The application of the tool improves the chance for success of a reference architecture.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {417–431},
numpages = {15},
keywords = {Software reference architecture, Software product line architecture, Software domain architecture, Software architecture design}
}

@inproceedings{10.1145/3106237.3106252,
author = {Kn\"{u}ppel, Alexander and Th\"{u}m, Thomas and Mennicke, Stephan and Meinicke, Jens and Schaefer, Ina},
title = {Is there a mismatch between real-world feature models and product-line research?},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106252},
doi = {10.1145/3106237.3106252},
abstract = {Feature modeling has emerged as the de-facto standard to compactly capture the variability of a software product line. Multiple feature modeling languages have been proposed that evolved over the last decades to manage industrial-size product lines. However, less expressive languages, solely permitting require and exclude constraints, are permanently and carelessly used in product-line research. We address the problem whether those less expressive languages are sufficient for industrial product lines. We developed an algorithm to eliminate complex cross-tree constraints in a feature model, enabling the combination of tools and algorithms working with different feature model dialects in a plug-and-play manner. However, the scope of our algorithm is limited. Our evaluation on large feature models, including the Linux kernel, gives evidence that require and exclude constraints are not sufficient to express real-world feature models. Hence, we promote that research on feature models needs to consider arbitrary propositional formulas as cross-tree constraints prospectively.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {291–302},
numpages = {12},
keywords = {require constraints, model transformation, feature modeling, expressiveness, exclude constraints, cross-tree constraints, Software product lines},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.5555/2821357.2821367,
author = {Baresi, Luciano and Quinton, Cl\'{e}ment},
title = {Dynamically evolving the structural variability of dynamic software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {A Dynamic Software Product Line (dspl) is a widely used approach to handle variability at runtime, e.g., by activating or deactivating features to adapt the running configuration. With the emergence of highly configurable and evolvable systems, dspls have to cope with the evolution of their structural variability, i.e., the Feature Model (fm) used to derive the configuration. So far, little is known about the evolution of the fm while a configuration derived from this fm is running. In particular, such a dynamic evolution changes the dspl configuration space, which is thus unsynchronized with the running configuration and its adaptation capabilities. In this position paper, we propose and describe an initial architecture to manage the dynamic evolution of dspls and their synchronization. In particular, we explain how this architecture supports the evolution of dspls based on fms extended with cardinality and attributes, which, to the best of our knowledge, has never been addressed yet.},
booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {57–63},
numpages = {7},
location = {Florence, Italy},
series = {SEAMS '15}
}

@inproceedings{10.1145/3180155.3180257,
author = {Xue, Yinxing and Li, Yan-Fu},
title = {Multi-objective integer programming approaches for solving optimal feature selection problem: a new perspective on multi-objective optimization problems in SBSE},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180257},
doi = {10.1145/3180155.3180257},
abstract = {The optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study we first expose the mathematical nature of this problem --- multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution).},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1231–1242},
numpages = {12},
keywords = {multi-objective integer programming (MOIP), multi-objective optimization (MOO), optimal feature selection problem},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.5555/2818754.2818819,
author = {Henard, Christopher and Papadakis, Mike and Harman, Mark and Le Traon, Yves},
title = {Combining multi-objective search and constraint solving for configuring large software product lines},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) feature selection involves the optimization of multiple objectives in a large and highly constrained search space. We introduce SATIBEA, that augments multi-objective search-based optimization with constraint solving to address this problem, evaluating it on five large real-world SPLs, ranging from 1,244 to 6,888 features with respect to three different solution quality indicators and two diversity metrics. The results indicate that SATIBEA statistically significantly outperforms the current state-of-the-art (p &lt; 0.01) for all five SPLs on all three quality indicators and with maximal effect size (\^{A}12 = 1.0). We also present results that demonstrate the importance of combining constraint solving with search-based optimization and the significant improvement SATIBEA produces over pure constraint solving. Finally, we demonstrate the scalability of SATIBEA: within less than half an hour, it finds thousands of constraint-satisfying optimized software products, even for the largest SPL considered in the literature to date.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {517–528},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1007/978-3-642-04211-9_19,
author = {Rossel, Pedro O. and Perovich, Daniel and Bastarrica, Mar\'{\i}a Cecilia},
title = {Reuse of Architectural Knowledge in SPL Development},
year = {2009},
isbn = {9783642042102},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-04211-9_19},
doi = {10.1007/978-3-642-04211-9_19},
abstract = {Software Product Lines (SPL) promote reuse within an application domain in an organized fashion. Preimplemented software components are arranged according to a product line architecture (PLA). Balancing possibly conflicting quality attributes of all potential products makes PLA design a challenging task. Moreover, if quality attributes are part of the variabilities of the SPL, then a unique PLA may result highly inconvenient for particular configurations. We consider the PLA as a set of architectural decisions organized by the features in the Feature Model. A particular product architecture (PA) is defined as the subset of decisions associated to the chosen features for the product. Architectural knowledge is then reused among products and when new features are required in the SPL. Variability at the quality attribute level will impact the style of the resulting architecture, thus choosing different quality features will produce PAs following different styles, even within the same SPL. We use MDE techniques to operationalize this procedure and we illustrate the technique using the case of a Meshing Tool SPL.},
booktitle = {Proceedings of the 11th International Conference on Software Reuse: Formal Foundations of Reuse and Domain Engineering},
pages = {191–200},
numpages = {10},
location = {Falls Church, Virginia},
series = {ICSR '09}
}

@inproceedings{10.1145/1385486.1385488,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Schirmeier, Horst and Sincero, Julio and Apel, Sven and Leich, Thomas and Spinczyk, Olaf and Saake, Gunter},
title = {FAME-DBMS: tailor-made data management solutions for embedded systems},
year = {2008},
isbn = {9781595939647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1385486.1385488},
doi = {10.1145/1385486.1385488},
abstract = {Data management functionality is not only needed in large-scale server systems, but also in embedded systems. Resource restrictions and heterogeneity of hardware, however, complicate the development of data management solutions for those systems. In current practice, this typically leads to the redevelopment of data management because existing solutions cannot be reused and adapted appropriately. In this paper, we present our ongoing work on FAME-DBMS, a research project that explores techniques to implement highly customizable data management solutions, and illustrate how such systems can be created with a software product line approach. With this approach a concrete instance of a DBMS is derived by composing features of the DBMS product line that are needed for a specific application scenario. This product derivation process is getting complex if a large number of features is available. Furthermore, in embedded systems also non-functional properties, e.g., memory consumption, have to be considered when creating a DBMS instance. To simplify the derivation process we present approaches for its automation.},
booktitle = {Proceedings of the 2008 EDBT Workshop on Software Engineering for Tailor-Made Data Management},
pages = {1–6},
numpages = {6},
location = {Nantes, France},
series = {SETMDM '08}
}

@inproceedings{10.1145/2188286.2188304,
author = {Tawhid, Rasha and Petriu, Dorina},
title = {User-friendly approach for handling performance parameters during predictive software performance engineering},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188304},
doi = {10.1145/2188286.2188304},
abstract = {A Software Product Line (SPL) is a set of similar software systems that share a common set of features. Instead of building each product from scratch, SPL development takes advantage of the reusability of the core assets shared among the SPL members. In this work, we integrate performance analysis in the early phases of SPL development process, applying the same reusability concept to the performance annotations. Instead of annotating from scratch the UML model of every derived product, we propose to annotate the SPL model once with generic performance annotations. After deriving the model of a product from the family model by an automatic transformation, the generic performance annotations need to be bound to concrete product-specific values provided by the developer. Dealing manually with a large number of performance annotations, by asking the developer to inspect every diagram in the generated model and to extract these annotations is an error-prone process. In this paper we propose to automate the collection of all generic parameters from the product model and to present them to the developer in a user-friendly format (e.g., a spreadsheet per diagram, indicating each generic parameter together with guiding information that helps the user in providing concrete binding values). There are two kinds of generic parametric annotations handled by our approach: product-specific (corresponding to the set of features selected for the product) and platform-specific (such as device choices, network connections, middleware, and runtime environment). The following model transformations for (a) generating a product model with generic annotations from the SPL model, (b) building the spreadsheet with generic parameters and guiding information, and (c) performing the actual binding are all realized in the Atlas Transformation Language (ATL).},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {109–120},
numpages = {12},
keywords = {uml, spl, performance model, performance completion, model-driven development, marte, atl},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@article{10.1007/s00766-015-0243-1,
author = {Pacheco, C. and Garcia, I. and Calvo-Manzano, J. A. and Arcilla, M.},
title = {Reusing functional software requirements in small-sized software enterprises: a model oriented to the catalog of requirements},
year = {2017},
issue_date = {June      2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {2},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-015-0243-1},
doi = {10.1007/s00766-015-0243-1},
abstract = {Software reuse can be defined as the process of creating software products from the existing ones rather than developing software from scratch. Thus, software reuse is normally proposed to increase software productivity and quality and leads to economic benefits. In this sense, the reuse of software requirements has received important attention because it provides a solid support to develop quality software through obtaining and reusing quality software requirements [i.e., software product line (SPL) approach used in large-sized software enterprises]. However, the small-sized enterprises--which represent up to 85 % of all software organizations in many countries around the world--cannot implement a SPL approach because it does not fit with the context, properties, and complexity of their software projects. Moreover, the software engineering community has not adequately explored a more proper approach in the context of small-sized software enterprises. The use of a software requirements catalog could be this proper approach. In this context, the aim of this paper was to introduce the requirements reuse model for software requirements catalog (RRMSRC). Also, a set of guidelines to perform the main activities defined for reusing functional requirements within small-sized software enterprises is provided. As evidence of its feasibility, RRMSRC has been used in an industrial context, and the obtained results and learned lessons are summarized.},
journal = {Requir. Eng.},
month = jun,
pages = {275–287},
numpages = {13},
keywords = {Small-sized software enterprises, Requirements reuse process, Requirements engineering, Functional requirements catalog}
}

@article{10.1007/s11219-005-4250-1,
author = {Kazman, Rick and Bass, Len and Klein, Mark and Lattanze, Tony and Northrop, Linda},
title = {A Basis for Analyzing Software Architecture Analysis Methods},
year = {2005},
issue_date = {December  2005},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-005-4250-1},
doi = {10.1007/s11219-005-4250-1},
abstract = {A software architecture is a key asset for any organization that builds complex software-intensive systems. Because of an architecture's central role as a project blueprint, organizations should analyze the architecture before committing resources to it. An analysis helps to ensure that sound architectural decisions are made. Over the past decade a large number of architecture analysis methods have been created, and at least two surveys of these methods have been published. This paper examines the criteria for analyzing architecture analysis methods, and suggests a new set of criteria that focus on the essence of what it means to be an architecture analysis method. These criteria could be used to compare methods, to help understand the suitability of a method, or to improve a method. We then examine two methods--the Architecture Tradeoff Analysis Method and Architecture-level Modifiability Analysis--in light of these criteria, and provide some insight into how these methods can be improved.},
journal = {Software Quality Journal},
month = dec,
pages = {329–355},
numpages = {27},
keywords = {software architecture, quality attributes, architecture analysis, analysis methods}
}

@article{10.1016/j.infsof.2021.106620,
author = {Tran, Huynh Khanh Vi and Unterkalmsteiner, Michael and B\"{o}rstler, J\"{u}rgen and Ali, Nauman bin},
title = {Assessing test artifact quality—A tertiary study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106620},
doi = {10.1016/j.infsof.2021.106620},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {22},
keywords = {Quality assurance, Test artifact quality, Test suite quality, Test case quality, Software testing}
}

@article{10.1016/j.cl.2018.05.004,
author = {Combemale, Benoit and Kienzle, J\"{o}rg and Mussbacher, Gunter and Barais, Olivier and Bousse, Erwan and Cazzola, Walter and Collet, Philippe and Degueule, Thomas and Heinrich, Robert and J\'{e}z\'{e}quel, Jean-Marc and Leduc, Manuel and Mayerhofer, Tanja and Mosser, S\'{e}bastien and Sch\"{o}ttle, Matthias and Strittmatter, Misha and Wortmann, Andreas},
title = {Concern-oriented language development (COLD): Fostering reuse in language engineering},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.05.004},
doi = {10.1016/j.cl.2018.05.004},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {139–155},
numpages = {17},
keywords = {Language reuse, Language concern, Domain-specific languages}
}

@article{10.1007/s10270-016-0569-2,
author = {Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Lochau, Malte and Meinicke, Jens and Saake, Gunter},
title = {Effective product-line testing using similarity-based product prioritization},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0569-2},
doi = {10.1007/s10270-016-0569-2},
abstract = {A software product line comprises a family of software products that share a common set of features. Testing an entire product-line product-by-product is infeasible due to the potentially exponential number of products in the number of features. Accordingly, several sampling approaches have been proposed to select a presumably minimal, yet sufficient number of products to be tested. Since the time budget for testing is limited or even a priori unknown, the order in which products are tested is crucial for effective product-line testing. Prioritizing products is required to increase the probability of detecting faults faster. In this article, we propose similarity-based prioritization, which can be efficiently applied on product samples. In our approach, we incrementally select the most diverse product in terms of features to be tested next in order to increase feature interaction coverage as fast as possible during product-by-product testing. We evaluate the gain in the effectiveness of similarity-based prioritization on three product lines with real faults. Furthermore, we compare similarity-based prioritization to random orders, an interaction-based approach, and the default orders produced by existing sampling algorithms considering feature models of various sizes. The results show that our approach potentially increases effectiveness in terms of fault detection ratio concerning faults within real-world product-line implementations as well as synthetically seeded faults. Moreover, we show that the default orders of recent sampling algorithms already show promising results, which, however, can still be improved in many cases using similarity-based prioritization.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {499–521},
numpages = {23},
keywords = {Test-case prioritization, Software product lines, Product-line testing, Model-based testing, Combinatorial interaction testing}
}

@article{10.1007/s11219-014-9258-y,
author = {Galindo, Jos\'{e} A. and Turner, Hamilton and Benavides, David and White, Jules},
title = {Testing variability-intensive systems using automated analysis: an application to Android},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-014-9258-y},
doi = {10.1007/s11219-014-9258-y},
abstract = {Software product lines are used to develop a set of software products that, while being different, share a common set of features. Feature models are used as a compact representation of all the products (e.g., possible configurations) of the product line. The number of products that a feature model encodes may grow exponentially with the number of features. This increases the cost of testing the products within a product line. Some proposals deal with this problem by reducing the testing space using different techniques. However, a daunting challenge is to explore how the cost and value of test cases can be modeled and optimized in order to have lower-cost testing processes. In this paper, we present TESting vAriAbiLity Intensive Systems (TESALIA), an approach that uses automated analysis of feature models to optimize the testing of variability-intensive systems. We model test value and cost as feature attributes, and then we use a constraint satisfaction solver to prune, prioritize and package product line tests complementing prior work in the software product line testing literature. A prototype implementation of TESALIA is used for validation in an Android example showing the benefits of maximizing the mobile market share (the value function) while meeting a budgetary constraint.},
journal = {Software Quality Journal},
month = jun,
pages = {365–405},
numpages = {41},
keywords = {Testing, Software product lines, Feature models, Automated analysis, Android}
}

@article{10.1016/j.infsof.2006.05.003,
author = {Olumofin, Femi G. and Mi\v{s}i\'{c}, Vojislav B.},
title = {A holistic architecture assessment method for software product lines},
year = {2007},
issue_date = {April, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.05.003},
doi = {10.1016/j.infsof.2006.05.003},
abstract = {The success of architecture-centric development of software product lines is critically dependent upon the availability of suitable architecture assessment methods. While a number of architecture assessment methods are available and some of them have been widely used in the process of evaluating single product architectures, none of them is equipped to deal with the main challenges of product line development. In this paper we present an adaptation of the Architecture Tradeoff Analysis Method (ATAM) for the task of assessing product line architectures. The new method, labeled Holistic Product Line Architecture Assessment (HoPLAA), uses a holistic approach that focuses on risks and quality attribute tradeoffs - not only for the common product line architecture, but for the individual product architectures as well. In addition, it prescribes a qualitative analytical treatment of variation points using scenarios. The use of the new method is illustrated through a case study.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {309–323},
numpages = {15},
keywords = {Software product line architectures, Software architecture assessment, Architecture Tradeoff Analysis Method (ATAM)}
}

@inproceedings{10.1109/ICSE.2019.00092,
author = {Lazreg, Sami and Cordy, Maxime and Collet, Philippe and Heymans, Patrick and Mosser, S\'{e}bastien},
title = {Multifaceted automated analyses for variability-intensive embedded systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00092},
doi = {10.1109/ICSE.2019.00092},
abstract = {Embedded systems, like those found in the automotive domain, must comply with stringent functional and non-functional requirements. To fulfil these requirements, engineers are confronted with a plethora of design alternatives both at the software and hardware level, out of which they must select the optimal solution wrt. possibly-antagonistic quality attributes (e.g. cost of manufacturing vs. speed of execution). We propose a model-driven framework to assist engineers in this choice. It captures high-level specifications of the system in the form of variable dataflows and configurable hardware platforms. A mapping algorithm then derives the design space, i.e. the set of compatible pairs of application and platform variants, and a variability-aware executable model, which encodes the functional and non-functional behaviour of all viable system variants. Novel verification algorithms then pinpoint the optimal system variants efficiently. The benefits of our approach are evaluated through a real-world case study from the automotive industry.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {854–865},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1007/s10664-014-9336-6,
author = {Sobernig, Stefan and Apel, Sven and Kolesnikov, Sergiy and Siegmund, Norbert},
title = {Quantifying structural attributes of system decompositions in 28 feature-oriented software product lines},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9336-6},
doi = {10.1007/s10664-014-9336-6},
abstract = {A key idea of feature orientation is to decompose a software product line along the features it provides. Feature decomposition is orthogonal to object-oriented decomposition--it crosscuts the underlying package and class structure. It has been argued often that feature decomposition improves system structure by reducing coupling and by increasing cohesion. However, recent empirical findings suggest that this is not necessarily the case. In this exploratory, observational study, we investigate the decompositions of 28 feature-oriented software product lines into classes, features, and feature-specific class fragments. The product lines under investigation are implemented using the feature-oriented programming language Fuji. In particular, we quantify and compare the internal attributes import coupling and cohesion of the different product-line decompositions in a systematic, reproducible manner. For this purpose, we adopt three established software measures (e.g., coupling between units, CBU; internal-ratio unit dependency, IUD) as well as standard concentration statistics (e.g., Gini coefficient). In our study, we found that feature decomposition can be associated with higher levels of structural coupling in a product line than a decomposition into classes. Although coupling can be concentrated in very few features in most feature decompositions, there are not necessarily hot-spot features  in all product lines. Interestingly, feature cohesion is not necessarily higher than class cohesion, whereas features are more equal in serving dependencies internally than classes of a product line. Our empirical study raises critical questions about alleged advantages of feature decomposition. At the same time, we demonstrate how our measurement approach of coupling and cohesion has potential to support static and dynamic analyses of software product lines (i.e., type checking and feature-interaction detection) by facilitating product sampling.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1670–1705},
numpages = {36},
keywords = {Structural coupling, Structural cohesion, Software product lines, Software measurement, Fuji, Feature-oriented programming}
}

@article{10.1007/s11219-018-9424-8,
author = {Alkharabsheh, Khalid and Crespo, Yania and Manso, Esperanza and Taboada, Jos\'{e} A.},
title = {Software Design Smell Detection: a systematic mapping study},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9424-8},
doi = {10.1007/s11219-018-9424-8},
abstract = {Design Smells are indicators of situations that negatively affect software quality attributes such as understandability, testability, extensibility, reusability, and maintainability in general. Improving maintainability is one of the cornerstones of making software evolution easier. Hence, Design Smell Detection is important in helping developers when making decisions that can improve software evolution processes. After a long period of research, it is important to organize the knowledge produced so far and to identify current challenges and future trends. In this paper, we analyze 18&nbsp;years of research into Design Smell Detection. There is a wide variety of terms that have been used in the literature to describe concepts which are similar to what we have defined as “Design Smells,” such as design defect, design flaw, anomaly, pitfall, antipattern, and disharmony. The aim of this paper is to analyze all these terms and include them in the study. We have used the standard systematic literature review method based on a comprehensive set of 395 articles published in different proceedings, journals, and book chapters. We present the results in different dimensions of Design Smell Detection, such as the type or scope of smell, detection approaches, tools, applied techniques, validation evidence, type of artifact in which the smell is detected, resources used in evaluation, supported languages, and relation between detected smells and software quality attributes according to a quality model. The main contributions of this paper are, on the one hand, the application of domain modeling techniques to obtain a conceptual model that allows the organization of the knowledge on Design Smell Detection and a collaborative web application built on that knowledge and, on the other, finding how tendencies have moved across different kinds of smell detection, as well as different approaches and techniques. Key findings for future trends include the fact that all automatic detection tools described in the literature identify Design Smells as a binary decision (having the smell or not), which is an opportunity to evolve to fuzzy and prioritized decisions. We also find that there is a lack of human experts and benchmark validation processes, as well as demonstrating that Design Smell Detection positively influences quality attributes.},
journal = {Software Quality Journal},
month = sep,
pages = {1069–1148},
numpages = {80},
keywords = {Systematic mapping study, Quality models, Detection tools, Antipatterns, DesignSmell}
}

@inproceedings{10.1145/2915970.2915985,
author = {Wnuk, Krzysztof and Kollu, Ravichandra Kumar},
title = {A systematic mapping study on requirements scoping},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2915985},
doi = {10.1145/2915970.2915985},
abstract = {Context: Requirements scoping is one of the key activities in requirements management but also a major risk for project management. Continuously changing scope may create a congestion state in handling the requirements inflow which causes negative consequences, e.g. delays or scope creep. Objectives: In this paper, we look at requirements scoping literature outside Software Product Line (SPL) by exploring the current literature on the phenomenon, summarizing publication trends, performing thematic analysis and analyzing the strength of the evidence in the light of rigor and relevance assessment. Method: We run a Systematic Mapping Study (SMS) using snowballing procedure, supported by a database search for the start set identification, and identified 21 primary studies and 2 secondary studies. Results: The research interest in this area steadily increases and includes mainly case studies, validation or evaluation studies. The results were categorized into four themes: definitions, negative effects associated with scoping, challenges and identified methods/tools. The identified scope management techniques are also matched against the identified requirements scoping challenges.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {32},
numpages = {11},
keywords = {systematic mapping study, snowballing, requirements scoping},
location = {Limerick, Ireland},
series = {EASE '16}
}

@inproceedings{10.1145/2304736.2304757,
author = {Pascual, Gustavo Garc\'{\i}a and Alarc\'{o}n, M\'{o}nica Pinto and Fern\'{a}ndez, Lidia Fuentes},
title = {Component and aspect-based service product line for pervasive systems},
year = {2012},
isbn = {9781450313452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304736.2304757},
doi = {10.1145/2304736.2304757},
abstract = {Pervasive systems have experienced an increase in demand due to the evolution and popularity of mobile devices and embedded systems. The development of applications for these systems imposes new challenges due to the necessity of adapting these applications both to the changes in the environment and to the resource-constrained devices (e.g. limited battery, memory and CPU) in which they run. These challenges are: (1) the same services are required by most applications for pervasive systems, and thus should be modeled as separate, ready-to-use (re)usable solutions; (2) services need to be customized to the requirements of applications, by generating different versions of the same service containing only the required functionality, and (3) the same service needs to be customized to the different devices in which a same application will run (e.g. with different operating systems, different memory and CPU capacities or different communication technologies). In order to consider all of the above challenges, in this paper we present a software product line approach that permits modelling the variability of these services using feature models, automatically generating different configurations of their software architecture depending on the particular requirements of each application. We use this approach to model typical services of pervasive systems, such as context-awareness and communication, and to evaluate the degree of variability, of reuse and of separation of concerns of these services.},
booktitle = {Proceedings of the 15th ACM SIGSOFT Symposium on Component Based Software Engineering},
pages = {115–124},
numpages = {10},
keywords = {spl, pervasive systems, context-awareness, cbse, aosd},
location = {Bertinoro, Italy},
series = {CBSE '12}
}

@article{10.1016/j.jss.2005.02.028,
author = {Feng, Qian and Lutz, Robyn R.},
title = {Bi-directional safety analysis of product lines},
year = {2005},
issue_date = {November 2005},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {78},
number = {2},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2005.02.028},
doi = {10.1016/j.jss.2005.02.028},
abstract = {As product-line engineering becomes more widespread, more safety-critical software product lines are being built. This paper describes a structured method for performing safety analysis on a software product line, building on standard product-line assets: product-line requirements, architecture, and scenarios. The safety-analysis method is bi-directional in that it combines a forward analysis (from failure modes to effects) with a backward analysis (from hazards to contributing causes). Safety-analysis results are converted to XML files to allow automated consistency checking between the forward and backward analysis results and to support reuse of the safety-analysis results throughout the product line. The paper demonstrates and evaluates the method on a safety-critical product-line subsystem, the Door Control System. Results show that the bi-directional safety-analysis method found both missing and incorrect software safety requirements. Some of the new safety requirements affected all the systems in the product line while others affected only some of the systems in the product line. The results demonstrate that the proposed method can handle the challenges to safety analysis posed by variations within a product line.},
journal = {J. Syst. Softw.},
month = nov,
pages = {111–127},
numpages = {17},
keywords = {XML, Software safety, Software architecture, Reuse, Product lines}
}

@article{10.4018/JITR.2018010104,
author = {Vegendla, Aparna and Duc, Anh Nguyen and Gao, Shang and Sindre, Guttorm},
title = {A Systematic Mapping Study on Requirements Engineering in Software Ecosystems},
year = {2018},
issue_date = {January 2018},
publisher = {IGI Global},
address = {USA},
volume = {11},
number = {1},
issn = {1938-7857},
url = {https://doi.org/10.4018/JITR.2018010104},
doi = {10.4018/JITR.2018010104},
abstract = {Software ecosystems SECOs and open innovation processes have been claimed as a way forward for the software industry. A proper understanding of requirements is as important for SECOs as for more traditional ones. This article presents a mapping study on the issues of RE and quality aspects in SECOs. Our findings indicate that among the various phases or subtasks of RE, most of the SECO specific research has been accomplished on elicitation, analysis, and modeling. On the other hand, requirement selection, prioritization, verification, and traceability has attracted few published studies. Among the various quality attributes, most of the SECOs research has been performed on security, performance and testability. On the other hand, reliability, safety, maintainability, transparency, usability attracted few published studies. The article provides a review of the academic literature about SECO-related RE activities, modeling approaches, and quality attributes, positions the source publications in a taxonomy of issues and identifies gaps where there has been little research.},
journal = {J. Inf. Technol. Res.},
month = jan,
pages = {49–69},
numpages = {21},
keywords = {Software Ecosystem, Requirements Engineering, Mapping Study}
}

@article{10.1016/j.datak.2006.06.009,
author = {Kim, Minseong and Park, Sooyong and Sugumaran, Vijayan and Yang, Hwasil},
title = {Managing requirements conflicts in software product lines: A goal and scenario based approach},
year = {2007},
issue_date = {June, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {61},
number = {3},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2006.06.009},
doi = {10.1016/j.datak.2006.06.009},
abstract = {The product line approach is recognized as a successful approach to reuse in software development. However, in many cases, it has resulted in interactions between requirements and/or features. Interaction detection, especially conflict detection between requirements has become more challenging. Thus, detecting conflicts between requirements is essential for successful product line development. Formal methods have been proposed to address this problem, however, they are hard to understand by non-experts and are limited to restricted domains. In addition, there is no overall process that covers all the steps for managing conflicts. We propose an approach for systematically identifying and managing requirements conflicts, which is based on requirements partition in natural language and supported by a tool. To demonstrate its feasibility, the proposed approach has been applied to the home integration system (HIS) domain and the results are discussed.},
journal = {Data Knowl. Eng.},
month = jun,
pages = {417–432},
numpages = {16},
keywords = {Syntactic and semantic requirements conflict detection, Software product line, Requirements partitioning, Requirements conflicts, Goal and scenario authoring}
}

@inproceedings{10.1145/2884781.2884821,
author = {Devroey, Xavier and Perrouin, Gilles and Papadakis, Mike and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Featured model-based mutation analysis},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884821},
doi = {10.1145/2884781.2884821},
abstract = {Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {655–666},
numpages = {12},
keywords = {variability, mutation analysis, featured transition systems},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.5555/3432601.3432616,
author = {Podolskiy, Vladimir and Patrou, Maria and Patros, Panos and Gerndt, Michael and Kent, Kenneth B.},
title = {The weakest link: revealing and modeling the architectural patterns of microservice applications},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {Cloud microservice applications comprise interconnected services packed into containers. Such applications generate complex communication patterns among their microservices. Studying such patterns can support assuring various quality attributes, such as autoscaling for satisfying performance, availability and scalability, or targeted penetration testing for satisfying security and correctness. We study the structure of containerized microservice applications via providing the methodology and the results of a structural graph-based analysis of 103 Docker Compose deployment files from open-sourced Github repositories. Our findings indicate the dominance of a power-law distribution of microservice interconnections. Further analysis highlights the suitability of the Barab\'{a}si-Albert model for generating large random graphs that model the architecture of real microservice applications. The exhibited structures and their usage for engineering microservice applications are discussed.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {113–122},
numpages = {10},
keywords = {software vulnerability, microservice, cloud-native application, application topology},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1007/978-3-642-33666-9_34,
author = {Vierhauser, Michael and Gr\"{u}nbacher, Paul and Heider, Wolfgang and Holl, Gerald and Lettner, Daniela},
title = {Applying a consistency checking framework for heterogeneous models and artifacts in industrial product lines},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_34},
doi = {10.1007/978-3-642-33666-9_34},
abstract = {Product line engineering relies on heterogeneous models and artifacts to define and implement the product line's reusable assets. The complexity and heterogeneity of product line artifacts as well as their interdependencies make it hard to maintain consistency during development and evolution, regardless of the modeling approaches used. Engineers thus need support for detecting and resolving inconsistencies within and between the various artifacts. In this paper we present a framework for checking and maintaining consistency of arbitrary product line artifacts. Our approach is flexible and extensible regarding the supported artifact types and the definition of constraints. We discuss tool support developed for the DOPLER product line tool suite. We report the results of applying the approach to sales support applications of industrial product lines.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {531–545},
numpages = {15},
keywords = {sales support, model-based product lines, consistency checking},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@article{10.1007/s11219-011-9170-7,
author = {Acher, Mathieu and Collet, Philippe and Gaignard, Alban and Lahire, Philippe and Montagnat, Johan and France, Robert B.},
title = {Composing multiple variability artifacts to assemble coherent workflows},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9170-7},
doi = {10.1007/s11219-011-9170-7},
abstract = {The development of scientific workflows is evolving toward the systematic use of service-oriented architectures, enabling the composition of dedicated and highly parameterized software services into processing pipelines. Building consistent workflows then becomes a cumbersome and error-prone activity as users cannot manage such large-scale variability. This paper presents a rigorous and tooled approach in which techniques from Software Product Line (SPL) engineering are reused and extended to manage variability in service and workflow descriptions. Composition can be facilitated while ensuring consistency. Services are organized in a rich catalog which is organized as a SPL and structured according to the common and variable concerns captured for all services. By relying on sound merging techniques on the feature models that make up the catalog, reasoning about the compatibility between connected services is made possible. Moreover, an entire workflow is then seen as a multiple SPL (i.e., a composition of several SPLs). When services are configured within, the propagation of variability choices is then automated with appropriate techniques and the user is assisted in obtaining a consistent workflow. The approach proposed is completely supported by a combination of dedicated tools and languages. Illustrations and experimental validations are provided using medical imaging pipelines, which are representative of current scientific workflows in many domains.},
journal = {Software Quality Journal},
month = sep,
pages = {689–734},
numpages = {46},
keywords = {Software product lines, Scientific workflows, Feature models, Composition}
}

@article{10.1016/j.jss.2019.01.044,
author = {Th\"{u}m, Thomas and Kn\"{u}ppel, Alexander and Kr\"{u}ger, Stefan and Bolle, Stefanie and Schaefer, Ina},
title = {Feature-oriented contract composition},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.044},
doi = {10.1016/j.jss.2019.01.044},
journal = {J. Syst. Softw.},
month = jun,
pages = {83–107},
numpages = {25},
keywords = {Formal methods, Deductive verification, Design by contract, Software product lines, Feature-oriented programming}
}

@inproceedings{10.1145/2701319.2701335,
author = {Gamez, Nadia and El Haddad, Joyce and Fuentes, Lidia},
title = {Managing the Variability in the Transactional Services Selection},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701335},
doi = {10.1145/2701319.2701335},
abstract = {Web service composition is the capability to recursively construct a value added service by means of picking up existing services. An important step in the composition process is the selection step, which includes choosing services located in repositories. The selection approaches of Web services need to consider their specifics which raises important challenges as the management of the inherent service variability in functionality and implementation and ensuring correct execution termination between others. To realize reliable service compositions, transactional properties of services must be considered during the selection step. We argue that the transactional properties should be considered at the operation level of each service to be composed. However, modelling transactional services composition at the operation level drastically increment the complexity of service selection. In order to overcome this difficulty, in this paper we report on our research in progress on transactional service selection, which follows a Software Product Line approach considering the set of services that provide the same functionality as part of a service family. We model the variable operations of the service families using Feature Models. In this way, the selection process consists of selecting each service from a service family such that the aggregated transactional property satisfies the user preference.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {88–95},
numpages = {8},
keywords = {Transactional Services, Feature Modeling, Discovery and Selection},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@article{10.1016/j.scico.2012.06.002,
author = {Th\"{u}m, Thomas and K\"{a}stner, Christian and Benduhn, Fabian and Meinicke, Jens and Saake, Gunter and Leich, Thomas},
title = {FeatureIDE: An extensible framework for feature-oriented software development},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {79},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.06.002},
doi = {10.1016/j.scico.2012.06.002},
abstract = {FeatureIDE is an open-source framework for feature-oriented software development (FOSD) based on Eclipse. FOSD is a paradigm for the construction, customization, and synthesis of software systems. Code artifacts are mapped to features, and a customized software system can be generated given a selection of features. The set of software systems that can be generated is called a software product line (SPL). FeatureIDE supports several FOSD implementation techniques such as feature-oriented programming, aspect-oriented programming, delta-oriented programming, and preprocessors. All phases of FOSD are supported in FeatureIDE, namely domain analysis, requirements analysis, domain implementation, and software generation.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {70–85},
numpages = {16},
keywords = {Tool support, Software product lines, Preprocessors, Feature-oriented software development, Feature-oriented programming, Feature modeling, Delta-oriented programming, Aspect-oriented programming}
}

@inproceedings{10.5555/2667025.2667027,
author = {Siegmund, Norbert and Mory, Maik and Feigenspan, Janet and Saake, Gunter and Nykolaychuk, Mykhaylo and Schumann, Marco},
title = {Interoperability of non-functional requirements in complex systems},
year = {2012},
isbn = {9781467318532},
publisher = {IEEE Press},
abstract = {Heterogeneity of embedded systems leads to the development of variable software, such as software product lines. From such a family of programs, stakeholders select the specific variant that satisfies their functional requirements. However, different functionality exposes different non-functional properties of these variants. Especially in the embedded-system domain, non-functional requirements are vital, because resources are scarce. Hence, when selecting an appropriate variant, we have to fulfill also non-functional requirements. Since more systems are interconnected, the challenge is to find a variant that additionally satisfies global nonfunctional (or quality) requirements. In this paper, we advert the problem of achieving interoperability of non-functional requirements among multiple interacting systems using a real-world scenario. Furthermore, we show an approach to find optimal variants for multiple systems that reduces computation effort by means of a stepwise configuration process.},
booktitle = {Proceedings of the Second International Workshop on Software Engineering for Embedded Systems},
pages = {2–8},
numpages = {7},
location = {Zurich, Switzerland},
series = {SEES '12}
}

@inproceedings{10.1145/568760.568805,
author = {Johansson, Enrico and H\"{o}st, Martin},
title = {Tracking degradation in software product lines through measurement of design rule violations},
year = {2002},
isbn = {1581135564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/568760.568805},
doi = {10.1145/568760.568805},
abstract = {In order to increase reuse, a number of product versions may be developed based on the same software platform. The platform must, however, be managed and updated according to new requirements if it should be reusable in a series of releases. This means that the platform is constantly changed during its lifecycle, and changes can result in degradation of the platform. In this paper, a measurement approach is proposed as a means of tracking the degradation of a software platform and consequently in the product line. The tracking approach is evaluated in a case study where it is applied to a series of different releases of a product. The result of the case study indicates that the presented approach is feasible.},
booktitle = {Proceedings of the 14th International Conference on Software Engineering and Knowledge Engineering},
pages = {249–254},
numpages = {6},
keywords = {software product line, software platform, project tracking, graph, design rules, degradation},
location = {Ischia, Italy},
series = {SEKE '02}
}

@inproceedings{10.1145/1944892.1944899,
author = {Galster, Matthias and Avgeriou, Paris},
title = {The notion of variability in software architecture: results from a preliminary exploratory study},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944899},
doi = {10.1145/1944892.1944899},
abstract = {Context: In the software product line domain, the concept of variability is well recognized. However, variability in the context of software architecture still seems to be poorly understood. Objective: In this paper, we aim at contributing to the development of a basic understanding of the notion of variability in the software architecture domain, beyond the idea of product lines. Method: We perform a preliminary exploratory study which consists of two parts: an expert survey among 11 subjects, and a mini focus group with 4 participants. For both parts, we collect and analyze mostly qualitative data. Results: Our observations indicate that there seems to be no common understanding of "variability" in the context of software architecture. On the other hand, some challenges related to variability in software architecture are similar to challenges identified in the product line domain. Conclusions: Variability in software architecture might require more theoretical foundations in order to establish "variability" as an architectural key concept and first-class quality attribute.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {59–67},
numpages = {9},
keywords = {variability, software architecture, questionnaire, product lines, mini focus group},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@inproceedings{10.1145/3330204.3330251,
author = {Sorgatto, Doglas W. and Paiva, D\'{e}bora M. B. and Cagnin, Maria Istela},
title = {Requirement Reuse in Business Processes Lines: Reutiliza\c{c}\~{a}o de requisitos em linhas de processos de neg\'{o}cio},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330251},
doi = {10.1145/3330204.3330251},
abstract = {The cost reduction in the Requirement Engineering process finds in business process modeling a way of aligning business goals with software requirements, and for development companies that have development demands in the same domain, greater savings can be found with the adoption of Business Process Lines (BPL). From this perspective, this paper presents the ARReq, which is an approach that allows the elicitation, specification and reuse of requirements from BPLs. It has been defined to provide quality attributes, suggested by ISO/IEC 29.148, to the functional, non-functional requirements and business rules elicited with the support of any elicitation technique applicable to BPMN business process models. A qualitative analysis was carried out and allowed to observe that ARReq is scalable, has low coupling with BPLs management approaches, besides specifying the requirements reused in the formats of user stories and requirements document and to provide a traceability matrix to support the software maintainability.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {41},
numpages = {8},
keywords = {Requirement reuse, Business Processes Line, BPMN},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@article{10.1016/j.infsof.2016.08.011,
author = {Tanhaei, Mohammad and Habibi, Jafar and Mirian-Hosseinabadi, Seyed-Hassan},
title = {Automating feature model refactoring},
year = {2016},
issue_date = {December 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {80},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.08.011},
doi = {10.1016/j.infsof.2016.08.011},
abstract = {Display Omitted Context: Feature model is an appropriate and indispensable tool for modeling similarities and differences among products of the Software Product Line (SPL). It not only exposes the validity of the products' configurations in an SPL but also changes in the course of time to support new requirements of the SPL. Modifications made on the feature model in the course of time raise a number of issues. Useless enlargements of the feature model, the existence of dead features, and violated constraints in the feature model are some of the key problems that make its maintenance difficult.Objective: The initial approach to dealing with the above-mentioned problems and improving maintainability of the feature model is refactoring. Refactoring modifies software artifacts in a way that their externally visible behavior does not change.Method: We introduce a method for defining refactoring rules and executing them on the feature model. We use the ATL model transformation language to define the refactoring rules. Moreover, we provide an Alloy model to check the feature model and the safety of the refactorings that are performed on it.Results: In this research, we propose a safe framework for refactoring a feature model. This framework enables users to perform automatic and semi-automatic refactoring on the feature model.Conclusions: Automated tool support for refactoring is a key issue for adopting approaches such as utilizing feature models and integrating them into the software development process of companies. In this work, we define some of the important refactoring rules on the feature model and provide tools that enable users to add new rules using the ATL M2M language. Our framework assesses the correctness of the refactorings using the Alloy language.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {138–157},
numpages = {20},
keywords = {Model transformation &amp; refactoring, Feature model refactoring}
}

@inproceedings{10.1145/1842752.1842815,
author = {Galster, Matthias},
title = {Describing variability in service-oriented software product lines},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842815},
doi = {10.1145/1842752.1842815},
abstract = {Service-oriented architectures are a standard-based and technology-independent distributed computing paradigm for discovering, binding and assembling loosely-coupled software services. Software product lines on the other hand allow a generic architecture to be configured and deployed in different instances. Product lines facilitate systematic reuse through managing variability. In this paper, we combine ideas from the service domain and the product line domain and investigate what types of variability exist in service-oriented software architectures. Moreover, we suggest a way for representing variability in service-oriented architectures by formalizing the notion of variability. To allow different viewpoints on variability, we define stakeholder roles that occur in the context of service-oriented software architectures. By applying the proposed concepts, we hope to improve variability management at the software architecture level of service-oriented systems.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {344–350},
numpages = {7},
keywords = {variability, service-oriented architectures, modeling},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1109/QUATIC.2012.14,
author = {Gonzalez-Huerta, Javier and Insfran, Emilio and Abrahao, Silvia},
title = {A Multimodel for Integrating Quality Assessment in Model-Driven Engineering},
year = {2012},
isbn = {9780769547770},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/QUATIC.2012.14},
doi = {10.1109/QUATIC.2012.14},
abstract = {The development of complex software systems following the Model-Driven Engineering (MDE) approach relies on the use of different models for describing the system (e.g., structure, behavior). These models should be specified first separately but then their inter-relationship must be established since they represent complementary aspects of the system. Besides, MDE development processes are mostly focused on functionality, and do not give proper support to the quality aspects of the system. In this paper, we present a generic multimodel and the process for its construction, allowing the representation of the different viewpoint models of a software system and the relationships among elements on these viewpoints. This multimodel is a means for integrating a quality viewpoint in MDE processes, allowing the quality attributes to become a decision factor in the choice among design decisions in transformation processes. The feasibility of this approach is illustrated through the use of the multimodel in a specific example for Software Product Line development.},
booktitle = {Proceedings of the 2012 Eighth International Conference on the Quality of Information and Communications Technology},
pages = {251–254},
numpages = {4},
keywords = {Software Product Lines, Quality Atttributes, Model Driven Development},
series = {QUATIC '12}
}

@inproceedings{10.1007/978-3-319-27343-3_1,
author = {Braubach, Lars and Pokahr, Alexander and Kalinowski, Julian and Jander, Kai},
title = {Tailoring Agent Platforms with Software Product Lines},
year = {2015},
isbn = {9783319273426},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-27343-3_1},
doi = {10.1007/978-3-319-27343-3_1},
abstract = {Agent platforms have been conceived traditionally as middleware, helping to deal with various application challenges like agent programming models, remote messaging, and coordination protocols. A\"{\i} \'{z}middleware is typically a bundle of functionalities necessary to execute multi-agent applications. In contrast to this traditional view, nowadays different use cases also for selected agent concepts have emerged requiring also different kinds of functionalities. Examples include a platform for conducting multi-agent simulations, intelligent agent behavior models for controlling non-player characters NPCs in games and a lightweight version suited for mobile devices. A one-size-fits-all software bundle often does not sufficiently match these requirements, because customers and developers want solutions specifically tailored to their needs, i.e. a small but focused solution is frequently preferred over bloated software with extraneous functionality. Software product lines are an approach suitable for creating a series of similar products from a common code base. In this paper we will show how software product line modeling and technology can help creating tailor-made products from multi-agent platforms. Concretely, the Jadex platform will be analyzed and a feature model as well as an implementation path will be presented.},
booktitle = {Revised Selected Papers of the 13th German Conference on Multiagent System Technologies - Volume 9433},
pages = {3–21},
numpages = {19},
location = {Cottbus, Germany},
series = {MATES 2015}
}

@article{10.1007/s00766-014-0211-1,
author = {Djouab, Rachida and Abran, Alain and Seffah, Ahmed},
title = {An ASPIRE-based method for quality requirements identification from business goals},
year = {2016},
issue_date = {March     2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0211-1},
doi = {10.1007/s00766-014-0211-1},
abstract = {Quality requirements are the main drivers for modeling and evaluating software quality at an early stage, and ASPIRE is an engineering method designed to elicit and document the quality requirements of embedded systems. This paper proposes an extension to ASPIRE to identify quality requirements from the business goals of the organization and ensure their traceability. This extension includes a set of added components created from the main concepts of the SOQUAREM methodology, including the BMM (business motivation model), derivation rules, the quality attribute utility tree, the quality attribute scenario template, the quality attribute documentation template, and ISO 9126. The applicability of the extended method is illustrated with a wireless plant control system as an example.},
journal = {Requir. Eng.},
month = mar,
pages = {87–106},
numpages = {20},
keywords = {Software quality engineering, Quality requirements (QRs), Quality attributes (QAs), QR elicitation method, Non-functional requirements (NFRs), Business goals (BGs), BMM (business motivation model)}
}

@inproceedings{10.5555/1785246.1785323,
author = {Li, Yi-Yuan and Yin, Jian-wei and Li, Yin and Dong, Jin-Xiang},
title = {Configuration modeling based software product development},
year = {2007},
isbn = {354076836X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product line is an effective way to implement software production for mass customization. How to organize and configure the software artifacts in software product line to rapidly produce customized software product meeting individual demands is one of the key problems. Corresponding to the phases of feature selection and software artifact binding in the process of software production, the feature configuration model and software artifact configuration model are constructed to provide a uniform framework of constraint description for feature model and domain application requirement. The results of problem solving are the sets of feature and software artifact meeting feature constraints and application requirements. The proposed method of configuration modeling and problem solving provide a theoretical foundation to rapidly produce software product on the base of configuration of reusable domain assets.},
booktitle = {Proceedings of the 7th International Conference on Advanced Parallel Processing Technologies},
pages = {624–639},
numpages = {16},
keywords = {software artifact configuration, problem solving, feature configuration, configuration rule},
location = {Guangzhou, China},
series = {APPT'07}
}

@inproceedings{10.1145/1837154.1837157,
author = {Siegmund, Norbert and Feigenspan, Janet and Soffner, Michael and Fruth, Jana and K\"{o}ppen, Veit},
title = {Challenges of secure and reliable data management in heterogeneous environments},
year = {2010},
isbn = {9781605589923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837154.1837157},
doi = {10.1145/1837154.1837157},
abstract = {Ubiquitous computing is getting more important since requirements for complex systems grow fast. In these systems, embedded devices have to fulfill different tasks. They have to monitor the environment, store data, communicate with other devices, and react to user input. In addition to this complexity, quality issues such as security and reliability have to be considered, as well, due to their increasing use in life critical application scenarios. Finally, different devices with different application goals are used, which results in interoperability problems. In this paper, we highlight challenges for interoperability, data management, and security, which arise with complex systems. Furthermore, we present approaches to overcome different problems and how an integrated solution can be realized using software product line techniques.},
booktitle = {Proceedings of the First International Workshop on Digital Engineering},
pages = {17–24},
numpages = {8},
keywords = {software product lines, security, digital engineering, data management},
location = {Magdeburg, Germany},
series = {IWDE '10}
}

@inproceedings{10.5555/645547.658835,
author = {Dobrica, Liliana and Niemel\"{a}, Eila},
title = {Software Architecture Quality Analysis Methods},
year = {2002},
isbn = {3540434836},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The open problem of structural methods is how to take a better advantage of software architectural concepts to analyse software systems for quality attributes in a systematic and repetitive way. Throughout the presentation we try to introduce a way of thinking founded on analysis at the architecture level of the quality attributes with the purpose to initiate and maintain a software product-line considering the quality as the main driver in product line development. This tutorial represents a study that shows the state of the research at this moment, in the quality analysis methods for software architectures, by presenting and discussing the most representative architecture analysis methods. The role of the discussion is to offer guidelines related to the use of the most suitable method for an architecture assessment process.},
booktitle = {Proceedings of the 7th International Conference on Software Reuse: Methods, Techniques, and Tools},
pages = {337–338},
numpages = {2},
series = {ICSR-7}
}

@inproceedings{10.1145/2642937.2642939,
author = {Segura, Sergio and S\'{a}nchez, Ana B. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated variability analysis and testing of an E-commerce site.: an experience report},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642939},
doi = {10.1145/2642937.2642939},
abstract = {In this paper, we report on our experience on the development of La Hilandera, an e-commerce site selling haberdashery products and craft supplies in Europe. The store has a huge input space where customers can place almost three millions of different orders which made testing an extremely difficult task. To address the challenge, we explored the applicability of some of the practices for variability management in software product lines. First, we used a feature model to represent the store input space which provided us with a variability view easy to understand, share and discuss with all the stakeholders. Second, we used techniques for the automated analysis of feature models for the detection and repair of inconsistent and missing configuration settings. Finally, we used test selection and prioritization techniques for the generation of a manageable and effective set of test cases. Our findings, summarized in a set of lessons learnt, suggest that variability techniques could successfully address many of the challenges found when developing e-commerce sites.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {139–150},
numpages = {12},
keywords = {variability, feature modelling, experience report, e-commerce, automated testing},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1016/j.jss.2011.06.026,
author = {Guo, Jianmei and White, Jules and Wang, Guangxin and Li, Jian and Wang, Yinglin},
title = {A genetic algorithm for optimized feature selection with resource constraints in software product lines},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.06.026},
doi = {10.1016/j.jss.2011.06.026},
abstract = {Abstract: Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86-97% of the optimality of other automated feature selection algorithms and in 45-99% less time than existing exact and heuristic feature selection techniques.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2208–2221},
numpages = {14},
keywords = {Software product lines, Product derivation, Optimization, Genetic algorithm, Feature models, Configuration}
}

@article{10.4018/ijswis.2014010103,
author = {Ermilov, Timofey and Khalili, Ali and Auer, S\"{o}ren},
title = {Ubiquitous Semantic Applications: A Systematic Literature Review},
year = {2014},
issue_date = {January 2014},
publisher = {IGI Global},
address = {USA},
volume = {10},
number = {1},
issn = {1552-6283},
url = {https://doi.org/10.4018/ijswis.2014010103},
doi = {10.4018/ijswis.2014010103},
abstract = {Recently practical approaches for development of ubiquitous semantic applications have made quite some progress. In particular in the area of the ubiquitous access to the semantic data the authors recently observed a large number of approaches, systems and applications being described in the literature. With this survey the authors aim to provide an overview on the rapidly emerging field of Ubiquitous Semantic Applications (UbiSA). The authors conducted a systematic literature review comprising a thorough analysis of 48 primary studies out of 172 initially retrieved papers. The authors obtained a comprehensive set of quality attributes for UbiSA together with corresponding application features suggested for their realization. The quality attributes include aspects such as mobility, usability, heterogeneity, collaboration, customizability and evolvability. The primary studies were surveyed in the light of these quality attributes and the authors performed a thorough analysis of five ubiquitous semantic applications, six frameworks for UbiSA, three UbiSA specific ontologies, five ubiquitous semantic systems and nine general approaches. The proposed quality attributes facilitate the evaluation of existing approaches and the development of novel, more effective and intuitive UbiSA.},
journal = {Int. J. Semant. Web Inf. Syst.},
month = jan,
pages = {66–99},
numpages = {34},
keywords = {Web Applications, Ubiquitous Device, Ubiquitous Applications, Survey, Semantic Web}
}

@article{10.1109/TSE.2002.1019479,
author = {Dobrica, Liliana and Niemel\"{a}, Eila},
title = {A survey on software architecture analysis methods},
year = {2002},
issue_date = {July 2002},
publisher = {IEEE Press},
volume = {28},
number = {7},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2002.1019479},
doi = {10.1109/TSE.2002.1019479},
abstract = {The purpose of the architecture evaluation of a software system is to analyze the architecture to identify potential risks and to verify that the quality requirements have been addressed in the design. This survey shows the state of the research at this moment, in this domain, by presenting and discussing eight of the most representative architecture analysis methods. The selection of the studied methods tries to cover as many particular views of objective reflections as possible to be derived from the general goal. The role of the discussion is to offer guidelines related to the use of the most suitable method for an architecture assessment process. We will concentrate on discovering similarities and differences between these eight available methods by making classifications, comparision and appropriateness studies.},
journal = {IEEE Trans. Softw. Eng.},
month = jul,
pages = {638–653},
numpages = {16},
keywords = {software architecture, scenarios, quality attributes, analysis techniques and methods}
}

@article{10.1016/j.jss.2015.08.026,
author = {Vogel-Heuser, Birgit and Fay, Alexander and Schaefer, Ina and Tichy, Matthias},
title = {Evolution of software in automated production systems},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {110},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.08.026},
doi = {10.1016/j.jss.2015.08.026},
abstract = {Automated Production Systems (aPS) impose specific requirements regarding evolution.We present a classification of how Automated Production Systems evolve.We discuss the state of art and research needs for the development phases of aPS.Model-driven engineering and Variability Management are key issues.Cross-discipline analysis of (non)-functional requirements must be improved. Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. Display Omitted},
journal = {J. Syst. Softw.},
month = dec,
pages = {54–84},
numpages = {31},
keywords = {Software engineering, Evolution, Automation, Automated production systems}
}

@article{10.1007/s10664-016-9462-4,
author = {Assun\c{c}\~{a}o, Wesley K. and Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Vergilio, Silvia R. and Egyed, Alexander},
title = {Multi-objective reverse engineering of variability-safe feature models based on code dependencies of system variants},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9462-4},
doi = {10.1007/s10664-016-9462-4},
abstract = {Maintenance of many variants of a software system, developed to supply a wide range of customer-specific demands, is a complex endeavour. The consolidation of such variants into a Software Product Line is a way to effectively cope with this problem. A crucial step for this consolidation is to reverse engineer feature models that represent the desired combinations of features of all the available variants. Many approaches have been proposed for this reverse engineering task but they present two shortcomings. First, they use a single-objective perspective that does not allow software engineers to consider design trade-offs. Second, they do not exploit knowledge from implementation artifacts. To address these limitations, our work takes a multi-objective perspective and uses knowledge from source code dependencies to obtain feature models that not only represent the desired feature combinations but that also check that those combinations are indeed well-formed, i.e. variability safe. We performed an evaluation of our approach with twelve case studies using NSGA-II and SPEA2, and a single-objective algorithm. Our results indicate that the performance of the multi-objective algorithms is similar in most cases and that both clearly outperform the single-objective algorithm. Our work also unveils several avenues for further research.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1763–1794},
numpages = {32},
keywords = {Reverse engineering, Multi-objective evolutionary algorithms, Feature models, Empirical evaluation}
}

@article{10.1007/s11219-015-9273-7,
author = {Grbac, Tihana Galinac and Runeson, Per and Huljeni\'{c}, Darko},
title = {A quantitative analysis of the unit verification perspective on fault distributions in complex software systems: an operational replication},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-015-9273-7},
doi = {10.1007/s11219-015-9273-7},
abstract = {Unit verification, including software inspections and unit tests, is usually the first code verification phase in the software development process. However, principles of unit verification are weakly explored, mostly due to the lack of data, since unit verification data are rarely systematically collected and only a few studies have been published with such data from industry. Therefore, we explore the theory of fault distributions, originating in the quantitative analysis by Fenton and Ohlsson, in the weakly explored context of unit verification in large-scale software development. We conduct a quantitative case study on a sequence of four development projects on consecutive releases of the same complex software product line system for telecommunication exchanges. We replicate the operationalization from earlier studies, analyzed hypotheses related to the Pareto principle of fault distribution, persistence of faults, effects of module size, and quality in terms of fault densities, however, now from the perspective of unit verification. The patterns in unit verification results resemble those of later verification phases, e.g., regarding the Pareto principle, and may thus be used for prediction and planning purposes. Using unit verification results as predictors may improve the quality and efficiency of software verification.},
journal = {Software Quality Journal},
month = dec,
pages = {967–995},
numpages = {29},
keywords = {Unit verification, Software metrics, Software fault distributions, Replication, Empirical research}
}

@inproceedings{10.1007/11741060_6,
author = {Lohmann, Daniel and Schr\"{o}der-Preikschat, Wolfgang and Spinczyk, Olaf},
title = {The design of application-tailorable operating system product lines},
year = {2005},
isbn = {3540336893},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11741060_6},
doi = {10.1007/11741060_6},
abstract = {System software for deeply embedded devices has to cope with a broad variety of requirements and platforms, but especially with strict resource constraints. To compete against proprietary systems (and thereby to facilitate reuse), an operating system product line for deeply embedded systems has to be highly configurable and tailorable. It is therefore crucial that all selectable and configurable features can be encapsulated into fine-grained, exchangeable and reusable implementation components. However, the encapsulation of non-functional properties is often limited, due to their cross-cutting character. Fundamental system policies, like synchronization or activation points for the scheduler, have typically to be reflected in many points of the operating system component code. The presented approach is based on feature modeling, C++ class composition and overcomes the above mentioned problems by means of aspect-oriented programming (AOP). It facilitates a fine-grained encapsulation and configuration of even non-functional properties in system software.},
booktitle = {Proceedings of the Second International Conference on Construction and Analysis of Safe, Secure, and Interoperable Smart Devices},
pages = {99–117},
numpages = {19},
location = {Nice, France},
series = {CASSIS'05}
}

@inproceedings{10.5555/1758398.1758458,
author = {Zhang, Hongyu and Jarzabek, Stan and Yang, Bo},
title = {Quality prediction and assessment for product lines},
year = {2003},
isbn = {3540404422},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In recent years, software product lines have emerged as a promising approach to improve software development productivity in IT industry. In the product line approach, we identify both commonalities and variabilities in a domain, and build generic assets for an organization. Feature diagrams are often used to model common and variant product line requirements and can be considered part of the organizational assets. Despite their importance, quality attributes (or non-functional requirements, NFRs) such as performance and security have not been sufficiently addressed in product line development. A feature diagram alone does not tell us how to select a configuration of variants to achieve desired quality attributes of a product line member. There is a lack of an explicit model that can represent the impact of variants on quality attributes. In this paper, we propose a Bayesian Belief Network (BBN) based approach to quality prediction and assessment for a software product line. A BBN represents domain experts' knowledge and experiences accumulated from the development of similar projects. It helps us capture the impact of variants on quality attributes, and helps us predict and assess the quality of a product line member by performing quantitative analysis over it. For developing specific systems, members of a product line, we reuse the expertise captured by a BBN instead of working from scratch. We use examples from the Computer Aided Dispatch (CAD) product line project to illustrate our approach.},
booktitle = {Proceedings of the 15th International Conference on Advanced Information Systems Engineering},
pages = {681–695},
numpages = {15},
location = {Klagenfurt, Austria},
series = {CAiSE'03}
}

@article{10.1023/A:1019791213967,
author = {Fornaciari, William and Sciuto, Donatella and Silvano, Cristina and Zaccaria, Vittorio},
title = {A Sensitivity-Based Design Space Exploration Methodology for Embedded Systems},
year = {2002},
issue_date = {September 2002},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {7},
number = {1–2},
issn = {0929-5585},
url = {https://doi.org/10.1023/A:1019791213967},
doi = {10.1023/A:1019791213967},
abstract = {In this paper, we propose a system-level design methodology for the efficient exploration of the architectural parameters of the memory sub-systems, from the energy-delay joint perspective. The aim is to find the best configuration of the memory hierarchy without performing the exhaustive analysis of the parameters space. The target system architecture includes the processor, separated instruction and data caches, the main memory, and the system buses. To achieve a fast convergence toward the near-optimal configuration, the proposed methodology adopts an iterative local-search algorithm based on the sensitivity analysis of the cost function with respect to the tuning parameters of the memory sub-system architecture. The exploration strategy is based on the Energy-Delay Product (EDP) metric taking into consideration both performance and energy constraints. The effectiveness of the proposed methodology has been demonstrated through the design space exploration of a real-world case study: the optimization of the memory hierarchy of a MicroSPARC2-based system executing the set of Mediabench benchmarks for multimedia applications. Experimental results have shown an optimization speedup of 2 orders of magnitude with respect to the full search approach, while the near-optimal system-level configuration is characterized by a distance from the optimal full search configuration in the range of 2%.},
journal = {Des. Autom. Embedded Syst.},
month = sep,
pages = {7–33},
numpages = {27},
keywords = {power and performance optimization, Design space exploration}
}

@inproceedings{10.1109/ASE.2011.6100096,
author = {Oster, Zachary J. and Santhanam, Ganesh Ram and Basu, Samik},
title = {Automating analysis of qualitative preferences in goal-oriented requirements engineering},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100096},
doi = {10.1109/ASE.2011.6100096},
abstract = {In goal-oriented requirements engineering, a goal model graphically represents relationships between the required goals (functional requirements), tasks (realizations of goals), and optional goals (non-functional properties) involved in designing a system. It may, however, be impossible to find a design that fulfills all required goals and all optional goals. In such cases, it is useful to find designs that provide the required functionality while satisfying the most preferred set of optional goals under the goal model's constraints. We present an approach that considers expressive qualitative preferences over optional goals, as these can model interacting and/or mutually exclusive subgoals. Our framework employs a model checking-based method for reasoning with qualitative preferences to identify the most preferred alternative(s). We evaluate our approach using existing goal models from the literature.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {448–451},
numpages = {4},
series = {ASE '11}
}

@inproceedings{10.5555/1885639.1885651,
author = {Nolan, Andy J. and Abrah\~{a}o, Silvia},
title = {Dealing with cost estimation in software product lines: experiences and future directions},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {After 5 years invested in developing accurate cost estimation tools, Rolls-Royce has learnt about the larger potential of the tools to shape many aspects of the business. A good estimation tool is a "model" of a project and is usually used to estimate cost and schedule, but it can also estimate and validate risks and opportunities. Estimation tools have unified engineering, project and business needs. The presence of good estimation tools has driven higher performance and stability in the business. It was evident we needed this capability to underpin decisions in our new Software Product Line strategy. The objective of this paper is twofold. First, we report the experiences gained in the past on the use of estimation tools. Second, we describe the current efforts and future directions on the development of an estimation tool for Software Product Lines. At the heart of the Product Line estimation tool is a simple representation of the product - represented as the number of Lines Of Code (LOC). The next generation of tool, will need to consider wider aspects of product quality in order to create more accurate estimates and support better decisions about our products.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {121–135},
numpages = {15},
keywords = {software product lines, industrial experiences, cost estimation},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.5555/2050167.2050171,
author = {Nunes, Ingrid and Cowan, Donald and Cirilo, Elder and De Lucena, Carlos J. P.},
title = {A case for new directions in agent-oriented software engineering},
year = {2010},
isbn = {9783642226359},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The state-of-the-art of Agent-oriented Software Engineering (AOSE) is insufficiently reflected in the state-of-practice in developing complex distributed systems. This paper discusses software engineering (SE) areas that have not been widely addressed in the context of AOSE, leading to a lack of mechanisms that support the development of Multiagent Systems (MASs) based on traditional SE principles, such as modularity, reusability and maintainability. This discussion is based on an exploratory study of the development of a family of buyer agents following the belief-desire-intention model and using a Software Product Line architecture. Based on the discussion presented in this paper, we hope to encourage the AOSE community to address particular SE issues on the development of MAS that have not yet been (widely) considered.},
booktitle = {Proceedings of the 11th International Conference on Agent-Oriented Software Engineering},
pages = {37–61},
numpages = {25},
keywords = {software reuse, software product lines, software architectures, multi-agent systems, agent-oriented software engineering},
location = {Toronto, Canada},
series = {AOSE'10}
}

@inproceedings{10.5555/2337223.2337468,
author = {Colanzi, Thelma Elita},
title = {Search based design of software product lines architectures},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {The Product-Line Architecture (PLA) is the main artifact of a Software Product Line (SPL). However, obtaining a modular, extensible and reusable PLA is a people-intensive and non-trivial task, related to different and possible conflicting factors. Hence, the PLA design is a hard problem and to find the best architecture can be formulated as an optimization problem with many factors. Similar Software Engineering problems have been efficiently solved by search-based algorithms in the field known as Search-based Software Engineering. The existing approaches used to optimize software architecture are not suitable since they do not encompass specific characteristics of SPL. To easy the SPL development and to automate the PLA design this work introduces a multi-objective optimization approach to the PLA design. The approach is now being implemented by using evolutionary algorithms. Empirical studies will be performed to validate the neighborhood operators, SPL measures and search algorithms chosen. Finally, we intend to compare the results of the proposed approach with PLAs designed by human architects.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1507–1510},
numpages = {4},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1109/ICSE.2019.00112,
author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
title = {Distance-based sampling of software configuration spaces},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00112},
doi = {10.1109/ICSE.2019.00112},
abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1084–1094},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/1454268.1454275,
author = {Bertoncello, Ivo Augusto and Dias, Marcelo Oliveira and Brito, Patrick H. S. and Rubira, Cec\'{\i}lia M. F.},
title = {Explicit exception handling variability in component-based product line architectures},
year = {2008},
isbn = {9781605582290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1454268.1454275},
doi = {10.1145/1454268.1454275},
abstract = {Separation of concerns is one of the overarching goals of exception handling in order to keep separate normal and exceptional behaviour of a software system. In the context of a software product line (SPL), this separation of concerns is also important for designing software variabilities related to different exception handling strategies, such as the choice of different handlers depending on the set of selected features. This paper presents a method for refactoring object-oriented product line architecture in order to separate explicitly their normal and exceptional behaviour into different software components. The new component-based software architecture includes variation points related to different choices of exception handlers that can be selected during product instantiations, thus facilitating the evolution of the exceptional behaviour. The feasibility of the proposed approach is assessed through a SPL of mobile applications.},
booktitle = {Proceedings of the 4th International Workshop on Exception Handling},
pages = {47–54},
numpages = {8},
keywords = {software architecture, exceptional behaviour, exception handling, component-based software development},
location = {Atlanta, Georgia},
series = {WEH '08}
}

@article{10.1145/3011286.3011291,
author = {Galster, Matthias and Zdun, Uwe and Weyns, Danny and Rabiser, Rick and Zhang, Bo and Goedicke, Michael and Perrouin, Gilles},
title = {Variability and Complexity in Software Design: Towards a Research Agenda},
year = {2017},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/3011286.3011291},
doi = {10.1145/3011286.3011291},
abstract = {Many of today's software systems accommodate different usage and deployment scenarios. Intentional and unintentional variability in functionality or quality attributes (e.g., performance) of software significantly increases the complexity of the problem and design space of those systems. The complexity caused by variability becomes increasingly difficult to handle due to the increasing size of software systems, new and emerging application domains, dynamic operating conditions under which software systems have to operate, fast moving and highly competitive markets, and more powerful and versatile hardware. This paper reports results of the first International Workshop on Variability and Complexity in Software Design that brought together researchers and engineers interested in the topic of complexity and variability. It also outlines directions the field might move in the future},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {27–30},
numpages = {4},
keywords = {software design, complexity, Variability}
}

@inproceedings{10.1007/11424758_6,
author = {Kim, Soo Dong and Chang, Soo Ho and La, Hyun Jung},
title = {A systematic process to design product line architecture},
year = {2005},
isbn = {3540258604},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11424758_6},
doi = {10.1007/11424758_6},
abstract = {Product Line Engineering is being accepted as a representative software reuse methodology by using core assets and product line architecture is known as a key element of core assets. However, current research on product line engineering has room to provide specific and detailed guidelines of designing product line architectures and reflecting variability in the architecture. In this paper, we present a reference model and a process to design the architecture with detailed instructions. Especially architectural variability is codified by describing decision model representing variation.},
booktitle = {Proceedings of the 2005 International Conference on Computational Science and Its Applications - Volume Part I},
pages = {46–56},
numpages = {11},
location = {Singapore},
series = {ICCSA'05}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {variability, developer study, configuration management and life cycle, Dimensions of software configuration},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/1944892.1944894,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Th\"{u}m, Thomas and Saake, Gunter},
title = {Multi-dimensional variability modeling},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944894},
doi = {10.1145/1944892.1944894},
abstract = {The variability of a software product line (SPL)is often described with a feature model. To avoid highly complex models, stakeholders usually try to separate different variability dimensions, such as domain variability and implementation variability. This results in distinct variability models, which are easier to handle than one large model. On the other hand, it is sometimes required to analyze the variability dimensions of an SPL in combination using a single model only. To combine separate modeling and integrated analysis of variability, we present Velvet, a language for multi-dimensional variability modeling. Velvet allows stakeholders to model each variability dimension of an SPL separately and to compose the separated dimensions on demand. This improves reuse of feature models and supports independent modeling variability dimensions. Furthermore, Velvet integrates feature modeling and configuration in a single language. The combination of both concepts creates further reuse opportunities and allows stakeholders to independently configure variability dimensions.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {11–20},
numpages = {10},
keywords = {variability modeling, separation of concerns, feature models},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@article{10.1007/s11761-014-0161-y,
author = {Huergo, Rosane S. and Pires, Paulo F. and Delicato, Flavia C. and Costa, Bruno and Cavalcante, Everton and Batista, Thais},
title = {A systematic survey of service identification methods},
year = {2014},
issue_date = {September 2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {3},
issn = {1863-2386},
url = {https://doi.org/10.1007/s11761-014-0161-y},
doi = {10.1007/s11761-014-0161-y},
abstract = {One of the major challenges for the adoption of the service-oriented architecture (SOA) is the service identification phase that aims to determine which services are appropriate to be implemented. In the last decade, several service identification methods (SIMs) were proposed. However, the service identification phase still remains a challenge to organizations due to the lack of systematic methods and comprehensive approaches that support the examination of the businesses from multiple perspectives and consider service quality attributes. This work aims to provide an overview of existing SIMs by detailing which service's perspectives, stated as relevant by the industry, are addressed by the SIMs and also by synthesizing the identification techniques used by them. We have performed a systematic survey over publications about SIMs from 2002 to June 2013, and 105 studies were selected. A detailed investigation on the analyzed SIMs revealed that the identification techniques applied by them have a correlation on how they address many of the service's perspectives. In addition, they are supporting the SOA adoption by handling many perspectives of the OASIS' reference architecture for SOA. However, most of them do not explicitly address service quality attributes and few studies support the evaluation of both. Therefore, future research should follow the direction toward hybrid methods with mechanisms to elicit business and service's quality attributes.},
journal = {Serv. Oriented Comput. Appl.},
month = sep,
pages = {199–219},
numpages = {21},
keywords = {Systematic survey, Service-oriented architecture, Service identification method, SOA, SIM}
}

@inproceedings{10.1145/1944892.1944897,
author = {Gilson, Fabian and Englebert, Vincent},
title = {Towards handling architecture design, variability and evolution with model transformations},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944897},
doi = {10.1145/1944892.1944897},
abstract = {Software systems have to face evolving requirements from information system stakeholders, infrastructure modifications, and evolving rationales about the implementation. This increases the rate of migration and redeployment of systems. Recent approaches intend to abstract architectural element specifications from the implementing technology and manage software design through model transformations. Based on an Architecture Description Language integrating infrastructure modelling facilities and a requirement modelling language, the present work manages architecturally significant requirements and infrastructure evolutions by model transformations. Our approach offers support for evolution and variability management tasks as it makes explicit the rationales concerning requirements, infrastructure and implementation alternatives that guide both the software architecture and the infrastructure definition.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {39–48},
numpages = {10},
keywords = {model transformation, infrastructure constraint, architecture variability, architecture description language, architecturally significant requirement},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@inproceedings{10.1145/3302333.3302350,
author = {Garc\'{\i}a, Sergio and Str\"{u}ber, Daniel and Brugali, Davide and Di Fava, Alessandro and Schillinger, Philipp and Pelliccione, Patrizio and Berger, Thorsten},
title = {Variability Modeling of Service Robots: Experiences and Challenges},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302350},
doi = {10.1145/3302333.3302350},
abstract = {Sensing, planning, controlling, and reasoning, are human-like capabilities that can be artificially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specifically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specific variability challenges in robotics software engineering and to inspire other researchers to advance this field.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {6},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3180155.3180163,
author = {Guo, Jianmei and Shi, Kai},
title = {To preserve or not to preserve invalid solutions in search-based software engineering: a case study in software product lines},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180163},
doi = {10.1145/3180155.3180163},
abstract = {Multi-objective evolutionary algorithms (MOEAs) have been successfully applied for software product lines (SPLs) to search for optimal or near-optimal solutions that balance multiple objectives. However, MOEAs usually produce invalid solutions that violate the constraints predefined. As invalid solutions are unbuildable in practice, we debate the preservation of invalid solutions during the search. We conduct experiments on seven real-world SPLs, including five largest SPLs hitherto reported and two SPLs with realistic values and constraints of quality attributes. We identify three potential limitations of preserving invalid solutions. Furthermore, based on the state-of-the-art, we design five algorithm variants that adopt different evolutionary operators. By performance evaluation, we provide empirical guidance on how to preserve valid solutions. Our empirical study demonstrates that whether or not to preserve invalid solutions deserves more attention in the community, and in some cases, we have to preserve valid solutions all along the way.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1027–1038},
numpages = {12},
keywords = {constraint solving, multi-objective evolutionary algorithms, search-based software engineering, software product lines, validity},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1504/IJAOSE.2008.016800,
author = {Verstraete, Paul and Germain, Bart Saint and Valckenaers, Paul and Brussel, Hendrik Van and Belle, Jan Van and Hadeli},
title = {Engineering manufacturing control systems using PROSA and delegate MAS},
year = {2008},
issue_date = {January 2008},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {2},
number = {1},
issn = {1746-1375},
url = {https://doi.org/10.1504/IJAOSE.2008.016800},
doi = {10.1504/IJAOSE.2008.016800},
abstract = {This paper presents a systematic description of a reusable software architecture for multiagent systems in the domain of manufacturing control. The architectural description consolidates the authors' expertise in this area. Until now, the research has taken a manufacturing control perspective of multiagent systems. The research team has focused on providing benefits to the manufacturing control domain by designing a novel type of control system. This paper takes a software architectural perspective of multiagent manufacturing control. The systematic description specifies a software product line architecture for manufacturing control. The paper describes the assets of the software product line architecture and how these assets can be combined.},
journal = {Int. J. Agent-Oriented Softw. Eng.},
month = jan,
pages = {62–89},
numpages = {28},
keywords = {software reuse, software architecture, multi-agent systems, manufacturing control, agent-based systems, MASs}
}

@article{10.1007/s00607-018-0646-1,
author = {Galindo, Jos\'{e} A. and Benavides, David and Trinidad, Pablo and Guti\'{e}rrez-Fern\'{a}ndez, Antonio-Manuel and Ruiz-Cort\'{e}s, Antonio},
title = {Automated analysis of feature models: Quo vadis?},
year = {2019},
issue_date = {May       2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {101},
number = {5},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-018-0646-1},
doi = {10.1007/s00607-018-0646-1},
abstract = {Feature models have been used since the 90s to describe software product lines as a way of reusing common parts in a family of software systems. In 2010, a systematic literature review was published summarizing the advances and settling the basis of the area of automated analysis of feature models (AAFM). From then on, different studies have applied the AAFM in different domains. In this paper, we provide an overview of the evolution of this field since 2010 by performing a systematic mapping study considering 423 primary sources. We found six different variability facets where the AAFM is being applied that define the tendencies: product configuration and derivation; testing and evolution; reverse engineering; multi-model variability-analysis; variability modelling and variability-intensive systems. We also confirmed that there is a lack of industrial evidence in most of the cases. Finally, we present where and when the papers have been published and who are the authors and institutions that are contributing to the field. We observed that the maturity is proven by the increment in the number of journals published along the years as well as the diversity of conferences and workshops where papers are published. We also suggest some synergies with other areas such as cloud or mobile computing among others that can motivate further research in the future.},
journal = {Computing},
month = may,
pages = {387–433},
numpages = {47},
keywords = {Variability-intensive systems, Software product lines, Feature models, Automated analysis, 68T35}
}

@inproceedings{10.1145/3251104,
author = {Langdon, William B. and Petke, Justyna and White, David R.},
title = {Session details: Genetic Improvement 2015 Workshop},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251104},
doi = {10.1145/3251104},
abstract = {It is our great pleasure to welcome you to the first international workshop on the Genetic Improvement of Software -- GI-2015, held at GECCO'15. Our goal was to bring together research from across the globe to exchange ideas on using optimisation techniques, particularly evolutionary computation such as genetic programming, to improve existing software. We invited short position papers to encourage the discussion of new ideas and recent work in addition to longer and more concrete submissions. The call for participation invited GI work on automatic bug-fixing; improving functionality; improving non-functional properties such as efficiency, memory and energy consumption; "plastic surgery" by transplanting functionality from other existing code to host software; and automatically specialising generic software for dedicated tasks. As you will see, we have accepted papers in most of these areas as well as papers on improving the nascent genetic improvement tools in use, improving parallel code, GI's relationship with software product lines (SPL), improving security and GI for embedded systems.We had submissions from Asia, Europe and both North and South America. They were exactly evenly split between full-length submissions (8) and two page position papers (8).},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1016/j.jss.2014.12.041,
author = {Pascual, Gustavo G. and Lopez-Herrejon, Roberto E. and Pinto, M\'{o}nica and Fuentes, Lidia and Egyed, Alexander},
title = {Applying multiobjective evolutionary algorithms to dynamic software product lines for reconfiguring mobile applications},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.12.041},
doi = {10.1016/j.jss.2014.12.041},
abstract = {Mobile applications require to self-adapt their behavior to context changes.We propose a DSPL approach to manage variability at runtime.Configurations are generated using multiobjective evolutionary algorithms.We apply a fix operator to generate only valid configurations at runtime.We demonstrate that this approach is suitable for mobile environments. Mobile applications require dynamic reconfiguration services (DRS) to self-adapt their behavior to the context changes (e.g., scarcity of resources). Dynamic Software Product Lines (DSPL) are a well-accepted approach to manage runtime variability, by means of late binding the variation points at runtime. During the system's execution, the DRS deploys different configurations to satisfy the changing requirements according to a multiobjective criterion (e.g., insufficient battery level, requested quality of service). Search-based software engineering and, in particular, multiobjective evolutionary algorithms (MOEAs), can generate valid configurations of a DSPL at runtime. Several approaches use MOEAs to generate optimum configurations of a Software Product Line, but none of them consider DSPLs for mobile devices. In this paper, we explore the use of MOEAs to generate at runtime optimum configurations of the DSPL according to different criteria. The optimization problem is formalized in terms of a Feature Model (FM), a variability model. We evaluate six existing MOEAs by applying them to 12 different FMs, optimizing three different objectives (usability, battery consumption and memory footprint). The results are discussed according to the particular requirements of a DRS for mobile applications, showing that PAES and NSGA-II are the most suitable algorithms for mobile environments.},
journal = {J. Syst. Softw.},
month = may,
pages = {392–411},
numpages = {20},
keywords = {Evolutionary algorithms, Dynamic reconfiguration, DSPL}
}

@article{10.1016/j.eswa.2013.12.028,
author = {Segura, Sergio and Parejo, Jos\'{e} A. and Hierons, Robert M. and Benavides, David and Ruiz-Cort\'{e}s, Antonio},
title = {Automated generation of computationally hard feature models using evolutionary algorithms},
year = {2014},
issue_date = {June, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.12.028},
doi = {10.1016/j.eswa.2013.12.028},
abstract = {A feature model is a compact representation of the products of a software product line. The automated extraction of information from feature models is a thriving topic involving numerous analysis operations, techniques and tools. Performance evaluations in this domain mainly rely on the use of random feature models. However, these only provide a rough idea of the behaviour of the tools with average problems and are not sufficient to reveal their real strengths and weaknesses. In this article, we propose to model the problem of finding computationally hard feature models as an optimization problem and we solve it using a novel evolutionary algorithm for optimized feature models (ETHOM). Given a tool and an analysis operation, ETHOM generates input models of a predefined size maximizing aspects such as the execution time or the memory consumption of the tool when performing the operation over the model. This allows users and developers to know the performance of tools in pessimistic cases providing a better idea of their real power and revealing performance bugs. Experiments using ETHOM on a number of analyses and tools have successfully identified models producing much longer executions times and higher memory consumption than those obtained with random models of identical or even larger size.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {3975–3992},
numpages = {18},
keywords = {Software product lines, Search-based testing, Performance testing, Feature models, Evolutionary algorithms, Automated analysis}
}

@article{10.1016/j.infsof.2019.01.004,
author = {Souza, Eric and Moreira, Ana and Goul\~{a}o, Miguel},
title = {Deriving architectural models from requirements specifications: A systematic mapping study},
year = {2019},
issue_date = {May 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {109},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.01.004},
doi = {10.1016/j.infsof.2019.01.004},
journal = {Inf. Softw. Technol.},
month = may,
pages = {26–39},
numpages = {14},
keywords = {Literature review, Mapping study, Software architecture}
}

@article{10.1016/j.infsof.2008.04.002,
author = {Deelstra, Sybren and Sinnema, Marco and Bosch, Jan},
title = {Variability assessment in software product families},
year = {2009},
issue_date = {January, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2008.04.002},
doi = {10.1016/j.infsof.2008.04.002},
abstract = {Software variability management is a key factor in the success of software systems and software product families. An important aspect of software variability management is the evolution of variability in response to changing markets, business needs, and advances in technology. To be able to determine whether, when, and how variability should evolve, we have developed the COVAMOF software variability assessment method (COSVAM). The contribution of COSVAM is that it is a novel, and industry-strength assessment process that addresses the issues that are associated to the current variability assessment practice. In this paper, we present the successful validation of COSVAM in an industrial software product family.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {195–218},
numpages = {24},
keywords = {Variability, Software product families, Evolution, Assessment}
}

@inproceedings{10.1109/ICSEA.2008.80,
author = {Kim, Kangtae and Kim, Hyungrok and Kim, Sundeok and Chang, Gihun},
title = {A Case Study on SW Product Line Architecture Evaluation: Experience in the Consumer Electronics Domain},
year = {2008},
isbn = {9780769533728},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSEA.2008.80},
doi = {10.1109/ICSEA.2008.80},
abstract = {A well-executed software architecture is one of the most critical factors for achieving the intended effectiveness of a software product line as a holistic picture of a system. Today, many organizations are investing in architecture and its quality attributes for productivity, time to market and etc. In this paper, we show an evaluation framework named which provides criteria for measuring execution(designing and implementing) of an architecture and guideline for improvement based on measurement, analysis and improvement principle. We focused on software architecture design itself because we are mainly concerned with design and implementation issues thus concentrating on the architecture criteria. The contribution of the paper is to identify and demonstrate a architecture analysis and improvement process with practical experimentation results. The framework is based on design attributes of product line architecture and static analysis of architecture implementation.},
booktitle = {Proceedings of the 2008 The Third International Conference on Software Engineering Advances},
pages = {192–197},
numpages = {6},
keywords = {product line, software architecture evaluation},
series = {ICSEA '08}
}

@inproceedings{10.1145/2361999.2362027,
author = {Galster, Matthias and Avgeriou, Paris and Weyns, Danny and Becker, Martin},
title = {Second international workshop on variability in software architecture},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362027},
doi = {10.1145/2361999.2362027},
abstract = {Variability is the ability of a software system or artifact to be adapted for specific contexts, in a preplanned manner. Many of today's software systems are built with variability in mind, e.g., product lines and families, self-adaptive systems, open platforms, or service-based systems that support dynamic runtime composition of web services. Variability is reflected in and facilitated through the software architecture. Also, as the software architecture is a reference point for many development activities and for achieving quality attributes, variability should be treated as a first-class and cross-cutting concern in software architecture. Therefore, the Second International Workshop on Variability in Software Architecture (VARSA 2012) aims at identifying critical challenges and progressing the state-of-the-art on variability in software architecture. VARSA 2012 is a follow-up of the First International Workshop on Variability in Software Architecture (VARSA 2011), held at WICSA 2011.},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {132–134},
numpages = {3},
keywords = {variability, software architecture, VARSA},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@inproceedings{10.1145/2577080.2577095,
author = {Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {Probabilistic model checking for energy analysis in software product lines},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577095},
doi = {10.1145/2577080.2577095},
abstract = {In a software product line (SPL), a collection of software products is defined by their commonalities in terms of features rather than explicitly specifying all products one-by-one. Several verification techniques were adapted to establish temporal properties of SPLs. Symbolic and family-based model checking have been proven to be successful for tackling the combinatorial blow-up arising when reasoning about several feature combinations. However, most formal verification approaches for SPLs presented in the literature focus on the static SPLs, where the features of a product are fixed and cannot be changed during runtime. This is in contrast to dynamic SPLs, allowing to adapt feature combinations of a product dynamically after deployment.The main contribution of the paper is a compositional modeling framework for dynamic SPLs, which supports probabilistic and nondeterministic choices and allows for quantitative analysis. We specify the feature changes during runtime within an automata-based coordination component, enabling to reason over strategies how to trigger dynamic feature changes for optimizing various quantitative objectives, e.g., energy or monetary costs and reliability. For our framework there is a natural and conceptually simple translation into the input language of the prominent probabilistic model checker PRISM. This facilitates the application of PRISM's powerful symbolic engine to the operational behavior of dynamic SPLs and their family-based analysis against various quantitative queries. We demonstrate feasibility of our approach by a case study issuing an energy-aware bonding network device.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {169–180},
numpages = {12},
keywords = {software product lines, probabilistic model checking, energy analysis, dynamic features},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@inproceedings{10.1145/1562860.1562864,
author = {Siegmund, Norbert and Pukall, Mario and Soffner, Michael and K\"{o}ppen, Veit and Saake, Gunter},
title = {Using software product lines for runtime interoperability},
year = {2009},
isbn = {9781605585482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1562860.1562864},
doi = {10.1145/1562860.1562864},
abstract = {Today, often small, heterogeneous systems have to cooperate in order to fulfill a certain task. Interoperability between these systems is needed for their collaboration. However, achieving this interoperability raises several problems. For example, embedded systems might induce a higher probability for a system failure due to constrained power supply. Nevertheless, interoperability must be guaranteed even in scenarios where embedded systems are used. To overcome this problem, we use services to abstract the functionality from the system which realizes it. We outline how services can be generated using software product line techniques to bridge the heterogeneity of cooperating systems. Additionally, we address runtime changes of already deployed services to overcome system failures. In this paper, we show the runtime adaption process of these changes which includes the following two points. First, we outline why feature-oriented programming is appropriate in such scenarios. Second, we describe the runtime adaption process of services with feature-oriented programming.},
booktitle = {Proceedings of the Workshop on AOP and Meta-Data for Software Evolution},
articleno = {4},
numpages = {7},
keywords = {software product lines, runtime adaption, interoperability},
location = {Genova, Italy},
series = {RAM-SE '09}
}

@inproceedings{10.1145/3330204.3330212,
author = {Silva, Thiciane Suely Couto and Rocha, Fabio Gomes and dos Santos, Rodrigo Pereira},
title = {Resource Demand Management in Java Ecosystem},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330212},
doi = {10.1145/3330204.3330212},
abstract = {The evolution of Software Ecosystems (SECO) requires to satisfy community's needs based on platform's and applications' requirements, known as SECO demands. Such SECO demands must be evaluated, approved and subsequently translated into platform resources (e.g., API, framework, library). In this context, this work applies a research approach based on primary and secondary studies to investigate how resource demands are managed in Java SECO. To do so, we conducted a systematic mapping study on the existing models, methods and conditioning factors for supporting emergence and/or inclusion of SECO platforms' resources. Based on the results, we planned and executed a survey research with members of a real SECO, more especifically Java Community Process (JCP) Committee members. From the studies, we identified seven methods for collecting data related to the emergence of new APIs, including a method for architecture assessment based on quality attributes, and finally ten conditioning factors for API management that can support SECO evolution. The results also allowed us to organize a process for managing demands of resources in Java SECO platform. We found out that the lack of standardization for management of SECO demands can hinder platforms/applications management. Therefore, this field is worth of further in-depth studies.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {3},
numpages = {8},
keywords = {Software Ecosystem, Resource Demand Management, Java},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@phdthesis{10.5555/2049065,
author = {Sun, Hongyu},
advisor = {Lutz, Robyn R.},
title = {Quantifiable non-functional requirements modeling and static verification for web service compositions},
year = {2010},
isbn = {9781124430300},
publisher = {Iowa State University},
address = {USA},
abstract = {As service oriented architectures have become more widespread in industry, many complex web services are assembled using independently developed modular services from different vendors. Although the functionalities of the composite web services are ensured during the composition process, the non-functional requirements (NFRs) are often ignored in this process. Since quality of services plays a more and more important role in modern service-based systems, there is a growing need for effective approaches to verifying that a composite web service not only offers the required functionality but also satisfies the desired NFRs. Current approaches to verifying NFRs of composite services (as opposed to individual services) remain largely ad-hoc and informal in nature. This is especially problematic for high-assurance composite web services. High-assurance composite web services are those composite web services with special concern on critical NFRs such as security, safety and reliability. Examples of such applications include traffic control, medical decision support and the coordinated response systems for civil emergencies. The latter serves to motivate and illustrate the work described here.In this dissertation we develop techniques for ensuring that a composite service meets the user-specified NFRs expressible as hard constraints, e.g., “the messages of particular operations must be authenticated.” We introduce an automata-based framework for verifying that a composite service satisfies the desired NFRs based on the known guarantees regarding the non-functional properties of the component services. This automata-based model is able to represent NFRs that are hard, quantitative constraints on the composite web services. This model addresses two issues previously not handled in the modeling and verification of NFRs for composite web services: (1) the scope of the NFRs and (2) consistency checking of multiple NFRs.A scope of a NFR on a web service composition is the effective range of the NFR on the sub-workflows and modular services of the web service composition. It allows more precise description of a NFR constraint and more efficient verification. When multiple NFRs exist and overlap in their scopes, consistency checking is necessary to avoid wasted verification efforts on conflicting constraints. The approach presented here captures scope information in the model and uses it to check the consistency of multiple NFRs prior to the static verification of web service compositions. We illustrate how our approach can be used to verify security requirements for an Emergency Management System. We then focus on families of highly-customizable, composed web services where repeated verification of similar sets of NFRs can waste computation resources. We introduce a new approach to extend software product line engineering techniques to the web service composition domain. The resulting technique uses a partitioning similar to that between domain engineering and application engineering in the product-line context. It specifies the options that the user can select and constructs the resulting web service compositions. By first creating a web-service composition search space that satisfies the common requirements and then querying the search space as the user makes customization decisions, the technique provides a more efficient way to verify customizable web services. A decision model, illustrated with examples from the emergency-response application, is created to interact with the customers and ensure the consistency of their specifications. The capability to reuse the composition search space is shown to improve the quality of the composite services and reduce the cost of reverifying the same compositions. By distinguishing the commonalities and the variabilities of the web services, we divide the web composition into two stages: the preparation stage (to construct all commonalities) and the customization stage (to choose optional and alternative features). We thus draw most of the computation overhead into the first stage during the design in order to enable improved runtime efficiency during the second stage.A simulation platform was constructed to conduct experiments on the two verification approaches and three strategies introduced in this dissertation. The results of these experiments were analyzed to show the advantage of our automaton-based model in its verification efficiency with scoping information. We have shown how to choose the most efficient verification strategy from the three strategies of verifying multiple NFRs introduced in this dissertation under different circumstances. The results indicate that the software product line approach has significant efficiency improvement over traditional on-demand verification for highly customizable web service compositions.},
note = {AAI3438737}
}

@inproceedings{10.1007/978-3-662-45234-9_10,
author = {Bure\v{s}, Tom\'{a}\v{s} and Hork\'{y}, Vojtundefinedch and Kit, Micha\l{} and Marek, Luk\'{a}\v{s} and T\r{u}ma, Petr},
title = {Towards Performance-Aware Engineering of Autonomic Component Ensembles},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_10},
doi = {10.1007/978-3-662-45234-9_10},
abstract = {Ensembles of autonomic components are a novel software engineering paradigm for development of open-ended distributed highly dynamic software systems e.g. smart cyber-physical systems. Recent research centered around the concept of ensemble-based systems resulted in design and development models that aim to systematize and simplify the engineering process of autonomic components and their ensembles. These methods highlight the importance of covering both the functional concepts and the non-functional properties, specifically performance-related aspects of the future systems. In this paper we propose an integration of the emerging techniques for performance assessment and awareness into different stages of the development process. Our goal is to aid both designers and developers of autonomic component ensembles with methods providing performance awareness throughout the entire development life cycle including runtime.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {131–146},
numpages = {16},
keywords = {performance engineering, ensemble-based systems, component systems}
}

@article{10.5555/3044222.3051232,
author = {Montalvillo, Leticia and D\'{\i}az, Oscar},
title = {Requirement-driven evolution in software product lines},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
abstract = {We conducted a systematic mapping study on SPL evolution.We identified 107 relevant contributions on the topic up to mid 2015.We elaborated on the traditional change mini-cycle to classify the contributions.We identified well-established topics, trends and open research issues. CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As SPLs exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the SPL as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring.OBJECTIVE. Research on SPL evolution has not been previously mapped. This work provides a mapping study along Petersen's and Kichenham's guidelines, to identify strong areas of knowledge, trends and gaps.RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, SPL architecture, code assets and products).CONCLUSION. Analyses of the results indicate that "Solution proposals" are the most common type of contribution (31%). Regarding the evolution activity, "Implement change" (43%) and "Analyze and plan change" (37%) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included.},
journal = {J. Syst. Softw.},
month = dec,
pages = {110–143},
numpages = {34},
keywords = {Systematic mapping study, Software product lines, Evolution}
}

@inproceedings{10.1007/11554844_17,
author = {Trew, Tim},
title = {Enabling the smooth integration of core assets: defining and packaging architectural rules for a family of embedded products},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_17},
doi = {10.1007/11554844_17},
abstract = {One of the remaining challenges in product line engineering is how to establish the quality of the reusable assets so that we can be confident that they can be configured and composed reliably. This is desirable, both to avoid having to completely re-test each product and to avoid integration faults only being detected late in product development. One of the diversity mechanisms of Philips’ high-end TV product line is the selection and com position of sub-systems, so different sub-system variants must integrate reli ably if the aims of the product line are to be realized. An earlier study of in tegration testing obligations in Philips products concluded that certain design policies must be imposed if integration testing is to be feasible, but it did not describe how relevant policies could be identified at the earliest stages of de sign. This paper addresses how a set of architectural rules were established for the TV product line through a root-cause analysis of problem reports, and packaged so that developers can recognize when they should be applied. The approach builds on other work on the impact of design choices on non-func-tional requirements to ensure that all quality attributes are addressed.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {137–149},
numpages = {13},
location = {Rennes, France},
series = {SPLC'05}
}

@article{10.1016/j.jss.2017.03.044,
author = {Nuez-Varela, Alberto S. and Prez-Gonzalez, Hctor G. and Martnez-Perez, Francisco E. and Soubervielle-Montalvo, Carlos},
title = {Source code metrics},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.044},
doi = {10.1016/j.jss.2017.03.044},
abstract = {Three major programming paradigms measured by source code metrics were identified.The CK metrics and the object oriented paradigm are the most studied subjects.Java benchmark systems are the most commonly measured systems in research.Technology on metrics extraction mechanisms are not up to research advances.Empirical studies have a major impact on the code metrics community. ContextSource code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. ObjectivesThis paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. MethodA systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. ResultsAlmost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. ConclusionsObject oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.},
journal = {J. Syst. Softw.},
month = jun,
pages = {164–197},
numpages = {34},
keywords = {Systematic mapping study, Source code metrics, Software metrics, Object-oriented metrics, Feature-oriented metrics, Aspect-oriented metrics}
}

@proceedings{10.1145/2304736,
title = {CBSE '12: Proceedings of the 15th ACM SIGSOFT symposium on Component Based Software Engineering},
year = {2012},
isbn = {9781450313452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 15th ACM SIGSOFT Symposium on Component Based Software Engineering (CBSE 2012) marks a milestone in the research on using components to build software systems in an efficient way. Over the years, this symposium has established a track record of bringing together researchers and practitioners from a variety of disciplines to promote a better understanding of CBSE from diverse perspectives, and to engage in active discussion and debate.Component-based software engineering continues to attract interest and evolve as a discipline for the rapid assembly of flexible software systems. CBSE combines elements of software requirements engineering, architecture, design, verification, testing, configuration and deployment. The role of and need for CBSE in industrial application remains critical.New trends in global services, distributed systems architectures, dynamically adaptable systems, and large-scale software systems often cross organizational boundaries and push the limits of established component-based methods, tools, and platforms. Innovative solutions from diverse paradigms (e.g., service-, aspect-, and agent-oriented) are needed to address these emerging trends. Topics of interest for CBSE 2012 therefore include, but are not limited to, the following:  Specification, architecture, and design of component models and component-based systems Software quality assurance for component-based engineering Verification, testing and certification of component-based systems Component composition, binding, and dynamic adaptation Component-based engineering with agents, aspects, or services Component-based product line engineering Non-functional properties (quality of service attributes) in component-based engineering Patterns and frameworks for component-based engineering Tools and methods for component-based engineering Industrial experience using component-based software development Empirical studies in component-based software engineering Teaching component-based software engineeringIn addition to the above, this year we have a special theme: Components for Achieving Long-Lived Systems. Many industrial systems have very strict requirements for uninterrupted operation. There are examples of systems that have aimed to provide continuous operation for more than 15 years (that is, since the first CBSE!). Such requirements place significant demands on the underlying architecture, mandating that the architecture be very well understood and carefully designed. In turn, the architecture, if implemented correctly, forms a foundation for achieving critical quality attributes such as dependability, robustness, usability, and flexibility. The principles of component-based software engineering offer a promise for achieving effective architectures for long-lived systems. This is especially so since this approach natively provides the ability to add, remove, replace, and/or modify components during operation. A related class of approaches deals with self-management in component-based systems in order to ensure continuous operation.CBSE 2012 received 50 submissions, each of which received at least three independent reviews by the CBSE Program Committee. The careful review process included an on-line discussion, after which 11 papers (22%) have been accepted for publication in this volume as full papers. Eleven more submissions have been selected as short papers. We have assembled an exciting program that shows that this is still a very vibrant and relevant community. We hope to see you at CBSE 2012 in Bertinoro!This year, CBSE is again part of the federated event CompArch, together with "QoSA 2012: 8th International ACM SIGSOFT Conference on the Quality of Software Architectures," "ISARCS 2012: 3rd International ACM SIGSOFT Symposium on Architecting Critical Systems," "WCOP 2012: 17th International Doctoral Symposium on Components and Architecture," and "ROSS 2012: Workshop on Reusing Open-Source Components."},
location = {Bertinoro, Italy}
}

@article{10.1016/j.jss.2016.09.045,
author = {Parejo, Jos\'{e} A. and S\'{a}nchez, Ana B. and Segura, Sergio and Ruiz-Cort\'{e}s, Antonio and Lopez-Herrejon, Roberto E. and Egyed, Alexander},
title = {Multi-objective test case prioritization in highly configurable systems},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.09.045},
doi = {10.1016/j.jss.2016.09.045},
abstract = {A multi-objective test case prioritization real-world case study is presented.Seven objective functions based on functional and non-functional data are proposed.Comparison of the effectiveness of 63 combinations of up to three objectives.NSGA-II evolutionary algorithm to solve the multi-objective prioritization problem.Multi-objective prioritization is more effective than mono-objective approaches. Test case prioritization schedules test cases for execution in an order that attempts to accelerate the detection of faults. The order of test cases is determined by prioritization objectives such as covering code or critical components as rapidly as possible. The importance of this technique has been recognized in the context of Highly-Configurable Systems (HCSs), where the potentially huge number of configurations makes testing extremely challenging. However, current approaches for test case prioritization in HCSs suffer from two main limitations. First, the prioritization is usually driven by a single objective which neglects the potential benefits of combining multiple criteria to guide the detection of faults. Second, instead of using industry-strength case studies, evaluations are conducted using synthetic data, which provides no information about the effectiveness of different prioritization objectives. In this paper, we address both limitations by studying 63 combinations of up to three prioritization objectives in accelerating the detection of faults in the Drupal framework. Results show that non-functional properties such as the number of changes in the features are more effective than functional metrics extracted from the configuration model. Results also suggest that multi-objective prioritization typically results in faster fault detection than mono-objective prioritization.},
journal = {J. Syst. Softw.},
month = dec,
pages = {287–310},
numpages = {24},
keywords = {Variability, Test case prioritization, Highly-configurable systems, Automated software testing}
}

@article{10.1016/j.infsof.2016.08.005,
author = {Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Requirements monitoring frameworks},
year = {2016},
issue_date = {December 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {80},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.08.005},
doi = {10.1016/j.infsof.2016.08.005},
abstract = {Analyze the characteristics and application areas of monitoring approaches proposed in different domains.Systematically identify frameworks supporting requirements monitoring.Analyze to what extent the monitoring frameworks support requirements monitoring in SoS. ContextSoftware systems today often interoperate with each other, thus forming a system of systems (SoS). Due to the scale, complexity, and heterogeneity of SoS, determining compliance with their requirements is challenging, despite the range of existing monitoring approaches. The fragmented research landscape and the diversity of existing approaches, however, make it hard to understand and analyze existing research regarding its suitability for SoS. ObjectiveThe aims of this paper are thus to systematically identify, describe, and classify existing approaches for requirements-based monitoring of software systems at runtime. Specifically, we (i) analyze the characteristics and application areas of monitoring approaches proposed in different domains, we (ii) systematically identify frameworks supporting requirements monitoring, and finally (iii) analyze their support for requirements monitoring in SoS. MethodWe performed a systematic literature review (SLR) to identify existing monitoring approaches and to classify their key characteristics and application areas. Based on this analysis we selected requirements monitoring frameworks, following a definition by Robinson, and analyzed them regarding their support for requirements monitoring in SoS. ResultsWe identified 330 publications, which we used to produce a comprehensive overview of the landscape of requirements monitoring approaches. We analyzed these publications regarding their support for Robinson's requirements monitoring layers, resulting in 37 identified frameworks. We investigated how well these frameworks support requirements monitoring in SoS. ConclusionsWe conclude that most existing approaches are restricted to certain kinds of checks, particular types of events and data, and mostly also limited to one particular architectural style and technology. This lack of flexibility makes their application in an SoS context difficult. Also, systematic and automated variability management is still missing. Regarding their evaluation, many existing frameworks focus on measuring the performance overhead, while only few frameworks have been assessed in cases studies with real-world systems.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {89–109},
numpages = {21},
keywords = {Systems of systems, Systematic literature review, Requirements monitoring}
}

@inproceedings{10.1145/2110147.2110160,
author = {Schroeter, Julia and Cech, Sebastian and G\"{o}tz, Sebastian and Wilke, Claas and A\ss{}mann, Uwe},
title = {Towards modeling a variable architecture for multi-tenant SaaS-applications},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110160},
doi = {10.1145/2110147.2110160},
abstract = {A widespread business model in cloud computing is to offer software as a service (SaaS) over the Internet. Such applications are often multi-tenant aware, which means that multiple tenants share hardware and software resources of the same application instance. However, SaaS stakeholders have different or even contradictious requirements and interests: For a user, the application's quality and non-functional properties have to be maximized (e.g., choosing the fastest available algorithm for a computation at runtime). In contrast, a resource or application provider is interested in minimizing the operating costs while maximizing his profit. Finally, tenants are interested in offering a customized functionality to their users. To identify an optimal compromise for all these objectives, multiple levels of variability have to be supported by reference architectures for multi-tenant SaaS applications. In this paper, we identify requirements for such a runtime architecture addressing the individual interests of all involved stakeholders. Furthermore, we show how our existing architecture for dynamically adaptive applications can be extended for the development and operation of multi-tenant applications.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {111–120},
numpages = {10},
keywords = {variability modeling, software-as-a-service, self-optimization, multi-tenancy, auto-tuning},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.5555/645882.672251,
author = {Yacoub, Sherif M.},
title = {Performance Analysis of Component-Based Applications},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Performance analysis is a software engineering activity that involves analyzing a software application with respect to performance quality attributes such as response and execution times. Performance analysis tools provide the necessary support for the analyst to monitor program execution, record and analyze performance data, and locate and understand areas of poor performance. Performance analysis methods and techniques are highly dependent on the properties of the software system to be analyzed. Product line engineering applications possess some special properties that impose constraints on the selection of the performance analysis techniques to be applied and the tools to be used. The development of a component-based reference architecture is crucial to the success of a true product line. The component-based nature facilitates the integration of components and the replacement of a component with another to meet the requirements of an instance application of the product line. In this paper, we discuss performance analysis of component-based software systems and its automation. We discuss how component-based system properties influence the selection of methods and tools used to obtain and analyze performance measures. We use a case study of the document content remastering product line to illustrate the application of a performance analysis method to component-based applications.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {299–315},
numpages = {17},
keywords = {performance analysis, component-based software engineering (CBSE), application and component profiling, and performance tools},
series = {SPLC 2}
}

@inproceedings{10.5555/1947545.1947579,
author = {Schlegel, Christian and Steck, Andreas and Brugali, Davide and Knoll, Alois},
title = {Design abstraction and processes in robotics: from code-driven to model-driven engineering},
year = {2010},
isbn = {3642173187},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Advanced software engineering is the key factor in the design of future complex cognitive robots. It will decide about their robustness, (run-time) adaptivity, cost-effectiveness and usability.We present a novel overall vision of a model-driven engineering approach for robotics that fuses strategies for robustness by design and robustness by adaptation. It enables rigid definitions of quality-of-service, re-configurability and physics-based simulation as well as for seamless system level integration of disparate technologies and resource awareness.We report on steps towards implementing this idea driven by a first robotics meta-model with first explications of non-functional properties. A model-driven toolchain provides the model transformation and code generation steps. It also provides design time analysis of resource parameters (e.g. schedulability analysis of realtime tasks) as step towards resource awareness in the development of integrated robotic systems.},
booktitle = {Proceedings of the Second International Conference on Simulation, Modeling, and Programming for Autonomous Robots},
pages = {324–335},
numpages = {12},
location = {Darmstadt, Germany},
series = {SIMPAR'10}
}

@book{10.5555/2692450,
author = {Mistrik, Ivan and Bahsoon, Rami and Eeles, Peter and Roshandel, Roshanak and Stal, Michael},
title = {Relating System Quality and Software Architecture},
year = {2014},
isbn = {0124170099},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {System Quality and Software Architecture collects state-of-the-art knowledge on how to intertwine software quality requirements with software architecture and how quality attributes are exhibited by the architecture of the system. Contributions from leading researchers and industry evangelists detail the techniques required to achieve quality management in software architecting, and the best way to apply these techniques effectively in various application domains (especially in cloud, mobile and ultra-large-scale/internet-scale architecture) Taken together, these approaches show how to assess the value of total quality management in a software development process, with an emphasis on architecture. The book explains how to improve system quality with focus on attributes such as usability, maintainability, flexibility, reliability, reusability, agility, interoperability, performance, and more. It discusses the importance of clear requirements, describes patterns and tradeoffs that can influence quality, and metrics for quality assessment and overall system analysis. The last section of the book leverages practical experience and evidence to look ahead at the challenges faced by organizations in capturing and realizing quality requirements, and explores the basis of future work in this area.Explains how design decisions and method selection influence overall system quality, and lessons learned from theories and frameworks on architectural qualityShows how to align enterprise, system, and software architecture for total qualityIncludes case studies, experiments, empirical validation, and systematic comparisons with other approaches already in practice.}
}

@inproceedings{10.5555/1768904.1768909,
author = {Gallina, Barbara and Guelfi, Nicolas},
title = {A template for requirement elicitation of dependable product lines},
year = {2007},
isbn = {9783540730309},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Engineering software quickly and at a low cost, while preserving quality, is a well-known objective that has not been reached. Reducing the development time can be achieved by reusing software components, as proposed in the software product line development approach. Dependability may be one of the most important attributes concerning quality, due to negative consequences (health, cost, time, etc.) induced by non-dependable software. Our proposal, presented in this article, is to offer a means to elicit the requirements of a product line, such that the dependability attribute would be explicitly considered, and such that reuse would be achieved by differentiating commonalities and variabilities between products. The proposed semi-formal template includes product commonality and variability elicitation, as well as elicitation of normal, misuse and recovery scenarios. Furthermore, we allow the elicitation of the advanced transactional nature of scenarios, since it provides us with a way to elicit fault tolerance requirements, which is our targeted means to achieving dependability.},
booktitle = {Proceedings of the 13th International Working Conference on Requirements Engineering: Foundation for Software Quality},
pages = {63–77},
numpages = {15},
location = {Trondheim, Norway},
series = {REFSQ'07}
}

@inproceedings{10.5555/2820518.2820528,
author = {Moura, Irineu and Pinto, Gustavo and Ebert, Felipe and Castor, Fernando},
title = {Mining energy-aware commits},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Over the last years, energy consumption has become a first-class citizen in software development practice. While energy-efficient solutions on lower-level layers of the software stack are well-established, there is convincing evidence that even better results can be achieved by encouraging practitioners to participate in the process. For instance, previous work has shown that using a newer version of a concurrent data structure can yield a 2.19x energy savings when compared to the old associative implementation [75]. Nonetheless, little is known about how much software engineers are employing energy-efficient solutions in their applications and what solutions they employ for improving energy-efficiency. In this paper we present a qualitative study of "energy-aware commits". Using Github as our primary data source, we perform a thorough analysis on an initial sample of 2,189 commits and carefully curate a set of 371 energy-aware commits spread over 317 real-world non-trivial applications. Our study reveals that software developers heavily rely on low-level energy management approaches, such as frequency scaling and multiple levels of idleness. Also, our findings suggest that ill-chosen energy saving techniques can impact the correctness of an application. Yet, we found what we call "energy-aware interfaces", which are means for clients (e.g., developers or end-users) to save energy in their applications just by using a function, abstracting away the low-level implementation details.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {56–67},
numpages = {12},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3338906.3338974,
author = {Ne\v{s}i\'{c}, Damir and Kr\"{u}ger, Jacob and St\u{a}nciulescu, undefinedtefan and Berger, Thorsten},
title = {Principles of feature modeling},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338974},
doi = {10.1145/3338906.3338974},
abstract = {Feature models are arguably one of the most intuitive and successful notations for modeling the features of a variant-rich software system. Feature models help developers to keep an overall understanding of the system, and also support scoping, planning, development, variant derivation, configuration, and maintenance activities that sustain the system's long-term success. Unfortunately, feature models are difficult to build and evolve. Features need to be identified, grouped, organized in a hierarchy, and mapped to software assets. Also, dependencies between features need to be declared. While feature models have been the subject of three decades of research, resulting in many feature-modeling notations together with automated analysis and configuration techniques, a generic set of principles for engineering feature models is still missing. It is not even clear whether feature models could be engineered using recurrent principles. Our work shows that such principles in fact exist. We analyzed feature-modeling practices elicited from ten interviews conducted with industrial practitioners and from 31 relevant papers. We synthesized a set of 34 principles covering eight different phases of feature modeling, from planning over model construction, to model maintenance and evolution. Grounded in empirical evidence, these principles provide practical, context-specific advice on how to perform feature modeling, describe what information sources to consider, and highlight common characteristics of feature models. We believe that our principles can support researchers and practitioners enhancing feature-modeling tooling, synthesis, and analyses techniques, as well as scope future research.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {62–73},
numpages = {12},
keywords = {software product lines, modeling principles, Feature models},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/648114.748898,
author = {Svahnberg, Mikael and Mattsson, Michael},
title = {Conditions and Restrictions for Product Line Generation Migration},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we describe a case study of a company in the domain of automatic guided vehicles (AGVs) that is in the process of migrating from a previous generation of software product line, which has mainly been centered around hardware, into a new product line generation, which will be software-centered. We describe the issues motivating this transition, and the factors that complicate it. Moreover, we present a three stage process for migrating into a new software product line. This process is currently initiated in collaboration with the aforementioned company.},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {143–154},
numpages = {12},
series = {PFE '01}
}

@inproceedings{10.1007/11554844_20,
author = {Etxeberria, Leire and Sagardui, Goiuria},
title = {Product-line architecture: new issues for evaluation},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_20},
doi = {10.1007/11554844_20},
abstract = {In the product-line context, where a lack or mismatch in a quality attribute is potentially replicated among all products, product-line evaluation could detect problems before concrete products are developed. The life span of a software product-line architecture is much longer than the one of an ordinary software product and it serves as a basis for a set of related systems. Therefore, the product-line architecture should be adaptable to evolution as well as support a number of different products. All these characteristics set new requirements to the product-line architecture evaluation. This paper highlights the new issues that can arise when evaluating a product-line architecture versus evaluating a single-system architecture, including classifications of relevant attributes in product-line architecture evaluation, new evaluation moments and techniques. These issues are used as components of a framework to survey product-line architecture evaluation methods and metrics.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {174–185},
numpages = {12},
location = {Rennes, France},
series = {SPLC'05}
}

@inproceedings{10.1109/WI-IAT.2014.170,
author = {Louati, Amine and Haddad, Joyce El and Pinson, Suzanne},
title = {A Multilevel Agent-Based Approach for Trustworthy Service Selection in Social Networks},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.170},
doi = {10.1109/WI-IAT.2014.170},
abstract = {The growing number of services available within social applications (viz. Social networks) raises a new and challenging search issue: selecting desired services from social networks. Traditional discovery and selection approaches, which are registry-based (e.g., UDDI, ebXML), have manifested their limitations as they often fall behind users' expectations. This is because registries fail to (i) take into consideration non functional properties such as QoS and trust and (ii) capitalize on the information resulting from the previous experiences between agents. To address these shortcomings, we use software agents as they support interactions and offer well-developed capabilities to formally express and interpret semantic information useful to evaluate trust. Trust in a service is a multi-aspect concept that includes a social-based aspect such as judging whether the provider is worthwhile pursuing before using his services (viz. Trust in sociability), expert-based aspect such as estimating whether the service behaves well and as expected (viz. Trust in expertise) and, recommender-based aspect such as assessing whether an agent is reliable and we can rely on its recommendations (viz. Trust in recommendation).},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 03},
pages = {214–221},
numpages = {8},
keywords = {Trust, Social Networks, Service Selection, Referral Systems, Multi-Agent Systems},
series = {WI-IAT '14}
}

@inproceedings{10.1109/ISPAW.2011.69,
author = {Kwon, Jagun and Hailes, Stephen},
title = {A Lightweight, Component-Based Approach to Engineering Reconfigurable Embedded Real-Time Control Software},
year = {2011},
isbn = {9780769544298},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISPAW.2011.69},
doi = {10.1109/ISPAW.2011.69},
abstract = {The cost of poor or repeat engineering in complex control systems is extremely high, and flexibility in software design and implementation is one of the key factors in staying competitive in the market. Complexity can be managed most effectively if the underlying software systems support structured, standardised, high-level abstraction layers that encapsulate unnecessary details behind well-defined interfaces. Moreover, since the costs of software maintenance are often as high as that of initial development, the ease with which it is possible flexibly to reconfigure, re-engineer, and replace software components in operational systems is also critical. In this paper, we present a lightweight, component-based approach to engineering embedded real-time control software, which is realized in the form of a middleware system named MIREA. The middleware supports dynamic reconfiguration of components written in C/C++, and addresses variability management in relation to non-functional properties, such as quality-of-service (QoS) and real-time scheduling. Users are allowed to componentize existing libraries easily, such as the standard NIST 4D/Real-time Control Systems (RCS) library, which has been successfully used in many U.S government-driven intelligent control projects, and to reuse them as dynamically reconfigurable components. A realistic illustration is provided showing how control systems are structured and reconfigured using our approach. In fact, we discuss our approach to control using a fusion of NIST RCS as a means of architecting a real time control system and MIREA as a means of realising that architecture. Our progress to date suggests that MIREA is indeed well suited as a middleware facilitating the construction of efficient, lightweight, and scalable real-time embedded control systems.},
booktitle = {Proceedings of the 2011 IEEE Ninth International Symposium on Parallel and Distributed Processing with Applications Workshops},
pages = {361–366},
numpages = {6},
series = {ISPAW '11}
}

@inproceedings{10.1007/11751113_3,
author = {Caporuscio, Mauro and Muccini, Henry and Pelliccione, Patrizio and Di Nisio, Ezio},
title = {Rapid system development via product line architecture implementation},
year = {2005},
isbn = {3540340637},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11751113_3},
doi = {10.1007/11751113_3},
abstract = {Software Product Line (SPL) engineering allows designers to reason about an entire family of software applications, instead of a single product, with a strategic importance for the rapid development of new applications. While much effort has been spent so far in understanding and modeling SPLs and their architectures, very little attention has been given on how to systematically enforce SPL architectural decisions into the implementation step.In this paper we propose a methodological approach and an implementation framework, based on a plugin component-based development, which allows us to move from an architectural specification of the SPL to its implementation in a systematic way. We show the suitability of this framework through its application to the TOOL one case study SPL.},
booktitle = {Proceedings of the Second International Conference on Rapid Integration of Software Engineering Techniques},
pages = {18–33},
numpages = {16},
location = {Heraklion, Crete, Greece},
series = {RISE'05}
}

@inproceedings{10.1145/3127041.3127059,
author = {Scheerer, Max and Busch, Axel and Koziolek, Anne},
title = {Automatic evaluation of complex design decisions in component-based software architectures},
year = {2017},
isbn = {9781450350938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127041.3127059},
doi = {10.1145/3127041.3127059},
abstract = {The quality of modern industrial plants depends on the quality of the hardware used, as well as software. While the impact on quality is comparably well understood by making decisions about the choice of hardware components, this is less true for the decisions on software components. The quality of the resulting software system is strongly influenced by its software architecture. Especially in early project phases a software architect has to make many design decisions. Each design decision highly influences the software architecture and thus, the resulting software quality. However, the impact on the resulting quality of architecture design decisions is hard to estimate in advance. For instance, a software architect could decide to deploy software components on a dedicated server in order to improve the system performance. However, such a decision may increase the network overhead as side-effect. Model-driven approaches have been shown as promising techniques enabling design-time quality prediction for different quality attributes such as performance or reliability. However, such approaches are limited in their automated decision support to simple design decisions like the exchange of one single component. In this paper, we present an approach that automatically evaluates complex design decisions in software architecture models. Such design decisions require the reuse of subsystems with many involved components coming with inhomogeneous architectures. We evaluate our approach using a real-world example system demonstrating the benefits of our approach.},
booktitle = {Proceedings of the 15th ACM-IEEE International Conference on Formal Methods and Models for System Design},
pages = {67–76},
numpages = {10},
keywords = {software quality, software architecture, reuse, model-driven engineering, decision support, component-based},
location = {Vienna, Austria},
series = {MEMOCODE '17}
}

@inproceedings{10.1145/1233901.1233908,
author = {Navarro, Luis Daniel Benavides and Schwanninger, Christa and Sobotzik, Robert and S\"{u}dholt, Mario},
title = {ATOLL: aspect-oriented toll system},
year = {2007},
isbn = {9781595936578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1233901.1233908},
doi = {10.1145/1233901.1233908},
abstract = {Product line development places emphasis on quality attributes like understandability, maintainability, reusability and variability. Better modularization techniques like aspect-oriented programming are supposed to improve these attributes.In the context of an industrial case study in the domain of infrastructure software for toll systems from Siemens AG, Germany, we have investigated how OO designs can be enhanced using AO techniques. We have explored, in particular, how sequential crosscutting concerns can be modularized using AspectJ and how distributed ones can be modularized using AWED, a system that features aspects with explicit distribution. Concretely, we show how sequential and distributed aspects improve the implementation of the charge calculation functionality that is central to real-world tolling systems.},
booktitle = {Proceedings of the 6th Workshop on Aspects, Components, and Patterns for Infrastructure Software},
pages = {7–es},
keywords = {software product lines, aspect-oriented software development},
location = {Vancouver, British Columbia, Canada},
series = {ACP4IS '07}
}

@inproceedings{10.1109/ECBS.2011.19,
author = {Galster, Matthias and Eberlein, Armin},
title = {Identifying Potential Core Assets in Service-Based Systems to Support the Transition to Service-Oriented Product Lines},
year = {2011},
isbn = {9780769543796},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ECBS.2011.19},
doi = {10.1109/ECBS.2011.19},
abstract = {Variability in service-oriented architectures (SOA) is usually addressed through loose coupling of services and dynamic retrieval, composition and binding of services. Facilitating variability can lead to different instances of one SOA. These instances share commonalities, but vary in certain aspects (e.g., in functionality or quality attributes). Current service-based development approaches do not adequately address variability and the management of different SOA instances. To handle different instances and to support systematic variability management, different instances of a service-based system may be treated as members of a product line. Therefore, we present a light-weight method to decide on what services to add to service-based systems to facilitate the transition from individual systems to a service-oriented product line. When adding services to service-based systems, the structural stability of these system decreases. We argue that a decrease in structural stability must be justified by additional value provided by the enhanced service-based systems. Based on the enhanced systems, our method then identifies potential core asset services for a service-oriented product line, taking into account common services within the different systems. Here, core asset services are reusable services that occur in any instance of the SOA. Thus, our method helps with the transition from individual products to a product line. A case study is included to illustrate our method.},
booktitle = {Proceedings of the 2011 18th IEEE International Conference and Workshops on Engineering of Computer-Based Systems},
pages = {179–186},
numpages = {8},
keywords = {variability, value, stability, service-oriented architectures, service-based, product lines, core assets, SOA},
series = {ECBS '11}
}

@article{10.1016/j.jss.2011.01.053,
author = {Clemente, Pedro J. and Hern\'{a}ndez, Juan and Conejero, Jos\'{e} M. and Ortiz, Guadalupe},
title = {Managing crosscutting concerns in component based systems using a model driven development approach},
year = {2011},
issue_date = {June, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {6},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.01.053},
doi = {10.1016/j.jss.2011.01.053},
abstract = {Abstract: In the last few years, Model-Driven Development (MDD), Aspect-Oriented Software Development (AOSD), and Component-Based Software Development (CBSD) have become interesting alternatives for the design and construction of complex distributed applications. Although these methodological approaches share the principle of separation of concerns and their further integration as key factors to obtaining high-quality and evolvable large software systems, they usually each address this principle from their own particular perspective. In the present work, we combine Component-Based and Aspect-Oriented Software Developments in a Model Driven software process targeted at the development of complex systems. This process constitutes an enhancement of the separation of concerns by allowing the isolation of crosscutting concerns in both Platform Independent and Platform Specific models. Following a pure MDD philosophy, a set of model transformations are used to generate the system, from preliminary models to the final source code for the Corba Component Model platform. A twofold empirical analysis was used to evaluate the approach's benefits in terms of two internal quality attributes: modularity and complexity. Conclusions were drawn from this evaluation regarding other quality attributes correlated with these two - stability, changeability, error-proneness, and reusability. An Eclipse plug-in was developed to drive the development of the entire system from early modeling to late deployment stages.},
journal = {J. Syst. Softw.},
month = jun,
pages = {1032–1053},
numpages = {22},
keywords = {Transformation models, Model Driven Development (MDD), Crosscutting concerns, Component Based Software Development (CBSD), CORBA Component Model (CCM), Aspect Oriented Software Development (AOSD)}
}

@inproceedings{10.1145/2739482.2768422,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Vergilio, Silvia R. and Egyed, Alexander},
title = {Genetic Improvement for Software Product Lines: An Overview and a Roadmap},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768422},
doi = {10.1145/2739482.2768422},
abstract = {Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {823–830},
numpages = {8},
keywords = {variability, software product lines, genetic programming, genetic improvement, evolutionary algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1007/s11219-012-9185-8,
author = {Bagheri, Ebrahim and Ga\v{s}evi\'{c}, Dragan},
title = {Foreword to the special issue on quality engineering for software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9185-8},
doi = {10.1007/s11219-012-9185-8},
journal = {Software Quality Journal},
month = sep,
pages = {421–424},
numpages = {4}
}

@article{10.1007/s11219-012-9193-8,
author = {Mets\"{a}, Jani and Maoz, Shahar and Katara, Mika and Mikkonen, Tommi},
title = {Using aspects for testing of embedded software: experiences from two industrial case studies},
year = {2014},
issue_date = {June      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9193-8},
doi = {10.1007/s11219-012-9193-8},
abstract = {Aspect-oriented software testing is emerging as an important alternative to conventional procedural and object-oriented testing techniques. This paper reports experiences from two case studies where aspects were used for the testing of embedded software in the context of an industrial application. In the first study, we used code-level aspects for testing non-functional properties. The methodology we used for deriving test aspect code was based on translating high-level requirements into test objectives, which were then implemented using test aspects in AspectC++. In the second study, we used high-level visual scenario-based models for the test specification, test generation, and aspect-based test execution. To specify scenario-based tests, we used a UML2-compliant variant of live sequence charts. To automatically generate test code from the models, a modified version of the S2A Compiler, outputting AspectC++ code, was used. Finally, to examine the results of the tests, we used the Tracer, a prototype tool for model-based trace visualization and exploration. The results of the two case studies show that aspects offer benefits over conventional techniques in the context of testing embedded software; these benefits are discussed in detail. Finally, towards the end of the paper, we also discuss the lessons learned, including the technological and other barriers to the future successful use of aspects in the testing of embedded software in industry.},
journal = {Software Quality Journal},
month = jun,
pages = {185–213},
numpages = {29},
keywords = {Software testing, Embedded software, Case studies, Aspect-oriented programming}
}

@article{10.1145/2413038.2382768,
author = {Brahmasani, Siva and Selvakumar, Subramanian and Sivasankar, E.},
title = {Prevention of XSS attacks using STCD: Server side tagging and client side differentiation},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2413038.2382768},
doi = {10.1145/2413038.2382768},
abstract = {Variability (the ability of a software system or software artifact to be adapted for use in a specific context) is reflected in and facilitated through the software architecture. The Second International Workshop on Variability in Software Architecture (VARSA) was held in conjunction with the Joint 10th Working IEEE/IFIP Conference on Software Architecture &amp; 6th European Conference on Software Architecture 2012 in Helsinki, Finland. The workshop aimed at exploring current and emerging methods, languages, notations technologies and tools to model, implement, and manage variability in the software architecture. It featured one industrial talk, five research paper presentations, and three working group discussions. Working groups discussed topics that emerged during the workshop. This report summarizes the themes of the workshop and presents the results of the working group discussions.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {46–49},
numpages = {4},
keywords = {variability, software architecture, VARSA}
}

@article{10.1145/2382756.2382768,
author = {Brahmasani, Siva and Selvakumar, Subramanian and Sivasankar, E.},
title = {Prevention of XSS attacks using STCD: Server side tagging and client side differentiation},
year = {2013},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2382756.2382768},
doi = {10.1145/2382756.2382768},
abstract = {Variability (the ability of a software system or software artifact to be adapted for use in a specific context) is reflected in and facilitated through the software architecture. The Second International Workshop on Variability in Software Architecture (VARSA) was held in conjunction with the Joint 10th Working IEEE/IFIP Conference on Software Architecture &amp; 6th European Conference on Software Architecture 2012 in Helsinki, Finland. The workshop aimed at exploring current and emerging methods, languages, notations technologies and tools to model, implement, and manage variability in the software architecture. It featured one industrial talk, five research paper presentations, and three working group discussions. Working groups discussed topics that emerged during the workshop. This report summarizes the themes of the workshop and presents the results of the working group discussions.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–9},
numpages = {9},
keywords = {variability, software architecture, VARSA}
}

@article{10.1016/j.jss.2007.04.011,
author = {Kim, Jintae and Park, Sooyong and Sugumaran, Vijayan},
title = {DRAMA: A framework for domain requirements analysis and modeling architectures in software product lines},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.04.011},
doi = {10.1016/j.jss.2007.04.011},
abstract = {One of the benefits of software product line approach is to improve time-to-market. The changes in market needs cause software requirements to be flexible in product lines. Whenever software requirements are changed, software architecture should be evolved to correspond with them. Therefore, domain architecture should be designed based on domain requirements. It is essential that there is traceability between requirements and architecture, and that the structure of architecture is derived from quality requirements. The purpose of this paper is to provide a framework for modeling domain architecture based on domain requirements within product lines. In particular, we focus on the traceable relationship between requirements and architectural structures. Our framework consists of processes, methods, and a supporting tool. It uses four basic concepts, namely, goal based domain requirements analysis, Analytical Hierarchy Process, Matrix technique, and architecture styles. Our approach is illustrated using HIS (Home Integration System) product line. Finally, industrial examples are used to validate DRAMA.},
journal = {J. Syst. Softw.},
month = jan,
pages = {37–55},
numpages = {19},
keywords = {Traceability, Quality attribute, Domain requirements, Domain architecture}
}

@inproceedings{10.1145/2993412.3011881,
author = {da Silva Amorim, Simone and McGregor, John D. and de Almeida, Eduardo Santana and von Flach G. Chavez, Christina},
title = {Software ecosystems architectural health: challenges x practices},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3011881},
doi = {10.1145/2993412.3011881},
abstract = {Over time many software ecosystems have achieved success. Several organizations are opening their software projects for external businesses, creating an multi-organizational government to development their software platform The software architecture has an important participation in this success. In this context, there are some studies describing architectural challenges for software ecosystems, but little research is investigating how these challenges are being faced by software ecosystems organizations. This paper presents an initial investigation how open source software (OSS) ecosystems have faced several architectural challenges. We conducted interviews with three architects of different OSS ecosystems and gathered some architectural practices to lead with challenges. We also analyzed how these architectural practices have influenced the software ecosystem health, introducing the concept of Software Ecosystems Architectural Health.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {4},
numpages = {7},
keywords = {software improvement, software ecosystems, software architecture},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@inproceedings{10.5555/648033.744227,
author = {Anastasopoulos, Michalis and Atkinson, Colin and Muthig, Dirk},
title = {A Concrete Method for Developing and Applying Product Line Architectures},
year = {2002},
isbn = {3540007377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software development organizations are often deterred from introducing product line architectures by the lack of simple, ready-touse methods for developing and applying them. The well-known, published product-line-engineering methods tend to focus on the early stages of the software life cycle and address product line issues at a high-level of abstraction. Connecting product-line concepts with established implementation technologies is thus largely left to the user.This paper introduces a method, known as the KobrA method, which addresses this problem by enabling product line concerns to be coupled with regular (nonproduct line) architectural artifacts, and thus introduced incrementally. By explaining how the method can be understood as a concrete instantiation of the well-established PuLSE-DSSA product-line architecture approach, the paper clarifies the product line features of the KobrA method and illustrates how they can be used in tandem with established, general-purpose product line methods.},
booktitle = {Revised Papers from the International Conference NetObjectDays on Objects, Components, Architectures, Services, and Applications for a Networked World},
pages = {294–312},
numpages = {19},
series = {NODe '02}
}

@article{10.5555/1629036.1629068,
author = {Hunt, John M. and McGregor, John D.},
title = {Building software that is predictable by Construction},
year = {2009},
issue_date = {December 2009},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {25},
number = {2},
issn = {1937-4771},
abstract = {Predictability by Construction is an effort to provide predictions for quality attributes such as performance during the design process in the same way that type systems allow predictions about aspects of correctness. This workshop introduces participants to the PACC Starter Kit (see: http://www.sei.cmu.edu/pacc/), which supports a state machine paradigm. Tools in the kit support computational theories that can be used to make predictions during design and to analyze the code to determine whether the predictions are correct. We will present classroom-tested exercises that use the kit in software engineering, formal methods, operating system, programming languages and theory of computation courses.},
journal = {J. Comput. Sci. Coll.},
month = dec,
pages = {203–204},
numpages = {2}
}

@inproceedings{10.1145/2420942.2420943,
author = {Bo\v{s}kovi\'{c}, Marko and Mussbacher, Gunter and Ga\v{s}evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {The Fourth International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML2012)},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420943},
doi = {10.1145/2420942.2420943},
abstract = {The International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML) series traditionally takes place as part of the Satellite Events of the ACM/IEEE International Conference on Model Driven Engineering Languages and Systems (MODELS). Traditionally, NFPinDSML gathers researchers and practitioners interested in the estimation and evaluation of system quality and their integration in Domain Specific Modeling Languages and Model Driven Engineering in general. This paper is the summary of the fourth NFPinDSML workshop which was affiliated with MODELS 2012.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {1},
numpages = {2},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/1842752.1842812,
author = {Abbas, Nadeem and Andersson, Jesper and L\"{o}we, Welf},
title = {Autonomic Software Product Lines (ASPL)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842812},
doi = {10.1145/1842752.1842812},
abstract = {We describe ongoing work on a variability mechanism for Autonomic Software Product Lines (ASPL). The autonomic software product lines have self-management characteristics that make product line instances more resilient to context changes and some aspects of product line evolution. Instances sense the context, selects and bind the best component variants to variation-points at run-time. The variability mechanism we describe is composed of a profile guided dispatch based on off-line and on-line training processes. Together they form a simple, yet powerful variability mechanism that continuously learns, which variants to bind given the current context and system goals.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {324–331},
numpages = {8},
keywords = {variation-points, variants, variability, on-line, off-line training, goals, context, autonomic elements, MAPE-K},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@article{10.1016/j.infsof.2009.11.008,
author = {Ovaska, Eila and Evesti, Antti and Henttonen, Katja and Palviainen, Marko and Aho, Pekka},
title = {Knowledge based quality-driven architecture design and evaluation},
year = {2010},
issue_date = {June, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {6},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.008},
doi = {10.1016/j.infsof.2009.11.008},
abstract = {Modelling and evaluating quality properties of software is of high importance, especially when our every day life depends on the quality of services produced by systems and devices embedded into our surroundings. This paper contributes to the body of research in quality and model driven software engineering. It does so by introducing; (1) a quality aware software architecting approach and (2) a supporting tool chain. The novel approach with supporting tools enables the systematic development of high quality software by merging benefits of knowledge modelling and management, and model driven architecture design enhanced with domain-specific quality attributes. The whole design flow of software engineering is semi-automatic; specifying quality requirements, transforming quality requirements to architecture design, representing quality properties in architectural models, predicting quality fulfilment from architectural models, and finally, measuring quality aspects from implemented source code. The semi-automatic design flow is exemplified by the ongoing development of a secure middleware for peer-to-peer embedded systems.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {577–601},
numpages = {25},
keywords = {Tool, Software architecture, Quality attribute, Ontology, Model-driven development, Evaluation}
}

@inproceedings{10.5555/648033.744208,
author = {Muthig, Dirk and Patzke, Thomas},
title = {Generic Implementation of Product Line Components},
year = {2002},
isbn = {3540007377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {An argument pro component-based software development is the idea of constructing software systems by assembling preexisting components instead of redeveloping similar or identical functionality always from scratch. Unfortunately, integrating existing components practically means adaptation and use rather than use only, which makes an ideal component-based development hard to realize in practice. Product line engineering, however, tackles this problem by making components as generic as needed for a particular product family and thus allows component reuse. Such a component covers variabilities and thus its implementation must consider variabilities as well.In this paper, we describe a process for implementing generic product line components and give an overview of variability mechanisms at the implementation level, illustrated by a running example, a generic test component.},
booktitle = {Revised Papers from the International Conference NetObjectDays on Objects, Components, Architectures, Services, and Applications for a Networked World},
pages = {313–329},
numpages = {17},
series = {NODe '02}
}

@inproceedings{10.1109/ICSEA.2009.59,
author = {Bartholdt, Joerg and Medak, Marcel and Oberhauser, Roy},
title = {Integrating Quality Modeling with Feature Modeling in Software Product Lines},
year = {2009},
isbn = {9780769537771},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSEA.2009.59},
doi = {10.1109/ICSEA.2009.59},
abstract = {Due to the large number of possible variants in typical Software Product Lines (SPLs), the modeling of, explicit knowledge of, and predictability of the quality tradeoffs inherent in certain feature selections are critical to the future viability of SPLs. This paper presents IQ-SPLE, an integrated tool-supported approach that considers both qualitative and quantitative quality attributes without imposing hierarchical structural constraints. The approach is demonstrated with an eHealth SPL scenario, with the results showing that this approach is promising for effectively addressing the integration of quality attributes in SPL Engineering (SPLE).},
booktitle = {Proceedings of the 2009 Fourth International Conference on Software Engineering Advances},
pages = {365–370},
numpages = {6},
keywords = {variability, software product lines, quality modeling, feature modeling},
series = {ICSEA '09}
}

@article{10.1145/3284971.3284975,
author = {Lazreg, Sami and Collet, Philippe and Mosser, S\'{e}bastien},
title = {Functional feasibility analysis of variability-intensive data flow-oriented applications over highly-configurable platforms},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3284971.3284975},
doi = {10.1145/3284971.3284975},
abstract = {Data-flow oriented embedded systems, such as automotive systems used to render HMI (e.g., instrument clusters, infotainments), are increasingly built from highly variable specifications while targeting different constrained hardware platforms configurable in a fine-grained way. These variabilities at two different levels lead to a huge number of possible embedded system solutions, which functional feasibility is extremely complex and tedious to predetermine. In this paper, we propose a tooled approach that capture high level specifications as variable dataflows, and targeted platforms as variable component models. Dataflows can then be mapped onto platforms to express a specification of such variability-intensive systems. The proposed solution transforms this specification into structural and behavioral variability models and reuses automated reasoning techniques to explore and assess the functional feasibility of all variants in a single run. We also report on the validation of the proposed approach. A qualitative evaluation has been conducted on an industrial case study of automotive instrument cluster, while a quantitative one is reported on large generated datasets.},
journal = {SIGAPP Appl. Comput. Rev.},
month = oct,
pages = {32–48},
numpages = {17},
keywords = {variability modeling, feature model, embedded system design engineering, behavioral product lines model checking}
}

@inproceedings{10.1145/337180.337455,
author = {Gannod, Gerald C. and Lutz, Robyn R.},
title = {An approach to architectural analysis of product lines},
year = {2000},
isbn = {1581132069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/337180.337455},
doi = {10.1145/337180.337455},
abstract = {This paper addresses the issue of how to perform architectural analysis on an existing product line architecture. The con tribution of the paper is to identify and demonstrate a repeatable product line architecture analysis process. The approach defines a “good” product line architecture in terms of those quality attributes required by the particular product line under development. It then analyzes the architecture against these criteria by both manual and tool-supported methods. The phased approach described in this paper provides a structured analysis of an existing product line architecture using (1) formal specification of the high-level architecture, (2) manual analysis of scenarios to exercise the architecture's support for required variabilities, and (3) model checking of critical behaviors at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain and evaluate the approach.},
booktitle = {Proceedings of the 22nd International Conference on Software Engineering},
pages = {548–557},
numpages = {10},
keywords = {software archtecture, software architecture analysis, product lines, interferometry software},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@article{10.1016/j.future.2018.09.053,
author = {Cecchinel, Cyril and Fouquet, Fran\c{c}ois and Mosser, S\'{e}bastien and Collet, Philippe},
title = {Leveraging live machine learning and deep sleep to support a self-adaptive efficient configuration of battery powered sensors},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.053},
doi = {10.1016/j.future.2018.09.053},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {225–240},
numpages = {16}
}

@inproceedings{10.1007/11554844_6,
author = {Kang, Kyo Chul and Kim, Moonzoo and Lee, Jaejoon and Kim, Byungkil},
title = {Feature-oriented re-engineering of legacy systems into product line assets: a case study},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_6},
doi = {10.1007/11554844_6},
abstract = {Home service robots have a wide range of potential applications, such as home security, patient caring, cleaning, etc. The services provided by the robots in each application area are being defined as markets are formed and, therefore, they change constantly. Thus, robot applications need to evolve both quickly and flexibly adopting frequently changing requirements. This makes software product line framework ideal for the domain of home service robots. Unfortunately, however, robot manufacturers often focus on developing technical components (e.g., vision recognizer and speech processor) and then attempt to develop robots by integrating these components in an ad-hoc way. This practice produces robot applications that are hard to re-use and evolve when requirements change. We believe that re-engineering legacy robot applications into product line assets can significantly enhance reusability and evolvability.In this paper, we present our experience of re-engineering legacy home service robot applications into product line assets through feature modeling and analysis. First, through reverse engineering, we recovered architectures and components of the legacy applications. Second, based on the recovered information and domain knowledge, we reconstructed a feature model for the legacy applications. Anticipating changes in business opportunities or technologies, we restructured and refined the feature model to produce a feature model for the product line. Finally, based on the refined feature model and engineering principles we adopted for asset development, we designed a new architecture and components for robot applications.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {45–56},
numpages = {12},
location = {Rennes, France},
series = {SPLC'05}
}

@inproceedings{10.5555/645882.672395,
author = {Kang, Kyo Chul and Donohoe, Patrick and Koh, Eunman and Lee, Jaejoon and Lee, Kwanwoo},
title = {Using a Marketing and Product Plan as a Key Driver for Product Line Asset Development},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The product line engineering paradigm has emerged recently to address the need to minimize the development cost and the time to market in this highly competitive global market. Product line development consists of product line asset development and product development using the assets. Product line requirements are essential inputs to product line asset development. These inputs, although critical, are not sufficient to develop product line assets. A marketing and product plan, which includes plans on what features are to be packaged in products, how these features will be delivered to customers (e.g., feature binding time), and how the products will evolve in the future, also drives product line asset development; thus this paper explores design issues from the marketing perspective and presents key design drivers that are tightly coupled with the marketing strategy. An elevator control software example is used to illustrate how product line asset development is related to marketing and product plans.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {366–382},
numpages = {17},
series = {SPLC 2}
}

@book{10.5555/2911053,
author = {Mistrik, Ivan and Soley, Richard M. and Ali, Nour and Grundy, John and Tekinerdogan, Bedir},
title = {Software Quality Assurance: In Large Scale and Complex Software-intensive Systems},
year = {2015},
isbn = {0128023015},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Quality Assurance in Large Scale and Complex Software-intensive Systems presents novel and high-quality research related approaches that relate the quality of software architecture to system requirements, system architecture and enterprise-architecture, or software testing. Modern software has become complex and adaptable due to the emergence of globalization and new software technologies, devices and networks. These changes challenge both traditional software quality assurance techniques and software engineers to ensure software quality when building today (and tomorrows) adaptive, context-sensitive, and highly diverse applications. This edited volume presents state of the art techniques, methodologies, tools, best practices and guidelines for software quality assurance and offers guidance for future software engineering research and practice. Each contributed chapter considers the practical application of the topic through case studies, experiments, empirical validation, or systematic comparisons with other approaches already in practice. Topics of interest include, but are not limited, to: quality attributes of system/software architectures; aligning enterprise, system, and software architecture from the point of view of total quality; design decisions and their influence on the quality of system/software architecture; methods and processes for evaluating architecture quality; quality assessment of legacy systems and third party applications; lessons learned and empirical validation of theories and frameworks on architectural quality; empirical validation and testing for assessing architecture quality.Focused on quality assurance at all levels of software design and developmentCovers domain-specific software quality assurance issues e.g. for cloud, mobile, security, context-sensitive, mash-up and autonomic systemsExplains likely trade-offs from design decisions in the context of complex software system engineering and quality assuranceIncludes practical case studies of software quality assurance for complex, adaptive and context-critical systems}
}

@article{10.1016/j.infsof.2012.11.001,
author = {Schmid, Klaus and De Almeida, Eduardo Santana and Kishi, Tomoji},
title = {Editorial: Guest Editors' Introduction: Special Issue on Software Reuse and Product Lines},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.001},
doi = {10.1016/j.infsof.2012.11.001},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {489–490},
numpages = {2}
}

@article{10.1007/s10664-019-09705-w,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Apel, Sven},
title = {On the relation of control-flow and performance feature interactions: a case study},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09705-w},
doi = {10.1007/s10664-019-09705-w},
abstract = {Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time-consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that information about potentially interacting features can be obtained by analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted an in-depth qualitative case study on two real-world systems (mbedTLS and SQLite), in which we explored the relation between internal (precisely control-flow) feature interactions, detected through static program analysis, and external (precisely performance) feature interactions, detected by performance-prediction techniques using performance measurements. We found that a relation exists that can potentially be exploited to predict performance interactions.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2410–2437},
numpages = {28},
keywords = {Variability, Performance feature interaction, Highly configurable software system, Feature-interaction prediction, Feature interaction, Feature, Control-flow feature interaction}
}

@inproceedings{10.5555/2666064.2666068,
author = {Li, Dong and Yang, Ye},
title = {Enhance value by building trustworthy software-reliant system of systems from software product lines},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {Ever growing and expanding mission-critical domains generate ever emerging and more serious challenges. Complexity which may greatly reduce the trustworthiness of a system is the key reason. Based on the practice of FISCAN in the security inspection domain, we believe a software-reliant system of systems (srSoS) built from software product lines (SPL) is a viable solution to address these issues. In this paper, we present a framework of SPL-to-srSoS to extend SPL practices to srSoS and argue its value-enhancement effects by rapidly meeting increased complexity and emergent behaviors of System of Systems (SoS). The framework employs a basic principle of system hiding, and consists of a SPL-to-srSoS process model and an initially conceived value model. A successfully deployed FISCAN srSoS at Airports provides demonstration for the discussion throughout the paper.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {13–16},
numpages = {4},
keywords = {value model, trustworthy, system of systems, system hiding, software-reliant, software product lines, SPL-to-srSOS},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@article{10.1016/j.infsof.2019.06.012,
author = {Balera, Juliana Marino and Santiago J\'{u}nior, Valdivino Alexandre de},
title = {A systematic mapping addressing Hyper-Heuristics within Search-based Software Testing},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.06.012},
doi = {10.1016/j.infsof.2019.06.012},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {176–189},
numpages = {14},
keywords = {Meta-heuristics, Genetic Algorithms, Evolutionary Algorithms, Systematic Mapping, Hyper-heuristics, Search-based Software Testing}
}

@article{10.1016/j.datak.2021.101929,
author = {Ali, Mughees and Khan, Saif Ur Rehman and Hussain, Shahid},
title = {Self-adaptation in smartphone applications: Current state-of-the-art techniques, challenges, and future directions},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {136},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2021.101929},
doi = {10.1016/j.datak.2021.101929},
journal = {Data Knowl. Eng.},
month = nov,
numpages = {19},
keywords = {Self-adaptive mobile apps, Smartphone applications, Mobile applications, Self-adaptation}
}

@article{10.1145/2020976.2020978,
author = {Galster, Matthias and Avgeriou, Paris and Weyns, Danny and M\"{a}nnist\"{o}, Tomi},
title = {Variability in software architecture: current practice and challenges},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2020976.2020978},
doi = {10.1145/2020976.2020978},
abstract = {Variability in software-intensive systems is usually understood as the ability of a software artifact to be changed in order to fit different contexts, environments, or purposes. Software architecture on the other hand determines the structure of a software system, and is described in an architecture description. This description includes the major stakeholders of a software system and their concerns. Variability is reflected in and facilitated through the software architecture. The First International Workshop on Variability in Software Architecture (VARSA) was held jointly with WICSA 2011 in Boulder, Colorado. The goal of the workshop was to explore and advance the state-of-the art in variability in software architecture. It featured four research paper presentations, two invited talks, and three working groups that discussed specific topics. This report summarizes the themes of the workshop, presents the results of the working group discussions, and suggests topics for further research.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {30–32},
numpages = {3}
}

@article{10.1016/j.jss.2014.09.042,
author = {Zhi, Junji and Garousi-Yusifo\u{g}lu, Vahid and Sun, Bo and Garousi, Golara and Shahnewaz, Shawn and Ruhe, Guenther},
title = {Cost, benefits and quality of software development documentation},
year = {2015},
issue_date = {January 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {99},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.09.042},
doi = {10.1016/j.jss.2014.09.042},
abstract = {38% of papers propose new techniques and 29% contribute empirical evidences.Validation research papers and solution proposals are dominating types.Agile development is only mentioned in six studies.Design, code and requirement are dominating document types.Formatted text, models and tool support are dominating document forms.Most papers had only one SUS whose sizes do not increase strictly over time.The average number of participants in survey-based studies was 106.17% of papers focus on cost, in contrast to benefit (54%) and quality (72%).Only six studies (8%) discuss the development or production cost.Documentation's main usage includes maintenance aid and program comprehension.Most discussed quality attributes are completeness, consistency and accessibility.Only 10% of papers are affiliated with industry. ContextSoftware documentation is an integral part of any software development process. Researchers and practitioners have expressed concerns about costs, benefits and quality of software documentation in practice. On the one hand, there is a lack of a comprehensive model to evaluate the quality of documentation. On the other hand, researchers and practitioners need to assess whether documentation cost outweighs its benefit. ObjectivesIn this study, we aim to summarize the existing literature and provide an overview of the field of software documentation cost, benefit and quality. MethodWe use the systematic-mapping methodology to map the existing body of knowledge related to software documentation cost, benefit and quality. To achieve our objectives, 11 Research Questions (RQ) are raised. The primary papers are carefully selected. After applying the inclusion and exclusion criteria, our study pool included a set of 69 papers from 1971 to 2011. A systematic map is developed and refined iteratively. ResultsWe present the results of a systematic mapping covering different research aspects related to software documentation cost, benefit and quality (RQ 1-11). Key findings include: (1) validation research papers are dominating (27 papers), followed by solution proposals (21 papers). (2) Most papers (61 out of 69) do not mention the development life-cycle model explicitly. Agile development is only mentioned in 6 papers. (3) Most papers include only one "System under Study" (SUS) which is mostly academic prototype. The average number of participants in survey-based papers is 106, the highest one having approximately 1000 participants. (4) In terms of focus of papers, 50 papers focused on documentation quality, followed by 37 papers on benefit, and 12 papers on documentation cost. (5) The quality attributes of documentation that appear in most papers are, in order: completeness, consistency and accessibility. Additionally, improved meta-models for documentation cost, benefit and quality are also presented. Furthermore, we have created an online paper repository of the primary papers analyzed and mapped during this study. ConclusionOur study results show that this research area is emerging but far from mature. Firstly, documentation cost aspect seems to have been neglected in the existing literature and there are no systematic methods or models to measure cost. Also, despite a substantial number of solutions proposed during the last 40 years, more and stronger empirical evidences are still needed to enhance our understanding of this area. In particular, what we expect includes (1) more validation or evaluation studies; (2) studies involving large-scale development projects, or from large number of study participants of various organizations; (3) more industry-academia collaborations; (4) more estimation models or methods to assess documentation quality, benefit and, especially, cost.},
journal = {J. Syst. Softw.},
month = jan,
pages = {175–198},
numpages = {24},
keywords = {Systematic mapping, Software documentation, Documentation benefit}
}

@inproceedings{10.1145/3023956.3023963,
author = {Halin, Axel and Nuttinck, Alexandre and Acher, Mathieu and Devroey, Xavier and Perrouin, Gilles and Heymans, Patrick},
title = {Yo variability! JHipster: a playground for web-apps analyses},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023963},
doi = {10.1145/3023956.3023963},
abstract = {Though variability is everywhere, there has always been a shortage of publicly available cases for assessing variability-aware tools and techniques as well as supports for teaching variability-related concepts. Historical software product lines contains industrial secrets their owners do not want to disclose to a wide audience. The open source community contributed to large-scale cases such as Eclipse, Linux kernels, or web-based plugin systems (Drupal, WordPress). To assess accuracy of sampling and prediction approaches (bugs, performance), a case where all products can be enumerated is desirable. As configuration issues do not lie within only one place but are scattered across technologies and assets, a case exposing such diversity is an additional asset. To this end, we present in this paper our efforts in building an explicit product line on top of JHipster, an industrial open-source Web-app configurator that is both manageable in terms of configurations (≈ 163,000) and diverse in terms of technologies used. We present our efforts in building a variability-aware chain on top of JHipster's configurator and lessons learned using it as a teaching case at the University of Rennes. We also sketch the diversity of analyses that can be performed with our infrastructure as well as early issues found using it. Our long term goal is both to support students and researchers studying variability analysis and JHipster developers in the maintenance and evolution of their tools.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {44–51},
numpages = {8},
keywords = {web-apps, variability-related analyses, case study},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@article{10.1147/rd.492.0437,
author = {Lorenz, J. and Kral, S. and Franchetti, F. and Ueberhuber, C. W.},
title = {Vectorization techniques for the Blue Gene/L double FPU},
year = {2005},
issue_date = {March 2005},
publisher = {IBM Corp.},
address = {USA},
volume = {49},
number = {2},
issn = {0018-8646},
url = {https://doi.org/10.1147/rd.492.0437},
doi = {10.1147/rd.492.0437},
abstract = {This paper presents vectorization techniques tailored to meet the specifics of the two-way single-instruction multiple-data (SIMD) double-precision floating-point unit (FPU), which is a core element of the node application-specific integrated circuit (ASIC) chips of the IBM 360-teraflops Blue Gene®/L supercomputer. This paper focuses on the general-purpose basic-block vectorization and optimization methods as they are incorporated in the Vienna MAP vectorizer and optimizer. The innovative technologies presented here, which have consistently delivered superior performance and portability across a wide range of platforms, were carried over to prototypes of Blue Gene/L and joined with the automatic performance-tuning system known as Fastest Fourier Transform in the West (FFTW). FFTW performance-optimization facilities working with the compiler technologies presented in this paper are able to produce vectorized fast Fourier transform (FFT) codes that are tuned automatically to single Blue Gene/L processors and are up to 80% faster than the best-performing scalar FFT codes generated by FFTW.},
journal = {IBM J. Res. Dev.},
month = mar,
pages = {437–446},
numpages = {10}
}

@inproceedings{10.1007/978-3-642-33182-4_11,
author = {Gaia, Felipe Nunes and Ferreira, Gabriel Coutinho Sousa and Figueiredo, Eduardo and de Almeida Maia, Marcelo},
title = {A quantitative assessment of aspectual feature modules for evolving software product lines},
year = {2012},
isbn = {9783642331817},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33182-4_11},
doi = {10.1007/978-3-642-33182-4_11},
abstract = {Feature-Oriented Programming (FOP) and Aspect-Oriented Programming (AOP) are programming techniques based on composition mechanisms, called refinements and aspects, respectively. These techniques are assumed to be good variability mechanisms for implementing Software Product Lines (SPLs). Aspectual Feature Modules (AFM) is an approach that combines advantages of feature modules and aspects to increase concern modularity. Some guidelines of how to integrate these techniques have been established in some studies, but these studies do not focus the analysis on how effectively AFM can preserve the modularity and stability facilitating SPL evolution. The main purpose of this paper is to investigate whether the simultaneous use aspects and features through the AFM approach facilitates the evolution of SPLs. The quantitative data were collected from a SPL developed using four different variability mechanisms: (1) feature modules, aspects and aspects refinements of AFM, (2) aspects of aspect-oriented programming (AOP), (3) feature modules of feature-oriented programming (FOP), and (4) conditional compilation (CC) with object-oriented programming. Metrics for change propagation and modularity stability were calculated and the results support the benefits of the AFM option in a context where the product line has been evolved with addition or modification of crosscutting concerns.},
booktitle = {Proceedings of the 16th Brazilian Conference on Programming Languages},
pages = {134–149},
numpages = {16},
keywords = {variability mechanisms, software product lines, feature-oriented programming, aspectual feature modules, aspect-oriented programming},
location = {Natal, Brazil},
series = {SBLP'12}
}

@article{10.1007/s00165-017-0441-3,
author = {Str\"{u}ber, D. and Rubin, J. and Arendt, T. and Chechik, M. and Taentzer, G. and Pl\"{o}ger, J.},
title = {Variability-based model transformation: formal foundation and application},
year = {2018},
issue_date = {Jan 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-017-0441-3},
doi = {10.1007/s00165-017-0441-3},
abstract = {Model transformation systems often contain transformation rules that are substantially similar to each other, causing maintenance issues and performance bottlenecks. To address these issues, we introduce variability-based model transformation. The key idea is to encode a set of similar rules into a compact representation, called variability-based rule. We provide an algorithm for applying such rules in an efficient manner. In addition, we introduce rule merging, a three-component mechanism for enabling the automatic creation of variability-based rules. Our rule application and merging mechanisms are supported by a novel formal framework, using category theory to provide precise definitions and to prove correctness. In two realistic application scenarios, the created variability-based rules enabled considerable speedups, while also allowing the overall specifications to become more compact.},
journal = {Form. Asp. Comput.},
month = jan,
pages = {133–162},
numpages = {30},
keywords = {Category theory, Variability, Graph transformation, Model transformation}
}

@inproceedings{10.1145/3377024.3377026,
author = {Kenner, Andy and Dassow, Stephan and Lausberger, Christian and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Using variability modeling to support security evaluations: virtualizing the right attack scenarios},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377026},
doi = {10.1145/3377024.3377026},
abstract = {A software system's security is constantly threatened by vulnerabilities that result from faults in the system's design (e.g., unintended feature interactions) and which can be exploited with attacks. While various databases summarize information on vulnerabilities and other security issues for many software systems, these databases face severe limitations. For example, the information's quality is unclear, often only semi-structured, and barely connected to other information. Consequently, it can be challenging for any security-related stakeholder to extract and understand what information is relevant, considering that most systems exist in different variants and versions. To tackle this problem, we propose to design vulnerability feature models that represent the vulnerabilities of a system and enable developers to virtualize corresponding attack scenarios. In this paper, we report a first case study on Mozilla Firefox for which we extracted vulnerabilities and used them to virtualize vulnerable instances in Docker. To this end, we focused on extracting information from available databases and on evaluating the usability of the results. Our findings indicate several problems with the extraction that complicate modeling, understanding, and testing of vulnerabilities. Nonetheless, the databases provide a valuable foundation for our technique, which we aim to extend with automatic synthesis and analyses of feature models, as well as virtualization for attack scenarios in future work.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {9},
keywords = {vulnerability, variability model, software architecture, feature model, exploit, docker-container, attack scenarios},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1016/j.jss.2014.01.021,
author = {Walraven, Stefan and Van Landuyt, Dimitri and Truyen, Eddy and Handekyn, Koen and Joosen, Wouter},
title = {Efficient customization of multi-tenant Software-as-a-Service applications with service lines},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.01.021},
doi = {10.1016/j.jss.2014.01.021},
abstract = {Application-level multi-tenancy is an architectural approach for Software-as-a-Service (SaaS) applications which enables high operational cost efficiency by sharing one application instance among multiple customer organizations (the so-called tenants). However, the focus on increased resource sharing typically results in a one-size-fits-all approach. In principle, the shared application instance satisfies only the requirements common to all tenants, without supporting potentially different and varying requirements of these tenants. As a consequence, multi-tenant SaaS applications are inherently limited in terms of flexibility and variability. This paper presents an integrated service engineering method, called service line engineering, that supports co-existing tenant-specific configurations and that facilitates the development and management of customizable, multi-tenant SaaS applications, without compromising scalability. Specifically, the method spans the design, implementation, configuration, composition, operations and maintenance of a SaaS application that bundles all variations that are based on a common core. We validate this work by illustrating the benefits of our method in the development of a real-world SaaS offering for document processing. We explicitly show that the effort to configure and compose an application variant for each individual tenant is significantly reduced, though at the expense of a higher initial development effort.},
journal = {J. Syst. Softw.},
month = may,
pages = {48–62},
numpages = {15},
keywords = {Variability, SaaS, Multi-tenancy}
}

@inproceedings{10.5555/381473.381482,
author = {Bosch, Jan},
title = {Software product lines: organizational alternatives},
year = {2001},
isbn = {0769510507},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Software product lines enjoy increasingly wide adoption in the software industry. Most authors focus on the technical and process aspects and assume an organizational model consisting of a domain engineering unit and several application engineering units. In our cooperation with several software development organizations applying software product line principles, we have identified several other organizational models that are employed as well. In this article, we present a number of organizational alternatives, organized around four main models, i.e. development department, business units, domain engineering unit and hierarchical domain engineering units. For each model, its characteristics, applicability and advantages and disadvantages are discussed, as well as an example. Based on an analysis of these models, we present three factors that influence the choice of the organizational model, i.e. product-line assets, the responsibility levels and the type of organizational units.},
booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
pages = {91–100},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {ICSE '01}
}

@article{10.1016/j.jss.2017.03.005,
author = {Haghighatkhah, Alireza and Banijamali, Ahmad and Pakanen, Olli-Pekka and Oivo, Markku and Kuvaja, Pasi},
title = {Automotive software engineering},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.005},
doi = {10.1016/j.jss.2017.03.005},
abstract = {A comprehensive survey of literature on Automotive Software Engineering (ASE).679 primary studies were identified, classified and analyzed with respect to five dimensions.Three most investigated areas include software architecture &amp; design, testing and reuse.ASE seems to have high industrial relevance but is relatively lower in its scientific rigor.Validation &amp; comparative studies are less represented and literature lacks practitioner-oriented guidelines. The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor.},
journal = {J. Syst. Softw.},
month = jun,
pages = {25–55},
numpages = {31},
keywords = {Systematic mapping study, Software-intensive systems, Literature survey, Embedded systems, Automotive systems, Automotive software engineering}
}

@article{10.4018/IJCSSA.2015070103,
author = {Kolagari, Ramin Tavakoli and Chen, DeJiu and Lanusse, Agnes and Librino, Renato and L\"{o}nn, Henrik and Mahmud, Nidhal and Mraidha, Chokri and Reiser, Mark-Oliver and Torchiaro, Sandra and Tucci-Piergiovanni, Sara and W\"{a}gemann, Tobias and Yakymets, Nataliya},
title = {Model-Based Analysis and Engineering of Automotive Architectures with EAST-ADL: Revisited},
year = {2015},
issue_date = {July 2015},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {2},
issn = {2166-7292},
url = {https://doi.org/10.4018/IJCSSA.2015070103},
doi = {10.4018/IJCSSA.2015070103},
abstract = {Modern cars have turned into complex high-technology products, subject to strict safety and timing requirements, in a short time span. This evolution has translated into development processes that are not as efficient, flexible and agile as they could or should be. This paper presents the main aspects and capabilities of a rich model-based design framework, founded on EAST-ADL. EAST-ADL is an architecture description language specific to the automotive domain and complemented by a methodology compliant with the functional safety standard for the automotive domain ISO26262. The language and the methodology are used to develop an information model in the sense of a conceptual model, providing the engineer the basis for specifying the various aspects of the system. Inconsistencies, redundancies, and partly even missing system description aspects can be found automaticlally by advanced analyses and optimization capabilities to effectively improve development processes of modern cars.},
journal = {Int. J. Concept. Struct. Smart Appl.},
month = jul,
pages = {25–70},
numpages = {46},
keywords = {Timing Modelling, Software Product Lines, Optimization, Model-Based Software Development, ISO 26262, Functional Safety, EAST-ADL, Dependability, Automotive Software Development, AUTOSAR}
}

@inproceedings{10.1145/2593743.2593746,
author = {Sierszecki, Krzysztof and Steffens, Michaela and Fogdal, Thomas and Savolainen, Juha and Mikkonen, Tommi},
title = {Towards green power electronics: software controllers and domain knowledge},
year = {2014},
isbn = {9781450328449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593743.2593746},
doi = {10.1145/2593743.2593746},
abstract = {One of the key challenges of green software is that various aspects have an impact to the overall energy consumption over the lifetime of a system operated by software. In particular, in the field of industrial applications, where embedded devices cooperate with many IT systems to make the industrial processes more efficient, to reduce waste or raw materials, and to save the environment, the concept of green software becomes unclear. In this paper, we address the green aspects of software in different phases – software construction, software execution, and software control in both inside an individual component and as a part of a complete industrial application. Furthermore, we demonstrate that the insight into system knowledge, not aspects related to software per se, is the key to create truly green software. Consequently, when considering truly software green, the focus is to be placed on the system level savings for embedded systems at the highest possible level where domain knowledge can be taken into account, not on software development or execution.},
booktitle = {Proceedings of the 3rd International Workshop on Green and Sustainable Software},
pages = {17–22},
numpages = {6},
keywords = {variable speed drives, software product lines, software development, green systems, embedded control systems, Green software},
location = {Hyderabad, India},
series = {GREENS 2014}
}

@article{10.1007/s10270-018-0662-9,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
title = {Tradeoffs in modeling performance of highly configurable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0662-9},
doi = {10.1007/s10270-018-0662-9},
abstract = {Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-influence model can fit different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more configuration options have only a minor influence on the prediction error and that ignoring them when learning a performance-influence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on specific regions of the configuration space and find a sweet spot between accuracy and effort. We further analyzed the causes for the configuration options and their interactions having the observed influences on the systems' performance. We were able to identify several patterns across subject systems, such as dominant configuration options and data pipelines, that explain the influences of highly influential configuration options and interactions, and give further insights into the domain of highly configurable systems.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2265–2283},
numpages = {19},
keywords = {Variability, Software product lines, Performance-influence models, Performance prediction, Machine learning, Highly configurable software systems, Feature interactions}
}

@article{10.1016/j.sysarc.2019.02.012,
author = {Brings, Jennifer and Daun, Marian and Bandyszak, Torsten and Stricker, Vanessa and Weyer, Thorsten and Mirzaei, Elham and Neumann, Martin and Zernickel, Jan Stefan},
title = {Model-based documentation of dynamicity constraints for collaborative cyber-physical system architectures: Findings from an industrial case study},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {97},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2019.02.012},
doi = {10.1016/j.sysarc.2019.02.012},
journal = {J. Syst. Archit.},
month = aug,
pages = {153–167},
numpages = {15},
keywords = {Dynamicity constraints, Dynamic morphology, CPS network, Exploratory case study, Cyber-physical systems, Cardinality-based feature models}
}

@inproceedings{10.1109/ICCBSS.2007.25,
author = {Bhattacharya, Sutirtha and Perry, Dewayne E.},
title = {Predicting Emergent Properties of Component Based Systems},
year = {2007},
isbn = {076952785X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCBSS.2007.25},
doi = {10.1109/ICCBSS.2007.25},
abstract = {Software Product Lines (SPL), Component Based Software Engineering (CBSE) and Commercial Off the Shelf (COTS) components provide a rich supporting base for creating software architectures. Further, they promise significant improvements in the quality of software configurations that can be composed from prebuilt components. Software architectural styles provide a way for achieving a desired coherence for such component-based architectures. This is because the different architectural styles enforce different quality attributes for a system. If the architectural style of an emergent system could be predicted in advance, the System Architect could make necessary changes to ensure that the quality attributes dictated by the system requirements were satisfied before the actual system was deployed. In this paper we propose a model for predicting architectural styles, and hence the quality attributes, based on use cases that need to be satisfied by a system configuration. Our technique can be used to determine stylistic conformance and hence indicate the presence or absence of architectural drift.},
booktitle = {Proceedings of the Sixth International IEEE Conference on Commercial-off-the-Shelf (COTS)-Based Software Systems},
pages = {41–50},
numpages = {10},
series = {ICCBSS '07}
}

@inproceedings{10.1145/3194760.3194761,
author = {Kessel, Marcus and Atkinson, Colin},
title = {Integrating reuse into the rapid, continuous software engineering cycle through test-driven search},
year = {2018},
isbn = {9781450357456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194760.3194761},
doi = {10.1145/3194760.3194761},
abstract = {Today's advanced agile practices such as Continuous Integration and Test-Driven Development support a wide range of software development activities to facilitate the rapid delivery of high-quality software. However, the reuse of pre-existing, third-party software components is not one of them. Software reuse is still primarily perceived as a time-consuming, unsystematic and ultimately, "discontinuous" activity even though it aims to deliver the same basic benefits as continuous software engineering - namely, a reduction in the time and effort taken to deliver quality software. However, the increasingly central role of testing in continuous software engineering offers a way of addressing this problem by exploiting the new generation of test-driven search engines that can harvest components based on tests. This search technology not only exploits artifacts that have already been created as part of the continuous testing process to harvest components, it returns results that have a high likelihood of being fit for purpose and thus of being worth reusing. In this paper, we propose to augment continuous software engineering with the rapid, continuous reuse of software code units by integrating the test-driven mining of software artifact repositories into the continuous integration process. More specifically, we propose to use tests written as part of the Test-First Development approach to perform test-driven searches for matching functionality while developers are working on their normal development activities. We discuss the idea of rapid, continuous code reuse based on recent advances in our test-driven search platform and elaborate on scenarios for its application in the future.},
booktitle = {Proceedings of the 4th International Workshop on Rapid Continuous Software Engineering},
pages = {8–11},
numpages = {4},
keywords = {test-driven search, test-driven reuse, test-driven development, rapid continuous integration, rapid continuous code reuse},
location = {Gothenburg, Sweden},
series = {RCoSE '18}
}

@article{10.1016/j.jss.2012.08.031,
author = {Kulesza, Uir\'{a} and Soares, S\'{e}Rgio and Chavez, Christina and Castor, Fernando and Borba, Paulo and Lucena, Carlos and Masiero, Paulo and Sant'Anna, Claudio and Ferrari, Fabiano and Alves, Vander and Coelho, Roberta and Figueiredo, Eduardo and Pires, Paulo F. and Delicato, Fl\'{a}Via and Piveta, Eduardo and Silva, Carla and Camargo, Valter and Braga, Rosana and Leite, Julio and Lemos, Ot\'{a}Vio and Mendon\c{c}A, Nabor and Batista, Thais and Bonif\'{a}Cio, Rodrigo and Cacho, N\'{e}Lio and Silva, Lyrene and Von Staa, Arndt and Silveira, F\'{a}Bio and Valente, Marco T\'{u}Lio and Alencar, Fernanda and Castro, Jaelson and Ramos, Ricardo and Penteado, Rosangela and Rubira, Cec\'{\i}Lia},
title = {The crosscutting impact of the AOSD Brazilian research community},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.08.031},
doi = {10.1016/j.jss.2012.08.031},
abstract = {Background: Aspect-Oriented Software Development (AOSD) is a paradigm that promotes advanced separation of concerns and modularity throughout the software development lifecycle, with a distinctive emphasis on modular structures that cut across traditional abstraction boundaries. In the last 15 years, research on AOSD has boosted around the world. The AOSD-BR research community (AOSD-BR stands for AOSD in Brazil) emerged in the last decade, and has provided different contributions in a variety of topics. However, despite some evidence in terms of the number and quality of its outcomes, there is no organized characterization of the AOSD-BR community that positions it against the international AOSD Research community and the Software Engineering Research community in Brazil. Aims: In this paper, our main goal is to characterize the AOSD-BR community with respect to the research developed in the last decade, confronting it with the AOSD international community and the Brazilian Software Engineering community. Method: Data collection, validation and analysis were performed in collaboration with several researchers of the AOSD-BR community. The characterization was presented from three different perspectives: (i) a historical timeline of events and main milestones achieved by the community; (ii) an overview of the research developed by the community, in terms of key challenges, open issues and related work; and (iii) an analysis on the impact of the AOSD-BR community outcomes in terms of well-known indicators, such as number of papers and number of citations. Results: Our analysis showed that the AOSD-BR community has impacted both the international AOSD Research community and the Software Engineering Research community in Brazil.},
journal = {J. Syst. Softw.},
month = apr,
pages = {905–933},
numpages = {29},
keywords = {Research impact, Modularity, Aspect-Oriented Software Development}
}

@article{10.1016/j.scico.2014.03.006,
author = {Gaia, Felipe Nunes and Ferreira, Gabriel Coutinho Sousa and Figueiredo, Eduardo and Maia, Marcelo de Almeida},
title = {A quantitative and qualitative assessment of aspectual feature modules for evolving software product lines},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {96},
number = {P2},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2014.03.006},
doi = {10.1016/j.scico.2014.03.006},
abstract = {Feature-Oriented Programming (FOP) and Aspect-Oriented Programming (AOP) are programming techniques based on composition mechanisms, called refinements and aspects, respectively. These techniques are assumed to be good variability mechanisms for implementing Software Product Lines (SPLs). Aspectual Feature Modules (AFM) is an approach that combines advantages of feature modules and aspects to increase concern modularity. Some guidelines on how to integrate these techniques have been established in some studies, but these studies do not focus the analysis on how effectively AFM can preserve the modularity and stability facilitating SPL evolution. The main purpose of this paper is to investigate whether the simultaneous use of aspects and features through the AFM approach facilitates the evolution of SPLs. The quantitative data were collected from two SPLs developed using four different variability mechanisms: (1) feature modules, aspects and aspects refinements of AFM, (2) aspects of aspect-oriented programming (AOP), (3) feature modules of feature-oriented programming (FOP), and (4) conditional compilation (CC) with object-oriented programming. Metrics for change propagation and modularity were calculated and the results support the benefits of the AFM option in a context where the product line has been evolved with addition or modification of crosscutting concerns. However a drawback of this approach is that refactoring components' design requires a higher degree of modifications to the SPL structure. Variability mechanisms are systematically evaluated in the evolution of SPLs.FOP and AFM have shown better adherence to the Open-Closed Principle than CC.When crosscutting concerns are present, AFM are recommended over FOP.Refactoring at component level has important impact in AFM and FOP.CC compilation should be avoided when modular design is an important requirement.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {230–253},
numpages = {24},
keywords = {Variability mechanisms, Software product lines, Feature-oriented programming, Aspectual feature modules, Aspect-oriented programming}
}

@article{10.1016/j.jss.2012.11.044,
author = {Souza, Iuri Santos and Da Silva Gomes, Gecynalda Soares and Da Mota Silveira Neto, Paulo Anselmo and Do Carmo Machado, Ivan and De Almeida, Eduardo Santana and De Lemos Meira, Silvio Romero},
title = {Evidence of software inspection on feature specification for software product lines},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.11.044},
doi = {10.1016/j.jss.2012.11.044},
abstract = {In software product lines (SPL), scoping is a phase responsible for capturing, specifying and modeling features, and also their constraints, interactions and variations. The feature specification task, performed in this phase, is usually based on natural language, which may lead to lack of clarity, non-conformities and defects. Consequently, scoping analysts may introduce ambiguity, inconsistency, omissions and non-conformities. In this sense, this paper aims at gathering evidence about the effects of applying an inspection approach to feature specification for SPL. Data from a SPL reengineering project were analyzed in this work and the analysis indicated that the correction activity demanded more effort. Also, Pareto's principle showed that incompleteness and ambiguity reported higher non-conformity occurrences. Finally, the Poisson regression analysis showed that sub-domain risk information can be a good indicator for prioritization of sub-domains in the inspection activity.},
journal = {J. Syst. Softw.},
month = may,
pages = {1172–1190},
numpages = {19},
keywords = {Software quality control, Software product lines, Software inspection, Empirical study}
}

@article{10.1016/j.infsof.2010.12.003,
author = {da Mota Silveira Neto, Paulo Anselmo and Carmo Machado, Ivan do and McGregor, John D. and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {A systematic mapping study of software product lines testing},
year = {2011},
issue_date = {May, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {5},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.12.003},
doi = {10.1016/j.infsof.2010.12.003},
abstract = {ContextIn software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development. ObjectiveThis paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature. MethodA systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated. ResultsAlthough several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed. ConclusionThe results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {407–423},
numpages = {17},
keywords = {Software testing, Software product lines, Mapping study}
}

@inproceedings{10.5555/2008503.2008518,
author = {Bo\v{s}kovi\'{c}, Marko and Mussbacher, Gunter and Bagheri, Ebrahim and Amyot, Daniel and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
title = {Aspect-oriented feature models},
year = {2010},
isbn = {9783642212093},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software Product Lines (SPLs) have emerged as a prominent approach for software reuse. SPLs are sets of software systems called families that are usually developed as a whole and share many common features. Feature models are most typically used as a means for capturing commonality and managing variability of the family. A particular product from the family is configured by selecting the desired features of that product. Typically, feature models are considered monolithic entities that do not support modularization well. As industrial feature models tend to be large, their modularization has become an important research topic lately. However, existing modularization approaches do not support modularization of crosscutting concerns. In this paper, we introduce Aspect-oriented Feature Models (AoFM) and argue that using aspect-oriented techniques improves the manageability and reduces the maintainability effort of feature models. Particularly, we advocate an asymmetric approach that allows for the modularization of basic and crosscutting concerns in feature models.},
booktitle = {Proceedings of the 2010 International Conference on Models in Software Engineering},
pages = {110–124},
numpages = {15},
keywords = {software product lines, feature models, aspect-oriented modeling},
location = {Oslo, Norway},
series = {MODELS'10}
}

@inproceedings{10.1145/1321631.1321711,
author = {Botterweck, Goetz and O'Brien, Liam and Thiel, Steffen},
title = {Model-driven derivation of product architectures},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321711},
doi = {10.1145/1321631.1321711},
abstract = {Product Derivation is one of the central activities in Software Product Lines (SPL). One of the main challenges of the process of product derivation is dealing with complexity, which is caused by the large number of artifacts and dependencies between them. Another major challenge is maximizing development efficiency and reducing time-to-market, while at the same time producing high quality products. One approach to overcome these challenges is to automate the derivation process. To this end, this paper focuses on one particular activity of the derivation process; the derivation of the product-specific architecture and describes how this activity can be automated using a model-driven approach. The approach derives the product-specific architecture by selectively copying elements from the product-line architecture. The decision, which elements are included in the derived architecture, is based on a product-specific feature configuration. We present a prototype that implements the derivation as a model transformation described in the Atlas Transformation Language (ATL). We conclude with a short overview of related work and directions for future research},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {469–472},
numpages = {4},
keywords = {software product lines, software architectures, product derivation, model-driven approaches, model transformation, ATL},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@inproceedings{10.1145/3239235.3239240,
author = {Aljarallah, Sulaiman and Lock, Russell},
title = {An exploratory study of software sustainability dimensions and characteristics: end user perspectives in the kingdom of Saudi Arabia (KSA)},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239240},
doi = {10.1145/3239235.3239240},
abstract = {Background: Sustainability has become an important topic globally and the focus on ICT sustainability is increasing. However, issues exist, including vagueness and complexity of the concept itself, in addition to immaturity of the Software Engineering (SE) field. Aims: The study surveys respondents on software sustainability dimensions and characteristics from their perspectives, and seeks to derive rankings for their priority. Method: An exploratory study was conducted to quantitatively investigate Saudi Arabian (KSA) software user's perceptions with regard to the concept itself, the dimensions and characteristics of the software sustainability. Survey data was gathered from 906 respondents. Results: The results highlight key dimensions for sustainability and their priorities to users. The results also indicate that the characteristics perceived to be the most significant, were security, usability, reliability, maintainability, extensibility and portability, whereas respondents were relatively less concerned with computer ethics (e.g. privacy and trust), functionality, efficiency and reusability. A key finding was that females considered the environmental dimension to be more important than males. Conclusions: The dimensions and characteristics identified here can be used as a means of providing valuable feedback for the planning and implementation of future development of sustainable software.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {14},
numpages = {10},
keywords = {sustainability dimensions, software sustainability, empirical study},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {sampling, machine learning, Performance-influence models},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/3382494.3410677,
author = {Shu, Yangyang and Sui, Yulei and Zhang, Hongyu and Xu, Guandong},
title = {Perf-AL: Performance Prediction for Configurable Software through Adversarial Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410677},
doi = {10.1145/3382494.3410677},
abstract = {Context: Many software systems are highly configurable. Different configuration options could lead to varying performances of the system. It is difficult to measure system performance in the presence of an exponential number of possible combinations of these options.Goal: Predicting software performance by using a small configuration sample.Method: This paper proposes Perf-AL to address this problem via adversarial learning. Specifically, we use a generative network combined with several different regularization techniques (L1 regularization, L2 regularization and a dropout technique) to output predicted values as close to the ground truth labels as possible. With the use of adversarial learning, our network identifies and distinguishes the predicted values of the generator network from the ground truth value distribution. The generator and the discriminator compete with each other by refining the prediction model iteratively until its predicted values converge towards the ground truth distribution.Results: We argue that (i) the proposed method can achieve the same level of prediction accuracy, but with a smaller number of training samples. (ii) Our proposed model using seven real-world datasets show that our approach outperforms the state-of-the-art methods. This help to further promote software configurable performance.Conclusion: Experimental results on seven public real-world datasets demonstrate that PERF-AL outperforms state-of-the-art software performance prediction methods.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {16},
numpages = {11},
keywords = {regularization, configurable systems, adversarial learning, Software performance prediction},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {sparsity regularization, software performance prediction, highly configurable systems, deep sparse feedforward neural network},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/3313789,
author = {Reuling, Dennis and Kelter, Udo and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Automated N-way Program Merging for Facilitating Family-based Analyses of Variant-rich Software},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3313789},
doi = {10.1145/3313789},
abstract = {Nowadays software tends to come in many different, yet similar variants, often derived from a common code base via clone-and-own. Family-based-analysis strategies have recently shown very promising potential for improving efficiency in applying quality-assurance techniques to such variant-rich programs, as compared to variant-by-variant approaches. Unfortunately, these strategies require a single program representation superimposing all program variants in a syntactically well-formed, semantically sound, and variant-preserving manner, which is usually not available and manually hard to obtain in practice. In this article, we present a novel methodology, called SiMPOSE, for automatically generating superimpositions of existing program variants to facilitate family-based analyses of variant-rich software. To this end, we propose a novel N-way model-merging methodology to integrate the control-flow automaton (CFA) representations of N given variants of a C program into one unified CFA representation. CFA constitute a unified program abstraction used by many recent software-analysis tools for automated quality assurance. To cope with the inherent complexity of N-way model-merging, our approach (1) utilizes principles of similarity-propagation to reduce the number of potential N-way matches, and (2) enables us to decompose a set of N variants into arbitrary subsets and to incrementally derive an N-way superimposition from partial superimpositions. We apply our tool implementation of SiMPOSE to a selection of realistic C programs, frequently considered for experimental evaluation of program-analysis techniques. In particular, we investigate applicability and efficiency/effectiveness trade-offs of our approach by applying SiMPOSE in the context of family-based unit-test generation as well as model-checking as sample program-analysis techniques. Our experimental results reveal very impressive efficiency improvements by an average factor of up to 2.6 for test-generation and up to 2.4 for model-checking under stable effectiveness, as compared to variant-by-variant approaches, thus amortizing the additional effort required for merging. In addition, our results show that merging all N variants at once produces, in almost all cases, clearly more precise results than incremental step-wise 2-way merging. Finally, our comparison with major existing N-way merging techniques shows that SiMPOSE constitutes, in most cases, the best efficiency/effectiveness trade-off.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {13},
numpages = {59},
keywords = {variability encoding, quality assurance, model matching, control flow automata, Program merging}
}

@inproceedings{10.1145/3196398.3196442,
author = {Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe},
title = {Data-driven search-based software engineering},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196442},
doi = {10.1145/3196398.3196442},
abstract = {This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {341–352},
numpages = {12},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.1016/j.jss.2007.08.033,
author = {Sadat-Mohtasham, S. Hossein and Ghorbani, Ali A.},
title = {A language for high-level description of adaptive web systems},
year = {2008},
issue_date = {July, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.08.033},
doi = {10.1016/j.jss.2007.08.033},
abstract = {Adaptive Web systems (AWS) are Web-based systems that can adapt their features such as, presentation, content, and structure, based on users' behaviour and preferences, device capabilities, and environment attributes. A framework was developed in our research group to provide the necessary components and protocols for the development of adaptive Web systems; however, there were several issues and shortcomings (e.g. low productivity, lack of verification mechanisms, etc.) in using the framework that inspired the development of a domain-specific language for the framework. This paper focuses on the proposal, design, and implementation of AWL, the Adaptive Web Language, which is used to develop adaptive Web systems within our framework. Not only does AWL address the existing issues in the framework, but it also offers mechanisms to increase software quality attributes, especially, reusability. An example application named PENS (a personalized e-News system) is explained and implemented in AWL. AWL has been designed based on the analysis of the adaptive Web domain, having taken into account the principles of reuse-based software engineering (product-lines), domain-specific languages, and aspect-oriented programming. Specially, a novel design decision, inspired by aspect-oriented programming paradigm, allows separate specification of presentation features in an application from its adaptation features. The AWL's design decisions and their benefits are explained.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1196–1217},
numpages = {22},
keywords = {Domain-specific programming language, Aspect-oriented programming, Adaptive web system}
}

@inproceedings{10.1145/2361999.2362032,
author = {Nakagawa, Elisa Yumi},
title = {Reference architectures and variability: current status and future perspectives},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362032},
doi = {10.1145/2361999.2362032},
abstract = {Reference architectures refer to a special type of software architecture that achieves well-recognized understanding of specific domains, promoting reuse of design expertise and facilitating the development, standardization, and evolution of software systems. Designed for various domains and purpose, they have increasingly impacted important aspects of system development, such as productivity and quality of such systems. In another perspective, variability has been considered in several research topics as a mechanism that facilitates software development and evolution. In this context, the main contribution of this paper is to present the current status regarding variability in the reference architecture engineering. It is also presented future research perspectives that could be conducted, providing new directions to the reference architecture engineering in order to become existing and new reference architectures more effective elements to the development and evolution of software-intensive systems.},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {159–162},
numpages = {4},
keywords = {variability, reference architecture},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@inproceedings{10.1145/3442391.3442409,
author = {G\"{o}ttmann, Hendrik and Bacher, Isabelle and Gottwald, Nicolas and Lochau, Malte},
title = {Static Analysis Techniques for Efficient Consistency Checking of Real-Time-Aware DSPL Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442409},
doi = {10.1145/3442391.3442409},
abstract = {Dynamic Software Product Lines (DSPL) have recently gained momentum as integrated engineering methodology for (self-)adaptive software. DSPL enhance statically configurable software by enabling run-time reconfiguration to facilitate continuous adaptations to changing environmental contexts. In a previous work, we presented a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Internally, we translate real-time-aware DSPL specifications into timed automata serving as input for off-the-shelf model checkers like Uppaal for automatically checking semantic consistency properties. However, due to the very high computational complexity of model checking timed automata, those consistency checks suffer from scalability problems thus obstructing practical applications of the proposed approach. In this paper, we tackle this issue by investigating various kinds of static-analysis techniques that (1) aim to avoid expensive model checker calls by statically detecting certain classes of inconsistencies beforehand and otherwise (2) perform model reduction by detecting and merging equivalence states prior to model checker calls. The results of our experimental evaluation show very promising performance improvements achievable by those techniques, especially by the model-reduction approach.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {9},
keywords = {Timed Automata, Reconfiguration Decisions, Dynamic Software Product Lines},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1109/MiSE.2019.00018,
author = {Sch\"{o}ttle, Matthias and Kienzle, J\"{o}rg},
title = {On the difficulties of raising the level of abstraction and facilitating reuse in software modelling: the case for signature extension},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MiSE.2019.00018},
doi = {10.1109/MiSE.2019.00018},
abstract = {Reuse is central to improving the software development process, increasing software quality and decreasing time-to-market. Hence it is of paramount importance that modelling languages provide features that enable the specification and modularization of reusable artefacts, as well as their subsequent reuse. In this paper we outline several difficulties caused by the finality of method signatures that make it hard to specify and use reusable artefacts encapsulating several variants. The difficulties are illustrated with a running example. To evaluate whether these difficulties can be observed at the programming level, we report on an empirical study conducted on the Java Platform API as well as present workarounds used in various programming languages to deal with the rigid nature of signatures. Finally, we outline signature extension as an approach to overcome these problems at the modelling level.},
booktitle = {Proceedings of the 11th International Workshop on Modelling in Software Engineerings},
pages = {71–77},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {MiSE '19}
}

@article{10.1007/s00607-013-0338-9,
author = {Bertolino, Antonia and Inverardi, Paola and Muccini, Henry},
title = {Software architecture-based analysis and testing: a look into achievements and future challenges},
year = {2013},
issue_date = {August    2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {95},
number = {8},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-013-0338-9},
doi = {10.1007/s00607-013-0338-9},
journal = {Computing},
month = aug,
pages = {633–648},
numpages = {16}
}

@article{10.1016/j.jss.2014.01.051,
author = {Galster, Matthias and Avgeriou, Paris and M\"{a}nnist\"{o}, Tomi and Weyns, Danny},
title = {Editorial: Variability in software architecture - State of the art},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.01.051},
doi = {10.1016/j.jss.2014.01.051},
abstract = {Many of today's software systems are built with variability in mind, such as software product families, self-adaptive systems, and open platforms. The architecture of a software system is the reference point for all development activities and the earliest point where significant design decisions are taken. We appeal to the software architecture community to continue the work on variability in software architecture as the needs for variability will only grow in the future, and the more we learn the more open questions we come across. This guarantees both challenging and rewarding times ahead.},
journal = {J. Syst. Softw.},
month = may,
pages = {1–2},
numpages = {2}
}

@article{10.1016/j.future.2019.04.032,
author = {Mousa, Afaf and Bentahar, Jamal and Alam, Omar},
title = {Context-aware composite SaaS using feature model},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {99},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.04.032},
doi = {10.1016/j.future.2019.04.032},
journal = {Future Gener. Comput. Syst.},
month = oct,
pages = {376–390},
numpages = {15}
}

@inbook{10.5555/1985668.1985673,
author = {Kazhamiakin, Raman and Benbernou, Salima and Baresi, Luciano and Plebani, Pierluigi and Uhlig, Maike and Barais, Olivier},
title = {Adaptation of service-based systems},
year = {2010},
isbn = {3642175988},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Service Research Challenges and Solutions for the Future Internet: S-Cube - towards Engineering, Managing and Adapting Service-Based Systems},
pages = {117–156},
numpages = {40}
}

@inproceedings{10.1007/978-3-642-30982-3_7,
author = {Petriu, Dorina C. and Alhaj, Mohammad and Tawhid, Rasha},
title = {Software performance modeling},
year = {2012},
isbn = {9783642309816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30982-3_7},
doi = {10.1007/978-3-642-30982-3_7},
abstract = {Ideally, a software development methodology should include both the ability to specify non-functional requirements and to analyze them starting early in the lifecycle; the goal is to verify whether the system under development would be able to meet such requirements. This chapter considers quantitative performance analysis of UML software models annotated with performance attributes according to the standard "UML Profile for Modeling and Analysis of Real-Time and Embedded Systems" (MARTE). The chapter describes a model transformation chain named PUMA (Performance by Unified Model Analysis) that enables the integration of performance analysis in a UML-based software development process, by automating the derivation of performance models from UML+MARTE software models, and by facilitating the interoperability of UML tools and performance tools. PUMA uses an intermediate model called "Core Scenario Model" (CSM) to bridge the gap between different kinds of software models accepted as input and different kinds of performance models generated as output. Transformation principles are described for transforming two kinds of UML behaviour representation (sequence and activity diagrams) into two kinds of performance models (Layered Queueing Networks and stochastic Petri nets). Next, PUMA extensions are described for two classes of software systems: service-oriented architecture (SOA) and software product lines (SPL).},
booktitle = {Proceedings of the 12th International Conference on Formal Methods for the Design of Computer, Communication, and Software Systems: Formal Methods for Model-Driven Engineering},
pages = {219–262},
numpages = {44},
location = {Bertinoro, Italy},
series = {SFM'12}
}

@inproceedings{10.1145/2851613.2851758,
author = {Bombonatti, Denise and Gralha, Catarina and Moreira, Ana and Ara\'{u}jo, Jo\~{a}o and Goul\~{a}o, Miguel},
title = {Usability of requirements techniques: a systematic literature review},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851758},
doi = {10.1145/2851613.2851758},
abstract = {The usability of requirements engineering (RE) techniques has been recognised as a key factor for their successful adoption by industry. RE techniques must be accessible to stakeholders with different backgrounds, so they can be empowered to effectively and efficiently contribute to building successful systems. When selecting an appropriate requirements engineering technique for a given context, one should consider the usability supported by each of the candidate techniques. The first step towards achieving this goal is to gather the best evidence available on the usability of RE approaches by performing a systematic literature review, to answer one research question: How is the usability of requirements engineering techniques and tools addressed? We systematically review articles published in the Requirements Engineering Journal, one of the main sources for mature work in RE, to motivate a research roadmap to make RE approaches more accessible to stakeholders with different backgrounds.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1270–1275},
numpages = {6},
keywords = {requirements engineering approaches, systematic literature review, usability},
location = {Pisa, Italy},
series = {SAC '16}
}

@article{10.1016/j.jss.2015.09.019,
author = {Vale, Tassio and Crnkovic, Ivica and de Almeida, Eduardo Santana and Silveira Neto, Paulo Anselmo da Mota and Cavalcanti, Yguarat\~{a} Cerqueira and Meira, Silvio Romero de Lemos},
title = {Twenty-eight years of component-based software engineering},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {111},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.09.019},
doi = {10.1016/j.jss.2015.09.019},
abstract = {We defined more precisely the identification of the gaps.We also defined more precisely the incentives for further research.In Section 4.3 we made explicit connection to the Fig. 15 and identified gaps.All pointed typos were fixed. The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research.},
journal = {J. Syst. Softw.},
month = jan,
pages = {128–148},
numpages = {21},
keywords = {Systematic mapping study, Software component, Component-based software engineering, Component-based software development}
}

@article{10.1016/j.jss.2016.06.102,
author = {Lung, Chung-Horng and Zhang, Xu and Rajeswaran, Pragash},
title = {Improving software performance and reliability in a distributed and concurrent environment with an architecture-based self-adaptive framework},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.102},
doi = {10.1016/j.jss.2016.06.102},
abstract = {We proposed a novel software architecture-level adaptation approach.We adopted known architectural patterns in distributed and concurrent systems.We developed a framework to support the self-adaptive mechanism.We developed and evaluated five adaptive policies.Our approach improved performance and increased reliability in our experiments. More and more, modern software systems in a distributed and parallel environment are becoming highly complex and difficult to manage. A self-adaptive approach that integrates monitoring, analyzing, and actuation functionalities has the potential to accommodate an ever dynamically changing environment. This paper proposes an architecture-level self-adaptive framework with the aim of improving performance and reliability. To meet such a goal, this paper presents a Self-Adaptive Framework for Concurrency Architectures (SAFCA) that consists of multiple well-documented architectural patterns in addition to monitoring and adaptive capabilities. With this framework, a system using an architectural alternative can activate another alternative at runtime to cope with increasing demands or to recover from failure. Five adaptation mechanisms have been developed for concept demonstration and evaluation; four focus on performance improvement and one deals with failover and reliability enhancement. We have performed a number of experiments with this framework. The experimental results demonstrate that the proposed adaptive framework can mitigate the over-provisioning method commonly used in practice. As a result, resource usage becomes more efficient for most normal conditions, while the system is still able to effectively handle bursty or growing demands using an adaptive mechanism. The performance of SAFCA is also better than systems using only standalone architectural alternatives without an adaptation scheme. Moreover, the experimental results show that a fast recovery can be realized in the case of failure by conducting an architecture switchover to maintain the desired service.},
journal = {J. Syst. Softw.},
month = nov,
pages = {311–328},
numpages = {18},
keywords = {Software architecture, Reliability, Performance, Patterns, Elastic computing, Distributed and concurrent architecture, Autonomic computing}
}

@inproceedings{10.1145/2578128.2578237,
author = {de Andrade, Hugo Sica and Almeida, Eduardo and Crnkovic, Ivica},
title = {Architectural bad smells in software product lines: an exploratory study},
year = {2014},
isbn = {9781450325233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578128.2578237},
doi = {10.1145/2578128.2578237},
abstract = {The Software Product Lines (SPL) paradigm has arisen for taking advantage of existing common aspects between different products, while also considering product-specific features. The architecture of a SPL comprises a model that will result in product architectures, and may include solutions leading to bad (architectural) design. One way to assess such design decisions is through the identification of architectural bad smells, which are properties that prejudice the overall software quality, but are not necessarily faulty or errant. In this paper, we conduct an exploratory study that aims at characterizing bad smells in the context of product line architectures. We analyzed an open source SPL project and extracted its architecture to investigate the occurrence or absence of four smells initially studied in single systems. In addition, we propose a smell specific to the SPL context and discuss possible causes and implications of having those smells in the architecture of a product line. The results indicate that the granularity of the SPL features may influence on the occurrence of smells.},
booktitle = {Proceedings of the WICSA 2014 Companion Volume},
articleno = {12},
numpages = {6},
keywords = {software product lines, exploratory study, evaluation, architecture, architectural bad smells},
location = {Sydney, Australia},
series = {WICSA '14 Companion}
}

@article{10.1016/j.future.2014.12.002,
author = {Weinreich, Rainer and Groher, Iris and Miesbauer, Cornelia},
title = {An expert survey on kinds, influence factors and documentation of design decisions in practice},
year = {2015},
issue_date = {June 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2014.12.002},
doi = {10.1016/j.future.2014.12.002},
abstract = {Support for capturing architectural knowledge has been identified as an important research challenge. As the basis for an approach to recovering design decisions and capturing their rationale, we performed an expert survey in practice to gain insights into the different kinds, influence factors, and sources for design decisions and also into how they are currently captured in practice. The survey was conducted with 25 software architects, software team leads, and senior developers from 22 different companies in 10 different countries with more than 13 years of experience in software development on average. The survey confirms earlier work by other authors on design decision classification and influence factors, and also identifies additional kinds of decisions and influence factors not mentioned in previous work. In addition, we gained insight into the practice of capturing, the relative importance of different decisions and influence factors, and into potential sources for recovering decisions. We present results of a qualitative expert survey on design decisions in practice.We examine design decision classification, documentation, and influence factors.We collect architects' experiences in decision making and documentation.We provide recommendations for potential improvements and research directions based on the results of our study.Results are compared to literature and similar studies.},
journal = {Future Gener. Comput. Syst.},
month = jun,
pages = {145–160},
numpages = {16},
keywords = {Software architecture knowledge management, Design decisions, Design decision influence factors, Design decision documentation, Design decision classification}
}

@inproceedings{10.1145/1083183.1083188,
author = {Kr\"{u}ger, Ingolf H. and Mathew, Reena and Meisinger, Michael},
title = {From scenarios to aspects: exploring product lines},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083183.1083188},
doi = {10.1145/1083183.1083188},
abstract = {Software product lines are gaining importance because they allow improvements in time to market, cost, productivity and quality of software products. Architecture evaluation is one important aspect in the development of product lines for large-scale distributed systems. It is desirable to evaluate and compare architectures for functionality and quality attributes before implementing or changing the whole system. Often, the effort required for the thorough evaluation of alternatives using prototypes is prohibitive. In this paper, we present an approach for cost-efficient software architecture evaluation, based on scenario-oriented software specifications, modeling the system services. We show how to map the same set of services to several possible target architectures and give a procedure to generate evaluation prototypes using aspect-oriented programming techniques. This significantly reduces the effort required to explore architectural alternatives. We explain our approach using the Center TRACON Automation System as an example.},
booktitle = {Proceedings of the Fourth International Workshop on Scenarios and State Machines: Models, Algorithms and Tools},
pages = {1–6},
numpages = {6},
location = {St. Louis, Missouri},
series = {SCESM '05}
}

@book{10.5555/2671146,
author = {Mistrik, Ivan and Bahsoon, Rami and Kazman, Rick and Zhang, Yuanyuan},
title = {Economics-Driven Software Architecture},
year = {2014},
isbn = {0124104649},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Economics-driven Software Architecture presents a guide for engineers and architects who need to understand the economic impact of architecture design decisions: the long term and strategic viability, cost-effectiveness, and sustainability of applications and systems. Economics-driven software development can increase quality, productivity, and profitability, but comprehensive knowledge is needed to understand the architectural challenges involved in dealing with the development of large, architecturally challenging systems in an economic way. This book covers how to apply economic considerations during the software architecting activities of a project. Architecture-centric approaches to development and systematic evolution, where managing complexity, cost reduction, risk mitigation, evolvability, strategic planning and long-term value creation are among the major drivers for adopting such approaches. It assists the objective assessment of the lifetime costs and benefits of evolving systems, and the identification of legacy situations, where architecture or a component is indispensable but can no longer be evolved to meet changing needs at economic cost. Such consideration will form the scientific foundation for reasoning about the economics of nonfunctional requirements in the context of architectures and architecting. Familiarizes readers with essential considerations in economic-informed and value-driven software design and analysis Introduces techniques for making value-based software architecting decisions Provides readers a better understanding of the methods of economics-driven architecting}
}

@inproceedings{10.5555/2820656.2820664,
author = {de Jesus Souza, Magno Lu\~{a} and Santos, Alcemir Rodrigues and de Almeida, Eduardo Santana},
title = {Towards the selection of modeling techniques for dynamic software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {Emerging domains such as smart homes and more recently smart cities represent a big challenge to software engineering. In such context, the need of runtime self-adaptations to cope with both user needs and environmental changes brings Dynamic Software Product Lines (DSPL) as a suitable solution. However, DSPL implementation itself is challenging, which demands a proper modeling. In this sense, the literature still lacks of means of choosing the modeling technique that best fits a given domain. This paper tackles such problem by defining a criteria for rank such techniques, which is used for ranking a set DSPL modeling techniques found in the literature.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {19–22},
numpages = {4},
keywords = {modeling techniques, dynamic variability, dynamic software product lines},
location = {Florence, Italy},
series = {PLEASE '15}
}

@inproceedings{10.1145/1596495.1596500,
author = {Peper, Christian and Schneider, Daniel},
title = {On runtime service quality models in adaptive ad-hoc systems},
year = {2009},
isbn = {9781605586816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1596495.1596500},
doi = {10.1145/1596495.1596500},
abstract = {Ad-hoc computer systems can automatically realize higher services when at least two distributed and communicating (embedded) devices come together. For this purpose, they must able to manage appearance and loss of devices and resources, and they have to adapt to changes in requirements and environment. Based on a component-oriented approach for adaptive ad-hoc systems, this paper suggests a high-level service quality reference model to advocate further research on the quality matching problem between service provider and client components.},
booktitle = {Proceedings of the 2009 ESEC/FSE Workshop on Software Integration and Evolution @ Runtime},
pages = {11–18},
numpages = {8},
keywords = {ubiquitous computing, quality-of-service, distributed systems, component-orientation, ambient intelligence, adaptivity, ad-hoc systems},
location = {Amsterdam, The Netherlands},
series = {SINTER '09}
}

@article{10.1145/3204459,
author = {Chen, Tao and Li, Ke and Bahsoon, Rami and Yao, Xin},
title = {FEMOSAA: Feature-Guided and Knee-Driven Multi-Objective Optimization for Self-Adaptive Software},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3204459},
doi = {10.1145/3204459},
abstract = {Self-Adaptive Software (SAS) can reconfigure itself to adapt to the changing environment at runtime, aiming to continually optimize conflicted nonfunctional objectives (e.g., response time, energy consumption, throughput, cost, etc.). In this article, we present Feature-guided and knEe-driven Multi-Objective optimization for Self-Adaptive softwAre (FEMOSAA), a novel framework that automatically synergizes the feature model and Multi-Objective Evolutionary Algorithm (MOEA) to optimize SAS at runtime. FEMOSAA operates in two phases: at design time, FEMOSAA automatically transposes the engineers’ design of SAS, expressed as a feature model, to fit the MOEA, creating new chromosome representation and reproduction operators. At runtime, FEMOSAA utilizes the feature model as domain knowledge to guide the search and further extend the MOEA, providing a larger chance for finding better solutions. In addition, we have designed a new method to search for the knee solutions, which can achieve a balanced tradeoff. We comprehensively evaluated FEMOSAA on two running SAS: One is a highly complex SAS with various adaptable real-world software under the realistic workload trace; another is a service-oriented SAS that can be dynamically composed from services. In particular, we compared the effectiveness and overhead of FEMOSAA against four of its variants and three other search-based frameworks for SAS under various scenarios, including three commonly applied MOEAs, two workload patterns, and diverse conflicting quality objectives. The results reveal the effectiveness of FEMOSAA and its superiority over the others with high statistical significance and nontrivial effect sizes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {5},
numpages = {50},
keywords = {self-adaptive system, search-based software engineering, performance engineering, multi-objective optimization, multi-objective evolutionary algorithm, Feature model}
}

@article{10.1155/2021/8822786,
author = {Zhao, Yiqing and Prabhu, M. and Ahmed, Ramyar Rzgar and Sahu, Anoop Kumar and Bruneo, Dario},
title = {Research Trends and Performance of IIoT Communication Network-Architectural Layers of Petrochemical Industry 4.0 for Coping with Circular Economy},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/8822786},
doi = {10.1155/2021/8822786},
abstract = {In the present era, many Petrochemical Industries (PIs) are driven energetically due to IIoT (Industrial Internet of Things) Communication Networks/Architectural Layers (CNs/ALs), abbreviated as PI4.0-CNs/ALs. PI4.0 fruitfully participated to achieve the Circular Economy (CE) by speeding the reutilization, recovery, and recycling of scrap materials by minimizing cost, unproductive operations, energy consumption, emission of flue gases, etc. Recently, it has been ascertained that the identification and measurement of Research Trends (RTs) of CNs-ALs help the PI4.0 to build the future CE. In addressing the said research challenge, the objective of this research dossier is turned towards inculcating into future PI4.0 researchers the RTs of CNs/ALs of PI4.0, so that the researches can be organized over the very weak and moderately performing CNs-ALs to hike the future CE. To materialize the RTs of PI4-CNs/ALs, the authors conducted the Systematic Literature Survey (SLS) focusing on PI4.0-CNs/ALs, i.e., Internet of Things (IoTs), Cyber Physical System (CPS), Virtual Reality (VR), Integration (I), Data Optimization (DO), Enterprise Resource Planning (ERP), Plant Control (PC), Data and Analytics (DA), Network (N), and Information and Data Management (IDM). The authors searched three hundred two research documents, wherein two hundred seventy-five research manuscripts qualified as RQ2. Next, the authors collected the DOIs/URLs corresponding to each CN-AL and explored the Sum of Digit Scoring System (SDSS) to summarize the DOIs/URLs of PI4.0-CNs/ALs. The RTs of DO have been determined as excellent and stronger over 2007-2017 than residue CNs/ALs. Eventually, the authors advised scholars to focus on the research areas of very weak and moderately weak performing CNs/ALs in order to attain future CE.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {32}
}

@article{10.1007/s10270-014-0405-5,
author = {Iqbal, Muhammad Zohaib and Ali, Shaukat and Yue, Tao and Briand, Lionel},
title = {Applying UML/MARTE on industrial projects: challenges, experiences, and guidelines},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-014-0405-5},
doi = {10.1007/s10270-014-0405-5},
abstract = {Modeling and Analysis of Real-Time and Embedded Systems (MARTE) is a Unified Modeling Language (UML) profile, which has been developed to model concepts specific to Real-Time and Embedded Systems (RTES). In the last 5 years, we have applied UML/MARTE to three distinct industrial problems in three industry sectors: architecture modeling and configuration of large-scale and highly configurable integrated control systems, model-based robustness testing of communication-intensive systems, and model-based environment simulator generation of large-scale RTES for testing. In this paper, we report on our experience of solving these problems by applying UML/MARTE on four industrial case studies. We highlight the challenges we faced with respect to the industrial adoption of MARTE. Based on our combined experience, we derive a framework to guide practitioners for future applications of UML/MARTE in an industrial context. The framework provides a set of detailed guidelines that help reduce the gap between the modeling notations and real-world industrial application needs.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {1367–1385},
numpages = {19},
keywords = {UML, Real-Time Embedded Systems, Model-based Testing, MARTE, Industrial Case Studies, Architecture Modeling}
}

@inproceedings{10.1109/WICSA.2005.50,
author = {Bhattacharya, Sutirtha and Perry, Dewayne E.},
title = {Predicting Architectural Styles from Component Specifications},
year = {2005},
isbn = {0769525482},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WICSA.2005.50},
doi = {10.1109/WICSA.2005.50},
abstract = {Software Product Lines (SPL), Component Based Software Engineering (CBSE) and Commercial Off The Shelf (COTS) components provide a rich supporting base for creating software architectures. Further, they promise significant improvements in the quality of software configurations that can be composed from pre-built components. Software architectural styles provide a way for achieving a desired coherence for such component-based architectures. This is because the different architectural styles enforce different quality attributes for a system. If the architectural style of an emergent system could be predicted in advance, a System Integrator could make necessary changes to ensure that the quality attributes dictated by the system requirements were satisfied before the actual system was deployed and tested. In this paper we propose a model for predicting architectural styles based on use cases that need to be met by a system configuration. Moreover, our technique can be used to determine stylistic conformance and hence indicate the presence or absence of architectural drift},
booktitle = {Proceedings of the 5th Working IEEE/IFIP Conference on Software Architecture},
pages = {231–232},
numpages = {2},
keywords = {System Composition, Reuse, Component Based Software Engineering, Architectural Style},
series = {WICSA '05}
}

@inproceedings{10.1145/2993236.2993246,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Collet, Philippe and Alam, Omar},
title = {Delaying decisions in variable concern hierarchies},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993246},
doi = {10.1145/2993236.2993246},
abstract = {Concern-Oriented Reuse (CORE) proposes a new way of structuring model-driven software development, where models of the system are modularized by domains of abstraction within units of reuse called concerns. Within a CORE concern, models are further decomposed and modularized by features. This paper extends CORE with a technique that enables developers of high-level concerns to reuse lower-level concerns without unnecessarily committing to a specific feature selection. The developer can select the functionality that is minimally needed to continue development, and reexpose relevant alternative lower-level features of the reused concern in the reusing concern's interface. This effectively delays decision making about alternative functionality until the higher-level reuse context, where more detailed requirements are known and further decisions can be made. The paper describes the algorithms for composing the variation (i.e., feature and impact models), customization, and usage interfaces of a concern, as well as the concern's realization models and finally an entire concern hierarchy, as is necessary to support delayed decision making in CORE.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {93–103},
numpages = {11},
keywords = {Reuse Hierarchies, Model-Driven Engineering, Model Reuse, Model Interfaces, Delaying of Decisions},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1109/ICSE.2019.00090,
author = {Lillack, Max and St\u{a}nciulescu, \c{S}tefan and Hedman, Wilhelm and Berger, Thorsten and W\k{a}sowski, Andrzej},
title = {Intention-based integration of software variants},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00090},
doi = {10.1109/ICSE.2019.00090},
abstract = {Cloning is a simple way to create new variants of a system. While cheap at first, it increases maintenance cost in the long term. Eventually, the cloned variants need to be integrated into a configurable platform. Such an integration is challenging: it involves merging the usual code improvements between the variants, and also integrating the variable code (features) into the platform. Thus, variant integration differs from traditional software merging, which does not produce or organize configurable code, but creates a single system that cannot be configured into variants. In practice, variant integration requires fine-grained code edits, performed in an exploratory manner, in multiple iterations. Unfortunately, little tool support exists for integrating cloned variants.In this work, we show that fine-grained code edits needed for integration can be alleviated by a small set of integration intentions---domain-specific actions declared over code snippets controlling the integration. Developers can interactively explore the integration space by declaring (or revoking) intentions on code elements. We contribute the intentions (e.g., 'keep functionality' or 'keep as a configurable feature') and the IDE tool INCLINE, which implements the intentions and five editable views that visualize the integration process and allow declaring intentions producing a configurable integrated platform. In a series of experiments, we evaluated the completeness of the proposed intentions, the correctness and performance of INCLINE, and the benefits of using intentions for variant integration. The experiments show that INCLINE can handle complex integration tasks, that views help to navigate the code, and that it consistently reduces mistakes made by developers during variant integration.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {831–842},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1016/j.infsof.2019.03.015,
author = {Borg, Markus and Chatzipetrou, Panagiota and Wnuk, Krzysztof and Al\'{e}groth, Emil and Gorschek, Tony and Papatheocharous, Efi and Shah, Syed Muhammad Ali and Axelsson, Jakob},
title = {Selecting component sourcing options: A survey of software engineering’s broader make-or-buy decisions},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.015},
doi = {10.1016/j.infsof.2019.03.015},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {18–34},
numpages = {17},
keywords = {Survey, Decision making, Software architecture, Sourcing, Component-based software engineering}
}

@inproceedings{10.5555/2018027.2018039,
author = {Johnsen, Andreas and Lundqvist, Kristina},
title = {Developing dependable software-intensive systems: AADL vs. EAST-ADL},
year = {2011},
isbn = {9783642213373},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Dependable software-intensive systems, such as embedded systems for avionics and vehicles are often developed under severe quality, schedule and budget constraints. As the size and complexity of these systems dramatically increases, the architecture design phase becomes more and more significant in order to meet these constraints. The use of Architecture Description Languages (ADLs) provides an important basis for mutual communication, analysis and evaluation activities. Hence, selecting an ADL suitable for such activities is of great importance. In this paper we compare and investigate the two ADLs - AADL and EASTADL. The level of support provided to developers of dependable software-intensive systems is compared, and several critical areas of the ADLs are highlighted. Results of using an extended comparison framework showed many similarities, but also one clear distinction between the languages regarding the perspectives and the levels of abstraction in which systems are modeled.},
booktitle = {Proceedings of the 16th Ada-Europe International Conference on Reliable Software Technologies},
pages = {103–117},
numpages = {15},
keywords = {software-intensive systems, dependable systems, architecture description languages, EAST-ADL, AADL},
location = {Edinburgh, UK},
series = {Ada-Europe'11}
}

@article{10.1016/j.jnca.2018.03.021,
title = {Resource management in cellular base stations powered by renewable energy sources},
year = {2018},
issue_date = {June 2018},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {112},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2018.03.021},
doi = {10.1016/j.jnca.2018.03.021},
abstract = {This paper aims to consolidate the work carried out in making base station (BS) green and energy efficient by integrating renewable energy sources (RES). Clean and green technologies are mandatory for reduction of carbon footprint in future cellular networks. RES, especially solar and wind, are emerging as a viable alternate to fossil fuel based energy, which is the main cause of climate pollution. With advances in technologies, renewable energy is making inroads into all sectors including information and communication technologies (ICT). The main contributors of energy consumption in ICT sector are data centers' and cellular networks'. In cellular networks the BS is the main consumer of energy, mostly powered by the utility and a diesel generator. This energy comes at a significant operating cost as well as the environmental cost in terms of harmful greenhouse gas (GHG) emissions. Recent research shows that powering BSs with renewable energy is technically feasible. Although installation cost of energy from non-renewable fuel is still lower than RES, optimized use of the two sources can yield the best results. This paper presents a comprehensive overview of resource management in cellular BSs powered by RES and an in-depth analysis of power consumption optimization in order to reduce both cost and GHGs. Renewable energy sources are not only feasible for a stand-alone or off-grid BSs, but also feasible for on-grid BSs. This paper covers different aspects of optimization in cellular networks to provide reader with a holistic view of concepts, directions, and advancements in renewable energy based systems incorporated in cellular communications. Energy management strategies are studied in the realm of smart grids and other technologies, increasing the possibilities for energy efficiency further by employing schemes such as energy cooperation. Finally, the paper supports the move towards green communication in order to contribute positively towards climate change.},
journal = {J. Netw. Comput. Appl.},
month = jun,
pages = {1–17},
numpages = {17}
}

@inproceedings{10.1145/3439961.3439975,
author = {Pald\^{e}s, Roberto Avila and Canedo, Edna Dias and Guimar\~{a}es, Fernando de Albuquerque and Calazans, Ang\'{e}lica Toffano Seidel},
title = {Functional Requirements Elicitation in IoT Systems: a follow-up study},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439975},
doi = {10.1145/3439961.3439975},
abstract = {As the Internet of Things (IoT) advances, specific views have been proposed for the entire software development cycle and also for Requirements Engineering (RE). The analysis of the use of RE techniques, tools, and models can contribute to obtain better results in this field. This paper presents a Systematic Mapping Study (SMS) to investigate techniques for Functional Requirements (FR) elicitation in IoT software systems, as well as gaps and limitations of current solutions. During the SMS, seventeen articles focused on FR in the IoT were found. The analysis was complemented with an input from the experience of practitioners who have dedicated to this topic, obtained through structured and semi-structured interviews. The results show that FR elicitation has started from the use of traditional techniques, but that these do not fully meet the specificities of the IoT. The majority of the models found are based on UML (Unified Modeling Language) and the most important techniques are based on scenarios. The tools that support these proposals are maturing or under development. In the conclusion, the study shows the advancements already achieved, as well as the challenges and opportunities that are still present.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {14},
numpages = {10},
keywords = {Systematic Mapping Study., Software System, Internet of Things, Functional Requirements Elicitation},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@inproceedings{10.1145/2721956.2721977,
author = {Kajtazovic, Nermin and Preschern, Christopher and H\"{o}ller, Andrea and Kreiner, Christian},
title = {Towards pattern-based reuse in safety-critical systems},
year = {2014},
isbn = {9781450334167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2721956.2721977},
doi = {10.1145/2721956.2721977},
abstract = {Challenges such as time-to-market, reduced costs for change and maintenance have radically influenced development of today's safety-critical systems. Many domains have already adopted their system's engineering to support modular and component-based architectures. With the component-based design paradigm, the system engineering is utilized allowing to distribute development among different development teams, however, with the price that there is no full trust in independently developed parts, which makes their reuse challenging. Until now, many approaches that address reuse, on conceptual or detailed level, have been proposed. A very important aspect addressed here is to document the information flow between system parts in detail, i.e. from higher abstraction levels down to the implementation details, in order to put more trust into independently developed parts of the system.In this paper, we describe a compact pattern system with the aim to establish a link between high level concepts for reuse and detailed description of the behavior of system parts. The main goal is to document these details up to the higher levels of abstraction in more systematic way.},
booktitle = {Proceedings of the 19th European Conference on Pattern Languages of Programs},
articleno = {33},
numpages = {15},
location = {Irsee, Germany},
series = {EuroPLoP '14}
}

@article{10.1016/j.scico.2019.102344,
author = {Basile, Davide and ter Beek, Maurice H. and Degano, Pierpaolo and Legay, Axel and Ferrari, Gian-Luigi and Gnesi, Stefania and Di Giandomenico, Felicita},
title = {Controller synthesis of service contracts with variability},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {187},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2019.102344},
doi = {10.1016/j.scico.2019.102344},
journal = {Sci. Comput. Program.},
month = feb,
numpages = {23},
keywords = {Behavioural variability, Variability, Service orchestrations, Contract automata, Supervisory control theory}
}

@inproceedings{10.1145/1370175.1370249,
author = {Avgeriou, Paris and Lago, Patricia and Kruchten, Philippe},
title = {Third international workshop on sharing and reusing architectural knowledge (SHARK 2008)},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370175.1370249},
doi = {10.1145/1370175.1370249},
abstract = {The shift of the software architecture community towards architectural knowledge has brought along some promising research directions. In this workshop we discuss the issues that lead to the application of architectural knowledge in research and industrial practice; ongoing research and new ideas to advance the field. In its previous editions we examined the state of the art and practice, future challenges and trends. This third edition will discuss, among others, architectural knowledge as perceived by different research communities, including requirements engineering, service-oriented computing and international standardization.},
booktitle = {Companion of the 30th International Conference on Software Engineering},
pages = {1065–1066},
numpages = {2},
keywords = {architectural knowledge},
location = {Leipzig, Germany},
series = {ICSE Companion '08}
}

@inproceedings{10.1109/CHASE.2009.5071420,
author = {Unphon, Hataichanok and Dittrich, Yvonne and Hubaux, Arnaud},
title = {Taking care of cooperation when evolving socially embedded systems: The PloneMeeting case},
year = {2009},
isbn = {9781424437122},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CHASE.2009.5071420},
doi = {10.1109/CHASE.2009.5071420},
abstract = {This paper proposes a framework to (i) analyse the contexts of socially embedded systems and (ii) support the understanding of change during their evolutions. Our finding is based on a co-operative project with a government agency developing a partially-automated variability configurator for an open source software product family. By employing our framework, we realised that the way variations and their management are implemented have to accommodate work practices from the use context as well as development practice, and here especially the cooperation within the development team and between users and developers. The empirical evidence has confirmed our understanding of what is relevant when estimating the evolvability of socially embedded systems. We propose to use our framework in architecture-level design and evaluation in order to take these cooperative relationships into account early in the evolution cycle.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Cooperative and Human Aspects on Software Engineering},
pages = {96–103},
numpages = {8},
series = {CHASE '09}
}

@article{10.1007/s11042-020-08849-y,
author = {Ullah, Inam and Jian, Muwei and Hussain, Sumaira and Guo, Jie and Yu, Hui and Wang, Xing and Yin, Yilong},
title = {A brief survey of visual saliency detection},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {45–46},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-08849-y},
doi = {10.1007/s11042-020-08849-y},
abstract = {Salient object detection models mimic the behavior of human beings and capture the most salient region/object from the images or scenes, this field contains many important applications in both computer vision and pattern recognition tasks. Despite hundreds of models that have been proposed in this field, but still, it requires a large room for research. This paper demonstrates a detailed overview of the recent progress of saliency detection models in terms of heuristic-based techniques and deep learning-based techniques. we have discussed and reviewed its co-related fields, such as Eye-fixation-prediction, RGBD salient-object-detection, co-saliency object detection, and video-saliency-detection models. We have reviewed the key issues of the current saliency models and discussed future trends and recommendations. The broadly utilized datasets and assessment strategies are additionally investigated in this paper.},
journal = {Multimedia Tools Appl.},
month = dec,
pages = {34605–34645},
numpages = {41},
keywords = {Saliency model, Salient object, Visual cues, Saliency detection}
}

@article{10.1007/s00450-011-0202-0,
author = {Drago, Mauro Luigi and Ghezzi, Carlo and Mirandola, Raffaela},
title = {A quality driven extension to the QVT-relations transformation language},
year = {2015},
issue_date = {February  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-011-0202-0},
doi = {10.1007/s00450-011-0202-0},
abstract = {An emerging approach to software development is Model Driven Software Development 				(MDSD). It shifts the focus from source code to models, aims at cost reduction, risk 				mitigation, and eases the engineering of complex applications. System models can be 				used in the early development stages to verify certain relevant properties, such as 				performance, before source code is available and problems become hard and costly to 				solve. The present status of Model Driven Engineering (MDE) is still far from this 				ideal situation. A well-known problem is feedback provisioning, which arises when 				different solutions for the same design problem exist. An approach for feedback 				provisioning automation leverages model transformations, which glue together models 				in an MDSD setting, encapsulate the design rationale, and promote knowledge reuse 				and solutions otherwise available only to experienced engineers. In this article we 				present QVTR2, our solution to the feedback problem. 					QVTR2 is an extension of the QVT-Relations language 				with constructs to express design alternatives, their impact on non-functional 				metrics, and how to evaluate them and guide the engineers in the selection of the 				most appropriate solution. We demonstrate the effectiveness of our solution by using 				the QVTR2 engine to perform a modified version of the 				standard UML-to-RDBMS transformation in the 				context of a real e-commerce application, and by showing how we can guide a 				non-expert engineer in the selection of a solution that satisfies given performance 				requirements.},
journal = {Comput. Sci.},
month = feb,
pages = {1–20},
numpages = {20},
keywords = {Model transformations, Model driven software development, Model driven quality prediction, Feedback provisioning}
}

@article{10.1145/3229048,
author = {Zheng, Yongjie and Cu, Cuong and Taylor, Richard N.},
title = {Maintaining Architecture-Implementation Conformance to Support Architecture Centrality: From Single System to Product Line Development},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3229048},
doi = {10.1145/3229048},
abstract = {Architecture-centric development addresses the increasing complexity and variability of software systems by focusing on architectural models, which are generally easier to understand and manipulate than source code. It requires a mechanism that can maintain architecture-implementation conformance during architectural development and evolution. The challenge is twofold. There is an abstraction gap between software architecture and implementation, and both may evolve. Existing approaches are deficient in support for both change mapping and product line architecture. This article presents a novel approach named 1.x-way mapping and its extension, 1.x-line mapping to support architecture-implementation mapping in single system development and in product line development, respectively. They specifically address mapping architecture changes to code, maintaining variability conformance between product line architecture and code, and tracing architectural implementation. We built software tools named xMapper and xLineMapper to realize the two approaches, and conducted case studies with two existing open-source systems to evaluate the approaches. The result shows that our approaches are applicable to the implementation of a real software system and are capable of maintaining architecture-implementation conformance during system evolution.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {8},
numpages = {52},
keywords = {variability conformance, architecture-centric feature traceability, architecture-centric development, architectural evolution, Architecture-implementation mapping}
}

@inproceedings{10.1145/3229345.3229419,
author = {Oliveira, Joyce Aline and Vargas, Matheus and Rodrigues, Roni},
title = {SOA Reuse: Systematic Literature Review Updating and Research Directions},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229419},
doi = {10.1145/3229345.3229419},
abstract = {Service Oriented Architecture (SOA) reuse has been used strategically in organizations to reduce development costs and increase the quality of applications. This article analyzes a systematic literature review in order to identify concepts, goals, strategies, and metrics of SOA reuse. The results show that the main goal of SOA reuse is to decrease development costs. The factor that most negatively influences SOA reuse is the existence of legacy systems. The strategy used most to potentialize SOA reuse is business process management. Metrics proposed by studies to measure SOA reuse are related to modularity and adaptability indicators. The study is relevant because it increases the body of knowledge of the area. Additionally, a set of gaps to be addressed by researchers and reuse practitioners was identified.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {71},
numpages = {8},
keywords = {systematic literature review, Service Oriented Architecture, SOA reuse},
location = {Caxias do Sul, Brazil},
series = {SBSI '18}
}

@inproceedings{10.1145/3426182.3426187,
author = {Kloibhofer, Sebastian and Pointhuber, Thomas and Heisinger, Maximilian and M\"{o}ssenb\"{o}ck, Hanspeter and Stadler, Lukas and Leopoldseder, David},
title = {SymJEx: symbolic execution on the GraalVM},
year = {2020},
isbn = {9781450388535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426182.3426187},
doi = {10.1145/3426182.3426187},
abstract = {Developing software systems is inherently subject to errors that can later cause failures in production. While testing can help to identify critical issues, it is limited to concrete inputs and states. Exhaustive testing is infeasible in practice; hence we can never prove the absence of faults. Symbolic execution, i.e., the process of symbolically reasoning about the program state during execution, can inspect the behavior of a system under all possible concrete inputs at run time. It automatically generates logical constraints that match the program semantics and uses theorem provers to verify the existence of error states within the application. This paper presents a novel symbolic execution engine called SymJEx, implemented on top of the multi-language Java Virtual Machine GraalVM. SymJEx uses the Graal compiler's intermediate representation to derive and evaluate path conditions, allowing GraalVM users to leverage the engine to improve software quality. In this work, we show how SymJEx finds non-trivial faults in existing software systems and compare our approach with established symbolic execution engines.},
booktitle = {Proceedings of the 17th International Conference on Managed Programming Languages and Runtimes},
pages = {63–72},
numpages = {10},
keywords = {Symbolic execution, Java, GraalVM, Compiler optimizations},
location = {Virtual, UK},
series = {MPLR '20}
}

@inproceedings{10.1145/2866614.2866628,
author = {Th\"{u}m, Thomas and Winkelmann, Tim and Schr\"{o}ter, Reimar and Hentschel, Martin and Kr\"{u}ger, Stefan},
title = {Variability Hiding in Contracts for Dependent Software Product Lines},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866628},
doi = {10.1145/2866614.2866628},
abstract = {Software product lines are used to efficiently develop and verify similar software products. While they focus on reuse of artifacts between products, a product line may also be reused itself in other product lines. A challenge with such dependent product lines is evolution; every change in a product line may influence all dependent product lines. With variability hiding, we aim to hide certain features and their artifacts in dependent product lines. In prior work, we focused on feature models and implementation artifacts. We build on this by discussing how variability hiding can be extended to specifications in terms of method contracts. We illustrate variability hiding in contracts by means of a running example and share our insights with preliminary experiments on the benefits for formal verification. In particular, we find that not every change in a certain product line requires a re-verification of other dependent product lines.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {method contracts, deductive verification, Multi product line},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/2745802.2745815,
author = {Zhou, You and Zhang, He and Huang, Xin and Yang, Song and Babar, Muhammad Ali and Tang, Hao},
title = {Quality assessment of systematic reviews in software engineering: a tertiary study},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745815},
doi = {10.1145/2745802.2745815},
abstract = {Context: The quality of an Systematic Literature Review (SLR) is as good as the quality of the reviewed papers. Hence, it is vital to rigorously assess the papers included in an SLR. There has been no tertiary study aimed at reporting the state of the practice of quality assessment used in SLRs in Software Engineering (SE).Objective: We aimed to study the practices of quality assessment of the papers included in SLRs in SE.Method: We conducted a tertiary study of the SLRs that have performed quality assessment of the reviewed papers.Results: We identified and analyzed different aspects of the quality assessment of the papers included in 127 SLRs.Conclusion: Researchers use a variety of strategies for quality assessment of the papers reviewed, but report little about the justification for the used criteria. The focus is creditability but not relevance aspect of the papers. Appropriate guidelines are required for devising quality assessment strategies.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {14},
numpages = {14},
keywords = {systematic (literature) review, software engineering, quality assessment},
location = {Nanjing, China},
series = {EASE '15}
}

@inproceedings{10.1007/978-3-662-45234-9_22,
author = {Johnsen, Einar Broch and Schlatte, Rudolf and Tapia Tarifa, S. Lizeth},
title = {Deployment Variability in Delta-Oriented Models},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_22},
doi = {10.1007/978-3-662-45234-9_22},
abstract = {Software engineering increasingly emphasizes variability by developing families of products for a range of application contexts or user requirements. ABS is a modeling language which supports variability in the formal modeling of software by using feature selection to transform a delta-oriented base model into a concrete product model. ABS also supports deployment models, with a separation of concerns between execution cost and server capacity. This allows the model-based assessment of deployment choices on a product's quality of service. This paper combines deployment models with the variability concepts of ABS, to model deployment choices as features when designing a family of products.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {304–319},
numpages = {16}
}

@inproceedings{10.5555/2486788.2486853,
author = {Sayyad, Abdel Salam and Menzies, Tim and Ammar, Hany},
title = {On the value of user preferences in search-based software engineering: a case study in software product lines},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Software design is a process of trading off competing objectives. If the user objective space is rich, then we should use optimizers that can fully exploit that richness. For example, this study configures software product lines (expressed as feature maps) using various search-based software engineering methods. As we increase the number of optimization objectives, we find that methods in widespread use (e.g. NSGA-II, SPEA2) perform much worse than IBEA (Indicator-Based Evolutionary Algorithm). IBEA works best since it makes most use of user preference knowledge. Hence it does better on the standard measures (hypervolume and spread) but it also generates far more products with 0% violations of domain constraints. Our conclusion is that we need to change our methods for search-based software engineering, particularly when studying complex decision spaces.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {492–501},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1007/s10270-017-0625-6,
author = {Str\"{u}ber, Daniel and Acre?Oaie, Vlad and Pl\"{o}ger, Jennifer},
title = {Model clone detection for rule-based model transformation languages},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0625-6},
doi = {10.1007/s10270-017-0625-6},
abstract = {Cloning is a convenient mechanism to enable reuse across and within software artifacts. On the downside, it is also a practice related to severe long-term maintainability impediments, thus generating a need to identify clones in affected artifacts. A large variety of clone detection techniques have been proposed for programming and modeling languages; yet no specific ones have emerged for model transformation languages. In this paper, we explore clone detection for rule-based model transformation languages, including graph-based ones, such as Henshin, and hybrid ones, such as ATL. We introduce use cases for such techniques in the context of constructive and analytical quality assurance, and a set of key requirements we derived from these use cases. To address these requirements, we describe our customization of existing model clone detection techniques: We consider eScan, an a-priori-based technique, ConQAT, a heuristic technique, and a hybrid technique based on a combination of eScan and ConQAT. We compare these techniques in a comprehensive experimental evaluation, based on three realistic Henshin rule sets, and a comprehensive body of examples from the ATL transformation zoo. Our results indicate that our customization of ConQAT enables the efficient detection of the considered clones, without sacrificing accuracy. With our contributions, we present the first evidence on the usefulness of model clone detection for the quality assurance of model transformations and pave the way for future research efforts at the intersection of model clone detection and model transformation.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {995–1016},
numpages = {22},
keywords = {Quality assurance, Model transformation, Model clone detection, Henshin, ATL}
}

@inproceedings{10.5555/1782814.1782819,
author = {Altintas, N. Ilker and Cetin, Semih and Dogru, Ali H.},
title = {Industrializing software development: the "factory automation" way},
year = {2006},
isbn = {3540759115},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Improving the productivity by means of systematic reuse has been a major challenge particularly for the last decade in software industry. Following the individual techniques like Architecture-Based Development, Model-Driven Development and Software Product Lines, Software Factories have eventually come to the stage as an umbrella solution to software productivity problem by assembling the applications with frameworks, patterns, models and tools. While this theoretically seems quite suitable, it still needs practical guidance at certain points such as defining and orchestrating reusable assets for setting up distinct software factories. This paper proposes a methodical way for such difficulties in establishing software factories as the way other manufacturing industries have been doing for several decades, which is known to be "factory automation". We articulate the "software factory automation" for managing reusable assets across distinct software product lines based on an architecture-driven software factory meta-model and tailoring them to form directly executable software assets.},
booktitle = {Proceedings of the 2nd International Conference on Trends in Enterprise Application Architecture},
pages = {54–68},
numpages = {15},
location = {Berlin, Germany},
series = {TEAA'06}
}

@inproceedings{10.1145/3474624.3476010,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Implementing Search-Based Software Engineering Approaches with Nautilus},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476010},
doi = {10.1145/3474624.3476010},
abstract = {Search-Based Software Engineering (SBSE) approaches adopt search-based techniques to solve Software Engineering (SE) optimization problems. Among these techniques, evolutionary algorithms are the most popular and successfully used, such as multi-objective evolutionary algorithms. However, some challenges still need to be addressed. Firstly, SE problems are complex and commonly impacted by many conflicting factors. In this context, the use of many-objective algorithms is necessary. Secondly, the users very often do not recognise the found solutions as feasible because these solutions are usually not generated considering the users’ needs and preferences. Thus, to deal properly with this situation, preference-based algorithms should be applied. Moreover, there are some practical issues regarding the choice of operators, evaluation of algorithms and visualization of solutions. Existing frameworks do not provide support to address these challenges. To overcome these limitations, we present Nautilus, an open-source Java web-platform tool that works with plugins to ease the addition of new problem instances, implementation of search operators and different multi and many-objective optimization algorithms, guided (or not) by human participation. This paper describes Nautilus-NRP, an extension implemented to address the Next Release Problem (NRP). NRP refers to the selection of requirements to be implemented in the next release of a software and is used to illustrate Nautilus’ main functionalities and how it can be extended to solve a SE problem. Link for the video: https://youtu.be/2dbwslTrvhg.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {303–308},
numpages = {6},
keywords = {preference-based algorithms, next release problem, many-objective optimization},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1016/j.cl.2018.01.003,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {Personalized recommender systems for product-line configuration processes},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.003},
doi = {10.1016/j.cl.2018.01.003},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {451–471},
numpages = {21},
keywords = {Personalized recommendations, Recommender systems, Product-line configuration, Feature model, Product lines}
}

@article{10.1016/j.jss.2012.10.013,
author = {Nakagawa, Elisa Y. and Antonino, Pablo O. and Becker, Martin and Maldonado, Jos\'{e} C. and Storf, Holger and Villela, Karina B. and Rombach, Dieter},
title = {Relevance and perspectives of AAL in Brazil},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.10.013},
doi = {10.1016/j.jss.2012.10.013},
abstract = {Population aging has been taking place in many countries across the globe and more recently in emerging countries. In this context, Ambient Assisted Living (AAL) has become one focus of attention, including methods, products, services, and AAL software systems that support the everyday lives of elderly people, promoting mainly their independence and dignity. From the perspective of computer science, efforts are already being dedicated to adequately developing AAL systems. However, in spite of its relevance, AAL has not been properly investigated in emerging countries, including Brazil. Thus, the contribution of this paper is to present the main perspectives of research in AAL, in particular in the area of software engineering, considering that the Brazilian population is also subject to the aging process. The main intention of this paper is to raise the interest of Brazilian researchers, as well as government and industry, for this important area.},
journal = {J. Syst. Softw.},
month = apr,
pages = {985–996},
numpages = {12},
keywords = {Reference architecture, Population aging, Ambient Assisted Living (AAL), AAL platform}
}

@article{10.1016/j.asoc.2016.08.030,
author = {Saeed, Aneesa and Ab Hamid, Siti Hafizah and Mustafa, Mumtaz Begum},
title = {The experimental applications of search-based techniques for model-based testing},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.030},
doi = {10.1016/j.asoc.2016.08.030},
abstract = {Graphical abstractDisplay Omitted HighlightsA systematic review of applications of search-based techniques for model-based testing is provided.Four taxonomies are proposed to classify the applications based on the purpose, problems, solutions and evaluations.The applications are analyzed based on the proposed taxonomies.The development of search-based techniques for model-based testing is discussed.Limitations and potential research directions are summarized. ContextModel-based testing (MBT) aims to generate executable test cases from behavioral models of software systems. MBT gains interest in industry and academia due to its provision of systematic, automated, and comprehensive testing. Researchers have successfully applied search-based techniques (SBTs) by automating the search for an optimal set of test cases at reasonable cost compared to other more expensive techniques. Thus, there is a recent surge toward the applications of SBTs for MBT because the generated test cases are optimal and have low computational cost. However, successful, future SBTs for MBT applications demand deep insight into its existing experimental applications that underlines stringent issues and challenges, which is lacking in the literature. ObjectiveThe objective of this study is to comprehensively analyze the current state-of-the-art of the experimental applications of SBTs for MBT and present the limitations of the current literature to direct future research. MethodWe conducted a systematic literature review (SLR) using 72 experimental papers from six data sources. We proposed a taxonomy based on the literature to categorize the characteristics of the current applications. ResultsThe results indicate that the majority of the existing applications of SBTs for MBT focus on functional and structural coverage purposes, as opposed to stress testing, regression testing and graphical user interface (GUI) testing. We found research gaps in the existing applications in five areas: applying multi-objective SBTs, proposing hybrid techniques, handling complex constraints, addressing data and requirement-based adequacy criteria, and adapting landscape visualization. Only twelve studies proposed and empirically evaluated the SBTs for complex systems in MBT. ConclusionThis extensive systematic analysis of the existing literature based on the proposed taxonomy enables to assist researchers in exploring the existing research efforts and reveal the limitations that need additional investigation.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1094–1117},
numpages = {24},
keywords = {Test case generation, Taxonomy, Systematic literature review, Software testing, Search-based techniques, Model-based testing}
}

@inproceedings{10.1007/978-3-030-95391-1_37,
author = {Zheng, Xuda and Zhang, Chi and Duan, Keqiang and Wu, Weiguo and Yan, Jie},
title = {SLA: A Cache Algorithm for&nbsp;SSD-SMR Storage System with&nbsp;Minimum RMWs},
year = {2021},
isbn = {978-3-030-95390-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-95391-1_37},
doi = {10.1007/978-3-030-95391-1_37},
abstract = {To satisfy the low-cost and massive data storage requirements imposed by data growth, Shingled Magnetic Recording (SMR) disks are extensively employed in the area of high-density storage. The primary drawback of SMR disks is the write amplification issue caused by sequential write constraints, which is becoming more prominent in the field of non-cold archives. Although a hybrid storage system comprised of Solid State Disks (SSDs) and SMRs may alleviate the aforementioned issue, current SSD cache replacement algorithms are still limited to the management method of Least Recently Used (LRU). The LRU queue, on the other hand, is ineffective in reducing the triggers of Read-Modify-Write (RMW), which is a critical factor for the performance of SMR disks. In this paper, we propose a new SMR Locality-Aware (SLA) algorithm based on a band-based management method. SLA adopts the Dual Locality Compare (DLC) strategy to solve the hit rate reduction problem caused by the traditional band-based management method, as well as the Relatively Clean Band First (RCBF) strategy to further minimize the number of RMW operations. Experiments indicate that, compared to the MSOT method, the SLA algorithm can maintain a similar hit rate as the LRU, while reducing the number of RMWs by 77.2% and the SMR disk write time by 95.1%.},
booktitle = {Algorithms and Architectures for Parallel Processing: 21st International Conference, ICA3PP 2021, Virtual Event, December 3–5, 2021, Proceedings, Part III},
pages = {587–601},
numpages = {15},
keywords = {Spatial locality, Hybrid storage system, Replacement algorithm, Shingled Magnetic Recording}
}

@inproceedings{10.5555/1987684.1987695,
author = {Juszczyk, Lukasz and Schall, Daniel and Mietzner, Ralph and Dustdar, Schahram and Leymann, Frank},
title = {CAGE: customizable large-scale SOA testbeds in the cloud},
year = {2010},
isbn = {9783642193934},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Large-scale and complex distributed systems are increasingly implemented as SOAs. These comprise diverse types of components, e.g., Web services, registries, workflow engines, and services buses, that interact with each others to establish composite functionality. The drawback of this trend is that testing of complex SOAs becomes a challenging task. During the development phase, testers must verify the system's correct functionality, but often do not have access to adequate testbeds. In this paper, we present an approach for solving this issue. We combine the Genesis2 testbed generator, that emulates SOA environments, with Cafe, a framework for provisioning of component-based applications in the cloud. Our approach allows to model large-scale service-based testbed infrastructures, to specify their behavior, and to deploy these automatically in the cloud. As a result, testers can emulate required environments on-demand for evaluating SOAs at runtime.},
booktitle = {Proceedings of the 2010 International Conference on Service-Oriented Computing},
pages = {76–87},
numpages = {12},
location = {San Francisco, CA},
series = {ICSOC'10}
}

@article{10.1145/3487921,
author = {Hezavehi, Sara M. and Weyns, Danny and Avgeriou, Paris and Calinescu, Radu and Mirandola, Raffaela and Perez-Palacin, Diego},
title = {Uncertainty in Self-adaptive Systems: A Research Community Perspective},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3487921},
doi = {10.1145/3487921},
abstract = {One of the primary drivers for self-adaptation is ensuring that systems achieve their goals regardless of the uncertainties they face during operation. Nevertheless, the concept of uncertainty in self-adaptive systems is still insufficiently understood. Several taxonomies of uncertainty have been proposed, and a substantial body of work exists on methods to tame uncertainty. Yet, these taxonomies and methods do not fully convey the research community’s perception on what constitutes uncertainty in self-adaptive systems and on the key characteristics of the approaches needed to tackle uncertainty. To understand this perception and learn from it, we conducted a survey comprising two complementary stages in which we collected the views of 54 and 51 participants, respectively. In the first stage, we focused on current research and development, exploring how the concept of uncertainty is understood in the community and how uncertainty is currently handled in the engineering of self-adaptive systems. In the second stage, we focused on directions for future research to identify potential approaches to dealing with unanticipated changes and other open challenges in handling uncertainty in self-adaptive systems. The key findings of the first stage are: (a) an overview of uncertainty sources considered in self-adaptive systems, (b) an overview of existing methods used to tackle uncertainty in concrete applications, (c) insights into the impact of uncertainty on non-functional requirements, (d) insights into different opinions in the perception of uncertainty within the community and the need for standardised uncertainty-handling processes to facilitate uncertainty management in self-adaptive systems. The key findings of the second stage are: (a) the insight that over 70% of the participants believe that self-adaptive systems can be engineered to cope with unanticipated change, (b) a set of potential approaches for dealing with unanticipated change, (c) a set of open challenges in mitigating uncertainty in self-adaptive systems, in particular in those with safety-critical requirements. From these findings, we outline an initial reference process to manage uncertainty in self-adaptive systems. We anticipate that the insights on uncertainty obtained from the community and our proposed reference process will inspire valuable future research on self-adaptive systems.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = dec,
articleno = {10},
numpages = {36},
keywords = {survey, uncertainty challenges, unanticipated change, uncertainty methods, uncertainty models, uncertainty, Self-adaptation}
}

@inproceedings{10.1145/3377024.3377036,
author = {Sprey, Joshua and Sundermann, Chico and Krieter, Sebastian and Nieke, Michael and Mauro, Jacopo and Th\"{u}m, Thomas and Schaefer, Ina},
title = {SMT-based variability analyses in FeatureIDE},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377036},
doi = {10.1145/3377024.3377036},
abstract = {Handling configurable systems with thousands of configuration options is a challenging problem in research and industry. One of the most common approaches to manage the configuration options of large systems is variability modelling. The verification and configuration process of large variability models is manually infeasible. Hence, they are usually assisted by automated analyses based on solving satisfiability problems (SAT). Recent advances in satisfiability modulo theories (SMT) could prove SMT solvers as a viable alternative to SAT solvers. However, SMT solvers are typically not utilized for variability analyses. A comparison for SAT and SMT could help to estimate SMT solvers potential for the automated analysis. We integrated two SMT solvers into FeatureIDE and compared them against a SAT solver on analyses for feature models, configurations, and realization artifacts. We give an overview of all variability analyses in FeatureIDE and present the results of our empirical evaluation for over 122 systems. We observed that SMT solvers are generally faster in generating explanations of unsatisfiable requests. However, the evaluated SAT solver outperformed SMT solvers for other analyses.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {9},
keywords = {variability analysis, preprocessor analysis, feature models, feature model analysis, feature attributes, configuration analysis, attribute optimization, SMT analysis, SMT, SAT vs SMT, SAT analysis, SAT},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1145/2088883.2088900,
author = {Tekinerdogan, Bedir and Cetin, Semih and Babar, Muhammad Ali and Lago, Patricia and M\"{a}ki\"{o}, Juho},
title = {Architecting in global software engineering},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2088883.2088900},
doi = {10.1145/2088883.2088900},
abstract = {This paper summarizes the results of the First Workshop on Arc-hitecting in Global Software Engineering (GSE), which was or-ganized in conjunction with the 6th International Conference on Global Software Engineering (ICGSE 2011). The workshop aimed to bring together researchers and practitioners for defining and advancing the state-of-the-art and state-of-the practice in architecture design of global software development systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–7},
numpages = {7},
keywords = {workshop, software architecture, global software engineering}
}

@article{10.1016/j.infsof.2012.09.011,
author = {Galster, Matthias and Avgeriou, Paris and Tofan, Dan},
title = {Constraints for the design of variability-intensive service-oriented reference architectures - An industrial case study},
year = {2013},
issue_date = {February, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.09.011},
doi = {10.1016/j.infsof.2012.09.011},
abstract = {Context: Service-oriented architecture has become a widely used concept in software industry. However, we currently lack support for designing variability-intensive service-oriented systems. Such systems could be used in different environments, without the need to design them from scratch. To support the design of variability-intensive service-oriented systems, reference architectures that facilitate variability in instantiated service-oriented architectures can help. Objective: The design of variability-intensive service-oriented reference architectures is subject to specific constraints. Architects need to know these constraints when designing such reference architectures. Our objective is to identify these constraints. Method: An exploratory case study was performed in the context of local e-government in the Netherlands to study constraints from the perspective of (a) the users of a variability-intensive service-oriented system (municipalities that implement national laws), and (b) the implementing organizations (software vendors). We collected data through interviews with representatives from five organizations, document analyses and expert meetings. Results: We identified ten constraints (e.g., organizational constraints, integration-related constraints) which affect the process of designing reference architectures for variability-intensive service-oriented systems. Also, we identified how stakeholders are affected by these constraints, and how constraints are specific to the case study domain. Conclusions: Our results help design variability-intensive service-oriented reference architectures. Furthermore, our results can be used to define processes to design such reference architectures.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {428–441},
numpages = {14},
keywords = {e-Government, Variability, Service-oriented architecture, SOA, Reference architectures, Case study}
}

@inproceedings{10.1145/2593770.2593781,
author = {Ramaswamy, Arunkumar and Monsuez, Bruno and Tapus, Adriana},
title = {Model-driven software development approaches in robotics research},
year = {2014},
isbn = {9781450328494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593770.2593781},
doi = {10.1145/2593770.2593781},
abstract = {Recently, there is an encouraging trend in adopting model-driven engineering approaches for software development in robotics research. In this paper, currently available model-driven techniques in robotics are analyzed with respect to the domain-specific requirements. A conceptual overview of our software development approach called 'Self Adaptive Framework for Robotic Systems (SafeRobots)' is explained and we also try to position our approach within this model ecosystem.},
booktitle = {Proceedings of the 6th International Workshop on Modeling in Software Engineering},
pages = {43–48},
numpages = {6},
keywords = {Robotics, Model-driven software development},
location = {Hyderabad, India},
series = {MiSE 2014}
}

@inproceedings{10.1145/3023956.3023959,
author = {Ochoa, Lina and Pereira, Juliana Alves and Gonz\'{a}lez-Rojas, Oscar and Castro, Harold and Saake, Gunter},
title = {A survey on scalability and performance concerns in extended product lines configuration},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023959},
doi = {10.1145/3023956.3023959},
abstract = {Product lines have been employed as a mass customisation method that reduces production costs and time-to-market. Multiple product variants are represented in a product line, however the selection of a particular configuration depends on stakeholders' functional and non-functional requirements. Methods like constraint programming and evolutionary algorithms have been used to support the configuration process. They consider a set of product requirements like resource constraints, stakeholders' preferences, and optimization objectives. Nevertheless, scalability and performance concerns start to be an issue when facing large-scale product lines and runtime environments. Thus, this paper presents a survey that analyses strengths and drawbacks of 21 approaches that support product line configuration. This survey aims to: i) evidence which product requirements are currently supported by studied methods; ii) how scalability and performance is considered in existing approaches; and iii) point out some challenges to be addressed in future research.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {5–12},
numpages = {8},
keywords = {survey, scalability, product requirements, product line, performance, literature review, configuration},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1007/978-3-319-31854-7_74,
author = {Su, Jian and Han, Lu and Li, Yue and Liu, Shufen and Yao, Zhilin and Bao, Tie},
title = {Research on Building and Analysis for Attribute Model in Quality Evaluation of Domain Software},
year = {2016},
isbn = {9783319318530},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-31854-7_74},
doi = {10.1007/978-3-319-31854-7_74},
abstract = {This paper looks into how to select attributes for domain software quality evaluation and put forward a method to build the attribute model of domain software. It also conducts an analysis into the correlation between the change in attributes and the change in software quality evaluation based on the model. Domain software pays more attention to domain features of the software. Based on quality evaluation need, a structured attribute model was built by selection of general, domain and application attributes. Based on the attribute model, an analysis on the impact of change in attributes on software quality evaluation can be conducted. At last, a case is used to demonstrate the process of analyzing how the change in the weight of a single attribute would impact on software quality evaluation. The building method of attribute model put forward in this paper focuses on general and domain attributes of domain software meanwhile looks into the trend of software quality evaluation variation based on the attribute model, thus provides better support for domain software quality evaluation.},
booktitle = {Revised Selected Papers of the Second International Conference on Human Centered Computing - Volume 9567},
pages = {752–758},
numpages = {7},
keywords = {Software engineering, Quality evaluation, Impact analysis, Attribute model},
location = {Colombo, Sri Lanka},
series = {HCC 2016}
}

@article{10.1016/j.micpro.2019.05.013,
author = {Pomante, Luigi and Muttillo, Vittoriano and K\v{r}ena, Bohuslav and Vojnar, Tom\'{a}\v{s} and Veljkovi\'{c}, Filip and Magnin, Pac\^{o}me and Matschnig, Martin and Fischer, Bernhard and Martinez, Jabier and Gruber, Thomas},
title = {The AQUAS ECSEL Project Aggregated Quality Assurance for Systems: Co-Engineering Inside and Across the Product Life Cycle},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {69},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2019.05.013},
doi = {10.1016/j.micpro.2019.05.013},
journal = {Microprocess. Microsyst.},
month = sep,
pages = {54–67},
numpages = {14},
keywords = {Product life-cycle, Co-engineering, Performance, Security, Safety, Cyber-physical systems}
}

@article{10.1016/j.jss.2015.08.054,
author = {Capilla, Rafael and Jansen, Anton and Tang, Antony and Avgeriou, Paris and Babar, Muhammad Ali},
title = {10 years of software architecture knowledge management},
year = {2016},
issue_date = {June 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.08.054},
doi = {10.1016/j.jss.2015.08.054},
abstract = {A retrospective analysis of state of the art of AKM.AKM practice from models to tools around three different generations.Our results of AK practice in industry, barriers and remedies.Use of AK in different software development contexts.A comparison of extended capabilities of AKM tools. The importance of architectural knowledge (AK) management for software development has been highlighted over the past ten years, where a significant amount of research has been done. Since the first systems using design rationale in the seventies and eighties to the more modern approaches using AK for designing software architectures, a variety of models, approaches, and research tools have leveraged the interests of researchers and practitioners in AK management (AKM). Capturing, sharing, and using AK has many benefits for software designers and maintainers, but the cost to capture this relevant knowledge hampers a widespread use by software companies. However, as the improvements made over the last decade didn't boost a wider adoption of AKM approaches, there is a need to identify the successes and shortcomings of current AK approaches and know what industry needs from AK. Therefore, as researchers and promoters of many of the AK research tools in the early stages where AK became relevant for the software architecture community, and based on our experience and observations, we provide in this research an informal retrospective analysis of what has been done and the challenges and trends for a future research agenda to promote AK use in modern software development practices.},
journal = {J. Syst. Softw.},
month = jun,
pages = {191–205},
numpages = {15},
keywords = {Architectural knowledge management, Architectural design decisions, Agile development}
}

@inproceedings{10.1145/3168365.3168372,
author = {Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc and Galindo, Jos\'{e} A. and Martinez, Jabier and Ziadi, Tewfik},
title = {VaryLATEX: Learning Paper Variants That Meet Constraints},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168372},
doi = {10.1145/3168365.3168372},
abstract = {How to submit a research paper, a technical report, a grant proposal, or a curriculum vitae that respect imposed constraints such as formatting instructions and page limits? It is a challenging task, especially when coping with time pressure. In this work, we present VaryLATEX, a solution based on variability, constraint programming, and machine learning techniques for documents written in LATEX to meet constraints and deliver on time. Users simply have to annotate LATEX source files with variability information, e.g., (de)activating portions of text, tuning figures' sizes, or tweaking line spacing. Then, a fully automated procedure learns constraints among Boolean and numerical values for avoiding non-acceptable paper variants, and finally, users can further configure their papers (e.g., aesthetic considerations) or pick a (random) paper variant that meets constraints, e.g., page limits. We describe our implementation and report the results of two experiences with VaryLATEX.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {83–88},
numpages = {6},
keywords = {variability modelling, technical writing, machine learning, generators, constraint programming, LATEX},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2361999.2362029,
author = {Kabbedijk, J. and Jansen, S.},
title = {The role of variability patterns in multi-tenant business software},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362029},
doi = {10.1145/2361999.2362029},
abstract = {Within the business software domain it is crucial for a software vendor to comply to different customer requirements. Traditionally this could be done by offering different products to different customers, but because multi-tenant business software deployments use one software product to serve all customers, this is no longer possible. Software vendors have to make sure that one instance of a software product is variable enough to support all different requirements from all different customers. This ability is defined as tenant-based variability.Within this paper a conceptual model is presented, explaining the role software patterns play in solving variability implementation problems in multi-tenant business software. Different important aspects of patterns are explained, like forces and consequences and are linked to concepts in the problem domain. The paper suggests that variability patterns play a large role in addressing variability in multi-tenant business software and provide a valuable vocabulary for researching, reporting, thinking and communicating about variability solutions in online software products.},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {143–146},
numpages = {4},
keywords = {variability, software patterns, online business software, multi-tenancy, SaaS},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@inproceedings{10.1007/978-3-642-29645-1_22,
author = {Mussbacher, Gunter and Al Abed, Wisam and Alam, Omar and Ali, Shaukat and Beugnard, Antoine and Bonnet, Valentin and Br\ae{}k, Rolv and Capozucca, Alfredo and Cheng, Betty H. C. and Fatima, Urooj and France, Robert and Georg, Geri and Guelfi, Nicolas and Istoan, Paul and J\'{e}z\'{e}quel, Jean-Marc and Kienzle, J\"{o}rg and Klein, Jacques and L\'{e}zoray, Jean-Baptiste and Malakuti, Somayeh and Moreira, Ana and Phung-Khac, An and Troup, Lucy},
title = {Comparing six modeling approaches},
year = {2011},
isbn = {9783642296444},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-29645-1_22},
doi = {10.1007/978-3-642-29645-1_22},
abstract = {While there are many aspect-oriented modeling (AOM) approaches, from requirements to low-level design, it is still difficult to compare them and know under which conditions different approaches are most applicable. This comparison, however, is crucially important to unify existing AOM and more traditional object-oriented modeling (OOM) approaches and to generalize individual approaches into a comprehensive end-to-end method. Such a method does not yet exist. This paper reports on work done at the inaugural Comparing Modeling Approaches (CMA) workshop towards the goal of identifying potential comprehensive methodologies: (i) a common, focused case study for six modeling approaches, (ii) a set of criteria applied to each of the six approaches, and (iii) the assessment results.},
booktitle = {Proceedings of the 2011th International Conference on Models in Software Engineering},
pages = {217–243},
numpages = {27},
keywords = {object-oriented modeling, comparison criteria, case study, aspect-oriented modeling},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@inproceedings{10.1007/978-3-030-59003-1_10,
author = {G\'{o}mez, Paola and Casallas, Rubby and Roncancio, Claudia},
title = {Automatic Schema Generation for Document-Oriented Systems},
year = {2020},
isbn = {978-3-030-59002-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59003-1_10},
doi = {10.1007/978-3-030-59003-1_10},
abstract = {Popular document-oriented systems store JSON-like data (e.g. MongoDB). Such data formats combine the flexibility of semi-structured models and traditional data structures like records and arrays. This allows numerous structuring possibilities even for simple data. The data structure choice is important as it impacts many aspects such as memory footprint, data access performances and programming complexity. Our work aims at helping users in selecting data structuring from a set of automatically generated alternatives. These alternatives can be analyzed considering complexity metrics, query requirements and best practices using such “schemaless” databases. Our approach for “schema” generation has been inspired from Software Product Lines strategies based on feature models. From a UML class diagram that represents user’s data, we generate automatically a feature model that implicitly contains the structure alternatives with their variations and common points. This feature model satisfies document-oriented constraints so as user constraints reflecting good practices or particular needs. It leads to a set of data structuring alternatives to be considered by the user for his operational choices.},
booktitle = {Database and Expert Systems Applications: 31st International Conference, DEXA 2020, Bratislava, Slovakia, September 14–17, 2020, Proceedings, Part I},
pages = {152–163},
numpages = {12},
keywords = {NoSQL, Document-oriented systems, Variability, Feature models},
location = {Bratislava, Slovakia}
}

@article{10.1016/j.infsof.2017.03.004,
author = {Dey, Sangeeta and Lee, Seok-Won},
title = {REASSURE},
year = {2017},
issue_date = {July 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {87},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.03.004},
doi = {10.1016/j.infsof.2017.03.004},
abstract = {ContextSocio-technical systems are expected to understand the dynamics of the execution environment and behave accordingly. Significant work has been done on formalizing and modeling requirements of such adaptive systems. However, not enough attention is paid on eliciting requirements from users and introducing flexibility in the system behavior at an early phase of requirements engineering. Most of the work is based on an assumption that general users cognitive level would be able to support the inherent complexity of variability acquisition. ObjectiveOur main focus is on providing help to the users with ordinary cognitive level to express their expectations from the complex system considering various contexts. This work also helps the designers to explore the design variability based on the general users preferences. MethodWe explore the idea of using a cognitive technique Repertory Grid (RG) to acquire knowledge from users and experts along multiple dimensions of problem and design space. We propose REASSURE methodology which guides requirements engineers to explore the intentional and design variability in an organized way. We also provide a tool support to analyze the knowledge captured in multiple repertory grid files and detect potential conflicts in the intentional variability. Finally, we evaluate the proposed idea by performing an empirical study using smart home system domain. ResultsThe result of our study shows that a greater number of requirements can be elicited after applying our approach. With the help of the provided tool support, it is even possible to detect a greater number of conflicts in users requirements than the traditional practices. ConclusionWe envision RG as a technique to filter design options based on the intentional variability in various contexts. The promising results of empirical study open up new research questions: how to elicit requirements from multiple stakeholders and reach consensus for multi-dimensional problem domain.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {160–179},
numpages = {20},
keywords = {Socio-technical systems, Requirements elicitation, Repertory grid, Adaptive systems}
}

@article{10.1016/j.infsof.2017.02.002,
author = {Pessoa, Leonardo and Fernandes, Paula and Castro, Thiago and Alves, Vander and Rodrigues, Genana N. and Carvalho, Hervaldo},
title = {Building reliable and maintainable Dynamic Software Product Lines},
year = {2017},
issue_date = {June 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {86},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.02.002},
doi = {10.1016/j.infsof.2017.02.002},
abstract = {Context: Dependability is a key requirement, especially in safety-critical applications. Many of these applications have changing context and configurations at runtime to achieve functional and quality goals and can be realized as Dynamic Software Product Lines (DSPLs). DSPL constitutes an emerging but promising research area. Nevertheless, ensuring dependability in DSPLs remains insufficiently explored, especially in terms of reliability and maintainability. This compromises quality assurance and applicability of DSPLs in safety-critical domains, such as Body Sensor Network (BSN).Objective: To address this issue, we propose an approach to developing reliable and maintainable DSPLs in the context of the BSN domain.Method: Adaptation plans are instances of a Domain Specific Language (DSL) describing reliability goals and adaptability at runtime. These instances are automatically checked for reliability goal satisfiability before being deployed and interpreted at runtime to provide more suitable adaptation goals complying with evolving needs perceived by a domain specialist.Results: The approach is evaluated in the BSN domain. Results show that reliability and maintainability could be provided with execution and reconfiguration times of around 30ms, notification and adaptation plan update time over the network around 5s, and space consumption around 5 MB.Conclusion: The method is feasible at reasonable cost. The incurred benefits are reliable vital signal monitoring for the patientthus providing early detection of serious health issues and the possibility of proactive treatmentand a maintainable infrastructure allowing medical DSL instance update to suit the needs of the domain specialist and ultimately of the patient.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {54–70},
numpages = {17},
keywords = {Reliability, Maintainability, Dynamic Software Product Lines, Context-awareness, Body Sensor Network, Adaptiveness}
}

@inproceedings{10.1145/3084226.3084253,
author = {Abrah\~{a}o, Silvia and Insfran, Emilio},
title = {Evaluating Software Architecture Evaluation Methods: An Internal Replication},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084253},
doi = {10.1145/3084226.3084253},
abstract = {Context: The size and complexity of software systems along with the demand for ensuring quality requirements have fostered the interest in software architecture evaluation methods. Although several empirical studies have been reported, the actual body of knowledge is still insufficient. To address this concern, we presented a family of four controlled experiments that compares a recently proposed method, the Quality-Driven Architecture Derivation and Improvement (QuaDAI) method against the well-known Architecture Tradeoff Analysis Method (ATAM).Objective: To provide further evidence on the efficiency, effectiveness, and perceived satisfaction of participants using these two software architecture evaluation methods. We report the results of a differentiated internal replication study.Method: The same materials used in the baseline experiments were employed in this replication but the participants were sixteen practitioners. In addition, we used a simpler design to reduce the treatments' application sequences.Results: The participants obtained architectures with better quality when applying QuaDAI, and they found this method to be more useful and likely to be used than ATAM, but no difference in terms of efficiency and perceived ease of use were found.Conclusions: The results are in line with the baseline experiments and support the hypothesis that QuaDAI achieve better results than ATAM when performing architectural evaluations; however, further work is need to improve the methods usability.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {144–153},
numpages = {10},
keywords = {Software Architecture Evaluation, Experiment Replication},
location = {Karlskrona, Sweden},
series = {EASE '17}
}

@inproceedings{10.5555/1768904.1768924,
author = {Regnell, Bj\"{o}rn and H\"{o}st, Martin and Svensson, Richard Berntsson},
title = {A quality performance model for cost-benefit analysis of non-functional requirements applied to the mobile handset domain},
year = {2007},
isbn = {9783540730309},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In market-driven requirements engineering for platform-based development of embedded systems such as mobile phones, it is crucial to market success to find the right balance among competing quality aspects (aka nonfunctional requirements). This paper presents a conceptual model that incorporates quality as a dimension in addition to the cost and value dimensions used in prioritisation approaches for functional requirements. The model aims at supporting discussion and decision-making in early requirements engineering related to activities such as roadmapping, release planning and platform scoping. The feasibility and relevance of the model is initially validated through interviews with requirements experts in six cases that represent important areas in the mobile handset domain. The validation suggests that the model is relevant and feasible for this particular domain.},
booktitle = {Proceedings of the 13th International Working Conference on Requirements Engineering: Foundation for Software Quality},
pages = {277–291},
numpages = {15},
location = {Trondheim, Norway},
series = {REFSQ'07}
}

@article{10.1016/j.jss.2012.04.079,
author = {Peng, Xin and Chen, Bihuan and Yu, Yijun and Zhao, Wenyun},
title = {Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop},
year = {2012},
issue_date = {December, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.04.079},
doi = {10.1016/j.jss.2012.04.079},
abstract = {Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2707–2719},
numpages = {13},
keywords = {Self-tuning, Preference, Goal-oriented reasoning, Feedback control theory, Earned business value}
}

@article{10.1016/j.jss.2009.06.051,
author = {Bosch, Jan and Bosch-Sijtsema, Petra},
title = {From integration to composition: On the impact of software product lines, global development and ecosystems},
year = {2010},
issue_date = {January, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.051},
doi = {10.1016/j.jss.2009.06.051},
abstract = {Three trends accelerate the increase in complexity of large-scale software development, i.e. software product lines, global development and software ecosystems. For the case study companies we studied, these trends caused several problems, which are organized around architecture, process and organization, and the problems are related to the efficiency and effectiveness of software development as these companies used too integration-centric approaches. We present five approaches to software development, organized from integration-centric to composition-oriented and describe the areas of applicability.},
journal = {J. Syst. Softw.},
month = jan,
pages = {67–76},
numpages = {10},
keywords = {Software product lines, Software integration, Software ecosystems, Software composition, Global development}
}

@inproceedings{10.1145/2889160.2889273,
author = {da Silva Sousa, Leonardo},
title = {Spotting design problems with smell agglomerations},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889273},
doi = {10.1145/2889160.2889273},
abstract = {Design problems are structures that indicate violations of key design principles or rules. The main difficulty to identify them in the source code is due to the fact they are scattered through several code elements. Thus, code smells - microstructures in the program - have been used to reveal surface indications of a design problem. However, individually, each code smell represents only a partial embodiment of a design problem. Since design problem is scattered through several program elements, we are investigating a strategy to select a group of code smells that is likely to help developers to find design problems. We call agglomeration this group of code smells. Our main goal is to summarize smell agglomerations that better indicate the occurrence of design problems. Our strategy to derive agglomerations is based on capturing semantic relations among closely-related code smells. We will assess to what extent code smell agglomerations help developers to locate and prioritize design problems.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {863–866},
numpages = {4},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.5555/2041790.2041822,
author = {Acher, Mathieu and Cleve, Anthony and Collet, Philippe and Merle, Philippe and Duchien, Laurence and Lahire, Philippe},
title = {Reverse engineering architectural feature models},
year = {2011},
isbn = {9783642237973},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Reverse engineering the variability of an existing system is a challenging activity. The architect knowledge is essential to identify variation points and explicit constraints between features, for instance in feature models (FMs), but the manual creation of FMs is both time-consuming and error-prone. On a large scale, it is very difficult for an architect to guarantee that the resulting FM is consistent with the architecture it is associated with. In this paper, we present a comprehensive, tool supported process for reverse engineering architectural FMs. We develop automated techniques to extract and combine different variability descriptions of an architecture. Then, alignment and reasoning techniques are applied to integrate the architect knowledge and reinforce the extracted FM. We illustrate the process when applied to a representative software system and we report on our experience in this context.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture},
pages = {220–235},
numpages = {16},
location = {Essen, Germany},
series = {ECSA'11}
}

@article{10.1016/j.infsof.2011.06.002,
author = {Breivold, Hongyu Pei and Crnkovic, Ivica and Larsson, Magnus},
title = {A systematic review of software architecture evolution research},
year = {2012},
issue_date = {January, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.06.002},
doi = {10.1016/j.infsof.2011.06.002},
abstract = {Context: Software evolvability describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for making strategic decisions, and increasing economic value of software. For long-lived systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. For this reason, many research studies have been proposed in this area both by researchers and industry practitioners. These studies comprise a spectrum of particular techniques and practices, covering various activities in software lifecycle. However, no systematic review has been conducted previously to provide an extensive overview of software architecture evolvability research. Objective: In this work, we present such a systematic review of architecting for software evolvability. The objective of this review is to obtain an overview of the existing approaches in analyzing and improving software evolvability at architectural level, and investigate impacts on research and practice. Method: The identification of the primary studies in this review was based on a pre-defined search strategy and a multi-step selection process. Results: Based on research topics in these studies, we have identified five main categories of themes: (i) techniques supporting quality consideration during software architecture design, (ii) architectural quality evaluation, (iii) economic valuation, (iv) architectural knowledge management, and (v) modeling techniques. A comprehensive overview of these categories and related studies is presented. Conclusion: The findings of this review also reveal suggestions for further research and practice, such as (i) it is necessary to establish a theoretical foundation for software evolution research due to the fact that the expertise in this area is still built on the basis of case studies instead of generalized knowledge; (ii) it is necessary to combine appropriate techniques to address the multifaceted perspectives of software evolvability due to the fact that each technique has its specific focus and context for which it is appropriate in the entire software lifecycle.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {16–40},
numpages = {25},
keywords = {Systematic review, Software evolvability, Software architecture, Evolvability analysis, Architecture evolution, Architecture analysis}
}

@inproceedings{10.1145/2576768.2598305,
author = {Lopez-Herrejon, Roberto Erick and Javier Ferrer, Javier and Chicano, Francisco and Haslinger, Evelyn Nicole and Egyed, Alexander and Alba, Enrique},
title = {A parallel evolutionary algorithm for prioritized pairwise testing of software product lines},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598305},
doi = {10.1145/2576768.2598305},
abstract = {Software Product Lines (SPLs) are families of related software systems, which provide different feature combinations. Different SPL testing approaches have been proposed. However, despite the extensive and successful use of evolutionary computation techniques for software testing, their application to SPL testing remains largely unexplored. In this paper we present the Parallel Prioritized product line Genetic Solver (PPGS), a parallel genetic algorithm for the generation of prioritized pairwise testing suites for SPLs. We perform an extensive and comprehensive analysis of PPGS with 235 feature models from a wide range of number of features and products, using 3 different priority assignment schemes and 5 product prioritization selection strategies. We also compare PPGS with the greedy algorithm prioritized-ICPL. Our study reveals that overall PPGS obtains smaller covering arrays with an acceptable performance difference with prioritized-ICPL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1255–1262},
numpages = {8},
keywords = {software product lines, pairwise testing, feature models, combinatorial interaction testing},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@article{10.1007/s00450-014-0273-9,
author = {Goltz, Ursula and Reussner, Ralf H. and Goedicke, Michael and Hasselbring, Wilhelm and M\"{a}rtin, Lukas and Vogel-Heuser, Birgit},
title = {Design for future: managed software evolution},
year = {2015},
issue_date = {August    2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {3–4},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-014-0273-9},
doi = {10.1007/s00450-014-0273-9},
abstract = {Innovative software engineering methodologies, concepts and tools which focus on supporting the ongoing evolution of complex software, in particular regarding its continuous adaptation to changing functional and quality requirements as well as platforms over a long period are required. Supporting such a co-evolution of software systems along with their environment represents a very challenging undertaking, as it requires a combination or even integration of approaches and insights from different software engineering disciplines. To meet these challenges, the Priority Programme 1593 Design for Future--Managed Software Evolution has been established, funded by the German Research Foundation, to develop fundamental methodologies and a focused approach for long-living software systems, maintaining high quality and supporting evolution during the whole life cycle. The goal of the priority programme is integrated and focused research in software engineering to develop methods for the continuous evolution of software and software/hardware systems for making systems adaptable to changing requirements and environments. For evaluation, we focus on two specific application domains: information systems and production systems in automation engineering. In particular two joint case studies from these application domains promote close collaborations among the individual projects of the priority programme. We consider several research topics that are of common interest, for instance co-evolution of models and implementation code, of models and tests, and among various types of models. Another research topic of common interest are run-time models to automatically synchronise software systems with their abstract models through continuous system monitoring. Both concepts, co-evolution and run-time models contribute to our vision to which we refer to as knowledge carrying software. We consider this as a major need for a long life of such software systems.},
journal = {Comput. Sci.},
month = aug,
pages = {321–331},
numpages = {11},
keywords = {maintenance and operation, Software life cycle, Legacy systems, Knowledge carrying software, Design, Co-evolution}
}

@inproceedings{10.5555/3297765.3297769,
author = {Trapp, Matthias and Pasewaldt, Sebastian and D\"{u}rschmid, Tobias and Semmo, Amir and D\"{o}llner, J\"{u}rgen},
title = {Teaching image-processing programming for mobile devices: a software development perspective},
year = {2018},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {In this paper we present a concept of a research course that teaches students in image processing as a building block of mobile applications. Our goal with this course is to teach theoretical foundations, practical skills in software development as well as scientific working principles to qualify graduates to start as fully-valued software developers or researchers. The course includes teaching and learning focused on the nature of small team research and development as encountered in the creative industries dealing with computer graphics, computer animation and game development. We discuss our curriculum design and issues in conducting undergraduate and graduate research that we have identified through four iterations of the course. Joint scientific demonstrations and publications of the students and their supervisors as well as quantitative and qualitative evaluation by students underline the success of the proposed concept. In particular, we observed that developing using a common software framework helps the students to jump start their course projects, while industry software processes such as branching coupled with a three-tier breakdown of project features helps them to structure and assess their progress.},
booktitle = {Proceedings of the 39th Annual European Association for Computer Graphics Conference: Education Papers},
pages = {17–24},
numpages = {8},
location = {Delft, The Netherlands},
series = {EG-EDU '18}
}

@article{10.1007/s10270-018-00712-x,
author = {Bencomo, Nelly and G\"{o}tz, Sebastian and Song, Hui},
title = {Models@run.time: a guided tour of the state of the art and research challenges},
year = {2019},
issue_date = {October   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-00712-x},
doi = {10.1007/s10270-018-00712-x},
abstract = {More than a decade ago, the research topic models@run.time was coined. Since then, the research area has received increasing attention. Given the prolific results during these years, the current outcomes need to be sorted and classified. Furthermore, many gaps need to be categorized in order to further develop the research topic by experts of the research area but also newcomers. Accordingly, the paper discusses the principles and requirements of models@run.time and the state of the art of the research line. To make the discussion more concrete, a taxonomy is defined and used to compare the main approaches and research outcomes in the area during the last decade and including ancestor research initiatives. We identified and classified 275 papers on models@run.time, which allowed us to identify the underlying research gaps and to elaborate on the corresponding research challenges. Finally, we also facilitate sustainability of the survey over time by offering tool support to add, correct and visualize data.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {3049–3082},
numpages = {34},
keywords = {Systematic literature review, Self-reflection, Models@run.time, Causal connection}
}

@article{10.1007/s10009-012-0253-y,
author = {Schaefer, Ina and Rabiser, Rick and Clarke, Dave and Bettini, Lorenzo and Benavides, David and Botterweck, Goetz and Pathak, Animesh and Trujillo, Salvador and Villela, Karina},
title = {Software diversity: state of the art and perspectives},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-012-0253-y},
doi = {10.1007/s10009-012-0253-y},
abstract = {Diversity is prevalent in modern software systems to facilitate adapting the software to customer requirements or the execution environment. Diversity has an impact on all phases of the software development process. Appropriate means and organizational structures are required to deal with the additional complexity introduced by software variability. This introductory article to the special section "Software Diversity--Modeling, Analysis and Evolution" provides an overview of the current state of the art in diverse systems development and discusses challenges and potential solutions. The article covers requirements analysis, design, implementation, verification and validation, maintenance and evolution as well as organizational aspects. It also provides an overview of the articles which are part of this special section and addresses particular issues of diverse systems development.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {477–495},
numpages = {19},
keywords = {Variability, Software product lines, Software diversity}
}

@inproceedings{10.1145/2031759.2031771,
author = {Dobrica, Liliana and Ovaska, Eila},
title = {Analysis of a cross-domain reference architecture using change scenarios},
year = {2011},
isbn = {9781450306188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2031759.2031771},
doi = {10.1145/2031759.2031771},
abstract = {The content of this paper addresses the issue of how to perform analysis of a cross domain reference architecture. The cross domain reference architecture is designed based on the domains requirements and features modeling. The definition of a cross domain reference architecture is based on well known concepts from software architecture description, service orientation and product line. We apply a method based on change scenarios to analyze variability at the architectural level. In order to handle complexity in analysis we propose categories of change scenarios to be derived from each problem domain and we provide informal guidelines for each step of the analysis method.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture: Companion Volume},
articleno = {10},
numpages = {9},
keywords = {variability, service, scenarios, quality, cross domain reference architecture, analysis methods},
location = {Essen, Germany},
series = {ECSA '11}
}

@article{10.1007/s10270-017-0592-y,
author = {Ross, Jordan A. and Murashkin, Alexandr and Liang, Jia Hui and Antkiewicz, Micha\l{} and Czarnecki, Krzysztof},
title = {Synthesis and exploration of multi-level, multi-perspective architectures of automotive embedded systems},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0592-y},
doi = {10.1007/s10270-017-0592-y},
abstract = {In industry, evaluating candidate architectures for automotive embedded systems is routinely done during the design process. Today's engineers, however, are limited in the number of candidates that they are able to evaluate in order to find the optimal architectures. This limitation results from the difficulty in defining the candidates as it is a mostly manual process. In this work, we propose a way to synthesize multi-level, multi-perspective candidate architectures and to explore them across the different layers and perspectives. Using a reference model similar to the EAST-ADL domain model but with a focus on early design, we explore the candidate architectures for two case studies: an automotive power window system and the central door locking system. Further, we provide a comprehensive set of question templates, based on the different layers and perspectives, that engineers can ask to synthesize only the candidates relevant to their task at hand. Finally, using the modeling language Clafer, which is supported by automated backend reasoners, we show that it is possible to synthesize and explore optimal candidate architectures for two highly configurable automotive sub-systems.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {739–767},
numpages = {29},
keywords = {Multi-perspective architectures, Multi-level architectures, Early design, E/E architecture, Candidate architectures, Architecture synthesis, Architecture optimization}
}

@inproceedings{10.1007/978-3-319-35122-3_9,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Alam, Omar and Sch\"{o}ttle, Matthias and Belloir, Nicolas and Collet, Philippe and Combemale, Benoit and Deantoni, Julien and Klein, Jacques and Rumpe, Bernhard},
title = {VCU: The Three Dimensions of Reuse},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_9},
doi = {10.1007/978-3-319-35122-3_9},
abstract = {Reuse, enabled by modularity and interfaces, is one of the most important concepts in software engineering. This is evidenced by an increasingly large number of reusable artifacts, ranging from small units such as classes to larger, more sophisticated units such as components, services, frameworks, software product lines, and concerns. This paper presents evidence that a canonical set of reuse interfaces has emerged over time: the variation, customization, and usage interfaces VCU. A reusable artifact that provides all three interfaces reaches the highest potential of reuse, as it explicitly exposes how the artifact can be manipulated during the reuse process along these three dimensions. We demonstrate the wide applicability of the VCU interfaces along two axes: across abstraction layers of a system specification and across existing reuse techniques. The former is shown with the help of a comprehensive case study including reusable requirements, software, and hardware models for the authorization domain. The latter is shown with a discussion on how the VCU interfaces relate to existing reuse techniques.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {122–137},
numpages = {16},
keywords = {Variability, Usage, Reuse, Interfaces, Extension, Customization, Configuration, Concern-oriented reuse},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/2025113.2025177,
author = {Mori, Marco},
title = {A software lifecycle process for context-aware adaptive systems},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025177},
doi = {10.1145/2025113.2025177},
abstract = {It is increasingly important for computing systems to evolve their behavior at run-time because of resources uncertainty, system failures and emerging user needs. Our approach supports software engineers to analyze and develop context-aware adaptive applications. The software lifecycle process we propose supports static and dynamic decision making mechanisms, run-time consistent evolution and it is amenable to be automated.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {412–415},
numpages = {4},
keywords = {software lifecycle process, feature engineering, context-aware adaptive systems, consistent evolution},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inbook{10.5555/1768283.1768287,
author = {Cuenot, Philippe and Chen, DeJiu and G\'{e}rard, S\'{e}bastien and L\"{o}nn, Henrik and Reiser, Mark-Oliver and Servat, David and Kolagari, Ramin Tavakoli and T\"{o}rngren, Martin and Weber, Matthias},
title = {Towards improving dependability of automotive systems by using the EAST-ADL architecture description language},
year = {2007},
isbn = {9783540740339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The complexity of embedded automotive systems calls for a more rigorous approach to system development compared to current state of practice. A critical issue is the management of the engineering information that defines the embedded system. Development time, cost efficiency, quality and most importantly, dependability, all benefit from appropriate information management. System modeling based on an architecture description language is a way to keep the engineering information in one information structure. The EAST-ADL was developed in the EAST-EEA project (www.east-eea.org) and is an architecture description language for automotive embedded systems. It is currently refined in the ATESST project (www.atesst.org). This chapter describes how dependability is addressed in the EAST-ADL. The engineering process defined in the EASIS project (www.easis-online.org) is used as an example to illustrate the support for engineering processes in EAST-ADL.},
booktitle = {Architecting Dependable Systems IV},
pages = {39–65},
numpages = {27}
}

@article{10.1007/s10664-019-09763-0,
author = {Kr\"{u}ger, Jacob and Lausberger, Christian and von Nostitz-Wallwitz, Ivonne and Saake, Gunter and Leich, Thomas},
title = {Search. Review. Repeat? An empirical study of threats to replicating SLR searches},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09763-0},
doi = {10.1007/s10664-019-09763-0},
abstract = {A systematic literature review (SLR) is an empirical method used to provide an overview of existing knowledge and to aggregate evidence within a domain. For computer science, several threats to the completeness of such reviews have been identified, leading to recommendations and guidelines on how to improve their quality. However, few studies address to what extent researchers can replicate an SLR. To conduct a replication, researchers have to first understand how the set of primary studies has been identified in the original study, and can ideally retrieve the same set when following the reported protocol. In this article, we focus on this initial step of a replication and report a two-fold empirical study: Initially, we performed a tertiary study using a sample of SLRs in computer science and identified what information that is needed to replicate the searches is reported. Based on the results, we conducted a descriptive, multi-case study on digital libraries to investigate to what extent these allow replications. The results reveal two threats to replications of SLRs: First, while researchers have improved the quality of their reports, relevant details are still missing—we refer to a reporting threat. Second, we found that some digital libraries are inconsistent in their query results—we refer to a searching threat. While researchers conducting a review can only overcome the first threat and the second may not be an issue for all kinds of replications, researchers should be aware of both threats when conducting, reviewing, and building on SLRs.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {627–677},
numpages = {51},
keywords = {Digital library, Replication, Threats to validity, Software engineering, Systematic literature review, Tertiary study}
}

@inproceedings{10.1145/3243734.3243739,
author = {Ispoglou, Kyriakos K. and AlBassam, Bader and Jaeger, Trent and Payer, Mathias},
title = {Block Oriented Programming: Automating Data-Only Attacks},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243739},
doi = {10.1145/3243734.3243739},
abstract = {With the widespread deployment of Control-Flow Integrity (CFI), control-flow hijacking attacks, and consequently code reuse attacks, are significantly more difficult. CFI limits control flow to well-known locations, severely restricting arbitrary code execution. Assessing the remaining attack surface of an application under advanced control-flow hijack defenses such as CFI and shadow stacks remains an open problem. We introduce BOPC, a mechanism to automatically assess whether an attacker can execute arbitrary code on a binary hardened with CFI/shadow stack defenses. BOPC computes exploits for a target program from payload specifications written in a Turing-complete, high-level language called SPL that abstracts away architecture and program-specific details. SPL payloads are compiled into a program trace that executes the desired behavior on top of the target binary. The input for BOPC is an SPL payload, a starting point (e.g., from a fuzzer crash) and an arbitrary memory write primitive that allows application state corruption. To map SPL payloads to a program trace, BOPC introduces Block Oriented Programming (BOP), a new code reuse technique that utilizes entire basic blocks as gadgets along valid execution paths in the program, i.e., without violating CFI or shadow stack policies. We find that the problem of mapping payloads to program traces is NP-hard, so BOPC first reduces the search space by pruning infeasible paths and then uses heuristics to guide the search to probable paths. BOPC encodes the BOP payload as a set of memory writes. We execute 13 SPL payloads applied to 10 popular applications. BOPC successfully finds payloads and complex execution traces -- which would likely not have been found through manual analysis -- while following the target's Control-Flow Graph under an ideal CFI policy in 81% of the cases.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1868–1882},
numpages = {15},
keywords = {program synthesis, exploitation, data only attacks, block oriented programming, binary analysis},
location = {Toronto, Canada},
series = {CCS '18}
}

@article{10.1016/j.jss.2018.07.014,
author = {Makki, Majid and Van Landuyt, Dimitri and Lagaisse, Bert and Joosen, Wouter},
title = {A comparative study of workflow customization strategies: Quality implications for multi-tenant SaaS},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.014},
doi = {10.1016/j.jss.2018.07.014},
journal = {J. Syst. Softw.},
month = oct,
pages = {423–438},
numpages = {16},
keywords = {Software quality, Functional customization, Workflow automation, Software-as-a-Service, Multi-tenancy}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {uncertainty, software architectures, performance analysis, feature selection},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@article{10.1504/ijwgs.2019.100837,
author = {Bani-Ismail, Basel and Baghdadi, Youcef},
title = {Migrating two legacy systems to SOA: a new approach for service selection based on data flow diagram},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {15},
number = {3},
issn = {1741-1106},
url = {https://doi.org/10.1504/ijwgs.2019.100837},
doi = {10.1504/ijwgs.2019.100837},
abstract = {There are many service identification methods (SIMs) to simplify service identification in SOA lifecycle. These SIMs vary in terms of their features (e.g., input artefact, technique). Due to this diversity, few evaluation frameworks have been proposed to guide organisations in selecting a suitable SIM based on their available input artefacts (e.g., source code, business process). This research concerns with SIMs that consider data flow diagram (DFD) as an input artefact, in order to migrate two legacy systems, modelled with DFD, to SOA. Only two SIMs are found in the literature to identify services based on DFD. However, these SIMs do not provide a way to select among the services identified to be implemented as web services. Therefore, this paper aims to bridge this gap by proposing a new approach for service selection based on DFD to assist organisations in speeding up the process of migrating their legacy systems to SOA.},
journal = {Int. J. Web Grid Serv.},
month = jan,
pages = {251–281},
numpages = {30},
keywords = {DFD, data flow diagram, evaluation framework, service quality, service selection, SIM, service identification method, service identification, SOA, service-oriented architecture}
}

@article{10.1007/s10586-014-0397-5,
author = {Tan, Yujuan and Yan, Zhichao and Feng, Dan and He, Xubin and Zou, Qiang and Yang, Lei},
title = {De-Frag: an efficient scheme to improve deduplication performance via reducing data placement de-linearization},
year = {2015},
issue_date = {Mar 2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-014-0397-5},
doi = {10.1007/s10586-014-0397-5},
abstract = {Data deduplication has become a commodity in large-scale storage systems, especially in data backup and archival systems. However, due to the removal of redundant data, data deduplication de-linearizes data placement and forces the data chunks of the same data object to be divided into multiple separate units. In our preliminary study, we found that the de-linearization of data placement compromises the data spatial locality that is used to improve data read performance, deduplication throughput and deduplication efficiency in some deduplication approaches, which significantly affects deduplication performance and makes some deduplication approaches become less effective. In this paper, we first analyze the negative effect of data placement de-linearization to deduplication performance, and then propose an effective approach called De-Frag to reduce the de-linearization of data placement. The key idea of De-Frag is to choose some redundant data to be written to the disks rather than be removed. It quantifies the spatial locality of each chunk group by spatial locality level (SPL for short) and writes the redundant chunks to disks when SPL value is smaller than a preset value, thus to reduce the de-linearization of data placement and enhance the spatial locality. As shown in our experimental results driven by real world datasets, De-Frag effectively enhances data spatial locality and improves deduplication throughput, deduplication efficiency, and data read performance, at the cost of slightly lower compression ratios.},
journal = {Cluster Computing},
month = mar,
pages = {79–92},
numpages = {14},
keywords = {Spatial locality, Data placement de-linearization, Data deduplication}
}

@inproceedings{10.1109/MISE.2009.5069898,
author = {Nguyen, Quyen L.},
title = {Non-functional requirements analysis modeling for software product lines},
year = {2009},
isbn = {9781424437221},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MISE.2009.5069898},
doi = {10.1109/MISE.2009.5069898},
abstract = {In most IT projects, software developers usually pay attention to functional requirements that satisfy business needs of the system. Non-functional requirements (NFR) such as performance, usability, security, etc. are usually handled ad-hoc during the system testing phase, when it is late and costly to fix problems. Due to the importance and criticality of NFR, I study the problem of modeling NFR for Software Product Lines (SPL), which adds yet an additional dimension of complexity. This paper will survey the software engineering literature, in search of a systematic way to analyze and design NFR, from the perspectives of the concept of commonality and variability of SPL and the characteristics of NFR. Finally, I will propose a methodology based on the extension of Product Line UML-Based Software Engineering (PLUS) techniques, for a unified and automated method to model NFR throughout all phases of SPL engineering.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Modeling in Software Engineering},
pages = {56–61},
numpages = {6},
series = {MISE '09}
}

@inproceedings{10.5555/2485288.2485437,
author = {Ayad, Gasser and Acquaviva, Andrea and Macii, Enrico and Sahbi, Brahim and Lemaire, Romain},
title = {HW-SW integration for energy-efficient/variability-aware computing},
year = {2013},
isbn = {9781450321532},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Recent trends in embedded system architectures brought a rapid shift towards multicore, heterogeneous and reconfigurable platforms. This imposes a large effort for programmers to develop their applications to efficiently exploit the underlying architecture. In addition, process variability issues lead to performance and power uncertainties, impacting expected quality of service and energy efficiency of the running software. In particular, variability may lead to sub-optimal runtime task allocation.In this paper we present a holistic approach to tackle these issues exploiting high level HW/SW modeling to customize the runtime library. The customization introduces variability awareness in task allocation decisions, with the final purpose of optimizing a given objective: Execution time, power consumption, or overall energy consumption.We present a complete walkthrough, from top-level modeling down to variability-aware execution using a parallelized computational kernel running on a next generation, NoC based, heterogeneous multicore simulation platform.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {607–611},
numpages = {5},
location = {Grenoble, France},
series = {DATE '13}
}

@article{10.1016/j.jss.2010.02.017,
author = {White, J. and Benavides, D. and Schmidt, D. C. and Trinidad, P. and Dougherty, B. and Ruiz-Cortes, A.},
title = {Automated diagnosis of feature model configurations},
year = {2010},
issue_date = {July, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.02.017},
doi = {10.1016/j.jss.2010.02.017},
abstract = {Software product-lines (SPLs) are software platforms that can be readily reconfigured for different project requirements. A key part of an SPL is a model that captures the rules for reconfiguring the software. SPLs commonly use feature models to capture SPL configuration rules. Each SPL configuration is represented as a selection of features from the feature model. Invalid SPL configurations can be created due to feature conflicts introduced via staged or parallel configuration or changes to the constraints in a feature model. When invalid configurations are created, a method is needed to automate the diagnosis of the errors and repair the feature selections. This paper provides two contributions to research on automated configuration of SPLs. First, it shows how configurations and feature models can be transformed into constraint satisfaction problems to automatically diagnose errors and repair invalid feature selections. Second, it presents empirical results from diagnosing configuration errors in feature models ranging in size from 100 to 5,000 features. The results of our experiments show that our CSP-based diagnostic technique can scale up to models with thousands of features.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1094–1107},
numpages = {14},
keywords = {Software product-lines, Optimization, Diagnosis, Constraint satisfaction, Configuration}
}

@article{10.1016/j.comnet.2013.02.025,
author = {Apel, Sven and Von Rhein, Alexander and Th\"{u}m, Thomas and K\"{a}stner, Christian},
title = {Feature-interaction detection based on feature-based specifications},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {57},
number = {12},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2013.02.025},
doi = {10.1016/j.comnet.2013.02.025},
abstract = {Formal specification and verification techniques have been used successfully to detect feature interactions. We investigate whether feature-based specifications can be used for this task. Feature-based specifications are a special class of specifications that aim at modularity in open-world, feature-oriented systems. The question we address is whether modularity of specifications impairs the ability to detect feature interactions, which cut across feature boundaries. In an exploratory study on 10 feature-oriented systems, we found that the majority of feature interactions could be detected based on feature-based specifications, but some specifications have not been modularized properly and require undesirable workarounds to modularization. Based on the study, we discuss the merits and limitations of feature-based specifications, as well as open issues and perspectives. A goal that underlies our work is to raise awareness of the importance and challenges of feature-based specification.},
journal = {Comput. Netw.},
month = aug,
pages = {2399–2409},
numpages = {11},
keywords = {Software product lines, Modularity, Feature-based specification, Feature orientation, Feature interaction}
}

@inproceedings{10.1145/2491246.2491250,
author = {Zhang, Xi and Ansari, Junaid and Arya, Manish and M\"{a}h\"{o}nen, Petri},
title = {Exploring parallelization for medium access schemes on many-core software defined radio architecture},
year = {2013},
isbn = {9781450321815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491246.2491250},
doi = {10.1145/2491246.2491250},
abstract = {As multi-standard devices and high speed communication standards are emerging, timeliness requirements and flexibility for both baseband modem and medium access schemes are becoming essential. Software Defined Radios (SDRs), in this context, aim at offering the desired flexibility while satisfying the real-time constraints. An SDR architecture consisting of many-core homogeneous computing elements provides easy protocol implementation, a high level of portability and extension possibilities. It does not require architecture specific program code which is needed by the popular heterogeneous SDR architectures. Therefore, in this paper, we explore how a homogeneous SDR architecture is used for efficient realization and execution of Medium Access Control (MAC) protocols. In particular, we investigate the performance of two broad classes of MAC schemes on the Platform 2012 (P2012) many-core programmable computing fabric. We provide a toolchain which utilizes the characteristics of P2012 for MAC parallelization, runtime scheduling, and execution. Our results indicate that by using the supporting toolchain, reconfigurable MAC implementations are able to exploit the computational power offered by the platform and adhere to the timeliness constraints. Computationally intensive algorithms for MAC layer parameter optimization show an improvement of up to 85% in the convergence time as compared to using a single-core architecture.},
booktitle = {Proceedings of the Second Workshop on Software Radio Implementation Forum},
pages = {37–44},
numpages = {8},
keywords = {sdr platform, parallelization, many-core, mac},
location = {Hong Kong, China},
series = {SRIF '13}
}

@article{10.1016/j.jss.2017.09.033,
author = {Badampudi, Deepika and Wnuk, Krzysztof and Wohlin, Claes and Franke, Ulrik and Smite, Darja and Cicchetti, Antonio},
title = {A decision-making process-line for selection of software asset origins and components},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.09.033},
doi = {10.1016/j.jss.2017.09.033},
abstract = {Presents a process-line for selecting software asset origins and components.Process-line helps decision-makers to build their decisions-making process.The process-line is evaluated through five case studies in three companies.The practitioners did not perceive any activity to be missing in the process-line.A sub-set of activities were followed by the companies without any specific order. Selecting sourcing options for software assets and components is an important process that helps companies to gain and keep their competitive advantage. The sourcing options include: in-house, COTS, open source and outsourcing. The objective of this paper is to further refine, extend and validate a solution presented in our previous work. The refinement includes a set of decision-making activities, which are described in the form of a process-line that can be used by decision-makers to build their specific decision-making process. We conducted five case studies in three companies to validate the coverage of the set of decision-making activities. The solution in our previous work was validated in two cases in the first two companies. In the validation, it was observed that no activity in the proposed set was perceived to be missing, although not all activities were conducted and the activities that were conducted were not executed in a specific order. Therefore, the refinement of the solution into a process-line approach increases the flexibility and hence it is better in capturing the differences in the decision-making processes observed in the case studies. The applicability of the process-line was then validated in three case studies in a third company.},
journal = {J. Syst. Softw.},
month = jan,
pages = {88–104},
numpages = {17},
keywords = {Decision-making, Component-based software engineering, Case study}
}

@inproceedings{10.1145/2000259.2000263,
author = {Koziolek, Heiko},
title = {Sustainability evaluation of software architectures: a systematic review},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000263},
doi = {10.1145/2000259.2000263},
abstract = {Long-living software systems are sustainable if they can be cost-efficiently maintained and evolved over their entire life-cycle. The quality of software architectures determines sustainability to a large extent. Scenario-based software architecture evaluation methods can support sustainability analysis, but they are still reluctantly used in practice. They are also not integrated with architecture-level metrics when evaluating implemented systems, which limits their capabilities. Existing literature reviews for architecture evaluation focus on scenario-based methods, but do not provide a critical reflection of the applicability of such methods for sustainability evaluation. Our goal is to measure the sustainability of a software architecture both during early design using scenarios and during evolution using scenarios and metrics, which is highly relevant in practice. We thus provide a systematic literature review assessing scenario-based methods for sustainability support and categorize more than 40 architecture-level metrics according to several design principles. Our review identifies a need for further empirical research, for the integration of existing methods, and for the more efficient use of formal architectural models.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {3–12},
numpages = {10},
keywords = {sustainability, survey, software architecture, evolution scenario, architectural metric},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00107,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-box performance-influence models: a profiling and learning approach (replication package)},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00107},
doi = {10.1109/ICSE-Companion52605.2021.00107},
abstract = {These artifacts refer to the study and implementation of the paper 'White-Box Performance-Influence Models: A Profiling and Learning Approach'. In this document, we describe the idea and process of how to build white-box performance models for configurable software systems. Specifically, we describe the general steps and tools that we have used to implement our approach, the data we have obtained, and the evaluation setup. We further list the available artifacts, such as raw measurements, configurations, and scripts at our software heritage repository.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {232–233},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1007/978-3-642-41533-3_37,
author = {Alam, Omar and Kienzle, J\"{o}rg and Mussbacher, Gunter},
title = {Concern-Oriented Software Design},
year = {2013},
isbn = {9783642415326},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41533-3_37},
doi = {10.1007/978-3-642-41533-3_37},
abstract = {There exist many solutions to solve a given design problem, and it is difficult to capture the essence of a solution and make it reusable for future designs. Furthermore, many variations of a given solution may exist, and choosing the best alternative depends on application-specific high-level goals and non-functional requirements. This paper proposes Concern-Oriented Software Design, a modelling technique that focuses on concerns as units of reuse. A concern groups related models serving the same purpose, and provides three interfaces to facilitate reuse. The variation interface presents the design alternatives and their impact on non-functional requirements. The customization interface of the selected alternative details how to adapt the generic solution to a specific context. Finally, the usage interface specifies the provided behaviour. We illustrate our approach by presenting the concern models of variations of the Observer design pattern, which internally depends on the Association concern to link observers and subjects.},
booktitle = {Proceedings of the 16th International Conference on Model-Driven Engineering Languages and Systems - Volume 8107},
pages = {604–621},
numpages = {18}
}

@inproceedings{10.1145/2610384.2610411,
author = {Galindo, Jos\'{e} A. and Alf\'{e}rez, Mauricio and Acher, Mathieu and Baudry, Benoit and Benavides, David},
title = {A variability-based testing approach for synthesizing video sequences},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610411},
doi = {10.1145/2610384.2610411},
abstract = {A key problem when developing video processing software is the difficulty to test different input combinations. In this paper, we present VANE, a variability-based testing approach to derive video sequence variants. The ideas of VANE are i) to encode in a variability model what can vary within a video sequence; ii) to exploit the variability model to generate testable configurations; iii) to synthesize variants of video sequences corresponding to configurations. VANE computes T-wise covering sets while optimizing a function over attributes. Also, we present a preliminary validation of the scalability and practicality of VANE in the context of an industrial project involving the test of video processing algorithms.},
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {293–303},
numpages = {11},
keywords = {Video analysis, Variability, Combinatorial testing},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1007/11554844_19,
author = {Helferich, Andreas and Herzwurm, Georg and Schockert, Sixten},
title = {QFD-PPP: product line portfolio planning using quality function deployment},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_19},
doi = {10.1007/11554844_19},
abstract = {In today’s competitive business environment, it is extremely important to offer customers exactly the products they want. Software product lines have the potential to enable companies to offer a large variety of products while still being able to manage the complexity caused by this increased number of products. But offering a large range of variants does not necessarily mean increased profits, as many manufacturing companies had to notice in the early 1990ies. The task of Product Portfolio Planning is the development of a product portfolio that optimally satisfies customer demands and at the same time restricts the number of products offered. Quality Function Deployment (QFD) is a well-known and successfully used Quality Management method that can help companies to identify true customer needs and the features needed to fulfil these needs. This paper demonstrates how QFD can be used for Product Portfolio Planning, thus offering potentially great benefits.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {162–173},
numpages = {12},
location = {Rennes, France},
series = {SPLC'05}
}

@article{10.1007/s11219-018-9405-y,
author = {Vale, Gustavo and Fernandes, Eduardo and Figueiredo, Eduardo},
title = {On the proposal and evaluation of a benchmark-based threshold derivation method},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9405-y},
doi = {10.1007/s11219-018-9405-y},
abstract = {Software-intensive systems have been growing in both size and complexity. Consequently, developers need better support for measuring and controlling the software quality. In this context, software metrics aim at quantifying different software quality aspects. However, the effectiveness of measurement depends on the definition of reliable metric thresholds, i.e., numbers that characterize a metric value as critical given a quality aspect. In fact, without proper metric thresholds, it might be difficult for developers to indicate problematic software components for correction, for instance. Based on a literature review, we have found several existing methods for deriving metric thresholds and observed their evolution. Such evolution motivated us to propose a new method that incorporates the best of the existing methods. In this paper, we propose a novel benchmark-based method for deriving metric thresholds. We assess our method, called Vale's method, using a set of metric thresholds derived with the support of our method, aimed at composing detection strategies for two well-known code smells, namely god class and lazy class. For this purpose, we analyze three benchmarks composed of multiple software product lines. In addition, we demonstrate our method in practice by applying it to a benchmark composed of 103 Java open-source software systems. In the evaluation, we compare Vale's method to two state-of-the-practice threshold derivation methods selected as a baseline, which are Lanza's method and Alves' method. Our results suggest that the proposed method provides more realistic and reliable thresholds, with better recall and precision in the code smell detection, when compared to both baseline methods.},
journal = {Software Quality Journal},
month = mar,
pages = {275–306},
numpages = {32},
keywords = {Threshold, Software product lines, Software metric, Code smell, Benchmark}
}

@article{10.1007/s11276-018-1718-z,
author = {Mukhlif, Fadhil and Noordin, Kamarul Ariffin Bin and Mansoor, Ali Mohammed and Kasirun, Zarinah Mohd},
title = {Green transmission for C-RAN based on SWIPT in 5G: a review},
year = {2019},
issue_date = {Jul 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {5},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-018-1718-z},
doi = {10.1007/s11276-018-1718-z},
abstract = {C-RAN is a promising new design for the next generation, an important aspect of it in the energy efficiency consideration. Hence, it is considering an innovative candidate to use it as an alternative cellular network instead of the traditional. Investigation green transmission of mobile cloud radio access networks based on SWIPT for 5G cellular networks. Especially, with considering SWIPT as a future solution for increasing the lifetime of end-user battery’s, that’s mean this technique will improving energy efficiency (EE). Addressing SWIPT into C-RAN is a challenging and it is needed to developing a new algorithm to use it on the cellular network with many trying to ensure the success of the system performance. C-RAN as a network and SWIPT as a promising technique with the suggesting green wireless network are discussed besides the importance of energy efficiency for the next generation. Furthermore, there was a study on fifth enabling technologies that can be used for 5G with emphasis on two of them (C-RAN and energy efficiency). Lastly, research challenges and future direction that require substantial research efforts are summarized.},
journal = {Wirel. Netw.},
month = jul,
pages = {2621–2649},
numpages = {29},
keywords = {MIMO, Power splitting, Time switching, Information decoding (ID), Energy harvesting (EH), Cloud radio access network, Power transfer, Green transmission}
}

@inproceedings{10.1145/2593882.2593886,
author = {Garlan, David},
title = {Software architecture: a travelogue},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593886},
doi = {10.1145/2593882.2593886},
abstract = {Over the past two and a half decades software architecture has emerged as an important subfield of software engineering. During that time there has been considerable progress in developing the technological and methodological base for treating architectural design as an engineering discipline. However, much still remains to be done to achieve that. Moreover, the changing face of technology raises a number of challenges for software architecture. This travelogue recounts the history of the field, its current state of practice and research, and speculates on some of the important emerging trends, challenges, and aspirations.},
booktitle = {Future of Software Engineering Proceedings},
pages = {29–39},
numpages = {11},
keywords = {software product lines, software frame-works, architecture trends, architecture styles, architecture description languages, architecture and agility, Software architecture},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/3387940.3391474,
author = {Brings, Jennifer and Daun, Marian},
title = {Towards automated safety analysis for architectures of dynamically forming networks of cyber-physical systems},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391474},
doi = {10.1145/3387940.3391474},
abstract = {Dynamically forming networks of cyber-physical systems are becoming increasingly widespread in manufacturing, transportation, automotive, avionics and more domains. The emergence of future internet technology and the ambition for ever closer integration of different systems leads to highly collaborative cyber-physical systems. Such cyber-physical systems form networks to provide additional functions, behavior, and benefits the individual systems cannot provide on their own. As safety is a major concern of systems from these domains, there is a need to provide adequate support for safety analyses of these collaborative cyber-physical systems. This support must explicitly consider the dynamically formed networks of cyber-physical systems. This is a challenging task as the configurations of these cyber-physical system networks (i.e. the architecture of the super system the individual system joins) can differ enormously depending on the actual systems joining a cyber-physical system network. Furthermore, the configuration of the network heavily impacts the adaptations performed by the individual systems and thereby impacting the architecture not only of the system network but of all individual systems involved. As existing safety analysis techniques, however, are not meant for supporting such an array of potential system network configurations the individual system will have to be able to cope with at runtime, we propose automated support for safety analysis for these systems that considers the configuration of the system network. Initial evaluation results from the application to industrial case examples show that the proposed support can aid in the detection of safety defects.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {258–265},
numpages = {8},
keywords = {system architecture, safety analysis, cyber-physical system},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1007/978-3-319-35122-3_16,
author = {Braga, Rosana T. and Feloni, Daniel and Pacini, Karen and Filho, Domenico Schettini and Gottardi, Thiago},
title = {AIRES: An Architecture to Improve Software Reuse},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_16},
doi = {10.1007/978-3-319-35122-3_16},
abstract = {Among the several challenges still faced by Software Engineering, software reuse can be listed as a potential solution towards improving productivity and quality, through the utilization of previously produced artifacts that can leverage development activities. Among these artifacts we can mention not only code, but also requirements' documents, analysis and design models, test cases, documentation, and even development processes that achieved success in the past and could be reused again and again. However, the diversity of methods, processes and tools for software engineering make it difficult to turn reuse into a systematic activity. Considering this context, the present paper aims at presenting an architectural model that encompasses the main elements needed to support software reuse in a large scale. This model, named AIRES, allows reuse to be realized intrinsically to the development process life cycle, providing mechanisms to facilitate a variety of processes and artifacts representation and a Service-Oriented Architecture SOA to make assets available to other software engineering environments or tools. The AIRES model is being implemented using open source platforms and will be available within the cloud.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {231–246},
numpages = {16},
keywords = {Software reuse, Reuse tools, Reuse environments},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/2833258.2833284,
author = {Bien, Ngo Huy and Thu, Tran Dan},
title = {Graphical User Interface Variability Architecture Pattern},
year = {2015},
isbn = {9781450338431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2833258.2833284},
doi = {10.1145/2833258.2833284},
abstract = {Designing software applications for multiple tenants is challenging. The task is even harder when designing pure multi-tenancy applications that must support different customers using a single codebase and data store. One of the most common problems when developing these systems is to support different graphical user interface not only for different users but also tenants. This critical requirement also applies to context-aware applications in which different graphical user interface should be presented to each user according to the user's context or software components and platforms that should allow developers easily to create different looks and feel for their applications. In this paper, we propose an architecture pattern for modeling graphical user interface that support different customizations and configurations. The modularity, the reusability and the maintainability of the pattern were evaluated by qualitative analysis based on well-known patterns used in the proposed architecture pattern. Real world systems were built to validate the applicability, the correctness, the security and the performance of the pattern. We believe that our pattern will be useful for software providers as well as normal organizations when building software components or software systems for different customers or creating multi-tenancy applications in a cloud-based environment or building context-aware applications.},
booktitle = {Proceedings of the 6th International Symposium on Information and Communication Technology},
pages = {304–311},
numpages = {8},
keywords = {variability pattern, software variability, software architecture, multi-tenancy, enterprise systems, architecture pattern, SaaS, PaaS, Graphical user interface},
location = {Hue City, Viet Nam},
series = {SoICT '15}
}

@inproceedings{10.1145/2304696.2304705,
author = {Alebrahim, Azadeh and Heisel, Maritta},
title = {Supporting quality-driven design decisions by modeling variability},
year = {2012},
isbn = {9781450313469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304696.2304705},
doi = {10.1145/2304696.2304705},
abstract = {Design decisions should take quality characteristics into account. To support such decisions, we capture various solution artifacts with different levels of satisfying quality requirements as variabilities in the solution space and provide them with rationales for selecting suitable variants. We present a UML-based approach to modeling variability in the problem and the solution space by adopting the notion of feature modeling. It provides a mapping of requirements variability to design solution variability to be used as a part of a general process for generating design alternatives. Our approach supports the software engineer in the process of decision-making for selecting suitable solution variants, reflecting quality concerns, and reasoning about it.},
booktitle = {Proceedings of the 8th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {43–48},
numpages = {6},
keywords = {variability modeling, quality requirements, feature modeling, design alternatives, decision-making},
location = {Bertinoro, Italy},
series = {QoSA '12}
}

@article{10.1145/3178315.3178328,
author = {Wang, Huaimin},
title = {Harnessing the crowd wisdom for software trustworthiness},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3178315.3178328},
doi = {10.1145/3178315.3178328},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {1–6},
numpages = {6}
}

@article{10.1007/s11257-015-9159-1,
author = {Dim, Eyal and Kuflik, Tsvi and Reinhartz-Berger, Iris},
title = {When user modeling intersects software engineering: the info-bead user modeling approach},
year = {2015},
issue_date = {August    2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-015-9159-1},
doi = {10.1007/s11257-015-9159-1},
abstract = {User models (UMs) allow systems to provide personalized services to their users. Nowadays, UMs are developed ad-hoc, as part of specific applications, thus requiring repetitive development efforts. In this paper, we propose the info-bead user modeling approach, which is based on ideas taken from software engineering in general and component-based software development in particular. The basic standalone unit, the info-bead, represents a single user attribute within time-tagged information-items. An info-bead encapsulates an inference process that uses data received from sensors or other info-beads and yields an information-item value. Having standard interfaces, info-beads can be linked, thus creating info-pendants. Both info-beads and info-pendants can be assembled as needed into complex and abstract user models (UMs) and group models (GMs). The goal of the suggested approach is to ease the modeling process and to allow reuse of info beads developed for one UM in other UMs that need the same information. In order to assess the reusability and collaboration capabilities of the info-bead user modeling approach, we developed a prototype tool that enables UM designers, who are not necessarily software developers, to easily select and integrate info-beads for constructing UMs and GMs. We further demonstrated the use of the approach in a museum environment, for modeling of assistive technology ontology and for user modeling in various specific domains. Finally, we analyzed and assessed the characteristics of the approach with respect to existing generic user modeling criteria.},
journal = {User Modeling and User-Adapted Interaction},
month = aug,
pages = {189–229},
numpages = {41},
keywords = {User modeling tool, User modeling software engineering, User model reusability, User model, Info-pendant, Info-bead, Group model, Component-based user model}
}

@inproceedings{10.1145/2984043.2998537,
author = {D\"{u}rschmid, Tobias},
title = {Design pattern builder: a concept for refinable reusable design pattern libraries},
year = {2016},
isbn = {9781450344371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984043.2998537},
doi = {10.1145/2984043.2998537},
abstract = {Reuse is one of the core principles in professional software engineering. Design patterns provide a reusable solution for common design problems. But their implementations are generally not reusable as they are often well tailored to a specific context. We introduce a concept, that facilitates the reuse of their implementations by defining an abstract design pattern definition that can be instantiated with specialized design decisions. This approach is a meta-level Builder constructing design patterns as first-class citizens. It simplifies the application of design patterns by providing a pattern library and still being able to adjust it to the concrete context.},
booktitle = {Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
pages = {45–46},
numpages = {2},
keywords = {reusability, modularity, Design patterns, AOP},
location = {Amsterdam, Netherlands},
series = {SPLASH Companion 2016}
}

@inproceedings{10.1007/978-3-030-26250-1_3,
author = {Ne\v{s}i\'{c}, Damir and Nyberg, Mattias},
title = {Modular Safety Cases for Product Lines Based on Assume-Guarantee Contracts},
year = {2019},
isbn = {978-3-030-26249-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26250-1_3},
doi = {10.1007/978-3-030-26250-1_3},
abstract = {Safety cases are recommended, and in some cases required, by a number of standards. In the product line context, unlike for single systems, safety cases are inherently complex because they must argue about the safety of a family of products that share various types of engineering assets. Safety case modularization has been proposed to reduce safety case complexity by separating concerns, modularizing tightly coupled arguments, and localizing effects of changes to particular modules. Existing modular safety-case approaches for product lines propose a feature-based modularization, which is too coarse to modularize the claims of different types, at different levels of abstraction. To overcome these limitation, a novel, modular safety-case architecture is presented. The modularization is based on a contract-based specification product-line model, which jointly captures the component-based architecture of systems and corresponding safety requirements as assume-guarantee contracts. The proposed safety-case architecture is analyzed against possible product-line changes and it is shown that it is robust both with respect to fine and coarse-grained, and also product and implementation-level changes. The proposed modular safety case is exemplified on a simplified, but real automotive system.},
booktitle = {Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings},
pages = {28–40},
numpages = {13},
keywords = {Product line, Assume-guarantee contract, Modular safety case},
location = {Turku, Finland}
}

@article{10.1016/j.future.2018.05.023,
author = {Guerrera, Danilo and Maffia, Antonio and Burkhart, Helmar},
title = {Reproducible stencil compiler benchmarks using prova!      },
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.05.023},
doi = {10.1016/j.future.2018.05.023},
journal = {Future Gener. Comput. Syst.},
month = mar,
pages = {933–946},
numpages = {14},
keywords = {Roofline, Performance engineering, Stencil, HPC, Reproducible research, Reproducibility}
}

@article{10.1016/j.jss.2016.02.026,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {A feature-driven crossover operator for multi-objective and evolutionary optimization of product line architectures},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.02.026},
doi = {10.1016/j.jss.2016.02.026},
abstract = {We propose feature-driven crossover to improve feature modularization in PLA design.We compare the performance of a search algorithm with and without that crossover.Empirical results show the feature-driven crossover provides benefits to PLA design.Such crossover allows achieving better solutions and greater diversity of solutions.Such crossover operator also contributes to improve basic design principles. The optimization of a Product Line Architecture (PLA) design can be modeled as a multi-objective problem, influenced by many factors, such as feature modularization, extensibility and other design principles. Due to this it has been properly solved in the Search Based Software Engineering (SBSE) field. However, previous empirical studies optimized PLA design using the multi-objective and evolutionary algorithm NSGA-II, without applying one of the most important genetic operators: the crossover. To overcome this limitation, this paper presents a feature-driven crossover operator that aims at improving feature modularization in PLA design. The proposed operator was applied in two empirical studies using NSGA-II in comparison with another version of NSGA-II that uses only mutation operators. The results show the usefulness and applicability of the proposed operator. The NSGA-II version that applies the feature-driven crossover found a greater diversity of solutions (potential PLA designs), with higher feature-based cohesion, and less feature scattering and tangling.},
journal = {J. Syst. Softw.},
month = nov,
pages = {126–143},
numpages = {18},
keywords = {Product line architecture design, Multi-objective genetic algorithm, Empirical study, Crossover operator}
}

@article{10.1016/j.infsof.2016.09.007,
author = {Weinreich, Rainer and Groher, Iris},
title = {Software architecture knowledge management approaches and their support for knowledge management activities},
year = {2016},
issue_date = {December 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {80},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.09.007},
doi = {10.1016/j.infsof.2016.09.007},
abstract = {Context: Numerous approaches for Software Architecture Knowledge Management (SAKM) have been developed by the research community over the last decade. Still, these approaches have not yet found widespread use in practice.Objective: This work identifies existing approaches to SAKM and analyzes them in terms of their support for central architecture knowledge management activities, i.e., capturing, using, maintaining, sharing, and reuse of architectural knowledge, along with presenting the evidence provided for this support.Method: A systematic literature review has been conducted for identifying and analyzing SAKM approaches, covering work published between January 2004 and August 2015. We identified 56 different approaches to SAKM based on 115 studies. We analyzed each approach in terms of its focus and support for important architecture knowledge management activities and in terms of the provided level of evidence for each supported activity.Results: Most of the developed approaches focus on using already-captured knowledge. Using is also the best-validated activity. The problem of efficient capturing is still not sufficiently addressed, and only a few approaches specifically address reuse, sharing, and, especially, maintaining.Conclusions: Without adequate support for other core architecture knowledge management activities besides using, the adoption of SAKM in practice will remain an elusive target. The problem of efficient capturing is still unsolved, as is the problem of maintaining captured knowledge over the long term. We also need more case studies and replication studies providing evidence for the usefulness of developed support for SAKM activities, as well as better reporting on these case studies.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {265–286},
numpages = {22},
keywords = {Systematic literature review, Software architecture knowledge management approaches, Software architecture knowledge management activities, Software architecture knowledge management, Software architecture}
}

@inproceedings{10.1145/2601248.2601257,
author = {H\"{a}ser, Florian and Felderer, Michael and Breu, Ruth},
title = {Software paradigms, assessment types and non-functional requirements in model-based integration testing: a systematic literature review},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601257},
doi = {10.1145/2601248.2601257},
abstract = {Context: In modern systems, like cyber-physical systems, where software and physical services are interacting, safety, security or performance play an important role. In order to guarantee the correct interoperability of such systems, with respect to functional and non-functional requirements, integration testing is an effective measure to achieve this. Model-based testing moreover not only enables early definition and validation, but also test automation. This makes it a good choice to overcome urgent challenges of integration testing. Objective: Many publications on model-based integration testing (MBIT) approaches can be found. Nevertheless, a study giving a systematic overview on the underlying software paradigms, measures for guiding the integration testing process as well as non-functional requirements they are suitable for, is missing. The aim of this paper is to find and synthesize the relevant primary studies to gain a comprehensive understanding of the current state of model-based integration testing. Method: For synthesizing the relevant studies, we conducted a systematic literature review (SLR) according to the guidelines of Kitchenham. Results: The systematic search and selection retrieved 83 relevant studies from which data has been extracted. Our review identified three assessment criteria for guiding the testing process, namely static metrics, dynamic metrics and stochastic &amp;random. In addition it shows that just a small fraction considers non-functional requirements. Most approaches are for component-oriented systems. Conclusion: Results from the SLR show that there are two major research gaps. First, there is an accumulated need for approaches in the MBIT field that support non-functional requirements, as they are gaining importance. Second, means for steering the integration testing process, especially together with automation, need to evolve.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {29},
numpages = {10},
keywords = {systematic literature review, non-functional requirements, model-based integration testing, assessment types},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@article{10.1145/3170432,
author = {Dayarathna, Miyuru and Perera, Srinath},
title = {Recent Advancements in Event Processing},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3170432},
doi = {10.1145/3170432},
abstract = {Event processing (EP) is a data processing technology that conducts online processing of event information. In this survey, we summarize the latest cutting-edge work done on EP from both industrial and academic research community viewpoints. We divide the entire field of EP into three subareas: EP system architectures, EP use cases, and EP open research topics. Then we deep dive into the details of each subsection. We investigate the system architecture characteristics of novel EP platforms, such as Apache Storm, Apache Spark, and Apache Flink. We found significant advancements made on novel application areas, such as the Internet of Things; streaming machine learning (ML); and processing of complex data types such as text, video data streams, and graphs. Furthermore, there has been significant body of contributions made on event ordering, system scalability, development of EP languages and exploration of use of heterogeneous devices for EP, which we investigate in the latter half of this article. Through our study, we found key areas that require significant attention from the EP community, such as Streaming ML, EP system benchmarking, and graph stream processing.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {33},
numpages = {36},
keywords = {data stream processing, complex event processing, Event processing}
}

@inproceedings{10.1007/978-3-642-27216-5_11,
author = {Nunes, Ingrid and Barbosa, Simone Diniz Junqueira and de Lucena, Carlos J. P.},
title = {Dynamically adapting BDI agents based on high-level user specifications},
year = {2011},
isbn = {9783642272158},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-27216-5_11},
doi = {10.1007/978-3-642-27216-5_11},
abstract = {Users are facing an increasing challenge of managing information and being available anytime anywhere, as the web exponentially grows. As a consequence, assisting them in their routine tasks has become a relevant issue to be addressed. In this paper, we introduce an adaptation mechanism that is responsible for dynamically adapting a BDI agent-based running system in order to support software customisation for users. This mechanism is used within a software framework for supporting the development of Personal Assistance Software (PAS), which relies on the idea of exposing a high-level user model to empower users to manage it as well as increase user trust in the task delegation process.},
booktitle = {Proceedings of the 10th International Conference on Advanced Agent Technology},
pages = {139–163},
numpages = {25},
keywords = {user modeling, software adaptation, personal assistance software, framework, BDI},
location = {Taipei, Taiwan},
series = {AAMAS'11}
}

@article{10.1016/j.jss.2017.09.025,
title = {Managing architectural technical debt},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.09.025},
doi = {10.1016/j.jss.2017.09.025},
abstract = {A systematic literature review on Architectural Technical Debt (ATD).One key contribution is our novel model of ATD.The model provides an holistic overview of ATD.The model illustrates information for managing and raising awareness about ATD. Large Software Companies need to support the continuous and fast delivery of customer value in both the short and long term. However, this can be impeded if the evolution and maintenance of existing systems is hampered by what has been recently termed Technical Debt (TD). Specifically, Architectural TD has received increased attention in the last few years due to its significant impact on system success and, left unchecked, it can cause expensive repercussions. It is therefore important to understand the underlying factors of architectural TD. With this as background, there is a need for a descriptive model to illustrate and explain different architectural TD issues. The aim of this study is to synthesize and compile research efforts with the goal of creating new knowledge with a specific interest in the architectural TD field. The contribution of this paper is the presentation of a novel descriptive model, providing a comprehensive interpretation of the architectural TD phenomenon. This model categorizes the main characteristics of architectural TD and reveals their relations. The results show that, by using this model, different stakeholders could increase the system's success rate, and lower the rate of negative consequences, by raising awareness about architectural TD.},
journal = {J. Syst. Softw.},
month = jan,
pages = {1–16},
numpages = {16}
}

@article{10.1016/j.jss.2007.01.047,
author = {Liu, Jing and Dehlinger, Josh and Lutz, Robyn},
title = {Safety analysis of software product lines using state-based modeling},
year = {2007},
issue_date = {November, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {80},
number = {11},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.01.047},
doi = {10.1016/j.jss.2007.01.047},
abstract = {The difficulty of managing variations and their potential interactions across an entire product line currently hinders safety analysis in safety-critical, software product lines. The work described here contributes to a solution by integrating product-line safety analysis with model-based development. This approach provides a structured way to construct state-based models of a product line having significant, safety-related variations and to systematically explore the relationships between behavioral variations and potential hazardous states through scenario-guided executions of the state model over the variations. The paper uses a product line of safety-critical medical devices to demonstrate and evaluate the technique and results.},
journal = {J. Syst. Softw.},
month = nov,
pages = {1879–1892},
numpages = {14},
keywords = {State-based modeling, Safety-critical systems, Product lines, Model-based development}
}

@article{10.1007/s10270-020-00823-4,
author = {Kretschmer, Roland and Khelladi, Djamel Eddine and Lopez-Herrejon, Roberto Erick and Egyed, Alexander},
title = {Consistent change propagation within models},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00823-4},
doi = {10.1007/s10270-020-00823-4},
abstract = {Developers change models with clear intentions—e.g., for refactoring, defects removal, or evolution. However, in doing so, developers are often unaware of the consequences of their changes. Changes to one part of a model may affect other parts of the same model and/or even other models, possibly created and maintained by other developers. The consequences are incomplete changes and with it inconsistencies within or across models. Extensive works exist on detecting and repairing inconsistencies. However, the literature tends to focus on inconsistencies as errors in need of repairs rather than on incomplete changes in need of further propagation. Many changes are non-trivial and require a series of coordinated model changes. As developers start changing the model, intermittent inconsistencies arise with other parts of the model that developers have not yet changed. These inconsistencies are cues for incomplete change propagation. Resolving these inconsistencies should be done in a manner that is consistent with the original changes. We speak of consistent change propagation. This paper leverages classical inconsistency repair mechanisms to explore the vast search space of change propagation. Our approach not only suggests changes to repair a given inconsistency but also changes to repair inconsistencies caused by the aforementioned repair. In doing so, our approach follows the developer’s intent where subsequent changes may not contradict or backtrack earlier changes. We argue that consistent change propagation is essential for effective model-driven engineering. Our approach and its tool implementation were empirically assessed on 18 case studies from industry, academia, and GitHub to demonstrate its feasibility and scalability. A comparison with two versioned models shows that our approach identifies actual repair sequences that developers had chosen. Furthermore, an experiment involving 22 participants shows that our change propagation approach meets the workflow of how developers handle changes by always computing the sequence of repairs resulting from the change propagation.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {539–555},
numpages = {17},
keywords = {Consistency detection, Change propagation, Inconsistency repair, Model-driven engineering}
}

@inproceedings{10.1145/2797433.2797445,
author = {Portocarrero, Jes\'{u}s M. T. and Delicato, Fl\'{a}via C. and Pires, Paulo F. and Nakagawa, Elisa Y. and Oquendo, Flavio},
title = {Self-Adaptive Middleware for Wireless Sensor Networks: A Reference Architecture},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797445},
doi = {10.1145/2797433.2797445},
abstract = {Wireless Sensor Networks (WSN) are networks composed by tiny devices equipped with sensing, processing, storage, and wireless communication capabilities. WSN are used in highly dynamic environments. Applications for WSN should have an autonomous behavior to adapt their operation and achieve the best network performance. Such adaptation should preferably be performed by a middleware layer tailored to the limited resources of WSN. In this paper, we introduce a Reference Architecture (RA) of a self-adaptive middleware for WSN to contribute for the development of solutions enabling autonomic behavior in WSN. Our RA follows an autonomic computing model (MAPE-K) proposed by IBM and it was specified using a formal description language (pi-ADL) that enables the specification of dynamic architectures. ProSA-RA was used to systematize the design, representation and evaluation of our RA.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {12},
numpages = {8},
keywords = {wireless sensor network, self-adaptive system, reference architecture, pi-ADL, Autonomic computing},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1145/3319008.3319015,
author = {Fu, Changlan and Zhang, He and Huang, Xin and Zhou, Xin and Li, Zhi},
title = {A Review of Meta-ethnographies in Software Engineering},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319015},
doi = {10.1145/3319008.3319015},
abstract = {Context: Data synthesis is one of the most significant tasks in Systematic Literature Review (SLR). Software Engineering (SE) researchers have adopted a variety of methods of synthesizing data that originated in other disciplines. One of the qualitative data synthesis methods is meta-ethnography, which is being used in SE SLRs. Objective: We aim at studying the adoption of meta-ethnography in SE SLRs in order to understand how this method has been used in SE. Method: We conducted a tertiary study of the use of meta-ethnography by reviewing sixteen SLRs. We carried out an empirical inquiry by integrating SLR and confirmatory email survey. Results: There is a general lack of knowledge, or even awareness, of different aspects of meta-ethnography and/or how to apply it. Conclusion: There is a need of investment in gaining in-depth knowledge and skills of correctly applying meta-ethnography in order to increase the quality and reliability of the findings generated from SE SLRs. Our study reveals that meta-ethnography is a suitable method to SE research. We discuss challenges and propose recommendations of adopting meta-ethnography in SE. Our effort also offers a preliminary checklist of the systematic considerations for doing meta-ethnography in SE and improving the quality of meta-ethnographic research in SE.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {68–77},
numpages = {10},
keywords = {systematic (literature) review, qualitative research synthesis, meta-ethnography},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@inproceedings{10.1145/2517208.2517213,
author = {Kolesnikov, Sergiy and von Rhein, Alexander and Hunsen, Claus and Apel, Sven},
title = {A comparison of product-based, feature-based, and family-based type checking},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517213},
doi = {10.1145/2517208.2517213},
abstract = {Analyzing software product lines is difficult, due to their inherent variability. In the past, several strategies for product-line analysis have been proposed, in particular, product-based, feature-based, and family-based strategies. Despite recent attempts to conceptually and empirically compare different strategies, there is no work that empirically compares all of the three strategies in a controlled setting. We close this gap by extending a compiler for feature-oriented programming with support for product-based, feature-based, and family-based type checking. We present and discuss the results of a comparative performance evaluation that we conducted on a set of 12 feature-oriented, Java-based product lines. Most notably, we found that the family-based strategy is superior for all subject product lines: it is substantially faster, it detects all kinds of errors, and provides the most detailed information about them.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {115–124},
numpages = {10},
keywords = {type checking, product-line analysis, fuji, feature-oriented programming},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@article{10.1016/j.jss.2016.06.068,
author = {Gholami, Mahdi Fahmideh and Daneshgar, Farhad and Low, Graham and Beydoun, Ghassan},
title = {Cloud migration process-A survey, evaluation framework, and open challenges},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.068},
doi = {10.1016/j.jss.2016.06.068},
abstract = {The relevant approaches for migrating legacy applications to the cloud are surveyed.An extensive analysis of existing approaches on the basis of a set of important criteria/features.Important cloud migration activities, techniques, and concerns that need to be properly addressed in a typical cloud migration process are delineated.Existing open issues and future research opportunities on the cloud migration research area are discussed. Moving mission-oriented enterprise software applications to cloud environments is a crucial IT task and requires a systematic approach. The foci of this paper is to provide a detailed review of extant cloud migration approaches from the perspective of the process model. To this aim, an evaluation framework is proposed and used to appraise and compare existing approaches for highlighting their features, similarities, and key differences. The survey distills the status quo and makes a rich inventory of important activities, recommendations, techniques, and concerns that are common in a typical cloud migration process in one place. This enables both academia and practitioners in the cloud computing community to get an overarching view of the process of the legacy application migration to the cloud. Furthermore, the survey identifies a number challenges that have not been yet addressed by existing approaches, developing opportunities for further research endeavours.},
journal = {J. Syst. Softw.},
month = oct,
pages = {31–69},
numpages = {39},
keywords = {Process model, Migration methodology, Legacy application, Evaluation framework, Cloud migration, Cloud computing}
}

@article{10.1016/j.scico.2021.102694,
author = {Liebrenz, Timm and Herber, Paula and Glesner, Sabine},
title = {Service-oriented decomposition and verification of hybrid system models using feature models and contracts},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {211},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2021.102694},
doi = {10.1016/j.scico.2021.102694},
journal = {Sci. Comput. Program.},
month = nov,
numpages = {25},
keywords = {Model-driven development, Theorem proving, Compositional verification, Hybrid systems}
}

@article{10.1504/IJMSO.2014.063133,
author = {Preschern, Christopher and Kajtazovic, Nermin and Kreiner, Christian},
title = {Efficient development and reuse of domain-specific languages for automation systems},
year = {2014},
issue_date = {July 2014},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {3},
issn = {1744-2621},
url = {https://doi.org/10.1504/IJMSO.2014.063133},
doi = {10.1504/IJMSO.2014.063133},
abstract = {Domain-Specific Languages DSLs help to decrease system development costs by providing developers with an effective way to construct systems for a specific domain. However, for DSL construction, a developer has to invest some upfront investment. If this investment is smaller than the benefit in terms of more effective development for domain-specific systems, then the construction of a DSL pays off. In order to decrease the initial effort to construct DSLs for a specific domain the automation domain, we present an efficient DSL architecture which allows structured reuse within the automation domain. With this DSL architecture, it is easier to build an initial automation DSL and when building multiple automation DSLs, significant parts of the DSL can be reused across different automation domains. We present the DSL architecture and discuss its benefits and drawbacks. Furthermore, we present and evaluate three automation DSL case studies which apply the described architecture.},
journal = {Int. J. Metadata Semant. Ontologies},
month = jul,
pages = {215–226},
numpages = {12}
}

@inproceedings{10.1145/2430502.2430522,
author = {von Rhein, Alexander and Apel, Sven and K\"{a}stner, Christian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {The PLA model: on the combination of product-line analyses},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430522},
doi = {10.1145/2430502.2430522},
abstract = {Product-line analysis has received considerable attention in the last decade. As it is often infeasible to analyze each product of a product line individually, researchers have developed analyses, called variability-aware analyses, that consider and exploit variability manifested in a code base. Variability-aware analyses are often significantly more efficient than traditional analyses, but each of them has certain weaknesses regarding applicability or scalability. We present the Product-Line-Analysis model, a formal model for the classification and comparison of existing analyses, including traditional and variability-aware analyses, and lay a foundation for formulating and exploring further, combined analyses. As a proof of concept, we discuss different examples of analyses in the light of our model, and demonstrate its benefits for systematic comparison and exploration of product-line analyses.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {14},
numpages = {8},
keywords = {software product lines, product-line analysis, PLA model},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@article{10.1016/j.jss.2019.110428,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Chen, Tao and Kazman, Rick},
title = {Run-time evaluation of architectures: A case study of diversification in IoT},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110428},
doi = {10.1016/j.jss.2019.110428},
journal = {J. Syst. Softw.},
month = jan,
numpages = {28},
keywords = {Design diversity, IoT, Internet of things, Software architectures for dynamic environments, Runtime architecture evaluation, Run-time architecture evaluation}
}

@inproceedings{10.5555/2337223.2337243,
author = {Siegmund, Norbert and Kolesnikov, Sergiy S. and K\"{a}stner, Christian and Apel, Sven and Batory, Don and Rosenm\"{u}ller, Marko and Saake, Gunter},
title = {Predicting performance via automated feature-interaction detection},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Customizable programs and program families provide user-selectable features to allow users to tailor a program to an application scenario. Knowing in advance which feature selection yields the best performance is difficult because a direct measurement of all possible feature combinations is infeasible. Our work aims at predicting program performance based on selected features. However, when features interact, accurate predictions are challenging. An interaction occurs when a particular feature combination has an unexpected influence on performance. We present a method that automatically detects performance-relevant feature interactions to improve prediction accuracy. To this end, we propose three heuristics to reduce the number of measurements required to detect interactions. Our evaluation consists of six real-world case studies from varying domains (e.g., databases, encoding libraries, and web servers) using different configuration techniques (e.g., configuration files and preprocessor flags). Results show an average prediction accuracy of 95%.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {167–177},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1109/WI-IAT.2009.363,
author = {Li, Mu and Huai, JinPeng and Guo, HuiPeng},
title = {An Adaptive Web Services Selection Method Based on the QoS Prediction Mechanism},
year = {2009},
isbn = {9780769538013},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2009.363},
doi = {10.1109/WI-IAT.2009.363},
abstract = {In recent years, many QoS-based web service selection methods have been proposed. However, as QoS changes dynamically, the atomic services of a composite web service could be replaced with other ones that have better quality. The performance of a composite web service will be decreased if this replacement happens frequently in runtime. Predicting the change of QoS accurately in select phase can effectively reduce this web services “thrash”. In this paper, we propose a web service selection algorithm GFS (Goodness-Fit Selection algorithm) based on QoS prediction mechanism in dynamic environments. We use structural equation to model the QoS measurement of web services. By taking the advantage of the prediction mechanism of structural equation model, we can quantitatively predict the change of quality of service dynamically. Optimal web service is selected based on the predicted results. Simulation results show that in dynamic environments, GFS provides higher selection accuracy than previous selection methods.},
booktitle = {Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {395–402},
numpages = {8},
keywords = {web service selection, prediction, Structural Equation Modeling, QoS},
series = {WI-IAT '09}
}

@inproceedings{10.1007/978-3-030-27455-9_4,
author = {Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Farah, Paulo Roberto and Vergilio, Silvia Regina and Guizzo, Giovani},
title = {A Review of Ten Years of the Symposium on Search-Based Software Engineering},
year = {2019},
isbn = {978-3-030-27454-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27455-9_4},
doi = {10.1007/978-3-030-27455-9_4},
abstract = {The year 2018 marked the tenth anniversary of the Symposium on Search Based Software Engineering (SSBSE). In order to better understand the characteristics and evolution of papers published in SSBSE, this work reports results from a mapping study targeting the ten proceedings of SSBSE. Our goal is to identify and to analyze authorship collaborations, the impact and relevance of SSBSE in terms of citations, the software engineering areas commonly studied as well as the new problems recently solved, the computational intelligence techniques preferred by authors and the rigour of experiments conducted in the papers. Besides this analysis, we list some recommendations to new authors who envisage to publish their work in SSBSE. Despite of existing mapping studies on SBSE, our contribution in this work is to provide information to researchers and practitioners willing to enter the SBSE field, being a source of information to strengthen the symposium, guide new studies, and motivate new collaboration among research groups.},
booktitle = {Search-Based Software Engineering: 11th International Symposium, SSBSE 2019, Tallinn, Estonia, August 31 – September 1, 2019, Proceedings},
pages = {42–57},
numpages = {16},
keywords = {Bibliometric analysis, SBSE, Systematic mapping},
location = {Tallinn, Estonia}
}

@inproceedings{10.1145/3361525.3361544,
author = {Ni, Xiang and Schneider, Scott and Pavuluri, Raju and Kaus, Jonathan and Wu, Kun-Lung},
title = {Automating Multi-level Performance Elastic Components for IBM Streams},
year = {2019},
isbn = {9781450370097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361525.3361544},
doi = {10.1145/3361525.3361544},
abstract = {Streaming applications exhibit abundant opportunities for pipeline parallelism, data parallelism and task parallelism. Prior work in IBM Streams introduced an elastic threading model that sought the best performance by automatically tuning the number of threads. In this paper, we introduce the ability to automatically discover where that threading model is profitable. However this introduces a new challenge: we have separate performance elastic mechanisms that are designed with different objectives, leading to potential negative interactions and unintended performance degradation. We present our experiences in overcoming these challenges by showing how to coordinate separate but interfering elasticity mechanisms to maxmize performance gains with stable and fast parallelism exploitation. We first describe an elastic performance mechanism that automatically adapts different threading models to different regions of an application. We then show a coherent ecosystem for coordinating this threading model elasticty with thread count elasticity. This system is an online, stable multi-level elastic coordination scheme that adapts different regions of a streaming application to different threading models and number of threads. We implemented this multi-level coordination scheme in IBM Streams and demonstrated that it (a) scales to over a hundred threads; (b) can improve performance by an order of magnitude on two different processor architectures when an application can benefit from multiple threading models; and (c) achieves performance comparable to hand-optimized applications but with much fewer threads.},
booktitle = {Proceedings of the 20th International Middleware Conference},
pages = {163–175},
numpages = {13},
keywords = {runtime, elastic scheduling, Stream processing},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@inproceedings{10.1145/1842752.1842810,
author = {Hilliard, Rich},
title = {On representing variation},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842810},
doi = {10.1145/1842752.1842810},
abstract = {Although primarily studied in the context of product lines, variability is a key fact about most systems and therefore a concern for the architectures of those systems. Thus it is essential for the Architect to have suitable tools for representing, managing and reasoning about variation. Most work on product line variation has focused on the variability of components and their connectors within an architecture. Meanwhile, Architects today often use multiple viewpoints to frame diverse stakeholders' concerns for an architecture. How can variation be expressed within the representational paradigm of multiple viewpoints? This paper uses a simplified model of variation reflecting current practice and explores the consequences of that model for the representation of variation as a part of architecture description, using the conceptual foundation of ISO/IEC 42010 (the revision of IEEE 1471:2000) and poses a number of questions for discussion at the VARI-ARCH workshop.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {312–315},
numpages = {4},
keywords = {variation, features, concerns, architecture description},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@article{10.1145/1366546.1366547,
author = {G\'{e}rard, S\'{e}bastien and Feiler, Peter and Rolland, Jean-Francois and Filali, Mamoun and Reiser, Mark-Oliver and Delanote, Didier and Berbers, Yolande and Pautet, Laurent and Perseil, Isabelle},
title = {UML&amp;AADL '2007 grand challenges},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/1366546.1366547},
doi = {10.1145/1366546.1366547},
abstract = {On today's sharply competitive industrial market, engineers must focus on their core competencies to produce ever more innovative products, while also reducing development times and costs. This has further heightened the complexity of the development process. At the same time, industrial systems, and specifically real-time embedded systems, have become increasingly software-intensive. New software development approaches and methods must therefore be found to free engineers from the even more complex technical constraints of development and to enable them to concentrate on their core business specialties. One emerging solution is to foster model-based development by defining modeling artifacts well-suited to their domain concerns instead of asking them to write code. However, model-driven approaches will be solutions to the previous issues only if models evolves from a contemplative role to a productive role within the development processes. In this context, model transformation is a key design paradigm that will foster this revolution. This paper is the result of discussions and exchanges that took place within the second edition of the workshop "UML&amp;AADL" (http://www.artist-embedded.org/artist/Topics.html) that-was hold in 2007 in Auckland, New Zealand, in conjunction with the ICECCS07 conference. The purpose of this workshop was to gather people of both communities from UML (including its domain specific extensions, with a focus on MARTE) and AADL (including its annexes) in order to foster sharing of results and experiments. More specially this year, the focus was on how both standards do subscribe to the model driven engineering paradigm, or to be more precise, how MDE may ease and foster the usage of both sets of standards for developing real-time embedded systems. This paper will show that, even if the work is not yet finished, the current results seems to be already very promising.},
journal = {SIGBED Rev.},
month = oct,
articleno = {1},
numpages = {1},
keywords = {xUML, real-time, embedded, UML, TLA+, MDE, MDD, MDA, MARTE, ADL, AADL}
}

@inproceedings{10.5555/2819009.2819038,
author = {Schroeder, Jan and Holzner, Daniela and Berger, Christian and Hoel, Carl-Johan and Laine, Leo and Magnusson, Anders},
title = {Design and evaluation of a customizable multi-domain reference architecture on top of product lines of self-driving heavy vehicles: an industrial case study},
year = {2015},
publisher = {IEEE Press},
abstract = {Self-driving vehicles for commercial use cases like logistics or overcast mines increase their owners' economic competitiveness. Volvo maintains, evolves, and distributes a vehicle control product line for different brands like Volvo Trucks, Renault, and Mack in more than 190 markets world-wide. From the different application domains of their customers originates the need for a multi-domain reference architecture concerned with transport mission planning, execution, and tracking on top of the vehicle control product line. This industrial case study is the first of its kind reporting about the systematic process to design such a reference architecture involving all relevant external and internal stakeholders, development documents, low-level artifacts, and literature. Quantitative and qualitative metrics were applied to evaluate non-functional requirements on the reference architecture level before a concrete variant was evaluated using a Volvo FMX truck in an exemplary construction site setting.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {189–198},
numpages = {10},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.5555/2664360.2664386,
author = {Lobato, Luanna Lopes and Silveira Neto, Paulo Anselmo da Mota and Machado, Ivan do Carmo and de Alemida, Eduardo Santana and Meira, Silvio Romero de Lemos},
title = {Risk management in software product lines: an industrial case study},
year = {2012},
isbn = {9781467323529},
publisher = {IEEE Press},
abstract = {Software Product Lines (SPL) adoption can affect several aspects of an organization and it involves significant investment and risk. This way, SPL risk management is a crucial activity of SPL adoption. This study aims to identify SPL risks during the scoping and requirement disciplines to provide information to better understand risk management in SPL. In order to achieve the previous stated goal, a case study research was applied in an industrial project in the medical information management domain. Using the captured risks, a classification scheme was built and risk mitigation strategies were identified. We spent five months, totaling 79 hours, performing risk management (RM) in the scoping discipline and twelve months, totaling 148 hours, performing RM on the requirements discipline. We identified 32 risks during the scoping discipline and 20 risks during the requirements discipline, 14 risks occurred in both disciplines. Some identified risks are not particular to SPL development, however, they have their impact increased due to the SPL characteristic. All the study results and lessons learned are useful for all project managers and researchers who are considering the introduction of SPL risk management in industry or academia.},
booktitle = {Proceedings of the International Conference on Software and System Process},
pages = {180–189},
numpages = {10},
keywords = {software product lines, software engineering, risk management, case study},
location = {Zurich, Switzerland},
series = {ICSSP '12}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.5555/2041790.2041818,
author = {Nakagawa, Elisa Yumi and Antonino, Pablo Oliveira and Becker, Martin},
title = {Reference architecture and product line architecture: a subtle but critical difference},
year = {2011},
isbn = {9783642237973},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Currently, the size and complexity of software systems, as well as critical time to market, demand new approaches from Software Engineering discipline for building such systems. In this context, the use of reference architectures and product line architectures is becoming a common practice. However, both of these concepts are sometimes mistakenly seen as the same thing; it is also not clearly established how they can be explored in a complementary way in order to contribute to software development. The main contribution of this paper is to make a clear differentiation between these architectures, by investigating and establishing definitions for each of them. Based on this, we also propose the use of reference architectures as a basis for product line architectures. As a result, a better understanding of both reference architectures and product line architectures, as well as an understanding of how to explore them jointly, can contribute to promoting more effective reuse in the development of software systems.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture},
pages = {207–211},
numpages = {5},
location = {Essen, Germany},
series = {ECSA'11}
}

@inproceedings{10.1007/978-3-319-26844-6_32,
author = {Brink, Christopher and Heisig, Philipp and Sachweh, Sabine},
title = {Using Cross-Dependencies During Configuration of System Families},
year = {2015},
isbn = {9783319268439},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26844-6_32},
doi = {10.1007/978-3-319-26844-6_32},
abstract = {Nowadays, the automotive industry uses software product lines to support the management and maintenance of software variants. However, the development of mechatronic systems includes not merely software, but also other system parts like operating system, hardware or even mechanical parts. We call a combination of these system parts a system family SF. This combination raises the question how different variable system parts can be modeled and used for a combined configuration in a flexible way. We argue that a modeling process should combine all of these system parts, while the product configuration has to consider dependencies between them. Based on our previous work, we address this question and discuss dependencies between different system parts.},
booktitle = {Proceedings of the 16th International Conference on Product-Focused Software Process Improvement - Volume 9459},
pages = {439–452},
numpages = {14},
keywords = {Systems, System families, Product lines, Hardware/software, Feature models, Dependencies},
location = {Bolzano, Italy},
series = {PROFES 2015}
}

@inproceedings{10.5555/2666064.2666067,
author = {Annosi, Maria Carmela and Di Penta, Massimiliano and Tortora, Genny},
title = {Managing and assessing the risk of component upgrades},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {This paper describes the experience, carried out by Ericsson Telecomunicazioni S.p.A (Italy), in managing the migration of their legacy products towards a product line approach and, specifically, how the update of third-party software products is handled in such product lines. The paper describes the Ericsson application scenario in the development and evolution of network management products. Then, it provides an overview of how the company adopts (i) an internal toolkit to manage third party software products, with the aim of determining the impact of their updates upon variants of the network management system, and (ii) a risk management framework, which helps the developer to decide whether and when update third-party products.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {9–12},
numpages = {4},
keywords = {third party components, software reuse, software product lines, code provenance},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@article{10.1016/j.scico.2012.06.008,
author = {Eklund, Ulrik and Gustavsson, H\r{a}kan},
title = {Architecting automotive product lines: Industrial practice},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.06.008},
doi = {10.1016/j.scico.2012.06.008},
abstract = {This paper presents an in-depth view of how architects work with maintaining product line architectures at two internationally well-known automotive companies. The case study shows several interesting results. The process of managing architectural changes as well as the information the architects maintain and update is surprisingly similar between the two companies, despite that one has a strong line organisation and the other a strong project organisation. The architecting process found does not differ from what can be seen in other business domains. What does differ is that the architects studied see themselves interacting much more with other stakeholders than architects in general. The actual architectures are based on similar technology, e.g. CAN, but the network topology, S/W deployment and interfaces are totally different. The results indicate how the company's different core values influence the architects when defining and maintaining the architectures over time. One company maintains four similar architectures in parallel, each at a different stage in their respective life-cycle, while the other has a single architecture for all products since 2002. The organisational belonging of the architects in the former company has been turbulent in contrast to the latter and there is some speculation if this is correlated.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {2347–2359},
numpages = {13},
keywords = {Process, Case study, Automotive industry, Architecting}
}

@inproceedings{10.1109/ASE.2013.6693104,
author = {Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
title = {Scalable product line configuration: a straw to break the camel's back},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693104},
doi = {10.1109/ASE.2013.6693104},
abstract = {Software product lines are hard to configure. Techniques that work for medium sized product lines fail for much larger product lines such as the Linux kernel with 6000+ features. This paper presents simple heuristics that help the Indicator-Based Evolutionary Algorithm (IBEA) in finding sound and optimum configurations of very large variability models in the presence of competing objectives. We employ a combination of static and evolutionary learning of model structure, in addition to utilizing a pre-computed solution used as a "seed" in the midst of a randomly-generated initial population. The seed solution works like a single straw that is enough to break the camel's back -given that it is a feature-rich seed. We show promising results where we can find 30 sound solutions for configuring upward of 6000 features within 30 minutes.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {465–474},
numpages = {10},
keywords = {variability models, multiobjective optimization, evolutionary algorithms, automated configuration, SMT solvers},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1007/978-3-642-37057-1_7,
author = {Rubin, Julia and Chechik, Marsha},
title = {Quality of merge-refactorings for product lines},
year = {2013},
isbn = {9783642370564},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-37057-1_7},
doi = {10.1007/978-3-642-37057-1_7},
abstract = {In this paper, we consider the problem of refactoring related software products specified in UML into annotative product line representations. Our approach relies on identifying commonalities and variabilities in existing products and further merging those into product line representations which reduce duplications and facilitate reuse. Varying merge strategies can lead to producing several semantically correct, yet syntactically different refactoring results. Depending on the goal of the refactoring, one result can be preferred to another. We thus propose to capture the goal using a syntactic quality function and use that function to guide the merge strategy. We define and implement a quality-based merge-refactoring framework for UML models containing class and statechart diagrams and report on our experience applying it on three case-studies.},
booktitle = {Proceedings of the 16th International Conference on Fundamental Approaches to Software Engineering},
pages = {83–98},
numpages = {16},
location = {Rome, Italy},
series = {FASE'13}
}

@inproceedings{10.1145/2245276.2231961,
author = {de Oliveira, Andr\'{e} Luiz and Ferrari, Fabiano Cuttigi and Penteado, Ros\^{a}ngela A. Dellosso and de Camargo, Valter Vieira},
title = {Investigating framework product lines},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231961},
doi = {10.1145/2245276.2231961},
abstract = {Frameworks are tools that promote the reuse of pieces of software within specific domains. An intrinsic property of frameworks is the large amount of intertwined code found across its several modules. This configures an architecture whose modules can hardly be decoupled. Consequently, an application derived from a framework usually carries on the full framework architecture, irrespective of the subset of application requirements. This compromises the maintainability, evolution and reusability of both framework and applications derived from it. To deal with this problem, this paper introduces the concept of Framework Product Lines (FPL). In a FPL, each member - or configuration -- is a framework that contains only a subset of the FPL features according to the application requirements and rules that constrain their composition. Thus, this paper presents the framework product lines concept and shows its use for evolving an application framework towards FPL. Results show preliminary gains in terms of reusability and maintainability in both evolved framework and applications derived from it.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1177–1182},
numpages = {6},
keywords = {software product lines, software architectures, reusability, maintainability, framework product lines, framework},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/2851613.2851941,
author = {Vidal, Santiago and Guimaraes, Everton and Oizumi, Willian and Garcia, Alessandro and Pace, Andr\'{e}s D\'{\i}az and Marcos, Claudia},
title = {On the criteria for prioritizing code anomalies to identify architectural problems},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851941},
doi = {10.1145/2851613.2851941},
abstract = {Architectural problems constantly affect evolving software projects. When not properly addressed, those problems can hinder the longevity of a software system. Some studies revealed that a wide range of architectural problems are reflected in source code through code anomalies. However, a software project often contains thousands of code anomalies and many of them have no relation to architectural problems. As a consequence, developers struggle to effectively determine which (groups of) anomalies are architecturally relevant. This work proposes criteria for prioritizing groups of code anomalies as indicators of architectural problems in evolving systems.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1812–1814},
numpages = {3},
keywords = {architectural problems, code anomalies, software maintenance},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1007/978-3-642-30982-3_8,
author = {Becker, Steffen},
title = {Model transformations in non-functional analysis},
year = {2012},
isbn = {9783642309816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30982-3_8},
doi = {10.1007/978-3-642-30982-3_8},
abstract = {The quality assessment of software design models in early development phases can prevent wrong design decisions on the architectural level. As such wrong decisions are usually very cost-intensive to revert in late testing phases, model-driven quality predictions offer early quality estimates to prevent such erroneous decisions. By model-driven quality predictions we refer to analyses which run fully automated based on model-driven methods and tools. In this paper, we give an overview on the process of model-driven quality analyses used today with a special focus on issues that arise in fully automated approaches.},
booktitle = {Proceedings of the 12th International Conference on Formal Methods for the Design of Computer, Communication, and Software Systems: Formal Methods for Model-Driven Engineering},
pages = {263–289},
numpages = {27},
keywords = {reliability, performance, palladio component model, model-driven quality analyses, MARTE},
location = {Bertinoro, Italy},
series = {SFM'12}
}

@inproceedings{10.1109/WI.2006.173,
author = {Zhou, Jiehan and Niemela, Eila},
title = {Toward Semantic QoS Aware Web Services: Issues, Related Studies and Experience},
year = {2006},
isbn = {0769527477},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2006.173},
doi = {10.1109/WI.2006.173},
abstract = {Semantic QoSaware Web services incorporating the emerging Web services in the QoSaware system development are promoting ServiceOriented Software Engineering (SOSE). To identify the steps toward semantic quality of service (QoS)aware Web services, this paper examines previous studies related to semantic QoSaware Web services, including QoSaware Web service architectures, QoS classification, QoS ontology, QoS specification languages, and Web service creation tools. Moreover, a case study is presented to discuss the gaps between our current quality driven software development approach and the semantic QoSaware Web services.},
booktitle = {Proceedings of the 2006 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {553–557},
numpages = {5},
series = {WI '06}
}

@article{10.1016/j.jss.2006.08.039,
author = {Kuz, Ihor and Liu, Yan and Gorton, Ian and Heiser, Gernot},
title = {CAmkES: A component model for secure microkernel-based embedded systems},
year = {2007},
issue_date = {May, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {80},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2006.08.039},
doi = {10.1016/j.jss.2006.08.039},
abstract = {Component-based software engineering promises to provide structure and reusability to embedded-systems software. At the same time, microkernel-based operating systems are being used to increase the reliability and trustworthiness of embedded systems. Since the microkernel approach to designing systems is partially based on the componentisation of system services, component-based software engineering is a particularly attractive approach to developing microkernel-based systems. While a number of widely used component architectures already exist, they are generally targeted at enterprise computing rather than embedded systems. Due to the unique characteristics of embedded systems, a component architecture for embedded systems must have low overhead, be able to address relevant non-functional issues, and be flexible to accommodate application specific requirements. In this paper we introduce a component architecture aimed at the development of microkernel-based embedded systems. The key characteristics of the architecture are that it has a minimal, low-overhead, core but is highly modular and therefore flexible and extensible. We have implemented a prototype of this architecture and confirm that it has very low overhead and is suitable for implementing both system-level and application level services.},
journal = {J. Syst. Softw.},
month = may,
pages = {687–699},
numpages = {13},
keywords = {Microkernel, Embedded system, Component architecture}
}

@inproceedings{10.5555/1926743.1926758,
author = {Ovaska, Eila},
title = {Ontology driven piecemeal development of smart spaces},
year = {2010},
isbn = {3642169163},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software development is facing new challenges due to transformation from product based software engineering towards integration and collaboration based software engineering that embodies high degree of dynamism both at design time and run time. Short time-to-markets require cost reduction by maximizing software reuse; openness for new innovations presumes a flexible innovation platform and agile software development; and user satisfaction assumes high quality in a situation based manner. How to deal with these contradictory requirements in software engineering? The main contribution of this paper is a novel approach that is influenced by business innovation, human centered design, model driven development and ontology oriented design. The approach is called Ontology driven Piecemeal Software Engineering (OPSE). OPSE facilitates incremental software development based on software pieces that follow the design principles defined by means of ontologies. Its key elements are abstraction, aggregation and adaptivity. The approach is intended for and applied to the development of smart spaces.},
booktitle = {Proceedings of the First International Joint Conference on Ambient Intelligence},
pages = {148–156},
numpages = {9},
keywords = {smart space, ontology, interoperability, context awareness, MDD},
location = {Malaga, Spain},
series = {AmI'10}
}

@article{10.1016/j.infsof.2011.09.003,
author = {Conejero, Jos\'{e} M. and Figueiredo, Eduardo and Garcia, Alessandro and Hern\'{a}ndez, Juan and Jurado, Elena},
title = {On the relationship of concern metrics and requirements maintainability},
year = {2012},
issue_date = {February, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.09.003},
doi = {10.1016/j.infsof.2011.09.003},
abstract = {Context: Maintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability. Objective: In this setting, this paper introduces an empirical analysis where the correlation between crosscutting properties and two ISO/IEC 9126 maintainability attributes, namely changeability and stability, is presented. Method: This correlation is based on the utilization of a set of concern metrics that allows the quantification of crosscutting, scattering and tangling. Results: Our study confirms that a change in a crosscutting concern is more difficult to be accomplished and that artefacts addressing crosscutting concerns are found to be less stable later as the system evolves. Moreover, our empirical analysis reveals that crosscutting properties introduce non-syntactic dependencies between software artefacts, thereby decreasing the quality of software in terms of changeability and stability as well. These subtle dependencies cannot be easily detected without the use of concern metrics. Conclusion: The correlation provides evidence that the presence of certain crosscutting properties negatively affects to changeability and stability. The whole analysis is performed using as target cases three software product lines, where maintainability properties are of upmost importance not only for individual products but also for the core architecture of the product line.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {212–238},
numpages = {27},
keywords = {Stability, Requirements engineering, Product lines, Maintainability, Crosscutting, Concern metrics}
}

@inproceedings{10.1007/978-3-642-54804-8_7,
author = {Kowal, Matthias and Schaefer, Ina and Tribastone, Mirco},
title = {Family-Based Performance Analysis of Variant-Rich Software Systems},
year = {2014},
isbn = {9783642548031},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-54804-8_7},
doi = {10.1007/978-3-642-54804-8_7},
abstract = {We study models of software systems with variants that stem from a specific choice of configuration parameters with a direct impact on performance properties. Using UML activity diagrams with quantitative annotations, we model such systems as a product line. The efficiency of a product-based evaluation is typically low because each product must be analyzed in isolation, making difficult the re-use of computations across variants. Here, we propose a family-based approach based on symbolic computation. A numerical assessment on large activity diagrams shows that this approach can be up to three orders of magnitude faster than product-based analysis in large models, thus enabling computationally efficient explorations of large parameter spaces.},
booktitle = {Proceedings of the 17th International Conference on Fundamental Approaches to Software Engineering - Volume 8411},
pages = {94–108},
numpages = {15}
}

@inproceedings{10.1145/1134650.1134678,
author = {Pandey, Raju and Wu, Jeffrey},
title = {BOTS: a constraint-based component system for synthesizing scalable software systems},
year = {2006},
isbn = {159593362X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134650.1134678},
doi = {10.1145/1134650.1134678},
abstract = {Embedded application developers create applications for a wide range of devices with different resource constraints. Developers want to maximize the use of the limited resources available on the device while still not exceeding the capabilities of the device. To do this, the developer must be able to scale his software for different platforms. In this paper, we present a software engineering methodology that automatically scales software to different platforms. We intend to have the application developer write high level functional specifications of his software and have tools that automatically scale the underlying runtime. These tools will use the functional and non-functional constraints of both the hardware and client application to produce an appropriate runtime. Our initial results show that the proposed approach can scale operating systems and virtual machines that satisfy the constraints of varying hardware/application combinations.},
booktitle = {Proceedings of the 2006 ACM SIGPLAN/SIGBED Conference on Language, Compilers, and Tool Support for Embedded Systems},
pages = {189–198},
numpages = {10},
keywords = {wireless sensor networks, runtime systems, generative programming, embedded systems, constraints, components},
location = {Ottawa, Ontario, Canada},
series = {LCTES '06}
}

@inproceedings{10.1145/2668930.2688051,
author = {Hork\'{y}, Vojt\v{e}ch and Libi\v{c}, Peter and Marek, Luk\'{a}\v{s} and Steinhauser, Antonin and T\r{u}ma, Petr},
title = {Utilizing Performance Unit Tests To Increase Performance Awareness},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688051},
doi = {10.1145/2668930.2688051},
abstract = {Many decisions taken during software development impact the resulting application performance. The key decisions whose potential impact is large are usually carefully weighed. In contrast, the same care is not used for many decisions whose individual impact is likely to be small -- simply because the costs would outweigh the benefits. Developer opinion is the common deciding factor for these cases, and our goal is to provide the developer with information that would help form such opinion, thus preventing performance loss due to the accumulated effect of many poor decisions.Our method turns performance unit tests into recipes for generating performance documentation. When the developer selects an interface and workload of interest, relevant performance documentation is generated interactively. This increases performance awareness -- with performance information available alongside standard interface documentation, developers should find it easier to take informed decisions even in situations where expensive performance evaluation is not practical. We demonstrate the method on multiple examples, which show how equipping code with performance unit tests works.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {289–300},
numpages = {12},
keywords = {performance testing, performance documentation, performance awareness, javadoc, java},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@inproceedings{10.1145/3357141.3357147,
author = {Sousa, Bruno L. and Bigonha, Mariza A. S. and Ferreira, Kecia A. M.},
title = {Analysis of Coupling Evolution on Open Source Systems},
year = {2019},
isbn = {9781450376372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357141.3357147},
doi = {10.1145/3357141.3357147},
abstract = {Software evolution is an intrinsic process of the software life cycle. The comprehension of this process is a central research topic in Software Engineering. It is widely accepted that as a software system evolves, its internal quality declines, and its complexity increases. However, there is a gap in the comprehension of how this process occurs in a fine-grained view. In this work, we apply a software metric approach to investigate how the internal quality of object-oriented software systems evolves in the aspect of coupling. More specifically, we analyze (i) how the coupling behavior may be described over the software evolution, (ii) how the coupling behavior affects the reusability and complexity of the systems, and (iii) the percentage of classes from the systems that directly impacts on the coupling evolution. The results and observations of this study are compiled in seven properties of coupling evolution, among which stand out: (i) the coupling behavior is better modeled by a cubic function, (ii) the coupling evolution tends to increase the complexity of the systems, (iii) the systems tend to be designed with a high level of complexity, and (iv) the coupling evolution is affected by a small group of classes.},
booktitle = {Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {23–32},
numpages = {10},
keywords = {software quality, software metrics, software evolution, open source, object-orientation, coupling},
location = {Salvador, Brazil},
series = {SBCARS '19}
}

@inproceedings{10.1145/1370062.1370078,
author = {Espinoza, Huascar and Servat, David and G\'{e}rard, S\'{e}bastien},
title = {Leveraging analysis-aided design decision knowledge in UML-based development of embedded systems},
year = {2008},
isbn = {9781605580388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370062.1370078},
doi = {10.1145/1370062.1370078},
abstract = {Many important works have been carried out to provide modeling languages (e.g., UML, SDL) with expressiveness to support embedded system design, validation and verification. A fundamental shortcoming in current model-driven approaches is the inability to explicitly capture design decisions and trade-offs between different non-functional parameters, among which timeliness, memory usage, and power consumption are of primary interest. This paper highlights technical limitations in UML to specify complex non-functional evaluation scenarios of candidate architectures, and outlines our current work to provide straightforward solutions.},
booktitle = {Proceedings of the 3rd International Workshop on Sharing and Reusing Architectural Knowledge},
pages = {55–62},
numpages = {8},
keywords = {trade-off analysis, model-driven engineering, embedded systems, design space exploration, UML},
location = {Leipzig, Germany},
series = {SHARK '08}
}

@inproceedings{10.1145/2993236.2993249,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {A feature-based personalized recommender system for product-line configuration},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993249},
doi = {10.1145/2993236.2993249},
abstract = {Today’s competitive marketplace requires the industry to understand unique and particular needs of their customers. Product line practices enable companies to create individual products for every customer by providing an interdependent set of features. Users configure personalized products by consecutively selecting desired features based on their individual needs. However, as most features are interdependent, users must understand the impact of their gradual selections in order to make valid decisions. Thus, especially when dealing with large feature models, specialized assistance is needed to guide the users in configuring their product. Recently, recommender systems have proved to be an appropriate mean to assist users in finding information and making decisions. In this paper, we propose an advanced feature recommender system that provides personalized recommendations to users. In detail, we offer four main contributions: (i) We provide a recommender system that suggests relevant features to ease the decision-making process. (ii) Based on this system, we provide visual support to users that guides them through the decision-making process and allows them to focus on valid and relevant parts of the configuration space. (iii) We provide an interactive open-source configurator tool encompassing all those features. (iv) In order to demonstrate the performance of our approach, we compare three different recommender algorithms in two real case studies derived from business experience.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {120–131},
numpages = {12},
keywords = {Software Product Lines, Recommenders, Product-Line Configuration, Personalized Recommendations},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.5555/2045753.2045804,
author = {Zhou, Jingang and Zhao, Dazhe and Liu, Jiren},
title = {A domain specific language for interactive enterprise application development},
year = {2011},
isbn = {9783642239816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Web-based enterprise applications (EAs) have become the mainstream for business systems; however, there are enormous challenges for EAs development to meet the software quality and delivery deadline. In this paper, we propose a domain specific language, called WL4EA, which combines components with generative reuse and targets for popular application frameworks (or platform) and supports high interactivity. With WL4EA, an EA can be declaratively specified as some sets of entities, views, business objects, and data access objects. Such language elements will be composed according to known EA architecture and patterns. Such a DSL and code generation can lower the development complexity and error proneness and improve efficiency.},
booktitle = {Proceedings of the 2011 International Conference on Web Information Systems and Mining - Volume Part II},
pages = {351–360},
numpages = {10},
keywords = {web Application, generative programming, enterprise application, domain specific language},
location = {Taiyuan, China},
series = {WISM'11}
}

@inproceedings{10.5555/1885639.1885701,
author = {McGregor, John D.},
title = {The many paths to quality core assets},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {502},
numpages = {1},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/3324884.3416620,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Mastering uncertainty in performance estimations of configurable software systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416620},
doi = {10.1145/3324884.3416620},
abstract = {Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {684–696},
numpages = {13},
keywords = {P4, configurable software systems, performance-influence modeling, probabilistic programming},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2884781.2884868,
author = {Oizumi, Willian and Garcia, Alessandro and da Silva Sousa, Leonardo and Cafeo, Bruno and Zhao, Yixue},
title = {Code anomalies flock together: exploring code anomaly agglomerations for locating design problems},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884868},
doi = {10.1145/2884781.2884868},
abstract = {Design problems affect every software system. Diverse software systems have been discontinued or reengineered due to design problems. As design documentation is often informal or nonexistent, design problems need to be located in the source code. The main difficulty to identify a design problem in the implementation stems from the fact that such problem is often scattered through several program elements. Previous work assumed that code anomalies -- popularly known as code smells -- may provide sufficient hints about the location of a design problem. However, each code anomaly alone may represent only a partial embodiment of a design problem. In this paper, we hypothesize that code anomalies tend to "flock together" to realize a design problem. We analyze to what extent groups of inter-related code anomalies, named agglomerations, suffice to locate design problems. We analyze more than 2200 agglomerations found in seven software systems of different sizes and from different domains. Our analysis indicates that certain forms of agglomerations are consistent indicators of both congenital and evolutionary design problems, with accuracy often higher than 80%.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {440–451},
numpages = {12},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2660190.2660191,
author = {Kolesnikov, Sergiy and Roth, Judith and Apel, Sven},
title = {On the relation between internal and external feature interactions in feature-oriented product lines: a case study},
year = {2014},
isbn = {9781450329804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660190.2660191},
doi = {10.1145/2660190.2660191},
abstract = {The feature-interaction problem has been explored for many years. Still, we lack sufficient knowledge about the interplay of different kinds of interactions in software product lines. Exploring the relations between different kinds of feature interactions will allow us to learn more about the nature of interactions and their causes. This knowledge can then be applied for improving existing approaches for detecting, managing, and resolving feature interactions. We present a framework for studying relations between different kinds of interactions. Furthermore, we report and discuss the results of a preliminary study in which we examined correlations between internal feature interactions (quantified by a set of software measures) and external feature interactions (represented by product-line-specific type errors). We performed the evaluation on a set of 15 feature-oriented, Java-based product lines. We observed moderate correlations between the interactions under discussion. This gives us confidence that we can apply our approach to studying other types of external feature interactions (e.g., performance interactions).},
booktitle = {Proceedings of the 6th International Workshop on Feature-Oriented Software Development},
pages = {1–8},
numpages = {8},
keywords = {software measures, feature-oriented software development, feature interactions},
location = {V\"{a}ster\r{a}s, Sweden},
series = {FOSD '14}
}

@inproceedings{10.1145/3357141.3357142,
author = {Oliveira, Anderson and Sousa, Leonardo and Oizumi, Willian and Garcia, Alessandro},
title = {On the Prioritization of Design-Relevant Smelly Elements: A Mixed-Method, Multi-Project Study},
year = {2019},
isbn = {9781450376372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357141.3357142},
doi = {10.1145/3357141.3357142},
abstract = {Software systems are likely to face what is called design problems. Given the typical lack of design documentation, developers have to rely on implementation-level symptoms, the so-called code smells, to identify and remove design problems. A code smell is a microstructure in the program that can indicate the presence of a design problem. Large programs have hundreds or thousands of program elements (e.g., classes) in which a significant proportion may be affected by smells. Consequently, due to time constraints and the large number of elements, developers have to prioritize the designrelevant program elements, i.e., locate a shortlist of elements that are suspects of having design-relevant smells. However, this task is hard and time-consuming. Unfortunately, the literature fails to provide developers with effective heuristics that automate such prioritization task. The objective of this paper is to propose heuristics that effectively locate a shortlist of design-relevant smelly program elements. For this purpose, we report two studies. In the first one, we investigated the criteria that developers used in practice to accurately prioritize design-relevant smelly elements. Based on these criteria, we derived a preliminary suite of prioritization heuristics. Since we do not know if the heuristics are suitable for an effective prioritization across multiple projects, we performed a second study to evaluate the proposed heuristics. We found that two out of nine heuristics reached an average precision higher than 75% for the four projects we analyzed. Thus, our findings suggest these heuristics are promising to support developers in prioritizing design-relevant smelly elements.},
booktitle = {Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {83–92},
numpages = {10},
keywords = {prioritization, heuristics, design problems},
location = {Salvador, Brazil},
series = {SBCARS '19}
}

@inproceedings{10.1145/2024436.2024438,
author = {Calinescu, Radu},
title = {When the requirements for adaptation and high integrity meet},
year = {2011},
isbn = {9781450308533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024436.2024438},
doi = {10.1145/2024436.2024438},
abstract = {Two classes of software that are notoriously difficult to develop on their own are rapidly merging into one. This will affect every key service that we rely upon in modern society, yet a successful merge is unlikely to be achievable using software development techniques specific to either class.This paper explains the growing demand for software capable of both self-adaptation and high integrity, and advocates the use of a collection of "@runtime" techniques for its development, operation and management. We summarise early research into the development of such techniques, and discuss the remaining work required to overcome the great challenge of self-adaptive high-integrity software.},
booktitle = {Proceedings of the 8th Workshop on Assurances for Self-Adaptive Systems},
pages = {1–4},
numpages = {4},
keywords = {self-adaptive software, model checking, high-integrity software},
location = {Szeged, Hungary},
series = {ASAS '11}
}

@article{10.1016/j.eswa.2012.08.026,
author = {Ognjanovi\'{c}, Ivana and Ga\v{s}Evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {A stratified framework for handling conditional preferences: An extension of the analytic hierarchy process},
year = {2013},
issue_date = {March, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.08.026},
doi = {10.1016/j.eswa.2012.08.026},
abstract = {Representing and reasoning over different forms of preferences is of crucial importance to many different fields, especially where numerical comparisons need to be made between critical options. Focusing on the well-known Analytical Hierarchical Process (AHP) method, we propose a two-layered framework for addressing different kinds of conditional preferences which include partial information over preferences and preferences of a lexicographic kind. The proposed formal two-layered framework, called CS-AHP, provides the means for representing and reasoning over conditional preferences. The framework can also effectively order decision outcomes based on conditional preferences in a way that is consistent with well-formed preferences. Finally, the framework provides an estimation of the potential number of violations and inconsistencies within the preferences. We provide and report extensive performance analysis for the proposed framework from three different perspectives, namely time-complexity, simulated decision making scenarios, and handling cyclic and partially defined preferences.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1094–1115},
numpages = {22},
keywords = {Well-formed preferences, S-AHP method, Lexicographic order, Conditional preferences, Comparative preferences, AHP method}
}

@article{10.1007/s10664-015-9364-x,
author = {Passos, Leonardo and Teixeira, Leopoldo and Dintzner, Nicolas and Apel, Sven and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof and Borba, Paulo and Guo, Jianmei},
title = {Coevolution of variability models and related software artifacts},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9364-x},
doi = {10.1007/s10664-015-9364-x},
abstract = {Variant-rich software systems offer a large degree of customization, allowing users to configure the target system according to their preferences and needs. Facing high degrees of variability, these systems often employ variability models to explicitly capture user-configurable features (e.g., systems options) and the constraints they impose. The explicit representation of features allows them to be referenced in different variation points across different artifacts, enabling the latter to vary according to specific feature selections. In such settings, the evolution of variability models interplays with the evolution of related artifacts, requiring the two to evolve together, or coevolve. Interestingly, little is known about how such coevolution occurs in real-world systems, as existing research has focused mostly on variability evolution as it happens in variability models only. Furthermore, existing techniques supporting variability evolution are usually validated with randomly-generated variability models or evolution scenarios that do not stem from practice. As the community lacks a deep understanding of how variability evolution occurs in real-world systems and how it relates to the evolution of different kinds of software artifacts, it is not surprising that industry reports existing tools and solutions ineffective, as they do not handle the complexity found in practice. Attempting to mitigate this overall lack of knowledge and to support tool builders with insights on how variability models coevolve with other artifact types, we study a large and complex real-world variant-rich software system: the Linux kernel. Specifically, we extract variability-coevolution patterns capturing changes in the variability model of the Linux kernel with subsequent changes in Makefiles and C source code. From the analysis of the patterns, we report on findings concerning evolution principles found in the kernel, and we reveal deficiencies in existing tools and theory when handling changes captured by our patterns.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1744–1793},
numpages = {50},
keywords = {Variability, Software product lines, Patterns, Linux, Evolution}
}

@article{10.1016/j.comnet.2019.07.013,
author = {Pimpinella, Andrea and Redondi, Alessandro E.C. and Cesana, Matteo},
title = {Walk this way! An IoT-based urban routing system for smart cities},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {162},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2019.07.013},
doi = {10.1016/j.comnet.2019.07.013},
journal = {Comput. Netw.},
month = oct,
numpages = {12},
keywords = {Temporal forecasting, Spatial interpolation, Urban routing, Smart cities, Internet of things}
}

@inproceedings{10.1109/CCGrid.2014.25,
author = {Almeida, Andr\'{e} and Dantas, Francisco and Cavalcante, Everton and Batista, Thais},
title = {A branch-and-bound algorithm for autonomic adaptation of multi-cloud applications},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.25},
doi = {10.1109/CCGrid.2014.25},
abstract = {Adaptation is an important concern in cloud-based applications composed of services provided by different cloud providers since cloud services can suffer from Quality of Services (QoS) fluctuations. Other conditions that can also trigger an adaptation process at runtime are the unavailability of services or the violation of user-defined policies. Moreover, the detection and reaction on such changes must be done in an autonomic way, without the need of user intervention. This paper presents a dynamic adaptation approach for multi-cloud applications supported by a Branch-and-Bound (B&amp;B) algorithm in order to optimize the adaptation process itself when selecting the services to be deployed within the application. Computational experiments comparing the B&amp;B algorithm with another algorithm that evaluates all possible configurations for adapting an application showed that the B&amp;B algorithm is faster than the previous version. This new algorithm brings benefits to the scalability of the adaptation process, which can deal with large configurations of multi-cloud applications composed by a plethora of cloud services.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {315–323},
numpages = {9},
keywords = {scalability, optimization, multi-cloud applications, dynamic adaptation, branch-and-bound algorithm},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {deep learning, constraint solving, configurable systems, AutoML},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1016/j.dss.2010.12.009,
author = {Ribeiro, Rita A. and Moreira, Ana M. and van den Broek, Pim and Pimentel, Afonso},
title = {Hybrid assessment method for software engineering decisions},
year = {2011},
issue_date = {April, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {51},
number = {1},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2010.12.009},
doi = {10.1016/j.dss.2010.12.009},
abstract = {During software development, many decisions need to be made to guarantee the satisfaction of the stakeholders' requirements and goals. The full satisfaction of all of these requirements and goals may not be possible, requiring decisions over conflicting human interests as well as technological alternatives, with an impact on the quality and cost of the final solution. This work aims at assessing the suitability of multi-criteria decision making (MCDM) methods to support software engineers' decisions. To fulfil this aim, a HAM (Hybrid Assessment Method) is proposed, which gives its user the ability to perceive the influence different decisions may have on the final result. HAM is a simple and efficient method that combines one single pairwise comparison decision matrix (to determine the weights of criteria) with one classical weighted decision matrix (to prioritize the alternatives). To avoid consistency problems regarding the scale and the prioritization method, HAM uses a geometric scale for assessing the criteria and the geometric mean for determining the alternative ratings.},
journal = {Decis. Support Syst.},
month = apr,
pages = {208–219},
numpages = {12},
keywords = {Software engineering, Non-functional software requirements, Multi-criteria decision making, Aggregation operators}
}

@article{10.1016/j.compeleceng.2017.08.004,
author = {Alfrez, Germn H. and Pelechano, Vicente},
title = {Achieving autonomic Web service compositions with models at runtime},
year = {2017},
issue_date = {October 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {63},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.08.004},
doi = {10.1016/j.compeleceng.2017.08.004},
abstract = {Several exceptional situations may arise in the complex, heterogeneous, and changing contexts where Web service operations run. For instance, a Web service operation may have greatly increased its execution time or may have become unavailable. The contribution of this article is to provide a tool-supported framework to guide autonomic adjustments of context-aware service compositions using models at runtime. During execution, when problematic events arise in the context, models are used by an autonomic architecture to guide changes of the service composition. Under the closed-world assumption, the possible context events are fully known at design time. Nevertheless, it is difficult to foresee all the possible situations arising in uncertain contexts where service compositions run. Therefore, the proposed framework also covers the dynamic evolution of service compositions to deal with unexpected events in the open world. An evaluation demonstrates that our framework is efficient during dynamic adjustments.},
journal = {Comput. Electr. Eng.},
month = oct,
pages = {332–352},
numpages = {21},
keywords = {Web service compositions, Models at runtime, Dynamic software product lines, Dynamic evolution, Dynamic adaptation, Autonomic computing}
}

@inproceedings{10.5555/1885639.1885700,
author = {John, Isabel and Schwanninger, Christa and Almeida, Eduardo},
title = {The rise and fall of product line architectures},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This panel addresses questions around architecture like: How do you think a good product line architecture should look like? How much up-front design do we need for a product line architecture? What are hot research topics in product line architecture? The panel is organized as a goldfish bowl, where the panelists are in the middle of the audience and panelists change during the panel.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {500–501},
numpages = {2},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/2088876.2088879,
author = {Merle, Philippe and Rouvoy, Romain and Seinturier, Lionel},
title = {A reflective platform for highly adaptive multi-cloud systems},
year = {2011},
isbn = {9781450310703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2088876.2088879},
doi = {10.1145/2088876.2088879},
abstract = {Cloud platforms are increasingly used for hosting a broad diversity of services from traditional e-commerce applications to interactive web-based IDEs. However, we observe that the proliferation of offers by Cloud vendors raises several challenges. Developers will not only have to deploy applications for a specific Cloud, but will also have to consider migrating services from one cloud to another, and to manage applications spanning multiple Clouds. In this paper, we therefore report on a first experiment we conducted to build a multi-Cloud system on top of thirteen existing IaaS/PaaS. From this experiment, we advocate for two dimensions of adaptability---design and execution time---that applications for such systems require to exhibit. Finally, we propose a roadmap for future multi-Cloud systems.},
booktitle = {Adaptive and Reflective Middleware on Proceedings of the International Workshop},
pages = {14–21},
numpages = {8},
location = {Lisbon, Portugal},
series = {ARM '11}
}

@article{10.1504/IJWGS.2012.051527,
author = {Charfi, Anis and Schmeling, Benjamin and Mezini, Mira},
title = {An aspect-oriented framework for specification and enforcement of non-functional concerns in WS-BPEL},
year = {2012},
issue_date = {January 2012},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {8},
number = {4},
issn = {1741-1106},
url = {https://doi.org/10.1504/IJWGS.2012.051527},
doi = {10.1504/IJWGS.2012.051527},
abstract = {Web Service processes in WS-BPEL have several non-functional requirements such as security and reliable messaging. Although there are many WS-* specifications that address these concerns, their integration with WS-BPEL is still open. In this paper, we discuss these non-functional requirements and present a survey on the current support for their specification and enforcement in WS-BPEL engines. Moreover, we introduce an aspect-oriented container framework that uses a declarative deployment descriptor to specify the non-functional requirements. For the enforcement, aspects in AO4BPEL 2.0 are generated, which intercept the process execution and call dedicated middleware Web Services.},
journal = {Int. J. Web Grid Serv.},
month = jan,
pages = {386–424},
numpages = {39}
}

@article{10.1007/s10664-019-09769-8,
author = {Ochodek, Miroslaw and Hebig, Regina and Meding, Wilhelm and Frost, Gert and Staron, Miroslaw},
title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09769-8},
doi = {10.1007/s10664-019-09769-8},
abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {220–265},
numpages = {46},
keywords = {Code reviews, Action research, Machine learning, Measurement}
}

@article{10.1016/j.infsof.2019.04.011,
author = {Rodrigues, Arthur and Rodrigues, Gena\'{\i}na Nunes and Knauss, Alessia and Ali, Raian and Andrade, Hugo},
title = {Enhancing context specifications for dependable adaptive systems: A data mining approach},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.04.011},
doi = {10.1016/j.infsof.2019.04.011},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {115–131},
numpages = {17},
keywords = {Dependability, Goal modelling, Design time, Data mining, Context uncertainty, Self-adaptive system}
}

@article{10.5555/2747013.2747140,
author = {Chen, Bihuan and Peng, Xin and Yu, Yijun and Zhao, Wenyun},
title = {Uncertainty handling in goal-driven self-optimization - Limiting the negative effect on adaptation},
year = {2014},
issue_date = {April 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {90},
number = {C},
issn = {0164-1212},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose techniques to handle contribution uncertainty and effect uncertainty in goal-driven self-optimization.We integrate these uncertainty handling techniques with preference uncertainty handling to a goal-driven self-optimization framework.We demonstrate the effectiveness of our approach with an evaluation on an online shopping system. Goal-driven self-optimization through feedback loops has shown effectiveness in reducing oscillating utilities due to a large number of uncertain factors in the runtime environments. However, such self-optimization is less satisfactory when there contains uncertainty in the predefined requirements goal models, such as imprecise contributions and unknown quality preferences, or during the switches of goal solutions, such as lack of understanding about the time for the adaptation actions to take effect. In this paper, we propose to handle such uncertainty in goal-driven self-optimization without interrupting the services. Taking the monitored quality values as the feedback, and the estimated earned value as the global indicator of self-optimization, our approach dynamically updates the quantitative contributions from alternative functionalities to quality requirements, tunes the preferences of relevant quality requirements, and determines a proper timing delay for the last adaptation action to take effect. After applying these runtime measures to limit the negative effect of the uncertainty in goal models and their suggested switches, an experimental study on a real-life online shopping system shows the improvements over goal-driven self-optimization approaches without uncertainty handling.},
journal = {J. Syst. Softw.},
month = apr,
pages = {114–127},
numpages = {14},
keywords = {Uncertainty, Requirements goal models, Goal-driven self-optimization}
}

@article{10.1007/s10664-017-9573-6,
author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
title = {Data-efficient performance learning for configurable systems},
year = {2018},
issue_date = {Jun 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9573-6},
doi = {10.1007/s10664-017-9573-6},
abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1826–1867},
numpages = {42},
keywords = {Parameter tuning, Model selection, Regression, Configurable systems, Performance prediction}
}

@inproceedings{10.5555/978-3-030-45234-6_fm,
title = {Front Matter},
year = {2020},
isbn = {978-3-030-45233-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Fundamental Approaches to Software Engineering: 23rd International Conference, FASE 2020, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2020, Dublin, Ireland, April 25–30, 2020, Proceedings},
pages = {i–xiii},
location = {Dublin, Ireland}
}

@article{10.1007/s11219-016-9320-z,
author = {Carvalho, Rainara Maia and Castro Andrade, Rossana Maria and Oliveira, K\'{a}thia Mar\c{c}al and Sousa Santos, Ismayle and Bezerra, Carla Ilane},
title = {Quality characteristics and measures for human---computer interaction evaluation in ubiquitous systems},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9320-z},
doi = {10.1007/s11219-016-9320-z},
abstract = {The advent of ubiquitous systems places even more focus on users, since these systems must support their daily activities in such a transparent way that does not disturb them. Thus, much more attention should be provided to human---computer interaction (HCI) and, as a consequence, to its quality. Dealing with quality issues implies first the identification of the quality characteristics that should be achieved and, then, which software measures should be used to evaluate them in a target system. Therefore, this work aims to identify what quality characteristics and measures have been used for the HCI evaluation of ubiquitous systems. In order to achieve our goal, we performed a large literature review, using a systematic mapping study, and we present our results in this paper. We identified 41 pertinent papers that were deeply analyzed to extract quality characteristics and software measures. We found 186 quality characteristics, but since there were divergences on their definitions and duplicated characteristics, an analysis of synonyms by peer review based on the equivalence of definitions was also done. This analysis allowed us to define a final suitable set composed of 27 quality characteristics, where 21 are generic to any system but are particularized for ubiquitous applications and 6 are specific for this domain. We also found 218 citations of measures associated with the characteristics, although the majority of them are simple definitions with no detail about their measurement functions. Our results provide not only an overview of this area to guide researchers in directing their efforts but also it can help practitioners in evaluating ubiquitous systems using these measures.},
journal = {Software Quality Journal},
month = sep,
pages = {743–795},
numpages = {53},
keywords = {Ubiquitous systems, Systematic mapping study, Software measures, Quality model, Quality characteristics, Human---computer interaction}
}

@article{10.1016/j.infsof.2016.11.009,
author = {Mariani, Thain\'{a} and Vergilio, Silvia Regina},
title = {A systematic review on search-based refactoring},
year = {2017},
issue_date = {March 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {83},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.11.009},
doi = {10.1016/j.infsof.2016.11.009},
abstract = {Context: To find the best sequence of refactorings to be applied in a software artifact is an optimization problem that can be solved using search techniques, in the field called Search-Based Refactoring (SBR). Over the last years, the field has gained importance, and many SBR approaches have appeared, arousing research interest.Objective: The objective of this paper is to provide an overview of existing SBR approaches, by presenting their common characteristics, and to identify trends and research opportunities.Method: A systematic review was conducted following a plan that includes the definition of research questions, selection criteria, a search string, and selection of search engines. 71 primary studies were selected, published in the last sixteen years. They were classified considering dimensions related to the main SBR elements, such as addressed artifacts, encoding, search technique, used metrics, available tools, and conducted evaluation.Results: Some results show that code is the most addressed artifact, and evolutionary algorithms are the most employed search technique. Furthermore, most times, the generated solution is a sequence of refactorings. In this respect, the refactorings considered are usually the ones of the Fowler's Catalog. Some trends and opportunities for future research include the use of models as artifacts, the use of many objectives, the study of the bad smells effect, and the use of hyper-heuristics.Conclusions: We have found many SBR approaches, most of them published recently. The approaches are presented, analyzed, and grouped following a classification scheme. The paper contributes to the SBR field as we identify a range of possibilities that serve as a basis to motivate future researches.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {14–34},
numpages = {21},
keywords = {Search-based software engineering, Refactoring, Evolutionary algorithms}
}

@inproceedings{10.5555/2663546.2663573,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, California},
series = {SEAMS '13}
}

@inproceedings{10.4108/eai.25-10-2016.2266615,
author = {Distefano, Salvatore and Scarpa, Marco},
title = {Quantitative assessment of workflow performance through PH reduction},
year = {2017},
isbn = {9781631901416},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/eai.25-10-2016.2266615},
doi = {10.4108/eai.25-10-2016.2266615},
abstract = {Workflows are logical abstraction of processes widely adopted in several contexts such as economy and management sciences (business processes), service engineering (service oriented architecture, Web services, BPEL), software engineering (component based systems, UML, flowcharts) distributed computing (Grid, Cloud, Mapreduce). Design and operation of workflows are critical stages in which problems and issues not manifested by the single block arise from compositions. To deal with such issues, proper techniques and tools should be implemented as support for workflow designers and operators. This paper proposes a solution for the evaluation of workflow performance starting from the components’ ones. Based on the stochastic characterization of the workflow tasks, phase type distributions and stochastic workflow reduction rules, the proposed approach allows to overcome the limits of existing solutions, considering general response time distributions while providing parametric analysis on customer usage profiles and design alternatives. To demonstrate the effectiveness of the proposed solution an example taken from literature is evaluated.},
booktitle = {Proceedings of the 10th EAI International Conference on Performance Evaluation Methodologies and Tools on 10th EAI International Conference on Performance Evaluation Methodologies and Tools},
pages = {117–124},
numpages = {8},
keywords = {workflow, usage profile, phase type, performance, non-markovian behaviors, design alternatives},
location = {Taormina, Italy},
series = {VALUETOOLS'16}
}

@article{10.1016/j.infsof.2006.11.003,
author = {Niemel\"{a}, Eila and Immonen, Anne},
title = {Capturing quality requirements of product family architecture},
year = {2007},
issue_date = {November, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {11–12},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.11.003},
doi = {10.1016/j.infsof.2006.11.003},
abstract = {Software quality is one of the major issues with software intensive systems. Moreover, quality is a critical success factor in software product families exploiting shared architecture and common components in a set of products. Our contribution is the QRF (Quality Requirements of a software Family) method, which explicitly focuses on how quality requirements have to be defined, represented and transformed to architectural models. The method has been applied to two experiments; one in a laboratory environment and the other in industry. The use of the QRF method is exemplified by the Distribution Service Platform (DiSeP), the laboratory experiment. The lessons learned are also based on our experiences of applying the method in industrial settings.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1107–1120},
numpages = {14},
keywords = {Traceability, Software product family, Software architecture, Quality requirement}
}

@article{10.1007/s11219-019-09489-8,
author = {Al\'{e}groth, Emil and Gorschek, Tony and Petersen, Kai and Mattsson, Michael},
title = {Characteristics that affect preference of decision models for asset selection: an industrial questionnaire survey},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09489-8},
doi = {10.1007/s11219-019-09489-8},
abstract = {Modern software development relies on a combination of development and re-use of technical asset, e.g., software components, libraries, and APIs. In the past, re-use was mostly conducted with internal assets but today external; open source, customer off-the-shelf (COTS), and assets developed through outsourcing are also common. This access to more asset alternatives presents new challenges regarding what assets to optimally chose and how to make this decision. To support decision-makers, decision theory has been used to develop decision models for asset selection. However, very little industrial data has been presented in literature about the usefulness, or even perceived usefulness, of these models. Additionally, only limited information has been presented about what model characteristics determine practitioner preference toward one model over another. The objective of this work is to evaluate what characteristics of decision models for asset selection determine industrial practitioner preference of a model when given the choice of a decision model of high precision or a model with high speed. An industrial questionnaire survey is performed where a total of 33 practitioners, of varying roles, from 18 companies are tasked to compare two decision models for asset selection. Textual analysis and formal and descriptive statistics are then applied on the survey responses to answer the study’s research questions. The study shows that the practitioners had clear preference toward the decision model that emphasized speed over the one that emphasized decision precision. This conclusion was determined to be because one of the models was perceived faster, had lower complexity, was more flexible in use for different decisions, and was more agile on how it could be used in operation, its emphasis on people, its emphasis on “good enough” precision and ability to fail fast if a decision was a failure. Hence, we found seven characteristics that the practitioners considered important for their acceptance of the model. Industrial practitioner preference, which relates to acceptance, of decision models for asset selection is dependent on multiple characteristics that must be considered when developing a model for different types of decisions such as operational day-to-day decisions as well as more critical tactical or strategic decisions. The main contribution of this work are the seven identified characteristics that can serve as industrial requirements for future research on decision models for asset selection.},
journal = {Software Quality Journal},
month = dec,
pages = {1675–1707},
numpages = {33},
keywords = {Model comparison, Survey, Industrial study, Characteristics, Decision models}
}

@inproceedings{10.5555/2487336.2487363,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, CA, USA},
series = {SEAMS '13}
}

@article{10.1145/1988997.1989008,
author = {Rahmani, M.},
title = {Software modeling &amp; design: UML, use cases, patterns, &amp; software architectures by Hassan Gomaa},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1988997.1989008},
doi = {10.1145/1988997.1989008},
journal = {SIGSOFT Softw. Eng. Notes},
month = aug,
pages = {35},
numpages = {1}
}

@inproceedings{10.1145/2517208.2517228,
author = {Ofenbeck, Georg and Rompf, Tiark and Stojanov, Alen and Odersky, Martin and P\"{u}schel, Markus},
title = {Spiral in scala: towards the systematic construction of generators for performance libraries},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517228},
doi = {10.1145/2517208.2517228},
abstract = {Program generators for high performance libraries are an appealing solution to the recurring problem of porting and optimizing code with every new processor generation, but only few such generators exist to date. This is due to not only the difficulty of the design, but also of the actual implementation, which often results in an ad-hoc collection of standalone programs and scripts that are hard to extend, maintain, or reuse. In this paper we ask whether and which programming language concepts and features are needed to enable a more systematic construction of such generators. The systematic approach we advocate extrapolates from existing generators: a) describing the problem and algorithmic knowledge using one, or several, domain-specific languages (DSLs), b) expressing optimizations and choices as rewrite rules on DSL programs, c) designing data structures that can be configured to control the type of code that is generated and the data representation used, and d) using autotuning to select the best-performing alternative. As a case study, we implement a small, but representative subset of Spiral in Scala using the Lightweight Modular Staging (LMS) framework. The first main contribution of this paper is the realization of c) using type classes to abstract over staging decisions, i.e. which pieces of a computation are performed immediately and for which pieces code is generated. Specifically, we abstract over different complex data representations jointly with different code representations including generating loops versus unrolled code with scalar replacement - a crucial and usually tedious performance transformation. The second main contribution is to provide full support for a) and d) within the LMS framework: we extend LMS to support translation between different DSLs and autotuning through search.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {125–134},
numpages = {10},
keywords = {synthesis, selective precomputation, scalar replacement, data representation, abstraction over staging},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@inproceedings{10.1145/2145204.2145402,
author = {Liu, Xiaoqing (Frank) and Barnes, Eric Christopher and Savolainen, Juha Erik},
title = {Conflict detection and resolution for product line design in a collaborative decision making environment},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145402},
doi = {10.1145/2145204.2145402},
abstract = {Ensuring that the non-functional requirements (NFRs), of a system are satisfied is an essential task in software development. However, this task is complicated by the fact that many NFRs conflict with each other from multiple perspectives. It is essential to resolve conflicts collectively in a collaborative decision making process since stakeholders often disagree on how conflicts should be resolved. In this paper, we describe a method for dividing high-level NFR conflicts within a product line into more manageable sub-problems. Stakeholders make use of an argumentation based collaborative decision support system to determine which design alternatives provide the best trade-offs between NFRs. Finally, we present an empirical study in which the aforementioned system was used to resolve a single instance of an NFR conflict across 3 members of a product line. It shows that the system is effective in resolving conflicts in a collaborative decision process.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {1327–1336},
numpages = {10},
keywords = {participatory/cooperative design, computer-mediated communication, collaborative software development, Collaboration architectures},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/2658761.2658767,
author = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
title = {Automatic feature selection in large-scale system-software product lines},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658767},
doi = {10.1145/2658761.2658767},
abstract = {System software can typically be configured at compile time via a comfortable feature-based interface to tailor its functionality towards a specific use case. However, with the growing number of features, this tailoring process becomes increasingly difficult: As a prominent example, the Linux kernel in v3.14 provides nearly 14 000 configuration options to choose from. Even developers of embedded systems refrain from trying to build a minimized distinctive kernel configuration for their device – and thereby waste memory and money for unneeded functionality. In this paper, we present an approach for the automatic use-case specific tailoring of system software for special-purpose embedded systems. We evaluate the effectiveness of our approach on the example of Linux by generating tailored kernels for well-known applications of the Rasperry Pi and a Google Nexus 4 smartphone. Compared to the original configurations, our approach leads to memory savings of 15–70 percent and requires only very little manual intervention.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {39–48},
numpages = {10},
keywords = {Software Tailoring, Software Product Lines, Linux, Feature Selection},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@inproceedings{10.1145/2000259.2000274,
author = {Brosch, Franz and Buhnova, Barbora and Koziolek, Heiko and Reussner, Ralf},
title = {Reliability prediction for fault-tolerant software architectures},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000274},
doi = {10.1145/2000259.2000274},
abstract = {Software fault tolerance mechanisms aim at improving the reliability of software systems. Their effectiveness (i.e., reliability impact) is highly application-specific and depends on the overall system architecture and usage profile. When examining multiple architecture configurations, such as in software product lines, it is a complex and error-prone task to include fault tolerance mechanisms effectively. Existing approaches for reliability analysis of software architectures either do not support modelling fault tolerance mechanisms or are not designed for an efficient evaluation of multiple architecture variants. We present a novel approach to analyse the effect of software fault tolerance mechanisms in varying architecture configurations. We have validated the approach in multiple case studies, including a large-scale industrial system, demonstrating its ability to support architecture design, and its robustness against imprecise input data.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {75–84},
numpages = {10},
keywords = {software product lines, reliability prediction, fault tolerance, component-based software architectures},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@article{10.1007/s10664-013-9263-y,
author = {Bjarnason, Elizabeth and Runeson, Per and Borg, Markus and Unterkalmsteiner, Michael and Engstr\"{o}m, Emelie and Regnell, Bj\"{o}rn and Sabaliauskaite, Giedre and Loconsole, Annabella and Gorschek, Tony and Feldt, Robert},
title = {Challenges and practices in aligning requirements with verification and validation: a case study of six companies},
year = {2014},
issue_date = {December  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9263-y},
doi = {10.1007/s10664-013-9263-y},
abstract = {Weak alignment of requirements engineering (RE) with verification and validation (VV) may lead to problems in delivering the required products in time with the right quality. For example, weak communication of requirements changes to testers may result in lack of verification of new requirements and incorrect verification of old invalid requirements, leading to software quality problems, wasted effort and delays. However, despite the serious implications of weak alignment research and practice both tend to focus on one or the other of RE or VV rather than on the alignment of the two. We have performed a multi-unit case study to gain insight into issues around aligning RE and VV by interviewing 30 practitioners from 6 software developing companies, involving 10 researchers in a flexible research process for case studies. The results describe current industry challenges and practices in aligning RE with VV, ranging from quality of the individual RE and VV activities, through tracing and tools, to change control and sharing a common understanding at strategy, goal and design level. The study identified that human aspects are central, i.e. cooperation and communication, and that requirements engineering practices are a critical basis for alignment. Further, the size of an organisation and its motivation for applying alignment practices, e.g. external enforcement of traceability, are variation factors that play a key role in achieving alignment. Our results provide a strategic roadmap for practitioners improvement work to address alignment challenges. Furthermore, the study provides a foundation for continued research to improve the alignment of RE with VV.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {1809–1855},
numpages = {47},
keywords = {Verification, Validation, Testing, Requirements engineering, Case study, Alignment}
}

@inproceedings{10.1145/2465478.2465490,
author = {Klein, John and van Vliet, Hans},
title = {A systematic review of system-of-systems architecture research},
year = {2013},
isbn = {9781450321266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465478.2465490},
doi = {10.1145/2465478.2465490},
abstract = {Context: A system of systems is an assemblage of components which individually may be regarded as systems, and which possesses the additional properties that the constituent systems are operationally independent, and are managerially independent. Much has been published about the field of systems of systems by researchers and practitioners, often with the assertion that the system-of-systems design context necessitates the use of architecture approaches that are somewhat different from system-level architecture. However, no systematic review has been conducted to provide an extensive overview of system of systems architecture research.Objective: This paper presents such a systematic review. The objective of this review is to classify and provide a thematic analysis of the reported results in system of systems architecture.Method: The primary studies for the systematic review were identified using a predefined search strategy followed by an extensive manual selection process.Results: We found the primary studies published in a large number of venues, mostly domain-oriented, with no obvious center of a research community of practice. The field seems to be maturing more slowly than other software technologies: Most reported results described individuals or teams working in apparent isolation to develop solutions to particular system-of-systems architecture problems, with no techniques gaining widespread adoption.Conclusions: A comprehensive research agenda for this field should be developed, and further studies should be performed to determine whether the information system-related problems of system of systems architecture are covered by existing software architecture knowledge, and if not, to develop general methods for system-of-systems architecture.},
booktitle = {Proceedings of the 9th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {13–22},
numpages = {10},
keywords = {systematic review, system of systems, architecture},
location = {Vancouver, British Columbia, Canada},
series = {QoSA '13}
}

@article{10.1007/s10515-017-0215-4,
author = {Boussa\"{\i}d, Ilhem and Siarry, Patrick and Ahmed-Nacer, Mohamed},
title = {A survey on search-based model-driven engineering},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0215-4},
doi = {10.1007/s10515-017-0215-4},
abstract = {Model-driven engineering (MDE) and search-based software engineering (SBSE) are both relevant approaches to software engineering. MDE aims to raise the level of abstraction in order to cope with the complexity of software systems, while SBSE involves the application of metaheuristic search techniques to complex software engineering problems, reformulating engineering tasks as optimization problems. The purpose of this paper is to survey the relatively recent research activity lying at the interface between these two fields, an area that has come to be known as search-based model-driven engineering. We begin with an introduction to MDE, the concepts of models, of metamodels and of model transformations. We also give a brief introduction to SBSE and metaheuristics. Then, we survey the current research work centered around the combination of search-based techniques and MDE. The literature survey is accompanied by the presentation of references for further details.},
journal = {Automated Software Engg.},
month = jun,
pages = {233–294},
numpages = {62},
keywords = {Search-based software engineering (SBSE), Model-driven engineering (MDE), Metaheuristics, Metaheuristic}
}

@inproceedings{10.1145/2593882.2593895,
author = {Hatcliff, John and Wassyng, Alan and Kelly, Tim and Comar, Cyrille and Jones, Paul},
title = {Certifiably safe software-dependent systems: challenges and directions},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593895},
doi = {10.1145/2593882.2593895},
abstract = {The amount and impact of software-dependence in critical systems impinging on daily life is increasing rapidly. In many of these systems, inadequate software and systems engineering can lead to economic disaster, injuries or death. Society generally does not recognize the potential of losses from deficiencies of systems due to software until after some mishap occurs. Then there is an outcry, reflecting societal expectations; however, few know what it takes to achieve the expected safety and, in general, loss-prevention.  On the one hand there are unprecedented, exponential increases in size, inter-dependencies, intricacies, numbers and variety in the systems and distribution of development processes across organizations and cultures. On the other hand, industry's capability to verify and validate these systems has not kept up. Mere compliance with existing standards, techniques, and regulations cannot guarantee the safety properties of these systems. The gap between practice and capability is increasing rapidly.  This paper considers the future of software engineering as needed to support development and certification of safety-critical software-dependent systems. We identify a collection of challenges and document their current state, the desired state, gaps and barriers to reaching the desired state, and potential directions in software engineering research and education that could address the gaps and barriers.},
booktitle = {Future of Software Engineering Proceedings},
pages = {182–200},
numpages = {19},
keywords = {verification, validation, standards, safety, requirements, hazard analysis, assurance, Certification},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@article{10.1016/j.infsof.2016.01.019,
author = {Arcaini, Paolo and Gargantini, Angelo and Riccobene, Elvinia and Vavassori, Paolo},
title = {A novel use of equivalent mutants for static anomaly detection in software artifacts},
year = {2017},
issue_date = {January 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {81},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.01.019},
doi = {10.1016/j.infsof.2016.01.019},
abstract = {Equivalent mutants are usually seen as an inconvenience in mutation analysis.We claim that equivalent mutants can be useful to detect and remove static anomalies.A process for detecting static anomalies is proposed.The process is based on mutation, equivalence checking, and quality measurement.The process is applicable to different kinds of software artifacts. Context: In mutation analysis, a mutant of a software artifact, either a program or a model, is said equivalent if it leaves the artifact meaning unchanged. Equivalent mutants are usually seen as an inconvenience and they reduce the applicability of mutation analysis.Objective: Instead, we here claim that equivalent mutants can be useful to define, detect, and remove static anomalies, i.e., deficiencies of given qualities: If an equivalent mutant has a better quality value than the original artifact, then an anomaly has been found and removed.Method: We present a process for detecting static anomalies based on mutation, equivalence checking, and quality measurement.Results: Our proposal and the originating technique are applicable to different kinds of software artifacts. We present anomalies and conduct several experiments in different contexts, at specification, design, and implementation level.Conclusion: We claim that in mutation analysis a new research direction should be followed, in which equivalent mutants and operators generating them are welcome.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {52–64},
numpages = {13},
keywords = {Static anomaly, Quality measure, Equivalent mutant}
}

@article{10.1145/2579281.2579312,
author = {Ionita, Anca Daniela and Lewis, Grace A. and Litoiu, Marin},
title = {Report of the 2013 IEEE 7th international symposium on the maintenance and evolution of service-oriented and cloud-based systems (MESOCA 2013)},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579312},
doi = {10.1145/2579281.2579312},
abstract = {The 2013 IEEE 7th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA 2013) took place in Eindhoven, The Netherlands, on September 24, 2013, as a co-located event of the 29th IEEE International Conference on Software Maintenance (ICSM 2013). MESOCA 2013 covered a wide range of academic and industrial experiences, brought together through one keynote, two invited presentations and eleven paper presentations, which triggered lively discussions. They approached aspects related to the entire software maintenance process, from requirements to testing, with specific solutions for Service-Oriented Architecture and Cloud Computing environments. Technical and business perspectives were discussed, including issues about optimization techniques, pre-migration evaluation of legacy software, decision analysis, energy efficiency, multi-cloud architectures and adaptability. It thus confirmed MESOCA as an ongoing forum for researchers and practitioners to identify and address the increasing challenges related to the evolution of service-provisioning systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {34–37},
numpages = {4},
keywords = {software maintenance, software evolution, services, serviceoriented architecture, service-oriented systems, cloudbased systems, cloud computing, SOA}
}

@article{10.1504/IJWET.2016.081768,
author = {Cobaleda, Luz-Viviana and Mazo, Ra\'{u}l and Becerra, Jorge Luis Risco and Duitama, John-Freddy},
title = {Reference software architecture for improving modifiability of personalised web applications - a controlled experiment},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {11},
number = {4},
issn = {1476-1289},
url = {https://doi.org/10.1504/IJWET.2016.081768},
doi = {10.1504/IJWET.2016.081768},
abstract = {Although web personalisation has been studied for the last two decades, there remains a need to address current challenges: context-awareness and the inclusion in a business environment. The wide variety of mobile devices and their continuous technological evolution demands the permanent development of new personalisation strategies. Additionally, two factors complicate the inclusion of personalised web applications in a business environment: the frequent change of personalisation strategies for each business, and the technical complexity to integrate these strategies in a short time. We propose a reference architecture as a tool to favour their modifiability. Moreover, our proposal facilitates the opportunity for enterprises to adopt web-personalised systems into their business as a strategic tool. A controlled experiment validates our approach; we compare five change scenarios that are implemented under two architectures: experimental and control architecture. We used change scenarios derived from a real Brazilian e-commerce enterprise.},
journal = {Int. J. Web Eng. Technol.},
month = jan,
pages = {351–370},
numpages = {20},
keywords = {web personalisation, software components, reference software architecture, personalised web applications, personalised apps, modifiability, electronic commerce, e-commerce, context awareness, business environment, Brazil}
}

@inproceedings{10.1007/978-3-319-05843-6_15,
author = {Alebrahim, Azadeh and Faβbender, Stephan and Heisel, Maritta and Meis, Rene},
title = {Problem-Based Requirements Interaction Analysis},
year = {2014},
isbn = {9783319058429},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-05843-6_15},
doi = {10.1007/978-3-319-05843-6_15},
abstract = {[Context] The ability to address the diverse interests of different stakeholders in a software project in a coherent way is one fundamental software quality. These diverse and maybe conflicting interests are reflected by the requirements of each stakeholder. [Problem] Thus, it is likely that aggregated requirements for a software system contain interactions. To avoid unwanted interactions and improve software quality, we propose a structured method consisting of three phases to find such interactions. [Principal ideas] For our method, we use problem diagrams, which describe requirements in a structured way. The information represented in the problem diagrams is translated into a formal Z model. Then we reduce the number of combinations of requirements, which might conflict. [Contribution] The reduction of requirements interaction candidates is crucial to lower the effort of the in depth interaction analysis. For validation of our method, we use a real-life example in the domain of smart grid.},
booktitle = {Proceedings of the 20th International Working Conference on Requirements Engineering: Foundation for Software Quality - Volume 8396},
pages = {200–215},
numpages = {16},
keywords = {problem frames, feature interaction, Z notation, Requirements interactions},
location = {Essen, Germany},
series = {REFSQ 2014}
}

@inproceedings{10.1145/2897010.2897011,
author = {Fischer, Stefan and Lopez-Herrejon, Roberto E. and Ramler, Rudolf and Egyed, Alexander},
title = {A preliminary empirical assessment of similarity for combinatorial interaction testing of software product lines},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897011},
doi = {10.1145/2897010.2897011},
abstract = {Extensive work on Search-Based Software Testing for Software Product Lines has been published in the last few years. Salient among them is the use of similarity as a surrogate metric for t-wise coverage whenever higher strengths are needed or whenever the size of the test suites is infeasible because of technological or budget limitations. Though promising, this metric has not been assessed with real fault data. In this paper, we address this limitation by using Drupal, a widely used open source web content management system, as an industry-strength case study for which both variability information and fault data have been recently made available. Our preliminary assessment corroborates some of the previous findings but also raises issues on some assumptions and claims made. We hope our work encourages further empirical evaluations of Combinatorial Interaction Testing approaches for Software Product Lines.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {15–18},
numpages = {4},
location = {Austin, Texas},
series = {SBST '16}
}

@inproceedings{10.1007/978-3-662-45234-9_25,
author = {Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania},
title = {Challenges in Modelling and Analyzing Quantitative Aspects of Bike-Sharing Systems},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_25},
doi = {10.1007/978-3-662-45234-9_25},
abstract = {Bike-sharing systems are becoming popular not only as a sustainable means of transportation in the urban environment, but also as a challenging case study that presents interesting run-time optimization problems. As a side-study within a research project aimed at quantitative analysis that used such a case study, we have observed how the deployed systems enjoy a wide variety of different features. We have therefore applied variability analysis to define a family of bike-sharing systems, and we have sought support in available tools. We have so established a tool chain that includes academic tools that provide different functionalities regarding the analysis of software product lines, from feature modelling to product derivation and from quantitative evaluation of the attributes of products to model checking value-passing modal specifications. The tool chain is currently experimented inside the mentioned project as a complement to more sophisticated product-based analysis techniques.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {351–367},
numpages = {17}
}

@inproceedings{10.5555/2337223.2337416,
author = {Perrouin, Gilles and Morin, Brice and Chauvel, Franck and Fleurey, Franck and Klein, Jacques and Le Traon, Yves and Barais, Olivier and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards flexible evolution of dynamically adaptive systems},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Modern software systems need to be continuously available under varying conditions. Their ability to dynamically adapt to their execution context is thus increasingly seen as a key to their success. Recently, many approaches were proposed to design and support the execution of Dynamically Adaptive Systems (DAS). However, the ability of a DAS to evolve is limited to the addition, update or removal of adaptation rules or reconfiguration scripts. These artifacts are very specific to the control loop managing such a DAS and runtime evolution of the DAS requirements may affect other parts of the DAS. In this paper, we argue to evolve all parts of the loop. We suggest leveraging recent advances in model-driven techniques to offer an approach that supports the evolution of both systems and their adaptation capabilities. The basic idea is to consider the control loop itself as an adaptive system.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1353–1356},
numpages = {4},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.5555/2662572.2662581,
author = {Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
title = {Optimum feature selection in software product lines: let your model and values guide your search},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {In Search-Based Software Engineering, well-known metaheuristic search algorithms are utilized to find solutions to common software engineering problems. The algorithms are usually taken "off the shelf" and applied with trust, i.e. software engineers are not concerned with the inner workings of algorithms, only with the results. While this may be sufficient is some domains, we argue against this approach, particularly where the complexity of the models and the variety of user preferences pose greater challenges to the metaheuristic search algorithms. We build on our previous investigation which uncovered the power of Indicator-Based Evolutionary Algorithm (IBEA) over traditionally-used algorithms (such as NSGA-II), and in this work we scrutinize the time behavior of user objectives subject to optimization. This analysis brings out the business perspective, previously veiled under Pareto-collective gauges such as Hypervolume and Spread. In addition, we show how slowing down the rates of crossover and mutation can help IBEA converge faster, as opposed to following the higher rates used in many other studies as "rules of thumb".},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {22–27},
numpages = {6},
keywords = {software product lines, search-based software engineering, optimal feature selection, multiobjective optimization, indicator-based evolutionary algorithm, feature models},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@inproceedings{10.5555/1939864.1939916,
author = {Johnsen, Einar Broch and Owe, Olaf and Schlatte, Rudolf and Tarifa, Silvia Lizeth Tapia},
title = {Dynamic resource reallocation between deployment components},
year = {2010},
isbn = {3642169007},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Today's software systems are becoming increasingly configurable and designed for deployment on a plethora of architectures, ranging from sequential machines via multicore and distributed architectures to the cloud. Examples of such systems are found in, e.g., software product lines, service-oriented computing, information systems, embedded systems, operating systems, and telephony. To model and analyze systems without a fixed architecture, the models need to naturally capture and range over relevant deployment scenarios. For this purpose, it is interesting to lift aspects of low-level deployment concerns to the abstraction level of the modeling language. In this paper, the object-oriented modeling language Creol is extended with a notion of dynamic deployment components with parametric processing resources, such that processor resources may be explicitly reallocated. The approach is compositional in the sense that functional models and reallocation strategies are both expressed in Creol, and functional models can be run alone or in combination with different reallocation strategies. The formal semantics of deployment components is given in rewriting logic, extending the semantics of Creol, and executes on Maude, which allows simulations and test suites to be applied to models which vary in their available resources as well as in their resource reallocation strategies.},
booktitle = {Proceedings of the 12th International Conference on Formal Engineering Methods and Software Engineering},
pages = {646–661},
numpages = {16},
location = {Shanghai, China},
series = {ICFEM'10}
}

@inproceedings{10.1145/2493288.2493311,
author = {Kumar, Kiran and Prabhakar, T. V.},
title = {Pattern-oriented knowledge model for architecture design},
year = {2010},
isbn = {9781450301077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493288.2493311},
doi = {10.1145/2493288.2493311},
abstract = {Software design patterns document the most recommended solutions to recurring design problems. Selection of the best design pattern in a given context involves analysis of available alternatives, which is a knowledge-intensive task. Pattern knowledge overload (due to the large number of design patterns) makes such analysis difficult. A knowledge base to generate available alternatives can alleviate the problem. In this paper, we propose a pattern-oriented knowledge model which considers four dimensions of the pattern knowledge space: Pattern to Tactic relationship, Pattern to Pattern relationship, Pattern to Quality-attribute relationship and Pattern to Application-type relationship. We perform analysis of these relationships for patterns in the two popular pattern catalogues viz GoF and POSA1.},
booktitle = {Proceedings of the 17th Conference on Pattern Languages of Programs},
articleno = {23},
numpages = {21},
keywords = {tactics, patterns, pattern to tactic relationship, pattern to quality attribute relationship, pattern to pattern relationship, pattern to application type relationship, decision view},
location = {Reno, Nevada, USA},
series = {PLOP '10}
}

@article{10.1007/s10617-006-9588-7,
author = {Silvano, Cristina and Agosta, Giovanni and Palermo, Gianluca},
title = {Efficient architecture/compiler co-exploration using analytical models},
year = {2007},
issue_date = {March     2007},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {11},
number = {1},
issn = {0929-5585},
url = {https://doi.org/10.1007/s10617-006-9588-7},
doi = {10.1007/s10617-006-9588-7},
abstract = {The hardware/software co-exploration is a critical phase for a broad range of embedded platforms based on the System-On-Chip approach. Traditionally, the compilation and the architectural design sub-spaces have been explored independently. Only recently, some approaches have analyzed the problem of the concurrent exploration of the compilation/architecture sub-spaces. This paper proposes a framework to support the co-exploration phase of the design space composed of architectural parameters and source program transformations. The objective space is multi-dimensional, including conflicting objectives such as energy and delay. In the proposed framework, heuristic co-exploration techniques based on Pareto Simulated Annealing (PSA) have been used to efficiently explore the architecture/compiler co-design space. A first result of this paper consists of showing how the architecture/compiler co-exploration can be more effective than a traditional two-phase exploration. Since the co-exploration space is quite large, to speed up the co-exploration phase by several orders of magnitude over simulation-based approaches, a methodology based on analytical models has been introduced in the co-exploration framework. The goal of analytical models is to quickly evaluate energy/delay metrics at the systems level, while maintaining accuracy with respect to simulation-based co-exploration. The proposed co-exploration framework has been applied to a parameterized SoC superscalar architecture during the execution of a selected set of multimedia applications.},
journal = {Des. Autom. Embedded Syst.},
month = mar,
pages = {1–23},
numpages = {23},
keywords = {Source code transformations, Low-power design, Hardware/software co-exploration, Embedded systems}
}

@article{10.1016/j.scico.2019.07.003,
author = {Vidal, Santiago and Oizumi, Willian and Garcia, Alessandro and D\'{\i}az Pace, Andr\'{e}s and Marcos, Claudia},
title = {Ranking architecturally critical agglomerations of code smells},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2019.07.003},
doi = {10.1016/j.scico.2019.07.003},
journal = {Sci. Comput. Program.},
month = aug,
pages = {64–85},
numpages = {22},
keywords = {Software architecture, Agglomerations, Code smells}
}

@inproceedings{10.1007/11763864_6,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {Inter-organisational approach in rapid software product family development — a case study},
year = {2006},
isbn = {3540346066},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11763864_6},
doi = {10.1007/11763864_6},
abstract = {Software product families provide an efficient means of reuse between a set of related products. However, software product families are often solely associated with intra-organisational reuse. This paper presents a case study of Fathammer, a small company developing games for different mobile devices. Reuse at Fathammer takes place at multiple levels. The game framework and engine of Fathammer is reused by partner companies that in turn produce game assets to be reused by Fathammer while developing games for various devices. Very rapid development of games is a necessity for Fathammer, whereas maintainability of games is not important. The above characteristics in particular distinguish Fathammer from other case studies and practices usually presented in the product family literature. The results show the applicability and challenges of software product family practices in the context of multiple collaborating companies and a fast-changing domain.},
booktitle = {Proceedings of the 9th International Conference on Reuse of Off-the-Shelf Components},
pages = {73–86},
numpages = {14},
location = {Turin, Italy},
series = {ICSR'06}
}

@article{10.1145/3280848,
author = {Pereira, Fernando Magno Quint\~{a}o and Leobas, Guilherme Vieira and Gamati\'{e}, Abdoulaye},
title = {Static Prediction of Silent Stores},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3280848},
doi = {10.1145/3280848},
abstract = {A store operation is called “silent” if it writes in memory a value that is already there. The ability to detect silent stores is important, because they might indicate performance bugs, might enable code optimizations, and might reveal opportunities of automatic parallelization, for instance. Silent stores are traditionally detected via profiling tools. In this article, we depart from this methodology and instead explore the following question: is it possible to predict silentness by analyzing the syntax of programs? The process of building an answer to this question is interesting in itself, given the stochastic nature of silent stores, which depend on data and coding style. To build such an answer, we have developed a methodology to classify store operations in terms of syntactic features of programs. Based on such features, we develop different kinds of predictors, some of which go much beyond what any trivial approach could achieve. To illustrate how static prediction can be employed in practice, we use it to optimize programs running on nonvolatile memory systems.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {44},
numpages = {26},
keywords = {static analysis, nonvolatile memory, machine learning, code optimization, Silent stores}
}

@inproceedings{10.5555/3124497.3124507,
author = {Guerra, Eduardo and Nakagawa, Elisa Yumi},
title = {Relating patterns and reference architectures},
year = {2015},
isbn = {9781941652039},
publisher = {The Hillside Group},
address = {USA},
abstract = {Both patterns and reference architectures aim to describe solutions to be reused for the software systems development. Despite that, they have a lot of differences and have been investigated separately. The objective of this paper is to discuss the relationship between them, how they can be complementary, while respecting their respective peculiarities. We also discuss how patterns can support the creation of reference architectures and how reference architectures can be a source for pattern mining.},
booktitle = {Proceedings of the 22nd Conference on Pattern Languages of Programs},
articleno = {8},
numpages = {9},
keywords = {software architecture, reference architecture, patterns},
location = {Pittsburgh, Pennsylvania},
series = {PLoP '15}
}

@inproceedings{10.1145/2884781.2884861,
author = {Tan, Tian Huat and Chen, Manman and Sun, Jun and Liu, Yang and Andr\'{e}, \'{E}tienne and Xue, Yinxing and Dong, Jin Song},
title = {Optimizing selection of competing services with probabilistic hierarchical refinement},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884861},
doi = {10.1145/2884781.2884861},
abstract = {Recently, many large enterprises (e.g., Netflix, Amazon) have decomposed their monolithic application into services, and composed them to fulfill their business functionalities. Many hosting services on the cloud, with different Quality of Service (QoS) (e.g., availability, cost), can be used to host the services. This is an example of competing services. QoS is crucial for the satisfaction of users. It is important to choose a set of services that maximize the overall QoS, and satisfy all QoS requirements for the service composition. This problem, known as optimal service selection, is NP-hard. Therefore, an effective method for reducing the search space and guiding the search process is highly desirable. To this end, we introduce a novel technique, called Probabilistic Hierarchical Refinement (ProHR). ProHR effectively reduces the search space by removing competing services that cannot be part of the selection. ProHR provides two methods, probabilistic ranking and hierarchical refinement, that enable smart exploration of the reduced search space. Unlike existing approaches that perform poorly when QoS requirements become stricter, ProHR maintains high performance and accuracy, independent of the strictness of the QoS requirements. ProHR has been evaluated on a publicly available dataset, and has shown significant improvement over existing approaches.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {85–95},
numpages = {11},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1016/j.csi.2019.103362,
author = {Barros-Justo, Jos\'{e} L. and Benitti, Fabiane B.V. and Tiwari, Saurabh},
title = {The impact of Use Cases in real-world software development projects: A systematic mapping study},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2019.103362},
doi = {10.1016/j.csi.2019.103362},
journal = {Comput. Stand. Interfaces},
month = oct,
numpages = {16},
keywords = {Software engineering, Impact in industry, UML Use Cases, Systematic mapping study, Evidence-based software engineering}
}

@inproceedings{10.5555/1949303.1949307,
author = {Johnsen, Einar Broch and Owe, Olaf and Schlatte, Rudolf and Tarifa, Silvia Lizeth Tapia},
title = {Validating timed models of deployment components with parametric concurrency},
year = {2010},
isbn = {3642180698},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Many software systems today are designed without assuming a fixed underlying architecture, and may be adapted for sequential, multicore, or distributed deployment. Examples of such systems are found in, e.g., software product lines, service-oriented computing, information systems, embedded systems, operating systems, and telephony. Models of such systems need to capture and range over relevant deployment scenarios, so it is interesting to lift aspects of low-level deployment concerns to the abstraction level of the modeling language. This paper proposes an abstract model of deployment components for concurrent objects, extending the Creol modeling language. The deployment components are parametric in the amount of concurrency they provide; i.e., they vary in processing resources. We give a formal semantics of deployment components and characterize equivalence between deployment components which differ in concurrent resources in terms of test suites. Our semantics is executable on Maude, which allows simulations and test suites to be applied to a deployment component with different concurrent resources.},
booktitle = {Proceedings of the 2010 International Conference on Formal Verification of Object-Oriented Software},
pages = {46–60},
numpages = {15},
location = {Paris, France},
series = {FoVeOOS'10}
}

@inproceedings{10.5555/3049877.3049879,
author = {Lapouchnian, Alexei and Yu, Yijun and Liaskos, Sotirios and Mylopoulos, John},
title = {Requirements-driven design of autonomic application software},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying the objectives of the system, analyzing alternative ways of how these objectives can be met, and designing a system that supports all or some of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and demonstrates this on an example.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {23–37},
numpages = {15},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@inproceedings{10.5555/978-3-030-29983-5_fm,
title = {Front Matter},
year = {2019},
isbn = {978-3-030-29982-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Software Architecture: 13th European Conference, ECSA 2019, Paris, France, September 9–13, 2019, Proceedings},
pages = {i–xxii},
location = {Paris, France}
}

@inproceedings{10.5555/2486788.2486923,
author = {Bellomo, Stephany and Nord, Robert L. and Ozkaya, Ipek},
title = {A study of enabling factors for rapid fielding: combined practices to balance speed and stability},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Agile projects are showing greater promise in rapid fielding as compared to waterfall projects. However, there is a lack of clarity regarding what really constitutes and contributes to success. We interviewed project teams with incremental development lifecycles, from five government and commercial organizations, to gain a better understanding of success and failure factors for rapid fielding on their projects. A key area we explored involves how Agile projects deal with the pressure to rapidly deliver high-value capability, while maintaining project speed (delivering functionality to the users quickly) and product stability (providing reliable and flexible product architecture). For example, due to schedule pressure we often see a pattern of high initial velocity for weeks or months, followed by a slowing of velocity due to stability issues. Business stakeholders find this to be disruptive as the rate of capability delivery slows while the team addresses stability problems. We found that experienced practitioners, when faced with these challenges, do not apply Agile practices alone. Instead they combine practicesAgile, architecture, or otherin creative ways to respond quickly to unanticipated stability problems. In this paper, we summarize the practices practitioners we interviewed from Agile projects found most valuable and provide an overarching scenario that provides insight into how and why these practices emerge.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {982–991},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/1808937.1808945,
author = {Ivanovi\'{c}, Ana and America, Pierre},
title = {Information needed for architecture decision making},
year = {2010},
isbn = {9781605589688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808937.1808945},
doi = {10.1145/1808937.1808945},
abstract = {This paper focuses on the business aspects of architecture decision making -- in particular information needed by managers and architects for making architecture investment decisions. We present the results of 19 interviews in an industrial organization aimed at identifying information used for architecture decision making in the context of product lines. We summarize the interview findings to investigate future possibilities in improving architecture decision making.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Product Line Approaches in Software Engineering},
pages = {54–57},
numpages = {4},
keywords = {managers, decision making, architecture investment, architects},
location = {Cape Town, South Africa},
series = {PLEASE '10}
}

@inproceedings{10.1145/2578128.2578238,
author = {Guessi, Milena and Oquendo, Flavio and Nakagawa, Elisa Yumi},
title = {Variability viewpoint to describe reference architectures},
year = {2014},
isbn = {9781450325233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578128.2578238},
doi = {10.1145/2578128.2578238},
abstract = {Reference architectures have emerged as a special type of software architecture that achieves well-recognized understanding of specific domains. Their purpose is therefore to be a guidance for the development, standardization, and evolution of systems of such domains or neighbor domains. Adequate representation of such architectures is essential to promote their effective use and dissemination, using, for instance, different architectural viewpoints. A comprehensive description of reference architectures should not only record common features and functionalities, but also variations that could be present in the instances of these architectures. In this scenario, the main contribution of our work is to propose an architecture viewpoint to represent variability in reference architectures. We also describe the steps for creating such viewpoint and present an example of a technique that could be used to represent it. A case study is also presented, demonstrating the feasibility of our approach. Based on initial results, we have observed that the variability viewpoint could contribute to a more adequate, complete description of reference architectures and, as a consequence, it could promote a more effective dissemination and use of such architectures.},
booktitle = {Proceedings of the WICSA 2014 Companion Volume},
articleno = {14},
numpages = {6},
keywords = {variability, reference architecture, architecture description},
location = {Sydney, Australia},
series = {WICSA '14 Companion}
}

@article{10.5555/2685119.2685127,
author = {Kazemi, Ali and Rostampour, Ali and Haghighi, Hassan and Abbasi, Sahel},
title = {A conceptual cohesion metric for service oriented systems},
year = {2014},
issue_date = {July 2014},
publisher = {Rinton Press, Incorporated},
address = {Paramus, NJ},
volume = {13},
number = {3–4},
issn = {1540-9589},
abstract = {Service conceptual cohesion has an incredible impact on the reusability and maintainability of service-oriented software systems. Conceptual cohesion indicates the degree of focus of services on a single business functionality. Current metrics for measuring service cohesion reflect the structural aspect of cohesion and therefore cannot be utilized to measure conceptual cohesion of services. Latent Semantic Indexing (LSI), on the other hand, is an information retrieval technique widely used to measure the degree of similarity between a set of text based documents. In our previous work, a metric, namely SCD (Service Cohesion Degree), has been proposed that measures conceptual cohesion of services based on the LSI technique. SCD provides a quantitative evaluation to measure how much a service concentrates on a single business functionality. In addition, SCD is applied in the service identification step, i.e., when services are not yet available, and the designer plans for developing services with high cohesion. This paper has two contributions in comparison to our previous work. At first, it resolves two anomalies occurring in our previous method when calculating conceptual relationship between service operations. Secondly, as the main contribution of the paper, it presents details of a theoretical validation and an empirical evaluation of SCD. By using a small-scale controlled study, the empirical evaluation demonstrates that SCD could measure conceptual cohesion of services acceptably.},
journal = {J. Web Eng.},
month = jul,
pages = {302–332},
numpages = {31},
keywords = {software metrics, service-oriented design principle, service conceptual cohesion, latent semantic indexing}
}

@inproceedings{10.1145/2465478.2465492,
author = {Schmid, Klaus},
title = {A formal approach to technical debt decision making},
year = {2013},
isbn = {9781450321266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465478.2465492},
doi = {10.1145/2465478.2465492},
abstract = {The notion of technical debt attracts significant attention, especially in the context of reconciling architecture and agile development. However, most work on technical debt is still largely informal and if it provides a formalization it is often ad-hoc. In this paper, we provide a detailed, formal analysis of decision making on technical debt in development. Using this formalization, we show that optimal decision making is not effectively computable in real-world situations and provide several well-defined approximations that allow to handle the problem nevertheless in practical situations. Combining these approximations in a single method leads to a light-weight approach that can be effectively applied in iterative software development, including agile approaches.},
booktitle = {Proceedings of the 9th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {153–162},
numpages = {10},
keywords = {Technical Debt, Software Systems, Incremental Development, Formal Model, Decision Making, Cost Estimation},
location = {Vancouver, British Columbia, Canada},
series = {QoSA '13}
}

@inproceedings{10.1145/2695664.2695875,
author = {Almeida, Andr\'{e} and Bencomo, Nelly and Batista, Thais and Cavalcante, Everton and Dantas, Francisco},
title = {Dynamic decision-making based on NFR for managing software variability and configuration selection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695875},
doi = {10.1145/2695664.2695875},
abstract = {Due to dynamic variability, identifying the specific conditions under which non-functional requirements (NFRs) are satisfied may be only possible at runtime. Therefore, it is necessary to consider the dynamic treatment of relevant information during the requirements specifications. The associated data can be gathered by monitoring the execution of the application and its underlying environment to support reasoning about how the current application configuration is fulfilling the established requirements. This paper presents a dynamic decision-making infrastructure to support both NFRs representation and monitoring, and to reason about the degree of satisfaction of NFRs during runtime. The infrastructure is composed of: (i) an extended feature model aligned with a domain-specific language for representing NFRs to be monitored at runtime; (ii) a monitoring infrastructure to continuously assess NFRs at runtime; and (iii) a flexible decision-making process to select the best available configuration based on the satisfaction degree of the NRFs. The evaluation of the approach has shown that it is able to choose application configurations that well fit user NFRs based on runtime information. The evaluation also revealed that the proposed infrastructure provided consistent indicators regarding the best application configurations that fit user NFRs. Finally, a benefit of our approach is that it allows us to quantify the level of satisfaction with respect to NFRs specification.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1376–1382},
numpages = {7},
keywords = {variability, non-functional requirements, monitoring, SPLs},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1016/j.infsof.2017.10.012,
author = {Budgen, David and Brereton, Pearl and Williams, Nikki and Drummond, Sarah},
title = {The contribution that empirical studies performed in industry make to the findings of systematic reviews},
year = {2018},
issue_date = {February 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {94},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.10.012},
doi = {10.1016/j.infsof.2017.10.012},
abstract = {ContextSystematic reviews can provide useful knowledge for software engineering practice, by aggregating and synthesising empirical studies related to a specific topic. ObjectiveWe sought to assess how far the findings of systematic reviews addressing practice-oriented topics have been derived from empirical studies that were performed in industry or that used industry data. MethodWe drew upon and augmented the data obtained from a tertiary study that performed a systematic review of systematic reviews published in the period up to the end of 2015, seeking to identify those with findings that are relevant for teaching and practice. For the supplementary analysis reported here, we then examined the profiles of the primary studies as reported in each systematic review. ResultsWe identified 48 systematic reviews as candidates for further analysis. The many differences that arise between systematic reviews, together with the incompleteness of reporting for these, mean that our counts should be treated as indicative rather than definitive. However, even when allowing for problems of classification, the findings from the majority of these systematic reviews were predominantly derived from using primary studies conducted in industry. There was also an emphasis upon the use of case studies, and a number of the systematic reviews also made some use of weaker experience or even opinion papers. ConclusionsPrimary studies from industry play an important role as inputs to systematic reviews. Using more rigorous industry-based primary studies can give greater authority to the findings of the systematic reviews, and should help with the creation of a corpus of sound empirical data to support evidence-informed decisions.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {234–244},
numpages = {11},
keywords = {Systematic review, Primary study, Industry study, Case study}
}

@inproceedings{10.1007/978-3-540-30554-5_19,
author = {Spinczyk, Olaf and Schoettner, Michael and Gal, Andreas},
title = {Programming languages and operating systems},
year = {2004},
isbn = {354023988X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30554-5_19},
doi = {10.1007/978-3-540-30554-5_19},
abstract = {This report gives an overview over the First ECOOP Workshop on Programming Languages and Operating Systems (PLOS 2004). It explains the motivation for the workshop and gives a summary of the workshop contributions and discussions during the workshop.},
booktitle = {Proceedings of the 2004 International Conference on Object-Oriented Technology},
pages = {202–213},
numpages = {12},
location = {Oslo, Norway},
series = {ECOOP'04}
}

@inproceedings{10.1007/978-3-642-33666-9_41,
author = {Iqbal, Muhammad Zohaib and Ali, Shaukat and Yue, Tao and Briand, Lionel},
title = {Experiences of applying UML/MARTE on three industrial projects},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_41},
doi = {10.1007/978-3-642-33666-9_41},
abstract = {MARTE (Modeling and Analysis of Real-Time and Embedded Systems) is a UML profile, which has been developed to model concepts specific to Real-Time and Embedded Systems (RTES). In previous years, we have applied UML/MARTE to three distinct industrial problems in various industry sectors: architecture modeling and configuration of large-scale and highly configurable integrated control systems, model-based robustness testing of communication-intensive systems, and model-based environment simulator generation of large-scale RTES for testing. In this paper, we report on our experiences of solving these problems by applying UML/MARTE on four industrial case studies. Based on our common experiences, we derive a framework to help practitioners for future applications of UML/MARTE. The framework provides a set of detailed guidelines on how to apply MARTE in industrial contexts and will help reduce the gap between the modeling standards and industrial needs.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {642–658},
numpages = {17},
keywords = {real-time embedded systems, model-based testing, architecture modeling, UML, MARTE},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.5555/1759394.1759396,
author = {Bosch, Jan},
title = {Software product families: towards compositionality},
year = {2007},
isbn = {9783540712886},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product families have become the most successful approach to intra-organizational reuse. Especially in the embedded systems industry, but also elsewhere, companies are building rich and diverse product portfolios based on software platforms that capture the commonality between products while allowing for their differences. Software product families, however, easily become victims of their own success in that, once successful, there is a tendency to increase the scope of the product family by incorporating a broader and more diverse product portfolio. This requires organizations to change their approach to product families from relying on a pre-integrated platform for product derivation to a compositional approach where platform components are composed in a product-specific configuration.},
booktitle = {Proceedings of the 10th International Conference on Fundamental Approaches to Software Engineering},
pages = {1–10},
numpages = {10},
keywords = {software product families, compositionality},
location = {Braga, Portugal},
series = {FASE'07}
}

@article{10.1016/j.cie.2016.09.023,
author = {Addo-Tenkorang, Richard and Helo, Petri T.},
title = {Big data applications in operations/supply-chain management},
year = {2016},
issue_date = {November 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {101},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2016.09.023},
doi = {10.1016/j.cie.2016.09.023},
abstract = {Harnessing optimum value from industrial data increased in the last two decades.A detailed review of "big data" application in operations/SC management processes.Proposed (Value-adding - V5) framework for operation/SC management. PurposeBig data is increasingly becoming a major organizational enterprise force to reckon with in this global era for all sizes of industries. It is a trending new enterprise system or platform which seemingly offers more features for acquiring, storing and analysing voluminous generated data from various sources to obtain value-additions. However, current research reveals that there is limited agreement regarding the performance of "big data." Therefore, this paper attempts to thoroughly investigate "big data," its application and analysis in operations or supply-chain management, as well as the trends and perspectives in this research area. This paper is organized in the form of a literature review, discussing the main issues of "big data" and its extension into "big data II"/IoT-value-adding perspectives by proposing a value-adding framework. Methodology/research approachThe research approach employed is a comprehensive literature review. About 100 or more peer-reviewed journal articles/conference proceedings as well as industrial white papers are reviewed. Harzing Publish or Perish software was employed to investigate and critically analyse the trends and perspectives of "big data" applications between 2010 and 2015. Findings/resultsThe four main attributes or factors identified with "big data" include - big data development sources (Variety - V1), big data acquisition (Velocity - V2), big data storage (Volume - V3), and finally big data analysis (Veracity - V4). However, the study of "big data" has evolved and expanded a lot based on its application and implementation processes in specific industries in order to create value (Value-adding - V5) - "Big Data cloud computing perspective/Internet of Things (IoT)". Hence, the four Vs of "big data" is now expanded into five Vs. Originality/value of researchThis paper presents original literature review research discussing "big data" issues, trends and perspectives in operations/supply-chain management in order to propose "Big data II" (IoT - Value-adding) framework. This proposed framework is supposed or assumed to be an extension of "big data" in a value-adding perspective, thus proposing that "big data" be explored thoroughly in order to enable industrial managers and businesses executives to make pre-informed strategic operational and management decisions for increased return-on-investment (ROI). It could also empower organizations with a value-adding stream of information to have a competitive edge over their competitors.},
journal = {Comput. Ind. Eng.},
month = nov,
pages = {528–543},
numpages = {16},
keywords = {Operations/supply-chain management, Master database management, Internet of Things (IoT), Cloud computing, Big data - applications and analysis}
}

@article{10.1016/j.jcss.2014.11.008,
author = {Yu, Jian and Sheng, Quan Z. and Swee, Joshua K.Y. and Han, Jun and Liu, Chengfei and Noor, Talal H.},
title = {Model-driven development of adaptive web service processes with aspects and rules},
year = {2015},
issue_date = {May 2015},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {81},
number = {3},
issn = {0022-0000},
url = {https://doi.org/10.1016/j.jcss.2014.11.008},
doi = {10.1016/j.jcss.2014.11.008},
abstract = {Modern software systems are frequently required to be adaptive in order to cope with constant changes. Unfortunately, service-oriented systems built with WS-BPEL are still too rigid. In this paper, we propose a novel model-driven approach to supporting the development of dynamically adaptive WS-BPEL based systems. We model the system functionality with two distinct but highly correlated parts: a stable part called the base model describing the flow logic aspect and a volatile part called the variable model describing the decision logic aspect. We develop an aspect-oriented method to weave the base model and the variable model together so that runtime changes can be applied to the variable model without affecting the base model. A model-driven platform has been implemented to support the development of adaptive WS-BPEL processes. In-lab experiments show that our approach has low performance overhead. A real-life case study also validates the applicability of our approach.},
journal = {J. Comput. Syst. Sci.},
month = may,
pages = {533–552},
numpages = {20},
keywords = {Web services, Model-driven development, Design tools and techniques, Aspect-oriented methodology, Adaptive systems}
}

@article{10.1016/j.jss.2016.11.028,
author = {Kaur, Loveleen and Mishra, Ashutosh},
title = {Software component and the semantic Web},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {125},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.11.028},
doi = {10.1016/j.jss.2016.11.028},
abstract = {A detailed study on the use of Semantic Web technologies in CBSE.Advancement is being made from using ontological concepts to Linked Data in CBSE.Semantic web techniques help in addressing the various challenges that CBSE faces.Simple and freely accessible CBSE applications that employ Semantic Web are limited.Some open issues relevant to the topic in concern have been examined. With the advent of Component-based software engineering (CBSE), large software systems are being built by integrating pre-built software components. The Semantic Web in association with CBSE has shown to offer powerful representation facilities and reasoning techniques to enhance and support querying, reasoning, discovery, etc. of software components. The goal of this paper is to research the applicability of Semantic Web technologies in performing the various tasks of CBSE and review the experimental results of the same in an easy and effective manner. To the best of our knowledge, this is the first study which provides an extensive review of the application of Semantic Web in CBSE from different perspectives. A systematic literature review of the Semantic Web approaches, employed for use in CBSE, reported from 2001 until 2015, is conducted in this research article. Empirical results have been drawn through the question-answer based analysis of the research, which clearly tells the year wise trend of the research articles, with the possible justification of the usage of Semantic Web technology and tools for a particular phase of CBSE. To conclude, gaps in the current research and potential future prospects have been discussed.},
journal = {J. Syst. Softw.},
month = mar,
pages = {152–169},
numpages = {18},
keywords = {Web services, Semantic Web, Reasoners, Ontology, Linked Data, Component-based software engineering}
}

@inproceedings{10.1109/MISE.2009.5069896,
author = {Acher, Mathieu and Lahire, Philippe and Moisan, Sabine and Rigault, Jean-Paul},
title = {Tackling high variability in video surveillance systems through a model transformation approach},
year = {2009},
isbn = {9781424437221},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MISE.2009.5069896},
doi = {10.1109/MISE.2009.5069896},
abstract = {This work explores how model-driven engineering techniques can support the configuration of systems in domains presenting multiple variability factors. Video surveillance is a good candidate for which we have an extensive experience. Ultimately, we wish to automatically generate a software component assembly from an application specification, using model to model transformations. The challenge is to cope with variability both at the specification and at the implementation levels. Our approach advocates a clear separation of concerns. More precisely, we propose two feature models, one for task specification and the other for software components. The first model can be transformed into one or several valid component configurations through step-wise specialization. This paper outlines our approach, focusing on the two feature models and their relations. We particularly insist on variability and constraint modeling in order to achieve the mapping from domain variability to software variability through model transformations.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Modeling in Software Engineering},
pages = {44–49},
numpages = {6},
series = {MISE '09}
}

@article{10.1007/s10270-017-0622-9,
author = {Bruneliere, Hugo and Burger, Erik and Cabot, Jordi and Wimmer, Manuel},
title = {A feature-based survey of model view approaches},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0622-9},
doi = {10.1007/s10270-017-0622-9},
abstract = {When dealing with complex systems, information is very often fragmented across many different models expressed within a variety of (modeling) languages. To provide the relevant information in an appropriate way to different kinds of stakeholders, (parts of) such models have to be combined and potentially revamped by focusing on concerns of particular interest for them. Thus, mechanisms to define and compute views over models are highly needed. Several approaches have already been proposed to provide (semi)automated support for dealing with such model views. This paper provides a detailed overview of the current state of the art in this area. To achieve this, we relied on our own experiences of designing and applying such solutions in order to conduct a literature review on this topic. As a result, we discuss the main capabilities of existing approaches and propose a corresponding research agenda. We notably contribute a feature model describing what we believe to be the most important characteristics of the support for views on models. We expect this work to be helpful to both current and potential future users and developers of model view techniques, as well as to any person generally interested in model-based software and systems engineering.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {1931–1952},
numpages = {22},
keywords = {Viewpoint, View, Survey, Modeling, Model}
}

@inproceedings{10.5555/645882.672257,
author = {Thiel, Steffen and Hein, Andreas},
title = {Systematic Integration of Variability into Product Line Architecture Design},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product lines consider related products, their commonalities and their differences. The differences between the single products are also referred to as variability. Consequently, variability is inherent in every product line and makes a key difference as compared to single systems. While, on the requirements level, the methods for analyzing product line variability are understood today, their transition to architecture remains vague. Bringing variability to architecture as an "add-on" is just a provisional solution and forebodes the risk of violating other intentions. This paper presents a systematic approach to integrate variability with product line architecture design. In particular, it promotes variability as an architectural driver, embeds variability requirements in the architecture design framework "Quality-Driven Software Architecting" (QUASAR), and gives guidelines and examples for documenting variability in architectural views.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {130–153},
numpages = {24},
series = {SPLC 2}
}

@article{10.1016/j.scico.2012.07.019,
author = {Chabridon, Sophie and Conan, Denis and Abid, Zied and Taconet, Chantal},
title = {Building ubiquitous QoC-aware applications through model-driven software engineering},
year = {2013},
issue_date = {October, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {10},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.07.019},
doi = {10.1016/j.scico.2012.07.019},
abstract = {As every-day mobile devices can easily be equipped with multiple sensing capabilities, ubiquitous applications are expected to exploit the richness of the context information that can be collected by these devices in order to provide the service that is the most appropriate to the situation of the user. However, the design and implementation of such context-aware ubiquitous appplications remain challenging as there exist very few models and tools to guide application designers and developers in mastering the complexity of context information. This becomes even more crucial as context is by nature imperfect. One way to address this issue is to associate to context information meta-data representing its quality. We propose a generic and extensible design process for context-aware applications taking into account the quality of context (QoC). We demonstrate its use on a prototype application for sending flash sale offers to mobile users. We present extensive performance results in terms of memory and processing time of both elementary context management operations and the whole context policy implementing the Flash sale application. The cost of adding QoC management is also measured and appears to be limited to a few milliseconds. We show that a context policy with 120 QoC-aware nodes can be processed in less than 100 ms on a mobile phone. Moreover, a policy of almost 3000 nodes can be instantiated before exhausting the resources of the phone. This enables very rich application scenarios enhancing the user experience and will favor the development of new ubiquitous applications.},
journal = {Sci. Comput. Program.},
month = oct,
pages = {1912–1929},
numpages = {18},
keywords = {Ubiquitous computing, Quality of context, Pervasive computing, Model-driven software engineering, Domain specific language, Context}
}

@inproceedings{10.1145/375212.375277,
author = {Savolainen, Juha and Kuusela, Juha},
title = {Violatility analysis framework for product lines},
year = {2001},
isbn = {1581133588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375212.375277},
doi = {10.1145/375212.375277},
abstract = {Evolution of a software intensive system is unavoidable. In fact, evolution can be seen as a part of reuse process. During the evolution of the software asset, the major part of the system functionality is normally reused. So the key issue is to identify the volatile parts of the domain requirements. Additionally, there is promise that tailored tool support may help supporting evolution in software intensive systems. In this paper, we describe the volatility analysis method for product lines. This highly practical method has been used in multiple domains and is able to express and estimate common types of evolutional characteristics. The method is able to represent volatility in multiple levels and has capacity to tie the volatility estimation to one product line member specification. We  also briefly describe current tool support for the method. The main contribution of this paper is a volatility analysis framework that can be used to describe how requirements are estimated to evolve in the future. The method is based on the definition hierarchy framework.},
booktitle = {Proceedings of the 2001 Symposium on Software Reusability: Putting Software Reuse in Context},
pages = {133–141},
numpages = {9},
keywords = {volatility analysis, variability, requirements engineering, product line, evolution, domain analysis, commonality},
location = {Toronto, Ontario, Canada},
series = {SSR '01}
}

@inproceedings{10.1145/967900.968080,
author = {Agosta, Giovanni and Palermo, Gianluca and Silvano, Cristina},
title = {Multi-objective co-exploration of source code transformations and design space architectures for low-power embedded systems},
year = {2004},
isbn = {1581138121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/967900.968080},
doi = {10.1145/967900.968080},
abstract = {The exploration of the architectural design space in terms of energy and performance is of mainly importance for a broad range of embedded platforms based on the System-On-Chip approach. This paper proposes a methodology for the co-exploration of the design space composed of architectural parameters and source program transformations. A heuristic technique based on Pareto Simulated Annealing (PSA) has been used to efficiently span the multi-objective co-design space composed of the product of the parameters related to the selected program transformations and the configurable architecture. The analysis of the proposed framework has been carried out for a parameterized superscalar architecture executing a selected set of benchmarks. The reported results show the effectiveness of the proposed co-exploration with respect to the independent exploration of the transformation and architectural spaces to efficiently derive approximate Pareto curves.},
booktitle = {Proceedings of the 2004 ACM Symposium on Applied Computing},
pages = {891–896},
numpages = {6},
keywords = {source code transformations, low-power design, hardware/software co-exploration, embedded systems},
location = {Nicosia, Cyprus},
series = {SAC '04}
}

@article{10.1007/s10664-009-9121-0,
author = {Falessi, Davide and Babar, Muhammad Ali and Cantone, Giovanni and Kruchten, Philippe},
title = {Applying empirical software engineering to software architecture: challenges and lessons learned},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-009-9121-0},
doi = {10.1007/s10664-009-9121-0},
abstract = {In the last 15 years, software architecture has emerged as an important software engineering field for managing the development and maintenance of large, software-intensive systems. Software architecture community has developed numerous methods, techniques, and tools to support the architecture process (analysis, design, and review). Historically, most advances in software architecture have been driven by talented people and industrial experience, but there is now a growing need to systematically gather empirical evidence about the advantages or otherwise of tools and methods rather than just rely on promotional anecdotes or rhetoric. The aim of this paper is to promote and facilitate the application of the empirical paradigm to software architecture. To this end, we describe the challenges and lessons learned when assessing software architecture research that used controlled experiments, replications, expert opinion, systematic literature reviews, observational studies, and surveys. Our research will support the emergence of a body of knowledge consisting of the more widely-accepted and well-formed software architecture theories.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {250–276},
numpages = {27},
keywords = {Software architecture, Empirical software engineering}
}

@inproceedings{10.1007/978-3-642-12261-3_7,
author = {Van Baelen, Stefan and Weigert, Thomas and Ober, Ileana and Espinoza, Huascar and Ober, Iulian},
title = {Model based architecting and construction of embedded systems (ACES-MB 2009)},
year = {2009},
isbn = {3642122604},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12261-3_7},
doi = {10.1007/978-3-642-12261-3_7},
abstract = {The second ACES-MB workshop brought together researchers and practitioners interested in model-based software engineering for real-time embedded systems, with a particular focus on the use of models for architecture description and domain-specific design, and for capturing non-functional constraints. Eleven presenters proposed contributions on domain-specific languages for embedded systems, the Architecture Analysis and Design Language (AADL), analysis and formalization, semantics preservation issues, and variability and reconfiguration. In addition, a lively group discussion tackled the issue of combining models from different Domain Specific Modeling Languages (DSMLs). This report presents an overview of the presentations and fruitful discussions that took place during the ACES-MB 2009 workshop.},
booktitle = {Proceedings of the 2009 International Conference on Models in Software Engineering},
pages = {63–67},
numpages = {5},
location = {Denver, CO},
series = {MODELS'09}
}

@article{10.1016/j.infsof.2012.04.001,
author = {Abramov, Jenny and Sturm, Arnon and Shoval, Peretz},
title = {Evaluation of the Pattern-based method for Secure Development (PbSD): A controlled experiment},
year = {2012},
issue_date = {September, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.04.001},
doi = {10.1016/j.infsof.2012.04.001},
abstract = {Context: Security in general, and database protection from unauthorized access in particular, are crucial for organizations. Although it has been long accepted that the important system requirements should be considered from the early stages of the development process, non-functional requirements such as security tend to get neglected or dealt with only at later stages of the development process. Objective: We present an empirical study conducted to evaluate a Pattern-based method for Secure Development - PbSD - that aims to help developers, in particular database designers, to design database schemata that comply with the organizational security policies regarding authorization, from the early stages of development. The method provides a complete framework to guide, enforce and verify the correct implementation of security policies within a system design, and eventually generate a database schema from that design. Method: The PbSD method was evaluated in comparison with a popular existing method that directly specifies the security requirements in SQL and Oracle's VPD. The two methods were compared with respect to the quality of the created access control specifications, the time it takes to complete the specification, and the perceived quality of the methods. Results: We found that the quality of the access control specifications using the PbSD method for secure development were better with respect to privileges granted in the table, column and row granularity levels. Moreover, subjects who used the PbSD method completed the specification task in less time compared to subjects who used SQL. Finally, the subjects perceived the PbSD method clearer and more easy to use. Conclusion: The pattern-based method for secure development can enhance the quality of security specification of databases, and decrease the software development time and cost. The results of the experiment may also indicate that the use of patterns in general has similar benefits; yet this requires further examinations.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {1029–1043},
numpages = {15},
keywords = {Security patterns, Secure software development, Model driven development, Database design, Controlled experiment, Authorization}
}

@inproceedings{10.1145/2088876.2088882,
author = {Huynh, Ngoc-Tho and Phung-Khac, An and Segarra, Maria-Teresa},
title = {Towards reliable distributed reconfiguration},
year = {2011},
isbn = {9781450310703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2088876.2088882},
doi = {10.1145/2088876.2088882},
abstract = {In component-based software engineering, reconfiguration often refers to the activity of changing a running software system at the component level. Reconfiguration is widely used for evolving and adapting software systems that can not be shut down for update. However, in distributed systems, supporting reconfiguration is a challenging task since a reconfiguration consists of distributed reconfiguration actions that need to be coordinated. Particularly, this task becomes much more challenging in the context of unstable networks where nodes may disconnect frequently, even during reconfiguration. To address this challenge, we propose a platform supporting distributed reconfiguration that embodies a solution for managing system states at reconfiguration time. We define (1) different system states regarding reconfiguration and (2) ways that the system will act accordingly. When a disconnection is detected during a reconfiguration, the system may correct reconfiguration plans to continue the reconfiguration if possible, or recover if the reconfiguration fails.},
booktitle = {Adaptive and Reflective Middleware on Proceedings of the International Workshop},
pages = {36–41},
numpages = {6},
keywords = {reliability, distributed reconfiguration, component-based software engineering},
location = {Lisbon, Portugal},
series = {ARM '11}
}

@inproceedings{10.5555/2041790.2041806,
author = {D\'{\i}az, Jessica and P\'{e}rez, Jennifer and Garbajosa, Juan and Wolf, Alexander L.},
title = {Change impact analysis in product-line architectures},
year = {2011},
isbn = {9783642237973},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Change impact analysis is fundamental in software evolution, since it allows one to determine potential effects upon a system resulting from changing requirements. While prior work has generically considered change impact analysis at architectural level, there is a distinct lack of support for the kinds of architectures used to realize software product lines, so-called product-line architectures (PLAs). In particular, prior approaches do not account for variability, a specific characteristic of software product lines. This paper presents a new technique for change impact analysis that targets product-line architectures. We propose to join a traceability-based algorithm and a rule-based inference engine to effectively traverse modeling artifacts that account for variability. In contrast to prior approaches, our technique supports the mechanisms for (i) specifying variability in PLAs, (ii) documenting PLA knowledge, and (iii) tracing variability between requirements and PLAs. We demonstrate our technique by applying it to the analysis of requirements changes in the product-line architecture of a banking system.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture},
pages = {114–129},
numpages = {16},
keywords = {product-line evolution, product-line architectures, change impact analysis},
location = {Essen, Germany},
series = {ECSA'11}
}

@article{10.1007/s10664-017-9580-7,
author = {Przyby\l{}ek, Adam},
title = {An empirical study on the impact of AspectJ on software evolvability},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9580-7},
doi = {10.1007/s10664-017-9580-7},
abstract = {Since its inception in 1996, aspect-oriented programming (AOP) has been believed to reduce the effort required to maintain software systems by replacing cross-cutting code with aspects. However, little convincing empirical evidence exists to support this claim, while several studies suggest that AOP brings new obstacles to maintainability. This paper discusses two controlled experiments conducted to evaluate the impact of AspectJ (the most mature and popular aspect-oriented programming language) versus Java on software evolvability. We consider evolvability as the ease with which a software system can be updated to fulfill new requirements. Since a minor language was compared to the mainstream, the experiments were designed so as to anticipate that the participants were much more experienced in one of the treatments. The first experiment was performed on 35 student subjects who were asked to comprehend either Java or AspectJ implementation of the same system, and perform the corresponding comprehension tasks. Participants of both groups achieved a high rate of correct answers without a statistically significant difference between the groups. Nevertheless, the Java group significantly outperformed the AspectJ group with respect to the average completion time. In the second experiment, 24 student subjects were asked to implement (in a non-invasive way) two extension scenarios to the system that they had already known. Each subject evolved either the Java version using Java or the AspectJ version using AspectJ. We found out that a typical AspectJ programmer needs significantly fewer atomic changes to implement the change scenarios than a typical Java programmer, but we did not observe a significant difference in completion time. The overall result indicates that AspectJ has a different effect on two sub-characteristics of the evolvability: understandability and changeability. While AspectJ decreases the former, it improves one aspect of the latter.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2018–2050},
numpages = {33},
keywords = {Understandability, Separation of concerns, Maintainability, Controlled experiment, Aspect-oriented programming, AOP}
}

@article{10.1504/IJWMC.2014.063054,
author = {Siala, Fatma and Ghedira, Khaled},
title = {How to select dynamically a QoS-driven composite web service by a multi-agent system using CBR method},
year = {2014},
issue_date = {July 2014},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {4},
issn = {1741-1084},
url = {https://doi.org/10.1504/IJWMC.2014.063054},
doi = {10.1504/IJWMC.2014.063054},
abstract = {Service-oriented architecture permits the composition of web services provided with different Quality of Service QoS levels. In a given composition, finding the set of services that optimises some QoS attributes under its constraints is a problem that needs to be solved. Our aim is to propose an intelligent approach to the selection of a Composite Web Service CWS based on QoS. This paper reports the authors' recent research on addressing the issue. An overview on the previously proposed approaches is presented. These approaches correspond to several improvements of an existing multi-agent one, which is well-cited in the specialised literature. Each framework, implemented on JADE Java Agent Development framework, improves another one in terms of CPU time and/or QoS score, to reach a new agent-based and scalable framework. The last framework utilises the agents' ability of negotiation, interaction and cooperation in order to facilitate the selection of composite web services. By using CBR method, the agents can memorise QoS scores and availability. The improvements are related not only to the CPU time but also to the Composite QoS CQoS value, while operating in a dynamic environment and taking into account user preferences.},
journal = {Int. J. Wire. Mob. Comput.},
month = jul,
pages = {327–347},
numpages = {21}
}

@inproceedings{10.1145/1774088.1774562,
author = {K\"{a}tev\"{a}, Janne and Laurinen, Perttu and Rautio, Taneli and Suutala, Jaakko and Tuovinen, Lauri and R\"{o}ning, Juha},
title = {SE-155 DBSA: a device-based software architecture for data mining},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774562},
doi = {10.1145/1774088.1774562},
abstract = {In this paper a new architecture for a variety of data mining tasks is introduced. The Device-Based Software Architecture (DBSA) is a highly portable and generic data mining software framework where processing tasks are modeled as components linked together to form a data mining application. The name of the architecture comes from the analogy that each processing task in the framework can be thought of as a device. The framework handles all the devices in the same manner, regardless of whether they have a counterpart in the real world or whether they are just logical devices inside the framework. The DBSA offers many reusable devices, ready to be included in applications, and the application programmer can easily code new devices for the architecture. The framework is bundled with connections to several widely used external tools and languages, making prototyping new applications easy and fast. In the paper we compare DBSA to existing data mining frameworks, review its design and present a case study application implemented with the framework. The paper shows that the DBSA can act as a base for diverse data mining applications.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {2273–2280},
numpages = {8},
keywords = {software frameworks, data mining},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1145/2783258.2783270,
author = {Yan, Feng and Ruwase, Olatunji and He, Yuxiong and Chilimbi, Trishul},
title = {Performance Modeling and Scalability Optimization of Distributed Deep Learning Systems},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783270},
doi = {10.1145/2783258.2783270},
abstract = {Big deep neural network (DNN) models trained on large amounts of data have recently achieved the best accuracy on hard tasks, such as image and speech recognition. Training these DNNs using a cluster of commodity machines is a promising approach since training is time consuming and compute-intensive. To enable training of extremely large DNNs, models are partitioned across machines. To expedite training on very large data sets, multiple model replicas are trained in parallel on different subsets of the training examples with a global parameter server maintaining shared weights across these replicas. The correct choice for model and data partitioning and overall system provisioning is highly dependent on the DNN and distributed system hardware characteristics. These decisions currently require significant domain expertise and time consuming empirical state space exploration.This paper develops performance models that quantify the impact of these partitioning and provisioning decisions on overall distributed system performance and scalability. Also, we use these performance models to build a scalability optimizer that efficiently determines the optimal system configuration that minimizes DNN training time. We evaluate our performance models and scalability optimizer using a state-of-the-art distributed DNN training framework on two benchmark applications. The results show our performance models estimate DNN training time with high estimation accuracy and our scalability optimizer correctly chooses the best configurations, minimizing the training time of distributed DNNs.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1355–1364},
numpages = {10},
keywords = {scalability, performance modeling, optimization, distributed system, deep learning},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@inproceedings{10.1145/2600821.2600826,
author = {Lettner, Daniela and Angerer, Florian and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {A case study on software ecosystem characteristics in industrial automation software},
year = {2014},
isbn = {9781450327541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600821.2600826},
doi = {10.1145/2600821.2600826},
abstract = {In software ecosystems (SECOs) both internal and external developers build software solutions for specific market segments based on common technological platforms. Despite a significant body of research on SECOs there is still a need to empirically investigate the characteristics of SECOs in specific industrial environments to understand and improve development processes. In particular, when defining software processes understanding the roles of the participants in the SECO is crucial. This paper thus reports results of an exploratory case study in the industrial automation domain. We explore two research questions on SECO characteristics and discuss research issues we derived from our analyses. While our study confirms key SECO characteristics reported in the literature we also identify additional properties relevant for development processes in the domain of industrial automation.},
booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
pages = {40–49},
numpages = {10},
keywords = {Software ecosystems, case study, development processes, industrial automation},
location = {Nanjing, China},
series = {ICSSP '14}
}

@inproceedings{10.1145/2994310.2994327,
author = {Mattila, Anna-Liisa and Ihantola, Petri and Kilamo, Terhi and Luoto, Antti and Nurminen, Mikko and V\"{a}\"{a}t\"{a}j\"{a}, Heli},
title = {Software visualization today: systematic literature review},
year = {2016},
isbn = {9781450343671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2994310.2994327},
doi = {10.1145/2994310.2994327},
abstract = {Software visualization means visualizing various aspects and artifacts related to software. By this definition a wide range of different software engineering aspects from program comprehension to understanding software process and usage are covered. This paper presents the results of systematic literature review spanning six years of software visualization literature. The main result shows that the most studied topics in the past six years are related to software structure, behavior and evolution. Software process and usage are addressed only in few studies. In the future studying the adoption of software visualization tools in industry context would be beneficial.},
booktitle = {Proceedings of the 20th International Academic Mindtrek Conference},
pages = {262–271},
numpages = {10},
keywords = {systematic literature review, software visualization, human-centered computing},
location = {Tampere, Finland},
series = {AcademicMindtrek '16}
}

@inproceedings{10.1145/1529282.1529701,
author = {Piveta, Eduardo and Pimenta, Marcelo and Ara\'{u}jo, Jo\~{a}o and Moreira, Ana and Guerreiro, Pedro and Price, R. Tom},
title = {Representing refactoring opportunities},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529701},
doi = {10.1145/1529282.1529701},
abstract = {Approaches for the representation of refactoring opportunities and their association with refactorings are usually described in an informal basis. This informality can hamper the creation of catalogues and tools to represent and search for refactoring opportunities. We propose an unified way to represent both the conditions in which the application of a refactoring can be advantageous and the mechanisms to associate these conditions with refactorings. The resulting representation mechanisms can be used to express search criteria based on software metrics, structural problems, heuristics or improvements on the software quality.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {1867–1872},
numpages = {6},
keywords = {representation, refactoring},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@article{10.1016/j.jpdc.2019.03.002,
author = {Zarei Zefreh, Ebrahim and Lotfi, Shahriar and Mohammad Khanli, Leyli and Karimpour, Jaber},
title = {Topology and computational-power aware tile mapping of perfectly nested loops with dependencies on distributed systems},
year = {2019},
issue_date = {Jul 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.03.002},
doi = {10.1016/j.jpdc.2019.03.002},
journal = {J. Parallel Distrib. Comput.},
month = jul,
pages = {14–35},
numpages = {22},
keywords = {Distributed systems, Topology aware tile mapping, Computational-power aware tile mapping, Parallelization, Nested loop}
}

@inproceedings{10.1145/2509136.2509522,
author = {Bhattacharya, Suparna and Gopinath, Kanchi and Nanda, Mangala Gowri},
title = {Combining concern input with program analysis for bloat detection},
year = {2013},
isbn = {9781450323741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509136.2509522},
doi = {10.1145/2509136.2509522},
abstract = {Framework based software tends to get bloated by accumulating optional features (or concerns) just-in-case they are needed. The good news is that such feature bloat need not always cause runtime execution bloat. The bad news is that often enough, only a few statements from an optional concern may cause execution bloat that may result in as much as 50% runtime overhead.We present a novel technique to analyze the connection between optional concerns and the potential sources of execution bloat induced by them. Our analysis automatically answers questions such as (1) whether a given set of optional concerns could lead to execution bloat and (2) which particular statements are the likely sources of bloat when those concerns are not required. The technique combines coarse grain concern input from an external source with a fine-grained static analysis. Our experimental evaluation highlights the effectiveness of such concern augmented program analysis in execution bloat assessment of ten programs.},
booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &amp; Applications},
pages = {745–764},
numpages = {20},
keywords = {software bloat, program concerns, feature oriented programming},
location = {Indianapolis, Indiana, USA},
series = {OOPSLA '13}
}

@inproceedings{10.1145/3136014.3136027,
author = {Bari\v{s}i\'{c}, Ankica and Blouin, Dominique and Amaral, Vasco and Goul\~{a}o, Miguel},
title = {A requirements engineering approach for usability-driven DSL development},
year = {2017},
isbn = {9781450355254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136014.3136027},
doi = {10.1145/3136014.3136027},
abstract = {There is currently a lack of Requirements Engineering (RE) approaches applied to, or supporting, the development of a Domain-Specific Language (DSL) taking into account the environment in which it is to be used. We present a model-based RE approach to support DSL development with a focus on usability concerns. RDAL is a RE fragment language that can be complemented with other languages to support RE and design. USE-ME is a model driven approach for DSLs usability evaluation which is integrable with a DSL development approach. We combine RDAL and a new DSL, named DSSL, that we created for the specification of DSL-based systems. Integrated with this combination we add USE-ME to support usability evaluation. This combination of existing languages and tools provides a comprehensive RE approach for DSL development. We illustrate the approach with the development of the Gyro DSL for programming robots.},
booktitle = {Proceedings of the 10th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {115–128},
numpages = {14},
keywords = {Usability evaluation, Requirements engineering, Domain-Specific language},
location = {Vancouver, BC, Canada},
series = {SLE 2017}
}

@article{10.4018/jkm.2010040103,
author = {Kamthan, Pankaj},
title = {A Viewpoint-Based Approach for Understanding the Morphogenesis of Patterns},
year = {2010},
issue_date = {April 2010},
publisher = {IGI Global},
address = {USA},
volume = {6},
number = {2},
issn = {1548-0666},
url = {https://doi.org/10.4018/jkm.2010040103},
doi = {10.4018/jkm.2010040103},
abstract = {An understanding of knowledge artifacts such as patterns is a necessary prerequisite for any subsequent action. In this article, as an initial step for formulating a theoretical basis for patterns, a conceptual model of primitive viewpoints is proposed and, by exploring one of the viewpoints, a conceptual model for stakeholders of a pattern is presented. This is followed by the description of a conceptual model of a process, namely P3, for the production of patterns. The workflows of P3 highlight, as appropriate, the interface of patterns to humans and/or machines. The implications of the Semantic Web and the Social Web towards P3 are briefly discussed.},
journal = {Int. J. Knowl. Manag.},
month = apr,
pages = {40–65},
numpages = {26},
keywords = {Web, Process, Knowledge Representation, Experiential Knowledge, Conceptual Reuse, Conceptual Model}
}

@article{10.1016/j.infsof.2015.04.002,
title = {How have we evaluated software pattern application? A systematic mapping study of research design practices},
year = {2015},
issue_date = {September 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {65},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.04.002},
doi = {10.1016/j.infsof.2015.04.002},
abstract = {ContextSoftware patterns encapsulate expert knowledge for constructing successful solutions to recurring problems. Although a large collection of software patterns is available in literature, empirical evidence on how well various patterns help in problem solving is limited and inconclusive. The context of these empirical findings is also not well understood, limiting applicability and generalizability of the findings. ObjectiveTo characterize the research design of empirical studies exploring software pattern application involving human participants. MethodWe conducted a systematic mapping study to identify and analyze 30 primary empirical studies on software pattern application, including 24 original studies and 6 replications. We characterize the research design in terms of the questions researchers have explored and the context of empirical research efforts. We also classify the studies in terms of measures used for evaluation, and threats to validity considered during study design and execution. ResultsUse of software patterns in maintenance is the most commonly investigated theme, explored in 16 studies. Object-oriented design patterns are evaluated in 14 studies while 4 studies evaluate architectural patterns. We identified 10 different constructs with 31 associated measures used to evaluate software patterns. Measures for 'efficiency' and 'usability' are commonly used to evaluate the problem solving process. While measures for 'completeness', 'correctness' and 'quality' are commonly used to evaluate the final artifact. Overall, 'time to complete a task' is the most frequently used measure, employed in 15 studies to measure 'efficiency'. For qualitative measures, studies do not report approaches for minimizing biases 27% of the time. Nine studies do not discuss any threats to validity. ConclusionSubtle differences in study design and execution can limit comparison of findings. Establishing baselines for participants' experience level, providing appropriate training, standardizing problem sets, and employing commonly used measures to evaluate performance can support replication and comparison of results across studies.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {14–38},
numpages = {25}
}

@inproceedings{10.5555/1885639.1885649,
author = {Gustavsson, H\r{a}kan and Eklund, Ulrik},
title = {Architecting automotive product lines: industrial practice},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents an in-depth view of how architects work with maintaining product line architectures in the automotive industry. The study has been performed at two internationally well-known companies, one car manufacture and one commercial vehicle manufacture. The results are based on 12 interviews with architects performed at the two companies. The study shows what effect differences such as a strong line organization or a strong project organization has on the architecting process. It also shows what consequence technical choices and business strategy have on the architecting process. Despite the differences the results are surprisingly similar with respect to the process of managing architectural changes as well as the information the architects maintain and update, especially in the light that the companies have had no direct cooperation.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {92–105},
numpages = {14},
keywords = {process, case study, automotive industry, architecting},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.5555/381473.381599,
author = {Bosch, Jan},
title = {Software product lines and software architecture design},
year = {2001},
isbn = {0769510507},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
pages = {717},
location = {Toronto, Ontario, Canada},
series = {ICSE '01}
}

@article{10.1007/s10664-016-9466-0,
author = {Behnamghader, Pooyan and Le, Duc Minh and Garcia, Joshua and Link, Daniel and Shahbazian, Arman and Medvidovic, Nenad},
title = {A large-scale study of architectural evolution in open-source software systems},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9466-0},
doi = {10.1007/s10664-016-9466-0},
abstract = {From its very inception, the study of software architecture has recognized architectural decay as a regularly occurring phenomenon in long-lived systems. Architectural decay is caused by repeated, sometimes careless changes to a system during its lifespan. Despite decay's prevalence, there is a relative dearth of empirical data regarding the nature of architectural changes that may lead to decay, and of developers' understanding of those changes. In this paper, we take a step toward addressing that scarcity by introducing an architecture recovery framework, ARCADE, for conducting large-scale replicable empirical studies of architectural change across different versions of a software system. ARCADE includes two novel architectural change metrics, which are the key to enabling large-scale empirical studies of architectural change. We utilize ARCADE to conduct an empirical study of changes found in software architectures spanning several hundred versions of 23 open-source systems. Our study reveals several new findings regarding the frequency of architectural changes in software systems, the common points of departure in a system's architecture during the system's maintenance and evolution, the difference between system-level and component-level architectural change, and the suitability of a system's implementation-level structure as a proxy for its architecture.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1146–1193},
numpages = {48},
keywords = {Software evolution, Software architecture, Open-source software, Architecture recovery, Architectural change}
}

@inproceedings{10.1145/2642803.2642807,
author = {Torjusen, Arild B. and Abie, Habtamu and Paintsil, Ebenezer and Trcek, Denis and Skomedal, \r{A}smund},
title = {Towards Run-Time Verification of Adaptive Security for IoT in eHealth},
year = {2014},
isbn = {9781450327787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642803.2642807},
doi = {10.1145/2642803.2642807},
abstract = {This paper integrates run-time verification enablers in the feedback adaptation loop of the ASSET adaptive security framework for Internet of Things (IoT) in the eHealth settings and instantiates the resulting framework with Colored Petri Nets. The run-time enablers make machine-readable formal models of a system state and context available at run-time. In addition, they make requirements that define the objectives of verification available at run-time as formal specifications and enable dynamic context monitoring and adaptation. Run-time adaptive behavior that deviates from the normal mode of operation of the system represents a major threat to the sustainability of critical eHealth services. Therefore, the integration of run-time enablers into the ASSET adaptive framework could lead to a sustainable security framework for IoT in eHealth.},
booktitle = {Proceedings of the 2014 European Conference on Software Architecture Workshops},
articleno = {4},
numpages = {8},
keywords = {eHealth, IoT, Formal Run-time Verification, Adaptive Security},
location = {Vienna, Austria},
series = {ECSAW '14}
}

@article{10.1007/s11219-013-9202-6,
author = {Nguyen, Tuong Huan and Vo, Bao Quoc and Lumpe, Markus and Grundy, John},
title = {KBRE: a framework for knowledge-based requirements engineering},
year = {2014},
issue_date = {March     2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9202-6},
doi = {10.1007/s11219-013-9202-6},
abstract = {Detecting inconsistencies is a critical part of requirements engineering (RE) and has been a topic of interest for several decades. Domain knowledge and semantics of requirements not only play important roles in elaborating requirements but are also a crucial way to detect conflicts among them. In this paper, we present a novel knowledge-based RE framework (KBRE) in which domain knowledge and semantics of requirements are central to elaboration, structuring, and management of captured requirements. Moreover, we also show how they facilitate the identification of requirements inconsistencies and other-related problems. In our KBRE model, description logic (DL) is used as the fundamental logical system for requirements analysis and reasoning. In addition, the application of DL in the form of Manchester OWL Syntax brings simplicity to the formalization of requirements while preserving sufficient expressive power. A tool has been developed and applied to an industrial use case to validate our approach.},
journal = {Software Quality Journal},
month = mar,
pages = {87–119},
numpages = {33},
keywords = {Requirements engineering, Ontology, Manchester OWL Syntax, Inconsistencies, Identification, Description logics}
}

@article{10.1007/s10270-012-0308-2,
author = {Farias, Kleinner and Garcia, Alessandro and Lucena, Carlos},
title = {Effects of stability on model composition effort: an exploratory study},
year = {2014},
issue_date = {October   2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-012-0308-2},
doi = {10.1007/s10270-012-0308-2},
abstract = {Model composition plays a central role in many software engineering activities, e.g., evolving design models to add new features. To support these activities, developers usually rely on model composition heuristics. The problem is that the models to-be-composed usually conflict with each other in several ways and such composition heuristics might be unable to properly deal with all emerging conflicts. Hence, the composed model may bear some syntactic and semantic inconsistencies that should be resolved. As a result, the production of the intended model is an error-prone and effort-consuming task. It is often the case that developers end up examining all parts of the output composed model instead of prioritizing the most critical ones, i.e., those that are likely to be inconsistent with the intended model. Unfortunately, little is known about indicators that help developers (1) to identify which model is more likely to exhibit inconsistencies, and (2) to understand which composed models require more effort to be invested. It is often claimed that software systems remaining stable over time tends to have a lower number of defects and require less effort to be fixed than unstable systems. However, little is known about the effects of software stability in the context of model evolution when supported by composition heuristics. This paper, therefore, presents an exploratory study analyzing stability as an indicator of inconsistency rate and resolution effort on model composition activities. Our findings are derived from 180 compositions performed to evolve design models of three software product lines. Our initial results, supported by statistical tests, also indicate which types of changes led to lower inconsistency rate and lower resolution effort.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {1473–1494},
numpages = {22},
keywords = {Software development effort, Model composition, Design stability}
}

@article{10.1016/j.scico.2009.05.003,
author = {Lung, Chung-Horng and Rajeswaran, Pragash and Sivadas, Sathyanarayanan and Sivabalasingam, Theleepan},
title = {Experience of building an architecture-based generator using GenVoca for distributed systems},
year = {2010},
issue_date = {August, 2010},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {75},
number = {8},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2009.05.003},
doi = {10.1016/j.scico.2009.05.003},
abstract = {Selecting the architecture that meets the requirements, both functional and non-functional, is a challenging task, especially at the early stage when more uncertainties exist. Architectural prototyping is a useful approach in supporting the evaluation of alternative architectures and balancing different architectural qualities. Generative programming has gained increasing attention, but it mostly deals with lower-level artifacts; hence, it usually supports lower degrees of software automation. This paper proposes an architecture-centric generative approach in facilitating architectural prototyping and evaluation. We also present our empirical experience in raising the level of abstraction to the architecture layer for distributed and concurrent systems using GenVoca. GenVoca is a generative programming approach that is used here to support the generation or instantiation of a particular architectural pattern in distributed computing based on user's selection. As a result, it can support rapid architectural prototyping and evaluation of both functional and non-functional requirements and encourage greater degrees of software automation and reuse. Lessons learned from the empirical study are also reported and could be applied to other areas.},
journal = {Sci. Comput. Program.},
month = aug,
pages = {672–688},
numpages = {17},
keywords = {Software patterns, Software architecture, Prototyping, Generative programming, GenVoca, Distributed systems}
}

@inproceedings{10.5555/1045658.1045692,
author = {Billig, Andreas and Busse, Susanne and Leicher, Andreas and S\"{u}\ss{}, J\"{o}rn Guy},
title = {Platform independent model transformation based on triple},
year = {2004},
isbn = {3540234284},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Reuse is an important topic in software engineering as it promises advantages like faster time-to-market and cost reduction. Reuse of models on an abstract level is more beneficial than on the code level, because these models can be mapped into several technologies and can be adapted according to different requirements. Unfortunately, development tools only provide fixed mappings between abstract models described in a language such as UML and source code for a particular technology. These mappings are based on one-to-one relationships between elements of both levels. As a consequence, it is rarely possible to customize mappings according to specific user requirements.We aim to improve model reuse by providing a framework that generates customized mappings according to specified requirements. The framework is able to handle mappings aimed for several component technologies as it is based on an ADL. It is realized in Triple to represent components on different levels of abstraction and to perform the actual transformation. It uses feature models to describe mapping alternatives.},
booktitle = {Proceedings of the 5th ACM/IFIP/USENIX International Conference on Middleware},
pages = {493–511},
numpages = {19},
location = {Toronto, Canada},
series = {Middleware '04}
}

@article{10.1145/1668862.1668863,
author = {Alvaro, Alexandre and Santana de Almeida, Eduardo and Romero de Lemos Meira, Silvio},
title = {A software component quality framework},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/1668862.1668863},
doi = {10.1145/1668862.1668863},
abstract = {One of the major problems with Component-Based Software Engineering (CBSE) is the quality of the components used in a system. The reliability of a component-based software system depends on the reliability of the components that is made of. In CBSE, the proper search, selection and evaluation process of components is considered the cornerstone for the development of any effective component-based system. So far the software industry was concentrated on the functional aspects of components, leaving aside the difficult task of assessing their quality. In this way, we propose a software component quality framework to evaluate the quality of software components in an efficient way. Moreover, an experimental study was accomplished in order to evaluate the viability of the proposed framework.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–18},
numpages = {18}
}

@article{10.1016/j.future.2015.03.006,
author = {Garc\'{\i}a-Gal\'{a}n, Jes\'{u}s and Trinidad, Pablo and Rana, Omer F. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated configuration support for infrastructure migration to the cloud},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.03.006},
doi = {10.1016/j.future.2015.03.006},
abstract = {With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configuration-in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy. We support the decision making in migration planning to the cloud.We use Feature Models to describe the configuration space of an IaaS.We automate the search of the most suitable IaaS configuration.Our approach improves the results of commercial applications on Amazon EC2.},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {200–212},
numpages = {13},
keywords = {IaaS, Feature model, EC2, Cloud migration, Automated analysis}
}

@inproceedings{10.1145/2108616.2108654,
author = {La, Hyun Jung and Oh, Sang Hun and Kim, Soo Dong},
title = {Methods to utilizing cloud computing in developing mobile internet device (MID) applications},
year = {2010},
isbn = {9781605588933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2108616.2108654},
doi = {10.1145/2108616.2108654},
abstract = {Cloud Computing (CC) is emerged as an effective reuse paradigm, where software functionality, hardware computing power, and other computing resources are delivered in the form of service. Mobile Internet Device (MID), as a portable handheld device, becomes a strong candidate for client-side computing. MID has a limited resource and computing power, hence, it is not deploy the applications which require complex computation and large amount of resources. The MID feature of limited resource forbids deploying and running complex software application on MID, and a key concept of CC is to deploy all the computing resources are placed on the provider side. Hence, by applying CC concepts to the MID environment, disadvantages derived from limited resource can be overcome. Therefore, we first identify key features of MID applications. To overcome a drawback of MID, limited resource, we present our justification of applying CC to MID environment and key methods which are raised in applying CC to MID environment.},
booktitle = {Proceedings of the 4th International Conference on Uniquitous Information Management and Communication},
articleno = {31},
numpages = {9},
keywords = {mobile internet device, cloud computing},
location = {Suwon, Republic of Korea},
series = {ICUIMC '10}
}

@inproceedings{10.1145/2480362.2480567,
author = {Durelli, Rafael S. and Santib\'{a}\~{n}ez, Daniel S. M. and Anquetil, Nicolas and Delamaro, M\'{a}rcio E. and de Camargo, Valter Vieira},
title = {A systematic review on mining techniques for crosscutting concerns},
year = {2013},
isbn = {9781450316569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480362.2480567},
doi = {10.1145/2480362.2480567},
abstract = {&lt;u&gt;Background:&lt;/u&gt; The several maintenance tasks a system is submitted during its life usually cause its architecture deviates from the original conceivable design, ending up with scattered and tangled concerns across the software. The research area named concern mining attempts to identify such scattered and tangled concerns to support maintenance and reverse-engineering. &lt;u&gt;Objectives:&lt;/u&gt; The aim of this paper is threefold: (i) identifying techniques employed in this research area, (ii) extending a taxonomy available on the literature and (iii) recommending an initial combination of some techniques. &lt;u&gt;Results:&lt;/u&gt; We selected 62 papers by their mining technique. Among these papers, we identified 18 mining techniques for crosscutting concern. Based on these techniques, we have extended a taxonomy available in the literature, which can be used to position each new technique, and to compare it with the existing ones along relevant dimensions. As consequence, we present some combinations of these techniques taking into account high values of precision and recall that could improve the identification of both Persistence and Observer concerns. The combination that we recommend may serve as a roadmap to potential users of mining techniques for crosscutting concerns.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
pages = {1080–1087},
numpages = {8},
keywords = {systematic review, cross-cutting concerns, concern mining, aspect mining},
location = {Coimbra, Portugal},
series = {SAC '13}
}

@inproceedings{10.5555/2028067.2028076,
author = {Snow, Kevin Z. and Krishnan, Srinivas and Monrose, Fabian and Provos, Niels},
title = {SHELLOS: enabling fast detection and forensic analysis of code injection attacks},
year = {2011},
publisher = {USENIX Association},
address = {USA},
abstract = {The availability of off-the-shelf exploitation toolkits for compromising hosts, coupled with the rapid rate of exploit discovery and disclosure, has made exploit or vulnerability-based detection far less effective than it once was. For instance, the increasing use of metamorphic and polymorphic techniques to deploy code injection attacks continues to confound signature-based detection techniques. The key to detecting these attacks lies in the ability to discover the presence of the injected code (or, shellcode). One promising technique for doing so is to examine data (be that from network streams or buffers of a process) and efficiently execute its content to find what lurks within. Unfortunately, current approaches for achieving this goal are not robust to evasion or scalable, primarily because of their reliance on software-based CPU emulators. In this paper, we argue that the use of software-based emulation techniques are not necessary, and instead propose a new framework that leverages hardware virtualization to better enable the detection of code injection attacks. We also report on our experience using this framework to analyze a corpus of malicious Portable Document Format (PDF) files and network-based attacks.},
booktitle = {Proceedings of the 20th USENIX Conference on Security},
pages = {9},
numpages = {1},
location = {San Francisco, CA},
series = {SEC'11}
}

@article{10.1016/j.cl.2018.01.002,
author = {Braz, Larissa and Gheyi, Rohit and Mongiovi, Melina and Ribeiro, M\'{a}rcio and Medeiros, Fl\'{a}vio and Teixeira, Leopoldo and Souto, Sabrina},
title = {A change-aware per-file analysis to compile configurable systems with #ifdefs      },
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.002},
doi = {10.1016/j.cl.2018.01.002},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {427–450},
numpages = {24},
keywords = {Impact analysis, Configurable systems, #ifdef, Compilation}
}

@article{10.1016/j.jss.2006.05.024,
author = {Hofmeister, Christine and Kruchten, Philippe and Nord, Robert L. and Obbink, Henk and Ran, Alexander and America, Pierre},
title = {A general model of software architecture design derived from five industrial approaches},
year = {2007},
issue_date = {January, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {80},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2006.05.024},
doi = {10.1016/j.jss.2006.05.024},
abstract = {We compare five industrial software architecture design methods and we extract from their commonalities a general software architecture design approach. Using this general approach, we compare across the five methods the artifacts and activities they use or recommend, and we pinpoint similarities and differences. Once we get beyond the great variance in terminology and description, we find that the five approaches have a lot in common and match more or less the ''ideal'' pattern we introduced. From the ideal pattern we derive an evaluation grid that can be used for further method comparisons.},
journal = {J. Syst. Softw.},
month = jan,
pages = {106–126},
numpages = {21},
keywords = {Software architecture design, Software architecture analysis, Software architecture, Architectural method}
}

@inproceedings{10.1007/978-3-642-33666-9_6,
author = {Alf\'{e}rez, Germ\'{a}n H. and Pelechano, Vicente},
title = {Dynamic evolution of context-aware systems with models at runtime},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_6},
doi = {10.1007/978-3-642-33666-9_6},
abstract = {Model-driven techniques have proven to yield significant benefits for context-aware systems. Specifically, semantically-rich models are used at runtime to monitor the system context and guide necessary changes. Under the closed-world assumption, adaptations are fully known at design time. Nevertheless, it is difficult to foresee all the possible situations that may arise in uncertain and complex contexts. In this paper, we present a model-based framework to support the dynamic evolution of context-aware systems to deal with unexpected context events in the open world. If model adaptations are not enough to solve uncertainty, our model-based evolution planner guides the evolution of the supporting models to preserve high-level requirements. A case study about a context-aware Web service composition, which is executed in a distributed computing infrastructure, illustrates the applicability of our framework. A realization methodology and a prototype system support our approach.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {70–86},
numpages = {17},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@article{10.1016/j.infsof.2017.03.011,
author = {Martnez-Fernndez, Silverio and Ayala, Claudia P. and Franch, Xavier and Marques, Helena Martins},
title = {Benefits and drawbacks of software reference architectures},
year = {2017},
issue_date = {August 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {88},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.03.011},
doi = {10.1016/j.infsof.2017.03.011},
abstract = {ContextSoftware Reference Architectures (SRAs) play a fundamental role for organizations whose business greatly depends on the efficient development and maintenance of complex software applications. However, little is known about the real value and risks associated with SRAs in industrial practice. ObjectiveTo investigate the current industrial practice of SRAs in a single company from the perspective of different stakeholders. MethodAn exploratory case study that investigates the benefits and drawbacks perceived by relevant stakeholders in nine SRAs designed by a multinational software consulting company. ResultsThe study shows the perceptions of different stakeholders regarding the benefits and drawbacks of SRAs (e.g., both SRA designers and users agree that they benefit from reduced development costs; on the contrary, only application builders strongly highlighted the extra learning curve as a drawback associated with mastering SRAs). Furthermore, some of the SRA benefits and drawbacks commonly highlighted in the literature were remarkably not mentioned as a benefit of SRAs (e.g., the use of best practices). Likewise, other aspects arose that are not usually discussed in the literature, such as higher time-to-market for applications when their dependencies on the SRA are managed inappropriately. ConclusionsThis study aims to help practitioners and researchers to better understand real SRAs projects and the contexts where these benefits and drawbacks appeared, as well as some SRA improvement strategies. This would contribute to strengthening the evidence regarding SRAs and support practitioners in making better informed decisions about the expected SRA benefits and drawbacks. Furthermore, we make available the instruments used in this study and the anonymized data gathered to motivate others to provide similar evidence to help mature SRA research and practice.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {37–52},
numpages = {16},
keywords = {Software architecture, Reference architecture, Empirical software engineering, Drawbacks, Case study, Benefits}
}

@article{10.1016/j.infsof.2015.07.005,
author = {Martini, Antonio and Bosch, Jan and Chaudron, Michel},
title = {Investigating Architectural Technical Debt accumulation and refactoring over time},
year = {2015},
issue_date = {November 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {67},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.07.005},
doi = {10.1016/j.infsof.2015.07.005},
abstract = {Display Omitted We provide a taxonomy of the causes for Architectural Technical Debt accumulation.Crisis model: shows the increasing accumulation of Architectural Technical Debt.Phases model: shows when Architectural Technical Debt is accumulated and refactored.Refactoring strategies: best and worst case scenarios with respect to crises.We conduct an empirical evaluation of the factors and models. ContextA known problem in large software companies is to balance the prioritization of short-term with long-term feature delivery speed. Specifically, Architecture Technical Debt is regarded as sub-optimal architectural solutions taken to deliver fast that might hinder future feature development, which, in turn, would hinder agility. ObjectiveThis paper aims at improving software management by shedding light on the current factors responsible for the accumulation of Architectural Technical Debt and to understand how it evolves over time. MethodWe conducted an exploratory multiple-case embedded case study in 7 sites at 5 large companies. We evaluated the results with additional cross-company interviews and an in-depth, company-specific case study in which we initially evaluate factors and models. ResultsWe compiled a taxonomy of the factors and their influence in the accumulation of Architectural Technical Debt, and we provide two qualitative models of how the debt is accumulated and refactored over time in the studied companies. We also list a set of exploratory propositions on possible refactoring strategies that can be useful as insights for practitioners and as hypotheses for further research. ConclusionSeveral factors cause constant and unavoidable accumulation of Architecture Technical Debt, which leads to development crises. Refactorings are often overlooked in prioritization and they are often triggered by development crises, in a reactive fashion. Some of the factors are manageable, while others are external to the companies. ATD needs to be made visible, in order to postpone the crises according to the strategic goals of the companies. There is a need for practices and automated tools to proactively manage ATD.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {237–253},
numpages = {17},
keywords = {Software management, Software life-cycle, Software architecture, Qualitative model, Architectural Technical Debt, Agile software development}
}

@inproceedings{10.1145/2973839.2973848,
author = {Cedrim, Diego and Sousa, Leonardo and Garcia, Alessandro and Gheyi, Rohit},
title = {Does refactoring improve software structural quality? A longitudinal study of 25 projects},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973848},
doi = {10.1145/2973839.2973848},
abstract = {Code smells in a program represent indications of structural quality problems, which can be addressed by software refactoring. Refactoring is widely practiced by developers, and considerable development effort has been invested in refactoring tooling support. There is an explicit assumption that software refactoring improves the structural quality of a program by reducing its density of code smells. However, little has been reported about whether and to what extent developers successfully remove code smells through refactoring. This paper reports a first longitudinal study intended to address this gap. We analyze how often the commonly-used refactoring types affect the density of 5 types of code smells along the version histories of 25 projects. Our findings are based on the analysis of 2,635 refactorings distributed in 11 different types. Surprisingly, 2,506 refactorings (95.1%) did not reduce or introduce code smells. Thus, these findings suggest that refactorings lead to smell reduction less often than what has been reported. According to our data, only 2.24% of refactoring changes removed code smells and 2.66% introduced new ones. Moreover, several smells induced by refactoring tended to live long, i.e., 146 days on average. These smells were only eventually removed when smelly elements started to exhibit poor structural quality and, as a consequence, started to be more costly to get rid of.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {73–82},
numpages = {10},
keywords = {Structural Quality, Refactoring, Code Smells},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@article{10.1007/s10270-011-0219-7,
author = {Mohagheghi, Parastoo and Gilani, Wasif and Stefanescu, Alin and Fernandez, Miguel A. and Nordmoen, Bj\o{}rn and Fritzsche, Mathias},
title = {Where does model-driven engineering help? Experiences from three industrial cases},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-011-0219-7},
doi = {10.1007/s10270-011-0219-7},
abstract = {There have been few experience reports from industry on how Model-Driven Engineering (MDE) is applied and what the benefits are. This paper summarizes the experiences of three large industrial participants in a European research project with the objective of developing techniques and tools for applying MDE on the development of large and complex software systems. The participants had varying degrees of previous experience with MDE. They found MDE to be particularly useful for providing abstractions of complex systems at multiple levels or from different viewpoints, for the development of domain-specific models that facilitate communication with non-technical experts, for the purposes of simulation and testing, and for the consumption of models for analysis, such as performance-related decision support and system design improvements. From the industrial perspective, a methodology is considered to be useful and cost-efficient if it is possible to reuse solutions in multiple projects or products. However, developing reusable solutions required extra effort and sometimes had a negative impact on the performance of tools. While the companies identified several benefits of MDE, merging different tools with one another in a seamless development environment required several transformations, which increased the required implementation effort and complexity. Additionally, user-friendliness of tools and the provision of features for managing models of complex systems were identified as crucial for a wider industrial adoption of MDE.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {619–639},
numpages = {21},
keywords = {Simulation, Model-driven engineering, Experience report, Eclipse, Domain-specific language, Complex systems}
}

@inproceedings{10.5555/1789757.1789774,
author = {Li, Juan and Hou, Lishan and Qin, Zhongsen and Wang, Qing and Chen, Guisheng},
title = {An empirically-based process to improve the practice of requirement review},
year = {2008},
isbn = {3540795871},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Requirement quality serves as the basis of the whole software development.How to improve and assure the quality of requirements is one of themost difficult issues. Aiming to improve requirement review in a softwarecompany, we propose a role-based requirement review process based on theidea that requirement quality should meet needs of all roles involved to thelargest extent, and not only determined by the number of defects found in requirements.This process helps reviewers focus on their own concerns and findmore defects related to their tasks, and provides a quantitative method to analyzeand evaluate the quality of the requirement document. We also provide acase study to illustrate the new process and report some preliminary results.},
booktitle = {Proceedings of the Software Process, 2008 International Conference on Making Globally Distributed Software Development a Success Story},
pages = {135–146},
numpages = {12},
keywords = {role-based, requirement review, requirement quality characteristic, quantitative},
location = {Leipzig, Germany},
series = {ICSP'08}
}

@article{10.1016/j.infsof.2007.12.002,
author = {Koning, Michiel and Sun, Chang-ai and Sinnema, Marco and Avgeriou, Paris},
title = {VxBPEL: Supporting variability for Web services in BPEL},
year = {2009},
issue_date = {February, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.12.002},
doi = {10.1016/j.infsof.2007.12.002},
abstract = {Web services provide a way to facilitate the business integration over the Internet. Flexibility is an important and desirable property of Web service-based systems due to dynamic business environments. The flexibility can be provided or addressed by incorporating variability into a system. In this study, we investigate how variability can be incorporated into service-based systems. We propose a language, VxBPEL, which is an adaptation of an existing language, BPEL, and able to capture variability in these systems. We develop a prototype to interpret this language. Finally, we illustrate our method by using it to handle variability of an example.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {258–269},
numpages = {12},
keywords = {Web service, Variability, Service-based system, Business Process Execution Language}
}

@inproceedings{10.1109/MODELS-C.2019.00090,
author = {Mendoza, Camilo and Garc\'{e}s, Kelly and Casallas, Rubby and Bocanegra, Jos\'{e}},
title = {Detecting architectural issues during the continuous integration pipeline},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00090},
doi = {10.1109/MODELS-C.2019.00090},
abstract = {The use of a software reference architecture limits possible deviations and errors in the implementation of software projects, as the code must follow predefined rules that developers must respect to guarantee quality. However, when introducing new code to projects these rules can be violated. As a result, architectural erosion, bad smells, or even bugs that can be difficult to find are introduced to the projects. This paper proposes an approach for reviewing compliance to predefined rules that map architectural decisions to code. During the continuous integration process, the automatic analysis raises an issue for each rule violation. Developers can analyze and correct issues, and trace/visualize improvements, or lack thereof, through time. We present a validation experiment carried out in the context of a Software Development course, and we show how the approach helps developers to write better code1.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {589–597},
numpages = {9},
keywords = {architectural rules, continuous integration, issue identification, issue visualization, rule violation},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@article{10.1145/979743.979745,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Back matter (abstracts and calendar)},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/979743.979745},
doi = {10.1145/979743.979745},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {27–62},
numpages = {36}
}

@inproceedings{10.1109/SHARK.2009.5069114,
author = {Unphon, Hataichanok},
title = {Making use of architecture throughout the software life cycle - How the build hierarchy can facilitate product line development},
year = {2009},
isbn = {9781424437269},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SHARK.2009.5069114},
doi = {10.1109/SHARK.2009.5069114},
abstract = {This paper presents an empirical study of how the application of genuine architecture can be employed beyond the design phase of product line development. The study is based on a co-operative research project with a company developing product line architecture for hydraulic modelling software. By concretising the architecture as a build hierarchy the architecture mediates the evolution of the design throughout the whole software life cycle. The empirical evidence has confirmed the improvements of (1) the software quality and flexibility, (2) the communication and cooperation with new developers, (3) the distribution of work and parallel implementation, and (4) the foreseen usage by hydraulic and environmental consultants who tailor the software. Our research further indicates requirements for the architectural analysis tools that are deliberately embedded in the daily development practices.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Sharing and Reusing Architectural Knowledge},
pages = {41–48},
numpages = {8},
series = {SHARK '09}
}

@article{10.1016/j.infsof.2011.06.001,
author = {La, Hyun Jung and Kim, Soo Dong},
title = {Static and dynamic adaptations for service-based systems},
year = {2011},
issue_date = {December, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {12},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.06.001},
doi = {10.1016/j.infsof.2011.06.001},
abstract = {Context: In service-oriented computing (SOC), service providers publish reusable services, and service consumers subscribe them. However, there exist potential problems in reusing services. Mismatch is a problem that occurs when a candidate service does not fully match to the feature expected. Fault is a problem that occurs when an invocation of services results in some abnormality at runtime. Without remedying mismatch problems, services would not be reusable. Without remedying fault problems, service invocations at runtime would result in failures. Static and dynamic adaptations are practical approaches to remedying the problems. Objective: Our objective is to define a comprehensive framework which includes a design of service adaptation framework (SAF), and design of static and dynamic adapters. Method: We design the SAF which governs dynamic adaptations, and define a service life-cycle with adaptation-related activities. Based on causal-effect relationships among mismatch, fault, cause, and adapter, we derive mismatches and faults, from which their relevant causes are identified. For the causes, we define six static adapters and five dynamic adapters. We specify instructions for designing static adapters, and provide step-wise algorithms for designing dynamic adapters based on enterprise service bus (ESB). And, we show a proof-of-concept (POC) of implementation to show applicability of the methods. Results: The paper presents service life-cycle with adaptation-related activities, SAF design, and design of static and dynamic adapters. Conclusion: Mismatch and fault problems in utilizing services present threats to high reusability of services. Static adaptations can remedy mismatch problems, and dynamic adaptations can remedy fault problems. In this paper, we presented technical insights of service adaption, SAF design, and definitions of static and dynamic adapters. By utilizing the proposed SAF and service adapters, reusability of services can be greatly enhanced.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {1275–1296},
numpages = {22},
keywords = {Static adaptation, Service adaptation, Mismatch problem, Fault, Dynamic adaptation}
}

@article{10.1007/s10270-013-0358-0,
author = {Ab. Rahim, Lukman and Whittle, Jon},
title = {A survey of approaches for verifying model transformations},
year = {2015},
issue_date = {May       2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-013-0358-0},
doi = {10.1007/s10270-013-0358-0},
abstract = {As with other software development artifacts, model transformations are not bug-free and so must be systematically verified. Their nature, however, means that transformations require specialist verification techniques. This paper brings together current research on model transformation verification by classifying existing approaches along two dimensions. Firstly, we present a coarse-grained classification based on the technical details of the approach (e.g., testing, theorem proving, model checking). Secondly, we present a finer-grained classification which categorizes approaches according to criteria such as level of formality, transformation language, properties verified. The purpose of the survey is to bring together research in model transformation verification to act as a resource for the community. Furthermore, based on the survey, we identify a number of trends in current and past research on model transformation verification.},
journal = {Softw. Syst. Model.},
month = may,
pages = {1003–1028},
numpages = {26},
keywords = {Verification, Survey, Model transformations}
}

@article{10.1016/j.jss.2016.03.038,
author = {Deb, Novarun and Chaki, Nabendu and Ghose, Aditya},
title = {Extracting finite state models from i* models},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.03.038},
doi = {10.1016/j.jss.2016.03.038},
abstract = {The Naive Algorithm (NA) extracts all possible finite state models from i* models.We observe an explosion in the finite state model space.The Semantic Implosion Algorithm (SIA) is a solution to this explosion problem.NA and SIA are extensively simulated on different categories of i* models.SIA drastically reduces the model space growth from O(1020) (for NA) to O(103). i* models are inherently sequence agnostic. This makes the process of cross-checking i* models against temporal properties quite impossible. There is an immediate industrial need to bridge the gap between such a sequence agnostic model and a standardized model verifier so that model checking can be performed in the requirement analysis phase itself. In this paper, we first spell out the Naive Algorithm that generates all possible finite state models corresponding to a given i* model. The growth of the finite state model space can be mapped to the problem of finding the number of possible paths between the Least Upper Bound (LUB) and the Greatest Lower Bound (GLB) of a k-dimensional hypercube lattice structure. The mathematics for doing a quantitative analysis of the space growth has also been presented. The Naive Algorithm has its main drawback in the hyperexponential growth of the model space. The Semantic Implosion Algorithm is proposed as a solution to the hyperexponential problem. This algorithm exploits the temporal information embedded within the i* model of an enterprise to reduce the rate of growth of the finite state model space. A comparative quantitative analysis between the two approaches concludes the superiority of the Semantic Implosion Algorithm.},
journal = {J. Syst. Softw.},
month = nov,
pages = {265–280},
numpages = {16},
keywords = {i* model, Model transformation, Model checking}
}

@article{10.1016/j.datak.2014.07.003,
author = {Bre\ss{}, Sebastian and Siegmund, Norbert and Heimel, Max and Saecker, Michael and Lauer, Tobias and Bellatreche, Ladjel and Saake, Gunter},
title = {Load-aware inter-co-processor parallelism in database query processing},
year = {2014},
issue_date = {September 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2014.07.003},
doi = {10.1016/j.datak.2014.07.003},
abstract = {For a decade, the database community has been exploring graphics processing units and other co-processors to accelerate query processing. While the developed algorithms often outperform their CPU counterparts, it is not beneficial to keep processing devices idle while overutilizing others. Therefore, an approach is needed that efficiently distributes a workload on available (co-)processors while providing accurate performance estimates for the query optimizer. In this paper, we contribute heuristics that optimize query processing for response time and throughput simultaneously via inter-device parallelism. Our empirical evaluation reveals that the new approach achieves speedups up to 1.85 compared to state-of-the-art approaches while preserving accurate performance estimations. In a further series of experiments, we evaluate our approach on two new use cases: joining and sorting. Furthermore, we use a simulation to assess the performance of our approach for systems with multiple co-processors and derive some general rules that impact performance in those systems. Contribute heuristics to enhance performance by exploiting inter-device parallelismHeuristics consider load and speed on (co-)processors.Extensive evaluation on four use cases: aggregation, selection, sort, and joinAssess the performance of best heuristic for systems with multiple co-processorsDiscuss how operator-stream-based scheduling can be used in a query processor},
journal = {Data Knowl. Eng.},
month = sep,
pages = {60–79},
numpages = {20},
keywords = {Query processing, Query optimization, Co-processing}
}

@article{10.1007/s13748-020-00205-3,
author = {Ram\'{\i}rez, Aurora and Delgado-P\'{e}rez, Pedro and Ferrer, Javier and Romero, Jos\'{e} Ra\'{u}l and Medina-Bulo, Inmaculada and Chicano, Francisco},
title = {A systematic literature review of the SBSE research community in Spain},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {2},
url = {https://doi.org/10.1007/s13748-020-00205-3},
doi = {10.1007/s13748-020-00205-3},
abstract = {Since its appearance in 2001, search-based software engineering has allowed software engineers to use optimisation techniques to automate distinctive human problems related to software management and development. The scientific community in Spain has not been alien to these advances. Their contributions cover both the optimisation of software engineering tasks and the proposal of new search algorithms. This review compiles the research efforts of this community in the area. With this aim, we propose a protocol to describe the review process, including the search sources, inclusion and exclusion criteria of candidate papers, the data extraction procedure and the categorisation of primary studies. After retrieving more than 3700 papers, 232 primary studies have been selected, whose analysis gives a precise picture of the current research state of the community, trends and future challenges. With 145 authors from 19 distinct institutions, results show that a diversity of tasks, including software planning, requirements, design and testing, and a large variety of techniques has been used, from exact search to evolutionary computation and swarm intelligence. Further, since 2015, specific scientific events have helped to bring together the community, improving collaborations, financial funding and internationalisation.},
journal = {Prog. in Artif. Intell.},
month = jun,
pages = {113–128},
numpages = {16},
keywords = {Spanish community, Research trends, Systematic review, Search-based software engineering}
}

@inproceedings{10.1145/1083063.1083075,
author = {Lapouchnian, Alexei and Liaskos, Sotirios and Mylopoulos, John and Yu, Yijun},
title = {Towards requirements-driven autonomic systems design},
year = {2005},
isbn = {1595930396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083063.1083075},
doi = {10.1145/1083063.1083075},
abstract = {Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying and analyzing alternative ways of how the main objectives of the system can be achieved and designing a system that supports all of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and sketches a possible architecture for autonomic systems that can be built using the this approach.},
booktitle = {Proceedings of the 2005 Workshop on Design and Evolution of Autonomic Application Software},
pages = {1–7},
numpages = {7},
keywords = {software variability, self-management, goal-oriented requirements engineering, autonomic computing software customization},
location = {St. Louis, Missouri},
series = {DEAS '05}
}

@inproceedings{10.1007/11754305_34,
author = {Kiebusch, Sebastian and Franczyk, Bogdan and Speck, Andreas},
title = {Process-Family-Points},
year = {2006},
isbn = {3540341994},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11754305_34},
doi = {10.1007/11754305_34},
abstract = {Software system families are characterized through a structured reuse of components and a high degree of automation based on a common infrastructure. It is possible to increase the efficiency of software system families by an explicit consideration of process flows in application domains which are driven by processes. Based on that fact this article briefly describes the approach of process family engineering. Afterwards the metrics of Process-Family-Points are explained in detail. These are the only framework to measure the size and estimate the effort of process families. Subsequently this paper shows the first results from a validation of the Process-Family-Points in the application domains of eBusiness and Automotive. After an evaluation of these empirical data this paper concludes with an outlook on future activities.},
booktitle = {Proceedings of the 2006 International Conference on Software Process Simulation and Modeling},
pages = {314–321},
numpages = {8},
location = {Shanghai, China},
series = {SPW/ProSim'06}
}

@inproceedings{10.1145/949344.949346,
author = {Thomas, Dave and Barry, Brian M.},
title = {Model driven development: the case for domain oriented programming},
year = {2003},
isbn = {1581137516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/949344.949346},
doi = {10.1145/949344.949346},
abstract = {In this paper, we offer an alternative vision for domain driven development (3D). Our approach is model driven and emphasizes the use of generic and specific domain oriented programming (DOP) languages. DOP uses strong specific languages, which directly incorporate domain abstractions, to allow knowledgeable end users to succinctly express their needs in the form of an application computation. Most domain driven development (3D) approaches and techniques are targeted at professional software engineers and computer scientists. We argue that DOP offers a promising alternative. Specifically we are focused on empowering application developers who have extensive domain knowledge as well as sound foundations in their professions, but may not be formally trained in computer science.We provide a brief survey of DOP experiences, which show that many of the best practices such as patterns, refactoring, and pair programming are naturally and ideally practiced in a Model Driven Development (MDD) setting. We compare and contrast our DOP with other popular approaches, most of which are deeply rooted in the OO community.Finally we highlight challenges and opportunities in the design and implementation of such languages.},
booktitle = {Companion of the 18th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {2–7},
numpages = {6},
keywords = {programming by professional end users, model driven development, end user programming, domain specific languages, domain driven development},
location = {Anaheim, CA, USA},
series = {OOPSLA '03}
}

@inproceedings{10.1145/1449814.1449874,
author = {Danovaro, Emanuele and Janes, Andrea and Succi, Giancarlo},
title = {Jidoka in software development},
year = {2008},
isbn = {9781605582207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1449814.1449874},
doi = {10.1145/1449814.1449874},
abstract = {Lean management is based on two concepts: the elimination of Muda, the waste, from the production process, and Jidoka, the introduction of quality inside the production process and product. In software production, the elimination of Muda received significant attention, while Jidoka has not yet been fully exploited. In this work we want to propose a holistic approach to insert Jidoka in software production. We depict the architecture of a tool to support Jidoka and describe the components that are part of it.},
booktitle = {Companion to the 23rd ACM SIGPLAN Conference on Object-Oriented Programming Systems Languages and Applications},
pages = {827–830},
numpages = {4},
keywords = {quality assurance, Jidoka},
location = {Nashville, TN, USA},
series = {OOPSLA Companion '08}
}

@inproceedings{10.1007/11527800_25,
author = {Czarnecki, Krzysztof},
title = {Overview of generative software development},
year = {2004},
isbn = {3540278842},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11527800_25},
doi = {10.1007/11527800_25},
abstract = {System family engineering seeks to exploit the commonalities among systems from a given problem domain while managing the variabilities among them in a systematic way. In system family engineering, new system variants can be rapidly created based on a set of reusable assets (such as a common architecture, components, models, etc.). Generative software development aims at modeling and implementing system families in such a way that a given system can be automatically generated from a specification written in one or more textual or graphical domain-specific languages. This paper gives an overview of the basic concepts and ideas of generative software development including DSLs, domain and application engineering, generative domain models, networks of domains, and technology projections. The paper also discusses the relationship of generative software development to other emerging areas such as Model Driven Development and Aspect-Oriented Software Development.},
booktitle = {Proceedings of the 2004 International Conference on Unconventional Programming Paradigms},
pages = {326–341},
numpages = {16},
location = {Le Mont Saint Michel, France},
series = {UPP'04}
}

@inproceedings{10.5555/3290281.3290302,
author = {Kalra, Sumit and T, Prabhakar},
title = {Implementation patterns for multi-tenancy},
year = {2017},
isbn = {9781941652060},
publisher = {The Hillside Group},
address = {USA},
abstract = {Recently multi-tenant applications for SaaS in cloud computing are on rise. These applications increase the degree of resource sharing among tenants with various functional and non-functional requirements. However, often it results in higher design complexity. In this work, we discuss various design patterns to build these applications with efficient tenant management. We divided these patterns in the three categories. The categorization is based on their applicability to design, development, and runtime phases during software development life cycle. These patterns make an application tenant aware and enable multi-tenancy without adding much overhead and complexity in its design.},
booktitle = {Proceedings of the 24th Conference on Pattern Languages of Programs},
articleno = {17},
numpages = {16},
keywords = {tenant operation and management, multi-tenant},
location = {Vancouver, British Columbia, Canada},
series = {PLoP '17}
}

@inproceedings{10.1145/2602928.2603080,
author = {Lytra, Ioanna and Sobernig, Stefan and Tran, Huy and Zdun, Uwe},
title = {A pattern language for service-based platform integration and adaptation},
year = {2012},
isbn = {9781450329439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602928.2603080},
doi = {10.1145/2602928.2603080},
abstract = {Often software systems accommodate one or more software platforms on top of which various applications are developed and executed. Different application areas, such as enterprise resource planning, mobile devices, telecommunications, and so on, require different and specialized platforms. Many of them offer their services using standardized interface technologies to support integration with the applications built on top of them and with other platforms. The diversity of platform technologies and interfaces, however, renders the integration of multiple platforms challenging. In this paper, we discuss design alternatives for tailoring heterogeneous service platforms by studying high-level and low-level architectural design decisions for integrating and for adapting platforms. We survey and organize existing patterns and design decisions in the literature as a pattern language. With this pattern language, we address the various decision categories and interconnections for the service-based integration and the adaptation of applications developed based on software platforms. We apply this pattern language in an industry case study.},
booktitle = {Proceedings of the 17th European Conference on Pattern Languages of Programs},
articleno = {4},
numpages = {27},
keywords = {service-based platform integration, pattern language, design patterns},
location = {Irsee, Germany},
series = {EuroPLoP '12}
}

@inproceedings{10.1007/978-3-642-35623-0_2,
author = {Toffetti, Giovanni},
title = {Web engineering for cloud computing (web engineering forecast: cloudy with a chance of opportunities)},
year = {2012},
isbn = {9783642356223},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-35623-0_2},
doi = {10.1007/978-3-642-35623-0_2},
abstract = {Web Engineering has always been concerned with modelling the functional aspects of Web applications. Non-functional (e.g., performance, availability) properties of Web applications have traditionally been a minor concern in the Web engineering community and have been seen as technology- or system-related issues. The advent of Cloud computing, with substantial delegation of "system concerns" to infrastructure or platform providers, seems at a first sight to confirm the validity of this choice. But is this really true?We will argue that, in order to be able to actually profit from the Cloud computing paradigm, Web Engineering methodologies need several interventions transcending the platform-specific concerns of adapting to Cloud technologies.In this position paper, we call for a long-due revamp of Web engineering methodologies to become more sound engineering practices with respect to both functional and non-functional aspects of Web applications. To this end, we propose a methodological framework that preserves the advantages of model-driven development, but also takes into account performance and cost considerations for Cloud-based applications.},
booktitle = {Proceedings of the 12th International Conference on Current Trends in Web Engineering},
pages = {5–19},
numpages = {15},
location = {Berlin, Germany},
series = {ICWE'12}
}

@inproceedings{10.1145/2513534.2513541,
author = {Guill\'{e}n, Joaqu\'{\i}n and Miranda, Javier and Murillo, Juan Manuel and Canal, Carlos},
title = {Developing migratable multicloud applications based on MDE and adaptation techniques},
year = {2013},
isbn = {9781450323079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513534.2513541},
doi = {10.1145/2513534.2513541},
abstract = {Developing software for the cloud usually implies using the tools and libraries supplied by cloud vendors for each of their platforms. This strongly couples the software to specific platforms and penalizes its migration or interoperability with external cloud services, in what is known as vendor lock-in. Under these circumstances multicloud applications become difficult to build and maintain since they require multidisciplinary teams with expertise on multiple platforms, and the redevelopment of some components if the cloud deployment scenario is altered. The MULTICLAPP framework described in this paper tackles these issues by presenting a three-stage development process that allows multicloud applications to be developed without being coupled to any concrete vendor. MDE and adaptation techniques are used throughout the software development stages in order to abstract the software from each vendor's service specifications. As a result of this, multicloud applications or their subcomponents can be reassigned to different cloud platforms without having to undergo a partial or complete redevelopment process.},
booktitle = {Proceedings of the Second Nordic Symposium on Cloud Computing &amp; Internet Technologies},
pages = {30–37},
numpages = {8},
keywords = {vendor lock-in, multicloud, framework, cloud, UML profile},
location = {Oslo, Norway},
series = {NordiCloud '13}
}

@article{10.1016/j.infsof.2016.11.007,
author = {Ouni, Ali and Kula, Raula Gaikovina and Kessentini, Marouane and Ishio, Takashi and German, Daniel M. and Inoue, Katsuro},
title = {Search-based software library recommendation using multi-objective optimization},
year = {2017},
issue_date = {March 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {83},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.11.007},
doi = {10.1016/j.infsof.2016.11.007},
abstract = {Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers.Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find "useful" third-party libraries to the implementation of their software systems.Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries.Results: We evaluated our approach on 6083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92%, precision of 51% and recall of 68%, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5.Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {55–75},
numpages = {21},
keywords = {Software reuse, Software library, Search-based software engineering, Multi-objective optimization}
}

@article{10.1007/s10270-015-0498-5,
author = {Rodrigues, Taniro and Delicato, Fl\'{a}via C. and Batista, Thais and Pires, Paulo F. and Pirmez, Luci},
title = {An approach based on the domain perspective to develop WSAN applications},
year = {2017},
issue_date = {October   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0498-5},
doi = {10.1007/s10270-015-0498-5},
abstract = {As wireless sensor and actuator networks (WSANs) can be used in many different domains, WSAN applications have to be built from two viewpoints: domain and network. These different viewpoints create a gap between the abstractions handled by the application developers, namely the domain and network experts. Furthermore, there is a coupling between the application logic and the underlying sensor platform, which results in platform-dependent projects and source codes difficult to maintain, modify, and reuse. Consequently, the process of developing an application becomes cumbersome. In this paper, we propose a model-driven architecture (MDA) approach for WSAN application development. Our approach aims to facilitate the task of the developers by: (1) enabling application design through high abstraction level models; (2) providing a specific methodology for developing WSAN applications; and (3) offering an MDA infrastructure composed of PIM, PSM, and transformation programs to support this process. Our approach allows the direct contribution of domain experts in the development of WSAN applications, without requiring specific knowledge of programming WSAN platforms. In addition, it allows network experts to focus on the specific characteristics of their area of expertise without the need of knowing each specific application domain.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {949–977},
numpages = {29},
keywords = {WSAN applications, UML profile, Model-driven architecture, Domain-specific language, Code generation, Architecture, Abstraction}
}

@inproceedings{10.1145/1810295.1810404,
author = {Farias, Kleinner},
title = {Empirical evaluation of effort on composing design models},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810295.1810404},
doi = {10.1145/1810295.1810404},
abstract = {The importance of model composition in model-centric software development is recognized by researchers and practitioners. However, the lack of empirical evidence about the impact of model composition techniques on developers' effort is a key impairment for their adoption in real-world design settings. Software engineers are left without any guidance on how to properly use certain model techniques in a way that effectively reduces their development effort. This work aims to address this problem by: (1) providing empirical evidence on model composition effort through a family of experimental studies; (2) defining quantitative indicators to objectively assess key attributes of model composition effort; (3) deriving a method to support the systematic application of composition techniques; and (4) conceiving a new model composition technique to overcome the problems identified throughout the experimental evaluations.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
pages = {405–408},
numpages = {4},
keywords = {model composition, empirical studies, UML},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1007/978-3-540-30587-3_36,
author = {Morris, Edwin and Anderson, Wm B. and Ward, Mary Catherine and Smith, Dennis},
title = {Ten signs of a good reuse management plan},
year = {2005},
isbn = {3540245480},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30587-3_36},
doi = {10.1007/978-3-540-30587-3_36},
abstract = {A Reuse Management Plan defines the strategy for selecting, approving and upgrading common reusable software components The SEI, in conjunction with the U.S. Army, the Boeing Company, and the Fraunhofer USA Center for Experimental Software Engineering, is developing a Reuse Management Plan for a large Army program. Ten critical features of quality Reuse Management Plans have been identified and are presented.},
booktitle = {Proceedings of the 4th International Conference on COTS-Based Software Systems},
pages = {268–277},
numpages = {10},
location = {Bilbao, Spain},
series = {ICCBSS'05}
}

@inproceedings{10.5555/648114.748900,
author = {Knauber, Peter and Thiel, Steffen},
title = {Session Report on Product Issues in Product Family Engineering},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This report gives an overview of the session on product issues of the 4th International Workshop on Product Family Engineering. It briefly sketches the issues presented in the technical session and summarizes the results and open issues of the subsequent discussion session.},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {3–12},
numpages = {10},
series = {PFE '01}
}

@article{10.1177/0037549715603480,
author = {Peters, Brady},
title = {Integrating acoustic simulation in architectural design workflows},
year = {2015},
issue_date = {9 2015},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
volume = {91},
number = {9},
issn = {0037-5497},
url = {https://doi.org/10.1177/0037549715603480},
doi = {10.1177/0037549715603480},
abstract = {Sound is an important part of our experience of buildings. However, architects design largely using visually based techniques and largely for visual phenomena. Aiming to address this problem, the research presented in this paper proposes four digital design workflows that integrate acoustic computer simulation into architectural design. These techniques enable architects to design for both visual and acoustic criteria. The goal is to develop rapid and accessible workflows for architects that allow acoustic performance to be tuned as geometry and materials are modified at the scale of the room, and also at the scale of the surface. The discovery and testing of these techniques takes place within the design of the FabPod, a semi-enclosed meeting room situated within an open-plan working environment. The project builds on previous research investigating the design principles, the acoustic performance, and the fabrication methods of hyperboloid surface geometry. Four design workflows were developed: two of these investigate the acoustic performance of the room and use existing acoustic simulation software, and the other two workflows investigate the acoustic performance of the surface and use custom-written scripts to calculate and visualize sound scattering. This paper presents the background to the study, outlines the digital workflows, describes how they integrate acoustic simulation, and shows some of the data produced by these simulations.},
journal = {Simulation},
month = sep,
pages = {787–808},
numpages = {22},
keywords = {simulation visualization, performance-based design, design workflows, computer-aided design, architectural design, architectural acoustics, acoustic simulation}
}

@inproceedings{10.1145/1185448.1185468,
author = {Hunt, John M. and McGregor, John D.},
title = {A series of choices variability in the development process},
year = {2006},
isbn = {1595933158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1185448.1185468},
doi = {10.1145/1185448.1185468},
abstract = {Software variability is "the ability of a software artifact to vary its behavior at some point in its life cycle" [12]. Almost every software artifact requires some type of variability. While variability is endemic to the creation of software it is rarely the direct focus of study. In addition, software systems have shown an increasing amount of variability in recent years. This work provides an analysis of the decisions involved in providing variability at a specific point in a product. A classification scheme and related choice model is provided that describes the decisions related to variability, making them more explicit and quantifiable.},
booktitle = {Proceedings of the 44th Annual ACM Southeast Conference},
pages = {85–90},
numpages = {6},
keywords = {modeling},
location = {Melbourne, Florida},
series = {ACMSE '06}
}

@article{10.1016/j.jss.2012.03.071,
author = {Poort, Eltjo R. and Van Vliet, Hans},
title = {RCDA: Architecting as a risk- and cost management discipline},
year = {2012},
issue_date = {September, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {9},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.03.071},
doi = {10.1016/j.jss.2012.03.071},
abstract = {We propose to view architecting as a risk- and cost management discipline. This point of view helps architects identify the key concerns to address in their decision making, by providing a simple, relatively objective way to assess architectural significance. It also helps business stakeholders to align the architect's activities and results with their own goals. We examine the consequences of this point of view on the architecture process. The point of view is the basis of RCDA, the Risk- and Cost Driven Architecture approach. So far, more than 150 architects have received RCDA training. For a majority of the trainees, RCDA has a significant positive impact on their architecting work.},
journal = {J. Syst. Softw.},
month = sep,
pages = {1995–2013},
numpages = {19},
keywords = {Software architecture, Risk Management, Cost management}
}

@inproceedings{10.1145/2576768.2598366,
author = {Mkaouer, Mohamed Wiem and Kessentini, Marouane and Bechikh, Slim and Deb, Kalyanmoy and \'{O} Cinn\'{e}ide, Mel},
title = {High dimensional search-based software engineering: finding tradeoffs among 15 objectives for automating software refactoring using NSGA-III},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598366},
doi = {10.1145/2576768.2598366},
abstract = {There is a growing need for scalable search-based software engineering approaches that address software engineering problems where a large number of objectives are to be optimized. Software refactoring is one of these problems where a refactoring sequence is sought that optimizes several software metrics. Most of the existing refactoring work uses a large set of quality metrics to evaluate the software design after applying refactoring operations, but current search-based software engineering approaches are limited to using a maximum of five metrics. We propose for the first time a scalable search-based software engineering approach based on a newly proposed evolutionary optimization method NSGA-III where there are 15 different objectives to be optimized. In our approach, automated refactoring solutions are evaluated using a set of 15 distinct quality metrics. We evaluated this approach on seven large open source systems and found that, on average, more than 92% of code smells were corrected. Statistical analysis of our experiments over 31 runs shows that NSGA-III performed significantly better than two other many-objective techniques (IBEA and MOEA/D), a multi-objective algorithm (NSGA-II) and two mono-objective approaches, hence demonstrating that our NSGA-III approach represents the new state of the art in fully-automated refactoring.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1263–1270},
numpages = {8},
keywords = {search-based software engineering, refactroing, code-smells},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.1145/2000259.2000285,
author = {Galster, Matthias and Avgeriou, Paris},
title = {Empirically-grounded reference architectures: a proposal},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000285},
doi = {10.1145/2000259.2000285},
abstract = {A reference architecture describes core elements of the software architecture for systems that stem from the same domain. A reference architecture ensures interoperability of systems through standardization. It also facilitates the instantiation of new concrete architectures. However, we currently lack procedures for systematically designing reference architectures that are empirically-grounded. Being empirically-grounded would increase the validity and reusability of a reference architecture. We therefore present an approach which helps systematically design reference architectures. Our approach consists of six steps performed by the software architect and domain experts. It helps design reference architectures either from scratch, or based on existing architecture artifacts. We also illustrate how our approach could be applied to the design of two existing reference architectures found in literature.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {153–158},
numpages = {6},
keywords = {software architecture, reference architecture, empirically-grounded, design process},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.5555/648114.748901,
author = {Northrop, Linda M. and Bachmann, Felix and Due\~{n}as, Juan C.},
title = {Report on Discussion Sessions "Diversity Solutions" and "Light-Weight Processes"},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This document reports about the papers presented in the sessions "Diversity solutions" and "Light-weight processes", as well as the discussion session. The main results, conclusions and open points in the sessions are included.},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {258–263},
numpages = {6},
series = {PFE '01}
}

@article{10.1016/j.websem.2006.11.006,
author = {Wang, Hai H. and Li, Yuan Fang and Sun, Jing and Zhang, Hongyu and Pan, Jeff},
title = {Verifying feature models using OWL},
year = {2007},
issue_date = {June, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {5},
number = {2},
issn = {1570-8268},
url = {https://doi.org/10.1016/j.websem.2006.11.006},
doi = {10.1016/j.websem.2006.11.006},
abstract = {Feature models are widely used in domain engineering to capture common and variant features among systems in a particular domain. However, the lack of a formal semantics and reasoning support of feature models has hindered the development of this area. Industrial experiences also show that methods and tools that can support feature model analysis are badly appreciated. Such reasoning tool should be fully automated and efficient. At the same time, the reasoning tool should scale up well since it may need to handle hundreds or even thousands of features a that modern software systems may have. This paper presents an approach to modeling and verifying feature diagrams using Semantic Web OWL ontologies. We use OWL DL ontologies to precisely capture the inter-relationships among the features in a feature diagram. OWL reasoning engines such as FaCT++ are deployed to check for the inconsistencies of feature configurations fully automatically. Furthermore, a general OWL debugger has been developed to tackle the disadvantage of lacking debugging aids for the current OWL reasoner and to complement our verification approach. We also developed a CASE tool to facilitate visual development, interchange and reasoning of feature diagrams in the Semantic Web environment.},
journal = {Web Semant.},
month = jun,
pages = {117–129},
numpages = {13},
keywords = {Semantic Web, Ontologies, OWL, Feature modeling}
}

@inproceedings{10.5555/648114.748905,
author = {Clements, Paul C.},
title = {On the Importance of Product Line Scope},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {70–78},
numpages = {9},
series = {PFE '01}
}

@article{10.1007/s10009-016-0432-3,
author = {Parizi, Reza Meimandi and Ghani, Abdul Azim and Lee, Sai Peck and Khan, Saif Ur},
title = {RAMBUTANS: automatic AOP-specific test generation tool},
year = {2017},
issue_date = {November  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {6},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-016-0432-3},
doi = {10.1007/s10009-016-0432-3},
abstract = {Aspect-oriented programming (AOP) is a programmatic methodology to handle better modularized code by separating crosscutting concerns from the traditional abstraction boundaries. Automated testing, as one of the most demanding needs of the software development to reduce both human effort and costs, is a delicate issue in testing aspect-oriented programs. Prior studies in the automated test generation for aspect-oriented programs have been very limited with respect to the need for both adequate tool support and capability concerning effectiveness and efficiency. This paper describes a new AOP-specific tool for testing aspect-oriented programs, called RAMBUTANS. The RAMBUTANS tool uses a directed random testing technique that is especially well suited for generating tests for aspectual features in AspectJ. The directed random aspect of the tool is parameterized by associating weights to aspects, advice, methods, and classes by controlling object and joint point creations during the test generation process. We present a comprehensive empirical evaluation of our tool against the current AOP test generation approaches on three industrial aspect-oriented projects. The results of the experimental and statistical tests showed that RAMBUTANS tool produces test suites that have higher fault-detection capability and efficiency for AspectJ-like programs.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = nov,
pages = {743–761},
numpages = {19},
keywords = {Testing tool, Software testing, Object-oriented programming, Automated test generation, AspectJ, Aspect-oriented programming}
}

@article{10.1007/s00766-013-0193-4,
author = {Antonelli, Leandro and Rossi, Gustavo and Leite, Julio Cesar and Ara\'{u}jo, Jo\~{a}o},
title = {Early identification of crosscutting concerns with the Language Extended Lexicon},
year = {2015},
issue_date = {June      2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {2},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0193-4},
doi = {10.1007/s00766-013-0193-4},
abstract = {Large-scale software applications are complex systems that involve a myriad of different concerns. Ideally, these concerns should be organized into separated and different modules, but often some of these concerns overlap and crosscut each other. Such a situation is problematic, as concerns are tangled and scattered into different modules; thus, design and source code become difficult to produce and maintain. The Modularity community has been addressing crosscutting concerns by developing techniques based on separation of concerns. This separation must be done as early as possible during software construction to obtain a more modular and consequently better maintainable software, where evolution is performed with less effort and the possibility of introducing unforeseen mistakes is minimal. In this paper, we propose a strategy to identify crosscutting concerns at requirements level, i.e., at early stages in the software development process, by using the Language Extended Lexicon.},
journal = {Requir. Eng.},
month = jun,
pages = {139–161},
numpages = {23},
keywords = {Requirements engineering, Modularity, Language Extended Lexicon, Crosscutting concerns}
}

@inproceedings{10.5555/1862739.1862748,
author = {Thalheim, Bernhard and Schewe, Klaus-Dieter and Ma, Hui},
title = {Conceptual application domain modelling},
year = {2009},
isbn = {9781920682774},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Application domain description precedes requirements engineering, and is the basis for the development of a software or information system that satisfies all expectations of its users. The greatest challenge in this area is the evolution of the application domain itself. In this paper we address this problem by explicit consideration of application cases that are defined by user profiles and intentions and the system environment, i.e. scope and context. User profiles and intentions are captured through the concept of persona. We show how the application domain description can be mapped to requirements and discuss engineering of application domain descriptions.},
booktitle = {Proceedings of the Sixth Asia-Pacific Conference on Conceptual Modeling - Volume 96},
pages = {49–58},
numpages = {10},
location = {Wellington, New Zealand},
series = {APCCM '09}
}

@inproceedings{10.5555/2025896.2025909,
author = {Przyby\l{}ek, Adam},
title = {Systems evolution and software reuse in object-oriented programming and aspect-oriented programming},
year = {2011},
isbn = {9783642219511},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Every new programming technique makes claims that software engineers want to hear. Such is the case with aspect-oriented programming (AOP). This paper describes a quasi-controlled experiment which compares the evolution of two functionally equivalent programs, developed in two different paradigms. The aim of the study is to explore the claims that software developed with aspect-oriented languages is easier to maintain and reuse than this developed with object-oriented languages. We have found no evidence to support these claims.},
booktitle = {Proceedings of the 49th International Conference on Objects, Models, Components, Patterns},
pages = {163–178},
numpages = {16},
keywords = {separation of concerns, reusability, maintainability, AOP},
location = {Zurich, Switzerland},
series = {TOOLS'11}
}

@inproceedings{10.5555/2666795.2666811,
author = {Weyns, Danny and Iftikhar, M. Usman and Malek, Sam and Andersson, Jesper},
title = {Claims and supporting evidence for self-adaptive systems: a literature study},
year = {2012},
isbn = {9781467317870},
publisher = {IEEE Press},
abstract = {Despite the vast body of work on self-adaption, no systematic study has been performed on the claims associated with self-adaptation and the evidence that exists for these claims. As such an insight is crucial for researchers and engineers, we performed a literature study of the research results from SEAMS since 2006 and the associated Dagstuhl seminar in 2008. The study shows that the primary claims of self-adaptation are improved flexibility, reliability, and performance of the system. On the other hand, the tradeoffs implied by self-adaptation have not received much attention. Evidence is obtained from basic examples, or simply lacking. Few systematic empirical studies have been performed, and no industrial evidence is reported. From the study, we offer the following recommendations to move the field forward: to improve evaluation, researchers should make their assessment methods, tools and data publicly available; to deal with poor discussion of limitations, conferences/workshops should require an explicit section on limitations in engineering papers; to improve poor treatment of tradeoffs, this aspect should be an explicit subject of reviews; and finally, to enhance industrial validation, the best academy-industry efforts could be formally recognized by the community.},
booktitle = {Proceedings of the 7th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {89–98},
numpages = {10},
location = {Zurich, Switzerland},
series = {SEAMS '12}
}

@inproceedings{10.5555/648114.746416,
author = {Stoermer, Christoph and Roeddiger, Markus},
title = {Introducing Product Lines in Small Embedded Systems},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {How do you introduce product lines into a hardware dominated organization that has increasing software architecture awareness and products with extremely limited memory resources__ __ This experience paper describes the transition steps from a conventional development to a first product, conformant to a product line design. Further steps towards a full product line are outlined in this on-going project. Key aspects like investigation of requirements, design, set of tools, speed of change, skills, and organization commitment are addressed. The investigation phase involved an architecture reconstruction of existing products and a requirements elicitation. The architectural design phase used the Attribute Driven Design method (ADD) of the Software Engineering Institute (SEI). The generated architecture had to be mapped onto the business unit's design tool, which generated the component code. Instead of reaching a full product line approach with the first product this experience report emphasizes the right speed of change by firstly reaching a high commitment level at the organization in software architecture techniques. This builds the necessary foundation to survive higher investments for the first few products until the cost benefit of product lines pay back later on. Essential in the introduction phase are personal skills, like integrity in order to support a successful change at the organization. Those skills form a foundation to achieve a committed organization.},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {101–112},
numpages = {12},
series = {PFE '01}
}

@article{10.1007/s00450-012-0234-0,
author = {Baresi, Luciano and Ghezzi, Carlo},
title = {A journey through SMScom: self-managing situational computing},
year = {2013},
issue_date = {November  2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {4},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-012-0234-0},
doi = {10.1007/s00450-012-0234-0},
abstract = {This article provides an overall view of the research that has been done in the context of self-managing software within the SMScom project. We start by the motivations that inspired the research, and then we focus on a reference framework that explains its conceptual underpinnings and on the paradigm shift it calls for in the way we currently engineer software. Next we focus on some specific research results achieved at the architecture and verification support level.},
journal = {Comput. Sci.},
month = nov,
pages = {267–277},
numpages = {11},
keywords = {pervasive systems, Ubiquitous, Internet of things, Cyber-physical systems}
}

@inproceedings{10.1145/1960314.1960321,
author = {Silva Filho, Roberto Silveira and Bronsard, Fran\c{c}ois and Hasling, William M.},
title = {Experiences documenting and preserving software constraints using aspects},
year = {2011},
isbn = {9781450306065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1960314.1960321},
doi = {10.1145/1960314.1960321},
abstract = {Software systems are increasingly being built as compositions of reusable artifacts (components, frameworks, toolkits, plug-ins, APIs, etc) that have non-trivial usage constraints in the form of interface contracts, underlying assumptions and design composition rules. Satisfying these constraints is challenging: they are often not well documented; or they are difficult to integrate into the software development process in ways that allow their identification by developers; or they may not be enforced by existing tools and development environments. Aspect-Oriented Programming has been advocated as an approach to represent and enforce software constraints in code artifacts. Aspects can be used to detect constraint violations, or more pro-actively, to ensure that the constraints are satisfied without requiring the developer's attention. This paper discusses our experience using aspects to document and enforce software constraints in an industrial application, specifically TDE/UML, a model-driven software testing tool developed at SIEMENS. We present an analysis of common constraints found in our case study, a set of primitive aspects developed to help the enforcement of software constraints, and show how AOP has been incorporated into existing software development and governance approaches in the TDE/UML project. We conclude with a discussion of strengths and limitations of AspectJ in supporting these constraints.},
booktitle = {Proceedings of the Tenth International Conference on Aspect-Oriented Software Development Companion},
pages = {7–18},
numpages = {12},
keywords = {software architecture, design documentation, aspect-oriented programming, architectural constraints},
location = {Porto de Galinhas, Brazil},
series = {AOSD '11}
}

@article{10.4018/joci.2010040101,
author = {Ghedira, Chirine and Maamar, Zakaria and Vincent, Lucien and Boukadi, Khouloud},
title = {CSMA: Context-Based, Service-Oriented Modeling and Analysis Method for Modern Enterprise Applications},
year = {2010},
issue_date = {April 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {2},
issn = {1947-9344},
url = {https://doi.org/10.4018/joci.2010040101},
doi = {10.4018/joci.2010040101},
abstract = {Since the beginning of the Service-Oriented Architecture SOA paradigm, with its various implementation technologies such as Web services, the focus of industrial communities has been on providing tools that would allow seamless and flexible application integration within and across enterprises' boundaries. In this paper, the authors present a Context-based, Service-oriented Modeling and Analysis CSMA method that guides service engineers in their choices of identifying, defining, and analyzing adaptable business services. The proposed method is business centric and comprises a set of structured steps grouped in two phases. Besides, the CSMA embraces Model-Driven Architecture MDA principles to model and refine adaptable business services models in the PIM level. The results from a pilot validation of CSMA for SOA enablement of a realistic enterprise training solutions are also presented.},
journal = {Int. J. Organ. Collect. Intell.},
month = apr,
pages = {1–28},
numpages = {28},
keywords = {Service-oriented Modeling and Analysis, Service-Oriented Architecture, Service engineers, Model-Driven Architecture, Adaptable Business Services}
}

@inproceedings{10.1145/2642937.2642990,
author = {Abal, Iago and Brabrand, Claus and Wasowski, Andrzej},
title = {42 variability bugs in the linux kernel: a qualitative analysis},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642990},
doi = {10.1145/2642937.2642990},
abstract = {Feature-sensitive verification pursues effective analysis of the exponentially many variants of a program family. However, researchers lack examples of concrete bugs induced by variability, occurring in real large-scale systems. Such a collection of bugs is a requirement for goal-oriented research, serving to evaluate tool implementations of feature-sensitive analyses by testing them on real bugs. We present a qualitative study of 42 variability bugs collected from bug-fixing commits to the Linux kernel repository. We analyze each of the bugs, and record the results in a database. In addition, we provide self-contained simplified C99 versions of the bugs, facilitating understanding and tool evaluation. Our study provides insights into the nature and occurrence of variability bugs in a large C software system, and shows in what ways variability affects and increases the complexity of software bugs.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {421–432},
numpages = {12},
keywords = {software variability, linux, feature interactions, bugs},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1109/SHARK-ADI.2007.9,
author = {Capilla, Rafael and Nava, Francisco and Duenas, Juan C.},
title = {Modeling and Documenting the Evolution of Architectural Design Decisions},
year = {2007},
isbn = {0769529518},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SHARK-ADI.2007.9},
doi = {10.1109/SHARK-ADI.2007.9},
abstract = {All software systems are built as a result of a set of design decisions that are made during the architecting phase. At present, there is still a lack of appropriate notations, methods and tools for recording and exploiting these architectural design decisions. In addition, the need for maintaining and evolving the decisions made in the past turns critical for the success of the evolution of the system. In this research paper we extend a previous work to detail those issues related to the evolution of architectural design decisions.},
booktitle = {Proceedings of the Second Workshop on SHAring and Reusing Architectural Knowledge Architecture, Rationale, and Design Intent},
pages = {9},
series = {SHARK-ADI '07}
}

@article{10.5555/1349897.1350159,
author = {Tekinerdogan, Bedir and Sozer, Hasan and Aksit, Mehmet},
title = {Software architecture reliability analysis using failure scenarios},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {4},
issn = {0164-1212},
abstract = {With the increasing size and complexity of software in embedded systems, software has now become a primary threat for the reliability. Several mature conventional reliability engineering techniques exist in literature but traditionally these have primarily addressed failures in hardware components and usually assume the availability of a running system. Software architecture analysis methods aim to analyze the quality of software-intensive system early at the software architecture design level and before a system is implemented. We propose a Software Architecture Reliability Analysis Approach (SARAH) that benefits from mature reliability engineering techniques and scenario-based software architecture analysis to provide an early software reliability analysis at the architecture design level. SARAH defines the notion of failure scenario model that is based on the Failure Modes and Effects Analysis method (FMEA) in the reliability engineering domain. The failure scenario model is applied to represent so-called failure scenarios that are utilized to derive fault tree sets (FTS). Fault tree sets are utilized to provide a severity analysis for the overall software architecture and the individual architectural elements. Despite conventional reliability analysis techniques which prioritize failures based on criteria such as safety concerns, in SARAH failure scenarios are prioritized based on severity from the end-user perspective. SARAH results in a failure analysis report that can be utilized to identify architectural tactics for improving the reliability of the software architecture. The approach is illustrated using an industrial case for analyzing reliability of the software architecture of the next release of a Digital TV.},
journal = {J. Syst. Softw.},
month = apr,
pages = {558–575},
numpages = {18},
keywords = {Scenario-based architectural evaluation, Reliability analysis, Fault trees, FMEA}
}

@article{10.1016/j.ijinfomgt.2015.09.008,
author = {Chang, Victor and Ramachandran, Muthu and Yao, Yulin and Kuo, Yen-Hung and Li, Chung-Sheng},
title = {A resiliency framework for an enterprise cloud},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {36},
number = {1},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2015.09.008},
doi = {10.1016/j.ijinfomgt.2015.09.008},
abstract = {We have presented a resilient framework for an enterprise cloud.We have developed an architecture with four major services to demonstrate resiliency, where the cloud computing adoption framework (CCAF) takes the center role to blend other services.We explain how our work is relevant to business resiliency.We have the support from a large scale survey to ensure that our design and service can meet the large number of user requirements. This paper presents a systematic approach to develop a resilient software system which can be developed as emerging services and analytics for resiliency. While using the resiliency as a good example for enterprise cloud security, all resilient characteristics should be blended together to produce greater impacts. A framework, cloud computing adoption framework (CCAF), is presented in details. CCAF has four major types of emerging services and each one has been explained in details with regard to the individual function and how each one can be integrated. CCAF is an architectural framework that blends software resilience, service components and guidelines together and provides real case studies to produce greater impacts to the organizations adopting cloud computing and security. CCAF provides business alignments and provides agility, efficiency and integration for business competitive edge. In order to validate user requirements and system designs, a large scale survey has been conducted with detailed analysis provided for each major question. We present our discussion and conclude that the use of CCAF framework can illustrate software resilience and security improvement for enterprise security. CCAF framework itself is validated as an emerging service for enterprise cloud computing with analytics showing survey analysis.},
journal = {Int. J. Inf. Manag.},
month = feb,
pages = {155–166},
numpages = {12},
keywords = {Resilient software for Enterprise Cloud, Cloud security and software engineering best practice, Cloud computing Adoption Framework (CCAF), - Software resiliency}
}

@article{10.1007/s10664-014-9313-0,
author = {Bavota, Gabriele and Qusef, Abdallah and Oliveto, Rocco and Lucia, Andrea and Binkley, Dave},
title = {Are test smells really harmful? An empirical study},
year = {2015},
issue_date = {August    2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9313-0},
doi = {10.1007/s10664-014-9313-0},
abstract = {Bad code smells have been defined as indicators of potential problems in source code. Techniques to identify and mitigate bad code smells have been proposed and studied. Recently bad test code smells (test smells for short) have been put forward as a kind of bad code smell specific to tests such a unit tests. What has been missing is empirical investigation into the prevalence and impact of bad test code smells. Two studies aimed at providing this missing empirical data are presented. The first study finds that there is a high diffusion of test smells in both open source and industrial software systems with 86 % of JUnit tests exhibiting at least one test smell and six tests having six distinct test smells. The second study provides evidence that test smells have a strong negative impact on program comprehension and maintenance. Highlights from this second study include the finding that comprehension is 30 % better in the absence of test smells.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1052–1094},
numpages = {43},
keywords = {Unit testing, Test smells, Mining software repositories, Controlled experiments}
}

@inproceedings{10.1145/1982185.1982333,
author = {Varela, Patr\'{\i}cia and Ara\'{u}jo, Jo\~{a}o and Brito, Isabel and Moreira, Ana},
title = {Aspect-oriented analysis for software product lines requirements engineering},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982333},
doi = {10.1145/1982185.1982333},
abstract = {Requirements analysis and modeling for Software Product Lines demands the use of feature models, but also requires additional models to help identifying, describing, and specifying features. Traditional approaches usually perform this manually and, in general, the identification and modularization of crosscutting features is ignored, or not handled systematically. This hinders requirements change. We propose an aspect-oriented approach for SPL enriched to automatically derive feature models where crosscutting features are identified and modularized using aspect-oriented concepts and techniques. This is achieved by adapting and extending the AORA (Aspect-Oriented Requirements Analysis) approach. AORA provides templates to specify and organize requirements based on concerns and responsibilities. A set of heuristics is defined to help identifying features and their dependencies in a product line. A tool was developed to automatically generate the feature model from AORA templates.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {667–674},
numpages = {8},
keywords = {software product lines, aspect-oriented requirements analysis},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1007/11678779_7,
author = {Pesonen, Jani and Katara, Mika and Mikkonen, Tommi},
title = {Production-Testing of embedded systems with aspects},
year = {2005},
isbn = {3540326049},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11678779_7},
doi = {10.1007/11678779_7},
abstract = {A test harness plays an important role in the development of any embedded system. Although the harness can be excluded from final products, its architecture should support maintenance and reuse, especially in the context of testing product families. Aspect-orientation is a new technique for software architecture that should enable scattered and tangled code to be addressed in a modular fashion, thus facilitating maintenance and reuse. However, the design of interworking between object-oriented baseline architecture and aspects attached on top of it is an issue, which has not been solved conclusively. For industrial-scale use, guidelines on what to implement with objects and what with aspects should be derived. In this paper, we introduce a way to reflect the use of aspect-orientation to production testing software of embedded systems. Such piece of a test harness is used to smoke test the proper functionality of a manufactured device. The selection of suitable implementation technique is based on variance of devices to be tested, with aspects used as means for increased flexibility. Towards the end of the paper, we also present the results of our experiments in the Symbian OS context that show some obstacles in the current tool support that should be addressed before further case studies can be conducted.},
booktitle = {Proceedings of the First Haifa International Conference on Hardware and Software Verification and Testing},
pages = {90–102},
numpages = {13},
location = {Haifa, Israel},
series = {HVC'05}
}

@article{10.1016/j.jss.2013.06.064,
author = {Tahir, Abbas and Tosi, Davide and Morasca, Sandro},
title = {A systematic review on the functional testing of semantic web services},
year = {2013},
issue_date = {November, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {11},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.064},
doi = {10.1016/j.jss.2013.06.064},
abstract = {Semantic web services are gaining more attention as an important element of the emerging semantic web. Therefore, testing semantic web services is becoming a key concern as an essential quality assurance measure. The objective of this systematic literature review is to summarize the current state of the art of functional testing of semantic web services by providing answers to a set of research questions. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the results, a total of 34 studies were identified as relevant. Required information was extracted from the studies and summarized. Our systematic literature review identified some approaches available for deriving test cases from the specifications of semantic web services. However, many of the approaches are either not validated or the validation done lacks credibility. We believe that a substantial amount of work remains to be done to improve the current state of research in the area of testing semantic web services.},
journal = {J. Syst. Softw.},
month = nov,
pages = {2877–2889},
numpages = {13},
keywords = {Testing approach, Systematic literature review, Semantic web services, Functional testing}
}

@article{10.1016/j.jnca.2014.07.019,
author = {Sun, Le and Dong, Hai and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Chang, Elizabeth},
title = {Cloud service selection},
year = {2014},
issue_date = {October 2014},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {45},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2014.07.019},
doi = {10.1016/j.jnca.2014.07.019},
abstract = {Cloud technology connects a network of virtualized computers that are dynamically provisioned as computing resources, based on negotiated agreements between service providers and users. It delivers information technology resources in diverse forms of service, and the explosion of Cloud services on the Internet brings new challenges in Cloud service discovery and selection. To address these challenges, a range of studies has been carried out to develop advanced techniques that will assist service users to choose appropriate services. In this paper, we survey state-of-the-art Cloud service selection approaches, which are analyzed from the following five perspectives: decision-making techniques; data representation models; parameters and characteristics of Cloud services; contexts, purposes. After comparing and summarizing the reviewed approaches from these five perspectives, we identify the primary research issues in contemporary Cloud service selection. This survey is expected to bring benefits to both researchers and business agents.},
journal = {J. Netw. Comput. Appl.},
month = oct,
pages = {134–150},
numpages = {17},
keywords = {Decision-making, Cloud service selection, Cloud computing}
}

@inproceedings{10.5555/1787553.1787572,
author = {Canal, Carlos and Murillo, Juan Manuel and Poizat, Pascal},
title = {Practical approaches for software adaptation: report on the 4th workshop WCAT at ECOOP 2007},
year = {2007},
isbn = {3540781943},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Coordination and Adaptation are two key issues when developing complex distributed systems. Coordination focuses on the interaction among software entities. Adaptation focuses on solving the problems that arise when the interacting entities do not match properly. This is the report of the fourth edition of the WCAT workshop, that took place in Berlin jointly with ECOOP 2007. Previous editions the workshop dealt with general issues which mainly served for a better characterization of Software Adaptation as an emerging discipline within the field of Software Engineering. For this edition, we wanted to put the focus on practical approaches for software adaptation, in order to show how this discipline helps in the construction of current software systems.},
booktitle = {Proceedings of the 2007 Conference on Object-Oriented Technology},
pages = {154–165},
numpages = {12},
location = {Berlin, Germany},
series = {ECOOP'07}
}

@inproceedings{10.1145/1806799.1806813,
author = {Ferrari, Fabiano and Burrows, Rachel and Lemos, Ot\'{a}vio and Garcia, Alessandro and Figueiredo, Eduardo and Cacho, Nelio and Lopes, Frederico and Temudo, Nathalia and Silva, Liana and Soares, Sergio and Rashid, Awais and Masiero, Paulo and Batista, Thais and Maldonado, Jos\'{e}},
title = {An exploratory study of fault-proneness in evolving aspect-oriented programs},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806799.1806813},
doi = {10.1145/1806799.1806813},
abstract = {This paper presents the results of an exploratory study on the fault-proneness of aspect-oriented programs. We analysed the faults collected from three evolving aspect-oriented systems, all from different application domains. The analysis develops from two different angles. Firstly, we measured the impact of the obliviousness property on the fault-proneness of the evaluated systems. The results show that 40% of reported faults were due to the lack of awareness among base code and aspects. The second analysis regarded the fault-proneness of the main aspect-oriented programming (AOP) mechanisms, namely pointcuts, advices and intertype declarations. The results indicate that these mechanisms present similar fault-proneness when we consider both the overall system and concern-specific implementations. Our findings are reinforced by means of statistical tests. In general, this result contradicts the common intuition stating that the use of pointcut languages is the main source of faults in AOP.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
pages = {65–74},
numpages = {10},
keywords = {software testing, fault-proneness, aspect-oriented programming},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1109/ESEM.2009.5316001,
author = {Barney, Sebastian and Wohlin, Claes and Aurum, Aybuke},
title = {Balancing software product investments},
year = {2009},
isbn = {9781424448425},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ESEM.2009.5316001},
doi = {10.1109/ESEM.2009.5316001},
abstract = {The long-term sustainability of a software product depends on more than developing features. Priorities are placed on aspects that support the development of software, like software product quality (eg. ISO 9126), project constraints — time and cost, and even the development of intellectual capital (IC). A greater focus on any one aspect takes priority from another, but as each aspects delivers a different type of value managers have trouble comparing and balancing these aspects. This paper presents a method to help determine the balance between key priorities in the software development process. The method is applied to a new case study, that also combines with results from previous studies. The results show it is possible to compare features, quality, time, cost and IC in a comprehensive way, with the case study showing that participants perceive a change from a shorter-term product perspective to a longer-term organisation beneficial to the business.},
booktitle = {Proceedings of the 2009 3rd International Symposium on Empirical Software Engineering and Measurement},
pages = {257–268},
numpages = {12},
series = {ESEM '09}
}

@article{10.1016/j.infsof.2007.01.001,
author = {Kim, Chul Jin and Chung, Hyun Sook and Cho, Eun Sook},
title = {Micro and macro workflow variability design techniques of component},
year = {2008},
issue_date = {March, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {50},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.01.001},
doi = {10.1016/j.infsof.2007.01.001},
abstract = {Components should provide variability in satisfying a variety of domains [C. Szyperski, Component Software: Beyond Object-Oriented Programming, Addison-Wesley, 2002.], but it is not easy to develop components which can be applied to all domains. Although components are developed by analyzing many different requirements, developing components that satisfy all requirements is difficult since unexpected requirements occur during the utilization of components. Hence, providing the variability of components becomes an important prerequisite for a successful component-based application development. In this paper, we propose a variability design technique that can satisfy the business workflow requirements of many different kinds of domains. The technique addresses a method for designing the variability of the workflow in a more detailed method and uses an object-oriented mechanism and design patterns. One of the most important goals of this technique is to provide a practical process can be effectively applied in component-based application development.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {259–279},
numpages = {21},
keywords = {Reusability, Micro/macro workflow, Component variability}
}

@article{10.1016/j.infsof.2012.11.005,
author = {Li, Zengyang and Liang, Peng and Avgeriou, Paris},
title = {Application of knowledge-based approaches in software architecture: A systematic mapping study},
year = {2013},
issue_date = {May, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {5},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.005},
doi = {10.1016/j.infsof.2012.11.005},
abstract = {Context: Knowledge management technologies have been employed across software engineering activities for more than two decades. Knowledge-based approaches can be used to facilitate software architecting activities (e.g., architectural evaluation). However, there is no comprehensive understanding on how various knowledge-based approaches (e.g., knowledge reuse) are employed in software architecture. Objective: This work aims to collect studies on the application of knowledge-based approaches in software architecture and make a classification and thematic analysis on these studies, in order to identify the gaps in the existing application of knowledge-based approaches to various architecting activities, and promising research directions. Method: A systematic mapping study is conducted for identifying and analyzing the application of knowledge-based approaches in software architecture, covering the papers from major databases, journals, conferences, and workshops, published between January 2000 and March 2011. Results: Fifty-five studies were selected and classified according to the architecting activities they contribute to and the knowledge-based approaches employed. Knowledge capture and representation (e.g., using an ontology to describe architectural elements and their relationships) is the most popular approach employed in architecting activities. Knowledge recovery (e.g., documenting past architectural design decisions) is an ignored approach that is seldom used in software architecture. Knowledge-based approaches are mostly used in architectural evaluation, while receive the least attention in architecture impact analysis and architectural implementation. Conclusions: The study results show an increased interest in the application of knowledge-based approaches in software architecture in recent years. A number of knowledge-based approaches, including knowledge capture and representation, reuse, sharing, recovery, and reasoning, have been employed in a spectrum of architecting activities. Knowledge-based approaches have been applied to a wide range of application domains, among which ''Embedded software'' has received the most attention.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {777–794},
numpages = {18},
keywords = {Systematic mapping study, Software architecture, Knowledge-based approach, Architecting activity}
}

@article{10.1016/j.datak.2010.07.009,
author = {Parreiras, Fernando Silva and Staab, Steffen},
title = {Editorial: Using ontologies with UML class-based modeling: The TwoUse approach},
year = {2010},
issue_date = {November, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {69},
number = {11},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2010.07.009},
doi = {10.1016/j.datak.2010.07.009},
abstract = {UML class-based models and OWL ontologies constitute modeling approaches with different strengths and weaknesses that make them appropriate for specifying distinct aspects of software systems. We propose an integrated use of both modeling approaches in a coherent framework - TwoUse. We present a framework involving different concrete syntaxes for developing integrated models and use a SPARQL-like approach for writing query operations. We illustrate TwoUse's applicability with a case study and conclude that TwoUse achieves enhancements of non-functional software requirements like maintainability, reusability and extensibility.},
journal = {Data Knowl. Eng.},
month = nov,
pages = {1194–1207},
numpages = {14},
keywords = {Ontologies, Manipulation, Language definition, CASE tools+UML}
}

@article{10.1016/j.jss.2011.06.071,
author = {Kilamo, Terhi and Hammouda, Imed and Mikkonen, Tommi and Aaltonen, Timo},
title = {From proprietary to open source-Growing an open source ecosystem},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.06.071},
doi = {10.1016/j.jss.2011.06.071},
abstract = {In today's business and software arena, Free/Libre/Open Source Software has emerged as a promising platform for software ecosystems. Following this trend, more and more companies are releasing their proprietary software as open source, forming a software ecosystem of related development projects complemented with a social ecosystem of community members. Since the trend is relatively recent, there are few guidelines on how to create and maintain a sustainable open source ecosystem for a proprietary software. This paper studies the problem of building open source communities for industrial software that was originally developed as closed source. Supporting processes, guidelines and best practices are discussed and illustrated through an industrial case study. The research is paving the road for new directions in growing a thriving open source ecosystem.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1467–1478},
numpages = {12},
keywords = {Software ecosystem, Opening proprietary software, Open source engineering, Open source}
}

@article{10.1145/1968587.1968607,
author = {Geetha, D. Evangelin and Kumar, T.V. Suresh and Kanth, K. Rajani},
title = {Framework for hybrid performance prediction process model: use case performance engineering approach},
year = {2011},
issue_date = {May 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/1968587.1968607},
doi = {10.1145/1968587.1968607},
abstract = {The dynamic behavior of distributed systems requires that their performance characteristics be determined rigorously, preferably in the early stages of software engineering process. Evaluation of the performance at the end of software development leads to increase in the cost of design change. To compare design alternatives or to identify system bottlenecks, quantitative system analysis must be carried out from the early stages of the software development life cycle. In this paper we describe a process model, Hybrid Performance Prediction Process Model that allows modeling and evaluating distributed systems with the explicit goal of assessing performance of the software system during feasibility study. The use case performance engineering approach proposed in this paper exploits use case model and provides flexibility to integrate the software performance prediction process with software engineering process. We use an e-parking application to demonstrate various elements in our framework. The performance metrics are obtained and analyzed by considering two software architectures. Sensitivity analysis on the behavior of resources is carried out. This analysis helps to determine the capacity of the execution environment to obtain the defined performance objectives.},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {1–15},
numpages = {15},
keywords = {use case performance engineering, unified modeling language, software performance prediction, simulation model, multitier architecture, hybrid performance prediction process model, 4+1 view model}
}

@article{10.1016/j.infsof.2009.04.004,
author = {Mohagheghi, Parastoo and Dehlen, Vegard and Neple, Tor},
title = {Definitions and approaches to model quality in model-based software development - A review of literature},
year = {2009},
issue_date = {December, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {12},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.04.004},
doi = {10.1016/j.infsof.2009.04.004},
abstract = {More attention is paid to the quality of models along with the growing importance of modelling in software development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {1646–1669},
numpages = {24},
keywords = {UML, Systematic review, Modelling, Model-driven development, Model quality}
}

@inproceedings{10.5555/2041790.2041832,
author = {Capilla, Rafael and Zimmermann, Olaf and Zdun, Uwe and Avgeriou, Paris and K\"{u}ster, Jochen M.},
title = {An enhanced architectural knowledge metamodel linking architectural design decisions to other artifacts in the software engineering lifecycle},
year = {2011},
isbn = {9783642237973},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software architects create and consume many interrelated artifacts during the architecting process. These artifacts may represent functional and nonfunctional requirements, architectural patterns, infrastructure topology units, code, and deployment descriptors as well as architecturally significant design decisions. Design decisions have to be linked to chunks of architecture description in order to achieve a fine-grained control when a design is modified. Moreover, it is imperative to identify quickly the key decisions affected by a runtime change that are critical for a system's mission. This paper extends previous work on architectural knowledge with a metamodel for architectural decision capturing and sharing to: (i) create and maintain fine-grained dependency links between the entities during decision identification, making, and enforcement, (ii) keep track of the evolution of the decisions, and (iii) support runtime decisions.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture},
pages = {303–318},
numpages = {16},
keywords = {traceability, runtime decisions, metamodel, evolution, architectural knowledge, architectural design decisions},
location = {Essen, Germany},
series = {ECSA'11}
}

@article{10.1016/j.jss.2009.08.032,
author = {Tang, Antony and Avgeriou, Paris and Jansen, Anton and Capilla, Rafael and Ali Babar, Muhammad},
title = {A comparative study of architecture knowledge management tools},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.08.032},
doi = {10.1016/j.jss.2009.08.032},
abstract = {Recent research suggests that architectural knowledge, such as design decisions, is important and should be recorded alongside the architecture description. Different approaches have emerged to support such architectural knowledge (AK) management activities. However, there are different notions of and emphasis on what and how architectural activities should be supported. This is reflected in the design and implementation of existing AK tools. To understand the current status of software architecture knowledge engineering and future research trends, this paper compares five architectural knowledge management tools and the support they provide in the architecture life-cycle. The comparison is based on an evaluation framework defined by a set of 10 criteria. The results of the comparison provide insights into the current focus of architectural knowledge management support, their advantages, deficiencies, and conformance to the current architectural description standard. Based on the outcome of this comparison a research agenda is proposed for future work on AK tools.},
journal = {J. Syst. Softw.},
month = mar,
pages = {352–370},
numpages = {19},
keywords = {Design rationale, Architectural knowledge management tool, Architectural design}
}

@inproceedings{10.1007/978-3-642-31762-0_13,
author = {Johnsen, Einar Broch and Schlatte, Rudolf and Tapia Tarifa, S. Lizeth},
title = {A formal model of user-defined resources in resource-restricted deployment scenarios},
year = {2011},
isbn = {9783642317613},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31762-0_13},
doi = {10.1007/978-3-642-31762-0_13},
abstract = {Software today is often developed for deployment on varying architectures. In order to model and analyze the consequences of such deployment choices at an early stage in software development, it seems desirable to capture aspects of low-level deployment concerns in high-level models. In this paper, we propose an integration of a generic cost model for resource consumption with deployment components in Timed ABS, an abstract behavioral specification language for executable object-oriented models. The actual cost model may be user-defined and specified by means of annotations in the executable Timed ABS model, and can be used to capture specific resource requirements such as processing capacity or memory usage. Architectural variations are specified by resource-restricted deployment scenarios with different capacities. For this purpose, the models have deployment components which are parametric in their assigned resources. The approach is demonstrated on an example of multimedia processing servers with a user-defined cost model for memory usage. We use our simulation tool to analyze deadline misses for given usage and deployment scenarios.},
booktitle = {Proceedings of the 2011 International Conference on Formal Verification of Object-Oriented Software},
pages = {196–213},
numpages = {18},
location = {Turin, Italy},
series = {FoVeOOS'11}
}

@inproceedings{10.5555/1129601.1129626,
author = {Chen, Tung-Chieh and Chang, Yao-Wen and Lin, Shyh-Chang},
title = {IMF: interconnect-driven multilevel floorplanning for large-scale building-module designs},
year = {2005},
isbn = {078039254X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present in this paper, a new interconnect-driven multilevel floorplanning, called IMF, to handle large-scale building-module designs. Unlike the traditional multilevel framework that adopts the "V-cycle" framework: bottom-up coarsening followed by top-down uncoarsening, in contrast, IMF works in the "/spl Lambda/-cycle" manner: top-down uncoarsening (partitioning) followed by bottom-up coarsening (merging). The top-down partitioning stage iteratively partitions the floorplan region based on mm-cut bipartitioning with exact net-weight modeling to reduce the number of global interconnections and thus the total wirelength. Then, the bottom-up merging stage iteratively applies fixed-outline floorplanning using simulated annealing for all regions and merges two neighboring regions recursively. We also propose an accelerative fixed-outline floorplanning (AFF) to speed up wirelength minimization under the outline constraint. Experimental results show that IMF consistently obtains the best floorplanning results with the smallest wirelength for large-scale building-module designs, compared with all publicly available floorplanners. In particular, IMF scales very well as the circuit size increases. The /spl Lambda/-cycle multilevel framework outperforms the V-cycle one in the optimization of global circuit effects, such as interconnection and crosstalk optimization, since the /spl Lambda/-cycle framework considers the global configuration first and then processes down to local ones level by level and thus the global effects can be handled at earlier stages. The /spl Lambda/-cycle multilevel framework is general and thus can be readily applied to other problems.},
booktitle = {Proceedings of the 2005 IEEE/ACM International Conference on Computer-Aided Design},
pages = {159–164},
numpages = {6},
location = {San Jose, CA},
series = {ICCAD '05}
}

@inproceedings{10.1145/1985793.1985882,
author = {Hutchinson, John and Rouncefield, Mark and Whittle, Jon},
title = {Model-driven engineering practices in industry},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985882},
doi = {10.1145/1985793.1985882},
abstract = {In this paper, we attempt to address the relative absence of empirical studies of model driven engineering through describing the practices of three commercial organizations as they adopted a model driven engineering approach to their software development. Using in-depth semi-structured interviewing we invited practitioners to reflect on their experiences and selected three to use as exemplars or case studies. In documenting some details of attempts to deploy model driven practices, we identify some 'lessons learned', in particular the importance of complex organizational, managerial and social factors - as opposed to simple technical factors - in the relative success, or failure, of the endeavour. As an example of organizational change management the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {633–642},
numpages = {10},
keywords = {software engineering, model driven engineering, empirical evaluation},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.5555/646781.705927,
author = {Pulvermueller, Elke and Speck, Andreas and Coplien, James and D'Hondt, Maja and Meuter, Wolfgang De},
title = {Feature Interaction in Composed Systems},
year = {2001},
isbn = {3540436758},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The history of computer science has shown that decomposing software applications helps managing their complexity and facilitates reuse, but also bears challenging problems still unsolved, such as the assembly of the decomposed features when non-trivial feature interactions are involved. Examples of features include concerns or aspects, black box or white box components, and functional and non-functional requirements. Approaches such as object-oriented and component-based software development, as well as relatively new directions such as aspect-oriented programming, multi-dimensional separation of concerns and generative programming, all provide technical support for the definition and syntactical assembly of features, but fall short on the semantic level, for example in spotting meaningless or even faulty combinations. At previous ECOOPs, OOPSLAsand GCSEs dedicated events have been organised around the aforementioned technologies, where we experienced a growing awareness of this feature interaction problem. However, feature interaction is often merely dismissed as a secondary problem, percolating as an afterthought while other issues are being addressed. The intention of this workshop was to be the first co-ordinated effort to address the general problem of feature interaction in composed systems separately from other issues.},
booktitle = {Proceedings of the Workshops on Object-Oriented Technology},
pages = {86–97},
numpages = {12},
series = {ECOOP '01}
}

@article{10.1145/383845.383863,
author = {Coady, Yvonne and Kiczales, Gregor and Feeley, Mike and Hutchinson, Norm and Ong, Joon Suan},
title = {Structuring operating system aspects: using AOP to improve OS structure modularity},
year = {2001},
issue_date = {Oct. 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/383845.383863},
doi = {10.1145/383845.383863},
journal = {Commun. ACM},
month = oct,
pages = {79–82},
numpages = {4}
}

@inproceedings{10.1109/ASEW.2008.4686323,
author = {Brcina, Robert and Riebisch, Matthias},
title = {Architecting for evolvability by means of traceability and features},
year = {2008},
isbn = {9781424427765},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASEW.2008.4686323},
doi = {10.1109/ASEW.2008.4686323},
abstract = {The frequent changes during the development and usage of large software systems often lead to a loss of architectural quality which hampers the implementation of further changes and thus the systems' evolution. To maintain the evolvability of such software systems, their architecture has to fulfil particular quality criteria. Available metrics and rigour approaches do not provide sufficient means to evaluate architectures regarding these criteria, and reviews require a high effort. This paper presents an approach for an evaluation of architectural models during design decisions, for early feedback and as part of architectural assessments. As the quality criteria for evolvability, model relations in terms of traceability links between feature model, design and implementation are evaluated. Indicators are introduced to assess these model relations, similar to metrics, but accompanied by problem resolution actions. The indicators are defined formally to enable a tool-based evaluation. The approach has been developed within a large software project for an IT infrastructure.},
booktitle = {Proceedings of the 23rd IEEE/ACM International Conference on Automated Software Engineering},
pages = {III–72–III–81},
location = {L'Aquila, Italy},
series = {ASE'08}
}

@inproceedings{10.1145/1988676.1988682,
author = {Nowak, Marcin and Pautasso, Cesare},
title = {Goals, questions and metrics for architectural decision models},
year = {2011},
isbn = {9781450305969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1988676.1988682},
doi = {10.1145/1988676.1988682},
abstract = {Architectural decisions are the key element behind the design process leading to a software architecture. Making software architects aware of the implications of their decisions is only the beginning of what can be achieved by capturing the rationale and the constraints influencing the decision making process in a reusable body of architectural knowledge. In this paper we propose a metric-based approach to the analysis of architectural decision models. Using a hierarchically-structured approach we identify a number of useful goals and stakeholders involved in the architectural design process. Next, we sketch a set of metrics to provide data for the evaluation of the aforementioned goals. Our aim is to stimulate a discussion on how to find indicators relevant for software architects by measuring the intrinsic properties of architectural knowledge.},
booktitle = {Proceedings of the 6th International Workshop on SHAring and Reusing Architectural Knowledge},
pages = {21–28},
numpages = {8},
keywords = {visualization, software architecture, architectural decision modeling},
location = {Waikiki, Honolulu, HI, USA},
series = {SHARK '11}
}

@article{10.1016/j.jss.2010.06.043,
author = {Unphon, Hataichanok and Dittrich, Yvonne},
title = {Software architecture awareness in long-term software product evolution},
year = {2010},
issue_date = {November, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {11},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.06.043},
doi = {10.1016/j.jss.2010.06.043},
abstract = {Software architecture has been established in software engineering for almost 40 years. When developing and evolving software products, architecture is expected to be even more relevant compared to contract development. However, the research results seem not to have influenced the development practice around software products very much. The architecture often only exists implicitly in discussions that accompany the development. Nonetheless many of the software products have been used for over 10, or even 20 years. How do development teams manage to accommodate changing needs and at the same time maintain the quality of the product? In order to answer this question, grounded theory study based on 15 semi-structured interviews was conducted in order to find out about the wide spectrum of architecture practices in software product developing organisations. Our results indicate that a chief architect or central developer acts as a 'walking architecture' devising changes and discussing local designs while at the same time updating his own knowledge about problematic aspects that need to be addressed. Architecture documentation and representations might not be used, especially if they replace the feedback from on-going developments into the 'architecturing' practices. Referring to results from Computer Supported Cooperative Work, we discuss how explicating the existing architecture needs to be complemented by social protocols to support the communication and knowledge sharing processes of the 'walking architecture'.},
journal = {J. Syst. Softw.},
month = nov,
pages = {2211–2226},
numpages = {16},
keywords = {Software products, Software architecture, Qualitative empirical studies, Long-term evolution, Cooperative and human aspects, Architecture knowledge management}
}

@inproceedings{10.5555/2362793.2362812,
author = {Borders, Kevin and Springer, Jonathan and Burnside, Matthew},
title = {Chimera: a declarative language for streaming network traffic analysis},
year = {2012},
publisher = {USENIX Association},
address = {USA},
abstract = {Intrusion detection systems play a vital role in network security. Central to these systems is the language used to express policies. Ideally, this language should be powerful, implementation-agnostic, and cross-platform. Unfortunately, today's popular intrusion detection systems fall short of this goal. Each has their own policy language in which expressing complicated logic requires implementation-specific code. Database systems have adapted SQL to handle streaming data, but have yet to achieve the efficiency and flexibility required for complex intrusion detection tasks.In this paper, we introduce Chimera, a declarative query language for network traffic processing that bridges the gap between powerful intrusion detection systems and a simple, platform-independent SQL syntax. Chimera extends streaming SQL languages to better handle network traffic by adding structured data types, first-class functions, and dynamic window boundaries. We show how these constructs can be applied to real-world scenarios, such as side-jacking detection and DNS feature extraction. Finally, we describe the implementation and evaluation of a compiler that translates Chimera queries into low-level code for the Bro event language.},
booktitle = {Proceedings of the 21st USENIX Conference on Security Symposium},
pages = {19},
numpages = {1},
location = {Bellevue, WA},
series = {Security'12}
}

@inproceedings{10.5555/2394450.2394490,
author = {Her, Jin Sun and Choi, Si Won and Cheun, Du Wan and Bae, Jeong Seop and Kim, Soo Dong},
title = {A component-based process for developing automotive ECU software},
year = {2007},
isbn = {3540734597},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software plays a vital role in operating modern automobiles, and it is a key element in providing innovative features such as Collision Prevention System. There are two essential issues to be resolved; managing software complexity, and reducing software cost and time-to-market. A key solution to these two issues is to maximize reusing components in building various Electronic Control Units (ECUs). Component-based development (CBD) is regarded as an effective reuse technology. However, current CBD methodologies do not effectively support developing reusable automotive components and ECUs. Hence, in this paper, we first define variability types and variation points for ECUs. Based on the variability types, we propose a component-based development process for developing ECUs. To assess the applicability of the proposed CBD process, we present the case study of developing an innovative automotive ECU for Automatic Parking System (APS).},
booktitle = {Proceedings of the 8th International Conference on Product-Focused Software Process Improvement},
pages = {358–373},
numpages = {16},
location = {Riga, Latvia},
series = {PROFES'07}
}

@inbook{10.5555/1980562.1980563,
author = {Kienzle, J\"{o}rg and Guelfi, Nicolas and Mustafiz, Sadaf},
title = {Crisis management systems: a case study for aspect-oriented modeling},
year = {2010},
isbn = {3642160859},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The intent of this document is to define a common case study for the aspect-oriented modeling research community. The domain of the case study is crisis management systems, i.e., systems that help in identifying, assessing, and handling a crisis situation by orchestrating the communication between all parties involved in handling the crisis, by allocating and managing resources, and by providing access to relevant crisis-related information to authorized users. This document contains informal requirements of crisis management systems (CMSs) in general, a feature model for a CMS product line, use case models for a car crash CMS (CCCMS), a domain model for the CCCMS, an informal physical architecture description of the CCCMS, as well as some design models of a possible object-oriented implementation of parts of the CCCMS backend. AOM researchers who want to demonstrate the power of their AOM approach or technique can hence apply the approach at the most appropriate level of abstraction.},
booktitle = {Transactions on Aspect-Oriented Software Development VII: A Common Case Study for Aspect-Oriented Modeling},
pages = {1–22},
numpages = {22}
}

@article{10.1109/TASLP.2018.2860786,
author = {Salehi, Haniyeh and Suelzle, David and Folkeard, Paula and Parsa, Vijay},
title = {Learning-Based Reference-Free Speech Quality Measures for Hearing Aid Applications},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2860786},
doi = {10.1109/TASLP.2018.2860786},
abstract = {Objective measures of speech quality are highly desirable in benchmarking and monitoring the performance of hearing aids HAs. Existing HA speech quality indices such as the hearing aid speech quality index HASQI are intrusive in that they require a properly time-aligned and frequency-shaped reference signal to predict the quality of HA output. Two new reference-free HA speech quality indices are proposed in this paper, based on a model that amalgamates perceptual linear prediction PLP, hearing loss HL modeling, and machine learning concepts. For the first index, HL-modified PLP coefficients and their statistics were used as the feature set, which was subsequently mapped to the predicted quality scores using support vector regression SVR. For the second index, HL-impacted gammatone auditory filterbank energies and their second-order statistics constituted the feature set, which was again mapped using SVR. Two databases involving HA recordings were collected and utilized for the evaluation of the robustness and generalizability of the two indices. Experimental results showed that the index based on the gammatone filterbank energies not only correlated well with HA quality ratings by hearing impaired listeners, but also exhibited robust performance across different test conditions and was comparable to the full-reference HASQI performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2277–2288},
numpages = {12}
}

@inbook{10.5555/1986548.1986549,
author = {Kienzle, J\"{o}rg and Guelfi, Nicolas and Mustafiz, Sadaf},
title = {Crisis management systems: a case study for aspect-oriented modeling},
year = {2010},
isbn = {3642160859},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The intent of this document is to define a common case study for the aspect-oriented modeling research community. The domain of the case study is crisis management systems, i.e., systems that help in identifying, assessing, and handling a crisis situation by orchestrating the communication between all parties involved in handling the crisis, by allocating and managing resources, and by providing access to relevant crisis-related information to authorized users. This document contains informal requirements of crisis management systems (CMSs) in general, a feature model for a CMS product line, use case models for a car crash CMS (CCCMS), a domain model for the CCCMS, an informal physical architecture description of the CCCMS, as well as some design models of a possible object-oriented implementation of parts of the CCCMS backend. AOM researchers who want to demonstrate the power of their AOM approach or technique can hence apply the approach at the most appropriate level of abstraction.},
booktitle = {Transactions on Aspect-Oriented Software Development VII: A Common Case Study for Aspect-Oriented Modeling},
pages = {1–22},
numpages = {22}
}

@article{10.1016/j.advengsoft.2009.10.005,
author = {Contreras, Felipe and Hitschfeld-Kahler, Nancy and Bastarrica, Mar\i{}a Cecilia and Lillo, Carlos},
title = {Balancing flexibility and performance in three dimensional meshing tools},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {41},
number = {3},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2009.10.005},
doi = {10.1016/j.advengsoft.2009.10.005},
abstract = {It is generally thought within the meshing tool community that object-orientation and other decoupling techniques penalize performance when they are used for building concrete meshing tools. In this paper we show that building a meshing tool with good object-oriented design metrics could not only improve maintainability and all other derived attributes such as portability and extensibility, but also its performance is comparable to a standard meshing tool that implements the same algorithms.},
journal = {Adv. Eng. Softw.},
month = mar,
pages = {471–479},
numpages = {9},
keywords = {Software architecture, Object-oriented design, Meshing tools}
}

@article{10.1016/j.jss.2007.08.025,
author = {Jansen, Anton and Bosch, Jan and Avgeriou, Paris},
title = {Documenting after the fact: Recovering architectural design decisions},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.08.025},
doi = {10.1016/j.jss.2007.08.025},
abstract = {Software architecture documentation helps people in understanding the software architecture of a system. In practice, software architectures are often documented after the fact, i.e. they are maintained or created after most of the design decisions have been made and implemented. To keep the architecture documentation up-to-date an architect needs to recover and describe these decisions. This paper presents ADDRA, an approach an architect can use for recovering architectural design decisions after the fact. ADDRA uses architectural deltas to provide the architect with clues about these design decisions. This allows the architect to systematically recover and document relevant architectural design decisions. The recovered architectural design decisions improve the documentation of the architecture, which increases traceability, communication, and general understanding of a system.},
journal = {J. Syst. Softw.},
month = apr,
pages = {536–557},
numpages = {22},
keywords = {Software architecture recovery, Architectural design decisions}
}

@inproceedings{10.1007/11763864_27,
author = {Kolb, Ronny and Ganesan, Dharmalingam and Muthig, Dirk and Kagino, Masanori and Teranishi, Hideharu},
title = {Goal-Oriented performance analysis of reusable software components},
year = {2006},
isbn = {3540346066},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11763864_27},
doi = {10.1007/11763864_27},
abstract = {To establish software reuse successfully in the long run, it is crucial for providers of reusable components to continuously react on problems or future trends arising around their component. In practice, however, many providers of reusable components are not able to do so due to insufficient feedback and details from reusers. Additionally, they often have too little knowledge on system context and constraints that may lead to major deficits of the reusable component especially with respect to non-functional aspects. This paper presents an approach for systematically engineering performance of reusable components that has been validated in an industrial context.},
booktitle = {Proceedings of the 9th International Conference on Reuse of Off-the-Shelf Components},
pages = {368–381},
numpages = {14},
location = {Turin, Italy},
series = {ICSR'06}
}

@inproceedings{10.1145/2897053.2897062,
author = {McGee, Ethan T. and McGregor, John D.},
title = {Using dynamic adaptive systems in safety-critical domains},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897062},
doi = {10.1145/2897053.2897062},
abstract = {The development of safety-critical Cyber-Physical Systems (CPS) is expanding due to the Internet of Things' promise to make high-integrity applications and services part of everyday life. This expansion is seen in the dependencies some connected vehicles have on cloud services that provide guidance and accident avoidance / detection features. Such systems are safety-critical since failure could result in serious injury or death. Due to the severe consequences of failure, fault-tolerance, reliability and dependability should be primary driving qualities in the design and development of these systems. However, the cost of the analysis, evaluation and certification activities needed to ensure that the possibility of failure has been sufficiently mitigated is significantly higher than the cost of developing traditional software.Our group is exploring the addition of dynamic adaptive capabilities to safety-critical systems. We postulate that dynamic adaptivity could provide several enhancements to safety-critical systems. It would allow systems to reason about the environment within which they are sited and about their internal operation enabling decision making that is context-specific and appropriately prioritized. However, the addition of adaptivity with the associated overhead of reasoning is not without drawbacks particularly when hard real-time safety-critical systems are involved. In this brief position paper, we explore some of the questions and concerns that are raised when dynamic adaptive behavior is introduced into safety-critical systems as well as ways that the Architecture Analysis &amp; Design Language (AADL) can be used to model / analyze such systems.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {115–121},
numpages = {7},
keywords = {software product lines, safety critical systems, dynamic software product lines, dynamic adaptive systems},
location = {Austin, Texas},
series = {SEAMS '16}
}

@inproceedings{10.5555/244522.244552,
author = {Cong, Jason and He, Lei},
title = {An efficient approach to simultaneous transistor and interconnect sizing},
year = {1997},
isbn = {0818675977},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we study the simultaneous transistor and interconnect sizing (STIS) problem. We define a class of optimization problems as CH-posynomial programs and reveal a general dominance property for all CH-posynomial programs. We show that the STIS problems under a number of transistor delay models are CH-posynomial programs and propose an efficient and near-optimal STIS algorithm based on the dominance property. When used to solve the simultaneous driver/buffer and wire sizing problem for real designs, it reduces the maximum delay by up to 16.1%, and more significantly, reduces the power consumption by a factor of 1.63X, when compared with the original designs. When used to solve the transistor sizing problem, it achieves a smooth area-delay trade-off. Moreover, the algorithm optimizes a clock net of 367 drivers/buffers and 59304 /spl mu/m-long wire in 120 seconds, and a 32-bit adder with 1026 transistors in 66 seconds on a SPARC-5 workstation.},
booktitle = {Proceedings of the 1996 IEEE/ACM International Conference on Computer-Aided Design},
pages = {181–186},
numpages = {6},
keywords = {wire sizing problem, transistor sizing, transistor and interconnect sizing, driver/buffer, circuit CAD, STIS, CH-posynomial programs},
location = {San Jose, California, USA},
series = {ICCAD '96}
}

@article{10.1016/j.future.2014.11.010,
author = {Lytra, Ioanna and Tran, Huy and Zdun, Uwe},
title = {Harmonizing architectural decisions with component view models using reusable architectural knowledge transformations and constraints},
year = {2015},
issue_date = {June 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2014.11.010},
doi = {10.1016/j.future.2014.11.010},
abstract = {Architectural design decisions (ADDs) have been used in recent years for capturing design rationale and documenting architectural knowledge (AK). However, various architectural design views still provide the most common means for describing and communicating architectural design. The evolution of software systems requires that both ADDs and architectural design views are documented and maintained, which is a tedious and time-consuming task in the long run. Also, in lack of a systematic and automated support for bridging between ADDs and architectural design views, decisions and designs tend to become inconsistent over time. In our proposal, we introduce a reusable AK transformation language for supporting the automated transformation of reusable AK knowledge to component-and-connector models, the architectural design view used most commonly today. In addition, reusable consistency checking rules verify the consistency between decisions and designs. We evaluate our approach in an industrial case study and show that it offers high reusability, provides automation, and can, in principle, deal with large numbers of recurring decisions. Reusable AK transformation language for automated transformation of ADDs into designs.Reusable constraints for consistency checking between decisions and designs.Tool support based on integration of two existing tools-ADvISE and VbMF.},
journal = {Future Gener. Comput. Syst.},
month = jun,
pages = {80–96},
numpages = {17},
keywords = {Consistency checking, Architectural knowledge, Architectural design, Architectural decisions, AK transformation language}
}

@article{10.1145/1754405.1754407,
author = {Ahmed, Waseem and Myers, Douglas},
title = {Concept-based partitioning for large multidomain multifunctional embedded systems},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1084-4309},
url = {https://doi.org/10.1145/1754405.1754407},
doi = {10.1145/1754405.1754407},
abstract = {Hardware-software partitioning is an important phase in embedded systems. Decisions made during this phase impact the quality, cost, performance, and the delivery date of the final product. Over the past decade or more, various partitioning approaches have been proposed. A majority operate at a relatively fine granularity and use a low-level executable specification as the starting point. This presents problems if the context is families of industrial products with frequent release of upgraded or new members. Managing complexity using a low-level specification is extremely challenging and impacts developer productivity. Designing using a high-level specification and component-based development, although a better option, imposes component integration and replacement problems during system evolution and new product release. A new approach termed Concept-Based Partitioning is presented that focuses on system evolution, product lines, and large-scale reuse when partitioning. Beginning with information from UML 2.0 sequence diagrams and a concept repository concepts are identified and used as the unit of partitioning within a specification. A methodology for the refinement of interpart communication in the system specification using sequence diagrams is also presented. Change localization during system evolution, composability during large-scale reuse, and provision for configurable feature variations for a product line are facilitated by a Generic Adaptive Layer (GAL) around selected concepts. The methodology was applied on a subsystem of an Unmanned Aerial Vehicle (UAV) using various concepts which improved the composability of concepts while keeping performance and size overhead within the 2% range.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {22},
numpages = {41},
keywords = {system partitioning, system evolution, product families, embedded system design, UML, Codesign}
}

@article{10.1016/j.infsof.2010.05.003,
author = {Ali, Muhammad Sarmad and Ali Babar, Muhammad and Chen, Lianping and Stol, Klaas-Jan},
title = {A systematic review of comparative evidence of aspect-oriented programming},
year = {2010},
issue_date = {September, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.05.003},
doi = {10.1016/j.infsof.2010.05.003},
abstract = {Context: Aspect-oriented programming (AOP) promises to improve many facets of software quality by providing better modularization and separation of concerns, which may have system wide affect. There have been numerous claims in favor and against AOP compared with traditional programming languages such as Objective Oriented and Structured Programming Languages. However, there has been no attempt to systematically review and report the available evidence in the literature to support the claims made in favor or against AOP compared with non-AOP approaches. Objective: This research aimed to systematically identify, analyze, and report the evidence published in the literature to support the claims made in favor or against AOP compared with non-AOP approaches. Method: We performed a systematic literature review of empirical studies of AOP based development, published in major software engineering journals and conference proceedings. Results: Our search strategy identified 3307 papers, of which 22 were identified as reporting empirical studies comparing AOP with non-AOP approaches. Based on the analysis of the data extracted from those 22 papers, our findings show that for performance, code size, modularity, and evolution related characteristics, a majority of the studies reported positive effects, a few studies reported insignificant effects, and no study reported negative effects; however, for cognition and language mechanism, negative effects were reported. Conclusion: AOP is likely to have positive effect on performance, code size, modularity, and evolution. However its effect on cognition and language mechanism is less likely to be positive. Care should be taken using AOP outside the context in which it has been validated.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {871–887},
numpages = {17},
keywords = {Systematic literature review, Evidence-based software engineering, Aspect-oriented programming}
}

@inproceedings{10.1145/1404520.1404523,
author = {Bennedssen, Jens and Caspersen, Michael E.},
title = {Abstraction ability as an indicator of success for learning computing science?},
year = {2008},
isbn = {9781605582160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1404520.1404523},
doi = {10.1145/1404520.1404523},
abstract = {Computing scientists generally agree that abstract thinking is a crucial component for practicing computer science.We report on a three-year longitudinal study to confirm the hypothesis that general abstraction ability has a positive impact on performance in computing science.Abstraction ability is operationalized as stages of cognitive development for which validated tests exist. Performance in computing science is operationalized as grade in the final assessment of ten courses of a bachelor's degree programme in computing science. The validity of the operationalizations is discussed.We have investigated the positive impact overall, for two groupings of courses (a content-based grouping and a grouping based on SOLO levels of the courses' intended learning outcome), and for each individual course.Surprisingly, our study shows that there is hardly any correlation between stage of cognitive development (abstraction ability) and final grades in standard CS courses, neither for the various group-ings, nor for the individual courses. Possible explanations are discussed.},
booktitle = {Proceedings of the Fourth International Workshop on Computing Education Research},
pages = {15–26},
numpages = {12},
keywords = {success, learning, indicator, computer science, abstraction, CS},
location = {Sydney, Australia},
series = {ICER '08}
}

@inproceedings{10.1145/974044.974052,
author = {Grassi, Vincenzo and Mirandola, Raffaela},
title = {Towards automatic compositional performance analysis of component-based systems},
year = {2004},
isbn = {1581136730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/974044.974052},
doi = {10.1145/974044.974052},
abstract = {To make predictive analysis an effective tool for component-based software development (CBSD), it should be, as much as possible: compositional, to allow the re-use of known information about the properties of existing components, and automatic, to keep the pace with the timeliness and cost-effectiveness promises of CBSD. Towards this end, focusing on the predictive analysis of performance properties, we define a simple language, based on an abstract component model, to describe a component assembly, outlining which information should be included in it to support compositional performance analysis. Moreover, we outline a mapping of the constructs of the proposed language to elements of the RT-UML Profile, to give them a precisely defined "performance semantics", and to get a starting point for the exploitation of proposed UML-based methodologies and algorithms for performance analysis.},
booktitle = {Proceedings of the 4th International Workshop on Software and Performance},
pages = {59–63},
numpages = {5},
keywords = {software component, predictive analysis, performance, component specification},
location = {Redwood Shores, California},
series = {WOSP '04}
}

@article{10.1007/s10270-016-0575-4,
author = {Voelter, Markus and Kolb, Bernd and Szab\'{o}, Tam\'{a}s and Ratiu, Daniel and Deursen, Arie},
title = {Lessons learned from developing mbeddr: a case study in language engineering with MPS},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0575-4},
doi = {10.1007/s10270-016-0575-4},
abstract = {Language workbenches are touted as a promising technology to engineer languages for use in a wide range of domains, from programming to science to business. However, not many real-world case studies exist that evaluate the suitability of language workbench technology for this task. This paper contains such a case study. In particular, we evaluate the development of mbeddr, a collection of integrated languages and language extensions built with the Jetbrains MPS language workbench. mbeddr consists of 81 languages, with their IDE support, 34 of them C extensions. The mbeddr languages use a wide variety of notations--textual, tabular, symbolic and graphical--and the C extensions are modular; new extensions can be added without changing the existing implementation of C. mbeddr's development has spanned 10 person-years so far, and the tool is used in practice and continues to be developed. This makes mbeddr a meaningful case study of non-trivial size and complexity. The evaluation is centered around five research questions: language modularity, notational freedom and projectional editing, mechanisms for managing complexity, performance and scalability issues and the consequences for the development process. We draw generally positive conclusions; language engineering with MPS is ready for real-world use. However, we also identify a number of areas for improvement in the state of the art in language engineering in general, and in MPS in particular.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {585–630},
numpages = {46},
keywords = {Languages, Language workbenches, Language extension, Language engineering, Experimentation, Domain-specific language, Case study}
}

@inproceedings{10.5555/645882.672392,
author = {Ommering, Rob C. van and Bosch, Jan},
title = {Widening the Scope of Software Product Lines - From Variation to Composition},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Architecture, components and reuse form the key elements to build a large variety of complex, high-quality products with a short lead-time. But the balance between an architecture-driven and a component-driven approach is influenced by the scope of the product line and the characteristics of the development organization. This paper discusses that balance and claims that a paradigm shift from variation to composition is necessary to cope with an increasing diversity of products created by an ever-larger part of an organization. We illustrate our claim with various examples.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {328–347},
numpages = {20},
keywords = {variation, software components, product population, composition},
series = {SPLC 2}
}

@article{10.1287/msom.2020.0869,
author = {Hu, Ming and Liu, Jingchen and Zhai, Xin},
title = {Intertemporal Segmentation via Flexible-Duration Group Buying},
year = {2021},
issue_date = {September–October 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {23},
number = {5},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2020.0869},
doi = {10.1287/msom.2020.0869},
abstract = {Problem definition
: We study a special form of group buying: the group buying succeeds only if the number of sign-ups reaches a preset threshold, with no duration constraint. Customers with heterogeneous valuations arrive sequentially and decide between signing up for the group buying or purchasing a regular product. To decide whether to join the group buying, customers need to estimate their expected waiting time, which varies depending on the cumulative sign-ups by the time of their arrival. The firm decides on the prices for the group-buying product and regular product, with the product quality levels and group-buying size exogenously determined. 
Academic/practical relevance
: This type of group buying is often adopted for a special edition of the product and offered alongside a constantly available regular product. 
Methodology
: We study the product line design with the group-buying sign-up behavior of customers characterized by the rational expectations equilibrium in a random pledging process. 
Results
: We show that group buying with flexible duration can result in intertemporal customer segmentation, as different segments might be admitted at different times in the dynamic sign-up process. Such intertemporal segmentation is a natural discrimination scheme and has nontrivial implications. First, the efficiency loss due to waiting for enough sign-ups may decrease when a larger batch size is required for economic production. Second, as valuation heterogeneity in the market increases, the firm may not always benefit from offering group buying along with the regular product. Third, group buying can achieve a win-win-win situation for both high-end and low-end customers as well as the firm. 
Managerial implications
: In addition to demonstrating the profitability of flexible-duration group buying, we show that the firm can strengthen its profitability by contingently setting prices or concealing sign-up information in group buying. We also confirm the robustness of our main insights by considering customers’ heterogeneous patience levels and horizontally differentiated products, among other factors.},
journal = {Manufacturing &amp; Service Operations Management},
month = sep,
pages = {1157–1174},
numpages = {18},
keywords = {price discrimination, intertemporal segmentation, flexible duration, group buying}
}

@inproceedings{10.1145/2212736.2212739,
author = {Takeuchi, Mikio and Makino, Yuki and Kawachiya, Kiyokuni and Horii, Hiroshi and Suzumura, Toyotaro and Suganuma, Toshio and Onodera, Tamiya},
title = {Compiling X10 to Java},
year = {2011},
isbn = {9781450307703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212736.2212739},
doi = {10.1145/2212736.2212739},
abstract = {X10 is a new programming language for improving the software productivity in the multicore era by making parallel/distributed programming easier. X10 programs are compiled into C++ or Java source code, but X10 supports various features not supported directly in Java. To implement them efficiently in Java, new compilation techniques are needed.This paper discusses problems in translating X10-specific functions to Java and provides our solutions. By using appropriate implementations, sequential execution performance has been improved by about 5 times making it comparable to native Java. The parallel execution performance has also been improved and the gap from Java Fork/Join performance is about 3 times when run at a single place. Initial evaluation of distributed execution shows good scalability. Most of the results in this paper have already been incorporated in X10 release 2.1.2.Many of the compilation techniques described in this paper can be useful for implementing other programming languages targeted for Java or other managed environments.},
booktitle = {Proceedings of the 2011 ACM SIGPLAN X10 Workshop},
articleno = {3},
numpages = {10},
keywords = {optimization, evaluation, code generation, X10, Java},
location = {San Jose, California},
series = {X10 '11}
}

@article{10.1007/s10270-012-0293-5,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel C.},
title = {Does aspect-oriented modeling help improve the readability of UML state machines?},
year = {2014},
issue_date = {July      2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-012-0293-5},
doi = {10.1007/s10270-012-0293-5},
abstract = {Aspect-oriented modeling (AOM) is a relatively recent and very active field of research, whose application has, however, been limited in practice. AOM is assumed to yield several potential benefits such as enhanced modularization, easier evolution, increased reusability, and improved readability of models, as well as reduced modeling effort. However, credible, solid empirical evidence of such benefits is lacking. We evaluate the "readability" of state machines when modeling crosscutting behavior using AOM and more specifically AspectSM, a recently published UML profile. This profile extends the UML state machine notation with mechanisms to define aspects using state machines. Readability is indirectly measured through defect identification and fixing rates in state machines, and the scores obtained when answering a comprehension questionnaire about the system behavior. With AspectSM, crosscutting behavior is modeled using so-called "aspect state machines". Their readability is compared with that of system state machines directly modeling crosscutting and standard behavior together. An initial controlled experiment and a much larger replication were conducted with trained graduate students, in two different institutions and countries, to achieve the above objective. We use two baselines of comparisons--standard UML state machines without hierarchical features (flat state machines) and standard state machines with hierarchical/concurrent features (hierarchical state machines). The results showed that defect identification and fixing rates are significantly better with AspectSM than with both flat and hierarchical state machines. However, in terms of comprehension scores and inspection effort, no significant difference was observed between any of the approaches. Results of the experiments suggest that one should use, when possible, aspect state machines along with hierarchical and/or concurrent features of UML state machines to model crosscutting behaviors.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {1189–1221},
numpages = {33},
keywords = {UML state machines, Defect identification and fixing, Controlled experiment, Comprehension, Aspect-oriented modeling}
}

@article{10.1016/j.jss.2016.07.031,
author = {Kuhrmann, Marco and Ternit\'{e}, Thomas and Friedrich, Jan and Rausch, Andreas and Broy, Manfred},
title = {Flexible software process lines in practice},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.07.031},
doi = {10.1016/j.jss.2016.07.031},
abstract = {An extension for software process metamodels to support software process lines.We present two concepts partitioned software process and variability operation.We present insights into the practical application and evidence of applicability.We present the practical implementation in the German V-Modell XT standard. Process flexibility and adaptability is a frequently discussed topic in literature, and several approaches propose techniques to improve and optimize software processes for a given organization- or project context. A software process line (SPrL) is an instrument to systematically construct and manage variable software processes, by combining pre-defined and standardized process assets that can be reused, modified, and extended using a well-defined customization approach. Hence, process engineers can ground context-specific process variants in a standardized or domain-specific reference model that can be adapted to the respective context. In this article, we present an approach to construct flexible software process lines and show its practical application in the German V-Modell\'{z}XT. The presented approach emerges from a 10-year research endeavor and was used to enhance the metamodel of the V-Modell\'{z}XT and to allow for improved process variability and lifecycle management. Practical dissemination and complementing empirical research show the suitability of the concept. We therefore contribute a proven approach that is presented as metamodel fragment for reuse and implementation in further process modeling approaches.},
journal = {J. Syst. Softw.},
month = nov,
pages = {49–71},
numpages = {23},
keywords = {V-Modell XT metamodel, Software process metamodel, Software process lines, Software process, Process realisation, Process design}
}

@inproceedings{10.1145/1173706.1173740,
author = {Leavens, Gary T. and Abrial, Jean-Raymond and Batory, Don and Butler, Michael and Coglio, Alessandro and Fisler, Kathi and Hehner, Eric and Jones, Cliff and Miller, Dale and Peyton-Jones, Simon and Sitaraman, Murali and Smith, Douglas R. and Stump, Aaron},
title = {Roadmap for enhanced languages and methods to aid verification},
year = {2006},
isbn = {1595932372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1173706.1173740},
doi = {10.1145/1173706.1173740},
abstract = {This roadmap describes ways that researchers in four areas---specification languages, program generation, correctness by construction, and programming languages---might help further the goal of verified software. It also describes what advances the "verified software" grand challenge might anticipate or demand from work in these areas. That is, the roadmap is intended to help foster collaboration between the grand challenge and these research areas.A common goal for research in these areas is to establish language designs and tool architectures that would allow multiple annotations and tools to be used on a single program. In the long term, researchers could try to unify these annotations and integrate such tools.},
booktitle = {Proceedings of the 5th International Conference on Generative Programming and Component Engineering},
pages = {221–236},
numpages = {16},
keywords = {verified software grand challenge, verification, tools, specification languages, programming languages, program generation, correctness by construction, annotations},
location = {Portland, Oregon, USA},
series = {GPCE '06}
}

@inproceedings{10.5555/787260.787727,
author = {Turgis, S. and Daga, J. M. and Portal, J. M. and Auvergne, D.},
title = {Internal power modelling and minimization in CMOS inverters},
year = {1997},
isbn = {0818677864},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present in this paper an alternative for the internal (short-circuit and overshoot) power dissipation estimation of CMOS structures. Using a first order macro-modelling, we consider submicronic additional effects such as: input slow dependency of short-circuit currents and input-to-output coupling. Considering an equivalent capacitance concept we directly compare the different power components. Validations are presented by comparing simulated values (HSPICE level 6, foundry model 0.7 /spl mu/m) to calculated ones. Application to buffer design enlightens the importance of the internal power component and clearly shows that common sizing alternatives for power and delay minimization can be considered.},
booktitle = {Proceedings of the 1997 European Conference on Design and Test},
pages = {603},
keywords = {short-circuit current, overshoot, minimization, macromodel, internal power modelling, input-to-output coupling, foundry model, equivalent capacitance, buffer design, HSPICE simulation, CMOS logic circuits, CMOS inverter},
series = {EDTC '97}
}

@inproceedings{10.1145/1188966.1188976,
author = {Lapouchnian, Alexei and Yu, Yijun and Liaskos, Sotirios and Mylopoulos, John},
title = {Requirements-driven design of autonomic application software},
year = {2006},
publisher = {IBM Corp.},
address = {USA},
url = {https://doi.org/10.1145/1188966.1188976},
doi = {10.1145/1188966.1188976},
abstract = {Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying the objectives of the system, analyzing alternative ways of how these objectives can be met, and designing a system that supports all or some of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and demonstrates this on an example.},
booktitle = {Proceedings of the 2006 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {7–es},
location = {Toronto, Ontario, Canada},
series = {CASCON '06}
}

@article{10.4018/jossp.2011010101,
author = {Raza, Arif and Capretz, Luiz Fernando and Ahmed, Faheem},
title = {An Empirical Study of Open Source Software Usability: The Industrial Perspective},
year = {2011},
issue_date = {January 2011},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/jossp.2011010101},
doi = {10.4018/jossp.2011010101},
abstract = {Recent years have seen a sharp increase in the use of open source projects by common novice users; Open Source Software OSS is thus no longer a reserved arena for software developers and computer gurus. Although user-centered designs are gaining popularity in OSS, usability is still not considered one of the prime objectives in many design scenarios. This paper analyzes industry users' perception of usability factors, including understandability, learnability, operability, and attractiveness on OSS usability. The research model of this empirical study establishes the relationship between the key usability factors and OSS usability from industrial perspective. In order to conduct the study, a data set of 105 industry users is included. The results of the empirical investigation indicate the significance of the key factors for OSS usability.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {1–16},
numpages = {16},
keywords = {Users, Usability, Open Source Software OSS, Industry, Empirical Study}
}

@article{10.1007/s10515-018-0247-4,
author = {Gonzalez-Fernandez, Yasser and Hamidi, Saeideh and Chen, Stephen and Liaskos, Sotirios},
title = {Efficient elicitation of software configurations using crowd preferences and domain knowledge},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-018-0247-4},
doi = {10.1007/s10515-018-0247-4},
abstract = {As software systems grow in size and complexity, the process of configuring them to meet individual needs becomes more and more challenging. Users, especially those that are new to a system, are faced with an ever increasing number of configuration possibilities, making the task of choosing the right one more and more daunting. However, users are rarely alone in using a software system. Crowds of other users or the designers themselves can provide with examples and rules as to what constitutes a meaningful configuration. We introduce a technique for designing optimal interactive configuration elicitation dialogs, aimed at utilizing crowd and expert information to reduce the amount of manual configuration effort. A repository of existing user configurations supplies us with information about popular ways to complete an existing partial configuration. Designers augment this information with their own constraints. A Markov decision process (MDP) model is then created to encode configuration elicitation dialogs that maximize the automatic configuration decisions based on the crowd and the designers' information. A genetic algorithm is employed to solve the MDP when problem sizes prevent use of common exact techniques. In our evaluation with various configuration models we show that the technique is feasible, saves configuration effort and scales for real problem sizes of a few hundreds of features.},
journal = {Automated Software Engg.},
month = mar,
pages = {87–123},
numpages = {37},
keywords = {Software customization, Software configuration, Markov decision processes, Genetic algorithms}
}

@article{10.1007/s10515-020-00273-8,
author = {Velez, Miguel and Jamshidi, Pooyan and Sattler, Florian and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {ConfigCrusher: towards white-box performance analysis for configurable systems},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3–4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-020-00273-8},
doi = {10.1007/s10515-020-00273-8},
abstract = {Stakeholders of configurable systems are often interested in knowing how configuration options influence the performance of a system to facilitate, for example, the debugging and optimization processes of these systems. Several black-box approaches can be used to obtain this information, but they either sample a large number of configurations to make accurate predictions or miss important performance-influencing interactions when sampling few configurations. Furthermore, black-box approaches cannot pinpoint the parts of a system that are responsible for performance differences among configurations. This article proposes ConfigCrusher, a white-box performance analysis that inspects the implementation of a system to guide the performance analysis, exploiting several insights of configurable systems in the process. ConfigCrusher employs a static data-flow analysis to identify how configuration options may influence control-flow statements and instruments code regions, corresponding to these statements, to dynamically analyze the influence of configuration options on the regions’ performance. Our evaluation on 10 configurable systems shows the feasibility of our white-box approach to more efficiently build performance-influence models that are similar to or more accurate than current state of the art approaches. Overall, we showcase the benefits of white-box performance analyses and their potential to outperform black-box approaches and provide additional information for analyzing configurable systems.},
journal = {Automated Software Engg.},
month = dec,
pages = {265–300},
numpages = {36},
keywords = {Dynamic analysis, Static analysis, Performance analysis, Configurable systems}
}

@article{10.1007/s10515-012-0117-4,
author = {N\"{o}hrer, Alexander and Egyed, Alexander},
title = {C2O configurator: a tool for guided decision-making},
year = {2013},
issue_date = {June      2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-012-0117-4},
doi = {10.1007/s10515-012-0117-4},
abstract = {Decision models are widely used in software engineering to describe and restrict decision-making (e.g., deriving a product from a product-line). Since decisions are typically interdependent, it is often neither obvious which decisions have the most significant impact nor which decisions might ultimately conflict. Unfortunately, the current state-of-the-art provides little support for dealing with such situations. On the one hand, some conflicts can be avoided by providing more freedom in which order decisions are made (i.e., most important decisions first). On the other hand, conflicts are unavoidable at times, and living with conflicts may be preferable over forcing the user to fix them right away--particularly because fixing conflicts becomes easier as more is known about a user's intentions. This paper introduces the C2O (Configurator 2.0) tool for guided decision-making. The tool allows the user to answer questions in an arbitrary order--with and without the presence of inconsistencies. While giving users those freedoms, it still supports and guides them by (i) rearranging the order of questions according to their potential to minimize user input, (ii) providing guidance to avoid follow-on conflicts, and (iii) supporting users in fixing conflicts at a later time.},
journal = {Automated Software Engg.},
month = jun,
pages = {265–296},
numpages = {32}
}

@article{10.1016/j.jss.2013.02.061,
author = {Anand, Saswat and Burke, Edmund K. and Chen, Tsong Yueh and Clark, John and Cohen, Myra B. and Grieskamp, Wolfgang and Harman, Mark and Harrold, Mary Jean and Mcminn, Phil},
title = {An orchestrated survey of methodologies for automated software test case generation},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {8},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.02.061},
doi = {10.1016/j.jss.2013.02.061},
abstract = {Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment.},
journal = {J. Syst. Softw.},
month = aug,
pages = {1978–2001},
numpages = {24},
keywords = {Test case generation, Test automation, Symbolic execution, Software testing, Search-based software testing, Orchestrated survey, Model-based testing, Combinatorial testing, Adaptive random testing}
}

@inbook{10.5555/2340883.2340909,
author = {Minsky, Naftaly H.},
title = {Decentralized governance of distributed systems via interaction control},
year = {2012},
isbn = {9783642294136},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper introduces an abstract reference model, called &lt;em&gt;interaction control&lt;/em&gt; (IC), for the governance of large and heterogeneous distributed systems. This model goes well beyond conventional access control, along a number of dimensions. In particular, the IC model has the following characteristics: (1) it is inherently decentralized, and thus scalable even for a wide range of stateful policies; (2) it is very general, and not biased toward any particular type of policies; thus providing a significant realization of the age-old principle of &lt;em&gt;separation of policy from mechanism&lt;/em&gt; ; and (3) it enables flexible, composition-free, interoperability between different policies.The IC model, which is an abstraction of a mechanism called law-governed interaction (LGI), has been designed as a minimalist reference model that can be reified into a whole family of potential control mechanisms that may support different types of communication, with different performance requirements and for different application domains.},
booktitle = {Logic Programs, Norms and Action: Essays in Honor of Marek J. Sergot on the Occasion of His 60th Birthday},
pages = {374–400},
numpages = {27}
}

@article{10.1016/j.jss.2009.02.011,
author = {White, Jules and Dougherty, Brian and Schmidt, Douglas C.},
title = {Selecting highly optimal architectural feature sets with Filtered Cartesian Flattening},
year = {2009},
issue_date = {August, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {8},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.02.011},
doi = {10.1016/j.jss.2009.02.011},
abstract = {Feature modeling is a common method used to capture the variability in a configurable application. A key challenge developers face when using a feature model is determining how to select a set of features for a variant that simultaneously satisfy a series of resource constraints. This paper presents an approximation technique for selecting highly optimal feature sets while adhering to resource limits. The paper provides the following contributions to configuring application variants from feature models: (1) we provide a polynomial time approximation algorithm for selecting a highly optimal set of features that adheres to a set of resource constraints, (2) we show how this algorithm can incorporate complex configuration constraints; and (3) we present empirical results showing that the approximation algorithm can be used to derive feature sets that are more than 90%+ optimal.},
journal = {J. Syst. Softw.},
month = aug,
pages = {1268–1284},
numpages = {17},
keywords = {Resource constraints, Optimization, Feature modeling, Approximation algorithm}
}

@inproceedings{10.5555/261693.261706,
author = {Gebotys, Catherine H.},
title = {An efficient model for DSP code generation: performance, code size, estimated energy},
year = {1997},
isbn = {0818679492},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents a model for simultaneous instruction selection, compaction, and register allocation. An arc mapping model along with logical propositions is used to create an optimization model. Code is generated in fast cpu times and is optimized for minimum code size, maximum performance or estimated energy dissipation. Code generated for realistic DSP applications provide performance and code size improvements from 1.09 up to 2.18 times for the TMS320C2x processor compared to previous research and a commercial compiler. In all examples up to 106 instructions are generated in under one cpu minute. This research is important for industry since DSP code can be efficiently generated with constraints on code size, performance, energy dissipation.},
booktitle = {Proceedings of the 10th International Symposium on System Synthesis},
pages = {41–47},
numpages = {7},
location = {Antwerp, Belgium},
series = {ISSS '97}
}

@article{10.1007/s10664-008-9094-4,
author = {Lee, Jihyun and Kang, Sungwon and Kim, Chang-Ki},
title = {Software architecture evaluation methods based on cost benefit analysis and quantitative decision making},
year = {2009},
issue_date = {August    2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9094-4},
doi = {10.1007/s10664-008-9094-4},
abstract = {Since many parts of the architecture evaluation steps of the Cost Benefit Analysis Method (CBAM) depend on the stakeholders' empirical knowledge and intuition, it is very important that such an architecture evaluation method be able to faithfully reflect the knowledge of the experts in determining Architectural Strategy (AS). However, because CBAM requires the stakeholders to make a consensus or vote for collecting data for decision making, it is difficult to accurately reflect the stakeholders' knowledge in the process. In order to overcome this limitation of CBAM, we propose the two new CBAM-based methods for software architecture evaluation, which respectively adopt the Analytic Hierarchy Process (AHP) and the Analytic Network Process (ANP). Since AHP and ANP use pair-wise comparison they are suitable for a cost and benefit analysis technique since its purpose is not to calculate correct values of benefit and cost but to decide AS with highest return on investment. For that, we first define a generic process of CBAM and develop variations from the generic process by applying AHP and ANP to obtain what we call the CBAM+AHP and CBAM+ANP methods. These new methods not only reflect the knowledge of experts more accurately but also reduce misjudgments. A case study comparison of CBAM and the two new methods is conducted using an industry software project. Because the cost benefit analysis process that we present is generic, new cost benefit analysis techniques with capabilities and characteristics different from the three methods we examine here can be derived by adopting various different constituent techniques.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {453–475},
numpages = {23},
keywords = {Software architecture evaluation, CBAM, ANP, AHP}
}

@article{10.1023/B:EMSE.0000027781.18360.9b,
author = {Khoshgoftaar, Taghi M. and Seliya, Naeem},
title = {Comparative Assessment of Software Quality Classification Techniques: An Empirical Case Study},
year = {2004},
issue_date = {September 2004},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {9},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1023/B:EMSE.0000027781.18360.9b},
doi = {10.1023/B:EMSE.0000027781.18360.9b},
abstract = {Software metrics-based quality classification models predict a software module as either fault-prone (fp) or not fault-prone (nfp). Timely application of such models can assist in directing quality improvement efforts to modules that are likely to be fp during operations, thereby cost-effectively utilizing the software quality testing and enhancement resources. Since several classification techniques are available, a relative comparative study of some commonly used classification techniques can be useful to practitioners. We present a comprehensive evaluation of the relative performances of seven classification techniques and/or tools. These include logistic regression, case-based reasoning, classification and regression trees (CART), tree-based classification with S-PLUS, and the Sprint-Sliq, C4.5, and Treedisc algorithms. The use of expected cost of misclassification (ECM), is introduced as a singular unified measure to compare the performances of different software quality classification models. A function of the costs of the Type I (a nfp module misclassified as fp) and Type II (a fp module misclassified as nfp) misclassifications, ECM is computed for different cost ratios. Evaluating software quality classification models in the presence of varying cost ratios is important, because the usefulness of a model is dependent on the system-specific costs of misclassifications. Moreover, models should be compared and preferred for cost ratios that fall within the range of interest for the given system and project domain. Software metrics were collected from four successive releases of a large legacy telecommunications system. A two-way ANOVA randomized-complete block design modeling approach is used, in which the system release is treated as a block, while the modeling method is treated as a factor. It is observed that predictive performances of the models is significantly different across the system releases, implying that in the software engineering domain prediction models are influenced by the characteristics of the data and the system being modeled. Multiple-pairwise comparisons are performed to evaluate the relative performances of the seven models for the cost ratios of interest to the case study. In addition, the performance of the seven classification techniques is also compared with a classification based on lines of code. The comparative approach presented in this paper can also be applied to other software systems.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {229–257},
numpages = {29},
keywords = {logistic regression, expected cost of misclassification, decision trees, case-based reasoning, analysis of variance, Software quality classification}
}

@inproceedings{10.1145/1081706.1081757,
author = {Estublier, Jacky and Vega, German},
title = {Reuse and variability in large software applications},
year = {2005},
isbn = {1595930140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081706.1081757},
doi = {10.1145/1081706.1081757},
abstract = {Reuse has always been a major goal in software engineering, since it promises large gains in productivity, quality and time to market reduction. Practical experience has shown that substantial reuse has only successfully happened in two cases: libraries, where many generic and small components can be found; and product lines, where domains-specific components can be assembled in different ways to produce variations of a given product.In this paper we examine how product lines have successfully achieved reuse of coarse-grained components, and the underlying factors limiting this approach to narrowly scoped domains. We then build on this insight to present an approach, called software federation, which proposes a mechanism to overcome the identified limitations, and therefore makes reuse of coarse-grained components possible over a larger range of applications. Our approach extends and generalizes the product line approach, extending the concepts and mechanisms available to manage variability. The system is in use in different companies, validating the claims made in this paper.},
booktitle = {Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {316–325},
numpages = {10},
keywords = {workflow, variability, reuse, product line, product families, process driven application, model driven software engineering, interoperability, MDA, EAI, COTS, AOP},
location = {Lisbon, Portugal},
series = {ESEC/FSE-13}
}

@article{10.1145/1363102.1363104,
author = {Mohagheghi, Parastoo and Conradi, Reidar},
title = {An empirical investigation of software reuse benefits in a large telecom product},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/1363102.1363104},
doi = {10.1145/1363102.1363104},
abstract = {Background. This article describes a case study on the benefits of software reuse in a large telecom product. The reused components were developed in-house and shared in a product-family approach. Methods. Quantitative data mined from company repositories are combined with other quantitative data and qualitative observations. Results. We observed significantly lower fault density and less modified code between successive releases of the reused components. Reuse and standardization of software architecture and processes allowed easier transfer of development when organizational changes happened. Conclusions. The study adds to the evidence of quality benefits of large-scale reuse programs and explores organizational motivations and outcomes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {13},
numpages = {31},
keywords = {standardization, risks, product family, fault density, Software reuse}
}

@inproceedings{10.5555/1927661.1927724,
author = {Si, Xiaojie and Zhang, Xuyun and Dou, Wanchun},
title = {A novel local optimization method for QoS-aware web service composition},
year = {2010},
isbn = {3642165141},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {QoS-aware web service selection has become a hot-spot research topic in the domain of web service composition. In previous works, the multiple tasks recruited in a composite schema are usually considered of equal importance. However, it is unreasonable for each task to have the absolutely same weight in certain circumstances. Hence, it is a great challenge to mine the weights among different tasks to reflect customers' partial preferences. In view of this challenge, a novel local optimization method is presented in this paper, which is based on a two-hierarchy weight, i.e., weight of task's criteria and weight of tasks. Finally, a case study is demonstrated to validate the feasibility of our proposal.},
booktitle = {Proceedings of the 2010 International Conference on Web Information Systems and Mining},
pages = {402–409},
numpages = {8},
keywords = {weight, service composition, local optimization, QoS},
location = {Sanya, China},
series = {WISM'10}
}

@inproceedings{10.1145/2328909.2328935,
author = {Sobernig, Stefan and Zdun, Uwe},
title = {Inversion-of-control layer},
year = {2010},
isbn = {9781450302593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2328909.2328935},
doi = {10.1145/2328909.2328935},
abstract = {Inversion of control is a common design practise that has been used in various application areas. It gained popularity in the context of object-oriented application frameworks and designs based on abstract classes and interfaces. Recently, dependency injection techniques, especially in the context of lightweight containers such as Spring, have raised the attention for inversion of control again. However, inversion of control has not yet been described in its architectural dimension with a focus on layering architectures, and the pros and cons of the design decision for control inversion. In this paper, we present the inversion-of-control layer pattern which describes the design practise from an architectural point of view, rather than focusing on particular implementation techniques.},
booktitle = {Proceedings of the 15th European Conference on Pattern Languages of Programs},
articleno = {21},
numpages = {22},
keywords = {layers, inversion of control, architectural patterns, application framework},
location = {Irsee, Germany},
series = {EuroPLoP '10}
}

@article{10.1023/A:1016589208824,
author = {Atkinson, Colin and Bunse, Christian and Gro\ss{}, Hans-Gerhard and K\"{u}hne, Thomas},
title = {Towards a General Component Model for Web-Based Applications},
year = {2002},
issue_date = {June 2002},
publisher = {J. C. Baltzer AG, Science Publishers},
address = {USA},
volume = {13},
number = {1–4},
issn = {1022-7091},
url = {https://doi.org/10.1023/A:1016589208824},
doi = {10.1023/A:1016589208824},
abstract = {The cost effective development of web applications is perhaps one of the most challenging areas of software engineering today. Not only are the problems to be solved, and the solution technologies to be used, in web application development among the most rapidly changing in the software industry, but the business pressures of cost, quality and time-to-market are among the most extreme. Web application development therefore has potentially the most to gain from software reuse approaches that can offer a greater return on development time than traditional approaches. However, simply combining ideas from these reuse paradigms and traditional web development technologies in ad-hoc ways will not result in sustainable improvements. In this paper we describe a systematic way of combining the benefits of component-based development and model driven architectures, two important reuse approaches, to support the cost effective development and maintenance of web applications. After first defining a suitably abstract component-model, the paper explains how component architectures can be systematically and rigorously modeled using UML. It then describes a powerful technique, known as stratification, for separating the various cross cutting aspects of a web application such that a suitable platform specific architecture can be traceably generated. Finally, the paper introduces a technique for increasing the trustworthiness of components by giving them the capability to check their deployment environment at run-time.},
journal = {Ann. Softw. Eng.},
month = jun,
pages = {35–69},
numpages = {35}
}

@inproceedings{10.5555/645882.672256,
author = {Smith, Dennis B. and Brien, Liam O' and Bergey, John},
title = {Using the Options Analysis for Reengineering (OAR) Method for Mining Components for a Product Line},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The Options Analysis for Reengineering (OAR) method is a systematic, architecture-centric means for mining existing components for a product line or new software architecture. The method incorporates a set of scalable techniques and exercises to collaboratively analyze existing components, determine viable mining options, and evaluate the most promising options. The OAR method has 5 activities that are followed in a systematic manner to identify components for mining and estimate the cost and risk of changes required to each legacy component to enable its reuse within a new software architecture. The OAR method provides visibility into this highly complex analysis activity. It also provides insights into implicit stakeholder assumptions, constraints, and other major drivers that impact the mining of components. Results from a pilot application of the OAR method are presented in this paper.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {316–327},
numpages = {12},
series = {SPLC 2}
}

@inproceedings{10.1145/568760.568881,
author = {Fresa, A. and Nucera, G. and Peciola, E. and Santucci, G.},
title = {Assessment of software architectures: a case study},
year = {2002},
isbn = {1581135564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/568760.568881},
doi = {10.1145/568760.568881},
abstract = {Producing high quality software is a very hard task. In the last years a big effort has been spent in devising techniques for estimating and/or measuring software properties. This ranges from forecasting, in a very early stage, the cost of software production to measuring several subcharacterics in order to assess the internal and external software quality. The role of predicting vs. measuring is gaining an increas ing relevance. As an example, the recently revised ISO 9126 standard [1] introduces the concept of Estimated (Predicted) Product Quality. It is clear that the sooner estimated figures are available, the better is possible to modify some design choices. Among all the aspects involved in software developing, a central role is played by the chosen software architecture. Estimating the quality characteristics of such architecture is a strategic activity that can drive several following design decision. In this paper we report the experience of an architectural assessment performed in Ericsson Lab Italy. The assessment was performed according to the framework presented by Jan Bosch in [2].},
booktitle = {Proceedings of the 14th International Conference on Software Engineering and Knowledge Engineering},
pages = {699–706},
numpages = {8},
location = {Ischia, Italy},
series = {SEKE '02}
}

@article{10.1016/j.jss.2009.11.709,
author = {Zhang, Pengcheng and Muccini, Henry and Li, Bixin},
title = {A classification and comparison of model checking software architecture techniques},
year = {2010},
issue_date = {May, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.11.709},
doi = {10.1016/j.jss.2009.11.709},
abstract = {Software architecture specifications are used for many different purposes, such as documenting architectural decisions, predicting architectural qualities before the system is implemented, and guiding the design and coding process. In these contexts, assessing the architectural model as early as possible becomes a relevant challenge. Various analysis techniques have been proposed for testing, model checking, and evaluating performance based on architectural models. Among them, model checking is an exhaustive and automatic verification technique, used to verify whether an architectural specification conforms to expected properties. While model checking is being extensively applied to software architectures, little work has been done to comprehensively enumerate and classify these different techniques. The goal of this paper is to investigate the state-of-the-art in model checking software architectures. For this purpose, we first define the main activities in a model checking software architecture process. Then, we define a classification and comparison framework and compare model checking software architecture techniques according to it.},
journal = {J. Syst. Softw.},
month = may,
pages = {723–744},
numpages = {22},
keywords = {Software architecture, Model checking}
}

@article{10.1504/IJCSE.2015.067054,
author = {Modica, Giuseppe Di and Tomarchio, Orazio},
title = {A semantic framework to support resource discovery in future cloud markets},
year = {2015},
issue_date = {January 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {1/2},
issn = {1742-7185},
url = {https://doi.org/10.1504/IJCSE.2015.067054},
doi = {10.1504/IJCSE.2015.067054},
abstract = {The market of cloud resources is currently dominated by proprietary solutions for what concerns resource delivering, pricing models and service level agreements. In the future cloud markets, when cloud standards will get mature and full interoperability among cloud systems will be a reality, the competition challenge among providers will be played on the capability of supplying high and differentiated QoS levels. In this new scenario advanced and flexible mechanisms to support the matchmaking between what providers offer and what customers demand must be devised. Along with an analysis of the current cloud offering in terms of pricing model, SLA negotiation capabilities, service performance levels and cloud application requirements, this work proposes the definition of a semantic model to support the supply-demand matchmaking process in future cloud markets. Leveraging on a semantic description of the cloud resources' features, customers will be able to discover cloud offers that best suit their own business needs. Tests conducted on an implementation prototype proved the viability of the approach.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {1–14},
numpages = {14}
}

@article{10.1007/s10270-013-0393-x,
author = {Fan, Zhiqiang and Yue, Tao and Zhang, Li},
title = {SAMM: an architecture modeling methodology for ship command and control systems},
year = {2016},
issue_date = {February  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-013-0393-x},
doi = {10.1007/s10270-013-0393-x},
abstract = {Ship command and control systems (SCCSs) are composed of large-scale, complex, real-time and software-intensive systems that complete tasks collaboratively. Open architecture has been introduced to design the architecture of SCCSs and has been refined into functional architecture (FA) and technical architecture (TA) to meet architectural requirements such as adapting fast-speed functional and technical changes. Thereby, specifying the architecture of SCCSs, based on FA and TA, becomes a key issue for stakeholders of the domain. In this paper, we propose an architecture modeling methodology (named as SAMM) for describing the architecture of SCCSs. SAMM is derived by following a systematic and generic framework--modeling Goal, domain-specific Conceptual model, architecture Viewpoint, and architecture description Language (GCVL), which guides domain experts to devise domain-specific architecture modeling methodologies of large-scale software-intensive systems. SAMM contains three viewpoints and 22 models, and a UML/SysML-based architecture description language. An industrial application of SAMM, along with the subsequent application of the derived SAMM architecture model (i.e., a deployed SCCS prototype) was conducted to evaluate SAMM. A questionnaire-based survey was also conducted to subjectively evaluate whether SAMM meets the modeling goals and its applicability. Results show that SAMM meets all modeling goals and is easy to apply.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {71–118},
numpages = {48},
keywords = {Viewpoint, UML, SysML, Ship command and control systems, Architecture modeling}
}

@inproceedings{10.1109/ASE.2015.45,
author = {Sarkar, Atri and Guo, Jianmei and Siegmund, Norbert and Apel, Sven and Czarnecki, Krzysztof},
title = {Cost-efficient sampling for performance prediction of configurable systems},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.45},
doi = {10.1109/ASE.2015.45},
abstract = {A key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected. It is usually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predict performance based on small samples of measured variants, but it is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort. In this paper, we adapt two widely-used sampling strategies for performance prediction to the domain of configurable systems and evaluate them in terms of sampling cost, which considers prediction accuracy and measurement effort simultaneously. To generate an initial sample, we introduce a new heuristic based on feature frequencies and compare it to a traditional method based on t-way feature coverage. We conduct experiments on six real-world systems and provide guidelines for stakeholders to predict performance by sampling.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {342–352},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/2095536.2095554,
author = {Siala, Fatma and Lajmi, Soufiene and Ghedira, Khaled},
title = {Multi-agent selection of multiple composite web services based on CBR method and driven by QoS},
year = {2011},
isbn = {9781450307840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2095536.2095554},
doi = {10.1145/2095536.2095554},
abstract = {Many companies aim to use Web services to integrate heterogeneous or remote applications in SOA (Service Oriented Architecture) contexts. Indeed, one of the main assets of service-orientation is a composition to develop higher level services, so-called composite services, by re-using existing services. Since many available Web services provide overlapping or identical functionality, with different Quality of Service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. However, for a composition, we can have different combinations and execution paths. Particularly, a composite service can generate different schemes that give various QoS scores.This paper presents a framework which deals with the selection of composite Web services on the base of Multi-Agents negotiation. The objective of these agents is to find out the best Composite QoS (CQoS) based on Web services availability. This scalable framework supports different combinations and execution paths using CBR technique. The proposed Multi-Agents framework is compared to an existing approach in terms of execution time. Experiments have demonstrated that our framework provide reliable results in comparison with the existing approach.},
booktitle = {Proceedings of the 13th International Conference on Information Integration and Web-Based Applications and Services},
pages = {90–97},
numpages = {8},
keywords = {web service, multi-agent system, execution paths, contract-net protocol, composition, QoS, CBR technique},
location = {Ho Chi Minh City, Vietnam},
series = {iiWAS '11}
}

@inbook{10.5555/1793854.1793863,
author = {Liu, Chunjian Robin and Gibbs, Celina and Coady, Yvonne},
title = {Safe and sound evolution with SONAR: sustainable optimization and navigation with aspects for system-wide reconciliation},
year = {2007},
isbn = {3540770410},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Traditional diagnostic and optimization techniques typically rely on static instrumentation of a small portion of an overall system. Unfortunately, solely static and localized approaches are simply no longer sustainable in the evolution of today's complex and dynamic systems. Sustainable Optimization and Navigation with Aspects for system-wide Reconciliation is a fluid and unified framework that enables stakeholders to explore and adapt meaningful entities that are otherwise spread across predefined abstraction boundaries. Through a combination of Aspect-Oriented Programming, Extensible Markup Language, and management tools such as Java Management Extensions, SONAR can comprehensively coalesce scattered artifacts--enabling evolution to be more inclusive of system-wide considerations by supporting both iterative and interactive practices. We believe this system-wide approach promotes the application of safe and sound principles in system evolution. This paper presents SONAR's model, examples of its concrete manifestation, and an overview of its associated costs and benefits. Case studies demonstrate how SONAR can be used to accurately identify performance bottlenecks and effectively evolve systems by optimizing behaviour, even at runtime.},
booktitle = {Transactions on Aspect-Oriented Software Development IV},
pages = {163–190},
numpages = {28}
}

@article{10.1145/1050849.1057988,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Frontmatter (TOC, Letters, Open Source Software (OSS) Patent Search Engine, Calendar of Events, Workshop and Conference Information)},
year = {2005},
issue_date = {March 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1050849.1057988},
doi = {10.1145/1050849.1057988},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {0},
numpages = {19}
}

@inproceedings{10.1145/218013.218084,
author = {Qin, Hong and Terzopoulos, Demetri},
title = {Dynamic manipulation of triangular B-splines},
year = {1995},
isbn = {0897916727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/218013.218084},
doi = {10.1145/218013.218084},
booktitle = {Proceedings of the Third ACM Symposium on Solid Modeling and Applications},
pages = {351–360},
numpages = {10},
keywords = {user interaction techniques, triangular B-splines, solid modeling, parametric design, finite elements, dynamics, constraint-based design, CAGD},
location = {Salt Lake City, Utah, USA},
series = {SMA '95}
}

@article{10.1016/j.infsof.2021.106693,
author = {Shamsujjoha, Md. and Grundy, John and Li, Li and Khalajzadeh, Hourieh and Lu, Qinghua},
title = {Developing Mobile Applications Via Model Driven Development: A&nbsp;Systematic Literature Review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {140},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106693},
doi = {10.1016/j.infsof.2021.106693},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {24},
keywords = {Tools and Techniques, Mobile App, Model Driven Development, Systematic Literature Review}
}

@inproceedings{10.1145/1985793.1986001,
author = {Dantas, Francisco},
title = {Reuse vs. maintainability: revealing the impact of composition code properties},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1986001},
doi = {10.1145/1985793.1986001},
abstract = {Over the last years, several composition mechanisms have emerged to improve program modularity. Even though these mechanisms widely vary in their notation and semantics, they all promote a shift in the way programs are structured. They promote expressive means to define the composition of two or more reusable modules. However, given the complexity of the composition code, its actual effects on software quality are not well understood. This PhD research aims at investigating the impact of emerging composition mechanisms on the simultaneous satisfaction of software reuse and maintainability. In order to perform this analysis, we intend to define a set of compositiondriven metrics and compare their efficacy with traditional modularity metrics. Finally, we plan to derive guidelines on how to use new composition mechanisms to maximize reuse and stability of software modules.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {1082–1085},
numpages = {4},
keywords = {software stability, software reuse, advanced composition mechanisms},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@article{10.1504/IJIPT.2016.079546,
author = {Balakrishnan, Senthil Murugan and Sangaiah, Arun Kumar},
title = {Aspect-oriented middleware framework for resolving service discovery issues in Internet of Things},
year = {2016},
issue_date = {January 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {2/3},
issn = {1743-8209},
url = {https://doi.org/10.1504/IJIPT.2016.079546},
doi = {10.1504/IJIPT.2016.079546},
abstract = {Internet of Things IoT is a model of future internet and pervasive computing which have its own challenges derived from the internet in terms of scalability, undefined topology and so on. The proposed work aims to resolve the challenges posed by IoT in service discovery functionality. Considering the pervasive and context dependent nature of IoT the planned work bases its development strategy using an aspect oriented software development methodology. The novelty lies in achieving high degree configuration and customisability by selecting subset of middleware functionality depending on the need. The performance is compared with MUSIC pervasive computing middleware and Android built-in configuration. The result reveals 6.5% decrease on average boot up and reconfiguration time for smart phones and 11.6% percentage decrease in Android tablets. In the context of boot up and reconfiguration time the middleware brings out 5% decrease for smart phones and 3% for Android tablets when compared with MUSIC middleware. The middleware shows 6.3% and 4% reduction in execution time of applications on smart phones and tablets when assessed with MUSIC middleware.},
journal = {Int. J. Internet Protoc. Technol.},
month = jan,
pages = {62–78},
numpages = {17},
keywords = {smartphones, service discovery, reconfiguration time, pervasive computing, internet of things, boot up time, aspect-oriented middleware, Spring AOP, IoT, Android tablets}
}

@inproceedings{10.5555/1765571.1765583,
author = {Liu, Shih-Hsi and Bryant, Barrett R. and Auguston, Mikhail and Gray, Jeff and Raje, Rajeev and Tuceryan, Mihran},
title = {A component-based approach for constructing high-confidence distributed real-time and embedded systems},
year = {2005},
isbn = {9783540711551},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In applying Component-Based Software Engineering (CBSE) techniques to the domain of Distributed Real-time and Embedded (DRE) Systems, there are five critical challenges: 1) discovery of relevant components and resources, 2) specification and modeling of components, 3) exploration and elimination of design assembly options, 4) automated generation of heterogeneous component bridges, and 5) validation of context-related embedded systems. To address these challenges, this paper introduces four core techniques to facilitate high-confidence DRE system construction from components: 1) A component and resource discovery technique promotes component searching based on rich and precise descriptions of components and context; 2) A timed colored Petri Net-based modeling toolkit enables design and analysis on DRE systems, as well as reduces unnecessary later work by eliminating infeasible design options; 3) A formal specification language describes all specifications consistently and automatically generates component bridges for seamless system integration; and 4) A grammar-based formalism specifies context behaviors and validates integrated systems using sufficient context-related test cases. The success of these ongoing techniques may not only accelerate the software development pace and reduce unnecessary development cost, but also facilitate high-confidence DRE system construction using different formalisms over the entire software life-cycle.},
booktitle = {Proceedings of the 12th Monterey Conference on Reliable Systems on Unreliable Networked Platforms},
pages = {225–247},
numpages = {23},
location = {Laguna Beach, CA, 2005}
}

@article{10.1007/s10664-011-9174-8,
author = {Wnuk, Krzysztof and H\"{o}st, Martin and Regnell, Bj\"{o}rn},
title = {Replication of an experiment on linguistic tool support for consolidation of requirements from multiple sources},
year = {2012},
issue_date = {June      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-011-9174-8},
doi = {10.1007/s10664-011-9174-8},
abstract = {Large market-driven software companies continuously receive large numbers of requirements and change requests from multiple sources. The task of analyzing those requests against each other and against already analyzed or implemented functionality then recording similarities between them, also called the requirements consolidation task, may be challenging and time consuming. This paper presents a replicated experiment designed to further investigate the linguistic tool support for the requirements consolidation task. In this replication study, 45 subjects, working in pairs on the same set of requirements as in the original study, were assigned to use two methods for the requirements consolidation: (1) lexical similarity and (2) searching and filtering. The results show that the linguistic method used in this experiment is not more efficient in consolidating requirements than the searching and filtering method, which contradicts the findings of the original study. However, we confirm the previous results that the assisted method (lexical similarity) can deliver more correct links and miss fewer links than the manual method (searching and filtering).},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {305–344},
numpages = {40},
keywords = {Requirements engineering, Replication, Linguistic method, Experiment}
}

@article{10.1016/j.infsof.2010.12.001,
author = {Petersen, Kai},
title = {Measuring and predicting software productivity},
year = {2011},
issue_date = {April, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.12.001},
doi = {10.1016/j.infsof.2010.12.001},
abstract = {ContextSoftware productivity measurement is essential in order to control and improve the performance of software development. For example, by identifying role models (e.g. projects, individuals, tasks) when comparing productivity data. The prediction is of relevance to determine whether corrective actions are needed, and to discover which alternative improvement action would yield the best results. ObjectiveIn this study we identify studies for software productivity prediction and measurement. Based on the identified studies we first create a classification scheme and map the studies into the scheme (systematic map). Thereafter, a detailed analysis and synthesis of the studies is conducted. MethodAs a research method for systematically identifying and aggregating the evidence of productivity measurement and prediction approaches systematic mapping and systematic review have been used. ResultsIn total 38 studies have been identified, resulting in a classification scheme for empirical research on software productivity. The mapping allowed to identify the rigor of the evidence with respect to the different productivity approaches. In the detailed analysis the results were tabulated and synthesized to provide recommendations to practitioners. ConclusionRisks with simple ratio-based measurement approaches were shown. In response to the problems data envelopment analysis seems to be a strong approach to capture multivariate productivity measures, and allows to identify reference projects to which inefficient projects should be compared. Regarding simulation no general prediction model can be identified. Simulation and statistical process control are promising methods for software productivity prediction. Overall, further evidence is needed to make stronger claims and recommendations. In particular, the discussion of validity threats should become standard, and models need to be compared with each other.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {317–343},
numpages = {27},
keywords = {Software productivity, Software development, Prediction, Performance, Measurement, Efficiency}
}

@article{10.1016/j.future.2018.09.006,
author = {Munoz, Daniel-Jesus and Montenegro, Jos\'{e} A. and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Energy-aware environments for the development of green applications for cyber–physical systems},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {91},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.006},
doi = {10.1016/j.future.2018.09.006},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {536–554},
numpages = {19},
keywords = {HADAS eco-assistant, Green plugin, Cyber–physical systems, Energy consumption}
}

@article{10.1016/j.scico.2013.03.017,
author = {Hutchinson, John and Whittle, Jon and Rouncefield, Mark},
title = {Model-driven engineering practices in industry},
year = {2014},
issue_date = {September 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {89},
number = {PB},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2013.03.017},
doi = {10.1016/j.scico.2013.03.017},
abstract = {In this article, we attempt to address the relative absence of empirical studies of model driven engineering (MDE) in two different but complementary ways. First, we present an analysis of a large online survey of MDE deployment and experience that provides some rough quantitative measures of MDE practices in industry. Second, we supplement these figures with qualitative data obtained from some semi-structured, in-depth interviews with MDE practitioners, and, in particular, through describing the practices of four commercial organizations as they adopted a model driven engineering approach to their software development practices. Using in-depth semi-structured interviewing, we invited practitioners to reflect on their experiences and selected four to use as exemplars or case studies. In documenting some details of their attempts to deploy model driven practices, we identify a number of factors, in particular the importance of complex organizational, managerial and social factors-as opposed to simple technical factors-that appear to influence the relative success, or failure, of the endeavor. Three of the case study companies describe genuine success in their use of model driven development, but explain that as examples of organizational change management, the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus. We present extensive results from a survey of MDE practices in industry.We present case studies of the adoption of model driven engineering (MDE) by four companies.We identify important factors that can affect the success or failure of MDE use from both the survey and case studies.MDE provides genuine benefits to those companies who use its appropriate contexts.Success/failure appears to be more dependent on organizational factors than technical.},
journal = {Sci. Comput. Program.},
month = sep,
pages = {144–161},
numpages = {18},
keywords = {Model driven engineering, Industry practice, Empirical software engineering}
}

@article{10.1007/s10270-010-0147-y,
author = {Choi, Yunja and Bunse, Christian},
title = {Design verification in model-based μ-controller development using an abstract component},
year = {2011},
issue_date = {February  2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-010-0147-y},
doi = {10.1007/s10270-010-0147-y},
abstract = {Component-based software development is a promising approach for controlling the complexity and quality of software systems. Nevertheless, recent advances in quality control techniques do not seem to keep up with the growing complexity of embedded software; embedded systems often consist of dozens to hundreds of software/hardware components that exhibit complex interaction behavior. Unanticipated quality defects in a component can be a major source of system failure. To address this issue, this paper suggests a design verification approach integrated into the model-driven, component-based development methodology Marmot. The notion of abstract components--the basic building blocks of Marmot--helps to lift the level of abstraction, facilitates high-level reuse, and reduces verification complexity by localizing verification problems between abstract components before refinement and after refinement. This enables the identification of unanticipated design errors in the early stages of development. This work introduces the Marmot methodology, presents a design verification approach in Marmot, and demonstrates its application on the development of a μ-controller-based abstraction of a car mirror control system. An application on TinyOS shows that the approach helps to reuse models as well as their verification results in the development process.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {91–115},
numpages = {25},
keywords = {Model-driven development, Embedded systems, Design verification, Abstract component}
}

@inproceedings{10.1145/336512.336537,
author = {Garlan, David},
title = {Software architecture: a roadmap},
year = {2000},
isbn = {1581132530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336512.336537},
doi = {10.1145/336512.336537},
booktitle = {Proceedings of the Conference on The Future of Software Engineering},
pages = {91–101},
numpages = {11},
keywords = {software engineering, software design, software architecture},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@article{10.1007/s11219-011-9169-0,
author = {Ouedraogo, Moussa and Savola, Reijo M. and Mouratidis, Haralambos and Preston, David and Khadraoui, Djamel and Dubois, Eric},
title = {Taxonomy of quality metrics for assessing assurance of security correctness},
year = {2013},
issue_date = {March     2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9169-0},
doi = {10.1007/s11219-011-9169-0},
abstract = {Assurance is commonly considered as "something said or done to inspire confidence" (Webster dictionary). However, the level of confidence inspired from a statement or an action depends on the quality of its source. Similarly, the assurance that the deployed security mechanisms exhibit an appropriate posture depends on the quality of the verification process adopted. This paper presents a novel taxonomy of quality metrics pertinent for gaining assurance in a security verification process. Inspired by the systems security engineering capability maturity model and the common criteria, we introduce five ordinal quality levels for a verification process aimed at probing the correctness of runtime security mechanisms. In addition, we analyse the mapping between the quality levels and different capability levels of the following verification metrics families: coverage, rigour, depth and independence of verification. The quality taxonomy is part of a framework for the Security Assurance of operational systems. These metrics can also be used for gaining assurance in other areas such as legal and safety compliance. Furthermore, the resulting metrics taxonomy could, by identifying appropriate quality security requirements, assist manufacturers of information technology (IT) in developing their products or systems. Additionally, the taxonomy could also empower consumers in IT security product selection to efficaciously and effectively match their organisational needs, while IT security evaluators can use it as a reference point when forming judgments about the quality of a security product. We demonstrate the applicability of the proposed taxonomy through access control examples.},
journal = {Software Quality Journal},
month = mar,
pages = {67–97},
numpages = {31},
keywords = {Verification quality, Software probe quality, Security verification process, Security Assurance, Metrics, Correctness measurement}
}

@article{10.1016/j.infsof.2010.03.012,
author = {Her, Jin Sun and Yuan, Hao and Kim, Soo Dong},
title = {Traceability-centric model-driven object-oriented engineering},
year = {2010},
issue_date = {August, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.03.012},
doi = {10.1016/j.infsof.2010.03.012},
abstract = {Context: Object-oriented (OO) development method is a popular paradigm in developing target systems. However, the current practices of OO analysis and design (OOAD) and implementation largely rely on human developers' experience and expertise, making it possible less efficient and more error-prone. Hence, there is room for improving the development efficiency while preserving high quality of programs. Objective: Model-driven development (MDD) is a promising approach to developing programs by machine-assisted model transformation, saving human efforts and reducing the possibility of introducing program faults. Hence, it is appealing to apply key disciplines of MDD in developing OO programs. Method: In this paper, we propose a comprehensive framework for applying MDD on OO program engineering in a rigorous and formal fashion. The framework consists of: (1) a hybrid engineering model of human and machine, (2) meta-models of OOAD artifacts, (3) traceability map with trace links, and (4) transformation rules. Results: We identified five platform independent models and two platform specific models, and defined formal representations for them. We identified 16 traceability links and accordingly 16 transformation rules among the eight artifacts. Through the case study, we showed that our work is feasible and applicable. We assessed our work and concluded that our work is sound, complete, and extendable. Our work established the foundation toward automatic generation of OO programs based on the traceability framework. Conclusion: It is concluded that it is essential to identify the OOAD artifacts, traceability links, and transformation rules for automatic generation of OO programs. It is also important to understand the human involvement nature in MDD and to explicitly treat them in the model transformation.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {845–870},
numpages = {26},
keywords = {Transformation, Traceability, Object-orientation, Model-driven}
}

@article{10.1145/1082983.1085124,
title = {Frontmatter (TOC, Letters, Election results, Software Reliability Resources!, Computing Curricula 2004 and the Software Engineering Volume SE2004, Software Reuse Research, ICSE 2005 Forward)},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1085124},
doi = {10.1145/1082983.1085124},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {0},
numpages = {63}
}

@article{10.1145/2379776.2379778,
author = {Bernardi, Simona and Merseguer, Jos\'{e} and Petriu, Dorina C.},
title = {Dependability modeling and analysis of software systems specified with UML},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2379776.2379778},
doi = {10.1145/2379776.2379778},
abstract = {The goal is to survey dependability modeling and analysis of software and systems specified with UML, with focus on reliability, availability, maintainability, and safety (RAMS). From the literature published in the last decade, 33 approaches presented in 43 papers were identified. They are evaluated according to three sets of criteria regarding UML modeling issues, addressed dependability characteristics, and quality assessment of the surveyed approaches. The survey shows that more works are devoted to reliability and safety, fewer to availability and maintainability, and none to integrity. Many methods support early life-cycle phases (from requirements to design). More research is needed for tool development to automate the derivation of analysis models and to give feedback to designers.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {2},
numpages = {48},
keywords = {safety, model transformation, maintainability, dependability analysis, Availability}
}

@article{10.1145/1022494.1022576,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Backmatter (Report abstracts, Paper abstracts, Calendar of Future Events, Call for Participation, Keynotes, Workshops, Tutorials)},
year = {2004},
issue_date = {September 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1022494.1022576},
doi = {10.1145/1022494.1022576},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {42},
numpages = {16}
}

@article{10.1007/s00779-010-0356-y,
author = {L\'{e}zoray, Jean-Baptiste and Segarra, Maria-Teresa and Phung-Khac, An and Th\'{e}paut, Andr\'{e} and Gilliot, Jean-Marie and Beugnard, Antoine},
title = {A design process enabling adaptation in pervasive heterogeneous contexts},
year = {2011},
issue_date = {April     2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {4},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-010-0356-y},
doi = {10.1007/s00779-010-0356-y},
abstract = {In the next decades, the growth in population aging will cause important problems to most industrialized countries. To tackle this issue, Ambient Assistive Living (AAL) systems can reinforce the well-being of elderly people, by providing emergency, autonomy enhancement, and comfort services. These services will postpone the need of a medicalized environment and will allow the elderly to stay longer at home. However, each elderly has specific needs and a deployment environment of such services is likely unique. Furthermore, the needs evolve over time, and so does the deployment environment of the system. In this paper, we propose the use of a model-based development method, the adaptive medium approach, to enable dynamic adaptation of AAL systems. We also propose improvements to make it more suited to the AAL domain, such as considering heterogeneity and a composition model. The paper includes an evaluation of the prototype implementing the approach, and a comparison with related work.},
journal = {Personal Ubiquitous Comput.},
month = apr,
pages = {353–363},
numpages = {11},
keywords = {Model-driven engineering, Heterogeneity, Dynamic adaptation, Adaptive medium approach, AAL}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@article{10.1145/1095430.1095431,
title = {Frontmatter (TOC, Letters, Philosophy of computer science, Interviewers needed, Taking software requirements creation from folklore to analysis, SW components and product lines: from business to systems and technology, Software engineering survey)},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1095430.1095431},
doi = {10.1145/1095430.1095431},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {0},
numpages = {45}
}

@article{10.1016/j.envsoft.2018.03.013,
author = {Cooper, E.S. and Dance, S.L. and Garcia-Pintado, J. and Nichols, N.K. and Smith, P.J.},
title = {Observation impact, domain length and parameter estimation in data assimilation for flood forecasting},
year = {2018},
issue_date = {Jun 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {104},
number = {C},
issn = {1364-8152},
url = {https://doi.org/10.1016/j.envsoft.2018.03.013},
doi = {10.1016/j.envsoft.2018.03.013},
journal = {Environ. Model. Softw.},
month = jun,
pages = {199–214},
numpages = {16},
keywords = {Ensemble Kalman filter, Joint state-parameter estimation, Observation impact, Fluvial flooding, Inundation forecasting, Data assimilation}
}

@article{10.1109/90.251895,
author = {Partridge, Craig and Pink, Stephen},
title = {A faster UDP},
year = {1993},
issue_date = {Aug. 1993},
publisher = {IEEE Press},
volume = {1},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/90.251895},
doi = {10.1109/90.251895},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {429–440},
numpages = {12}
}

@article{10.1177/1094342004041291,
author = {P\"{u}schel, Markus and Moura, Jos\'{e} M. F. and Singer, Bryan and Xiong, Jianxin and Johnson, Jeremy and Padua, David and Veloso, Manuela and Johnson, Robert W.},
title = {Spiral: A Generator for Platform-Adapted Libraries of Signal Processing Algorithms},
year = {2004},
issue_date = {February  2004},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {18},
number = {1},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342004041291},
doi = {10.1177/1094342004041291},
abstract = {SPIRAL is a generator for libraries of fast software implementations of linear signal processing transforms. These libraries are adapted to the computing platform and can be re-optimized as the hardware is upgraded or replaced. This paper describes the main components of SPIRAL: the mathematical framework that concisely describes signal transforms and their fast algorithms; the formula generator that captures at the algorithmic level the degrees of freedom in expressing a particular signal processing transform; the formula translator that encapsulates the compilation degrees of freedom when translating a specific algorithm into an actual code implementation; and, finally, an intelligent search engine that finds within the large space of alternative formulas and implementations the "best" match to the given computing platform. We present empirical data that demonstrate the high performance of SPIRAL generated code.},
journal = {Int. J. High Perform. Comput. Appl.},
month = feb,
pages = {21–45},
numpages = {25},
keywords = {signal transform, signal processing, search, program generation, optimization, domain-specific language, automatic performance tuning, Fourier transform, FFT, DFT}
}

@article{10.1145/1516533.1516538,
author = {Salehie, Mazeiar and Tahvildari, Ladan},
title = {Self-adaptive software: Landscape and research challenges},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/1516533.1516538},
doi = {10.1145/1516533.1516538},
abstract = {Software systems dealing with distributed applications in changing environments normally require human supervision to continue operation in all conditions. These (re-)configuring, troubleshooting, and in general maintenance tasks lead to costly and time-consuming procedures during the operating phase. These problems are primarily due to the open-loop structure often followed in software development. Therefore, there is a high demand for management complexity reduction, management automation, robustness, and achieving all of the desired quality requirements within a reasonable cost and time range during operation. Self-adaptive software is a response to these demands; it is a closed-loop system with a feedback loop aiming to adjust itself to changes during its operation. These changes may stem from the software system's self (internal causes, e.g., failure) or context (external events, e.g., increasing requests from users). Such a system is required to monitor itself and its context, detect significant changes, decide how to react, and act to execute such decisions. These processes depend on adaptation properties (called self-* properties), domain characteristics (context information or models), and preferences of stakeholders. Noting these requirements, it is widely believed that new models and frameworks are needed to design self-adaptive software. This survey article presents a taxonomy, based on concerns of adaptation, that is, how, what, when and where, towards providing a unified view of this emerging area. Moreover, as adaptive systems are encountered in many disciplines, it is imperative to learn from the theories and models developed in these other areas. This survey article presents a landscape of research in self-adaptive software by highlighting relevant disciplines and some prominent research projects. This landscape helps to identify the underlying research gaps and elaborates on the corresponding challenges.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = may,
articleno = {14},
numpages = {42},
keywords = {survey, self-properties, self-adaptive software, research challenges, Adaptation processes}
}

@inproceedings{10.5555/1768961.1768977,
author = {Sato, Danilo and Goldman, Alfredo and Kon, Fabio},
title = {Tracking the evolution of object-oriented quality metrics on agile projects},
year = {2007},
isbn = {9783540731009},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The automated collection of source code metrics can help agile teams to understand the software they are producing, allowing them to adapt their daily practices towards an environment of continuous improvement. This paper describes the evolution of some object-oriented metrics in several agile projects we conducted recently in both academic and governmental environments. We analyze seven different projects, some where agile methods were used since the beginning and others where some agile practices were introduced later. We analyze and compare the evolution of such metrics in these projects and evaluate how the different project context factors have impacted the source code.},
booktitle = {Proceedings of the 8th International Conference on Agile Processes in Software Engineering and Extreme Programming},
pages = {84–92},
numpages = {9},
keywords = {tracking, object-oriented metrics, extreme programming, agile methods},
location = {Como, Italy},
series = {XP'07}
}

@article{10.1023/A:1013303202532,
author = {Port, Dan and Dincel, Ebru},
title = {Experiences Using Domain Specific Techniques within Multimedia Software Engineering},
year = {2001},
issue_date = {December 2001},
publisher = {J. C. Baltzer AG, Science Publishers},
address = {USA},
volume = {12},
number = {1},
issn = {1022-7091},
url = {https://doi.org/10.1023/A:1013303202532},
doi = {10.1023/A:1013303202532},
abstract = {Domain specific techniques take advantage of the commonalities among applications developed within a certain domain. They are known to improve quality and productivity by incorporating domain knowledge and previous project experiences and promote reuse. This paper describes six domain specific software engineering techniques for developing multimedia applications within the digital library domain. We provide examples of each technique from several projects in which they were used, how the techniques are used within general software engineering practice (in particular, MBASE), how the techniques address some of the particular challenges multimedia software engineering, and the positive impacts we have measured resulting from their use within a graduate level software engineering course.},
journal = {Ann. Softw. Eng.},
month = dec,
pages = {11–45},
numpages = {35}
}

@inproceedings{10.1145/319151.319161,
author = {Chiueh, Tzi-cker and Venkitachalam, Ganesh and Pradhan, Prashant},
title = {Integrating segmentation and paging protection for safe, efficient and transparent software extensions},
year = {1999},
isbn = {1581131402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319151.319161},
doi = {10.1145/319151.319161},
abstract = {The trend towards extensible software architectures and component-based software development demands safe, efficient, and easy-to-use extension mechanisms to enforce protection boundaries among software modules residing in the same address space. This paper describes the design, implementation, and evaluation of a novel intra-address space protection mechanism called Palladium, which exploits the segmentation and paging hardware in the Intel X86 architecture and efficiently supports safe kernel-level and user-level extensions in a way that is largely transparent to programmers and existing programming tools. Based on the considerations on ease of extension programming and systems implementation complexity, Palladium uses different approaches to support user-level and kernel-level extension mechanisms. To demonstrate the effectiveness of the Palladium architecture, we built a Web server that exploits the user-level extension mechanism to invoke CGI scripts as local function calls in a safe way, and we constructed a compiled network packet filter that exploits the kernel-level extension mechanism to run packet-filtering binaries safely inside the kernel at native speed. The current Palladium prototype implementation demonstrates that a protected procedure call and return costs 142 CPU cycles on a Pentium 200MHz machine running Linux.},
booktitle = {Proceedings of the Seventeenth ACM Symposium on Operating Systems Principles},
pages = {140–153},
numpages = {14},
location = {Charleston, South Carolina, USA},
series = {SOSP '99}
}

@article{10.1007/s10270-014-0408-2,
author = {Farias, Kleinner and Garcia, Alessandro and Whittle, Jon and Flach Garcia Chavez, Christina and Lucena, Carlos},
title = {Evaluating the effort of composing design models: a controlled experiment},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-014-0408-2},
doi = {10.1007/s10270-014-0408-2},
abstract = {Model composition plays a key role in many tasks in model-centric software development, e.g., evolving UML diagrams to add new features or reconciling models developed in parallel by different software development teams. However, based on our experience in previous empirical studies, one of the main impairments for the widespread adoption of composition techniques is the lack of empirical knowledge about their effects on developers' effort. This problem applies to both existing categories of model composition techniques, i.e., specification-based (e.g., Epsilon) and heuristic-based techniques (e.g., IBM RSA). This paper, therefore, reports on a controlled experiment that investigates the effort of (1) applying both categories of model composition techniques and (2) detecting and resolving inconsistencies in the output composed models. We evaluate the techniques in 144 evolution scenarios, where 2,304 compositions of elements of UML class diagrams were produced. The main results suggest that (1) the employed heuristic-based techniques require less effort to produce the intended model than the chosen specification-based technique, (2) there is no significant difference in the correctness of the output composed models generated by these techniques, and (3) the use of manual heuristics for model composition outperforms their automated counterparts.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {1349–1365},
numpages = {17},
keywords = {Model composition effort, Empirical studies, Effort measurement}
}

@inproceedings{10.5555/1995456.1995798,
author = {Rabe, Markus and Spieckermann, Sven and Wenzel, Sigrid},
title = {Verification and validation activities within a new procedure model for V&amp;V in production and logistics simulation},
year = {2009},
isbn = {9781424457717},
publisher = {Winter Simulation Conference},
abstract = {Verification and Validation (V&amp;V) of simulation models have been strongly investigated in the context of defense applications. Significantly less substantial work can be found for applications in production and logistics, which is surprising when taking into account the massive impact that wrong or inadequate simulation results can have on strategic and investment-related decisions for large production and logistics systems. The authors have, therefore, founded an expert group for this specific topic in the year 2003. The major result of this expert group was the development of a specific procedure model for V&amp;V in the context of simulation for production and logistics, which was documented in a book (in German) and summarized in a paper at the WSC'2008. This paper explains details of the approach, discussing the criteria to be applied when assessing the validity of a model, and providing examples how to conduct V&amp;V with reference to these criteria.},
booktitle = {Winter Simulation Conference},
pages = {2509–2519},
numpages = {11},
location = {Austin, Texas},
series = {WSC '09}
}

@article{10.1561/2500000045,
author = {Ringer, Talia and Palmskog, Karl and Sergey, Ilya and Gligoric, Milos and Tatlock, Zachary},
title = {QED at Large: A Survey of Engineering of Formally Verified Software},
year = {2019},
issue_date = {Sep 2019},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {5},
number = {2–3},
issn = {2325-1107},
url = {https://doi.org/10.1561/2500000045},
doi = {10.1561/2500000045},
abstract = {Development of formal proofs of correctness of programs can
increase actual and perceived reliability and facilitate better understanding
of program specifications and their underlying assumptions.
Tools supporting such development have been available
for over 40 years, but have only recently seen wide practical use.
Projects based on construction of machine-checked formal proofs
are now reaching an unprecedented scale, comparable to large software
projects, which leads to new challenges in proof development
and maintenance. Despite its increasing importance, the field of
proof engineering is seldom considered in its own right; related
theories, techniques, and tools span many fields and venues. This
survey of the literature presents a holistic understanding of proof
engineering for program correctness, covering impact in practice,
foundations, proof automation, proof organization, and practical
proof development.},
journal = {Found. Trends Program. Lang.},
month = sep,
pages = {102–281},
numpages = {183}
}

@inproceedings{10.1145/2479871.2479922,
author = {Bulej, Lubom\'{\i}r and Burea, Tom\'{a}\v{s} and Hork\'{y}, Vojt\v{c}ch and Keznikl, Jaroslav},
title = {Adaptive deployment in ad-hoc systems using emergent component ensembles: vision paper},
year = {2013},
isbn = {9781450316361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479871.2479922},
doi = {10.1145/2479871.2479922},
abstract = {Mobile cloud computing in the context of ad-hoc clouds brings new challenges when offloading computation from mobile devices. The management of application deployment needs to ensure that the offloading provides users with the expected benefits, but it suddenly needs to cope with a highly dynamic environment which lacks a central authority and in which computational nodes appear and disappear.We propose an approach to the management of ad-hoc systems in such dynamic environment using component ensembles that connect mobile devices with more powerful computation nodes. Our approach aims to address the challenges of scalability and robustness of such systems without the need for central authority, relying instead on simple patterns that lead to reasonable adaptation decisions based on limited and imprecise information.},
booktitle = {Proceedings of the 4th ACM/SPEC International Conference on Performance Engineering},
pages = {343–346},
numpages = {4},
keywords = {ensembles, adaptive deployment, ad-hoc cloud},
location = {Prague, Czech Republic},
series = {ICPE '13}
}

@inproceedings{10.1145/224538.224547,
author = {Chiueh, Tzi-cker and Verma, Manish},
title = {A compiler-directed distributed shared memory system},
year = {1995},
isbn = {0897917286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/224538.224547},
doi = {10.1145/224538.224547},
booktitle = {Proceedings of the 9th International Conference on Supercomputing},
pages = {77–86},
numpages = {10},
location = {Barcelona, Spain},
series = {ICS '95}
}

@article{10.5555/2773570.2773724,
author = {Lluch-Lafuente, Alberto and Montanari, Ugo},
title = {Quantitative μ-calculus and CTL Based on Constraint Semirings},
year = {2005},
issue_date = {January 2005},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1571-0661},
abstract = {Model checking and temporal logics are boolean. The answer to the model checking question does a system satisfy a property? is either true or false, and properties expressed in temporal logics are defined over boolean propositions. While this classic approach is enough to specify and verify boolean temporal properties, it does not allow to reason about quantitative aspects of systems. Some quantitative extensions of temporal logics has been already proposed, especially in the context of probabilistic systems. They allow to answer questions like with which probability does a system satisfy a property?We present a generalization of two well-known temporal logics: CTL and the μ-calculus. Both extensions are defined over c-semirings, an algebraic structure that captures many problems and that has been proposed as a general framework for soft constraint satisfaction problems (CSP). Basically, a c-semiring consists of a domain, an additive operation and a multiplicative operation, which satisfy some properties. We present the semantics of the extended logics over transition systems, where a formula is interpreted as a mapping from the set of states to the domain of the c-semiring, and show that the usual connection between CTL and μ-calculus does not hold in general. In addition, we reason about the feasibility of computing the logics and illustrate some applications of our framework, including boolean model checking.},
journal = {Electron. Notes Theor. Comput. Sci.},
month = jan,
pages = {37–59},
numpages = {23},
keywords = {Temporal Logics, Quantitative Model Checking, Constraints, Constraint Semirings}
}

@article{10.1145/270849.270854,
author = {Edwards, Stephen H. and Weide, Bruce W.},
title = {WISR8: 8th annual workshop on software reuse: summary and working group reports},
year = {1997},
issue_date = {Sept. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/270849.270854},
doi = {10.1145/270849.270854},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {17–32},
numpages = {16}
}

@article{10.1016/j.is.2015.06.003,
author = {Alam, Khubaib Amjad and Ahmad, Rodina and Akhunzada, Adnan and Nasir, Mohd Hairul Nizam Md and Khan, Samee U.},
title = {Impact analysis and change propagation in service-oriented enterprises},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {54},
number = {C},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2015.06.003},
doi = {10.1016/j.is.2015.06.003},
abstract = {ContextThe adoption of Service-oriented Architecture (SOA) and Business Process Management (BPM) is fairly recent. The major concern is now shifting towards the maintenance and evolution of service-based business information systems. Moreover, these systems are highly dynamic and frequent changes are anticipated across multiple levels of abstraction. Impact analysis and change propagation are identified as potential research areas in this regard. ObjectiveThe aim of this study is to systematically review extant research on impact analysis and propagation in the BPM and SOA domains. Identifying, categorizing and synthesizing relevant solutions are the main study objectives. MethodThrough careful review and screening, we identified 60 studies relevant to 4 research questions. Two classification schemes served to comprehend and analyze the anatomy of existing solutions. BPM is considered at the business level for business operations and processes, while SOA is considered at the service level as deployment architecture. We focused on both horizontal and vertical impacts of changes across multiple abstraction layers. ResultsImpact analysis solutions were mainly divided into dependency analysis, traceability analysis and history mining. Dependency analysis is the most frequently adopted technique followed by traceability analysis. Further categorization of dependency analysis indicates that graph-based techniques are extensively used, followed by formal dependency modeling. While considering hierarchical coverage, inter-process and inter-service change analyses have received considerable attention from the research community, whereas bottom-up analysis has been the most neglected research area. The majority of change propagation solutions are top-down and semi-automated. ConclusionsThis study concludes with new insight suggestions for future research. Although, the evolution of service-based systems is becoming of grave concern, existing solutions in this field are less mature. Studies on hierarchical change impact are scarce. Complex relationships of services with business processes and semantic dependencies are poorly understood and require more attention from the research community.},
journal = {Inf. Syst.},
month = dec,
pages = {43–73},
numpages = {31},
keywords = {Web service, Systematic literature review (SLR), Semantic annotation, SOC, SOA, MSR, Dependency analysis, Change propagation, CIA, BPM}
}

@article{10.1016/j.jss.2011.05.026,
author = {Pinto, M\'{o}nica and Fuentes, Lidia and Fern\'{a}ndez, Luis},
title = {Deriving detailed design models from an aspect-oriented ADL using MDD},
year = {2012},
issue_date = {March, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.05.026},
doi = {10.1016/j.jss.2011.05.026},
abstract = {Software architects can separate crosscutting concerns more appropriately by using an aspect-oriented ADL, concretely AO-ADL. This paper illustrates how aspect-orientation and model-driven development technologies can be used to enhance the system design phase; by automatically deriving detailed designs that take into account the ''aspects'' identified at the architectural level. Specifically, we have defined model-to-model transformation rules to automatically generate either aspect-oriented or object-oriented UML 2.0 models, closing the gap between ADLs and the notations used at the detailed design phase. By using AO-ADL it is possible to specify separately crosscutting concerns and base functionality. Another advantage of using AO-ADL is that it allows the specification of parameterizable architectures, promoting the definition of architectural templates. AO-ADL, then, enforces the specification of crosscutting concerns as separate architectural templates, which can be later instantiated and integrated with the core functionality of the system being developed. The AO-ADL language and the transformation rules from AO-ADL to UML 2.0 are available throughout the AO-ADL Tool Suite, which can be used to progressively refine and elaborate aspect-oriented software architectures. These refined architectures are the starting point of the detailed design phase. This means that our approach provides support to automatically generate a skeleton of the detailed design that preserves the information about the crosscutting and the non-crosscutting functionalities identified and modelled at the architecture level.},
journal = {J. Syst. Softw.},
month = mar,
pages = {525–545},
numpages = {21},
keywords = {UML 2.0, Theme/UML, Software architectures, Model-driven development, Aspect-oriented software development, ATL, AO-ADL}
}

@inproceedings{10.1145/336512.336546,
author = {Lamsweerde, Axel van},
title = {Formal specification: a roadmap},
year = {2000},
isbn = {1581132530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336512.336546},
doi = {10.1145/336512.336546},
booktitle = {Proceedings of the Conference on The Future of Software Engineering},
pages = {147–159},
numpages = {13},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@techreport{10.5555/886940,
author = {Clemans A., Powell and Brenda M., Sullivan},
title = {Potential Subjective Effectiveness of Active Interior Noise Control in Propeller Airplanes},
year = {2000},
publisher = {NASA Langley Technical Report Server},
abstract = {Active noise control technology offers the potential for weight-efficient aircraft interior noise reduction, particularly for propeller aircraft. However, there is little information on how passengers respond to this type of interior noise control. This paper presents results of two experiments that use sound quality engineering practices to determine the subjective effectiveness of hypothetical active noise control (ANC) systems in a range of propeller aircraft. The two experiments differed by the type of judgments made by the subjects: pair comparisons based on preference in the first and numerical category scaling of noisiness in the second. Although the results of the two experiments were in general agreement that the hypothetical active control measures improved the interior noise environments, the pair comparison method appears to be more sensitive to subtle changes in the characteristics of the sounds which are related to passenger preference. The reductions in subjective response due to the ANC conditions were predicted with reasonable accuracy by reductions in measured loudness level. Inclusion of corrections for the sound quality characteristics of tonality and fluctuation strength in multiple regression models improved the prediction of the ANC effects.}
}

@proceedings{10.1145/2984043,
title = {SPLASH Companion 2016: Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
year = {2016},
isbn = {9781450344371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.5555/2207071.2207083,
author = {Tseng, Chin-Chin},
title = {Typological universals and intrinsic universals on the L2 acquisition of consonant clusters},
year = {2011},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This study is to examine if typological universals built upon primary languages are applicable to interlanguage data in SLA. Implicational universal is considered the classic example of a typological universal by Croft (2003). Thus, the Interlanguage Structural Conformity Hypothesis, which consists of two implicational universals proposed by Eckman (1991), were tested against data from an interlanguage. The interlanguage data reconfirms that syllable structure plays a key role in the Fricative-Stop Prinicple. However, the Fricative-Stop Principle is sensitive to the position which clusters occur in a syllable. This typological universal is only applicable to final consonant clusters only. The test results do not conform with the Resolvability Principle. The Resolvability Principle claims that if a language has a consonantal sequence of length &lt;u&gt;m&lt;/u&gt; in either initial or final position, it also has at least one continuous subsequence of length &lt;u&gt;m-1&lt;/u&gt; in this same position. Taiwanese speakers' interlanguage data show that they can produce a consonantal sequence of 3 [spr-], but fail to produce a consonantal sequence of 2 [bl-], which violates the proposed typological universal. Thus, intrinsic universals are proposed to explain the interlanguage data in this study, i.e. the position that a consonant cluster occurs in a syllable and its articulatory components all contributed to the intrinsic universals.},
booktitle = {ROCLING 2011 Poster Papers},
pages = {333–348},
numpages = {16},
keywords = {typological universals, structural conformity hypothesis, second language acquisition, consonant clusters},
location = {Taipei, China},
series = {ROCLING '11}
}

@inproceedings{10.1145/800008.808038,
author = {Slamecka, Vladimir},
title = {Conference abstracts},
year = {1977},
isbn = {9781450373739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800008.808038},
doi = {10.1145/800008.808038},
abstract = {One problem in computer program testing arises when errors are found and corrected after a portion of the tests have run properly. How can it be shown that a fix to one area of the code does not adversely affect the execution of another area? What is needed is a quantitative method for assuring that new program modifications do not introduce new errors into the code. This model considers the retest philosophy that every program instruction that could possibly be reached and tested from the modified code be retested at least once. The problem is how to determine the minimum number of test cases to be rerun. The process first involves generating the test case dependency matrix and the reachability matrix. Using the test case dependency matrix and the appropriate rows of the reachability matrix, a 0-1 integer program can be specified. The solution of the integer program yields the minimum number of test cases to be rerun, and the coefficients of the objective function identify which specific test cases to rerun.},
booktitle = {Proceedings of the 5th Annual ACM Computer Science Conference},
pages = {1–36},
numpages = {36},
series = {CSC '77}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@article{10.1145/279437.279460,
author = {Thompson, Craig},
title = {Workshop on compositional software architectures: workshop report},
year = {1998},
issue_date = {May 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/279437.279460},
doi = {10.1145/279437.279460},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {44–63},
numpages = {20}
}

@book{10.5555/2886235,
author = {Bird, Christian and Menzies, Tim and Zimmermann, Thomas},
title = {The Art and Science of Analyzing Software Data},
year = {2015},
isbn = {0124115195},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {The Art and Science of Analyzing Software Data provides valuable information on analysis techniques often used to derive insight from software data. This book shares best practices in the field generated by leading data scientists, collected from their experience training software engineering students and practitioners to master data science. The book covers topics such as the analysis of security data, code reviews, app stores, log files, and user telemetry, among others. It covers a wide variety of techniques such as co-change analysis, text analysis, topic analysis, and concept analysis, as well as advanced topics such as release planning and generation of source code comments. It includes stories from the trenches from expert data scientists illustrating how to apply data analysis in industry and open source, present results to stakeholders, and drive decisions.Presents best practices, hints, and tips to analyze data and apply tools in data science projectsPresents research methods and case studies that have emerged over the past few years to further understanding of software dataShares stories from the trenches of successful data science initiatives in industry}
}

@book{10.5555/2505465,
author = {Schmidt, Richard},
title = {Software Engineering: Architecture-driven Software Development},
year = {2013},
isbn = {0124077684},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Engineering: Architecture-driven Software Development is the first comprehensive guide to the underlying skills embodied in the IEEE's Software Engineering Body of Knowledge (SWEBOK) standard. Standards expert Richard Schmidt explains the traditional software engineering practices recognized for developing projects for government or corporate systems. Software engineering education often lacks standardization, with many institutions focusing on implementation rather than design as it impacts product architecture. Many graduates join the workforce with incomplete skills, leading to software projects that either fail outright or run woefully over budget and behind schedule. Additionally, software engineers need to understand system engineering and architecture-the hardware and peripherals their programs will run on. This issue will only grow in importance as more programs leverage parallel computing, requiring an understanding of the parallel capabilities of processors and hardware. This book gives both software developers and system engineers key insights into how their skillsets support and complement each other. With a focus on these key knowledge areas, Software Engineering offers a set of best practices that can be applied to any industry or domain involved in developing software products. A thorough, integrated compilation on the engineering of software products, addressing the majority of the standard knowledge areas and topics Offers best practices focused on those key skills common to many industries and domains that develop software Learn how software engineering relates to systems engineering for better communication with other engineering professionals within a project environment}
}

@inproceedings{10.1145/800213.806527,
author = {Popek, Gerald J. and Kline, Charles S.},
title = {The PDP-11 virtual machine architecture: A case study},
year = {1975},
isbn = {9781450378635},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800213.806527},
doi = {10.1145/800213.806527},
abstract = {At UCLA, a virtual machine system prototype has been constructed for the Digital Equipment Corporation PDP-11/45. In order to successfully implement that system, a number of hardware changes have been necessary. Some overcome basic inadequacies in the original hardware for this purpose, and others enhance the performance of the virtual machine software. Steps in the development of the modified hardware architecture, as well as relevant aspects of the software structure, are discussed. In addition, a case study of interactions between hardware and software developments is presented, together with conclusions motivated by that experience.},
booktitle = {Proceedings of the Fifth ACM Symposium on Operating Systems Principles},
pages = {97–105},
numpages = {9},
keywords = {Virtual machines, Virtual machine monitor, PDP-11/45, Computer security, Computer architecture},
location = {Austin, Texas, USA},
series = {SOSP '75}
}

@book{10.5555/1537084,
author = {Shinder, Thomas W. and Shinder, Debra Littlejohn and Dimcev, Adrian F. and Eaton-Lee, James and Jones, Jason and Moffat, Steve},
title = {Dr. Tom Shinder's ISA Server 2006 Migration Guide},
year = {2007},
isbn = {9780080555515},
publisher = {Syngress Publishing},
abstract = {Dr. Tom Shinder's ISA Server 2006 Migration Guide provides a clear, concise, and thorough path to migrate from previous versions of ISA Server to ISA Server 2006. ISA Server 2006 is an incremental upgrade from ISA Server 2004, this book provides all of the tips and tricks to perform a successful migration, rather than rehash all of the features which were rolled out in ISA Server 2004. Also, learn to publish Exchange Server 2007 with ISA 2006 and to build a DMZ. * Highlights key issues for migrating from previous versions of ISA Server to ISA Server 2006. * Learn to Publish Exchange Server 2007 Using ISA Server 2006. * Create a DMZ using ISA Server 2006. * Dr. Tom Shinder's previous two books on configuring ISA Server have sold more than 50,000 units worldwide. * Dr. Tom Shinder is a Microsoft Most Valuable Professional (MVP) for ISA Server and a member of the ISA Server beta testing team. * This book will be the "Featured Product" on the Internet's most popular ISA Server site www.isaserver.org.}
}

@book{10.5555/2742301,
author = {Preim, Bernhard and Botha, Charl P.},
title = {Visual Computing for Medicine: Theory, Algorithms, and Applications},
year = {2013},
isbn = {9780124159792},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Visual Computing for Medicine, Second Edition, offers cutting-edge visualization techniques and their applications in medical diagnosis, education, and treatment. The book includes algorithms, applications, and ideas on achieving reliability of results and clinical evaluation of the techniques covered. Preim and Botha illustrate visualization techniques from research, but also cover the information required to solve practical clinical problems. They base the book on several years of combined teaching and research experience. This new edition includes six new chapters on treatment planning, guidance and training; an updated appendix on software support for visual computing for medicine; and a new global structure that better classifies and explains the major lines of work in the field.}
}

@book{10.5555/1564784,
author = {Wang},
title = {System-on-Chip Test Architectures: Nanometer  Design for Testability},
year = {2007},
isbn = {9780080556802},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Modern electronics testing has a legacy of more than 40 years. The introduction of new technologies, especially nanometer technologies with 90nm or smaller geometry, has allowed the semiconductor industry to keep pace with the increased performance-capacity demands from consumers. As a result, semiconductor test costs have been growing steadily and typically amount to 40% of today's overall product cost. This book is a comprehensive guide to new VLSI Testing and Design-for-Testability techniques that will allow students, researchers, DFT practitioners, and VLSI designers to master quickly System-on-Chip Test architectures, for test debug and diagnosis of digital, memory, and analog/mixed-signal designs. KEY FEATURES * Emphasizes VLSI Test principles and Design for Testability architectures, with numerous illustrations/examples. * Most up-to-date coverage available, including Fault Tolerance, Low-Power Testing, Defect and Error Tolerance, Network-on-Chip (NOC) Testing, Software-Based Self-Testing, FPGA Testing, MEMS Testing, and System-In-Package (SIP) Testing, which are not yet available in any testing book. * Covers the entire spectrum of VLSI testing and DFT architectures, from digital and analog, to memory circuits, and fault diagnosis and self-repair from digital to memory circuits. * Discusses future nanotechnology test trends and challenges facing the nanometer design era; promising nanotechnology test techniques, including Quantum-Dots, Cellular Automata, Carbon-Nanotubes, and Hybrid Semiconductor/Nanowire/Molecular Computing. * Practical problems at the end of each chapter for students.}
}

@book{10.5555/2222488,
author = {Poynton, Charles},
title = {Digital Video and HD: Algorithms and Interfaces},
year = {2012},
isbn = {9780123919267},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Digital Video and HD: Algorithms and Interfaces provides a one-stop shop for the theory and engineering of digital video systems. Equally accessible to video engineers and those working in computer graphics, Charles Poyntons revision to his classic text covers emergent compression systems, including H.264 and VP8WebM, and augments detailed information on JPEG, DVC, and MPEG-2 systems. This edition also introduces the technical aspects of file-based workflows and outlines the emerging domain of metadata, placing it in the context of digital video processing. With the help of hundreds of high quality technical illustrations, this book presents the following topics: * Basic concepts of digitization, sampling, quantization, gamma, and filtering * Principles of color science as applied to image capture and display * Scanning and coding of SDTV and HDTV * Video color coding: luma, chroma (4:2:2 component video, 4 fSC composite video) * Analog NTSC and PAL * Studio systems and interfaces * Compression technology, including M-JPEG and MPEG-2 * Broadcast standards and consumer video equipment Table of Contents Acknowledgments List of Figures List of Tables Preface Part 1 - Introduction Chapter 1 - Raster Images Chapter 2 - Quantization Chapter 3 - Brightness Contrast Controls Chapter 4 - Raster Images in Computing Chapter 5 - Raster Scanning Chapter 6 - Image Structure Chapter 7 - Resolution Chapter 8 - Constant Luminance Chapter 9 - Rendering Intent Chapter 10 - Introduction to Luma Chroma Chapter 11 - Introduction to Component SDTV Chapter 12 - Introduction to Composite NTSC PAL Chapter 13 - Introduction to HDTV Chapter 14 - Introduction to Compression Chapter 15 - Digital Video Interfaces Part 2 - Principles Chapter 16 - Filtering and Sampling Chapter 17 - Resampling, Interpolation, and decimation Chapter 18 - Image Digitization and Reconstruction Chapter 19 - Perception and Visual Acuity Chapter 20 - Luminance and Lightness Chapter 21 - The CIE System of Colorimetry Chapter 22 - Color Science for Video Chapter 23 - Gamma Chapter 24 - Luma and Color Differences Chapter 25 - Component Video Color Coding for SDTV Chapter 26 - Component Video Color Coding for HDTV Chapter 27 - NTSC PAL Chroma Modulation Chapter 28 - NTSC PAL Frequency Interleaving Chapter 29 - NTSC YIQ System Chapter 30 - Frame, Field, Line, and Sample Rates Chapter 31 - Timecode Chapter 32 - Video Signal Structure Chapter 33 - Digital Sync., TRS, Ancillary Data, and Interface Chapter 34 - Analog SDTV Sync, Genlock, and Interface Chapter 35 - Videotape Recording Chapter 36 - 2-3 Pulldown Chapter 37 - Deinterlacing Part 3 - Video Compression Chapter 38 - JPEG and Motion-JPEG Compression Chapter 39 - MPEG-2 Video Compression Part 4 - Studio Standards Chapter 40 - 52559.94 Component Video Chapter 41 - 52559.94 NTSC Composite Video Chapter 42 - 62550 Component Video Chapter 43 - 62550 PAL Composite Video Chapter 44 - SDTV Test Signals Chapter 45 - 1280x720 HDTV Chapter 46 - 1920x1080 HDTV Chapter 47 - Electrical and Mechanical Interfaces Part 5 - Broadcast Consumer Video Chapter 48 - Analog NTSC nbsp; PAL Broadcast Standards Chapter 49 - Consumer Analog NTSC PAL Chapter 50 - Digital Television Broadcast Standards Appendices A - YUV and Luminance Considered Harmful B - Introduction to Radiometry Photometry C - Glossary of Video Signal Terms Index}
}

@proceedings{10.1145/2951913,
title = {ICFP 2016: Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
year = {2016},
isbn = {9781450342193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nara, Japan}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

