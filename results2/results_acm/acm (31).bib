@inproceedings{10.1145/3336294.3336304,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Software Product Line Engineering: A Practical Experience},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336304},
doi = {10.1145/3336294.3336304},
abstract = {The lack of mature tool support is one of the main reasons that make the industry to be reluctant to adopt Software Product Line (SPL) approaches. A number of systematic literature reviews exist that identify the main characteristics offered by existing tools and the SPL phases in which they can be applied. However, these reviews do not really help to understand if those tools are offering what is really needed to apply SPLs to complex projects. These studies are mainly based on information extracted from the tool documentation or published papers. In this paper, we follow a different approach, in which we firstly identify those characteristics that are currently essential for the development of an SPL, and secondly analyze whether the tools provide or not support for those characteristics. We focus on those tools that satisfy certain selection criteria (e.g., they can be downloaded and are ready to be used). The paper presents a state of practice with the availability and usability of the existing tools for SPL, and defines different roadmaps that allow carrying out a complete SPL process with the existing tool support.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {164–176},
numpages = {13},
keywords = {tooling roadmap, tool support, state of practice, spl in practice},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2934466.2934489,
author = {Nagamine, Motoi and Nakajima, Tsuyoshi and Kuno, Noriyoshi},
title = {A case study of applying software product line engineering to the air conditioner domain},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934489},
doi = {10.1145/2934466.2934489},
abstract = {Software development for embedded products requires high quality, high productivity, and short delivery time because of strong business demands. Although software product line engineering (SPLE) is widely recognized as a good approach for systematic reuse of software, few reports present the information needed for other organizations to implement SPLE. This paper describes a case study of applying SPLE to a product family of air-conditioners, including the effects on degree of implementation of SPLE'S three essential activities (domain engineering, application engineering, and management) and its evaluation over the long period. The use of an incomplete implementation of SPLE's three essential activities temporally improves the productivity of the application developments due to the effect of refactored software, but this gradually decreases through architecture erosion.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {220–226},
numpages = {7},
keywords = {software product line, embedded system, case study, SPL},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.infsof.2018.01.016,
author = {Soares, Larissa Rocha and Schobbens, Pierre-Yves and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Feature interaction in software product line engineering: A systematic mapping study},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.01.016},
doi = {10.1016/j.infsof.2018.01.016},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {44–58},
numpages = {15},
keywords = {Systematic mapping, Software product lines, Feature interaction}
}

@inproceedings{10.5555/2814058.2814112,
author = {Lobato, Luanna Lopes and Bittar, Thiago Jabur},
title = {A Risk Management Approach for Software Product Line Engineering},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {TSoftware Product Line (SPL) Engineering is a software development paradigm that fosters systematic reuse. It is focused on improving software practices, leading companies to experience benefits, such as reduced time-to-market and effort, and higher quality for the products delivered to customers. However, establishing a SPL is neither a simple nor a cheap task, and may affect several aspects of a software company. Besides, it involves a range of risks that may hinder project success. These have to be managed accordingly, so as to minimize the likelihood of project failure. Despite the importance of Risk Management (RM) for SPL Engineering, little has been published in terms of suitable and structured practices to cope with that. This present paper reports an approach for RM in SPL Engineering, named RiPLERM (Rise Product Line Engineering and Risk Management). The approach presents activities to structure RM in SPL projects, The design of the RiPLE-RM approach elaborated on results from empirical investigations, and was proposed to facilitate the management and provide significant insights that can be used to avoid and solve risks.},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
pages = {331–338},
numpages = {8},
keywords = {Software Product Line Engineering, Software Process, Risk Management, Project management},
location = {Goiania, Goias, Brazil},
series = {SBSI '15}
}

@inproceedings{10.1145/3382025.3414971,
author = {Martinez, Jabier and Wolfart, Daniele and Assun\c{c}\~{a}o, Wesley K. G. and Figueiredo, Eduardo},
title = {Insights on software product line extraction processes: ArgoUML to ArgoUML-SPL revisited},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414971},
doi = {10.1145/3382025.3414971},
abstract = {Software Product Lines (SPLs) are rarely developed from scratch. Commonly, they emerge from monolithic architectures when there is a need to create tailored variants, or from existing variants created in an ad-hoc way once their separated maintenance and evolution become challenging. Despite the vast literature about re-engineering systems into SPLs and related technical approaches, there is a lack of detailed analysis about the process itself and the effort that is involved. We provide and analyze empirical data of an existing SPL extraction process: the ArgoUML monolithic architecture transition to ArgoUML-SPL. The analysis relies on information mined from the version control history of the source-code repository and the discussion with developers that took part in the process. The contribution of this study is an in-depth characterization of the process compared to previous works that focused only on the structural results of the final SPL. We made publicly available the dataset and the analysis scripts to be used as baseline for extractive SPL adoption research and practice.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {6},
numpages = {6},
keywords = {software product line architecture, re-engineering, mining software repositories, ArgoUML},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {program synthesis, genetic programming, SPL, SBSE},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {variability, transfer learning, software product lines, machine learning, deep neural networks},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {software product line, requirements engineering, natural language processing, feature model extraction, NLTK},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1007/s10664-020-09913-9,
author = {Lindohf, Robert and Kr\"{u}ger, Jacob and Herzog, Erik and Berger, Thorsten},
title = {Software product-line evaluation in the large},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09913-9},
doi = {10.1007/s10664-020-09913-9},
abstract = {Software product-line engineering is arguably one of the most successful methods for establishing large portfolios of software variants in an application domain. However, despite the benefits, establishing a product line requires substantial upfront investments into a software platform with a proper product-line architecture, into new software-engineering processes (domain engineering and application engineering), into business strategies with commercially successful product-line visions and financial planning, as well as into re-organization of development teams. Moreover, establishing a full-fledged product line is not always possible or desired, and thus organizations often adopt product-line engineering only to an extent that deemed necessary or was possible. However, understanding the current state of adoption, namely, the maturity or performance of product-line engineering in an organization, is challenging, while being crucial to steer investments. To this end, several measurement methods have been proposed in the literature, with the most prominent one being the Family Evaluation Framework (FEF), introduced almost two decades ago. Unfortunately, applying it is not straightforward, and the benefits of using it have not been assessed so far. We present an experience report of applying the FEF to nine medium- to large-scale product lines in the avionics domain. We discuss how we tailored and executed the FEF, together with the relevant adaptations and extensions we needed to perform. Specifically, we elicited the data for the FEF assessment with 27 interviews over a period of 11 months. We discuss experiences and assess the benefits of using the FEF, aiming at helping other organizations assessing their practices for engineering their portfolios of software variants.},
journal = {Empirical Softw. Engg.},
month = mar,
numpages = {41},
keywords = {family evaluation framework, experience report, process maturity, software product lines}
}

@inproceedings{10.1145/3109729.3109744,
author = {Munoz, Daniel-Jesus},
title = {Achieving energy efficiency using a Software Product Line Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109744},
doi = {10.1145/3109729.3109744},
abstract = {Green computing and energy-aware software engineering are trend approaches that try to address the development of applications respectful with the environment. To reduce the energy consumption of an application the developer needs: (i) to identify what are the concerns that will impact more in the energy consumption; (ii) to model the variability of alternative designs and implementations of each concern; (iii) to store and compare the experimentation results related with the energy and time consumption of concerns; (iv) to find out what is the most eco-efficient solution for each concern. HADAS addresses these issues by modelling the variability of energy consuming concerns for different energy contexts. It connects the variability model with a repository that stores energy measurements, providing a Software Product Line (SPL) service, helping developers to reason and find out what are the most eco-friendly configurations. We have an initial implementation of the HADAS toolkit using Clafer. We have tested our implementation with several case studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {131–138},
numpages = {8},
keywords = {Variability, Software Product Line, Repository, Optimisation, Metrics, Energy Efficiency, Clafer},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1007/s11334-011-0159-y,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {An architecture process maturity model of software product line engineering},
year = {2011},
issue_date = {September 2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {7},
number = {3},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-011-0159-y},
doi = {10.1007/s11334-011-0159-y},
abstract = {Software architecture has been a key research area in the software engineering community due to its significant role in creating high-quality software. The trend of developing product lines rather than single products has made the software product line a viable option in the industry. Software product line architecture (SPLA) is regarded as one of the crucial components in the product lines, since all of the resulting products share this common architecture. The increased popularity of software product lines demands a process maturity evaluation methodology. Consequently, this paper presents an architecture process maturity model for software product line engineering to evaluate the current maturity of the product line architecture development process in an organization. Assessment questionnaires and a rating methodology comprise the framework of this model. The objective of the questionnaires is to collect information about the SPLA development process. Thus, in general this work contributes towards the establishment of a comprehensive and unified strategy for the process maturity evaluation of software product line engineering. Furthermore, we conducted two case studies and reported the assessment results, which show the maturity of the architecture development process in two organizations.},
journal = {Innov. Syst. Softw. Eng.},
month = sep,
pages = {191–207},
numpages = {17},
keywords = {Software product line, Software architecture, Process assessment, Domain engineering, Application engineering}
}

@inproceedings{10.1145/3106195.3106220,
author = {Young, Bobbi and Cheatwood, Judd and Peterson, Todd and Flores, Rick and Clements, Paul},
title = {Product Line Engineering Meets Model Based Engineering in the Defense and Automotive Industries},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106220},
doi = {10.1145/3106195.3106220},
abstract = {Product line engineering and model based engineering are two powerful engineering approaches that each bring significant advantages to system engineering projects. This paper explores how three companies - Raytheon, General Dynamics, and General Motors - are combining these two paradigms in unique and innovative ways in very challenging application domains to achieve engineering goals of critical importance to them.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {175–179},
numpages = {5},
keywords = {variation points, product configurator, model-based engineering, feature-based product line engineering, feature profiles, feature models, Product line engineering, PLE factory},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2420942.2420948,
author = {Gonz\'{a}lez-Huerta, Javier and Insfran, Emilio and Abrah\~{a}o, Silvia and McGregor, John D.},
title = {Non-functional requirements in model-driven software product line engineering},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420948},
doi = {10.1145/2420942.2420948},
abstract = {Developing variant-rich software systems through the application of the software product line approach requires the management of a wide set of requirements. However, in most cases, the focus of those requirements is limited to the functional requirements. The non-functional requirements are often informally defined and their management does not provide traceability mechanisms for their validation. In this paper, we present a multimodel approach that allows the explicit representation of non-functional requirements for software product lines both at domain engineering, and application engineering levels. The multimodel allows the representation of different viewpoints of a software product line, including the non-functional requirements and the relationships that these non-functional requirements might have with features and functionalities. The feasibility of this approach is illustrated through a specific example from the automotive domain.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {6},
numpages = {6},
keywords = {software product lines, non-functional requirements, model driven engineering},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/2480362.2480694,
author = {Diwan, Piyush and Carey, Patricia and Franz, Eric and Li, Yixue and Bitterman, Thomas and Hudak, David E. and Ramnath, Rajiv},
title = {Applying software product line engineering in building web portals for supercomputing services},
year = {2013},
isbn = {9781450316569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480362.2480694},
doi = {10.1145/2480362.2480694},
abstract = {Supercomputing centers, typically non-profit, government or university-based organizations with scarce resources, are increasingly being requested to provide customized web portals for user-centered access to their services in order to support a demanding customer base. These portals often have very similar architectures and meet similar requirements, with the variations primarily being in the specialized analysis applications, and in the input and output of these applications. Given these characteristics, Software Production Line Engineering (SPLE) approaches will be valuable in enabling development teams to cost-effectively meet demands. In this paper, we demonstrate a suite of web portals developed at The Ohio Supercomputer Center (OSC) by applying SPLE methodologies. We show how we applied feature modeling on these applications to identify commonalities in their application level features despite differences in their problem domains. We describe a common framework (we term it Per User DrupaL, or PUDL), which serves as the common foundation for these portals. We demonstrate the effectiveness of SPLE in terms of reduced development time and effort, and discuss the technical challenges faced in this process. Finally we propose, as an extension to our work, an automation framework for portal generation, which users could build their own customized portals.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
pages = {1765–1771},
numpages = {7},
keywords = {supercomputing, software-as-a-service, software product line engineering, portals, high performance computing, feature modeling, end-user computing, drupal},
location = {Coimbra, Portugal},
series = {SAC '13}
}

@inproceedings{10.1145/2791060.2791067,
author = {Yue, Tao and Ali, Shaukat and Selic, Bran},
title = {Cyber-physical system product line engineering: comprehensive domain analysis and experience report},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791067},
doi = {10.1145/2791060.2791067},
abstract = {Cyber-Physical Systems (CPSs) are the future generation of highly connected embedded systems having applications in diverse domains including Oil and Gas. Employing Product Line Engineering (PLE) is believed to bring potential benefits with respect to reduced cost, higher productivity, higher quality, and faster time-to-market. However, relatively few industrial field studies are reported regarding the application of PLE to develop large-scale systems, and more specifically CPSs. In this paper, we report about our experiences and insights gained from investigating the application of model-based PLE at a large international organization developing subsea production systems (typical CPSs) to manage the exploitation of oil and gas production fields. We report in this paper 1) how two systematic domain analyses (on requirements engineering and product configuration/derivation) were conducted to elicit CPS PLE requirements and challenges, 2) key results of the domain analysis (commonly observed in other domains), and 3) our initial experience of developing and applying two Model Based System Engineering (MBSE) PLE solution to address some of the requirements and challenges elicited during the domain analyses.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {338–347},
numpages = {10},
keywords = {requirements engineering, product line engineering (PLE), model based system engineering, domain analysis, cyber physical system (CPS)},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1007/978-3-642-33678-2_30,
author = {Braga, Rosana T. Vaccare and Trindade Junior, Onofre and Castelo Branco, Kalinka Regina and Neris, Luciano De Oliveira and Lee, Jaejoon},
title = {Adapting a software product line engineering process for certifying safety critical embedded systems},
year = {2012},
isbn = {9783642336775},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33678-2_30},
doi = {10.1007/978-3-642-33678-2_30},
abstract = {Software Product Line Engineering (SPLE) is a software development paradigm that aims at reducing the development effort and shorting time-to-market through systematic software reuse. While this paradigm has been successfully applied for the development of embedded systems in various domains, new challenges have emerged from the development of safety critical systems that require certification against a specific standard. Existing SPLE approaches do not explicitly consider the various certification standards or levels that products should satisfy. In this paper, we focus on several practical issues involved in the SPLE process, establishing an infrastructure of a product line engineering for certified products. A metamodel is proposed to capture the entities involved in SPL certification and the relationships among them. ProLiCES, which is a model-driven process for the development of SPLs, was modified to serve as an example of our approach, in the context of the UAV (Unmanned Aerial Vehicle) domain.},
booktitle = {Proceedings of the 31st International Conference on Computer Safety, Reliability, and Security},
pages = {352–363},
numpages = {12},
keywords = {software certification, safety-critical embedded systems, development process},
location = {Magdeburg, Germany},
series = {SAFECOMP'12}
}

@inproceedings{10.1145/2791060.2791081,
author = {Trask, Bruce and Roman, Angel},
title = {Leveraging model driven engineering in software product line architectures},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791081},
doi = {10.1145/2791060.2791081},
abstract = {The process of Developing Software Product Line Architectures can be a complex task. However, the use of Model Driven Engineering (MDE) techniques can facilitate the development of SPLAs by introducing Domain Specific Languages, Graphical Editors, and Generators. Together these are considered the sacred triad of MDE. Key to understanding MDE and how it fits into SPLAs is to know exactly what each part of the trinity means, how it relates to the other parts, and what the various implementations are for each.This tutorial has its foundations in years of industrial experience with large and complex SPLAs in various industries. This tutorial continues to be updated each year to include recent and critical innovations in MDE and SPL. This year will include information on key Model Transformation, Constraints and Textual Modeling Languages targeted at Software Product Lines. Additionally, it will cover advances in Software Product Line migration technologies which include techniques as to how to effectively migrate legacy systems toward and MDE/SPLA architecture and implementation. This year's tutorial includes extensive industrial experience on the testing of large and complex SPLAs.The goal of this tutorial is to educate attendees on what MDE technologies are, how exactly they relate synergistically to Software Product Line Architectures, and how to actually apply them using an existing Eclipse implementation.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {392},
numpages = {1},
keywords = {transformation, traceability, textual, requirements engineering, refinement, programming languages, modeling, meta model, language workbench, graphical, domain specific testing, domain specific, constraint, abstraction, MDE, DSL},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2648511.2648558,
author = {Trask, Bruce and Roman, Angel},
title = {Leveraging model driven engineering in software product line architectures},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648558},
doi = {10.1145/2648511.2648558},
abstract = {The process of Developing Software Product Line Architectures can be a complex task. However, the use of Model Driven Engineering (MDE) techniques can facilitate the development of SPLAs by introducing Domain Specific Languages, Graphical Editors, and Generators. Together these are considered the sacred triad of MDE. Key to understanding MDE and how it fits into SPLAs is to know exactly what each part of the trinity means, how it relates to the other parts, and what the various implementations are for each. This tutorial will demonstrate the use of the Eclipse Modeling Framework (EMF) and Eclipse's Graphical Modeling Framework (GMF) to create an actual MDE solution as applied to a sample SPLA. These tools collectively form what is called a Language Workbench. During this tutorial we will also illustrate how to model the visual artifacts of our Domain Model and generate a Domain Specific Graphical Editor using GMF.This tutorial has its foundations in years of industrial experience with large and complex SPLAs in various industries. This tutorial continues to be updated each year to include recent and critical innovations in MDE and SPL. This year will include information on key Model Transformation, Constraints and Textual Modeling Languages targeted at Software Product Lines. Additionally, it will cover advances in Software Product Line migration technologies which include techniques as to how to effectively migrate legacy systems toward and MDE/SPLA architecture and implementation. This year's tutorial includes extensive industrial experience on the testing of large and complex SPLAs.The goal of this tutorial is to educate attendees on what MDE technologies are, how exactly they relate synergistically to Software Product Line Architectures, and how to actually apply them using an existing Eclipse implementation.The benefits of the technology are so far reaching that we feel the intended audience spans technical managers, developers and CTOs. In general the target audience includes researchers and practitioners who are working on problems related to the design and implementation of SPLAs and would like to understand the benefits of applying MDE techniques towards SPLAs and leverage Eclipse as a framework to develop MDE solutions. The first half will be less technical than the second half where we cover the details of SPLA and MDE in action in complete detail showing patterns and code.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {360–361},
numpages = {2},
keywords = {transformation, traceability, textual, requirements engineering, refinement, programming languages, modeling, meta model, language workbench, graphical, domain specific testing, domain specific, constraint, abstraction, MDE, DSL},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1007/978-3-030-63882-5_12,
author = {Alves, Thayonara and Teixeira, Leopoldo and Alves, Vander and Castro, Thiago},
title = {Porting the Software Product Line Refinement Theory to the Coq Proof&nbsp;Assistant},
year = {2020},
isbn = {978-3-030-63881-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63882-5_12},
doi = {10.1007/978-3-030-63882-5_12},
abstract = {Software product lines are an engineering approach to systematically build similar software products from a common asset base. When evolving such systems, it is important to have assurance that we are not introducing errors or changing the behavior of existing products. The product line refinement theory establishes the necessary conditions for such assurance. This theory has been specified and proved using the PVS proof assistant. However, the Coq proof assistant is increasingly popular among researchers and practitioners, and, given that some programming languages are already formalized into such tool, the refinement theory might benefit from the potential integration. Therefore, in this work we present a case study on porting the PVS specification of the refinement theory to Coq. We compare the proof assistants based on the noted differences between the specifications and proofs of this theory, providing some reflections on the tactics and strategies used to compose the proofs. According to our study, PVS provided more succinct definitions than Coq, in several cases, as well as a greater number of successful automatic commands that resulted in shorter proofs. Despite that, Coq also brought facilities in definitions such as enumerated and recursive types, and features that support developers in their proofs.},
booktitle = {Formal Methods: Foundations and Applications: 23rd Brazilian Symposium, SBMF 2020, Ouro Preto, Brazil, November 25–27, 2020, Proceedings},
pages = {192–209},
numpages = {18},
keywords = {PVS, Coq, Theorem provers, Software product lines},
location = {Ouro Preto, Brazil}
}

@inproceedings{10.1145/2491627.2491649,
author = {Lanman, Jeremy and Darbin, Rowland and Rivera, Jorge and Clements, Paul and Krueger, Charles},
title = {The challenges of applying service orientation to the U.S. Army's live training software product line},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491649},
doi = {10.1145/2491627.2491649},
abstract = {Live Training Transformation (LT2) is the product line strategy put in place by the United States Army Program Executive Office for Simulation, Training and Instrumentation (PEO STRI). The purpose of the LT2 product line is to provide a common set of core assets including architectures, software components, standards and processes that form the basis of all Army Live Training systems. As products consuming LT2 core assets evolve to meet the latest requirements of the military live training community, changes to the core product line architecture must also be made. Based on thorough analysis of the LT2 core capabilities and user trends toward web-enabled and mobile computing technologies, a Service Oriented Architecture (SOA) strategy was identified and adopted as the objective architecture for the evolving LT2 product line. Future success of the LT2 product line now depends on the alignment of product line engineering concepts with the business and technical benefits of SOA, and to ensure that systematic reuse continues to provide substantial return-on-investment for the Army. This paper addresses the challenges of adopting SOA into an existing software product line, the unique circumstances of the LT2 SOA environment, and present a set of analysis and design considerations for the product line engineering community.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {244–253},
numpages = {10},
keywords = {variation points, software product lines, second generation product line engineering, product portfolio, product line engineering, product derivation, product configurator, product baselines, product audit, hierarchical product lines, feature profiles, feature modeling, bill-of-features},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Software Product Lines, Recommender Systems, Non-Functional Properties, Feature Model, Configuration},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1016/j.knosys.2019.104883,
author = {Ayala, Inmaculada and Amor, Mercedes and Horcas, Jose-Miguel and Fuentes, Lidia},
title = {A goal-driven software product line approach for evolving multi-agent systems in the Internet of Things},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {184},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.104883},
doi = {10.1016/j.knosys.2019.104883},
journal = {Know.-Based Syst.},
month = nov,
numpages = {18},
keywords = {GORE, Goal models, MAS-PL, Internet of Things, Evolution, Software product line}
}

@inproceedings{10.1145/2648511.2648534,
author = {Dieumegard, Arnaud and Toom, Andres and Pantel, Marc},
title = {A software product line approach for semantic specification of block libraries in dataflow languages},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648534},
doi = {10.1145/2648511.2648534},
abstract = {Dataflow modelling languages such as SCADE or Simulink are the de-facto standard for the Model Driven Development of safety critical embedded control and command systems. Software is mainly being produced by Automated Code Generators whose correctness can only be assessed meaningfully if the input language semantics is well known. These semantics share a common part but are mainly defined through block libraries. The writing of a complete formal specification for the block libraries of the usual languages is highly challenging due to the high variability of the structure and semantics of each block. This contribution relates the use of software product line principles in the design of a domain specific language targeting the formal specification of block libraries. It summarises the advantages of this DSL regarding the writing, validation and formal verification of such specifications. These experiments have been carried out in the context of the GeneAuto embedded code generator project targeting Simulink and Scicos; and are being extended and applied in its follow up projects ProjetP and Hi-MoCo.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {217–226},
numpages = {10},
keywords = {software qualification, simulink, scicos, model driven engineering, formal specification, feature modelling, automated code generation, Xcos, Why3},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Test generation, Software product line, Genetic algorithm, EXAM score, Debugging/fault localization, Coverage}
}

@article{10.1016/j.infsof.2012.07.020,
author = {Siegmund, Norbert and Rosenm\"{u}Ller, Marko and K\"{a}Stner, Christian and Giarrusso, Paolo G. and Apel, Sven and Kolesnikov, Sergiy S.},
title = {Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.020},
doi = {10.1016/j.infsof.2012.07.020},
abstract = {Context: A software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product. Objective: Our overall goal is to derive optimal products with respect to non-functional requirements by showing customers which features must be selected. Method: We propose an approach to predict a product's non-functional properties based on the product's feature selection. We aggregate the influence of each selected feature on a non-functional property to predict a product's properties. We generate and measure a small set of products and, by comparing measurements, we approximate each feature's influence on the non-functional property in question. As a research method, we conducted controlled experiments and evaluated prediction accuracy for the non-functional properties footprint and main-memory consumption. But, in principle, our approach is applicable for all quantifiable non-functional properties. Results: With nine software product lines, we demonstrate that our approach predicts the footprint with an average accuracy of 94%, and an accuracy of over 99% on average if feature interactions are known. In a further series of experiments, we predicted main memory consumption of six customizable programs and achieved an accuracy of 89% on average. Conclusion: Our experiments suggest that, with only few measurements, it is possible to accurately predict non-functional properties of products of a product line. Furthermore, we show how already little domain knowledge can improve predictions and discuss trade-offs between accuracy and required number of measurements. With this technique, we provide a basis for many reasoning and product-derivation approaches.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {491–507},
numpages = {17},
keywords = {Software product lines, SPL Conqueror, Prediction, Non-functional properties, Measurement}
}

@inproceedings{10.1145/3382025.3414946,
author = {Fritsch, Claudia and Abt, Richard and Renz, Burkhardt},
title = {The benefits of a feature model in banking},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414946},
doi = {10.1145/3382025.3414946},
abstract = {This experience report describes the surprisingly beneficial introduction of feature modeling at KfW, a government promotional bank. On behalf of the government and based on promotional directives, KfW grants retail loans to small and medium enterprises, business founders, self-employed professionals, municipalities and private individuals. The promotional directives, called programs, define mandatory and optional properties of these loans. We have now successfully built a feature model from these properties.Our feature model will be presented with its outstanding characteristic, which is an additional subtree containing the programs as features. Complete and correct cross-tree constraints will also allow us to analyze and scope the portfolio, reduce complexity, and speed-up time-to-market. This is the advent of product line development at KfW.In order to standardize our portfolio, we have subsequently developed tools on top of the feature model, namely, a browser-based, multi-user configurator assisting non-technical-affine users in their product design, and a generator producing complete product documentation from the feature model and partial configurations. More applications are currently underway.This is our story of applying Software Product Line Engineering in banking, a domain where it is unusual or even unknown. We share our ideas, analyses, progress, and findings where the results have been thrilling us for the past two years and will continue to do so.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {9},
numpages = {11},
keywords = {software product line engineering, retail loans, partial configuration, mass customization, feature modeling, experience report, document generation},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.5555/1753235.1753250,
author = {Jepsen, Hans Peter and Beuche, Danilo},
title = {Running a software product line: standing still is going backwards},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Danfoss Drives - one of the largest producers of frequency converters in the world - has been doing Software Product Line development for its frequency converter products for about 3 years. This paper describes the approach used and the experiences with it. It discusses processes, ways to convince the unconvinced and arising tool issues when doing product line development.This paper is a follow-up on a previous article which described the product line migration process in detail.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {101–110},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2797433.2797456,
author = {Galster, Matthias},
title = {Architecting for Variability in Quality Attributes of Software Systems},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797456},
doi = {10.1145/2797433.2797456},
abstract = {Variability in software systems is usually concerned with variability in features and functionality. However, variability also occurs in quality attributes (e.g., performance, security) and quality attribute requirements (for example, a performance requirement may state that a system must respond to a user request within 0.1 seconds). We discuss what variability in quality attributes is, including several scenarios in which variability in quality attributes can occur. We then discuss the state of research and what we know about variability in quality attributes, including some existing research to address the challenge of identifying, implementing and managing variability in quality attributes. Finally, we discuss potential directions for future research.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {23},
numpages = {4},
keywords = {software architecture, quality attributes, Variability},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@article{10.1145/2180921.2180941,
author = {Ripon, Shamim H.},
title = {A unified tabular method for modeling variants of software product line},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/2180921.2180941},
doi = {10.1145/2180921.2180941},
abstract = {Reuse of software is a promising approach to improving the efficiency of software development regarding time, cost and quality. Reuse requires a systematic approach. The best results are achieved if we focus on systems in a specific domain, so-called product line. The key difference between the conventional software engineering and software product line engineering is variant management. The main idea of software product line is to identify the common core functionality which can be implemented once and reused afterwards for all members of the product line. To facilitate this reuse opportunity the domain engineering phase makes the domain model comprising the common as well as variant requirements. In principle, common requirements among systems in a family are easy to handle. However, problem arises during handling variants. Different variants have dependencies on each other; a single variant can affect several variants of the domain model. These problems become complex when the volume of information grows in a domain and there are a lot of variants with several interdependencies. Hence, a separate model is required for handling the variants. This paper presents a mechanism, which we call, Unified Tabular Method to facilitate the management of variant dependencies in product lines. The tabular method consists of a variant part to model the variants and their dependencies, and a decision table to depict the customization decision regarding each variant while deriving customized products. Tabular method alleviates the problem of possible explosion of variant combinations and facilitates the tracing of variant information in the domain model},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {1–7},
numpages = {7},
keywords = {unified tabular method, software product line, modeling variants}
}

@article{10.1016/j.infsof.2012.06.014,
author = {Andersson, Henric and Herzog, Erik and \"{O}Lvander, Johan},
title = {Experience from model and software reuse in aircraft simulator product line engineering},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.06.014},
doi = {10.1016/j.infsof.2012.06.014},
abstract = {Context: ''Reuse'' and ''Model Based Development'' are two prominent trends for improving industrial development efficiency. Product lines are used to reduce the time to create product variants by reusing components. The model based approach provides the opportunity to enhance knowledge capture for a system in the early stages in order to be reused throughout its lifecycle. This paper describes how these two trends are combined to support development and support of a simulator product line for the SAAB 39 Gripen fighter aircraft. Objective: The work aims at improving the support (in terms of efficiency and quality) when creating simulation model configurations. Software based simulators are flexible so variants and versions of included models may easily be exchanged. The objective is to increase the reuse when combining models for usage in a range of development and training simulators. Method: The research has been conducted with an interactive approach using prototyping and demonstrations, and the evaluation is based on an iterative and a retrospective method. Results: A product line of simulator models for the SAAB 39 Gripen aircraft has been analyzed and defined in a Product Variant Master. A configurator system has been implemented for creation, integration, and customization of stringent simulator model configurations. The system is currently under incorporation in the standard development process at SAAB Aeronautics. Conclusion: The explicit and visual description of products and their variability through a configurator system enables better insights and a common understanding so that collaboration on possible product configurations improves and the potential of software reuse increases. The combination of application fields imposes constraints on how traditional tools and methods may be utilized. Solutions for Design Automation and Knowledge Based Engineering are available, but their application has limitations for Software Product Line engineering and the reuse of simulation models.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {595–606},
numpages = {12},
keywords = {Software Product Line, SPL, PDM, Model Based Development, Knowledge Based Engineering, Configurator}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {software variability, computer-aided software engineering, Software engineering}
}

@inproceedings{10.5555/1753235.1753258,
author = {Ganesan, Dharmalingam and Lindvall, Mikael and Ackermann, Chris and McComas, David and Bartholomew, Maureen},
title = {Verifying architectural design rules of the flight software product line},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {This paper presents experiences of verifying architectural design rules of the NASA Core Flight Software (CFS) product line implementation. The goal is to check whether the implementation is consistent with the CFS' architectural rules derived from the developer's guide. The results indicate that consistency checking helps a) identifying architecturally significant deviations that were eluded during code reviews, b) clarifying the design rules to the team, and c) assessing the overall implementation quality. Furthermore, it helps connecting business goals to architectural principles, and to the implementation. This paper is the first step in the definition of a method for analyzing and evaluating product line implementations from an architecture-centric perspective.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {161–170},
numpages = {10},
keywords = {architectural rules, business goals, flight software, implemented architecture},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/1842752.1842773,
author = {McGregor, John D.},
title = {A method for analyzing software product line ecosystems},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842773},
doi = {10.1145/1842752.1842773},
abstract = {The ecosystem for a software product line includes all of the entities with which the software product line organization interacts. Information, artifacts, customers, money and products move among these entities as a part of the planning, development, and deployment processes. In this paper we present an analysis technique that uses the economic notion of a transaction to examine the transfers between the entities. The result of the analysis is data that is used to evaluate and structure the organization. We illustrate with an example.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {73–80},
numpages = {8},
keywords = {software product line, software ecosystem},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1007/978-3-642-34032-1_22,
author = {Ferrari, Alessio and Spagnolo, Giorgio Oronzo and Martelli, Giacomo and Menabeni, Simone},
title = {Product line engineering applied to CBTC systems development},
year = {2012},
isbn = {9783642340314},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34032-1_22},
doi = {10.1007/978-3-642-34032-1_22},
abstract = {Communications-based Train Control (CBTC) systems are the new frontier of automated train control and operation. Currently developed CBTC platforms are actually very complex systems including several functionalities, and every installed system, developed by a different company, varies in extent, scope, number, and even names of the implemented functionalities. International standards have emerged, but they remain at a quite abstract level, mostly setting terminology.This paper reports intermediate results in an effort aimed at defining a global model of CBTC, by mixing semi-formal modelling and product line engineering. The effort has been based on an in-depth market analysis, not limiting to particular aspects but considering as far as possible the whole picture. The adopted methodology is discussed and a preliminary model is presented.},
booktitle = {Proceedings of the 5th International Conference on Leveraging Applications of Formal Methods, Verification and Validation: Applications and Case Studies - Volume Part II},
pages = {216–230},
numpages = {15},
location = {Heraklion, Crete, Greece},
series = {ISoLA'12}
}

@article{10.1016/j.advengsoft.2014.01.011,
author = {Rossel, Pedro O. and Bastarrica, Mar\'{\i}a Cecilia and Hitschfeld-Kahler, Nancy and D\'{\i}az, Violeta and Medina, Mario},
title = {Domain modeling as a basis for building a meshing tool software product line},
year = {2014},
issue_date = {April, 2014},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {70},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2014.01.011},
doi = {10.1016/j.advengsoft.2014.01.011},
abstract = {Meshing tools are highly complex software for generating and managing geometrical discretizations. Due to their complexity, they have generally been developed by end users - physicists, forest engineers, mechanical engineers - with ad hoc methodologies and not by applying well established software engineering practices. Different meshing tools have been developed over the years, making them a good application domain for Software Product Lines (SPLs). This paper proposes building a domain model that captures the different domain characteristics such as features, goals, scenarios and a lexicon, and the relationships among them. The model is partly specified using a formal language. The domain model captures product commonalities and variabilities as well as the particular characteristics of different SPL products. The paper presents a rigorous process for building the domain model, where specific roles, activities and artifacts are identified. This process also clearly establishes consistency and completeness conditions. The usefulness of the model and the process are validated by using them to generate a software product line of Tree Stem Deformation (TSD) meshing tools. We also present Meshing Tool Generator, a software that follows the SPL approach for generating meshing tools belonging to the TSD SPL. We show how an end user can easily generate three different TSD meshing tools using Meshing Tool Generator.},
journal = {Adv. Eng. Softw.},
month = apr,
pages = {77–89},
numpages = {13},
keywords = {Tree stem deformation, Software product line, Meshing tools, Domain model, Domain analysis, Code generator}
}

@article{10.1016/j.infsof.2007.10.013,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {The software product line architecture: An empirical investigation of key process activities},
year = {2008},
issue_date = {October, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {50},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.10.013},
doi = {10.1016/j.infsof.2007.10.013},
abstract = {Software architecture has been a key area of concern in software industry due to its profound impact on the productivity and quality of software products. This is even more crucial in case of software product line, because it deals with the development of a line of products sharing common architecture and having controlled variability. The main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting a comprehensive empirical investigation covering a broad range of organizations currently involved in the business of software product lines. This is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge. The results of this investigation provide empirical evidence that software product line architecture process activities play a significant role in successfully developing and managing a software product line.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1098–1113},
numpages = {16},
keywords = {Software product line, Software engineering, Software architecture, Empirical study, Domain engineering}
}

@inproceedings{10.1145/2491627.2491655,
author = {Dumitrescu, Cosmin and Mazo, Raul and Salinesi, Camille and Dauron, Alain},
title = {Bridging the gap between product lines and systems engineering: an experience in variability management for automotive model based systems engineering},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491655},
doi = {10.1145/2491627.2491655},
abstract = {We present in this paper an experience in modeling a family of parking brake systems, with shared assets and alternative solutions, and relate them to the needs of Renault in terms of variability management. The models are realized using a set of customized tools for model based systems engineering and variability management, based on SysML models. The purpose is to present an industrial context that requires the adoption of a product line approach and of variability modeling techniques, outside of a pure-software domain. At Renault, the interest is in identifying variations and reuse opportunities early in the product development cycle, as well as in preparing vehicle configuration specifications during the systems engineering process. This would lead to lowering the engineering effort and to higher quality and confidence in carry-over and carry across based solutions. We advocate for a tight integration of variability management with the model based systems engineering approach, which needs to address methodological support, modeling techniques and efficient tools for interactive configuration, adapted for engineering activities.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {254–263},
numpages = {10},
keywords = {variability management, systems engineering},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3233027.3236395,
author = {Pereira, Juliana Alves and Maciel, Lucas and Noronha, Thiago F. and Figueiredo, Eduardo},
title = {Heuristic and exact algorithms for product configuration in software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236395},
doi = {10.1145/3233027.3236395},
abstract = {The Software Product Line (SPL) configuration field is an active area of research and has attracted both practitioners and researchers attention in the last years. A key part of an SPL configuration is a feature model that represents features and their dependencies (i.e., SPL configuration rules). This model can be extended by adding Non-Functional Properties (NFPs) as feature attributes resulting in Extended Feature Models (EFMs). Configuring products from an EFM requires considering the configuration rules of the model and satisfying the product functional and non-functional requirements. Although the configuration of a product arising from EFMs may reduce the space of valid configurations, selecting the most appropriate set of features is still an overwhelming task due to many factors including technical limitations and diversity of contexts. Consequently, configuring large and complex SPLs by using configurators is often beyond the users' capabilities of identifying valid combinations of features that match their (non-functional) requirements. To overcome this limitation, several approaches have modeled the product configuration task as a combinatorial optimization problem and proposed constraint programming algorithms to automatically derive a configuration. Although these approaches do not require any user intervention to guarantee the optimality of the generated configuration, due to the NP-hard computational complexity of finding an optimal variant, exact approaches have inefficient exponential time. Thus, to improve scalability and performance issues, we introduced the adoption of a greedy heuristic algorithm and a biased random-key genetic algorithm (BRKGA). Our experiment results show that our proposed heuristics found optimal solutions for all instances where those are known. For the instances where optimal solutions are not known, the greedy heuristic outperformed the best solution obtained by a one-hour run of the exact algorithm by up to 67.89%. Although the BRKGA heuristic slightly outperformed the greedy heuristic, it has shown larger running times (especially on the largest instances). Therefore, to ensure a good user experience and enable a very fast configuration task, we extended a state-of-the-art configurator with the proposed greedy heuristic approach.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {247},
numpages = {1},
keywords = {software product lines, software product line configuration, search-based software engineering, configuration optimization},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.jss.2013.12.038,
author = {Capilla, Rafael and Bosch, Jan and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio and Hinchey, Mike},
title = {An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.12.038},
doi = {10.1016/j.jss.2013.12.038},
abstract = {Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures.},
journal = {J. Syst. Softw.},
month = may,
pages = {3–23},
numpages = {21},
keywords = {Software architecture, Feature models, Dynamic variability, Dynamic Software Product Lines}
}

@article{10.1007/s11219-011-9156-5,
author = {Roos-Frantz, Fabricia and Benavides, David and Ruiz-Cort\'{e}s, Antonio and Heuer, Andr\'{e} and Lauenroth, Kim},
title = {Quality-aware analysis in product line engineering with the orthogonal variability model},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9156-5},
doi = {10.1007/s11219-011-9156-5},
abstract = {Software product line engineering is about producing a set of similar products in a certain domain. A variability model documents the variability amongst products in a product line. The specification of variability can be extended with quality information, such as measurable quality attributes (e.g., CPU and memory consumption) and constraints on these attributes (e.g., memory consumption should be in a range of values). However, the wrong use of constraints may cause anomalies in the specification which must be detected (e.g., the model could represent no products). Furthermore, based on such quality information, it is possible to carry out quality-aware analyses, i.e., the product line engineer may want to verify whether it is possible to build a product that satisfies a desired quality. The challenge for quality-aware specification and analysis is threefold. First, there should be a way to specify quality information in variability models. Second, it should be possible to detect anomalies in the variability specification associated with quality information. Third, there should be mechanisms to verify the variability model to extract useful information, such as the possibility to build a product that fulfils certain quality conditions (e.g., is there any product that requires less than 512 MB of memory?). In this article, we present an approach for quality-aware analysis in software product lines using the orthogonal variability model (OVM) to represent variability. We propose to map variability represented in the OVM associated with quality information to a constraint satisfaction problem and to use an off-the-shelf constraint programming solver to automatically perform the verification task. To illustrate our approach, we use a product line in the automotive domain which is an example that was created in a national project by a leading car company. We have developed a prototype tool named FaMa-OVM, which works as a proof of concepts. We were able to identify void models, dead and false optional elements, and check whether the product line example satisfies quality conditions.},
journal = {Software Quality Journal},
month = sep,
pages = {519–565},
numpages = {47},
keywords = {Software product lines, Quality-aware analysis, Quality modelling, Orthogonal variability model, Automated analysis}
}

@article{10.1016/j.infsof.2012.08.010,
author = {Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
title = {Variability in quality attributes of service-based software systems: A systematic literature review},
year = {2013},
issue_date = {February, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.08.010},
doi = {10.1016/j.infsof.2012.08.010},
abstract = {Context: Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective: We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method: A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results: Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions: The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {320–343},
numpages = {24},
keywords = {Variability, Systematic literature review, Service-based systems, Quality attributes}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Software product lines, SPL Conqueror, Non-functional properties, Measurement and optimization, Feature-oriented software development}
}

@article{10.1007/s00766-013-0185-4,
author = {Derakhshanmanesh, Mahdi and Fox, Joachim and Ebert, J\"{u}rgen},
title = {Requirements-driven incremental adoption of variability management techniques and tools: an industrial experience report},
year = {2014},
issue_date = {November  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0185-4},
doi = {10.1007/s00766-013-0185-4},
abstract = {In theory, software product line engineering has reached a mature state. In practice though, implementing a variability management approach remains a tough case-by-case challenge for any organization. To tame the complexity of this undertaking, it is inevitable to handle variability from multiple perspectives and to manage variability consistently across artifacts, tools, and workflows. Especially, a solid understanding and management of the requirements to be met by the products is an inevitable prerequisite. In this article, we share experiences from the ongoing incremental adoption of explicit variability management at TRW Automotive's department for automotive slip control systems--located in Koblenz, Germany. On the technical side, the three key drivers of this adoption effort are (a) domain modeling and scoping, (b) handling of variability in requirements and (c) tighter integration of software engineering focus areas (e.g., domain modeling, requirements engineering, architectural modeling) to make use of variability-related data. In addition to implementation challenges with using and integrating concrete third-party tools, social and workflow-related issues are covered as well. The lessons learned are presented, discussed, and thoroughly compared with the state of the art in research.},
journal = {Requir. Eng.},
month = nov,
pages = {333–354},
numpages = {22},
keywords = {Tool integration, Software product lines, Reuse, Requirements, Incremental adoption, Features}
}

@article{10.1016/j.infsof.2010.12.006,
author = {Chen, Lianping and Ali Babar, Muhammad},
title = {A systematic review of evaluation of variability management approaches in software product lines},
year = {2011},
issue_date = {April, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.12.006},
doi = {10.1016/j.infsof.2010.12.006},
abstract = {ContextVariability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated. ObjectiveThe objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches. MethodWe carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007. ResultsWe selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects. ConclusionThe findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {344–362},
numpages = {19},
keywords = {Variability management, Systematic literature reviews, Software product line, Empirical studies}
}

@inproceedings{10.1145/2245276.2231956,
author = {Horikoshi, Hisayuki and Nakagawa, Hiroyuki and Tahara, Yasuyuki and Ohsuga, Akihiko},
title = {Dynamic reconfiguration in self-adaptive systems considering non-functional properties},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231956},
doi = {10.1145/2245276.2231956},
abstract = {Self-adaptive systems have recently been receiving much attention because of their ability to cope with the changes of environment, failures, and unanticipated events. These systems need an adaptation mechanism, which automatically computes the possible configurations, and decides the most appropriate configuration to fit the environment. In particular, the satisfaction of non-functional requirements must be considered when selecting the best reconfiguration. However, there are trade-off problems among non-functional requirements. Moreover, the adaptation mechanisms are typically developed separately from the components to be implemented, and it complicates the construction of such systems. We propose (1) a feature-oriented analysis technique, which can identify adaptation points, and calculate the contribution to non-functional goals of the configuration; (2) a component specification model, which extends an architectural description language for self-adaptation; (3) a reconfiguration framework aimed to reduce the complexity of the reconfiguration and generate the best configuration at run-time. We evaluate the feasibility of our framework by four different scenarios, and show that our framework reduces the complexity of the reconfiguration, and solves the trade-off problem among non-functional requirements.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1144–1150},
numpages = {7},
keywords = {software architecture, self-adaptive systems, feature-oriented analysis, dynamic reconfiguration, architecture description language},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/3084226.3084253,
author = {Abrah\~{a}o, Silvia and Insfran, Emilio},
title = {Evaluating Software Architecture Evaluation Methods: An Internal Replication},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084253},
doi = {10.1145/3084226.3084253},
abstract = {Context: The size and complexity of software systems along with the demand for ensuring quality requirements have fostered the interest in software architecture evaluation methods. Although several empirical studies have been reported, the actual body of knowledge is still insufficient. To address this concern, we presented a family of four controlled experiments that compares a recently proposed method, the Quality-Driven Architecture Derivation and Improvement (QuaDAI) method against the well-known Architecture Tradeoff Analysis Method (ATAM).Objective: To provide further evidence on the efficiency, effectiveness, and perceived satisfaction of participants using these two software architecture evaluation methods. We report the results of a differentiated internal replication study.Method: The same materials used in the baseline experiments were employed in this replication but the participants were sixteen practitioners. In addition, we used a simpler design to reduce the treatments' application sequences.Results: The participants obtained architectures with better quality when applying QuaDAI, and they found this method to be more useful and likely to be used than ATAM, but no difference in terms of efficiency and perceived ease of use were found.Conclusions: The results are in line with the baseline experiments and support the hypothesis that QuaDAI achieve better results than ATAM when performing architectural evaluations; however, further work is need to improve the methods usability.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {144–153},
numpages = {10},
keywords = {Software Architecture Evaluation, Experiment Replication},
location = {Karlskrona, Sweden},
series = {EASE '17}
}

@inproceedings{10.1145/1629716.1629729,
author = {Liebig, J\"{o}rg and Apel, Sven and Lengauer, Christian and Leich, Thomas},
title = {RobbyDBMS: a case study on hardware/software product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629729},
doi = {10.1145/1629716.1629729},
abstract = {The development of a highly configurable data management system is a challenging task, especially if it is to be implemented on an embedded system that provides limited resources. We present a case study of such a data management system, called RobbyDBMS, and give it a feature-oriented design. In our case study, we evaluate the system's efficiency and variability. We pay particular attention to the interaction between the features of the data management system and the components of the underlying embedded platform. We also propose an integrated development process covering both hardware and software.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {63–68},
numpages = {6},
keywords = {FeatureC++, domain engineering, feature oriented software development, hardware product lines, software product lines},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/3461002.3473074,
author = {Fantechi, Alessandro and Gnesi, Stefania and Livi, Samuele and Semini, Laura},
title = {A spaCy-based tool for extracting variability from NL requirements},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473074},
doi = {10.1145/3461002.3473074},
abstract = {In previous work, we have shown that ambiguity detection in requirements can also be used as a way to capture latent aspects of variability. Natural Language Processing (NLP) tools have been used for a lexical analysis aimed at ambiguity indicators detection, and we have studied the necessary adaptations to those tools for pointing at potential variability, essentially by adding specific dictionaries for variability. We have identified also some syntactic rules able to detect potential variability, such as disjunction between nouns or pairs of indicators in a subordinate proposition. This paper describes a new prototype NLP tool, based on the spaCy library, specifically designed to detect variability. The prototype is shown to preserve the same recall exhibited by previously used lexical tools, with a higher precision.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {32–35},
numpages = {4},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342416,
author = {Rodriguez, Germania and P\'{e}rez, Jennifer and Benavides, David},
title = {Accessibility Variability Model: The UTPL MOOC Case Study},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342416},
doi = {10.1145/3307630.3342416},
abstract = {Several approaches to define Variability Models (VM) of non-functional requirements or quality attributes have been proposed. However, these approaches have focused on specific quality attributes rather than more general non-functional aspects established by standards such as ISO/IEC 25010 for software evaluation and quality. Thus, developing specific software products by selecting features and at the same time measuring the level of compliance with a standard/guideline is a challenge. In this work, we present the definition of an accessibility VM based on the web content accessibility guides (WCAG) 2.1 W3C recommendation, to obtain a quantitative measure to improve or construct specific SPL products that require to be accessibility-aware. This paper is specially focused on illustrating the experience of measuring the accessibility in a software product line (SPL) in order to check if it is viable measuring products and recommending improvements in terms of features before addressing the construction of accessibility-aware products. The adoption of the VM accessibility has been putted into practice through a pilot case study, the MOOC (Massive Open Online Course) initiative of the Universidad T\'{e}cnica Particular de Loja. The conduction of this pilot case study has allowed us to illustrate how it is possible to model and measure the accessibility in SPL using accessibility VM, as well as to recommend accessibility configuration improvements for the construction of new or updated MOOC platforms.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {114–121},
numpages = {8},
keywords = {software product lines, software development techniques, software creation and management, software and its engineering, reusability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/978-3-642-28714-5_13,
author = {Adam, Sebastian},
title = {Providing software product line knowledge to requirements engineers --- a template for elicitation instructions},
year = {2012},
isbn = {9783642287138},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-28714-5_13},
doi = {10.1007/978-3-642-28714-5_13},
abstract = {[Context &amp; Motivation] Developing new software systems based on a software product line (SPL) in so-called application engineering (AE) projects is still a time-consuming and expensive task. Especially when a large number of customer-specific requirements exists, there is still no systematic support for efficiently aligning these non-anticipated requirements with SPL characteristics early on. [Question/problem] In order to improve this process significantly, sound knowledge about an SPL must be available when guiding the requirements elicitation during AE. Thus, an appropriate reflection of SPL characteristics in process-supporting artifacts is indispensable for actually supporting a requirements engineer in this task. [Principal ideas/results] In this paper, a validated template for elicitation instructions that aims at providing a requirements engineer with knowledge about an underlying SPL in an appropriate manner is presented. This template consists of predefined text blocks and algorithms that explain how SPL-relevant product and process knowledge can be systematically reflected into capability-aware elicitation instructions. [Contribution] By using such elicitation instructions, requirements engineers are enabled to elicit requirements in an AE project more effectively.},
booktitle = {Proceedings of the 18th International Conference on Requirements Engineering: Foundation for Software Quality},
pages = {147–164},
numpages = {18},
location = {Essen, Germany},
series = {REFSQ'12}
}

@inproceedings{10.1145/3233027.3233048,
author = {Hayashi, Kengo and Aoyama, Mikio},
title = {A multiple product line development method based on variability structure analysis},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233048},
doi = {10.1145/3233027.3233048},
abstract = {This article proposes a multiple product line development method based on variability structure analysis. In product line development, the problem area is divided into the domain engineering and application engineering for delivering diverse products. Now, the development of automotive software requires to meet both agility and extreme diversity, which is a big challenge. We developed a structural analysis method of variability for multiple product lines using an extended model of OVM (Orthogonal Variability Model). Together with the variability analysis method, we propose an agile application development method to refine development items according to variability dependency based on the analysis, and develop them incrementally. We applied the proposed method to the development of the multiple product lines of automotive software systems, and demonstrated to reduce the volatility of the test efforts and usage of the test environment, and higher velocity and better manageability of the value stream.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {160–169},
numpages = {10},
keywords = {variability analysis, software product line, multiple product lines, automotive software, agile development},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2815782.2815799,
author = {Schaefer, Ina and Seidl, Christoph and Cleophas, Loek and Watson, Bruce W.},
title = {SPLicing TABASCO: Custom-Tailored Software Product Line Variants from Taxonomy-Based Toolkits},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815799},
doi = {10.1145/2815782.2815799},
abstract = {Taxonomy-Based Software Construction (TABASCO) applies extensive domain analyses to create conceptual hierarchies of algorithmic domains. Those are used as basis for the implementation of software toolkits. The monolithic structure of TABASCO-based toolkits restricts their adoption on resource-constrained or special-purpose devices. In this paper, we address this problem by applying Software Product Line (SPL) techniques to TABASCO-based toolkits: We use software taxonomies as input to creating a conceptual representation of variability as feature models of an SPL. We apply the variability realization mechanism delta modeling to transform realization artifacts, such as source code, to only contain elements for a particular selection of features. Our method is suitable for proactive, reactive and extractive SPL development so that it supports a seamless adoption and evolution of an SPL approach for TABASCO-based toolkits. We demonstrate the feasibility of the method with three case studies by proactively, reactively and extractively transforming TABASCO-based toolkits to SPLs, which allow derivation of variants with custom-tailored functionality.},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {34},
numpages = {10},
keywords = {Taxonomy-Based Software Construction (TABASCO) toolkit, Software Product Line (SPL) adoption},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.5555/2022115.2022120,
author = {Alf\'{e}rez, Mauricio and Lopez-Herrejon, Roberto E. and Moreira, Ana and Amaral, Vasco and Egyed, Alexander},
title = {Supporting consistency checking between features and software product line use scenarios},
year = {2011},
isbn = {9783642213465},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A key aspect for effective variability modeling of Software Product Lines (SPL) is to harmonize the need to achieve separation of concerns with the need to satisfy consistency of requirements and constraints. Techniques for variability modeling such as feature models used together with use scenarios help to achieve separation of stakeholders' concerns but ensuring their joint consistency is largely unsupported. Therefore, inconsistent assumptions about system's expected use scenarios and the way in which they vary according to the presence or absence of features reduce the models usefulness and possibly renders invalid SPL systems. In this paper we propose an approach to check consistency -- the verification of semantic relationships among the models -- between features and use scenarios that realize them. The novelty of this approach is that it is specially tailored for the SPL domain and considers complex composition situations where the customization of use scenarios for specific products depends on the presence or absence of sets of features. We illustrate our approach and supporting tools using variant constructs that specify how the inclusion of sets of variable features (that refer to uncommon requirements between products of a SPL) adapt use scenarios related to other features.},
booktitle = {Proceedings of the 12th International Conference on Top Productivity through Software Reuse},
pages = {20–35},
numpages = {16},
location = {Pohang, South Korea},
series = {ICSR'11}
}

@inproceedings{10.1145/3461002.3473949,
author = {Romero, David and Galindo, Jos\'{e} \'{A}. and Horcas, Jose-Miguel and Benavides, David},
title = {A first prototype of a new repository for feature model exchange and knowledge sharing},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473949},
doi = {10.1145/3461002.3473949},
abstract = {Feature models are the "de facto" standard for variability modelling and are used in both academia and industry. The MODEVAR initiative tries to establish a common textual feature modelling language that can be used by different communities and can allow information sharing. Feature model related researches use different models for different purposes such as analysis, sampling, testing, debugging, teaching, etc. Those models are shared in private repositories and there is a risk that all that knowledge is spread across different platforms which hinder collaboration and knowledge reuse. In this paper, we propose a first working version of a new feature model repository that allows to centralise the knowledge generated in the community together with advanced capabilities such as DOI generation, an API, analysis reports, among others. Our solution is a front end interface that uses the popular open science repository Zenodo as an end point to materialise the storage of all the information. Zenodo is enhanced with characteristics that facilitate the management of the models. The idea of our repository is to provide existing but also new features that are not present in other repositories (e.g., SPLOT). We propose to populate our repository with all the existing models of many sources including SPLOT.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {80–85},
numpages = {6},
keywords = {variability, requirements, feature model repository, characteristics},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414973,
author = {Schlie, Alexander and Kn\"{u}ppel, Alexander and Seidl, Christoph and Schaefer, Ina},
title = {Incremental feature model synthesis for clone-and-own software systems in MATLAB/Simulink},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414973},
doi = {10.1145/3382025.3414973},
abstract = {Families of related MATLAB/Simulink systems commonly emerge ad hoc using clone-and-own practices. Extractively migrating systems towards a software product line (SPL) can be a remedy. A feature model (FM) represents all potential configurations of an SPL, ideally, in non-technical domain terms. However, yielding a sensible FM from automated synthesis remains a major challenge due to domain knowledge being a prerequisite for features to be adequate abstractions. In incremental reverse engineering, subsequent generation of FMs may further overwrite changes and design decisions made during previous manual FM refinement.In this paper, we propose an approach to largely automate the synthesis of a suitable FM from a set of cloned MATLAB/Simulink models as part of reverse engineering an SPL. We fully automate the extraction of an initial, i.e., a technical, FM that closely aligns with realization artifacts and their variability, and further provide operations to manually refine it to incorporate domain knowledge. Most importantly, we provide concepts to capture such operations and to replay them on a structurally different technical FM stemming from a subsequent reverse engineering increment that included further systems of the portfolio. We further provide an implementation and demonstrate the feasibility of our approach using two MATLAB/Simulink data sets from the automotive domain.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {7},
numpages = {12},
keywords = {variability, synthesis, refinement, mapping, individual, incremental, feature model, clone-and-own, MATLAB/Simulink, 150% model},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1007/s00766-018-0307-0,
author = {Reinhartz-Berger, Iris and Kemelman, Mark},
title = {Extracting core requirements for software product lines},
year = {2020},
issue_date = {Mar 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-018-0307-0},
doi = {10.1007/s00766-018-0307-0},
abstract = {Software Product Line Engineering (SPLE) is a promising paradigm for reusing knowledge and artifacts among similar software products. However, SPLE methods and techniques require a high up-front investment and hence are profitable if several similar software products are developed. Thus in practice adoption of SPLE commonly takes a bottom-up approach, in which analyzing the commonality and variability of existing products and transforming them into reusable ones (termed core assets) are needed. These time-consuming and error-prone tasks call for automation. The literature partially deals with solutions for early software development stages, mainly in the form of variability analysis. We aim for further creation of core requirements—reusable requirements that can be adapted for different software products. To this end, we introduce an automated extractive method, named CoreReq, to generate core requirements from product requirements written in a natural language. The approach clusters similar requirements, captures variable parts utilizing natural language processing techniques, and generates core requirements following an ontological variability framework. Focusing on cloning scenarios, we evaluated CoreReq through examples and a controlled experiment. Based on the results, we claim that core requirements generation with CoreReq is feasible and usable for specifying requirements of new similar products in cloning scenarios.},
journal = {Requir. Eng.},
month = mar,
pages = {47–65},
numpages = {19},
keywords = {Variability analysis, Requirements specification, Systematic reuse, Software Product Line Engineering}
}

@inproceedings{10.1145/3461002.3473944,
author = {Ballesteros, Joaqu\'{\i}n and Fuentes, Lidia},
title = {Transfer learning for multiobjective optimization algorithms supporting dynamic software product lines},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473944},
doi = {10.1145/3461002.3473944},
abstract = {Dynamic Software Product Lines (DSPLs) are a well-accepted approach for self-adapting Cyber-Physical Systems (CPSs) at run-time. The DSPL approaches make decisions supported by performance models, which capture system features' contribution to one or more optimization goals. Combining performance models with Multi-Objectives Evolutionary Algorithms (MOEAs) as decision-making mechanisms is common in DSPLs. However, MOEAs algorithms start solving the optimization problem from a randomly selected population, not finding good configurations fast enough after a context change, requiring too many resources so scarce in CPSs. Also, the DSPL engineer must deal with the hardware and software particularities of the target platform in each CPS deployment. And although each system instantiation has to solve a similar optimization problem of the DSPL, it does not take advantage of experiences gained in similar CPS. Transfer learning aims at improving the efficiency of systems by sharing the previously acquired knowledge and applying it to similar systems. In this work, we analyze the benefits of transfer learning in the context of DSPL and MOEAs testing on 8 feature models with synthetic performance models. Results are good enough, showing that transfer learning solutions dominate up to 71% of the non-transfer learning ones for similar DSPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {51–59},
numpages = {9},
keywords = {transfer learning, self-adaptation, multiobjective optimization algorithms, dynamic software product lines, cyber-physical systems},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414961,
author = {Favalli, Luca and K\"{u}hn, Thomas and Cazzola, Walter},
title = {Neverlang and FeatureIDE just married: integrated language product line development environment},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414961},
doi = {10.1145/3382025.3414961},
abstract = {Language development is inherently complex. With the support of a suitable language development environment most computer scientists could develop their own domain-specific language (DSL) with relative ease. Yet, when the DSL is the result of a configuration over a language product line (LPL)---a special software product line (SPL) of compilers/interpreters and corresponding IDE services---they fail to provide adequate support. An environment for LPL engineering should facilitate the underlying process involving three distinct roles: a language engineer developing the LPL, a language deployer configuring a language product, and a language user using the language product. Neither IDEs nor SPLE environments can cater all three roles and fully support the LPL engineering process with distributed, incremental development, configuration, and deployment of language variants. In this paper, we present an LPL engineering process for the distributed, incremental development of LPLs and an integrated language product line development environment supporting this process, catering the three roles, and ensuring the consistency among all artifacts of the LPL: language components implementing a language feature, the feature model, language configurations and the resulting language products. To create such an environment, we married the Neverlang language workbench and AiDE its LPL engineering environment with the FeatureIDE SPL engineering environment. While Neverlang supports the development of LPLs and deployment of language products, AiDE generates the feature model for the LPL under development, whereas FeatureIDE handles the feature configuration. We illustrate the applicability of the LPL engineering process and the suitability of our development environment for the three roles by showcasing its application for teaching programming with a growable language. In there, an LPL for Javascript was developed/refactored, 15 increasingly complex language products were configured/updated and finally deployed.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {33},
numpages = {11},
keywords = {neverlang, language product lines, domain specific languages},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382026.3431252,
author = {Michelon, Gabriela Karoline},
title = {Evolving System Families in Space and Time},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431252},
doi = {10.1145/3382026.3431252},
abstract = {Managing the evolution of system families in space and time, i.e., system variants and their revisions is still an open challenge. The software product line (SPL) approach can support the management of product variants in space by reusing a common set of features. However, feature changes over time are often necessary due to adaptations and/or bug fixes, leading to different product versions. Such changes are commonly tracked in version control systems (VCSs). However, VCSs only deal with the change history of source code, and, even though their branching mechanisms allow to develop features in isolation, VCS does not allow propagating changes across variants. Variation control systems have been developed to support more fine-grained management of variants and to allow tracking of changes at the level of files or features. However, these systems are also limited regarding the types and granularity of artifacts. Also, they are cognitively very demanding with increasing numbers of revisions and variants. Furthermore, propagating specific changes over variants of a system is still a complex task that also depends on the variability-aware change impacts. Based on these existing limitations, the goal of this doctoral work is to investigate and define a flexible and unified approach to allow an easy and scalable evolution of SPLs in space and time. The expected contributions will aid the management of SPL products and support engineers to reason about the potential impact of changes during SPL evolution. To evaluate the approach, we plan to conduct case studies with real-world SPLs.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {104–111},
numpages = {8},
keywords = {version control systems, software product lines, software evolution, feature-oriented software development},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3471144,
author = {Uta, Mathias and Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Tran, Thi Ngoc Trang and Helic, Denis},
title = {Evaluating recommender systems in feature model configuration},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471144},
doi = {10.1145/3461001.3471144},
abstract = {Configurators can be evaluated in various ways such as efficiency and completeness of solution search, optimality of the proposed solutions, usability of configurator user interfaces, and configuration consistency. Due to the increasing size and complexity of feature models, the integration of recommendation algorithms with feature model configurators becomes relevant. In this paper, we show how the output of a recommender system can be evaluated within the scope of feature model configuration scenarios. Overall, we argue that the discussed ways of measuring recommendation quality help developers to gain a broader view on evaluation techniques in constraint-based recommendation domains.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {58–63},
numpages = {6},
keywords = {recommender systems, feature models, evaluation, configuration},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2364412.2364419,
author = {Asadi, Mohsen and Bagheri, Ebrahim and Mohabbati, Bardia and Ga\v{s}evi\'{c}, Dragan},
title = {Requirements engineering in feature oriented software product lines: an initial analytical study},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364419},
doi = {10.1145/2364412.2364419},
abstract = {Requirements engineering is recognized as a critical stage in software development lifecycle. Given the nature of Software Product Lines (SPL), the importance of requirements engineering is more pronounced as SPLs pose more complex challenges than development of a 'single' product. Several methods have been proposed in the literature, which encompass activities for capturing requirements, their variability and commonality. To investigate the maturity and effectiveness of the current requirements engineering approaches in software product lines, we develop an evaluation framework containing a set of evaluation criteria and assess feature oriented requirements engineering methods based on the proposed criteria. As a result of this initial study, we find out the majority of approaches lacks proper techniques for supporting the validation of family requirements models as well as dealing with delta requirements. Additionally, capturing stakeholders' preferences and applying them during the course of software feature configuration have not been taken into account and addressed in the proposed approaches.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {36–44},
numpages = {9},
keywords = {software product line engineering, software engineering, requirements engineering, evaluation criteria},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2648511.2648550,
author = {Dillon, Michael and Rivera, Jorge and Darbin, Rowland},
title = {A methodical approach to product line adoption},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648550},
doi = {10.1145/2648511.2648550},
abstract = {The evolution of the U.S. Army's Live Training Transformation (LT2) product line of combat training systems, including the move by the Army to consolidate management of the product line under a single contracting team, has provided a natural experiment that validates the hypothesis that product line engineering practices are more effective than traditional software engineering practices, and has demonstrated which product line adoption approaches are more successful than others. By analyzing this natural experiment, the product line team has been able to apply a methodical approach to product line adoption across the development organization and successfully adopt second generation product line processes. This paper explores that methodical approach. It will enumerate the steps that led to successes and explore the contributing factors and unintended consequences of failures along the way. Additionally this paper will explore how this approach is being employed to extend the LT2 product line beyond software.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {340–349},
numpages = {10},
keywords = {variation points, software product lines, second generation product line engineering, product portfolio, product line governance, product line engineering, product line adoption, product configurator, product baselines, feature profiles, feature modeling, feature constraints hierarchical product lines, bill-of-features},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3307630.3342398,
author = {Beek, Maurice H. ter and Schmid, Klaus and Eichelberger, Holger},
title = {Textual Variability Modeling Languages: An Overview and Considerations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342398},
doi = {10.1145/3307630.3342398},
abstract = {During the three decades since the invention of the first variability modeling approach [28], there have been multiple attempts to introduce advanced variability modeling capabilities. More recently, we have seen increased attention on textual variability modeling languages. In this paper, we summarize the main capabilities of state of the art textual variability modeling languages, based on [23], including updates regarding more recent work. Based on this integrated characterization, we provide a discussion of additional concerns, opportunities and challenges that are relevant for designing future (textual) variability modeling languages. The paper also summarizes relevant contributions by the authors as input to further discussions on future (textual) variability modeling languages.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {151–157},
numpages = {7},
keywords = {variability modeling, textual specification languages, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/1629716.1629738,
author = {Alf\'{e}rez, Mauricio and Moreira, Ana and Kulesza, Uir\'{a} and Ara\'{u}jo, Jo\~{a}o and Mateus, Ricardo and Amaral, Vasco},
title = {Detecting feature interactions in SPL requirements analysis models},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629738},
doi = {10.1145/1629716.1629738},
abstract = {The consequences of unwanted feature interactions in a Software Product Line (SPL) can range from minor problems to critical software failures. However, detecting feature interactions in reasonably complex model-based SPLs is a non-trivial task. This is due to the often large number of interdependent models that describe the SPL features and the lack of support for analyzing the relationships inside those models. We believe that the early detection of the points, where two or more features interact --- based on the models that describe the behavior of the features ---, is a starting point for the detection of conflicts and inconsistencies between features, and therefore, take an early corrective action.This vision paper foresees a process to find an initial set of points where it is likely to find potential feature interactions in model-based SPL requirements, by detecting: (i) dependency patterns between features using use case models; and (ii) overlapping between use case scenarios modeled using activity models.We focus on requirements models, which are special, since they do not contain many details about the structural components and the interactions between the higher-level abstraction modules of the system. Therefore, use cases and activity models are the means that help us to analyze the functionality of a complex system looking at it from a high level end-user view to anticipate the places where there are potential feature interactions. We illustrate the approach with a home automation SPL and then discuss about its applicability.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {117–123},
numpages = {7},
keywords = {feature interactions, software product lines requirements},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/3307630.3342397,
author = {Shaaban, Abdelkader Magdy and Gruber, Thomas and Schmittner, Christoph},
title = {Ontology-Based Security Tool for Critical Cyber-Physical Systems},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342397},
doi = {10.1145/3307630.3342397},
abstract = {Industry 4.0 considers as a new advancement concept of the industrial revolution, which introduces a full utilization of Internet technologies. This concept aims to combine diverse technological resources into the industry field, which enables the communication between two worlds: the physical and the cyber one. Cyber-physical Systems are one of the special forces that integrate and build a variety of existing technologies and components. The diversity of components and technologies creates new security threats that can exploit vulnerabilities to attack a critical system. This work introduces an ontology-based security tool-chain able to be integrated with the initial stages of the development process of critical systems. The tool detects the potential threats, and apply the suitable security requirements which can address these threats. Eventually, it uses the ontology approach to ensure that the security requirements are fulfilled.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {207–210},
numpages = {4},
keywords = {threats, security, ontology, cyber-physical system},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233049,
author = {Horcas, Jose-Miguel and Corti\~{n}as, Alejandro and Fuentes, Lidia and Luaces, Miguel R.},
title = {Integrating the common variability language with multilanguage annotations for web engineering},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233049},
doi = {10.1145/3233027.3233049},
abstract = {Web applications development involves managing a high diversity of files and resources like code, pages or style sheets, implemented in different languages. To deal with the automatic generation of custom-made configurations of web applications, industry usually adopts annotation-based approaches even though the majority of studies encourage the use of composition-based approaches to implement Software Product Lines. Recent work tries to combine both approaches to get the complementary benefits. However, technological companies are reticent to adopt new development paradigms such as feature-oriented programming or aspect-oriented programming. Moreover, it is extremely difficult, or even impossible, to apply these programming models to web applications, mainly because of their multilingual nature, since their development involves multiple types of source code (Java, Groovy, JavaScript), templates (HTML, Markdown, XML), style sheet files (CSS and its variants, such as SCSS), and other files (JSON, YML, shell scripts). We propose to use the Common Variability Language as a composition-based approach and integrate annotations to manage fine grained variability of a Software Product Line for web applications. In this paper, we (i) show that existing composition and annotation-based approaches, including some well-known combinations, are not appropriate to model and implement the variability of web applications; and (ii) present a combined approach that effectively integrates annotations into a composition-based approach for web applications. We implement our approach and show its applicability with an industrial real-world system.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {196–207},
numpages = {12},
keywords = {web engineering, variability, composition, automation, annotations, SPL, CVL},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2019136.2019149,
author = {Murugesupillai, Esan and Mohabbati, Bardia and Ga\v{s}evi\'{c}, Dragan},
title = {A preliminary mapping study of approaches bridging software product lines and service-oriented architectures},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019149},
doi = {10.1145/2019136.2019149},
abstract = {Service Oriented Architectures (SOA) and Software Product Lines (SPL) have individually proven to be software engineering concepts that create added value to the development of software systems. Recently, the research community has recognized and investigated potentials for combining these two concepts. However, there have been no mapping study and literature surveys that systematically review the present research results in combining the two. This paper presents results of a preliminary work on a systematic mapping study of research papers that report on combining SOA and SPL. The main goal of a systematic mapping study is to provide a breath overview, classification of approaches and the quantity and type of research as well as available research results, which is complimentary step toward further systematic literature review. This paper, based on selected papers published from 2002 to mid-2010, reports on various aspects of the analyzed literature, including the motivations for combining the two concepts; contributions to specific stages of software engineering lifecycles; types of synergies and characteristics that are accomplished through combinations of the two concepts; and the methods used for and the rigor of the evaluations of the research conducted on the studied topic.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {11},
numpages = {8},
keywords = {variability management, software variability, software product line, service-oriented product line, service-oriented architecture},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2791060.2791108,
author = {Berger, Thorsten and Lettner, Daniela and Rubin, Julia and Gr\"{u}nbacher, Paul and Silva, Adeline and Becker, Martin and Chechik, Marsha and Czarnecki, Krzysztof},
title = {What is a feature? a qualitative study of features in industrial software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791108},
doi = {10.1145/2791060.2791108},
abstract = {The notion of features is commonly used to describe the functional and non-functional characteristics of a system. In software product line engineering, features often become the prime entities of software reuse and are used to distinguish the individual products of a product line. Properly decomposing a product line into features, and correctly using features in all engineering phases, is core to the immediate and long-term success of such a system. Yet, although more than ten different definitions of the term feature exist, it is still a very abstract concept. Definitions lack concrete guidelines on how to use the notion of features in practice.To address this gap, we present a qualitative empirical study on actual feature usage in industry. Our study covers three large companies and an in-depth, contextualized analysis of 23 features, perceived by the interviewees as typical, atypical (outlier), good, or bad representatives of features. Using structured interviews, we investigate the rationales that lead to a feature's perception, and identify and analyze core characteristics (facets) of these features. Among others, we find that good features precisely describe customer-relevant functionality, while bad features primarily arise from rashly executed processes. Outlier features, serving unusual purposes, are necessary, but do not require the full engineering process of typical features.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {16–25},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.5555/1647636.1647649,
author = {Hara, Akihiko and Takada, Shingo},
title = {Requirements analysis using similar sequence diagrams},
year = {2007},
isbn = {9780889867062},
publisher = {ACTA Press},
address = {USA},
abstract = {Requirements analysis is a very important phase in software development. Unfortunately, it is very difficult to make explicit all requirements because developers and customers may not really know what they want. Development cost will become higher if the number of unknown and implicit behaviors is large, because developers will need to repeatedly modify the analysis and design models. This paper proposes an approach which suggests to the developer various behaviors, in the form of sequence diagrams, that the application may have but is not currently included. Our approach is implemented in a tool which searches a database of sequence diagrams for previously developed applications. Developers search the database for "similar" sequence diagrams based on the diagrams they have already developed.},
booktitle = {Proceedings of the 11th IASTED International Conference on Software Engineering and Applications},
pages = {66–71},
numpages = {6},
keywords = {similarity, sequence diagrams, requirements analysis, category},
location = {Cambridge, Massachusetts},
series = {SEA '07}
}

@inproceedings{10.1145/2499777.2500710,
author = {Gabillon, Yoann and Biri, Nicolas and Otjacques, Beno\^{\i}t},
title = {Methodology to integrate multi-context UI variations into a feature model},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500710},
doi = {10.1145/2499777.2500710},
abstract = {Software product line (SPL) paradigm aims to explore commonalities and variabilities in a set of applications for developing an efficient derivation of products. One of the most common ways to model variability in this paradigm is to use a Feature Model. However, variability in SPL is often limited to functional features. The User Interface (UI) variations are modeled as entire UIs and thus these variations are not reusable and inspectable. Research in the Human Computer Interaction (HCI) field has proven the importance of variability for non functional, purely UI centric features. The HCI community has proposed several levels of abstraction for multi-context UI design. Indeed, new variations can be introduced at each abstraction level. UI designers are used to them and they usually introduce variability at each step of the UI definition without using SPL. To build usable softwares that take into account UI, we propose to merge functional concerns and UI concerns, providing a methodology to integrate variability of both aspects into a single Feature Model.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {74–81},
numpages = {8},
keywords = {variability, user interface, usability, software product line, multi-context, feature model, abstraction levels},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.5555/998675.999419,
author = {Matinlassi, Mari},
title = {Comparison of Software Product Line Architecture Design Methods: COPA, FAST, FORM, KobrA and QADA},
year = {2004},
isbn = {0769521630},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Product line architectures (PLAs) have been undercontinuous attention in the software research communityduring the past few years. Although several methods havebeen established to create PLAs there are not availablestudies comparing PLA methods. Five methods are knownto answer the needs of software product lines: COPA,FAST, FORM, KobrA and QADA. In this paper, anevaluation framework is introduced for comparing PLAdesign methods. The framework considers the methodsfrom the points of view of method context, user, structureand validation. Comparison revealed distinguishableideologies between the methods. Therefore, methods donot overlap even though they all are PLA design methods.All the methods have been validated on various domains.The most common domains are telecommunicationinfrastructure and information domains. Some of themethods apply software standards; at least OMG\'{y}s MDAfor method structures, UML for language and IEEE Std-1471-2000 for viewpoint definitions.},
booktitle = {Proceedings of the 26th International Conference on Software Engineering},
pages = {127–136},
numpages = {10},
series = {ICSE '04}
}

@inproceedings{10.1145/2647648.2647649,
author = {Raschke, Wolfgang and Zilli, Massimiliano and Loinig, Johannes and Weiss, Reinhold and Steger, Christian and Kreiner, Christian},
title = {Embedding research in the industrial field: a case of a transition to a software product line},
year = {2014},
isbn = {9781450330459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647648.2647649},
doi = {10.1145/2647648.2647649},
abstract = {Java Cards [4, 5] are small resource-constrained embedded systems that have to fulfill rigorous security requirements. Multiple application scenarios demand diverse product performance profiles which are targeted towards markets such as banking applications and mobile applications. In order to tailor the products to the customer's needs we implemented a Software Product Line (SPL). This paper reports on the industrial case of an adoption to a SPL during the development of a highly-secure software system. In order to provide a scientific method which allows the description of research in the field, we apply Action Research (AR). The rationale of AR is to foster the transition of knowledge from a mature research field to practical problems encountered in the daily routine. Thus, AR is capable of providing insights which might be overlooked in a traditional research approach. In this paper we follow the iterative AR process, and report on the successful transfer of knowledge from a research project to a real industrial application.},
booktitle = {Proceedings of the 2014 International Workshop on Long-Term Industrial Collaboration on Software Engineering},
pages = {3–8},
numpages = {6},
keywords = {software reuse, knowledge transfer, action research},
location = {Vasteras, Sweden},
series = {WISE '14}
}

@article{10.1016/j.jss.2006.05.034,
author = {Ajila, Samuel A. and Dumitrescu, Razvan T.},
title = {Experimental use of code delta, code churn, and rate of change to understand software product line evolution},
year = {2007},
issue_date = {January, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {80},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2006.05.034},
doi = {10.1016/j.jss.2006.05.034},
abstract = {This research is a longitudinal study of change processes. It links changes in the product line architecture of a large telecommunications equipment supplier with the company's customers, inner context, and eight line card products over six-year period. There are three important time related constructs in this study: the time it takes to develop a new product line release; the frequency in which a metric is collected; and the frequency at which financial results and metrics related to the customer layer are collected and made available. Data collection has been organized by product release. The original goal of this research is to study the economic impact of market reposition on the product line and identify metrics that can be used to records changes in product line. We later look at the product line evolution vis-a-vis the changes in the products that form the product line. Our results show that there is no relationship between the size of the code added to the product line and the number of designers required to develop and test it; and there is a positive relationship between designer turnover and impact of change.},
journal = {J. Syst. Softw.},
month = jan,
pages = {74–91},
numpages = {18},
keywords = {Software product line, Software metric, Software evolution, Rate of change, Impact analysis, Code delta, Code churn}
}

@inproceedings{10.1145/2934466.2934490,
author = {Iida, Takahiro and Matsubara, Masahiro and Yoshimura, Kentaro and Kojima, Hideyuki and Nishino, Kimio},
title = {PLE for automotive braking system with management of impacts from equipment interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934490},
doi = {10.1145/2934466.2934490},
abstract = {We report here an industrial application of the Product Line Engineering (PLE) for the development of electronic braking systems.The cost of software engineering in automotive control systems is increasing as new functions for safety, comfort, and improved fuel efficiency are integrated into electronic control units.Therefore, Component suppliers for automotive control systems adapt their products to the requirements of car manufacturers by modifying the software specifications, such that it makes minimal changes to the mechanical structure and the electrical and electronic (E/E) components hence reduces the cost. PLE is an effective approach to manage or even reduce the software variations resulting from these modifications.However, one problem is that the software specifications of automotive control systems need to be redesigned after system testing with vehicles. This is because vehicles consist of many mechanical parts manufactured by different suppliers, and the characteristics of the parts can interact with each other. This problem makes it difficult to reap the full benefits of PLE.We propose an approach to analyze the potential impact from such interactions by using a system model that expresses the system architecture that includes the parts of different suppliers. Based on this model, the software architecture was designed to localize the impact to several software components. Additionally, a feature model was designed to the enable management of the localized impact by expressing it as variability. This method helps software engineers specify the software components that can have an effect on the actual equipment, and determine which modifications to the software specifications are necessary.We applied PLE with the proposed method in the development of electronic brake control system. We confirmed that our approach greatly increased the efficiency of PLE for the development of such automotive control systems.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {232–241},
numpages = {10},
keywords = {system modeling, software modification, software engineering, product lines engineering, feature modeling, control system},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/1363686.1363832,
author = {Kaiya, Haruhiko and Sato, Tomonori and Osada, Akira and Kitazawa, Naoyuki and Kaijiri, Kenji},
title = {Toward quality requirements analysis based on domain specific quality spectrum},
year = {2008},
isbn = {9781595937537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1363686.1363832},
doi = {10.1145/1363686.1363832},
abstract = {It is difficult to identify whether quality requirements are defined adequately or not, but there are few methods to support this kind of requirements analysis. In this paper, we propose a method based on software quality spectrum, that shows a ratio of quality characteristics embedded in a software engineering artifact, such as a requirements specification, a manual and so on. We assume similar kinds of software systems have similar spectrum, thus we can identify the adequacy of quality requirements for a new system by using spectrum of already existing similar systems. We confirmed the assumption above by analyzing actual software systems, i.e., web browsers and drawing tools.},
booktitle = {Proceedings of the 2008 ACM Symposium on Applied Computing},
pages = {596–601},
numpages = {6},
keywords = {requirements analysis, quality requirements, non-functional requirements},
location = {Fortaleza, Ceara, Brazil},
series = {SAC '08}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {software product lines, self-configuration, runtime decision-making, recommender systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3236405.3236407,
author = {Ghofrani, Javad and Fehlhaber, Anna Lena},
title = {ProductlinRE: online management tool for requirements engineering of software product lines},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236407},
doi = {10.1145/3236405.3236407},
abstract = {The lack of online tools for managing various artifacts of software product lines is problematic, and stands in contradiction to findings about the need to support collaboration. In this paper, we present ProductLinRE, a web application allowing product line engineers to work cooperatively on artifacts of requirements engineering for software product lines. Our proposed online tool allows distributed teamwork, using a tracking mechanism for projects, artifacts and features while tailoring the requirements artifacts according to the selected features.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {17–22},
numpages = {6},
keywords = {software product lines, requirements engineering, online tools},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2362536.2362546,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {A systematically conducted literature review: quality attribute variability in software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362546},
doi = {10.1145/2362536.2362546},
abstract = {Typically, products in a software product line differ by their functionality, and quality attributes are not intentionally varied. Why, how, and which quality attributes to vary has remained an open issue. A systematically conducted literature review on quality attribute variability is presented, where primary studies are selected by reading all content of full studies in Software Product Line Conference. The results indicate that the success of feature modeling influences the proposed approaches, different approaches suit specific quality attributes differently, and empirical evidence on industrial quality variability is lacking.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {41–45},
numpages = {5},
keywords = {variability, systematic literature review, quality attribute},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {smart production, smart product, smart factory, product line of factories, product and production configuration},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791111,
author = {Cordy, Maxime and Davril, Jean-Marc and Greenyer, Joel and Gressi, Erika and Heymans, Patrick},
title = {All-at-once-synthesis of controllers from scenario-based product line specifications},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791111},
doi = {10.1145/2791060.2791111},
abstract = {Software-intensive systems often consist of multiple components that interact to realize complex requirements. An additional dimension of complexity arises when one designs many variants of a system at once, that is, a software product line (SPL). We propose a scenario-based approach to design SPLs, based on a combination of Modal Sequence Diagrams (MSDs) and a feature model. It consists in associating every MSD to the set of variants that have to satisfy its specification. Variability constitutes a new source of complexity, which can lead to inconsistencies in the specification of one or multiple variants. It is therefore crucial to detect these inconsistencies, and to produce a controller for each variant that makes it behave so that it satisfies its specification. We present a new controller synthesis technique that checks the absence of inconsistencies in all variants at once, thereby more radically exploiting the similarities between them. Our method first translates the MSD specification into a variability-aware B\"{u}chi game, and then solves this game for all variants in a single execution. We implemented the approach in ScenarioTools, a software tool which we use to evaluate our algorithms against competing methods.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {26–35},
numpages = {10},
keywords = {message sequence diagrams, features, controller synthesis},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3236405.3236411,
author = {Raatikainen, Mikko and Tiihonen, Juha and M\"{a}nnist\"{o}, Tomi and Felfernig, Alexander and Stettinger, Martin and Samer, Ralph},
title = {Using a feature model configurator for release planning},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236411},
doi = {10.1145/3236405.3236411},
abstract = {The requirements for a system have many dependencies that can be expressed in the individual requirements managed in an issue tracker or a requirements management system. However, managing the entire body of requirements taking into account all complex dependencies is not well supported. We describe how a feature model based configurator can be used as a tool to help manage requirements data. Data transfer and constructing the needed requirements model can be carried out automatically by relying on a model generator. We implemented a prototype tool for requirements and release management that utilizes a knowledge-based configurator.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {29–33},
numpages = {5},
keywords = {requirements engineering, release management, feature modeling},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1007/s10664-004-6190-y,
author = {Svahnberg, Mikael and Wohlin, Claes},
title = {An Investigation of a Method for Identifying a Software Architecture Candidate with Respect to Quality Attributes},
year = {2005},
issue_date = {April     2005},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {10},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-004-6190-y},
doi = {10.1007/s10664-004-6190-y},
abstract = {To sustain the qualities of a software system during evolution, and to adapt the quality attributes as the requirements evolve, it is necessary to have a clear software architecture that is understood by all developers and to which all changes to the system adheres. This software architecture can be created beforehand, but must also be updated to reflect changes in the domain, and hence the requirements of the software. The choice of which software architecture to use is typically based on informal decisions. There exist, to the best of our knowledge, little factual knowledge of which quality attributes are supported or obstructed by different architecture approaches. In this paper we present an empirical study of a method that enables quantification of the perceived support different software architectures give for different quality attributes. This in turn enables an informed decision of which architecture candidate best fit the mixture of quality attributes required by a system being designed.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {149–181},
numpages = {33},
keywords = {quality attributes, analytic hierarchy process, Software architectures}
}

@inproceedings{10.1145/2019136.2019153,
author = {Nascimento, Amanda S\'{a}vio and Rubira, Cec\'{\i}lia Mary Fischer and Lee, Jaejoon},
title = {An SPL approach for adaptive fault tolerance in SOA},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019153},
doi = {10.1145/2019136.2019153},
abstract = {It is challenging to apply existing fault tolerance strategies for developing dependable Service Oriented systems, due to the lack of capabilities to adapt themselves at runtime to cope with dynamic changes of (a) user requirements and (b) the level of quality of services (QoS). In order to support such dynamic changes, we propose to adopt Software Product Line techniques. In particular, we adopt a feature model and product line architecture to capture the variability among software fault tolerance strategies based on design diversity. We propose an infrastructure that supports the strategy changes at runtime through dynamic management of variability and is responsible for the dependable mediation logic between service clients and redundant services. The infrastructure has an autonomous controller (i.e. managing a loop of collection, analysis, planning and execution), which is responsible for monitoring the changes of (i) QoS level and (ii) user requirements and decides, in accordance with high-level policies, an appropriate fault tolerance strategy to be executed. Also, our solution allows the dynamic provision of redundant services by describing them in terms of semantics. Finally, we performed a proof of concept which indicates the feasibility of the proposed solution.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {15},
numpages = {8},
keywords = {semantic services, dynamic software product line, dependability improvement, autonomic control loop, QoS-aware},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {variability modeling, software testing, software product lines, machine learning, constraints and variability mining},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3109729.3109746,
author = {Estrada-Torres, Bedilia},
title = {Improve Performance Management in Flexible Business Processes},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109746},
doi = {10.1145/3109729.3109746},
abstract = {The performance of business processes is evaluated and monitored with the aim of identifying whether strategic and operational goals are being achieved. Most approaches about performance measurement have been defined over traditional highly repetitive and well-structured processes. However, current organizational and business needs have encouraged the appearance of customizable processes to manage collections of process variants derived from a process, and loosely specified processes to manage non-repeatable and unpredictable processes. However, current techniques of performance measurement have not evolved to the same pace that business processes, thus generating a gap between processes and the measurement of their performance. The thesis introduced in this paper, is focused on enhancing the performance measurement of business processes by means of the improvement of existing techniques for the definition of process performance indicators and their applicability to different types of processes. With this purpose a set of artifacts, including a metamodel, notations, tools and methodologies will be developed. They will be validated by means of case studies based on real scenarios.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {145–149},
numpages = {5},
keywords = {performance indicators, flexible processes, Business process management},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1109/MISE.2009.5069898,
author = {Nguyen, Quyen L.},
title = {Non-functional requirements analysis modeling for software product lines},
year = {2009},
isbn = {9781424437221},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MISE.2009.5069898},
doi = {10.1109/MISE.2009.5069898},
abstract = {In most IT projects, software developers usually pay attention to functional requirements that satisfy business needs of the system. Non-functional requirements (NFR) such as performance, usability, security, etc. are usually handled ad-hoc during the system testing phase, when it is late and costly to fix problems. Due to the importance and criticality of NFR, I study the problem of modeling NFR for Software Product Lines (SPL), which adds yet an additional dimension of complexity. This paper will survey the software engineering literature, in search of a systematic way to analyze and design NFR, from the perspectives of the concept of commonality and variability of SPL and the characteristics of NFR. Finally, I will propose a methodology based on the extension of Product Line UML-Based Software Engineering (PLUS) techniques, for a unified and automated method to model NFR throughout all phases of SPL engineering.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Modeling in Software Engineering},
pages = {56–61},
numpages = {6},
series = {MISE '09}
}

@inproceedings{10.1145/2647908.2655977,
author = {El Yamany, Ahmed Eid and Shaheen, Mohamed and Sayyad, Abdel Salam},
title = {OPTI-SELECT: an interactive tool for user-in-the-loop feature selection in software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655977},
doi = {10.1145/2647908.2655977},
abstract = {Opti-Select is an Interactive Multi-objective feature analysis and optimization tool for software product lines configuration and feature models optimization based on an innovative UIL (User-In-the-loop) idea. In this tool, the experience of system analysts and stakeholders are merged with optimization techniques and algorithms.Opti-Select interactive tool is an integrated set of techniques providing step by step feature model and attribute configuration, selecting and excluding features, solution set optimization, and user interaction utilities that can all together reach satisfactory set of solutions that fits stakeholder preferences.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {126–129},
numpages = {4},
keywords = {user-in-the-loop (UIL), software product lines, search-based software engineering, product line engineering, optimal variant, optimal feature selection, multi-objective optimization, modeling, features, feature models, feature modeling, exploration, Pareto front visualization},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3233027.3233046,
author = {Beek, Maurice H. ter and Fantechi, Alessandro and Gnesi, Stefania},
title = {Product line models of large cyber-physical systems: the case of ERTMS/ETCS},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233046},
doi = {10.1145/3233027.3233046},
abstract = {A product line perspective may help to understand the possible variants in interactions between the subsystems of a large, cyber-physical system. This observation is exemplified in this paper by proposing a feature model of the family of ERTMS/ETCS train control systems and their foreseen extensions. This model not only shows the different components that have to be installed when deploying the system at the different levels established by the ERTMS/ETCS standards, but it also helps to identify and discuss specific issues, such as the borders between onboard and wayside equipment, different manufacturers of the subsystems, interoperability among systems developed at different levels, backward compatibility of trains equipped with higher level equipment running on lines equipped with lower level equipment, and evolution towards future trends of railway signalling. The feature model forms the basis for formal modelling of the behaviour of the critical components of the system and for evaluating the overall cost, effectiveness and sustainability, for example by adding cost and performance attributes to the feature model.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {208–214},
numpages = {7},
keywords = {variability, product lines, feature models, cyber-physical systems, ERTMS/ETCS train control systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2934466.2934476,
author = {Eichelberger, Holger and Qin, Cui and Sizonenko, Roman and Schmid, Klaus},
title = {Using IVML to model the topology of big data processing pipelines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934476},
doi = {10.1145/2934466.2934476},
abstract = {Creating product lines of Big Data stream processing applications introduces a number of novel challenges to variability modeling. In this paper, we discuss these challenges and demonstrate how advanced variability modeling capabilities can be used to directly model the topology of processing pipelines as well as their variability. We also show how such processing pipelines can be modeled, configured and validated using the Integrated Variability Modeling Language (IVML).},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {204–208},
numpages = {5},
keywords = {variability modeling, topologies, software product lines},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2491627.2491634,
author = {Ferrari, Alessio and Spagnolo, Giorgio O. and Dell'Orletta, Felice},
title = {Mining commonalities and variabilities from natural language documents},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491634},
doi = {10.1145/2491627.2491634},
abstract = {A company who wishes to enter an established marked with a new, competitive product is required to analyse the product solutions of the competitors. Identifying and comparing the features provided by the other vendors might greatly help during the market analysis. However, mining common and variant features of from the publicly available documents of the competitors is a time consuming and error-prone task. In this paper, we suggest to employ a natural language processing approach based on contrastive analysis to identify commonalities and variabilities from the brochures of a group of vendors. We present a first step towards a practical application of the approach, in the the context of the market of Communications-Based Train Control (CBTC) systems.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {116–120},
numpages = {5},
keywords = {variability mining, software product lines},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3218585.3218670,
author = {Martins, Luana Almeida and Parreira, Paulo Afonso and Freire, Andr\'{e} Pimenta and Costa, Heitor},
title = {Exploratory Study on the Use of Software Product Lines in the Development of Quality Assistive Technology Software},
year = {2018},
isbn = {9781450364676},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3218585.3218670},
doi = {10.1145/3218585.3218670},
abstract = {The use of Software Product Line for the development of Assistive Technologies has not been widely explored yet. However, some studies point to the viability of using this approach to develop Assistive Technology software. Through this approach, important limiting factors to use Assistive Technologies can be overcome. These factors are related to the acquisition costs and difficulty to find products corresponding to specific and varying user needs. Considering that Software Product Line approach provides mass customization of software products, the specific needs of each user can be more easily satisfied by software developers. Furthermore, the reuse of code artifacts to development provides a fall in the acquisition cost of these software products. We present in this paper a literature review that aims to investigate how this approach has been applied to the development of Assistive Technology software. Also, we present some quality factors that should be considered to develop Assistive Technologies using Software Product Lines. Thus, the main findings of the review are grouped in order to find the main gaps to be explored in future work.},
booktitle = {Proceedings of the 8th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {262–269},
numpages = {8},
keywords = {Software Quality, Software Product Line, Assistive Technology},
location = {Thessaloniki, Greece},
series = {DSAI '18}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based test case selection of cyber-physical system product lines for simulation-based validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {test case selection, search-based software engineering, cyber-physical system product lines},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2648511.2648533,
author = {Simidchieva, Borislava I. and Osterweil, Leon J.},
title = {Generation, composition, and verification of families of human-intensive systems},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648533},
doi = {10.1145/2648511.2648533},
abstract = {Software products are rarely developed without providing different sets of features to better meet varying user needs, whether through tiered products as part of a product line or different subscription levels for software as a service (SaaS). Software product line approaches for generating and maintaining a family of different variants of software products address such needs for variation quite well. Real-world human-intensive systems (HISs) display similar needs for families of variants. A key contribution of this paper is to show how many of these needs can be rigorously and systematically addressed by adapting established techniques from system and software product line engineering (SPLE).In this paper, we present an approach for creating such families by explicitly modeling variation in HISs. We focus on two kinds of variation we have previously described in other work---functional detail variation and service variation. We describe a prototype system that is able to meet the need for these kinds of variation within an existing modeling framework and present a case study of the application of our prototype system to generate a family in an HIS from the domain of elections. Our approach also demonstrates how to perform model-checking of this family to discover whether any variants in the family may violate specified system requirements.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {207–216},
numpages = {10},
keywords = {system variation, software product lines, process families},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.5555/1753235.1753255,
author = {Sun, Hongyu and Lutz, Robyn R. and Basu, Samik},
title = {Product-line-based requirements customization for web service compositions},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Customizing web services according to users' individual functional and non-functional requirements has become increasingly difficult as the number of users increases. This paper introduces a new way to customize and verify composite web services by incorporating a software product-line engineering approach into web-service composition. The approach uses a partitioning similar to that between domain engineering and application engineering in the product-line context. It specifies the options that the user can select and constructs the resulting web-service compositions. By first creating a web-service composition search space that satisfies the common requirements and then querying the search space as the user selects values for the parameters of variation, we provide a more efficient way to customize web services. A decision model, illustrated with examples from an emergency-response application, is created to interact with the customers and ensure the consistency of their specifications. The capability to reuse the composition search space may also help improve the quality and reliability of the composite services and reduce the cost of re-verifying the same compositions.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {141–150},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2364412.2364414,
author = {Derakhshanmanesh, Mahdi and Fox, Joachim and Ebert, J\"{u}rgen},
title = {Adopting feature-centric reuse of requirements assets: an industrial experience report},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364414},
doi = {10.1145/2364412.2364414},
abstract = {In this paper, we share practical experiences from an ongoing effort towards adopting a feature-centric method that enhances reuse of requirements at TRW Automotive's slip control system department (based in Koblenz, Germany). After introducing identified challenges in detail, key solution factors and a technical reuse concept for managing and deriving product-specific requirements are presented. Then, we demonstrate one way of implementing this solution approach based on industry-standard tools. In addition, identified pitfalls and lessons learned are discussed.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {2–9},
numpages = {8},
keywords = {software product lines, reuse, requirements, features},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1007/s11219-013-9197-z,
author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
title = {Quality attribute modeling and quality aware product configuration in software product lines},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9197-z},
doi = {10.1007/s11219-013-9197-z},
abstract = {In software product line engineering, the customers mostly concentrate on the functionalities of the target product during product configuration. The quality attributes of a target product, such as security and performance, are often assessed until the final product is generated. However, it might be very costly to fix the problem if it is found that the generated product cannot satisfy the customers' quality requirements. Although the quality of a generated product will be affected by all the life cycles of product development, feature-based product configuration is the first stage where the estimation or prediction of the quality attributes should be considered. As we know, the key issue of predicting the quality attributes for a product configured from feature models is to measure the interdependencies between functional features and quality attributes. The current existing approaches have several limitations on this issue, such as requiring real products for the measurement or involving domain experts' efforts. To overcome these limitations, we propose a systematic approach of modeling quality attributes in feature models based on domain experts' judgments using the analytic hierarchical process (AHP) and conducting quality aware product configuration based on the captured quality knowledge. Domain experts' judgments are adapted to avoid generating the real products for quality evaluation, and AHP is used to reduce domain experts' efforts involved in the judgments. A prototype tool is developed to implement the concepts of the proposed approach, and a formal evaluation is carried out based on a large-scale case study.},
journal = {Software Quality Journal},
month = sep,
pages = {365–401},
numpages = {37},
keywords = {Software product line, Quality attributes assessment, Product configuration, Non-functional requirement (NFR) framework, Feature model, Analytic hierarchical process (AHP)}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {weighted approach, product derivation, critical path analysis, configuration management, Software product line}
}

@inproceedings{10.1007/978-3-030-64694-3_11,
author = {Abbas, Muhammad and Saadatmand, Mehrdad and Enoiu, Eduard and Sundamark, Daniel and Lindskog, Claes},
title = {Automated Reuse Recommendation of Product Line Assets Based on Natural Language Requirements},
year = {2020},
isbn = {978-3-030-64693-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64694-3_11},
doi = {10.1007/978-3-030-64694-3_11},
abstract = {Software product lines (SPLs) are based on reuse rationale to aid quick and quality delivery of complex products at scale. Deriving a new product from a product line requires reuse analysis to avoid redundancy and support a high degree of assets reuse. In this paper, we propose and evaluate automated support for recommending SPL assets that can be reused to realize new customer requirements. Using the existing customer requirements as input, the approach applies natural language processing and clustering to generate reuse recommendations for unseen customer requirements in new projects. The approach is evaluated both quantitatively and qualitatively in the railway industry. Results show that our approach can recommend reuse with 74% accuracy and 57.4% exact match. The evaluation further indicates that the recommendations are relevant to engineers and can support the product derivation and feasibility analysis phase of the projects. The results encourage further study on automated reuse analysis on other levels of abstractions.},
booktitle = {Reuse in Emerging Software Engineering Practices: 19th International Conference on Software and Systems Reuse, ICSR 2020, Hammamet, Tunisia, December 2–4, 2020, Proceedings},
pages = {173–189},
numpages = {17},
keywords = {Word embedding, Natural language processing, Reuse recommender, Software product line},
location = {Hammamet, Tunisia}
}

@inproceedings{10.1145/2499777.2500712,
author = {Kolokolov, Viktor and Baumann, Paul and Santini, Silvia and Ruehl, Stefan T. and Verclas, Stephan A. W.},
title = {Flexible development of variable software features for mobile business applications},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500712},
doi = {10.1145/2499777.2500712},
abstract = {With recent advances in development and deployment of mobile business applications (MBAs) based on the hybrid Web approach (hybrid MBAs) enterprises around the world well recognize new potentials to mobilize their business processes (BPs). Variability has a natural appearance in complex environments of different enterprises, where even similar BPs can have varying facets on the cross-enterprise scale. Yet, despite this fact current development tools for hybrid MBAs are lacking systematic variability management. Further, the literature on this particular technological landscape is scarce. We highlight in this paper emerging importance of this research field and describe its context and a research methodology. We propose an SPL-based approach to tackle considerable variabilities of hybrid MBAs.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {67–73},
numpages = {7},
keywords = {variability modeling, software product lines, mobile business applications, hybrid web},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {software product-lines, software design, self-adaptation, product-line management, online learning, knowledge sharing},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.asoc.2016.07.040,
author = {Xue, Yinxing and Zhong, Jinghui and Tan, Tian Huat and Liu, Yang and Cai, Wentong and Chen, Manman and Sun, Jun},
title = {IBED},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.040},
doi = {10.1016/j.asoc.2016.07.040},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose to combine IBEA and DE for the optimal feature selection in SPLE.We propose a feedback-directed method into EAs to improve the correctness of results.Our IBED with the seeding method has significantly shortened the search time.In most cases, IBED finds more unique and non-dominated solutions than IBEA. Software configuration, which aims to customize the software for different users (e.g., Linux kernel configuration), is an important and complicated task. In software product line engineering (SPLE), feature oriented domain analysis is adopted and feature model is used to guide the configuration of new product variants. In SPLE, product configuration is an optimal feature selection problem, which needs to find a set of features that have no conflicts and meanwhile achieve multiple design objectives (e.g., minimizing cost and maximizing the number of features). In previous studies, several multi-objective evolutionary algorithms (MOEAs) were used for the optimal feature selection problem and indicator-based evolutionary algorithm (IBEA) was proven to be the best MOEA for this problem. However, IBEA still suffers from the issues of correctness and diversity of found solutions. In this paper, we propose a dual-population evolutionary algorithm, named IBED, to achieve both correctness and diversity of solutions. In IBED, two populations are individually evolved with two different types of evolutionary operators, i.e., IBEA operators and differential evolution (DE) operators. Furthermore, we propose two enhancement techniques for existing MOEAs, namely the feedback-directed mechanism to fast find the correct solutions (e.g., solutions that satisfy the feature model constraints) and the preprocessing method to reduce the search space. Our empirical results have shown that IBED with the enhancement techniques can outperform several state-of-the-art MOEAs on most case studies in terms of correctness and diversity of found solutions.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1215–1231},
numpages = {17},
keywords = {Software product line engineering, Optimal feature selection, Indicator-based evolutionary algorithm (IBEA), Differential evolutionary algorithm (DE)}
}

@inproceedings{10.5555/776816.776947,
author = {Knauber, Peter and Bosch, Jan},
title = {ICSE workshop on: Software Variability Management},
year = {2003},
isbn = {076951877X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {During recent years, the amount of variability that has to be supported by a software artifact is growing considerably and its management is developing as a main challenge during development, usage, and evolution of software artifacts. Successful management of variability in software artifacts leads to better customizable software products that are in turn likely to result in higher market success.The aim of this workshop is to study software variability management both from a 'problems' and from a 'solutions' perspective by bringing together people from industrial practice and from applied research in academia to present and discuss their respective experience.Issues to be addressed include, but are not limited to, technological, process, and organizational aspects as well as notation, assessment, design, and evolution aspects.},
booktitle = {Proceedings of the 25th International Conference on Software Engineering},
pages = {779–780},
numpages = {2},
keywords = {variability management, software variability, software customization, software configuration, software adaptation},
location = {Portland, Oregon},
series = {ICSE '03}
}

@inproceedings{10.1145/2019136.2019159,
author = {Otsuka, Jun and Kawarabata, Kouichi and Iwasaki, Takashi and Uchiba, Makoto and Nakanishi, Tsuneo and Hisazumi, Kenji},
title = {Small inexpensive core asset construction for large gainful product line development: developing a communication system firmware product line},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019159},
doi = {10.1145/2019136.2019159},
abstract = {Product line development of communication system firmware with more than 2,000 features was performed in a large-scale project that involved more than 300 engineers (at a maximum) across four distributed sites. However, since intense demands to reduce development costs and time made it prohibitive to construct core assets for all those identified features, the project screened a limited number of the features, for which core assets were constructed, and then performed partial application of product line engineering. Nevertheless, when compared with previously engineered derivative developments, when the second product of the product line was released, it was clear that the project had achieved significant improvements in quality, as well as reductions in development costs and time requirements. Automatic code generation also contributed to those improvements.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {20},
numpages = {5},
keywords = {product line, feature modeling, core assets, case study},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green software development and research with the HADAS toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {variability, software product line, repository, optimisation, metrics, energy efficiency, clafer, CVL},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@article{10.1007/s00766-014-0201-3,
author = {Lung, Chung-Horng and Balasubramaniam, Balasangar and Selvarajah, Kamalachelva and Elankeswaran, Poopalasingham and Gopalasundaram, Umatharan},
title = {On building architecture-centric product line architecture},
year = {2015},
issue_date = {September 2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0201-3},
doi = {10.1007/s00766-014-0201-3},
abstract = {Software architects typically spend a great deal of time and effort exploring uncertainties, evaluating alternatives, and balancing the concerns of stakeholders. Selecting the best architecture to meet both the functional and non-functional requirements is a critical but difficult task, especially at the early stage of software development when there may be many uncertainties. For example, how will a technology match the operational or performance expectations in reality? This paper presents an approach to building architecture-centric product line. The main objective of the proposed approach is to support effective requirements validation and architectural prototyping for the application-level software. Architectural prototyping is practically essential to architecture design and evaluation. However, architectural prototyping practiced in the field mostly is not used to explore alternatives. Effective construction and evaluation of multiple architecture alternatives is one of the critically challenging tasks. The product line architecture advocated in this paper consists of multiple software architecture alternatives, from which the architect can select and rapidly generate a working application prototype. The paper presents a case study of developing a framework that is primarily built with robust architecture patterns in distributed and concurrent computing and includes variation mechanisms to support various applications even in different domains. The development process of the framework is an application of software product line engineering with an aim to effectively facilitate upfront requirements analysis for an application and rapid architectural prototyping to explore and evaluate architecture alternatives.},
journal = {Requir. Eng.},
month = sep,
pages = {301–321},
numpages = {21},
keywords = {Software product line, Software performance, Requirements validation, Patterns, Architecture evaluation, Architectural prototyping}
}

@article{10.1007/s10664-020-09915-7,
author = {Temple, Paul and Perrouin, Gilles and Acher, Mathieu and Biggio, Battista and J\'{e}z\'{e}quel, Jean-Marc and Roli, Fabio},
title = {Empirical assessment of generating adversarial configurations for software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09915-7},
doi = {10.1007/s10664-020-09915-7},
abstract = {Software product line (SPL) engineering allows the derivation of products tailored to stakeholders’ needs through the setting of a large number of configuration options. Unfortunately, options and their interactions create a huge configuration space which is either intractable or too costly to explore exhaustively. Instead of covering all products, machine learning (ML) approximates the set of acceptable products (e.g., successful builds, passing tests) out of a training set (a sample of configurations). However, ML techniques can make prediction errors yielding non-acceptable products wasting time, energy and other resources. We apply adversarial machine learning techniques to the world of SPLs and craft new configurations faking to be acceptable configurations but that are not and vice-versa. It allows to diagnose prediction errors and take appropriate actions. We develop two adversarial configuration generators on top of state-of-the-art attack algorithms and capable of synthesizing configurations that are both adversarial and conform to logical constraints. We empirically assess our generators within two case studies: an industrial video synthesizer (MOTIV) and an industry-strength, open-source Web-app configurator (JHipster). For the two cases, our attacks yield (up to) a 100% misclassification rate without sacrificing the logical validity of adversarial configurations. This work lays the foundations of a quality assurance framework for ML-based SPLs.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {49},
keywords = {Quality assurance, Machine learning, Software testing, Software variability, Configurable system, Software product line}
}

@article{10.1016/j.jss.2018.07.054,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.054},
doi = {10.1016/j.jss.2018.07.054},
journal = {J. Syst. Softw.},
month = oct,
pages = {511–532},
numpages = {22},
keywords = {Systematic literature review, Product configuration, Extended product line}
}

@article{10.1007/s10664-008-9094-4,
author = {Lee, Jihyun and Kang, Sungwon and Kim, Chang-Ki},
title = {Software architecture evaluation methods based on cost benefit analysis and quantitative decision making},
year = {2009},
issue_date = {August    2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9094-4},
doi = {10.1007/s10664-008-9094-4},
abstract = {Since many parts of the architecture evaluation steps of the Cost Benefit Analysis Method (CBAM) depend on the stakeholders' empirical knowledge and intuition, it is very important that such an architecture evaluation method be able to faithfully reflect the knowledge of the experts in determining Architectural Strategy (AS). However, because CBAM requires the stakeholders to make a consensus or vote for collecting data for decision making, it is difficult to accurately reflect the stakeholders' knowledge in the process. In order to overcome this limitation of CBAM, we propose the two new CBAM-based methods for software architecture evaluation, which respectively adopt the Analytic Hierarchy Process (AHP) and the Analytic Network Process (ANP). Since AHP and ANP use pair-wise comparison they are suitable for a cost and benefit analysis technique since its purpose is not to calculate correct values of benefit and cost but to decide AS with highest return on investment. For that, we first define a generic process of CBAM and develop variations from the generic process by applying AHP and ANP to obtain what we call the CBAM+AHP and CBAM+ANP methods. These new methods not only reflect the knowledge of experts more accurately but also reduce misjudgments. A case study comparison of CBAM and the two new methods is conducted using an industry software project. Because the cost benefit analysis process that we present is generic, new cost benefit analysis techniques with capabilities and characteristics different from the three methods we examine here can be derived by adopting various different constituent techniques.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {453–475},
numpages = {23},
keywords = {Software architecture evaluation, CBAM, ANP, AHP}
}

@inproceedings{10.1145/2362536.2362568,
author = {Hofman, Peter and Stenzel, Tobias and Pohley, Thomas and Kircher, Michael and Bermann, Andreas},
title = {Domain specific feature modeling for software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362568},
doi = {10.1145/2362536.2362568},
abstract = {This paper summarizes our experience with introducing feature modeling into a product line for imaging and therapy systems in the Siemens Healthcare Sector. Determining and negotiating the scope in a product line that spans several business units with their own economic goals is challenging. Feature modeling offers a good way to do variability/commonality analysis for complex product lines. A precondition for feature modeling is the identification of all features supporting the product line. To identify these features, we developed a method for systematically deriving a feature model top down based on domain know-how. We call this method domain specific feature modeling. As the primary artifact to describe the problem space, a domain specific feature model additionally improves the requirement understanding for all stakeholders by considerably improving the scoping, traceability, testing, efficiency and transparency of planning activities and making the development efforts easier to estimate. In this paper, we share our experience with domain specific feature modeling in a large platform project and describe the lessons learned. We describe our general approach that can also be used for other domains.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {229–238},
numpages = {10},
keywords = {variability analysis, product line, feature modeling, feature dependency diagram, domain specific feature model, commonality analysis, agile, Scrum},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/1842713.1842717,
author = {Robinson, William N. and Ding, Yi},
title = {A survey of customization support in agent-based business process simulation tools},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/1842713.1842717},
doi = {10.1145/1842713.1842717},
abstract = {Agent-based business process simulation has grown in popularity, in part because of its analysis capabilities. The analyses depend on the kinds of simulations that can be built, adapted, and extended, which in turn depend on the underlying simulation framework. We report the results of our analysis of 19 agent-based process simulation tools and their simulation frameworks. We conclude that a growing number of simulation tools are using component-based software techniques. Nevertheless, most simulation tools do not directly support requirements models, their transformation into executable simulations, or the management of model variants over time. Such practices are becoming more widely applied in software engineering under the term software product line engineering (SPLE). Based on our analysis, agent-based process simulation tools may improve their customization capacity by: (1) supporting object modeling more completely and (2) supporting software product line engineering issues.},
journal = {ACM Trans. Model. Comput. Simul.},
month = oct,
articleno = {14},
numpages = {29},
keywords = {software product line engineering, modularity, event-driven simulation, encapsulation, application frameworks, Agent-based modeling}
}

@inproceedings{10.5555/1753235.1753263,
author = {Than Tun, Thein and Boucher, Quentin and Classen, Andreas and Hubaux, Arnaud and Heymans, Patrick},
title = {Relating requirements and feature configurations: a systematic approach},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {A feature model captures various possible configurations of products within a product family. When configuring a product, several features are selected and composed. Selecting features at the program level has a general limitation of not being able to relate the resulting configuration to its requirements. As a result, it is difficult to decide whether a given configuration of features is optimal. An optimal configuration satisfies all stakeholder requirements and quantitative constraints, while ensuring that there is no extraneous feature in it. In relating requirements and feature configurations, we use the description of the problem world context in which the software is designed to operate as the intermediate description between them. The advantage of our approach is that feature selection can be done at the requirements level, and an optimal program level configuration can be generated from the requirements selected. Our approach is illustrated with a real-life problem of configuring a satellite communication software. The use of an existing tool to support our approach is also discussed.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {201–210},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.5555/1753235.1753249,
author = {Montagud, Sonia and Abrah\~{a}o, Silvia},
title = {Gathering current knowledge about quality evaluation in software product lines},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Recently, a number of methods and techniques for assessing the quality of software product lines have been proposed. However, to the best of our knowledge, there is no study which summarizes all the existing evidence about them. This paper presents a systematic review that investigates what methods and techniques have been employed (in the last 10 years) to evaluate the quality of software product lines and how they were employed. A total of 39 research papers have been reviewed from an initial set of 1388 papers. The results show that 25% of the papers reported evaluations at the Design phase of the Domain Engineering phase. The most widely used mechanism for modeling quality attributes was extended feature models and the most evaluated artifact was the base architecture. In addition, the results of the review have identified several research gaps. Specifically, 77% of the papers employed case studies as a "proof of concept" whereas 23% of the papers did not perform any type of validation. Our results are particularly relevant in positioning new research activities and in the selection of quality evaluation methods or techniques that best fit a given purpose.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {91–100},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2771783.2771808,
author = {Tan, Tian Huat and Xue, Yinxing and Chen, Manman and Sun, Jun and Liu, Yang and Dong, Jin Song},
title = {Optimizing selection of competing features via feedback-directed evolutionary algorithms},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771808},
doi = {10.1145/2771783.2771808},
abstract = {Software that support various groups of customers usually require complicated configurations to attain different functionalities. To model the configuration options, feature model is proposed to capture the commonalities and competing variabilities of the product variants in software family or Software Product Line (SPL). A key challenge for deriving a new product is to find a set of features that do not have inconsistencies or conflicts, yet optimize multiple objectives (e.g., minimizing cost and maximizing number of features), which are often competing with each other. Existing works have attempted to make use of evolutionary algorithms (EAs) to address this problem. In this work, we incorporated a novel feedback-directed mechanism into existing EAs. Our empirical results have shown that our method has improved noticeably over all unguided version of EAs on the optimal feature selection. In particular, for case studies in SPLOT and LVAT repositories, the feedback-directed Indicator-Based EA (IBEA) has increased the number of correct solutions found by 72.33% and 75%, compared to unguided IBEA. In addition, by leveraging a pre-computed solution, we have found 34 sound solutions for Linux X86, which contains 6888 features, in less than 40 seconds.},
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {246–256},
numpages = {11},
keywords = {evolutionary algorithms, Software product line, SAT solvers},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.5555/1753235.1753245,
author = {Cetina, Carlos and Haugen, \O{}ystein and Zhang, Xiaorui and Fleurey, Franck and Pelechano, Vicente},
title = {Strategies for variability transformation at run-time},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {More and more approaches propose to use Software Product Lines (SPLs) modelling techniques to implement dynamic adaptive systems. The resulting Dynamic Software Product Lines (DSPLs) present new challenges since the variability transformations used to derive alternative configurations have to be intensively used at runtime. This paper proposes to use the Common Variability Language (CVL) for modelling runtime variability and evaluates a set of alternative strategies for implementing the associated variability transformations. All the proposed strategies have been implemented and evaluated on the case-study of a smart-home system. Results show that the proposed strategies provide the same reconfiguration service with significant differences in quality-of-service.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {61–70},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.jss.2018.05.069,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-adaptation of service compositions through product line reconfiguration},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.05.069},
doi = {10.1016/j.jss.2018.05.069},
journal = {J. Syst. Softw.},
month = oct,
pages = {84–105},
numpages = {22},
keywords = {Self adaptation, Software product lines, Feature model, Service composition}
}

@article{10.1145/3088440,
author = {Acher, Mathieu and Lopez-Herrejon, Roberto E. and Rabiser, Rick},
title = {Teaching Software Product Lines: A Snapshot of Current Practices and Challenges},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
url = {https://doi.org/10.1145/3088440},
doi = {10.1145/3088440},
abstract = {Software Product Line (SPL) engineering has emerged to provide the means to efficiently model, produce, and maintain multiple similar software variants, exploiting their common properties, and managing their variabilities (differences). With over two decades of existence, the community of SPL researchers and practitioners is thriving, as can be attested by the extensive research output and the numerous successful industrial projects. Education has a key role to support the next generation of practitioners to build highly complex, variability-intensive systems. Yet, it is unclear how the concepts of variability and SPLs are taught, what are the possible missing gaps and difficulties faced, what are the benefits, and what is the material available. Also, it remains unclear whether scholars teach what is actually needed by industry. In this article, we report on three initiatives we have conducted with scholars, educators, industry practitioners, and students to further understand the connection between SPLs and education, that is, an online survey on teaching SPLs we performed with 35 scholars, another survey on learning SPLs we conducted with 25 students, as well as two workshops held at the International Software Product Line Conference in 2014 and 2015 with both researchers and industry practitioners participating. We build upon the two surveys and the workshops to derive recommendations for educators to continue improving the state of practice of teaching SPLs, aimed at both individual educators as well as the wider community.},
journal = {ACM Trans. Comput. Educ.},
month = oct,
articleno = {2},
numpages = {31},
keywords = {variability modeling, software product line teaching, software engineering teaching, Software product lines}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Configurable systems, Machine learning, Software product lines, Systematic literature review}
}

@article{10.1016/j.jss.2014.08.034,
author = {Alsawalqah, Hamad I. and Kang, Sungwon and Lee, Jihyun},
title = {A method to optimize the scope of a software product platform based on end-user features},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.08.034},
doi = {10.1016/j.jss.2014.08.034},
abstract = {A novel method to optimize the scope of a software product platform is proposed.The method is supported with a mathematical formulation and an optimization solver.Depending on the input parameters and the objectives, competing scopes can exist.The method shows how trade-off analysis can be performed among competing scopes.The results of the method were validated as "satisfiable" to "very satisfiable". ContextDue to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development. AimThis paper proposes a novel method to find the optimized scope of a software product platform based on end-user features. MethodThe proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives. ResultsIn a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope.},
journal = {J. Syst. Softw.},
month = dec,
pages = {79–106},
numpages = {28},
keywords = {Software product line engineering, Product platform scope, Commonality decision}
}

@inproceedings{10.1145/3474624.3476016,
author = {Bezerra, Carla and Lima, Rafael and Silva, Publio},
title = {DyMMer 2.0: A Tool for Dynamic Modeling and Evaluation of Feature Model},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476016},
doi = {10.1145/3474624.3476016},
abstract = {Managing dynamic variability has motivated several researchers to combine Dynamic Software Product Lines (DSPLs) practices with runtime variability mechanisms. By combining these approaches, a DSPL acquires important features, ranging from the ability to reconfigure by changing the context, adding or removing features, crash recovery, and re-adaptation based on changes in the model’s features. Feature model (FM) is an important artifact of a DPSL and there is a lack of tools that support the modeling of this artifact. We have extended the DyMMer tool for modeling FM of DSPLs from an adaptation mechanism based on MAPE-K to solve this problem. We migrated the DyMMer tool to a web version and incorporated new features: (i) modeling of FMs from SPLs and DSPLs, (ii) development of an adaptation mechanism for FM of DSPLs, (iii) repository of FMs, (iv) inclusion of thresholds for measures, and (v) user authentication. We believe that this tool is useful for research in the area of DSPLs, and also for dynamic domain modeling and evaluation. Video: https://youtu.be/WVHW6bI8ois},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {121–126},
numpages = {6},
keywords = {Modeling, Feature Model, Dynamic Software Product Line},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/2556624.2556637,
author = {Machado, Ivan do Carmo and Santos, Alcemir Rodrigues and Cavalcanti, Yguarat\~{a} Cerqueira and Trzan, Eduardo Gomes and de Souza, Marcio Magalh\~{a}es and de Almeida, Eduardo Santana},
title = {Low-level variability support for web-based software product lines},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556637},
doi = {10.1145/2556624.2556637},
abstract = {The Web systems domain has faced an increasing number of devices, browsers, and platforms to cope with, driving software systems to be more flexible to accomodate them. Software product line (SPL) engineering can be used as a strategy to implement systems capable of handling such a diversity. To this end, automated tool support is almost indispensable. However, current tool support gives more emphasis to modeling variability in the problem domain, over the support of variability at the solution domain. There is a need for mapping the variability between both abstraction levels, so as to determine what implementation impact a certain variability has. In this paper, we propose the FeatureJS, a FeatureIDE extension aiming at Javascript and HTML support for SPL engineering. The tool combines feature-oriented programming and preprocessors, as a strategy to map variability at source code with the variability modeled at a higher level of abstraction. We carried out a preliminary evaluation with an industrial project, aiming to characterize the capability of the tool to handle SPL engineering in the Web systems domain.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {8},
keywords = {web systems domain, software product line engineering, feature oriented software development, feature composition, FeatureIDE, Eclipse plugin},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1007/s10270-017-0610-0,
author = {Guo, Jianmei and Liang, Jia Hui and Shi, Kai and Yang, Dingyu and Zhang, Jingsong and Czarnecki, Krzysztof and Ganesh, Vijay and Yu, Huiqun},
title = {SMTIBEA: a hybrid multi-objective optimization algorithm for configuring large constrained software product lines},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0610-0},
doi = {10.1007/s10270-017-0610-0},
abstract = {A key challenge to software product line engineering is to explore a huge space of various products and to find optimal or near-optimal solutions that satisfy all predefined constraints and balance multiple often competing objectives. To address this challenge, we propose a hybrid multi-objective optimization algorithm called SMTIBEA that combines the indicator-based evolutionary algorithm (IBEA) with the satisfiability modulo theories (SMT) solving. We evaluated the proposed algorithm on five large, constrained, real-world SPLs. Compared to the state-of-the-art, our approach significantly extends the expressiveness of constraints and simultaneously achieves a comparable performance. Furthermore, we investigate the performance influence of the SMT solving on two evolutionary operators of the IBEA.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1447–1466},
numpages = {20},
keywords = {Software product lines, Search-based software engineering, Multi-objective evolutionary algorithms, Feature models, Constraint solving}
}

@inproceedings{10.1145/2737182.2737183,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {Representing and Configuring Security Variability in Software Product Lines},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737182.2737183},
doi = {10.1145/2737182.2737183},
abstract = {In a software product line, security may need to be varied. Consequently, security variability must be managed both from the customer and product line architecture point of view. We utilize design science to build an artifact and a generalized design theory for representing and configuring security and functional variability from the requirements to the architecture in a configurable software product line. An open source web shop product line, Magento, is used as a case example to instantiate and evaluate the contribution. The results indicate that security variability can be represented and distinguished as countermeasures; and that a configurator tool is able to find consistent products as stable models of answer set programs.},
booktitle = {Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {1–10},
numpages = {10},
keywords = {variability, software product line, software architecture, security},
location = {Montr\'{e}al, QC, Canada},
series = {QoSA '15}
}

@inproceedings{10.1145/2364412.2364441,
author = {Schroeter, Julia and Mucha, Peter and Muth, Marcel and Jugel, Kay and Lochau, Malte},
title = {Dynamic configuration management of cloud-based applications},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364441},
doi = {10.1145/2364412.2364441},
abstract = {Cloud-based applications are multi-tenant aware, whereas customers (i.e., tenants) share hardware and software resources. Offering highly configurable applications to thousands of tenants in a shared cloud environment demands for scalable configuration management. Based on an example scenario taken from the Indenica project, we identify requirements for applying methods from software product line (SPL) engineering to configure cloud-based multi-tenant aware applications. Using an extended feature model (EFM) to express variability of functionality and service qualities, we propose a concept for dynamic configuration management to address the identified requirements. Our proposed configuration management includes an adaptive staged configuration process that is capable of adding and removing stakeholders dynamically and that allows for reconfiguration of variants as stakeholders' objectives change.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {171–178},
numpages = {8},
keywords = {staged configuration, software engineering, software configuration management, software, multi-tenancy, extended feature model, cloud computing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.cl.2016.09.004,
author = {M\'{e}ndez-Acu\~{n}a, David and Galindo, Jos\'{e} A. and Degueule, Thomas and Combemale, Beno\^{\i}t and Baudry, Beno\^{\i}t},
title = {Leveraging Software Product Lines Engineering in the development of external DSLs},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2016.09.004},
doi = {10.1016/j.cl.2016.09.004},
abstract = {The use of domain-specific languages (DSLs) has become a successful technique in the development of complex systems. Consequently, nowadays we can find a large variety of DSLs for diverse purposes. However, not all these DSLs are completely different; many of them share certain commonalities coming from similar modeling patterns - such as state machines or petri nets - used for several purposes. In this scenario, the challenge for language designers is to take advantage of the commonalities existing among similar DSLs by reusing, as much as possible, formerly defined language constructs. The objective is to leverage previous engineering efforts to minimize implementation from scratch. To this end, recent research in software language engineering proposes the use of product line engineering, thus introducing the notion of language product lines. Nowadays, there are several approaches that result useful in the construction of language product lines. In this article, we report on an effort for organizing the literature on language product line engineering. More precisely, we propose a definition for the life-cycle of language product lines, and we use it to analyze the capabilities of current approaches. In addition, we provide a mapping between each approach and the technological space it supports. HighlightsSurvey on the applicability of software product lines in the construction of DSLs.General life-cycle for language product lines.Mapping current approaches on language product lines and technological spaces.Research map in language product lines engineering.},
journal = {Comput. Lang. Syst. Struct.},
month = nov,
pages = {206–235},
numpages = {30},
keywords = {Variability management, Software language engineering, Software Product Lines Engineering, Domain-specific languages}
}

@article{10.1145/2853073.2853095,
author = {Alebrahim, Azadeh and Fa\ss{}bender, Stephan and Filipczyk, Martin and Goedicke, Michael and Heisel, Maritta and Zdun, Uwe},
title = {Variability for Qualities in Software Architecture},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853095},
doi = {10.1145/2853073.2853095},
abstract = {Variability is a key factor of most systems. While there are many works covering variability in functionality, there is a research gap regarding variability in software qualities. There is an obvious imbalance between the importance of variability in the context of quality attributes, and the intensity of research in this area. To improve this situation, the First International Workshop on VAri- ability for QUalIties in SofTware Architecture (VAQUITA) was held jointly with ECSA 2015 in Cavtat/Dubrovnik, Croatia as a one-day workshop. The goal of VAQUITA was to investigate and stimulate the discourse about the matter of variability, qualities, and software architectures. The workshop featured three research paper presentations, one keynote talk, and two working group discussions. In this workshop report, we summarize the keynote talk and the presented papers. Additionally, we present the results of the working group discussions},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {32–35},
numpages = {4},
keywords = {variability, quality attributes, Software architecture}
}

@article{10.1016/j.jss.2010.01.048,
author = {Lee, Jaejoon and Muthig, Dirk and Naab, Matthias},
title = {A feature-oriented approach for developing reusable product line assets of service-based systems},
year = {2010},
issue_date = {July, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.01.048},
doi = {10.1016/j.jss.2010.01.048},
abstract = {Service orientation (SO) is a relevant promising candidate for accommodating rapidly changing user needs and expectations. One of the goals of adopting SO is the improvement of reusability, however, the development of service-based system in practice has uncovered several challenging issues, such as how to identify reusable services, how to determine configurations of services that are relevant to users' current product configuration and context, and how to maintain service validity after configuration changes. In this paper, we propose a method that addresses these issues by adapting a feature-oriented product line engineering approach. The method is notable in that it guides developers to identify reusable services at the right level of granularity and to map users' context to relevant service configuration, and it also provides a means to check the validity of services at runtime in terms of invariants and pre/post-conditions of services. Moreover, we propose a heterogeneous style based architecture model for developing such systems.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1123–1136},
numpages = {14},
keywords = {Software product line engineering, Software architecture styles, Software architecture, Service-based systems, Feature-oriented}
}

@article{10.1007/s10009-012-0252-z,
author = {Pleuss, Andreas and Botterweck, Goetz},
title = {Visualization of variability and configuration options},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-012-0252-z},
doi = {10.1007/s10009-012-0252-z},
abstract = {When designing, constructing, and maintaining diverse and variable software systems, a key challenge is the complexity of systems. A potential approach to tackle this challenge are techniques from variability management and product line engineering to handle the diversity and variability. A key asset in variability management is a variability model, which explicitly specifies the commonalities and variability of a system and the constraints between variants. However, handling variability and configurations remains a challenge due to the complexity on a cognitive level as human engineers reach their limits in identifying, understanding, and using all relevant details. In this paper we address this issue by providing concepts for interactive visual tool support for the configuration of systems with the help of feature models. We discuss relevant principles from the area of information visualization and their application to the domain of feature model configuration. We discuss techniques for interactive configuration support based on a reasoning engine, which, e.g., ensures the validity of configurations. We illustrate our findings by a concrete tool solution called S2T2 Configurator.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {497–510},
numpages = {14},
keywords = {Visualization, Variability, Software product line, Product configuration, Interactive tool, Feature model}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Software Product Line, Requirements Engineering, Natural Language Processing, Machine Learning, Feature Model Validation},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1016/j.eswa.2015.02.020,
author = {Dermeval, Diego and Ten\'{o}rio, Thyago and Bittencourt, Ig Ibert and Silva, Alan and Isotani, Seiji and Ribeiro, M\'{a}rcio},
title = {Ontology-based feature modeling},
year = {2015},
issue_date = {July 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {11},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.02.020},
doi = {10.1016/j.eswa.2015.02.020},
abstract = {We compare two ontology-based feature modeling styles by conducting an experiment.The results show that ontology factor has statistical significance in all metrics.The results show that the ontology based on instances is more flexible.The results show that the ontology based on instances demands less time to change. A software product line (SPL) is a set of software systems that have a particular set of common features and that satisfy the needs of a particular market segment or mission. Feature modeling is one of the key activities involved in the design of SPLs. The feature diagram produced in this activity captures the commonalities and variabilities of SPLs. In some complex domains (e.g., ubiquitous computing, autonomic systems and context-aware computing), it is difficult to foresee all functionalities and variabilities a specific SPL may require. Thus, Dynamic Software Product Lines (DSPLs) bind variation points at runtime to adapt to fluctuations in user needs as well as to adapt to changes in the environment. In this context, relying on formal representations of feature models is important to allow them to be automatically analyzed during system execution. Among the mechanisms used for representing and analyzing feature models, description logic (DL) based approaches demand to be better investigated in DSPLs since it provides capabilities, such as automated inconsistency detection, reasoning efficiency, scalability and expressivity. Ontology is the most common way to represent feature models knowledge based on DL reasoners. Previous works conceived ontologies for feature modeling either based on OWL classes and properties or based on OWL individuals. However, considering change or evolution scenarios of feature models, we need to compare whether a class-based or an individual-based feature modeling style is recommended to describe feature models to support SPLs, and especially its capabilities to deal with changes in feature models, as required by DSPLs. In this paper, we conduct a controlled experiment to empirically compare two approaches based on each one of these modeling styles in several changing scenarios (e.g., add/remove mandatory feature, add/remove optional feature and so on). We measure time to perform changes, structural impact of changes (flexibility) and correctness for performing changes in our experiment. Our results indicate that using OWL individuals requires less time to change and is more flexible than using OWL classes and properties. These results provide insightful assumptions towards the definition of an approach relying on reasoning capabilities of ontologies that can effectively support products reconfiguration in the context of DSPL.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {4950–4964},
numpages = {15},
keywords = {Software product line, Ontology, Feature modeling, Empirical software engineering}
}

@inproceedings{10.1145/2361999.2362028,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Modeling variability in product lines using domain quality attribute scenarios},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362028},
doi = {10.1145/2361999.2362028},
abstract = {The concept of variability is fundamental in software product lines and a successful implementation of a product line largely depends on how well domain requirements and their variability are specified, managed, and realized. While developing an educational software product line, we identified a lack of support to specify variability in quality concerns. To address this problem we propose an approach to model variability in quality concerns, which is an extension of quality attribute scenarios. In particular, we propose domain quality attribute scenarios, which extend standard quality attribute scenarios with additional information to support specification of variability and deriving product specific scenarios. We demonstrate the approach with scenarios for robustness and upgradability requirements in the educational software product line.},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {135–142},
numpages = {8},
keywords = {variability, software product lines, scenarios, quality attributes},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@inproceedings{10.1145/3368089.3409684,
author = {Kr\"{u}ger, Jacob and Berger, Thorsten},
title = {An empirical analysis of the costs of clone- and platform-oriented software reuse},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409684},
doi = {10.1145/3368089.3409684},
abstract = {Software reuse lowers development costs and improves the quality of software systems. Two strategies are common: clone &amp; own (copying and adapting a system) and platform-oriented reuse (building a configurable platform). The former is readily available, flexible, and initially cheap, but does not scale with the frequency of reuse, imposing high maintenance costs. The latter scales, but imposes high upfront investments for building the platform, and reduces flexibility. As such, each strategy has distinctive advantages and disadvantages, imposing different development activities and software architectures. Deciding for one strategy is a core decision with long-term impact on an organization’s software development. Unfortunately, the strategies’ costs are not well-understood - not surprisingly, given the lack of systematically elicited empirical data, which is difficult to collect. We present an empirical study of the development activities, costs, cost factors, and benefits associated with either reuse strategy. For this purpose, we combine quantitative and qualitative data that we triangulated from 26 interviews at a large organization and a systematic literature review covering 57 publications. Our study both confirms and refutes common hypotheses on software reuse. For instance, we confirm that developing for platform-oriented reuse is more expensive, but simultaneously reduces reuse costs; and that platform-orientation results in higher code quality compared to clone &amp; own. Surprisingly, refuting common hypotheses, we find that change propagation can be more expensive in a platform, that platforms can facilitate the advancement into innovative markets, and that there is no strict distinction of clone &amp; own and platform-oriented reuse in practice.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {432–444},
numpages = {13},
keywords = {software reuse, software product line, platform engineering, empirical study, economics, clone &amp; own},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1016/j.infsof.2013.02.007,
author = {Santos Rocha, Roberto dos and Fantinato, Marcelo},
title = {The use of software product lines for business process management},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.02.007},
doi = {10.1016/j.infsof.2013.02.007},
abstract = {ContextBusiness Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT. ObjectivePresenting the results of a study undertaken to seek and assess PL approaches for BPM through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic PL approaches for BPM. MethodA SLR was conducted with four research questions formulated to evaluate PL approaches for BPM. Results63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that PLs only partially address the BPM lifecycle since the last business process phase is not a current concern on the found approaches. ConclusionsThe found PL approaches for BPM only cover partially the BPM lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic PL proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that PL approaches for BPM are still at an early stage and gaining maturity.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1355–1373},
numpages = {19},
keywords = {Software product line, PL, Business process management, BPM}
}

@article{10.1016/j.jss.2007.04.011,
author = {Kim, Jintae and Park, Sooyong and Sugumaran, Vijayan},
title = {DRAMA: A framework for domain requirements analysis and modeling architectures in software product lines},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.04.011},
doi = {10.1016/j.jss.2007.04.011},
abstract = {One of the benefits of software product line approach is to improve time-to-market. The changes in market needs cause software requirements to be flexible in product lines. Whenever software requirements are changed, software architecture should be evolved to correspond with them. Therefore, domain architecture should be designed based on domain requirements. It is essential that there is traceability between requirements and architecture, and that the structure of architecture is derived from quality requirements. The purpose of this paper is to provide a framework for modeling domain architecture based on domain requirements within product lines. In particular, we focus on the traceable relationship between requirements and architectural structures. Our framework consists of processes, methods, and a supporting tool. It uses four basic concepts, namely, goal based domain requirements analysis, Analytical Hierarchy Process, Matrix technique, and architecture styles. Our approach is illustrated using HIS (Home Integration System) product line. Finally, industrial examples are used to validate DRAMA.},
journal = {J. Syst. Softw.},
month = jan,
pages = {37–55},
numpages = {19},
keywords = {Domain architecture, Domain requirements, Quality attribute, Traceability}
}

@techreport{10.5555/886682,
author = {B. L. Di, Vito and L. W., Roberts},
title = {Using Formal Methods to Assist in the Requirements Analysis of the Space Shuttle GPS Change Request},
year = {1996},
publisher = {NASA Langley Technical Report Server},
abstract = {We describe a recent NASA-sponsored pilot project intended to gauge the effectiveness of using formal methods in Space Shuttle software requirements analysis. Several Change Requests (CRs) were selected as promising targets to demonstrate the utility of formal methods in this application domain. A CR to add new navigation capabilities to the Shuttle, based on Global Positioning System (GPS) technology, is the focus of this report. Carried out in parallel with the shuttle program''s conventional requirements analysis process was a limited form of analysis based on formalized requirements. Portions of the GPS CR were modeled using the language of SRI''s Prototype Verification System (PVS). During the formal methods-based analysis, numerous requirements issues were discovered and submitted as official issues through the normal requirements inspection process. Shuttle analysts felt that many of these issues were uncovered earlier than would have occurred with conventional methods. We present a summary of these encouraging results and conclusions we have drawn from the pilot project.}
}

@article{10.1016/j.jss.2019.01.057,
author = {Kr\"{u}ger, Jacob and Mukelabai, Mukelabai and Gu, Wanzi and Shen, Hui and Hebig, Regina and Berger, Thorsten},
title = {Where is my feature and what is it about? A case study on recovering feature facets},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.057},
doi = {10.1016/j.jss.2019.01.057},
journal = {J. Syst. Softw.},
month = jun,
pages = {239–253},
numpages = {15},
keywords = {Feature location, Marlin, Bitcoin-wallet, Case study, Feature facets, Software product line}
}

@article{10.1016/j.jss.2013.10.010,
author = {White, Jules and Galindo, Jos\'{e} A. and Saxena, Tripti and Dougherty, Brian and Benavides, David and Schmidt, Douglas C.},
title = {Evolving feature model configurations in software product lines},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {87},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.10.010},
doi = {10.1016/j.jss.2013.10.010},
abstract = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps.},
journal = {J. Syst. Softw.},
month = jan,
pages = {119–136},
numpages = {18},
keywords = {Feature model, Multi-step configuration, Software product line}
}

@inproceedings{10.1145/1985484.1985493,
author = {Chastek, Gary and Donohoe, Patrick and McGregor, John D.},
title = {Commonality and variability analysis for resource constrained organizations},
year = {2011},
isbn = {9781450305846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985484.1985493},
doi = {10.1145/1985484.1985493},
abstract = {This position paper describes our current work in adapting a software product line technique to the constraints of a development organization. We report on applying a commonality and variability analysis with an organization adopting a software product line approach while facing sever resource constraints because of current product development commitments. The immediate focus of the paper is on blending commonality and variability analysis into the organization's existing requirements development process. The longer-term goal of this work is to facilitate the transition to product lines in a minimally intrusive way. The paper describes how the approach was introduced and implemented, and summarizes the benefits achieved and the issues arising from the work to date.},
booktitle = {Proceedings of the 2nd International Workshop on Product Line Approaches in Software Engineering},
pages = {31–34},
numpages = {4},
keywords = {commonality and variability analysis, software product line},
location = {Waikiki, Honolulu, HI, USA},
series = {PLEASE '11}
}

@inproceedings{10.1145/2852339.2852341,
author = {Latif, Khalid and Selva, Manuel and Effiong, Charles and Ursu, Roman and Gamatie, Abdoulaye and Sassatelli, Gilles and Zordan, Leonardo and Ost, Luciano and Dziurzanski, Piotr and Indrusiak, Leandro Soares},
title = {Design space exploration for complex automotive applications: an engine control system case study},
year = {2016},
isbn = {9781450340724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2852339.2852341},
doi = {10.1145/2852339.2852341},
abstract = {With technological advances, significant changes are taking place in automotive domain. Modern automobile combines functionalities ranging from safety critical functions such as control systems for engine to navigation and infotainment. To meet the performances requirements of these systems, automotive industry is shifting to multi-core systems. This increases the design complexity. Efficient and fast design space exploration frameworks are required to deal with this design complexity. This paper presents a framework for exploring automotive application design on multi-core systems. It considers an automotive-specific application modeling language named Amalthea and a distributed-memory multi-core system architecture for execution. The effectiveness of our framework is shown on an engine control application.},
booktitle = {Proceedings of the 2016 Workshop on Rapid Simulation and Performance Evaluation: Methods and Tools},
articleno = {2},
numpages = {7},
location = {Prague, Czech Republic},
series = {RAPIDO '16}
}

@article{10.1007/s10270-012-0289-1,
author = {Leitner, Andrea and Preschern, Christopher and Kreiner, Christian},
title = {Effective development of automation systems through domain-specific modeling in a small enterprise context},
year = {2014},
issue_date = {February  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-012-0289-1},
doi = {10.1007/s10270-012-0289-1},
abstract = {High development and maintenance costs and a high error rate are the major problems in the development of automation systems, which are mainly caused by bad communication and inefficient reuse methods. To overcome these problems, we propose a more systematic reuse approach. Though systematic reuse approaches such as software product lines are appealing, they tend to involve rather burdensome development and management processes. This paper focuses on small enterprises. Since such companies are often unable to perform a "big bang" adoption of the software product line, we suggest an incremental, more lightweight process to transition from single-system development to software product line development. Besides the components of the transition process, this paper discusses tool selection, DSL technology, stakeholder communication support, and business considerations. Although based on problems from the automation system domain, we believe the approach may be general enough to be applicable in other domains as well. The approach has proven successful in two case studies. First, we applied it to a research project for the automation of a logistics lab model, and in the second case (a real-life industry case), we investigated the approaches suitability for fish farm automation systems. Several metrics were collected throughout the evolution of each case, and this paper presents the data for single system development, clone&amp;own and software product line development. The results and observable effects are compared, discussed, and finally summarized in a list of lessons learned.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {35–54},
numpages = {20},
keywords = {Automation system, Domain-specific modeling, Small enterprise cost model, Software product line, System development process}
}

@inproceedings{10.1145/1509239.1509259,
author = {Niu, Nan and Easterbrook, Steve},
title = {Concept analysis for product line requirements},
year = {2009},
isbn = {9781605584423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1509239.1509259},
doi = {10.1145/1509239.1509259},
abstract = {Traditional methods characterize a software product line's requirements using either functional or quality criteria. This appears to be inadequate to assess modularity, detect interferences, and analyze trade-offs. We take advantage of both symmetric and asymmetric views of aspects, and perform formal concept analysis to examine the functional and quality requirements of an evolving product line. The resulting concept lattice provides a rich notion which allows remarkable insights into the modularity and interactions of requirements. We formulate a number of problems that aspect-oriented product line requirements engineering should address, and present our solutions according to the concept lattice. We describe a case study applying our approach to analyze a mobile game product line's requirements, and review lessons learned.},
booktitle = {Proceedings of the 8th ACM International Conference on Aspect-Oriented Software Development},
pages = {137–148},
numpages = {12},
keywords = {formal concept analysis, functional requirements profiles, product line engineering, quality attribute scenarios},
location = {Charlottesville, Virginia, USA},
series = {AOSD '09}
}

@inproceedings{10.5555/1753235.1753246,
author = {Hendrickson, Scott A. and Wang, Yang and van der Hoek, Andr\'{e} and Taylor, Richard N. and Kobsa, Alfred},
title = {Modeling PLA variation of privacy-enhancing personalized systems},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Privacy-enhancing personalized (PEP) systems address individual users' privacy preferences as well as privacy laws and regulations. Building such systems entails modeling two different domains: (a) privacy constraints as mandated by law, voluntary self-regulation, or users' individual privacy preferences, and modeled by legal professionals, and (b) software architectures as dictated by available software components and modeled by software architects. Both can evolve independently, e.g., as new laws go into effect or new components become available. In prior work, we proposed modeling PEP systems using a product line architecture (PLA). However, with an extensional PLA, these domain models became strongly entangled making it difficult to modify one without inadvertently affecting the other. This paper evaluates an approach towards modeling both domains within an intensional PLA. We find evidence that this results in a clearer separation between the two domain models, making each easier to evolve and maintain.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {71–80},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1007/s10009-013-0298-6,
author = {Ferrari, Alessio and Spagnolo, Giorgio O. and Martelli, Giacomo and Menabeni, Simone},
title = {From commercial documents to system requirements: an approach for the engineering of novel CBTC solutions},
year = {2014},
issue_date = {November  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {6},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-013-0298-6},
doi = {10.1007/s10009-013-0298-6},
abstract = {Communications-based train control (CBTC) systems are the new frontier of automated train control and operation. Currently developed CBTC platforms are actually very complex systems including several functionalities, and every installed system, developed by a different company, varies in extent, scope, number, and even names of the implemented functionalities. International standards have emerged, but they remain at a quite abstract level, mostly setting terminology. This paper presents the results of an experience in defining a global model of CBTC, by mixing semi-formal modelling and product line engineering. The effort has been based on an in-depth market analysis, not limiting to particular aspects but considering as far as possible the whole picture. The paper also describes a methodology to derive novel CBTC products from the global model, and to define system requirements for the individual CBTC components. To this end, the proposed methodology employs scenario-based requirements elicitation aided with rapid prototyping. To enhance the quality of the requirements, these are written in a constrained natural language (CNL), and evaluated with natural language processing (NLP) techniques. The final goal is to go toward a formal representation of the requirements for CBTC systems. The overall approach is discussed, and the current experience with the implementation of the method is presented. In particular, we show how the presented methodology has been used in practice to derive a novel CBTC architecture.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = nov,
pages = {647–667},
numpages = {21},
keywords = {CBTC, CENELEC, Constrained natural language, Experience report, Formal methods, Product line engineering}
}

@article{10.1007/s00766-013-0184-5,
author = {Alf\'{e}rez, Mauricio and Bonif\'{a}cio, Rodrigo and Teixeira, Leopoldo and Accioly, Paola and Kulesza, Uir\'{a} and Moreira, Ana and Ara\'{u}jo, Jo\~{a}o and Borba, Paulo},
title = {Evaluating scenario-based SPL requirements approaches: the case for modularity, stability and expressiveness},
year = {2014},
issue_date = {November  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0184-5},
doi = {10.1007/s00766-013-0184-5},
abstract = {Software product lines (SPL) provide support for productivity gains through systematic reuse. Among the various quality attributes supporting these goals, modularity,
 stability and expressiveness of feature specifications, their composition and configuration knowledge emerge as strategic values in modern software development paradigms. This paper presents a metric-based evaluation aiming at assessing how well the chosen qualities are supported by scenario-based SPL requirements
approaches. The selected approaches for this study span from type of notation (textual or graphical based), style to support variability (annotation or composition based), and specification expressiveness. They are compared using the metrics developed in a set of releases from an exemplar case study. Our major findings indicate that composition-based approaches have greater potential to support modularity and stability, and that quantification mechanisms simplify and increase expressiveness of configuration knowledge and composition specifications.},
journal = {Requir. Eng.},
month = nov,
pages = {355–376},
numpages = {22},
keywords = {Requirements specification, Software product lines, Use scenarios, Variability modeling}
}

@inproceedings{10.1007/978-3-642-41533-3_24,
author = {Gonz\'{a}lez-Huerta, Javier and Insfr\'{a}n, Emilio and Abrah\~{a}o, Silvia},
title = {Defining and Validating a Multimodel Approach for Product Architecture Derivation and Improvement},
year = {2013},
isbn = {9783642415326},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41533-3_24},
doi = {10.1007/978-3-642-41533-3_24},
abstract = {Software architectures are the key to achieving the non-functional requirements NFRs in any software project. In software product line SPL development, it is crucial to identify whether the NFRs for a specific product can be attained with the built-in architectural variation mechanisms of the product line architecture, or whether additional architectural transformations are required. This paper presents a multimodel approach for quality-driven product architecture derivation and improvement QuaDAI. A controlled experiment is also presented with the objective of comparing the effectiveness, efficiency, perceived ease of use, intention to use and perceived usefulness with regard to participants using QuaDAI as opposed to the Architecture Tradeoff Analysis Method ATAM. The results show that QuaDAI is more efficient and perceived as easier to use than ATAM, from the perspective of novice software architecture evaluators. However, the other variables were not found to be statistically significant. Further replications are needed to obtain more conclusive results.},
booktitle = {Proceedings of the 16th International Conference on Model-Driven Engineering Languages and Systems - Volume 8107},
pages = {388–404},
numpages = {17},
keywords = {Architectural Patterns, Controlled Experiment, Model Transformations, Quality Attributes, Software Product Lines}
}

@article{10.1016/j.infsof.2012.02.002,
author = {Holl, Gerald and Gr\"{u}nbacher, Paul and Rabiser, Rick},
title = {A systematic review and an expert survey on capabilities supporting multi product lines},
year = {2012},
issue_date = {August, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.02.002},
doi = {10.1016/j.infsof.2012.02.002},
abstract = {Context: Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented. Objective: The aims of this paper are thus to identify, describe, and classify existing approaches supporting MPLs and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs. Method: Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting MPLs from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry. Results: The paper discusses key research issues in MPLs and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized. Conclusions: We conclude that approaches supporting MPLs need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate MPL approaches.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {828–852},
numpages = {25},
keywords = {Large-scale systems, Multi product lines, Product line engineering, Systematic literature review}
}

@article{10.1007/s10586-019-03012-1,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e} and Ther\'{o}n, Roberto and Amo Filv\`{a}, Daniel and Fonseca Escudero, David},
title = {Connecting domain-specific features to source code: towards the automatization of dashboard generation},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-019-03012-1},
doi = {10.1007/s10586-019-03012-1},
abstract = {Dashboards are useful tools for generating knowledge and support decision-making processes, but the extended use of technologies and the increasingly available data asks for user-friendly tools that allow any user profile to exploit their data. Building tailored dashboards for any potential user profile would involve several resources and long development times, taking into account that dashboards can be framed in very different contexts that should be studied during the design processes to provide practical tools. This situation leads to the necessity of searching for methodologies that could accelerate these processes. The software product line paradigm is one recurrent method that can decrease the time-to-market of products by reusing generic core assets that can be tuned or configured to meet specific requirements. However, although this paradigm can solve issues regarding development times, the configuration of the dashboard is still a complex challenge; users’ goals, datasets, and context must be thoroughly studied to obtain a dashboard that fulfills the users’ necessities and that fosters insight delivery. This paper outlines the benefits and a potential approach to automatically configuring information dashboards by leveraging domain commonalities and code templates. The main goal is to test the functionality of a workflow that can connect external algorithms, such as artificial intelligence algorithms, to infer dashboard features and feed a generator based on the software product line paradigm.},
journal = {Cluster Computing},
month = sep,
pages = {1803–1816},
numpages = {14},
keywords = {SPL, Domain engineering, Meta-model, Information dashboards, Feature model, Artificial intelligence, Automatic configuration}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Energy Aware, Machine Learning, Software Product Line, Web System},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature model extraction from large collections of informal product descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Domain Analysis, Feature Models, Product Lines},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.5555/2820656.2820662,
author = {Chitchyan, Ruzanna and Noppen, Joost and Groher, Iris},
title = {What can software engineering do for sustainability: case of software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {Sustainable living, i.e., living within the bounds of the available environmental, social, and economic resources, is the focus of many present-day social and scientific discussions. But what does sustainability mean within the context of Software Product Line Engineering (SPLE)? And what does SPLE do for sustainable living? In this paper we take the first step towards identification of the sustainability-related characteristics relevant to SPLE. The paper also discusses how the key areas of interest to the current SPL community (as reflected by what is measured and optimised in SPLs today) relate to these sustainability characteristics.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {11–14},
numpages = {4},
location = {Florence, Italy},
series = {PLEASE '15}
}

@article{10.1145/3444689,
author = {Zhao, Liping and Alhoshan, Waad and Ferrari, Alessio and Letsholo, Keletso J. and Ajagbe, Muideen A. and Chioasca, Erol-Valeriu and Batista-Navarro, Riza T.},
title = {Natural Language Processing for Requirements Engineering: A Systematic Mapping Study},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3444689},
doi = {10.1145/3444689},
abstract = {Natural Language Processing for Requirements Engineering (NLP4RE) is an area of research and development that seeks to apply natural language processing (NLP) techniques, tools, and resources to the requirements engineering (RE) process, to support human analysts to carry out various linguistic analysis tasks on textual requirements documents, such as detecting language issues, identifying key domain concepts, and establishing requirements traceability links. This article reports on a mapping study that surveys the landscape of NLP4RE research to provide a holistic understanding of the field. Following the guidance of systematic review, the mapping study is directed by five research questions, cutting across five aspects of NLP4RE research, concerning the state of the literature, the state of empirical research, the research focus, the state of tool development, and the usage of NLP technologies. Our main results are as follows: (i) we identify a total of 404 primary studies relevant to NLP4RE, which were published over the past 36 years and from 170 different venues; (ii) most of these studies (67.08%) are solution proposals, assessed by a laboratory experiment or an example application, while only a small percentage (7%) are assessed in industrial settings; (iii) a large proportion of the studies (42.70%) focus on the requirements analysis phase, with quality defect detection as their central task and requirements specification as their commonly processed document type; (iv) 130 NLP4RE tools (i.e., RE specific NLP tools) are extracted from these studies, but only 17 of them (13.08%) are available for download; (v) 231 different NLP technologies are also identified, comprising 140 NLP techniques, 66 NLP tools, and 25 NLP resources, but most of them—particularly those novel NLP techniques and specialized tools—are used infrequently; by contrast, commonly used NLP technologies are traditional analysis techniques (e.g., POS tagging and tokenization), general-purpose tools (e.g., Stanford CoreNLP and GATE) and generic language lexicons (WordNet and British National Corpus). The mapping study not only provides a collection of the literature in NLP4RE but also, more importantly, establishes a structure to frame the existing literature&nbsp;through categorization, synthesis and conceptualization of the main theoretical concepts and relationships that encompass&nbsp;both RE and NLP aspects. Our work thus produces a conceptual framework of NLP4RE. The framework is used to identify research gaps and directions, highlight technology transfer needs, and encourage more synergies between the RE community, the NLP one, and the software&nbsp;and systems&nbsp;practitioners. Our results can be used as a starting point to frame future studies according to a well-defined terminology and can be expanded as new technologies and novel solutions emerge.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {55},
numpages = {41},
keywords = {Requirements engineering (RE), natural language processing (NLP), software engineering (SE), systematic mapping study, systematic review}
}

@inproceedings{10.5555/2666064.2666076,
author = {Hu, Jie and Yang, Ye and Wang, Qing and Ruhe, Guenther and Wang, Haitao},
title = {Value-based portfolio scoping: an industrial case study},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {Customization is considered as a promising way for better satisfying diversity of customer needs. In organizations short of resources, it is a frequent challenge to get balance between development and customization workload in order to ensure product success as well as customer satisfaction. In this paper, we proposed a value-based product portfolio scoping approach to determine optimal product scale for planning software product line adoption. The approach blends existing methods in domain analysis, requirements clustering, and valuation theory. An industrial case study is presented to demonstrate the application of the approach and its effectiveness.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {45–48},
numpages = {4},
keywords = {cost benefit, customization, product line, product portfolio, requirements analysis, scoping},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@article{10.1016/j.jss.2019.02.027,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Modelling equivalence classes of feature models with concept lattices to assist their extraction from product descriptions},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.027},
doi = {10.1016/j.jss.2019.02.027},
journal = {J. Syst. Softw.},
month = jun,
pages = {1–23},
numpages = {23},
keywords = {Software product lines, Reverse engineering, Formal concept analysis, Variability modelling, Feature models}
}

@inproceedings{10.1145/3330204.3330251,
author = {Sorgatto, Doglas W. and Paiva, D\'{e}bora M. B. and Cagnin, Maria Istela},
title = {Requirement Reuse in Business Processes Lines: Reutiliza\c{c}\~{a}o de requisitos em linhas de processos de neg\'{o}cio},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330251},
doi = {10.1145/3330204.3330251},
abstract = {The cost reduction in the Requirement Engineering process finds in business process modeling a way of aligning business goals with software requirements, and for development companies that have development demands in the same domain, greater savings can be found with the adoption of Business Process Lines (BPL). From this perspective, this paper presents the ARReq, which is an approach that allows the elicitation, specification and reuse of requirements from BPLs. It has been defined to provide quality attributes, suggested by ISO/IEC 29.148, to the functional, non-functional requirements and business rules elicited with the support of any elicitation technique applicable to BPMN business process models. A qualitative analysis was carried out and allowed to observe that ARReq is scalable, has low coupling with BPLs management approaches, besides specifying the requirements reused in the formats of user stories and requirements document and to provide a traceability matrix to support the software maintainability.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {41},
numpages = {8},
keywords = {BPMN, Business Processes Line, Requirement reuse},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@inproceedings{10.1145/3183440.3183480,
author = {Kr\"{o}her, Christian and El-Sharkawy, Sascha and Schmid, Klaus},
title = {KernelHaven: an experimentation workbench for analyzing software product lines},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183480},
doi = {10.1145/3183440.3183480},
abstract = {Systematic exploration of hypotheses is a major part of any empirical research. In software engineering, we often produce unique tools for experiments and evaluate them independently on different data sets. In this paper, we present KernelHaven as an experimentation workbench supporting a significant number of experiments in the domain of static product line analysis and verification. It addresses the need for extracting information from a variety of artifacts in this domain by means of an open plug-in infrastructure. Available plug-ins encapsulate existing tools, which can now be combined efficiently to yield new analyses. As an experimentation workbench, it provides configuration-based definitions of experiments, their documentation, and technical services, like parallelization and caching. Hence, researchers can abstract from technical details and focus on the algorithmic core of their research problem.KernelHaven supports different types of analyses, like correctness checks, metrics, etc., in its specific domain. The concepts presented in this paper can also be transferred to support researchers of other software engineering domains. The infrastructure is available under Apache 2.0: https://github.com/KernelHaven. The plug-ins are available under their individual licenses.Video: https://youtu.be/IbNc-H1NoZU},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {73–76},
numpages = {4},
keywords = {empirical software engineering, software product line analysis, static analysis, variability extraction},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.5555/2093889.2093921,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Ensan, Faezeh and Ga\v{s}evi\'{c}, Dragan and Mohabbati, Bardia},
title = {Bringing semantics to feature models with SAFMDL},
year = {2011},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of a target domain and through the development of comprehensive and variability-covering domain models. The domain models developed within the software product line development process need to cover all of the possible features and aspects of the target domain. In other words, the domain models often described using feature models should be elaborate representations of the feature space of that domain. In order to operationalize feature-based representations of a software application, appropriate implementation mechanisms need to be employed. In this paper, we propose a Semantic Web-oriented language, called Semantic Annotations for Feature Modeling Description Language (SAFMDL) that provides the means to semantically describe feature models. We will show that using SAFMDL along with Semantic Web Query techniques, we are able to bridge the gap between software product lines and SOA technology. Our proposed work allows software practitioners to use Semantic Web technology to quickly and rapidly develop new software products based on SOA technology from software product lines.},
booktitle = {Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {287–300},
numpages = {14},
location = {Toronto, Ontario, Canada},
series = {CASCON '11}
}

@article{10.1016/j.infsof.2014.04.002,
author = {Machado, Ivan Do Carmo and Mcgregor, John D. and Cavalcanti, Yguarat\~{a} Cerqueira and De Almeida, Eduardo Santana},
title = {On strategies for testing software product lines: A systematic literature review},
year = {2014},
issue_date = {October, 2014},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {56},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2014.04.002},
doi = {10.1016/j.infsof.2014.04.002},
abstract = {Context: Testing plays an important role in the quality assurance process for software product line engineering. There are many opportunities for economies of scope and scale in the testing activities, but techniques that can take advantage of these opportunities are still needed. Objective: The objective of this study is to identify testing strategies that have the potential to achieve these economies, and to provide a synthesis of available research on SPL testing strategies, to be applied towards reaching higher defect detection rates and reduced quality assurance effort. Method: We performed a literature review of two hundred seventy-six studies published from the year 1998 up to the 1st semester of 2013. We used several filters to focus the review on the most relevant studies and we give detailed analyses of the core set of studies. Results: The analysis of the reported strategies comprised two fundamental aspects for software product line testing: the selection of products for testing, and the actual test of products. Our findings indicate that the literature offers a large number of techniques to cope with such aspects. However, there is a lack of reports on realistic industrial experiences, which limits the inferences that can be drawn. Conclusion: This study showed a number of leveraged strategies that can support both the selection of products, and the actual testing of products. Future research should also benefit from the problems and advantages identified in this study.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1183–1199},
numpages = {17},
keywords = {Software product lines, Software quality, Software testing, Systematic literature review}
}

@inproceedings{10.1145/2915970.2915985,
author = {Wnuk, Krzysztof and Kollu, Ravichandra Kumar},
title = {A systematic mapping study on requirements scoping},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2915985},
doi = {10.1145/2915970.2915985},
abstract = {Context: Requirements scoping is one of the key activities in requirements management but also a major risk for project management. Continuously changing scope may create a congestion state in handling the requirements inflow which causes negative consequences, e.g. delays or scope creep. Objectives: In this paper, we look at requirements scoping literature outside Software Product Line (SPL) by exploring the current literature on the phenomenon, summarizing publication trends, performing thematic analysis and analyzing the strength of the evidence in the light of rigor and relevance assessment. Method: We run a Systematic Mapping Study (SMS) using snowballing procedure, supported by a database search for the start set identification, and identified 21 primary studies and 2 secondary studies. Results: The research interest in this area steadily increases and includes mainly case studies, validation or evaluation studies. The results were categorized into four themes: definitions, negative effects associated with scoping, challenges and identified methods/tools. The identified scope management techniques are also matched against the identified requirements scoping challenges.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {32},
numpages = {11},
keywords = {requirements scoping, snowballing, systematic mapping study},
location = {Limerick, Ireland},
series = {EASE '16}
}

@inproceedings{10.1145/1629716.1629735,
author = {Asadi, Mohsen and Mohabbati, Bardia and Kaviani, Nima and Ga\v{s}evi\'{c}, Dragan and Bo\v{s}kovi\'{c}, Marko and Hatala, Marek},
title = {Model-driven development of families of Service-Oriented Architectures},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629735},
doi = {10.1145/1629716.1629735},
abstract = {The paradigms of Service Oriented Architecture (SOA) and Software Product Line Engineering (SPLE) facilitate the development of families of software-intensive products. Software Product Line practices can be leveraged to support the development of service-oriented applications to promote the reusability of assets throughout the iterative and incremental development of software product families. Such an approach enables various service oriented business processes and software products of the same family to be systematically created and integrated. In this paper, we advocate integration of software product line engineering with model driven engineering to enable a model driven specification of software services, capable of creating software products from a family of software services. Using the proposed method, we aim to provide a consistent view of a composed software system from a higher business administration perspective to lower levels of service implementation and deployment. We demonstrate how Model Driven Engineering (MDE) can help with injecting the set of required commonalities and variabilities of a software product from a high level business process design to the lower levels of service use.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {95–102},
numpages = {8},
keywords = {business process management, semantic web, service-oriented architectures, software product lines},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1007/11663430_27,
author = {Loughran, Neil and Sampaio, Am\'{e}rico and Rashid, Awais},
title = {From requirements documents to feature models for aspect oriented product line implementation},
year = {2005},
isbn = {3540317805},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11663430_27},
doi = {10.1007/11663430_27},
abstract = {Software product line engineering has emerged as an approach to developing software which targets a given domain. However, the processes involved in developing a software product line can be time consuming and error prone without adequate lifecycle tool support. In this paper we describe our approach, NAPLES, which uses natural language processing and aspect-oriented techniques to facilitate requirements analysis, commonality and variability analysis, concern identification to derive suitable feature oriented models for implementation.},
booktitle = {Proceedings of the 2005 International Conference on Satellite Events at the MoDELS},
pages = {262–271},
numpages = {10},
location = {Montego Bay, Jamaica},
series = {MoDELS'05}
}

@article{10.1145/3229096,
author = {Logre, Ivan and D\'{e}ry-Pinna, Anne-Marie},
title = {MDE in Support of Visualization Systems Design: a Multi-Staged Approach Tailored for Multiple Roles},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {EICS},
url = {https://doi.org/10.1145/3229096},
doi = {10.1145/3229096},
abstract = {Visualization systems such as dashboards are commonly used to analyze data and support users in their decision making, in communities as different as medical care, transport and software engineering. The increasing amount of data produced and continuous development of new visualizations exacerbate the difficulty of designing such dashboards, while the visualization need is broaden to specialist and non-specialist final users. In this context, we offer a multi-user approach, based on Model Driven Engineering (MDE). The idea is for the designer to express the visualization need by characterization, according to a given taxonomy. We provide a Domain Specific Language (DSL) to design the system and a Software Product Line (SPL) to capture the technological variability of visualization widgets. We performed a user study, using a software project management use case, to validate if dashboard users and designers are able to use a taxonomy to express their visualization need.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {14},
numpages = {17},
keywords = {domain specific language, meta-model, visualization}
}

@article{10.1016/j.jss.2008.08.026,
author = {Lago, Patricia and Muccini, Henry and van Vliet, Hans},
title = {A scoped approach to traceability management},
year = {2009},
issue_date = {January, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2008.08.026},
doi = {10.1016/j.jss.2008.08.026},
abstract = {Traceability is the ability to describe and follow the life of a software artifact and a means for modeling the relations between software artifacts in an explicit way. Traceability has been successfully applied in many software engineering communities and has recently been adopted to document the transition among requirements, architecture and implementation. We present an approach to customize traceability to the situation at hand. Instead of automating tracing, or representing all possible traces, we scope the traces to be maintained to the activities stakeholders must carry out. We define core traceability paths, consisting of essential traceability links required to support the activities. We illustrate the approach through two examples: product derivation in software product lines, and release planning in software process management. By using a running software product line example, we explain why the core traceability paths identified are needed when navigating from feature to structural models and from family to product level and backward between models used in software product derivation. A feasibility study in release planning carried out in an industrial setting further illustrates the use of core traceability paths during production and measures the increase in performance of the development processes supported by our approach. These examples show that our approach can be successfully used to support both product and process traceability in a pragmatic yet efficient way.},
journal = {J. Syst. Softw.},
month = jan,
pages = {168–182},
numpages = {15},
keywords = {Software process management, Software product line, Traceability issues, Traceability paths}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {feature-oriented development, optimization, service selection, service-oriented architecture, software product line},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@article{10.3233/JID210027,
author = {Salinesi, Camille and Achtaich, Asmaa and Souissi, Nissrine and Mazo, Ra\'{u}l and Roudies, Ounsa and Villota, Angela},
title = {State-Constraint Transition: A Language for the Formal Specification of Dynamic Cyber-System Requirements},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {2},
issn = {1092-0617},
url = {https://doi.org/10.3233/JID210027},
doi = {10.3233/JID210027},
abstract = {Existing formal languages for the specification of self-adaptive cyber-physical systems focus on re-configuring the system-to-be depending on its current context, to satisfy the user’s requirements, that is by dynamically composing the software’s structure and behavior. While these approaches specify context-sensitive requirements, they rarely consider their run-time dynamic and scalable nature. The State-Constraint Transition (SCT) modeling language, introduced in this paper, provides an answer to the problems linked to the specification of dynamic requirements by introducing the concept of configuration states, in which requirements are translated into constraints. The expressiveness of existing approaches is thus extended, combining the ease of use of well-established notations, notably those based on characteristics, and those based on Finite-state Machines (FSM), with the computational power and expressiveness of the constraint programming approach. The paper briefly presents the results of the preliminary evaluation, which assesses the expressiveness, scalability, and domain independence of the SCT language.},
journal = {J. Integr. Des. Process Sci.},
month = jan,
pages = {80–99},
numpages = {20},
keywords = {Cyber-physical systems, self-adaptive systems, software product lines, requirements engineering, state-machines, constraint programming, languages}
}

@article{10.1016/j.jss.2007.10.031,
author = {Karam, Marcel and Dascalu, Sergiu and Safa, Haidar and Santina, Rami and Koteich, Zeina},
title = {A product-line architecture for web service-based visual composition of web applications},
year = {2008},
issue_date = {June, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {6},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.10.031},
doi = {10.1016/j.jss.2007.10.031},
abstract = {A web service-based web application (WSbWA) is a collection of web services or reusable proven software parts that can be discovered and invoked using standard Internet protocols. The use of these web services in the development process of WSbWAs can help overcome many problems of software use, deployment and evolution. Although the cost-effective software engineering of WSbWAs is potentially a very rewarding area, not much work has been done to accomplish short time to market conditions by viewing and dealing with WSbWAs as software products that can be derived from a common infrastructure and assets with a captured specific abstraction in the domain. Both Product Line Engineering (PLE) and Agile Methods (AMs), albeit with different philosophies, are software engineering approaches that can significantly shorten the time to market and increase the quality of products. Using the PLE approach we built, at the domain engineering level, a WSbWA-specific lightweight product-line architecture and combined it, at the application engineering level, with an Agile Method that uses a domain-specific visual language with direct manipulation and extraction capabilities of web services to perform customization and calibration of a product or WSBWA for a specific customer. To assess the effectiveness of our approach we designed and implemented a tool that we used to investigate the return on investment of the activities related to PLE and AMs. Details of our proposed approach, the related tool developed, and the experimental study performed are presented in this article together with a discussion of planned directions of future work.},
journal = {J. Syst. Softw.},
month = jun,
pages = {855–867},
numpages = {13},
keywords = {Agile methods, Product line architecture, Product line engineering, Visual languages, Web services}
}

@inproceedings{10.1145/2188286.2188304,
author = {Tawhid, Rasha and Petriu, Dorina},
title = {User-friendly approach for handling performance parameters during predictive software performance engineering},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188304},
doi = {10.1145/2188286.2188304},
abstract = {A Software Product Line (SPL) is a set of similar software systems that share a common set of features. Instead of building each product from scratch, SPL development takes advantage of the reusability of the core assets shared among the SPL members. In this work, we integrate performance analysis in the early phases of SPL development process, applying the same reusability concept to the performance annotations. Instead of annotating from scratch the UML model of every derived product, we propose to annotate the SPL model once with generic performance annotations. After deriving the model of a product from the family model by an automatic transformation, the generic performance annotations need to be bound to concrete product-specific values provided by the developer. Dealing manually with a large number of performance annotations, by asking the developer to inspect every diagram in the generated model and to extract these annotations is an error-prone process. In this paper we propose to automate the collection of all generic parameters from the product model and to present them to the developer in a user-friendly format (e.g., a spreadsheet per diagram, indicating each generic parameter together with guiding information that helps the user in providing concrete binding values). There are two kinds of generic parametric annotations handled by our approach: product-specific (corresponding to the set of features selected for the product) and platform-specific (such as device choices, network connections, middleware, and runtime environment). The following model transformations for (a) generating a product model with generic annotations from the SPL model, (b) building the spreadsheet with generic parameters and guiding information, and (c) performing the actual binding are all realized in the Atlas Transformation Language (ATL).},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {109–120},
numpages = {12},
keywords = {atl, marte, model-driven development, performance completion, performance model, spl, uml},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@inproceedings{10.1145/2973839.2973852,
author = {Santos, Ismayle S. and Rocha, Lincoln S. and Neto, Pedro A. Santos and Andrade, Rossana M. C.},
title = {Model Verification of Dynamic Software Product Lines},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973852},
doi = {10.1145/2973839.2973852},
abstract = {Dynamic Software Product Lines (DSPLs) extend the concept of Software Product Lines enabling adaptation at runtime according to context changes. Such dynamic behavior is typically designed using adaptation rules, context-triggered actions responsible for features activation and deactivation at runtime. The erroneous specification and the interleaving of adaptation rules (i.e., the parallel execution of adaptation rules) can lead DSPL to reach an undesired (improperly or defective) product configuration at runtime. Thus, in order to improve the reliability of DSPL behavior, design faults must be rigorously identified and eliminated in the early stages of DSPL development. In this paper, we address this issue introducing Dynamic Feature Transition Systems (DFTSs) that allow the modeling and formal verification of the DSPLs adaptive behavior. These transition systems are derived from the adaptation rules and a Context Kripke Structure, which is a context evolution model. Furthermore, we formally define five properties that can be used to identify existing design faults in DSPL design. Aiming to assess the feasibility of our approach, a feasibility study was conducted using two DSPLs, Mobile Visit Guides and Car. In both cases, design faults were automatically detected indicating that our formalism can help in the detection of design faults in the DSPLs adaptive behavior.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {113–122},
numpages = {10},
keywords = {Dynamic Software Product Line, Model Checking, Software Reliability, Software Verification},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@article{10.1007/s11219-011-9153-8,
author = {Mussbacher, Gunter and Ara\'{u}jo, Jo\~{a}o and Moreira, Ana and Amyot, Daniel},
title = {AoURN-based modeling and analysis of software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9153-8},
doi = {10.1007/s11219-011-9153-8},
abstract = {Software Product Line Engineering concerns itself with domain engineering and application engineering. During domain engineering, the whole product family is modeled with a preferred flavor of feature models and additional models as required (e.g., domain models or scenario-based models). During application engineering, the focus shifts toward a single family member and the configuration of the member's features. Recently, aspectual concepts have been employed to better encapsulate individual features of a Software Product Line (SPL), but the existing body of SPL work does not include a unified reasoning framework that integrates aspect-oriented feature description artifacts with the capability to reason about stakeholders' goals while taking feature interactions into consideration. Goal-oriented SPL approaches have been proposed, but do not provide analysis capabilities that help modelers meet the needs of the numerous stakeholders involved in an SPL while at the same time considering feature interactions. We present an aspect-oriented SPL approach for the requirements phase that allows modelers (a) to capture features, goals, and scenarios in a unified framework and (b) to reason about stakeholders' needs and perform trade-off analyses while considering undesirable interactions that are not obvious from the feature model. The approach is based on the Aspect-oriented User Requirements Notation (AoURN) and helps identify, prioritize, and choose products based on analysis results provided by AoURN editor and analysis tools. We apply the AoURN-based SPL framework to the Via Verde SPL to demonstrate the feasibility of this approach through the selection of a Via Verde product configuration that satisfies stakeholders' needs and results in a high-level, scenario-based specification that is free from undesirable feature interactions.},
journal = {Software Quality Journal},
month = sep,
pages = {645–687},
numpages = {43},
keywords = {Aspect-oriented modeling, Feature interactions, Goal-based requirements engineering, Scenario-based requirements engineering, Software product lines, User Requirements Notation}
}

@article{10.1016/j.scico.2012.06.007,
author = {Cetina, Carlos and Giner, Pau and Fons, Joan and Pelechano, Vicente},
title = {Prototyping Dynamic Software Product Lines to evaluate run-time reconfigurations},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.06.007},
doi = {10.1016/j.scico.2012.06.007},
abstract = {Dynamic Software Product Lines (DSPL) encompass systems that are capable of modifying their own behavior with respect to changes in their operating environment by using run-time reconfigurations. A failure in these reconfigurations can directly impact the user experience since the reconfigurations are performed when the system is already under the users control. In this work, we prototype a Smart Hotel DSPL to evaluate the reliability-based risk of the DSPL reconfigurations, specifically, the probability of malfunctioning (Availability) and the consequences of malfunctioning (Severity). This DSPL prototype was performed with the participation of human subjects by means of a Smart Hotel case study which was deployed with real devices. Moreover, we successfully identified and addressed two challenges associated with the involvement of human subjects in DSPL prototyping: enabling participants to (1) trigger the run-time reconfigurations and to (2) understand the effects of the reconfigurations. The evaluation of the case study reveals positive results regarding both Availability and Severity. However, the participant feedback highlights issues with recovering from a failed reconfiguration or a reconfiguration triggered by mistake. To address these issues, we discuss some guidelines learned in the case study. Finally, although the results achieved by the DSPL may be considered satisfactory for its particular domain, DSPL engineers must provide users with more control over the reconfigurations or the users will not be comfortable with DSPLs.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {2399–2413},
numpages = {15},
keywords = {Dynamic Software Product Line, Smart Hotel, Variability modeling}
}

@inproceedings{10.5555/3049877.3049879,
author = {Lapouchnian, Alexei and Yu, Yijun and Liaskos, Sotirios and Mylopoulos, John},
title = {Requirements-driven design of autonomic application software},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying the objectives of the system, analyzing alternative ways of how these objectives can be met, and designing a system that supports all or some of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and demonstrates this on an example.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {23–37},
numpages = {15},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@article{10.5555/2873826.2874010,
author = {Saeed, Mazin and Saleh, Faisal and Al-Insaif, Sadiq and El-Attar, Mohamed},
title = {Empirical validating the cognitive effectiveness of a new feature diagrams visual syntax},
year = {2016},
issue_date = {March 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {71},
number = {C},
issn = {0950-5849},
abstract = {ContextFeature models are commonly used to capture and communicate the commonality and variability of features in a Software Product Line. The core component of Feature models is feature diagrams, which graphically depict features in a hierarchical form. In previous work we have proposed a new notation that aims to improve the cognitive effectiveness of feature diagrams. ObjectiveThe objective of this paper is to empirically validate the cognitive effectiveness of the new feature diagrams notation in comparison to its original form. MethodsWe use two distinct empirical user-studies to validate the new notation. The first empirical study uses the survey approach while the second study is a subject-based experiment. The survey study investigates the semantic transparency of the new notation while the second study investigates the speed and accuracy of reading the notation. ResultsThe results of the studies indicate that the proposed changes have significantly improved its cognitive effectiveness. ConclusionsThe cognitive effectiveness of feature diagrams has been improved, however there remains further research for full acceptance of the new notation by its potential user community.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {1–26},
numpages = {26},
keywords = {Feature models, Software product lines, Visual syntax evaluation}
}

@inproceedings{10.1007/978-3-642-34026-0_34,
author = {Kitamura, Takashi and Do, Ngoc Thi Bich and Ohsaki, Hitoshi and Fang, Ling and Yatabe, Shunsuke},
title = {Test-Case design by feature trees},
year = {2012},
isbn = {9783642340253},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34026-0_34},
doi = {10.1007/978-3-642-34026-0_34},
abstract = {This paper proposes a test-case design method for black-box testing, called "Feature Oriented Testing (FOT)". The method is realized by applying Feature Models (FMs) developed in software product line engineering to test-case designs. We develop a graphical language for test-case design called "Feature Trees for Testing (FTT)" based on FMs. To firmly underpin the method, we provide a formal semantics of FTT, by means of test-cases derived from test-case designs modelled with FTT. Based on the semantics we develop an automated test-suite generation and correctness checking of test-case designs using SAT, as computer-aided analysis techniques of the method. Feasibility of the method is demonstrated from several viewpoints including its implementation, complexity analysis, experiments, a case study, and an assistant tool.},
booktitle = {Proceedings of the 5th International Conference on Leveraging Applications of Formal Methods, Verification and Validation: Technologies for Mastering Change - Volume Part I},
pages = {458–473},
numpages = {16},
keywords = {SAT-based analysis, black-box testing, combination testing},
location = {Heraklion, Crete, Greece},
series = {ISoLA'12}
}

@article{10.1145/1183236.1183238,
author = {Maamar, Zakaria and Benslimane, Djamal and Narendra, Nanjangud C.},
title = {What can context do for web services?},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183238},
doi = {10.1145/1183236.1183238},
abstract = {Context-aware Web service would significantly benefit the interactions between human, applications, and the environment.},
journal = {Commun. ACM},
month = dec,
pages = {98–103},
numpages = {6}
}

@article{10.1016/j.scico.2012.06.002,
author = {Th\"{u}m, Thomas and K\"{a}stner, Christian and Benduhn, Fabian and Meinicke, Jens and Saake, Gunter and Leich, Thomas},
title = {FeatureIDE: An extensible framework for feature-oriented software development},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {79},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.06.002},
doi = {10.1016/j.scico.2012.06.002},
abstract = {FeatureIDE is an open-source framework for feature-oriented software development (FOSD) based on Eclipse. FOSD is a paradigm for the construction, customization, and synthesis of software systems. Code artifacts are mapped to features, and a customized software system can be generated given a selection of features. The set of software systems that can be generated is called a software product line (SPL). FeatureIDE supports several FOSD implementation techniques such as feature-oriented programming, aspect-oriented programming, delta-oriented programming, and preprocessors. All phases of FOSD are supported in FeatureIDE, namely domain analysis, requirements analysis, domain implementation, and software generation.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {70–85},
numpages = {16},
keywords = {Aspect-oriented programming, Delta-oriented programming, Feature modeling, Feature-oriented programming, Feature-oriented software development, Preprocessors, Software product lines, Tool support}
}

@article{10.1016/j.datak.2006.06.009,
author = {Kim, Minseong and Park, Sooyong and Sugumaran, Vijayan and Yang, Hwasil},
title = {Managing requirements conflicts in software product lines: A goal and scenario based approach},
year = {2007},
issue_date = {June, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {61},
number = {3},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2006.06.009},
doi = {10.1016/j.datak.2006.06.009},
abstract = {The product line approach is recognized as a successful approach to reuse in software development. However, in many cases, it has resulted in interactions between requirements and/or features. Interaction detection, especially conflict detection between requirements has become more challenging. Thus, detecting conflicts between requirements is essential for successful product line development. Formal methods have been proposed to address this problem, however, they are hard to understand by non-experts and are limited to restricted domains. In addition, there is no overall process that covers all the steps for managing conflicts. We propose an approach for systematically identifying and managing requirements conflicts, which is based on requirements partition in natural language and supported by a tool. To demonstrate its feasibility, the proposed approach has been applied to the home integration system (HIS) domain and the results are discussed.},
journal = {Data Knowl. Eng.},
month = jun,
pages = {417–432},
numpages = {16},
keywords = {Goal and scenario authoring, Requirements conflicts, Requirements partitioning, Software product line, Syntactic and semantic requirements conflict detection}
}

@article{10.1016/j.jss.2009.06.048,
author = {Khurum, Mahvish and Gorschek, Tony},
title = {A systematic review of domain analysis solutions for product lines},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.048},
doi = {10.1016/j.jss.2009.06.048},
abstract = {Domain analysis is crucial and central to software product line engineering (SPLE) as it is one of the main instruments to decide what to include in a product and how it should fit in to the overall software product line. For this reason many domain analysis solutions have been proposed both by researchers and industry practitioners. Domain analysis comprises various modeling and scoping activities. This paper presents a systematic review of all the domain analysis solutions presented until 2007. The goal of the review is to analyze the level of industrial application and/or empirical validation of the proposed solutions with the purpose of mapping maturity in terms of industrial application, as well as to what extent proposed solutions might have been evaluated in terms of usability and usefulness. The finding of this review indicates that, although many new domain analysis solutions for software product lines have been proposed over the years, the absence of qualitative and quantitative results from empirical application and/or validation makes it hard to evaluate the potential of proposed solutions with respect to their usability and/or usefulness for industry adoption. The detailed results of the systematic review can be used by individual researchers to see large gaps in research that give opportunities for future work, and from a general research perspective lessons can be learned from the absence of validation as well as from good examples presented. From an industry practitioner view, the results can be used to gauge as to what extent solutions have been applied and/or validated and in what manner, both valuable as input prior to industry adoption of a domain analysis solution.},
journal = {J. Syst. Softw.},
month = dec,
pages = {1982–2003},
numpages = {22},
keywords = {Domain analysis, Domain modeling, Domain scoping, Empirical evidence, Systematic review, Usability, Usefulness}
}

@article{10.1016/j.datak.2012.09.005,
author = {Reinhartz-Berger, Iris and Sturm, Arnon and Wand, Yair},
title = {Comparing functionality of software systems: An ontological approach},
year = {2013},
issue_date = {September, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {87},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2012.09.005},
doi = {10.1016/j.datak.2012.09.005},
abstract = {Organizations can reduce the costs and enhance the quality of required software by adapting existing software systems. Software adaptation decisions often involve comparing alternatives on two criteria: (1) how well a system meets users' requirements and (2) the effort required for adapting the system. These criteria reflect two points of view - of users and of developers. Common to both views is the notion of functionality, which software developers have traditionally used for effort estimation utilizing concepts such as function points. However, users involved in selecting systems are not necessarily familiar with such concepts. We propose an approach for comparing software functionality from users' point of view. The approach employs ontological concepts to define functionality in terms of system behaviors. To evaluate whether or not the approach is also usable by software developers, we conducted an exploratory experiment. In the experiment, software engineering students ranked descriptions of software systems on the amount of changes needed to adapt the systems to given requirements. The results demonstrated that the ontological approach was usable after a short training and provided results comparable to ranking done by expert software developers. We also compared the ontological approach to a method which employed function point concepts. The results showed no statistically significant differences in performance, but there seemed to be an advantage to the ontological approach for cases that were difficult to analyze. Moreover, it took less time to apply the ontological approach than the function point-based approach, and the difference was statistically significant.},
journal = {Data Knowl. Eng.},
month = sep,
pages = {320–338},
numpages = {19},
keywords = {Development effort estimation, Function point analysis, Ontologies, Requirements engineering, Software comparison, Variability management}
}

@article{10.1016/j.eswa.2014.05.049,
author = {Mizouni, Rabeb and Matar, Mohammad Abu and Mahmoud, Zaid Al and Alzahmi, Salwa and Salah, Aziz},
title = {A framework for context-aware self-adaptive mobile applications SPL},
year = {2014},
issue_date = {November, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {16},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.05.049},
doi = {10.1016/j.eswa.2014.05.049},
abstract = {Mobile Applications are rapidly emerging as a convenient medium for using a variety of services. Over time and with the high penetration of smartphones in society, self-adaptation has become an essential capability required by mobile application users. In an ideal scenario, an application is required to adjust its behavior according to the current context of its use. This raises the challenge in mobile computing towards the design and development of applications that sense and react to contextual changes to provide a value-added user experience. In its general sense, context information can relate to the environment, the user, or the device status. In this paper, we propose a novel framework for building context aware and adaptive mobile applications. Based on feature modeling and Software Product Lines (SPL) concepts, this framework guides the modeling of adaptability at design time and supports context awareness and adaptability at runtime. In the core of the approach, is a feature meta-model that incorporates, in addition to SPL concepts, application feature priorities to drive the adaptability. A tool, based on that feature model, is presented to model the mobile application features and to derive the SPL members. A mobile framework, built on top of OSGI framework to dynamically adapt the application at runtime is also described.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {7549–7564},
numpages = {16},
keywords = {Feature priority, Mobile devices, Multi-view variability model, Runtime adaptability, SPL}
}

@article{10.1016/j.jss.2017.11.004,
author = {Carvalho, Michelle Larissa Luciano and da Silva, Matheus Lessa Gonalves and Gomes, Gecynalda Soares da Silva and Santos, Alcemir Rodrigues and Machado, Ivan do Carmo and Souza, Magno Lu de Jesus and de Almeida, Eduardo Santana},
title = {On the implementation of dynamic software product lines},
year = {2018},
issue_date = {February 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.11.004},
doi = {10.1016/j.jss.2017.11.004},
abstract = {A set of criteria to characterize mechanisms suitable to implement dynamic variability.A characterization of thirteen DSPL-ready variability mechanisms.Empirical evaluation of OOP and AOP from the perspective of DSPL evolution.Evidence showing that AOP is a feasible strategy to implement DSPL projects. Dynamic Software Product Line (DSPL) engineering is a paradigm aimed at handling adaptations at runtime. An inherent challenge in DSPL engineering is to reduce the design complexity of adaptable software, particularly in terms of evolution. Existing research only recently started to investigate evolution in this field, but does not assess the impact of different implementations under software quality in evolutionary scenarios. This work presents a characterization of thirteen dynamic variability mechanisms. Based on such characterization, we implemented a DSPL using Object-oriented Programming (OOP) mechanisms. From this implementation, we evidenced that DSPL requires changes and extensions to design, in terms of functionality and adaptation capabilities. Since Aspect-oriented Programming (AOP) was well ranked according to characterization and some studies have demonstrated the likely synergies between AOP and DSPL, we decided to compare it with OOP. We empirically evaluated how OOP and AOP could affect source code quality from the viewpoint of an evolving DSPL. As a result, AOP yields better results in terms of size, SoC, cohesion, and coupling measures. Conversely, AOP provides lower change propagation impact. Although the packages in AOP were more susceptible to changes than in OOP, we could indicate that AOP may be a feasible strategy for DSPL implementation.},
journal = {J. Syst. Softw.},
month = feb,
pages = {74–100},
numpages = {27},
keywords = {Dynamic software product lines, Evidence-based software engineering, Software evolution, Variability mechanisms}
}

@article{10.1016/j.infsof.2010.03.014,
author = {Alves, Vander and Niu, Nan and Alves, Carina and Valen\c{c}a, George},
title = {Requirements engineering for software product lines: A systematic literature review},
year = {2010},
issue_date = {August, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.03.014},
doi = {10.1016/j.infsof.2010.03.014},
abstract = {Context: Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed. Objective: This paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement. Method: A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009. Results: The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy. Conclusions: Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {806–820},
numpages = {15},
keywords = {Requirements engineering, Software product lines, Systematic literature review}
}

@inproceedings{10.1007/978-3-030-79382-1_6,
author = {Burgue\~{n}o, Loli and Claris\'{o}, Robert and G\'{e}rard, S\'{e}bastien and Li, Shuai and Cabot, Jordi},
title = {An NLP-Based Architecture for&nbsp;the&nbsp;Autocompletion of Partial Domain Models},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_6},
doi = {10.1007/978-3-030-79382-1_6},
abstract = {Domain models capture the key concepts and relationships of a business domain. Typically, domain models are manually defined by software designers in the initial phases of a software development cycle, based on their interactions with the client and their own domain expertise. Given the key role of domain models in the quality of the final system, it is important that they properly reflect the reality of the business.To facilitate the definition of domain models and improve their quality, we propose to move towards a more assisted domain modeling building process where an NLP-based assistant will provide autocomplete suggestions for the partial model under construction based on the automatic analysis of the textual information available for the project (contextual knowledge) and/or its related business domain (general knowledge). The process will also take into account the feedback collected from the designer’s interaction with the assistant. We have developed a proof-of-concept tool and have performed a preliminary evaluation that shows promising results.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {91–106},
numpages = {16},
keywords = {Domain model, Autocomplete, Modeling recommendations, Assistant, Natural language processing},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3297280.3297511,
author = {Allian, Ana Paula and Sena, Bruno and Nakagawa, Elisa Yumi},
title = {Evaluating variability at the software architecture level: an overview},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297511},
doi = {10.1145/3297280.3297511},
abstract = {Software architecture are designed for developing software systems needed for a diverse of business goals. Consequently, architecture has to deal with a significant amount of variability in functionality and quality attributes to create different products. Due to this variability, the evaluation in software architectures is much more complex, as different alternatives of systems might be developed leading to an expensive and time consuming task. Several methods and techniques have been proposed to evaluate product line architectures (PLAs) aiming to asses whether or not the architecture will lead to the desired quality attributes. However, there is little consensus on the existing evaluations methods is most suitable for evaluating variability in software architectures, instead of only considering PLAs. Understanding and explicitly evaluating variations in architectures is a cost-effective way of mitigating substantial risk to organizations and their software systems. Therefore, the main contribution of this research work is to present the state of the art about means for evaluating software architectures (including, PLAs, software architectures, reference and enterprise architectures) that contain variability information. We conducted a Systematic Mapping Study (SMS) to provide an overview and insight to practitioners about the most relevant techniques and methods developed for this evaluation. Results indicate that most evaluation techniques assess variability as a quality attribute in PLAs through scenario-based; however, little is known about their real effectiveness as most studies present gaps and lack of evaluation, which difficult the usage of such techniques in an industrial environment.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2354–2361},
numpages = {8},
keywords = {evaluation, software architecture, software variability, systematic mapping study},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1007/s00766-013-0187-2,
author = {Zdravkovic, Jelena and Svee, Eric-Oluf and Giannoulis, Constantinos},
title = {Capturing consumer preferences as requirements for software product lines},
year = {2015},
issue_date = {March     2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0187-2},
doi = {10.1007/s00766-013-0187-2},
abstract = {Delivering great consumer experiences in competitive market conditions requires software vendors to move away from traditional modes of thinking to an outside-in perspective, one that shifts their business to becoming consumer-centric. Requirements engineers operating in these conditions thus need new means to both capture real preferences of consumers and then relate them to requirements for software customized in different ways to fit anyone. Additionally, because system development models require inputs that are more concrete than abstract, the indistinct values of consumers need to be classified and formalized. To address this challenge, this study aims to establish a conceptual link between preferences of consumers and system requirements, using software product line (SPL) as a means for systematically accommodating the variations within the preferences. The novelty of this study is a conceptual model of consumer preference, which integrates generic value frameworks from both psychology and marketing, and a method for its transformation to requirements for SPL using a goal-oriented RE framework as the mediator. The presented artifacts are grounded in an empirical study related to the development of a system for online education.},
journal = {Requir. Eng.},
month = mar,
pages = {71–90},
numpages = {20},
keywords = {Consumer value, Features, Goal modeling, Requirements, SPL, Value, Value modeling}
}

@inproceedings{10.1145/3377812.3381399,
author = {Abbas, Muhammad},
title = {Variability aware requirements reuse analysis},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381399},
doi = {10.1145/3377812.3381399},
abstract = {Problem: The goal of a software product line is to aid quick and quality delivery of software products, sharing common features. Effectively achieving the above-mentioned goals requires reuse analysis of the product line features. Existing requirements reuse analysis approaches are not focused on recommending product line features, that can be reused to realize new customer requirements. Hypothesis: Given that the customer requirements are linked to product line features' description satisfying them: then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customer requirements can be evaluated against existing customer requirements and reuse of product line features can be recommended. Contributions: We treated the problem of feature reuse analysis as a text classification problem at the requirements-level. We use Natural Language Processing and clustering to recommend reuse of features based on similarities and historic reuse information. The recommendations can be used to realize new customer requirements.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {190–193},
numpages = {4},
keywords = {product line, requirements, similarities, software reuse, variability},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1016/j.infsof.2019.04.011,
author = {Rodrigues, Arthur and Rodrigues, Gena\'{\i}na Nunes and Knauss, Alessia and Ali, Raian and Andrade, Hugo},
title = {Enhancing context specifications for dependable adaptive systems: A data mining approach},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.04.011},
doi = {10.1016/j.infsof.2019.04.011},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {115–131},
numpages = {17},
keywords = {Self-adaptive system, Context uncertainty, Data mining, Design time, Goal modelling, Dependability}
}

@inproceedings{10.5555/1885639.1885667,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Gasevic, Dragan and Soltani, Samaneh},
title = {Stratified analytic hierarchy process: prioritization and selection of software features},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product line engineering allows for the rapid development of variants of a domain specific application by using a common set of reusable assets often known as core assets. Variability modeling is a critical issue in product line engineering, where the use of feature modeling is one of most commonly used formalisms. To support an effective and automated derivation of concrete products for a product family, staged configuration has been proposed in the research literature. In this paper, we propose the integration of well-known requirements engineering principles into stage configuration. Being inspired by the well-established Preview requirements engineering framework, we initially propose an extension of feature models with capabilities for capturing business oriented requirements. This representation enables a more effective capturing of stakeholders' preferences over the business requirements and objectives (e.g.,. implementation costs or security) in the form of fuzzy linguistic variables (e.g., high, medium, and low). On top of this extension, we propose a novel method, the Stratified Analytic Hierarchy process, which first helps to rank and select the most relevant high level business objectives for the target stakeholders (e.g., security over implementation costs), and then helps to rank and select the most relevant features from the feature model to be used as the starting point in the staged configuration process. Besides a complete formalization of the process, we define the place of our proposal in existing software product line lifecycles as well as demonstrate the use of our proposal on the widely-used e-Shop case study. Finally, we report on the results of our user study, which indicates a high appreciation of the proposed method by the participating industrial software developers. The tool support for S-AHP is also introduced.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {300–315},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.5555/1759779.1759839,
author = {Moon, Mikyeong and Yeom, Keunhyuk},
title = {Product line architecture for RFID-enabled applications},
year = {2007},
isbn = {9783540720348},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Radio Frequency Identification (RFID) is an established technology and has the potential, in a variety of applications, to significantly reduce cost and improve performance. RFID may dramatically change an organization's capacity to obtain real-time information concerning the location and properties of tagged people or objects. However, simply adding RFID to an existing process is a losing proposition. The entire process should be reconsidered in order to take advantage of real-time inventory data and the near real-time tracking and management of inventory. As RFID-enabled applications will fulfill similar tasks across a range of processes adapted to use the data gained from RFID tags, they can be considered as software products derived from a common infrastructure and assets that capture specific abstractions in the domain. That is, it may be appropriate to design RFID-enabled applications as elements of a product line. This paper discusses product line architecture for RFID-enabled applications. In developing this architecture, common activities are identified among the RFID-enabled applications and the variability in the common activities is analyzed in detail using variation point concepts. A product line architecture explicitly representing commonality and variability is described using UML activity diagrams. Sharing a common architecture and reusing assets to deploy recurrent services may be considered an advantage in terms of economic significance and overall quality.},
booktitle = {Proceedings of the 10th International Conference on Business Information Systems},
pages = {638–651},
numpages = {14},
keywords = {RFID, RFID-enabled application, product line architecture, software product line},
location = {Poznan, Poland},
series = {BIS'07}
}

@inproceedings{10.1145/1982185.1982333,
author = {Varela, Patr\'{\i}cia and Ara\'{u}jo, Jo\~{a}o and Brito, Isabel and Moreira, Ana},
title = {Aspect-oriented analysis for software product lines requirements engineering},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982333},
doi = {10.1145/1982185.1982333},
abstract = {Requirements analysis and modeling for Software Product Lines demands the use of feature models, but also requires additional models to help identifying, describing, and specifying features. Traditional approaches usually perform this manually and, in general, the identification and modularization of crosscutting features is ignored, or not handled systematically. This hinders requirements change. We propose an aspect-oriented approach for SPL enriched to automatically derive feature models where crosscutting features are identified and modularized using aspect-oriented concepts and techniques. This is achieved by adapting and extending the AORA (Aspect-Oriented Requirements Analysis) approach. AORA provides templates to specify and organize requirements based on concerns and responsibilities. A set of heuristics is defined to help identifying features and their dependencies in a product line. A tool was developed to automatically generate the feature model from AORA templates.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {667–674},
numpages = {8},
keywords = {aspect-oriented requirements analysis, software product lines},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/1944892.1944894,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Th\"{u}m, Thomas and Saake, Gunter},
title = {Multi-dimensional variability modeling},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944894},
doi = {10.1145/1944892.1944894},
abstract = {The variability of a software product line (SPL)is often described with a feature model. To avoid highly complex models, stakeholders usually try to separate different variability dimensions, such as domain variability and implementation variability. This results in distinct variability models, which are easier to handle than one large model. On the other hand, it is sometimes required to analyze the variability dimensions of an SPL in combination using a single model only. To combine separate modeling and integrated analysis of variability, we present Velvet, a language for multi-dimensional variability modeling. Velvet allows stakeholders to model each variability dimension of an SPL separately and to compose the separated dimensions on demand. This improves reuse of feature models and supports independent modeling variability dimensions. Furthermore, Velvet integrates feature modeling and configuration in a single language. The combination of both concepts creates further reuse opportunities and allows stakeholders to independently configure variability dimensions.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {11–20},
numpages = {10},
keywords = {feature models, separation of concerns, variability modeling},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@inproceedings{10.1145/1900008.1900087,
author = {Bhowmik, Tanmay and Niu, Nan and Allen, Edward B.},
title = {Modular development and verification of domain requirements via model checking},
year = {2010},
isbn = {9781450300643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1900008.1900087},
doi = {10.1145/1900008.1900087},
abstract = {A holistic domain model of a software product line is costly to build and difficult to verify and evolve. We propose a framework to incrementally develop domain requirements and to iteratively verify behavioral properties through model checking. We leverage state vectors to derive both local and global properties, and co-develop statechart models with temporal specifications in a modular way. We illustrate our framework using a worked example. The study shows that our framework can effectively detect inconsistencies and tighten the development feedback loop by automatically verifying domain properties.},
booktitle = {Proceedings of the 48th Annual ACM Southeast Conference},
articleno = {58},
numpages = {4},
keywords = {domain requirements, model checking, product lines, verification},
location = {Oxford, Mississippi},
series = {ACMSE '10}
}

@inproceedings{10.1007/978-3-030-64266-2_5,
author = {Gottschalk, Sebastian and Yigitbas, Enes and Schmidt, Eugen and Engels, Gregor},
title = {Model-Based Product Configuration in Augmented Reality Applications},
year = {2020},
isbn = {978-3-030-64265-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64266-2_5},
doi = {10.1007/978-3-030-64266-2_5},
abstract = {Augmented Reality (AR) has recently found high attention in mobile shopping apps such as in domains like furniture or decoration. Here, the developers of the apps focus on the positioning of atomic 3D objects in the physical environment. With this focus, they neglect the configuration of multi-faceted 3D object composition according to the user needs and environmental constraints. To tackle these challenges, we present a model-based approach to support AR-assisted product configuration based on the concept of Dynamic Software Product Lines. Our approach splits products (e.g. table) into parts (e.g. tabletop, table legs, funnier) with their 3D objects and additional information (e.g. name, price). The possible products, which can be configured out of these parts, are stored in a feature model. At runtime, this feature model can be used to configure 3D object compositions out of the product parts and adapt to user needs and environmental constraints. The benefits of this approach are demonstrated by a case study of configuring modular kitchens with the help of a prototypical mobile-based implementation.},
booktitle = {Human-Centered Software Engineering: 8th IFIP WG 13.2 International Working Conference, HCSE 2020, Eindhoven, The Netherlands, November 30 – December 2, 2020, Proceedings},
pages = {84–104},
numpages = {21},
keywords = {Dynamic Software Product Lines, Runtime adaptation, Augmented Ueality, Product configuration},
location = {Eindhoven, The Netherlands}
}

@inproceedings{10.1145/3180155.3180257,
author = {Xue, Yinxing and Li, Yan-Fu},
title = {Multi-objective integer programming approaches for solving optimal feature selection problem: a new perspective on multi-objective optimization problems in SBSE},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180257},
doi = {10.1145/3180155.3180257},
abstract = {The optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study we first expose the mathematical nature of this problem --- multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution).},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1231–1242},
numpages = {12},
keywords = {multi-objective integer programming (MOIP), multi-objective optimization (MOO), optimal feature selection problem},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1016/j.csi.2008.03.004,
author = {Mellado, Daniel and Fern\'{a}ndez-Medina, Eduardo and Piattini, Mario},
title = {Towards security requirements management for software product lines: A security domain requirements engineering process},
year = {2008},
issue_date = {August, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {30},
number = {6},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2008.03.004},
doi = {10.1016/j.csi.2008.03.004},
abstract = {Security and requirements engineering are one of the most important factors of success in the development of a software product line due to the complexity and extensive nature of them, given that a weakness in security can cause problems throughout the products of a product line. The main contribution of this work is that of providing a security standard-based process for software product line development, which is an add-in of activities in the domain engineering. This process deals with security requirements from the early stages of the product line lifecycle in a systematic and intuitive way especially adapted for product line based development. It is based on the use of the latest security requirements techniques, together with the integration of the Common Criteria (ISO/IEC 15408) and the ISO/IEC 17799 controls into the product line lifecycle. Additionally, it deals with security artefacts variability and traceability, providing us with a Security Core Assets Repository. Moreover, it facilitates the conformance to the most relevant security standards with regard to the management of security requirements, such as ISO/IEC 27001 and ISO/IEC 17799. Finally, we will illustrate our proposed process by describing part of a real case study, as a preliminary validation of it.},
journal = {Comput. Stand. Interfaces},
month = aug,
pages = {361–371},
numpages = {11},
keywords = {Common Criteria, ISMS, ISO/IEC 17799, ISO/IEC 27001, Product lines, Security requirement, Security requirements engineering}
}

@article{10.1016/j.infsof.2021.106620,
author = {Tran, Huynh Khanh Vi and Unterkalmsteiner, Michael and B\"{o}rstler, J\"{u}rgen and Ali, Nauman bin},
title = {Assessing test artifact quality—A tertiary study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106620},
doi = {10.1016/j.infsof.2021.106620},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {22},
keywords = {Software testing, Test case quality, Test suite quality, Test artifact quality, Quality assurance}
}

@article{10.1016/j.jss.2011.06.026,
author = {Guo, Jianmei and White, Jules and Wang, Guangxin and Li, Jian and Wang, Yinglin},
title = {A genetic algorithm for optimized feature selection with resource constraints in software product lines},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.06.026},
doi = {10.1016/j.jss.2011.06.026},
abstract = {Abstract: Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86-97% of the optimality of other automated feature selection algorithms and in 45-99% less time than existing exact and heuristic feature selection techniques.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2208–2221},
numpages = {14},
keywords = {Configuration, Feature models, Genetic algorithm, Optimization, Product derivation, Software product lines}
}

@article{10.1016/j.infsof.2006.05.003,
author = {Olumofin, Femi G. and Mi\v{s}i\'{c}, Vojislav B.},
title = {A holistic architecture assessment method for software product lines},
year = {2007},
issue_date = {April, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.05.003},
doi = {10.1016/j.infsof.2006.05.003},
abstract = {The success of architecture-centric development of software product lines is critically dependent upon the availability of suitable architecture assessment methods. While a number of architecture assessment methods are available and some of them have been widely used in the process of evaluating single product architectures, none of them is equipped to deal with the main challenges of product line development. In this paper we present an adaptation of the Architecture Tradeoff Analysis Method (ATAM) for the task of assessing product line architectures. The new method, labeled Holistic Product Line Architecture Assessment (HoPLAA), uses a holistic approach that focuses on risks and quality attribute tradeoffs - not only for the common product line architecture, but for the individual product architectures as well. In addition, it prescribes a qualitative analytical treatment of variation points using scenarios. The use of the new method is illustrated through a case study.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {309–323},
numpages = {15},
keywords = {Architecture Tradeoff Analysis Method (ATAM), Software architecture assessment, Software product line architectures}
}

@article{10.4018/JITR.2018010104,
author = {Vegendla, Aparna and Duc, Anh Nguyen and Gao, Shang and Sindre, Guttorm},
title = {A Systematic Mapping Study on Requirements Engineering in Software Ecosystems},
year = {2018},
issue_date = {January 2018},
publisher = {IGI Global},
address = {USA},
volume = {11},
number = {1},
issn = {1938-7857},
url = {https://doi.org/10.4018/JITR.2018010104},
doi = {10.4018/JITR.2018010104},
abstract = {Software ecosystems SECOs and open innovation processes have been claimed as a way forward for the software industry. A proper understanding of requirements is as important for SECOs as for more traditional ones. This article presents a mapping study on the issues of RE and quality aspects in SECOs. Our findings indicate that among the various phases or subtasks of RE, most of the SECO specific research has been accomplished on elicitation, analysis, and modeling. On the other hand, requirement selection, prioritization, verification, and traceability has attracted few published studies. Among the various quality attributes, most of the SECOs research has been performed on security, performance and testability. On the other hand, reliability, safety, maintainability, transparency, usability attracted few published studies. The article provides a review of the academic literature about SECO-related RE activities, modeling approaches, and quality attributes, positions the source publications in a taxonomy of issues and identifies gaps where there has been little research.},
journal = {J. Inf. Technol. Res.},
month = jan,
pages = {49–69},
numpages = {21},
keywords = {Mapping Study, Requirements Engineering, Software Ecosystem}
}

@article{10.1016/j.cl.2018.05.004,
author = {Combemale, Benoit and Kienzle, J\"{o}rg and Mussbacher, Gunter and Barais, Olivier and Bousse, Erwan and Cazzola, Walter and Collet, Philippe and Degueule, Thomas and Heinrich, Robert and J\'{e}z\'{e}quel, Jean-Marc and Leduc, Manuel and Mayerhofer, Tanja and Mosser, S\'{e}bastien and Sch\"{o}ttle, Matthias and Strittmatter, Misha and Wortmann, Andreas},
title = {Concern-oriented language development (COLD): Fostering reuse in language engineering},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.05.004},
doi = {10.1016/j.cl.2018.05.004},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {139–155},
numpages = {17},
keywords = {Domain-specific languages, Language concern, Language reuse}
}

@inproceedings{10.1145/3365438.3410963,
author = {Alwidian, Sanaa and Amyot, Daniel},
title = {"Union is power": analyzing families of goal models using union models},
year = {2020},
isbn = {9781450370196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365438.3410963},
doi = {10.1145/3365438.3410963},
abstract = {A goal model family is a set of related goal models that conform to the same metamodel, with commonalities and variabilities between models. Such families stem from the evolution of initial models into several versions over time and/or the variation of models over the space dimension (e.g., products). In contexts where there are several versions/variations of a goal model, analyzing a set of related models with typical similarities, one model at a time, often involves redundant computations and may require repeated user assistance (e.g., for interactive analysis) and laborious activities. This paper proposes the use of union models as first-class artifacts to analyze families of goal models, in order to improve performance of language-specific analysis procedures. The paper empirically evaluates the performance gain resulting from adapting (or lifting) an existing analysis technique specific to the Goal-oriented Requirement Language (GRL) to a family of GRL models, all at once using a union model, compared to analyzing individual models. Our experiments show, based on the use of the IBM CPLEX optimizer, the usefulness and performance gains of using union models to perform a computationally expensive analysis, namely quantitative backward propagation, on families of GRL models.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {252–262},
numpages = {11},
keywords = {analysis, backward propagation, goal modeling, model family},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/2851613.2851758,
author = {Bombonatti, Denise and Gralha, Catarina and Moreira, Ana and Ara\'{u}jo, Jo\~{a}o and Goul\~{a}o, Miguel},
title = {Usability of requirements techniques: a systematic literature review},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851758},
doi = {10.1145/2851613.2851758},
abstract = {The usability of requirements engineering (RE) techniques has been recognised as a key factor for their successful adoption by industry. RE techniques must be accessible to stakeholders with different backgrounds, so they can be empowered to effectively and efficiently contribute to building successful systems. When selecting an appropriate requirements engineering technique for a given context, one should consider the usability supported by each of the candidate techniques. The first step towards achieving this goal is to gather the best evidence available on the usability of RE approaches by performing a systematic literature review, to answer one research question: How is the usability of requirements engineering techniques and tools addressed? We systematically review articles published in the Requirements Engineering Journal, one of the main sources for mature work in RE, to motivate a research roadmap to make RE approaches more accessible to stakeholders with different backgrounds.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1270–1275},
numpages = {6},
keywords = {requirements engineering approaches, systematic literature review, usability},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2371401.2371404,
author = {Th\"{u}m, Thomas and Schaefer, Ina and Apel, Sven and Hentschel, Martin},
title = {Family-based deductive verification of software product lines},
year = {2012},
isbn = {9781450311298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2371401.2371404},
doi = {10.1145/2371401.2371404},
abstract = {A software product line is a set of similar software products that share a common code base. While software product lines can be implemented efficiently using feature-oriented programming, verifying each product individually does not scale, especially if human effort is required (e.g., as in interactive theorem proving). We present a family-based approach of deductive verification to prove the correctness of a software product line efficiently. We illustrate and evaluate our approach for software product lines written in a feature-oriented dialect of Java and specified using the Java Modeling Language. We show that the theorem prover KeY can be used off-the-shelf for this task, without any modifications. Compared to the individual verification of each product, our approach reduces the verification time needed for our case study by more than 85%.},
booktitle = {Proceedings of the 11th International Conference on Generative Programming and Component Engineering},
pages = {11–20},
numpages = {10},
keywords = {deductive verification, product-line analysis, program families, software product lines, theorem proving},
location = {Dresden, Germany},
series = {GPCE '12}
}

@article{10.1007/s11219-018-9424-8,
author = {Alkharabsheh, Khalid and Crespo, Yania and Manso, Esperanza and Taboada, Jos\'{e} A.},
title = {Software Design Smell Detection: a systematic mapping study},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9424-8},
doi = {10.1007/s11219-018-9424-8},
abstract = {Design Smells are indicators of situations that negatively affect software quality attributes such as understandability, testability, extensibility, reusability, and maintainability in general. Improving maintainability is one of the cornerstones of making software evolution easier. Hence, Design Smell Detection is important in helping developers when making decisions that can improve software evolution processes. After a long period of research, it is important to organize the knowledge produced so far and to identify current challenges and future trends. In this paper, we analyze 18&nbsp;years of research into Design Smell Detection. There is a wide variety of terms that have been used in the literature to describe concepts which are similar to what we have defined as “Design Smells,” such as design defect, design flaw, anomaly, pitfall, antipattern, and disharmony. The aim of this paper is to analyze all these terms and include them in the study. We have used the standard systematic literature review method based on a comprehensive set of 395 articles published in different proceedings, journals, and book chapters. We present the results in different dimensions of Design Smell Detection, such as the type or scope of smell, detection approaches, tools, applied techniques, validation evidence, type of artifact in which the smell is detected, resources used in evaluation, supported languages, and relation between detected smells and software quality attributes according to a quality model. The main contributions of this paper are, on the one hand, the application of domain modeling techniques to obtain a conceptual model that allows the organization of the knowledge on Design Smell Detection and a collaborative web application built on that knowledge and, on the other, finding how tendencies have moved across different kinds of smell detection, as well as different approaches and techniques. Key findings for future trends include the fact that all automatic detection tools described in the literature identify Design Smells as a binary decision (having the smell or not), which is an opportunity to evolve to fuzzy and prioritized decisions. We also find that there is a lack of human experts and benchmark validation processes, as well as demonstrating that Design Smell Detection positively influences quality attributes.},
journal = {Software Quality Journal},
month = sep,
pages = {1069–1148},
numpages = {80},
keywords = {DesignSmell, Antipatterns, Detection tools, Quality models, Systematic mapping study}
}

@article{10.1016/j.csi.2019.04.005,
author = {Meridji, Kenza and Al-Sarayreh, Khalid T. and Abran, Alain and Trudel, Sylvie},
title = {System security requirements: A framework for early identification, specification and measurement of related software requirements},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2019.04.005},
doi = {10.1016/j.csi.2019.04.005},
journal = {Comput. Stand. Interfaces},
month = oct,
numpages = {20},
keywords = {COSMIC-SOA, COSMIC – ISO 19761, Graphs (SIG), Soft-goal interdependency, Security measurement, International standards, Security requirements, Non-functional requirements (NFR)}
}

@article{10.1016/j.jss.2005.02.028,
author = {Feng, Qian and Lutz, Robyn R.},
title = {Bi-directional safety analysis of product lines},
year = {2005},
issue_date = {November 2005},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {78},
number = {2},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2005.02.028},
doi = {10.1016/j.jss.2005.02.028},
abstract = {As product-line engineering becomes more widespread, more safety-critical software product lines are being built. This paper describes a structured method for performing safety analysis on a software product line, building on standard product-line assets: product-line requirements, architecture, and scenarios. The safety-analysis method is bi-directional in that it combines a forward analysis (from failure modes to effects) with a backward analysis (from hazards to contributing causes). Safety-analysis results are converted to XML files to allow automated consistency checking between the forward and backward analysis results and to support reuse of the safety-analysis results throughout the product line. The paper demonstrates and evaluates the method on a safety-critical product-line subsystem, the Door Control System. Results show that the bi-directional safety-analysis method found both missing and incorrect software safety requirements. Some of the new safety requirements affected all the systems in the product line while others affected only some of the systems in the product line. The results demonstrate that the proposed method can handle the challenges to safety analysis posed by variations within a product line.},
journal = {J. Syst. Softw.},
month = nov,
pages = {111–127},
numpages = {17},
keywords = {Product lines, Reuse, Software architecture, Software safety, XML}
}

@inproceedings{10.1145/3131151.3131162,
author = {Guedes, Gabriela and Silva, Carla and Soares, Monique},
title = {Comparing Configuration Approaches for Dynamic Software Product Lines},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131162},
doi = {10.1145/3131151.3131162},
abstract = {Dynamic Software Product Lines (DSPLs) are Software Product Lines (SPLs) in which the configuration may occur at runtime. DSPL approaches provide means for modeling variability as well as configuring the product according to its runtime context and/or non-functional requirements (NFRs) satisfaction. In this paper, we present a Requirements Engineering (RE) approach for DSPL, ConG4DaS (Contextual Goal models For Dynamic Software product lines), which provides: (i) models for capturing variability with goals, NFRs, contexts and the relationship between them; and (ii) a configuration process that takes contexts, NFRs and their priority and interactions into account. We have used simulation based assessment to compare ConG4DaS with another approach, REFAS (Requirements Engineering For self-Adaptive Software systems), with respect to the satisfaction level of the highest priority softgoal. For the comparison, we modeled two DSPL examples and simulated different scenarios where reconfiguration is necessary. Next, we compared the configurations selected by the approaches with respect to overall NFRs' satisfaction. The results showed that ConG4DaS, which uses utility function in the configuration process, selects configurations that better satisfy NFRs compared to REFAS, which uses constraint programming.},
booktitle = {Proceedings of the XXXI Brazilian Symposium on Software Engineering},
pages = {134–143},
numpages = {10},
keywords = {Dynamic Software Product Lines, Dynamic Variability, Goal Models, Self-Adaptive Systems},
location = {Fortaleza, CE, Brazil},
series = {SBES '17}
}

@article{10.1016/j.infsof.2012.03.008,
author = {O'Leary, P\'{a}Draig and De Almeida, Eduardo Santana and Richardson, Ita},
title = {The Pro-PD Process Model for Product Derivation within software product lines},
year = {2012},
issue_date = {September, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.03.008},
doi = {10.1016/j.infsof.2012.03.008},
abstract = {Background: The derivation of products from a software product line is a time consuming and expensive activity. Despite recognition that an effective process could alleviate many of the difficulties associated with product derivation, existing approaches have different scope, emphasise different aspects of the derivation process and are frequently too specialised to serve as a general solution. Objective: To define a systematic process that will provide a structured approach to the derivation of products from a software product line, based on a set of tasks, roles and artefacts. Method: Through a series of research stages using sources in industry and academia, this research has developed a Process Model for Product Derivation (Pro-PD). We document the evidence for the construction of Pro-PD and the design decisions taken. We evaluate Pro-PD through comparison with prominent existing approaches and standards. Results: This research presents a Process Model for Product Derivation (Pro-PD). Pro-PD describes the tasks, roles and work artefacts used to derive products from a software product line. Conclusion: In response to a need for methodological support, we developed Pro-PD (Process Model for Product Derivation). Pro-PD was iteratively developed and evaluated through four research stages. Our research is a first step toward an evidence-based methodology for product derivation and a starting point for the definition of a product derivation approach.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {1014–1028},
numpages = {15},
keywords = {Process, Product derivation, Software product lines}
}

@inproceedings{10.1145/2701319.2701335,
author = {Gamez, Nadia and El Haddad, Joyce and Fuentes, Lidia},
title = {Managing the Variability in the Transactional Services Selection},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701335},
doi = {10.1145/2701319.2701335},
abstract = {Web service composition is the capability to recursively construct a value added service by means of picking up existing services. An important step in the composition process is the selection step, which includes choosing services located in repositories. The selection approaches of Web services need to consider their specifics which raises important challenges as the management of the inherent service variability in functionality and implementation and ensuring correct execution termination between others. To realize reliable service compositions, transactional properties of services must be considered during the selection step. We argue that the transactional properties should be considered at the operation level of each service to be composed. However, modelling transactional services composition at the operation level drastically increment the complexity of service selection. In order to overcome this difficulty, in this paper we report on our research in progress on transactional service selection, which follows a Software Product Line approach considering the set of services that provide the same functionality as part of a service family. We model the variable operations of the service families using Feature Models. In this way, the selection process consists of selecting each service from a service family such that the aggregated transactional property satisfies the user preference.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {88–95},
numpages = {8},
keywords = {Discovery and Selection, Feature Modeling, Transactional Services},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@article{10.1016/j.eswa.2020.113748,
author = {Mougouei, Davoud and Powers, David M.W.},
title = {Dependency-aware software requirements selection using fuzzy graphs and integer programming},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113748},
doi = {10.1016/j.eswa.2020.113748},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {23},
keywords = {Software, Dependencies, Value, Integer programming, Fuzzy}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {Performance-influence models, machine learning, sampling},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.4018/ijswis.2014010103,
author = {Ermilov, Timofey and Khalili, Ali and Auer, S\"{o}ren},
title = {Ubiquitous Semantic Applications: A Systematic Literature Review},
year = {2014},
issue_date = {January 2014},
publisher = {IGI Global},
address = {USA},
volume = {10},
number = {1},
issn = {1552-6283},
url = {https://doi.org/10.4018/ijswis.2014010103},
doi = {10.4018/ijswis.2014010103},
abstract = {Recently practical approaches for development of ubiquitous semantic applications have made quite some progress. In particular in the area of the ubiquitous access to the semantic data the authors recently observed a large number of approaches, systems and applications being described in the literature. With this survey the authors aim to provide an overview on the rapidly emerging field of Ubiquitous Semantic Applications (UbiSA). The authors conducted a systematic literature review comprising a thorough analysis of 48 primary studies out of 172 initially retrieved papers. The authors obtained a comprehensive set of quality attributes for UbiSA together with corresponding application features suggested for their realization. The quality attributes include aspects such as mobility, usability, heterogeneity, collaboration, customizability and evolvability. The primary studies were surveyed in the light of these quality attributes and the authors performed a thorough analysis of five ubiquitous semantic applications, six frameworks for UbiSA, three UbiSA specific ontologies, five ubiquitous semantic systems and nine general approaches. The proposed quality attributes facilitate the evaluation of existing approaches and the development of novel, more effective and intuitive UbiSA.},
journal = {Int. J. Semant. Web Inf. Syst.},
month = jan,
pages = {66–99},
numpages = {34},
keywords = {Semantic Web, Survey, Ubiquitous Applications, Ubiquitous Device, Web Applications}
}

@article{10.1016/j.procs.2018.07.295,
author = {Azouzi, Sameh and Brahmi, Zaki and Ghannouchi, Sonia Ayachi},
title = {Customization of multi-tenant learning process as a service with Business Process Feature Model},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {126},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.07.295},
doi = {10.1016/j.procs.2018.07.295},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {606–615},
numpages = {10},
keywords = {E-learning, BPM, multi-tenant, SPL, cloud computing, variability, customization}
}

@inproceedings{10.1145/2480362.2480596,
author = {Ara\'{u}jo, Jo\~{a}o and Goul\~{a}o, Miguel and Moreira, Ana and Sim\~{a}o, In\^{e}s and Amaral, Vasco and Baniassad, Elisa},
title = {Advanced modularity for building SPL feature models: a model-driven approach},
year = {2013},
isbn = {9781450316569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480362.2480596},
doi = {10.1145/2480362.2480596},
abstract = {Feature Models are commonly used to specify commonalities and variabilities in Software Product Lines (SPL). Our goal is to enhance feature modeling with traceability and improved support for crosscutting concerns. While traceability will show the features' requirement-origins, providing means to reason about their existence, crosscutting concerns will be handled through advanced modularity mechanisms (e.g. aspects), making the impact of changes to SPL models less difficult to understand and analyze. The result is Theme/SPL, a novel SPL requirements technique based on a concern-driven approach (Theme/Doc). Theme/SPL includes the proposal of a domain-specific language for specifying Theme/Doc models and uses model-driven development to generate automatically feature models from them. We show the applicability of the technique through a case study using a within-group design to evaluate the final results and tools developed.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
pages = {1246–1253},
numpages = {8},
keywords = {advanced modularity, model-driven development, software product lines},
location = {Coimbra, Portugal},
series = {SAC '13}
}

@article{10.1007/s00766-020-00337-x,
author = {Dede, Georgia and Mitropoulou, Persefoni and Nikolaidou, Mara and Kamalakis, Thomas and Michalakelis, Christos},
title = {Safety requirements for symbiotic human–robot collaboration systems in smart factories: a pairwise comparison approach to explore requirements dependencies},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-020-00337-x},
doi = {10.1007/s00766-020-00337-x},
abstract = {Industry 4.0 is expected to deliver significant productivity gain taking advantage of Internet of things (IoT). Smart solutions, enhanced by IoT, are constantly driving revolutionary approaches in multiple domains. Smart factories are one domain where intelligent integrated robotic systems will revolutionize manufacturing, resulting in a complex ecosystem, where humans, robots and machinery are combined. In this setting, human safety requirements are of paramount importance. This paper focuses on symbiotic human–robot collaboration systems (HRC), where human safety requirements are essential. Hence, it aims to explore and prioritize human safety requirement dependencies, as well as their dependencies with other critical requirements of smart factory operation, as effectiveness and performance. Toward this end, the proposed approach is based on SysML to represent the requirements dependencies and pairwise comparisons, a fundamental decision-making method, to quantify the importance of these dependencies. This model-driven approach is used as the primary medium for conveying traceability among human safety requirements as well as traceability from safety requirements to effectiveness and performance requirements in the system model. The analysis is based on the operational requirements identified in the European project HORSE, which aims to develop a methodological/technical framework for easy adaptation of robotic solutions from small-/medium-sized enterprises. Validation of the results is also performed to further elaborate on human safety requirement dependency exploration. The outcomes of this paper may be beneficial for symbiotic HRC systems in the early design stage. As the system is being developed with an emphasis on human safety, all these requirements that have been assessed with highly prioritized dependencies should be taken into account, whereas those with negligible ones have to be ignored since they do not significantly affect the rest of the process. Since operational requirements may be conflicted and incompatible, this approach may be very useful for other systems as well during the system design phase to find the appropriate solution satisfying the majority of the requirements, giving a priority to the ones with highly ranked dependencies and hence facilitating the implementation phase and afterward the production line. The outcomes may be used as a step in developing a model-driven approach which should be able to support the manufacturing process, facilitating the integration of systems and software modeling, which is increasingly important for robotic systems in smart factories incorporating HRC.},
journal = {Requir. Eng.},
month = mar,
pages = {115–141},
numpages = {27},
keywords = {Pairwise comparison, Decision making, SysML, Dependencies, Requirement analysis, Safety, Symbiotic human–robot collaboration systems}
}

@article{10.1016/j.infsof.2019.03.004,
author = {Duran, Mustafa Berk and Mussbacher, Gunter},
title = {Reusability in goal modeling: A systematic literature review},
year = {2019},
issue_date = {Jun 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {110},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.004},
doi = {10.1016/j.infsof.2019.03.004},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {156–173},
numpages = {18},
keywords = {Goal model, Reuse, Context, Requirements reuse, Model-driven requirements engineering, Systematic literature review}
}

@inproceedings{10.1145/3377811.3380927,
author = {Alrajeh, Dalal and Cailliau, Antoine and van Lamsweerde, Axel},
title = {Adapting requirements models to varying environments},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380927},
doi = {10.1145/3377811.3380927},
abstract = {The engineering of high-quality software requirements generally relies on properties and assumptions about the environment in which the software-to-be has to operate. Such properties and assumptions, referred to as environment conditions in this paper, are highly subject to change over time or from one software variant to another. As a consequence, the requirements engineered for a specific set of environment conditions may no longer be adequate, complete and consistent for another set.The paper addresses this problem through a tool-supported requirements adaptation technique. A goal-oriented requirements modelling framework is considered to make requirements' refinements and dependencies on environment conditions explicit. When environment conditions change, an adapted goal model is computed that is correct with respect to the new environment conditions. The space of possible adaptations is not fixed a priori; the required changes are expected to meet one or more environment-independent goal(s) to be satisfied in any version of the system. The adapted goal model is generated using a new counterexample-guided learning procedure that ensures the correctness of the updated goal model, and prefers more local adaptations and more similar goal models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {50–61},
numpages = {12},
keywords = {context-dependent requirements, formal verification, logic-based learning, requirements adaptation, requirements evolution},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1007/s00766-014-0211-1,
author = {Djouab, Rachida and Abran, Alain and Seffah, Ahmed},
title = {An ASPIRE-based method for quality requirements identification from business goals},
year = {2016},
issue_date = {March     2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0211-1},
doi = {10.1007/s00766-014-0211-1},
abstract = {Quality requirements are the main drivers for modeling and evaluating software quality at an early stage, and ASPIRE is an engineering method designed to elicit and document the quality requirements of embedded systems. This paper proposes an extension to ASPIRE to identify quality requirements from the business goals of the organization and ensure their traceability. This extension includes a set of added components created from the main concepts of the SOQUAREM methodology, including the BMM (business motivation model), derivation rules, the quality attribute utility tree, the quality attribute scenario template, the quality attribute documentation template, and ISO 9126. The applicability of the extended method is illustrated with a wireless plant control system as an example.},
journal = {Requir. Eng.},
month = mar,
pages = {87–106},
numpages = {20},
keywords = {BMM (business motivation model), Business goals (BGs), Non-functional requirements (NFRs), QR elicitation method, Quality attributes (QAs), Quality requirements (QRs), Software quality engineering}
}

@inproceedings{10.1007/978-3-319-27343-3_1,
author = {Braubach, Lars and Pokahr, Alexander and Kalinowski, Julian and Jander, Kai},
title = {Tailoring Agent Platforms with Software Product Lines},
year = {2015},
isbn = {9783319273426},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-27343-3_1},
doi = {10.1007/978-3-319-27343-3_1},
abstract = {Agent platforms have been conceived traditionally as middleware, helping to deal with various application challenges like agent programming models, remote messaging, and coordination protocols. A\"{\i} \'{z}middleware is typically a bundle of functionalities necessary to execute multi-agent applications. In contrast to this traditional view, nowadays different use cases also for selected agent concepts have emerged requiring also different kinds of functionalities. Examples include a platform for conducting multi-agent simulations, intelligent agent behavior models for controlling non-player characters NPCs in games and a lightweight version suited for mobile devices. A one-size-fits-all software bundle often does not sufficiently match these requirements, because customers and developers want solutions specifically tailored to their needs, i.e. a small but focused solution is frequently preferred over bloated software with extraneous functionality. Software product lines are an approach suitable for creating a series of similar products from a common code base. In this paper we will show how software product line modeling and technology can help creating tailor-made products from multi-agent platforms. Concretely, the Jadex platform will be analyzed and a feature model as well as an implementation path will be presented.},
booktitle = {Revised Selected Papers of the 13th German Conference on Multiagent System Technologies - Volume 9433},
pages = {3–21},
numpages = {19},
location = {Cottbus, Germany},
series = {MATES 2015}
}

@article{10.1007/s10664-020-09856-1,
author = {Ros, Rasmus and Hammar, Mikael},
title = {Data-driven software design with Constraint Oriented Multi-variate Bandit Optimization (COMBO)},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09856-1},
doi = {10.1007/s10664-020-09856-1},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3841–3872},
numpages = {32},
keywords = {Continuous experimentation, A/B testing, Machine learning, Multi-armed bandits, Combinatorial optimization}
}

@inproceedings{10.1145/2304696.2304705,
author = {Alebrahim, Azadeh and Heisel, Maritta},
title = {Supporting quality-driven design decisions by modeling variability},
year = {2012},
isbn = {9781450313469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304696.2304705},
doi = {10.1145/2304696.2304705},
abstract = {Design decisions should take quality characteristics into account. To support such decisions, we capture various solution artifacts with different levels of satisfying quality requirements as variabilities in the solution space and provide them with rationales for selecting suitable variants. We present a UML-based approach to modeling variability in the problem and the solution space by adopting the notion of feature modeling. It provides a mapping of requirements variability to design solution variability to be used as a part of a general process for generating design alternatives. Our approach supports the software engineer in the process of decision-making for selecting suitable solution variants, reflecting quality concerns, and reasoning about it.},
booktitle = {Proceedings of the 8th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {43–48},
numpages = {6},
keywords = {feature modeling, quality requirements, variability modeling, decision-making, design alternatives},
location = {Bertinoro, Italy},
series = {QoSA '12}
}

@article{10.1145/3011286.3011291,
author = {Galster, Matthias and Zdun, Uwe and Weyns, Danny and Rabiser, Rick and Zhang, Bo and Goedicke, Michael and Perrouin, Gilles},
title = {Variability and Complexity in Software Design: Towards a Research Agenda},
year = {2017},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/3011286.3011291},
doi = {10.1145/3011286.3011291},
abstract = {Many of today's software systems accommodate different usage and deployment scenarios. Intentional and unintentional variability in functionality or quality attributes (e.g., performance) of software significantly increases the complexity of the problem and design space of those systems. The complexity caused by variability becomes increasingly difficult to handle due to the increasing size of software systems, new and emerging application domains, dynamic operating conditions under which software systems have to operate, fast moving and highly competitive markets, and more powerful and versatile hardware. This paper reports results of the first International Workshop on Variability and Complexity in Software Design that brought together researchers and engineers interested in the topic of complexity and variability. It also outlines directions the field might move in the future},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {27–30},
numpages = {4},
keywords = {Variability, complexity, software design}
}

@article{10.1145/3457139,
author = {Wu, Fang-jing and Chen, Ying-Jun and Sou, Sok-Ian},
title = {CoCo: Quantifying Correlations between Mobility Traces using Sensor Data from Smartphones},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3457139},
doi = {10.1145/3457139},
abstract = {As mobility is an important key to many applications, this work proposes a location-less model to represent mobility that is used to quantify correlations between mobility traces collected by built-in sensors on smartphones. We analyze the mobility correlations from two aspects: co-direction relationship and co-movement relationship. The former is to quantify the similarity of macroscopic moving directions between mobility traces, whereas the latter is to quantify the similarity of their microscopic vibrations. To verify the merits of the two proposed metrics, an exemplary use case, termed co-mobility detection, is considered to determine if two mobile devices share the same journey on the same mobile entity (e.g., carried by the same person). Comprehensive experiments with diverse combinations of mobility traces are conducted in three different environments with different density of Wi-Fi networks. The experimental results indicate that the proposed metrics can effectively evaluate both the coarse-grained similarity of moving directions and the fine-grained similarity of movement variations along mobility traces. The accuracy of the co-mobility detection algorithm can achieve 90% on average for mobility traces with a duration of 70 s.},
journal = {ACM Trans. Internet Things},
month = jul,
articleno = {20},
numpages = {22},
keywords = {Cyber-physical systems, mobile sensing, mobility analytics, smart cities, urban computing}
}

@article{10.1016/j.scico.2008.04.003,
author = {Kulk, G. P. and Verhoef, C.},
title = {Quantifying requirements volatility effects},
year = {2008},
issue_date = {August, 2008},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {72},
number = {3},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2008.04.003},
doi = {10.1016/j.scico.2008.04.003},
abstract = {In an organization operating in the bancassurance sector we identified a low-risk IT subportfolio of 84 IT projects comprising together 16,500 function points, each project varying in size and duration, for which we were able to quantify its requirements volatility. This representative portfolio stems from a much larger portfolio of IT projects. We calculated the volatility from the function point countings that were available to us. These figures were aggregated into a requirements volatility benchmark. We found that maximum requirements volatility rates depend on size and duration, which refutes currently known industrial averages. For instance, a monthly growth rate of 5% is considered a critical failure factor, but in our low-risk portfolio we found more than 21% of successful projects with a volatility larger than 5%. We proposed a mathematical model taking size and duration into account that provides a maximum healthy volatility rate that is more in line with the reality of low-risk IT portfolios. Based on the model, we proposed a tolerance factor expressing the maximal volatility tolerance for a project or portfolio. For a low-risk portfolio its empirically found tolerance is apparently acceptable, and values exceeding this tolerance are used to trigger IT decision makers. We derived two volatility ratios from this model, the @p-ratio and the @r-ratio. These ratios express how close the volatility of a project has approached the danger zone when requirements volatility reaches a critical failure rate. The volatility data of a governmental IT portfolio were juxtaposed to our bancassurance benchmark, immediately exposing a problematic project, which was corroborated by its actual failure. When function points are less common, e.g. in the embedded industry, we used daily source code size measures and illustrated how to govern the volatility of a software product line of a hardware manufacturer. With the three real-world portfolios we illustrated that our results serve the purpose of an early warning system for projects that are bound to fail due to excessive volatility. Moreover, we developed essential requirements volatility metrics that belong on an IT governance dashboard and presented such a volatility dashboard.},
journal = {Sci. Comput. Program.},
month = aug,
pages = {136–175},
numpages = {40},
keywords = {π-ratio, ρ-ratio, Compound monthly growth rate, IT dashboard, IT portfolio management, Quantitative IT portfolio management, Requirements churn, Requirements creep, Requirements metric, Requirements scrap, Requirements volatility, Requirements volatility dashboard, Scope creep, Volatility benchmark, Volatility tolerance factor}
}

@inproceedings{10.1007/11554844_20,
author = {Etxeberria, Leire and Sagardui, Goiuria},
title = {Product-line architecture: new issues for evaluation},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_20},
doi = {10.1007/11554844_20},
abstract = {In the product-line context, where a lack or mismatch in a quality attribute is potentially replicated among all products, product-line evaluation could detect problems before concrete products are developed. The life span of a software product-line architecture is much longer than the one of an ordinary software product and it serves as a basis for a set of related systems. Therefore, the product-line architecture should be adaptable to evolution as well as support a number of different products. All these characteristics set new requirements to the product-line architecture evaluation. This paper highlights the new issues that can arise when evaluating a product-line architecture versus evaluating a single-system architecture, including classifications of relevant attributes in product-line architecture evaluation, new evaluation moments and techniques. These issues are used as components of a framework to survey product-line architecture evaluation methods and metrics.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {174–185},
numpages = {12},
location = {Rennes, France},
series = {SPLC'05}
}

@article{10.1016/j.infsof.2007.10.010,
author = {Snook, Colin and Poppleton, Michael and Johnson, Ian},
title = {Rigorous engineering of product-line requirements: A case study in failure management},
year = {2008},
issue_date = {January, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {50},
number = {1–2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.10.010},
doi = {10.1016/j.infsof.2007.10.010},
abstract = {We consider the failure detection and management function for engine control systems as an application domain where product line engineering is indicated. The need to develop a generic requirement set - for subsequent system instantiation - is complicated by the addition of the high levels of verification demanded by this safety-critical domain, subject to avionics industry standards. We present our case study experience in this area as a candidate method for the engineering, validation and verification of generic requirements using domain engineering and Formal Methods techniques and tools. For a defined class of systems, the case study produces a generic requirement set in UML and an example system instance. Domain analysis and engineering produce a validated model which is integrated with the formal specification/verification method B by the use of our UML-B profile. The formal verification both of the generic requirement set, and of a simple system instance, is demonstrated using our U2B, ProB and prototype Requirements Manager tools. This work is a demonstrator for a tool-supported method which will be an output of EU project RODIN (This work is conducted in the setting of the EU funded Research Project: IST 511599 RODIN (Rigorous Open Development Environment for Complex Systems) http://rodin.cs.ncl.ac.uk/). The use of existing and prototype formal verification and support tools is discussed. The method, developed in application to this novel combination of product line, failure management and safety-critical engineering, is evaluated and considered to be applicable to a wide range of domains.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {112–129},
numpages = {18},
keywords = {Formal specification, Generic requirements, Product line, Refinement, Tools, UML-B, Verification}
}

@article{10.1145/3377329,
author = {Guggenheim, Jacob W. and Parietti, Federico and Flash, Tamar and Asada, H. Harry},
title = {Laying the Groundwork for Intra-Robotic-Natural Limb Coordination: Is Fully Manual Control Viable?},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
url = {https://doi.org/10.1145/3377329},
doi = {10.1145/3377329},
abstract = {Supernumerary Robotic Limbs (SRLs) have been successfully applied in bracing and as an assistive technology for people with disabilities. These tasks only require perception internal to the SRL-human system. However, SRLs show promise in applications requiring external perception such as opening a door when one’s hands are full. One path toward developing SRLs that accomplish these tasks is to use human-in-the-loop control, thus leveraging the human’s superior perception system to help the SRLs. However, the effects on the user of controlling additional limbs are unclear. This article presents an experimental study where humans, wearing two single degree of freedom SRLs, were instructed to minimize the position error between the subject’s natural and robotic limbs and the corresponding targets, one for each limb. First, subjects performed worse with their natural limbs when asked to perform the task with two natural and two robotic limbs as opposed to with just their natural limbs, suggesting that shared control could help. Second, subjects moved their natural limbs together followed by moving their SRLs together. This informs both the choice of control scheme for the SRLs and the division of labor within a task. Third, subjects showed significant concurrent use of the natural and robotic limbs.},
journal = {J. Hum.-Robot Interact.},
month = may,
articleno = {18},
numpages = {12},
keywords = {Supernumerary robotic limbs}
}

@inproceedings{10.1145/1083063.1083075,
author = {Lapouchnian, Alexei and Liaskos, Sotirios and Mylopoulos, John and Yu, Yijun},
title = {Towards requirements-driven autonomic systems design},
year = {2005},
isbn = {1595930396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083063.1083075},
doi = {10.1145/1083063.1083075},
abstract = {Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying and analyzing alternative ways of how the main objectives of the system can be achieved and designing a system that supports all of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and sketches a possible architecture for autonomic systems that can be built using the this approach.},
booktitle = {Proceedings of the 2005 Workshop on Design and Evolution of Autonomic Application Software},
pages = {1–7},
numpages = {7},
keywords = {autonomic computing software customization, goal-oriented requirements engineering, self-management, software variability},
location = {St. Louis, Missouri},
series = {DEAS '05}
}

@article{10.1016/j.jss.2019.110428,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Chen, Tao and Kazman, Rick},
title = {Run-time evaluation of architectures: A case study of diversification in IoT},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110428},
doi = {10.1016/j.jss.2019.110428},
journal = {J. Syst. Softw.},
month = jan,
numpages = {28},
keywords = {Run-time architecture evaluation, Runtime architecture evaluation, Software architectures for dynamic environments, Internet of things, IoT, Design diversity}
}

@inproceedings{10.1145/581339.581416,
author = {Bratthall, Lars G. and van der Geest, Robert and Hofmann, Holger and Jellum, Edgar and Korendo, Zbigniew and Martinez, Robert and Orkisz, Michal and Zeidler, Christian and Andersson, Johan S},
title = {Integrating hundred's of products through one architecture: the industrial IT architecture},
year = {2002},
isbn = {158113472X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/581339.581416},
doi = {10.1145/581339.581416},
abstract = {During the last few years, software product line engineering has gained significant interest as a way for creating software products faster and cheaper. But what architecture is needed to integrate huge amounts of products, from different product lines? This paper describes such an architecture and its support processes and tools. Through cases, it is illustrated how the architecture is used to integrate new --- and old --- products in such diverse integration projects as vessel motion control, airport baggage handling systems, pulp&amp;paper and oil&amp;gas, in a very large organization. However, in a large organization it is a challenge to make everyone follow an architecture. Steps taken to ensure global architectural consistency are presented. It is concluded that a single architecture can be used to unify development in a huge organization, where the distributed development practices otherwise may prohibit integration of various products.},
booktitle = {Proceedings of the 24th International Conference on Software Engineering},
pages = {604–614},
numpages = {11},
location = {Orlando, Florida},
series = {ICSE '02}
}

@article{10.1016/j.jss.2018.06.075,
author = {Zhang, Man and Yue, Tao and Ali, Shaukat and Selic, Bran and Okariz, Oscar and Norgre, Roland and Intxausti, Karmele},
title = {Specifying uncertainty in use case models},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.06.075},
doi = {10.1016/j.jss.2018.06.075},
journal = {J. Syst. Softw.},
month = oct,
pages = {573–603},
numpages = {31},
keywords = {Use case modeling, Belief, Uncertainty}
}

@inproceedings{10.5555/2820656.2820664,
author = {de Jesus Souza, Magno Lu\~{a} and Santos, Alcemir Rodrigues and de Almeida, Eduardo Santana},
title = {Towards the selection of modeling techniques for dynamic software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {Emerging domains such as smart homes and more recently smart cities represent a big challenge to software engineering. In such context, the need of runtime self-adaptations to cope with both user needs and environmental changes brings Dynamic Software Product Lines (DSPL) as a suitable solution. However, DSPL implementation itself is challenging, which demands a proper modeling. In this sense, the literature still lacks of means of choosing the modeling technique that best fits a given domain. This paper tackles such problem by defining a criteria for rank such techniques, which is used for ranking a set DSPL modeling techniques found in the literature.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {19–22},
numpages = {4},
keywords = {dynamic software product lines, dynamic variability, modeling techniques},
location = {Florence, Italy},
series = {PLEASE '15}
}

@article{10.1007/s11761-014-0161-y,
author = {Huergo, Rosane S. and Pires, Paulo F. and Delicato, Flavia C. and Costa, Bruno and Cavalcante, Everton and Batista, Thais},
title = {A systematic survey of service identification methods},
year = {2014},
issue_date = {September 2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {3},
issn = {1863-2386},
url = {https://doi.org/10.1007/s11761-014-0161-y},
doi = {10.1007/s11761-014-0161-y},
abstract = {One of the major challenges for the adoption of the service-oriented architecture (SOA) is the service identification phase that aims to determine which services are appropriate to be implemented. In the last decade, several service identification methods (SIMs) were proposed. However, the service identification phase still remains a challenge to organizations due to the lack of systematic methods and comprehensive approaches that support the examination of the businesses from multiple perspectives and consider service quality attributes. This work aims to provide an overview of existing SIMs by detailing which service's perspectives, stated as relevant by the industry, are addressed by the SIMs and also by synthesizing the identification techniques used by them. We have performed a systematic survey over publications about SIMs from 2002 to June 2013, and 105 studies were selected. A detailed investigation on the analyzed SIMs revealed that the identification techniques applied by them have a correlation on how they address many of the service's perspectives. In addition, they are supporting the SOA adoption by handling many perspectives of the OASIS' reference architecture for SOA. However, most of them do not explicitly address service quality attributes and few studies support the evaluation of both. Therefore, future research should follow the direction toward hybrid methods with mechanisms to elicit business and service's quality attributes.},
journal = {Serv. Oriented Comput. Appl.},
month = sep,
pages = {199–219},
numpages = {21},
keywords = {SIM, SOA, Service identification method, Service-oriented architecture, Systematic survey}
}

@inproceedings{10.1007/978-3-030-49913-6_42,
author = {Lin, Wei and Lin, Hsuan and Huang, Zih Yu and Lee, Yun Hsuan},
title = {Concerning the Perspective of Sound Insulation on Approaches of Interior Design},
year = {2020},
isbn = {978-3-030-49912-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-49913-6_42},
doi = {10.1007/978-3-030-49913-6_42},
abstract = {In the study of interior design, experience in space perception and related senses is very important. In addition to the visual aesthetic performance, the sound effects of musical instruments, voices, and relative sounds are as important as visual perception. In the process of sound insulation of interior element, the physical performance of sound is presented. The design approaches for the characteristics of space shape, room volume, material, and detailed decoration are based on the above-mentioned design concerns to predict the room sound performance requirements. Effects of sound insulation measurement is mainly based on impulse response, which represents the physical reaction between a certain point in space and the sound source. In the performance design process, different acoustic model evaluation techniques can be used to obtain the impulse response as the basis for sound field performance evaluation. This study explains the correspondence between interior design and sound insulation performance for topics such as sound transmission loss theory, computer simulation, and on-site measurement techniques.},
booktitle = {Cross-Cultural Design. Applications in Health, Learning, Communication, and Creativity: 12th International Conference, CCD 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings, Part II},
pages = {516–525},
numpages = {10},
keywords = {Interior design, Sound insulation, Computer simulation, On-site measurements},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1145/1188966.1188976,
author = {Lapouchnian, Alexei and Yu, Yijun and Liaskos, Sotirios and Mylopoulos, John},
title = {Requirements-driven design of autonomic application software},
year = {2006},
publisher = {IBM Corp.},
address = {USA},
url = {https://doi.org/10.1145/1188966.1188976},
doi = {10.1145/1188966.1188976},
abstract = {Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying the objectives of the system, analyzing alternative ways of how these objectives can be met, and designing a system that supports all or some of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and demonstrates this on an example.},
booktitle = {Proceedings of the 2006 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {7–es},
location = {Toronto, Ontario, Canada},
series = {CASCON '06}
}

@article{10.1016/j.infsof.2019.01.004,
author = {Souza, Eric and Moreira, Ana and Goul\~{a}o, Miguel},
title = {Deriving architectural models from requirements specifications: A systematic mapping study},
year = {2019},
issue_date = {May 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {109},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.01.004},
doi = {10.1016/j.infsof.2019.01.004},
journal = {Inf. Softw. Technol.},
month = may,
pages = {26–39},
numpages = {14},
keywords = {Software architecture, Mapping study, Literature review}
}

@inproceedings{10.1145/2851613.2851757,
author = {Vilela, J\'{e}ssyka and Gon\c{c}alves, Enyo and Holanda, Ana and Figueiredo, Bruno and Castro, Jaelson},
title = {Retrospective, relevance, and trends of SAC requirements engineering track},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851757},
doi = {10.1145/2851613.2851757},
abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: This study aims to investigate how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 79 papers over the 8 previous SAC RE-Track editions, which were analyzed and discussed.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1264–1269},
numpages = {6},
keywords = {SAC, relevance, requirements engineering, retrospective, scoping study, symposium on applied computing, systematic mapping study, trends},
location = {Pisa, Italy},
series = {SAC '16}
}

@article{10.1504/IJAOSE.2008.016800,
author = {Verstraete, Paul and Germain, Bart Saint and Valckenaers, Paul and Brussel, Hendrik Van and Belle, Jan Van and Hadeli},
title = {Engineering manufacturing control systems using PROSA and delegate MAS},
year = {2008},
issue_date = {January 2008},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {2},
number = {1},
issn = {1746-1375},
url = {https://doi.org/10.1504/IJAOSE.2008.016800},
doi = {10.1504/IJAOSE.2008.016800},
abstract = {This paper presents a systematic description of a reusable software architecture for multiagent systems in the domain of manufacturing control. The architectural description consolidates the authors' expertise in this area. Until now, the research has taken a manufacturing control perspective of multiagent systems. The research team has focused on providing benefits to the manufacturing control domain by designing a novel type of control system. This paper takes a software architectural perspective of multiagent manufacturing control. The systematic description specifies a software product line architecture for manufacturing control. The paper describes the assets of the software product line architecture and how these assets can be combined.},
journal = {Int. J. Agent-Oriented Softw. Eng.},
month = jan,
pages = {62–89},
numpages = {28},
keywords = {MASs, agent-based systems, manufacturing control, multi-agent systems, software architecture, software reuse}
}

@article{10.4018/JECO.2017040104,
author = {El Faquih, Loubna and Fredj, Mounia},
title = {Ontology-Based Framework for Quality in Configurable Process Models},
year = {2017},
issue_date = {April 2017},
publisher = {IGI Global},
address = {USA},
volume = {15},
number = {2},
issn = {1539-2937},
url = {https://doi.org/10.4018/JECO.2017040104},
doi = {10.4018/JECO.2017040104},
abstract = {In recent years, business process modeling has increasingly drawn the attention of enterprises. As a result of the wide use of business processes, redundancy problems have arisen and researchers introduced the variability management, in order to enhance the business process reuse. The most approach used in this context is the Configurable Process Model solution, which consists in representing the variable and the fixed parts together in a unique model. Due to the increasing number of variants, the configurable models become complex and incomprehensible, and their quality is therefore impacted. Most of research work is limited to the syntactic quality of process variants. The approach presented in this paper aims at providing a novel method towards syntactic verification and semantic validation of configurable process models based on ontology languages. We define validation rules for assessing the quality of configurable process models. An example in the e-healthcare domain illustrates the main steps of our approach.},
journal = {J. Electron. Commer. Organ.},
month = apr,
pages = {48–60},
numpages = {13},
keywords = {Configurable Process Models Quality, Ontology, Semantic Validation, Syntactic Verification, Variant Rich BPMN}
}

@article{10.1007/s10590-011-9088-7,
author = {Gim\'{e}nez, Jes\'{u}s and M\`{a}rquez, Llu\'{\i}s},
title = {Linguistic measures for automatic machine translation evaluation},
year = {2010},
issue_date = {December  2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {3–4},
issn = {0922-6567},
url = {https://doi.org/10.1007/s10590-011-9088-7},
doi = {10.1007/s10590-011-9088-7},
abstract = {Assessing the quality of candidate translations involves diverse linguistic facets. However, most automatic evaluation methods in use today rely on limited quality assumptions, such as lexical similarity. This introduces a bias in the development cycle which in some cases has been reported to carry very negative consequences. In order to tackle this methodological problem, we explore a novel path towards heterogeneous automatic Machine Translation evaluation. We have compiled a rich set of specialized similarity measures operating at different linguistic dimensions and analyzed their individual and collective behaviour over a wide range of evaluation scenarios. Results show that measures based on syntactic and semantic information are able to provide more reliable system rankings than lexical measures, especially when the systems under evaluation are based on different paradigms. At the sentence level, while some linguistic measures perform better than most lexical measures, some others perform substantially worse, mainly due to parsing problems. Their scores are, however, suitable for combination, yielding a substantially improved evaluation quality.},
journal = {Machine Translation},
month = dec,
pages = {209–240},
numpages = {32},
keywords = {Automatic evaluation methods, Combined measures, Linguistic analysis, Machine translation, Semantic similarity, Syntactic similarity}
}

@inproceedings{10.1145/3229345.3229408,
author = {Vasconcelos, Ana Klyssia Martins and Oliveira, Alberto Dumont Alves and van der Weerd, Inge and Eler, Marcelo Medeiros},
title = {Towards a Software Product Management Framework for the Brazilian federal universities},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229408},
doi = {10.1145/3229345.3229408},
abstract = {Studies have shown that most Brazilian federal universities do not fully comply with e-government standards and fail to deliver software that meets the requirements and expectations coming from different sources at the same time: stakeholders, institutions, and the federal government. One of the main issue in this scenario is the great effort spent on the development of tailor made software instead of standard products meant for a wider range of customers, such as product lines, for instance, specially because many institutions face problems related to increasing demand and technical staff shortage. Software Product Management (SPM) is a discipline proposed to govern software products to ensure that business goals are achieved and resources are responsibly utilized. Since federal institutions can benefit from such type of management strategy, we present an investigation aiming to understand the main differences between the actors and business processes proposed in SPM reference models and those present in public universities when it comes to deliver software products and services. Accordingly, we propose a high level SPM model for federal universities which was adapted from a well known SPM reference model. Results and discussion provided in this paper can shed light into how to tailor a detailed SPM model for this specific domain.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {60},
numpages = {8},
keywords = {Digital Government, E-government, Software Product Management},
location = {Caxias do Sul, Brazil},
series = {SBSI '18}
}

@inproceedings{10.1145/2405136.2405145,
author = {Almeida, Andr\'{e} and Cavalcante, Everton and Batista, Thais and Lopes, Frederico and Delicato, Flavia C. and Pires, Paulo F. and Alves, Gustavo and Cacho, N\'{e}lio},
title = {Towards an SPL-based monitoring middleware strategy for cloud computing applications},
year = {2012},
isbn = {9781450316088},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2405136.2405145},
doi = {10.1145/2405136.2405145},
abstract = {Cloud-based applications are composed of services offered by distinct third-party cloud providers. The selection of the proper cloud services that fit the application needs is based on cloud-related information, i.e. properties of the services such as price, availability, response time, among others. Typically, applications rely on a middleware that abstracts away the burden of direct dealing with underlying mechanisms for service selection and communication with the cloud providers. In this context, in a previous work we already discussed the benefits of using the software product lines (SPL) paradigm for representing alternative cloud services and their properties, which is suitable for the process of choosing the proper services to compose the application. As most cloud-related information are dynamic and may change any time during the application execution, the continuous monitoring of such information is essential to ensure that the deployed application is composed of cloud services that adhere to the application requirements. In this paper we present an SPL-based monitoring middleware strategy to continuously monitoring the dynamic properties of cloud services used by an application.},
booktitle = {Proceedings of the 10th International Workshop on Middleware for Grids, Clouds and e-Science},
articleno = {9},
numpages = {6},
keywords = {cloud computing, monitoring, monitoring strategy, selection, software product lines},
location = {Montreal, Quebec, Canada},
series = {MGC '12}
}

@article{10.4018/IJHCR.2017040103,
author = {Hunaiti, Ziad},
title = {Digital Learning Technologies: Subjective and Objective Effectiveness Evaluation in Higher Education Settings},
year = {2017},
issue_date = {April 2017},
publisher = {IGI Global},
address = {USA},
volume = {8},
number = {2},
issn = {1947-9158},
url = {https://doi.org/10.4018/IJHCR.2017040103},
doi = {10.4018/IJHCR.2017040103},
abstract = {Embedding Information and Communication Technology ICT within education is continually evolving research topic, because of the fast changes in information technology, software applications, and speed of internet. Therefore, many researchers around the globe are working in new projects of Digital Learning Technologies DLTs. The success of new DLT is linked with the methods used during the evaluation process, which sometimes misunderstood. Hence, this article emphasises the importance of following the correct approach when selecting evaluation methods for DLT research and propose an approach, which can be followed by researchers, developers, students, educators, teaching &amp; teaching specialists and other stakeholders to evaluate the effectiveness of new DLT.},
journal = {Int. J. Handheld Comput. Res.},
month = apr,
pages = {41–50},
numpages = {10},
keywords = {Digital Learning Technologies DLTs, Higher Education, ICTs, Subjective and Objective Effectiveness, Technology Evaluation}
}

@inproceedings{10.1145/2025113.2025177,
author = {Mori, Marco},
title = {A software lifecycle process for context-aware adaptive systems},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025177},
doi = {10.1145/2025113.2025177},
abstract = {It is increasingly important for computing systems to evolve their behavior at run-time because of resources uncertainty, system failures and emerging user needs. Our approach supports software engineers to analyze and develop context-aware adaptive applications. The software lifecycle process we propose supports static and dynamic decision making mechanisms, run-time consistent evolution and it is amenable to be automated.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {412–415},
numpages = {4},
keywords = {consistent evolution, context-aware adaptive systems, feature engineering, software lifecycle process},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inproceedings{10.1145/3450614.3462237,
author = {Welling, Jana and Fischer, Rosa-Linde and Schinkel-Bielefeld, Nadja},
title = {Is it Possible to Identify Careless Responses with Post-hoc Analysis in EMA Studies?},
year = {2021},
isbn = {9781450383677},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450614.3462237},
doi = {10.1145/3450614.3462237},
abstract = {Data quality is a major issue when conducting studies in behavioral sciences. One of the possible threats to data quality in user modeling, in particular in questionnaire studies, is providing careless responses (CR). When responding carelessly, subjects do not pay sufficient attention to the questions and therefore compromise the interpretability of the responses. The aim of the current study was to gain a better understanding of the occurrence and identification of CR in Ecological Momentary Assessment (EMA) studies, where several questionnaires usually are administered daily to the participants over the course of some days, weeks or even months. For this purpose, explorative post-hoc analysis was conducted using the data of an existing EMA study in audiological research. Completion time, variance, skipped items, acquiescence bias and number of textboxes were analyzed as potential indicators for CR both inter- and intraindividually. Furthermore, consistency was examined using linear mixed models and scanning individual questionnaires. Results showed minimal systematic inconsistencies, indicating the absence of large-scale CR. However, this type of analysis might not be appropriate for identifying CR when only occurring occasionally. Moreover, the reliability of indicators of CR might be limited in EMA studies, as the indicators also vary over the course of the study and between different situations. Possibilities for future studies are discussed.},
booktitle = {Adjunct Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {150–156},
numpages = {7},
location = {Utrecht, Netherlands},
series = {UMAP '21}
}

@article{10.1016/j.micpro.2019.05.013,
author = {Pomante, Luigi and Muttillo, Vittoriano and K\v{r}ena, Bohuslav and Vojnar, Tom\'{a}\v{s} and Veljkovi\'{c}, Filip and Magnin, Pac\^{o}me and Matschnig, Martin and Fischer, Bernhard and Martinez, Jabier and Gruber, Thomas},
title = {The AQUAS ECSEL Project Aggregated Quality Assurance for Systems: Co-Engineering Inside and Across the Product Life Cycle},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {69},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2019.05.013},
doi = {10.1016/j.micpro.2019.05.013},
journal = {Microprocess. Microsyst.},
month = sep,
pages = {54–67},
numpages = {14},
keywords = {Cyber-physical systems, Safety, Security, Performance, Co-engineering, Product life-cycle}
}

@book{10.5555/2692450,
author = {Mistrik, Ivan and Bahsoon, Rami and Eeles, Peter and Roshandel, Roshanak and Stal, Michael},
title = {Relating System Quality and Software Architecture},
year = {2014},
isbn = {0124170099},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {System Quality and Software Architecture collects state-of-the-art knowledge on how to intertwine software quality requirements with software architecture and how quality attributes are exhibited by the architecture of the system. Contributions from leading researchers and industry evangelists detail the techniques required to achieve quality management in software architecting, and the best way to apply these techniques effectively in various application domains (especially in cloud, mobile and ultra-large-scale/internet-scale architecture) Taken together, these approaches show how to assess the value of total quality management in a software development process, with an emphasis on architecture. The book explains how to improve system quality with focus on attributes such as usability, maintainability, flexibility, reliability, reusability, agility, interoperability, performance, and more. It discusses the importance of clear requirements, describes patterns and tradeoffs that can influence quality, and metrics for quality assessment and overall system analysis. The last section of the book leverages practical experience and evidence to look ahead at the challenges faced by organizations in capturing and realizing quality requirements, and explores the basis of future work in this area.Explains how design decisions and method selection influence overall system quality, and lessons learned from theories and frameworks on architectural qualityShows how to align enterprise, system, and software architecture for total qualityIncludes case studies, experiments, empirical validation, and systematic comparisons with other approaches already in practice.}
}

@article{10.1016/j.intcom.2007.12.002,
author = {Lin, Tao and Imamiya, Atsumi and Mao, Xiaoyang},
title = {Using multiple data sources to get closer insights into user cost and task performance},
year = {2008},
issue_date = {May, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {20},
number = {3},
issn = {0953-5438},
url = {https://doi.org/10.1016/j.intcom.2007.12.002},
doi = {10.1016/j.intcom.2007.12.002},
abstract = {This pilot study explores the use of combining multiple data sources (subjective, physical, physiological, and eye tracking) in understanding user cost and behavior. Specifically, we show the efficacy of such objective measurements as heart rate variability (HRV), and pupillary response in evaluating user cost in game environments, along with subjective techniques, and investigate eye and hand behavior at various levels of user cost. In addition, a method for evaluating task performance at the micro-level is developed by combining eye and hand data. Four findings indicate the great potential value of combining multiple data sources to evaluate interaction: first, spectral analysis of HRV in the low frequency band shows significant sensitivity to changes in user cost, modulated by game difficulty-the result is consistent with subjective ratings, but pupillary response fails to accord with user cost in this game environment; second, eye saccades seem to be more sensitive to user cost changes than eye fixation number and duration, or scanpath length; third, a composite index based on eye and hand movements is developed, and it shows more sensitivity to user cost changes than a single eye or hand measurement; finally, timeline analysis of the ratio of eye fixations to mouse clicks demonstrates task performance changes and learning effects over time. We conclude that combining multiple data sources has a valuable role in human-computer interaction (HCI) evaluation and design.},
journal = {Interact. Comput.},
month = may,
pages = {364–374},
numpages = {11},
keywords = {User cost, Usability evaluation, Task performance, Multiple data sources}
}

@article{10.4018/jismd.2012040102,
author = {Mazo, Ra\'{u}l and Salinesi, Camille and Diaz, Daniel and Djebbi, Olfa and Lora-Michiels, Alberto},
title = {Constraints: The Heart of Domain and Application Engineering in the Product Lines Engineering Strategy},
year = {2012},
issue_date = {April 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {2},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012040102},
doi = {10.4018/jismd.2012040102},
abstract = {Drawing from an analogy between features based Product Line PL models and Constraint Programming CP, this paper explores the use of CP in the Domain Engineering and Application Engineering activities that are put in motion in a Product Line Engineering strategy. Specifying a PL as a constraint program instead of a feature model carries out two important qualities of CP: expressiveness and direct automation. On the one hand, variables in CP can take values over boolean, integer, real or even complex domains and not only boolean values as in most PL languages such as the Feature-Oriented Domain Analysis FODA. Specifying boolean, arithmetic, symbolic and reified constraint, provides a power of expression that spans beyond that provided by the boolean dependencies in FODA models. On the other hand, PL models expressed as constraint programs can directly be executed and analyzed by off-the-shelf solvers. This paper explores the issues of a how to specify a PL model using CP, including in the presence of multi-model representation, b how to verify PL specifications, c how to specify configuration requirements, and d how to support the product configuration activity. Tests performed on a benchmark of 50 PL models show that the approach is efficient and scales up easily to very large and complex PL specifications.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = apr,
pages = {33–68},
numpages = {36},
keywords = {Computer Science, Constraint-Based Product Lines, Information Systems, Product Line Analysis, Product Line Configuration, Product Line Integration, Product Line Reasoning, Product Line Specification, Product Line Verification}
}

@inproceedings{10.1109/WI-IAT.2014.170,
author = {Louati, Amine and Haddad, Joyce El and Pinson, Suzanne},
title = {A Multilevel Agent-Based Approach for Trustworthy Service Selection in Social Networks},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.170},
doi = {10.1109/WI-IAT.2014.170},
abstract = {The growing number of services available within social applications (viz. Social networks) raises a new and challenging search issue: selecting desired services from social networks. Traditional discovery and selection approaches, which are registry-based (e.g., UDDI, ebXML), have manifested their limitations as they often fall behind users' expectations. This is because registries fail to (i) take into consideration non functional properties such as QoS and trust and (ii) capitalize on the information resulting from the previous experiences between agents. To address these shortcomings, we use software agents as they support interactions and offer well-developed capabilities to formally express and interpret semantic information useful to evaluate trust. Trust in a service is a multi-aspect concept that includes a social-based aspect such as judging whether the provider is worthwhile pursuing before using his services (viz. Trust in sociability), expert-based aspect such as estimating whether the service behaves well and as expected (viz. Trust in expertise) and, recommender-based aspect such as assessing whether an agent is reliable and we can rely on its recommendations (viz. Trust in recommendation).},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 03},
pages = {214–221},
numpages = {8},
keywords = {Multi-Agent Systems, Referral Systems, Service Selection, Social Networks, Trust},
series = {WI-IAT '14}
}

@inproceedings{10.1145/2695664.2695875,
author = {Almeida, Andr\'{e} and Bencomo, Nelly and Batista, Thais and Cavalcante, Everton and Dantas, Francisco},
title = {Dynamic decision-making based on NFR for managing software variability and configuration selection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695875},
doi = {10.1145/2695664.2695875},
abstract = {Due to dynamic variability, identifying the specific conditions under which non-functional requirements (NFRs) are satisfied may be only possible at runtime. Therefore, it is necessary to consider the dynamic treatment of relevant information during the requirements specifications. The associated data can be gathered by monitoring the execution of the application and its underlying environment to support reasoning about how the current application configuration is fulfilling the established requirements. This paper presents a dynamic decision-making infrastructure to support both NFRs representation and monitoring, and to reason about the degree of satisfaction of NFRs during runtime. The infrastructure is composed of: (i) an extended feature model aligned with a domain-specific language for representing NFRs to be monitored at runtime; (ii) a monitoring infrastructure to continuously assess NFRs at runtime; and (iii) a flexible decision-making process to select the best available configuration based on the satisfaction degree of the NRFs. The evaluation of the approach has shown that it is able to choose application configurations that well fit user NFRs based on runtime information. The evaluation also revealed that the proposed infrastructure provided consistent indicators regarding the best application configurations that fit user NFRs. Finally, a benefit of our approach is that it allows us to quantify the level of satisfaction with respect to NFRs specification.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1376–1382},
numpages = {7},
keywords = {SPLs, monitoring, non-functional requirements, variability},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1016/j.datak.2021.101893,
author = {G\'{o}mez, Paola and Roncancio, Claudia and Casallas, Rubby},
title = {Analysis and evaluation of document-oriented structures},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {134},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2021.101893},
doi = {10.1016/j.datak.2021.101893},
journal = {Data Knowl. Eng.},
month = jul,
numpages = {21},
keywords = {NoSQL, Structural metrics, Document-oriented systems, MongoDB}
}

@article{10.1145/3284971.3284973,
author = {Poltronieri, Ildevana and Zorzo, Avelino Francisco and Bernardino, Maicon and de Borba Campos, Marcia},
title = {Usability evaluation framework for domain-specific language: a focus group study},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3284971.3284973},
doi = {10.1145/3284971.3284973},
abstract = {Software engineers are increasingly taking advantage of new methods to improve software quality. The use of languages developed for specific domains, which in the literature are known as Domain-Specific Languages (DSLs), has grown in the past years. Although several experimental studies that subjectively evaluate usability of these languages can be found in the literature, few of them have taken advantage of applying Human-Computer Interaction (HCI) techniques in those evaluations. Therefore, the main goals of this paper are to present a usability evaluation framework for DSLs, called Usa-DSL, and to show the evaluation of the framework through a Focus Group method. The evaluation was performed by seven specialists that discussed the framework usability and suggested some modifications of our initial proposal. The specialists recommendations were incorporated in the final framework presented in this paper.},
journal = {SIGAPP Appl. Comput. Rev.},
month = oct,
pages = {5–18},
numpages = {14},
keywords = {domain-specific languages, focus group, usability evaluation, usability testing}
}

@article{10.1145/3130901,
author = {Beattie, David and Baillie, Lynne and Halvey, Martin},
title = {Exploring How Drivers Perceive Spatial Earcons in Automated Vehicles},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130901},
doi = {10.1145/3130901},
abstract = {Automated vehicles seek to relieve the human driver from primary driving tasks, but this substantially diminishes the connection between driver and vehicle compared to manual operation. At present, automated vehicles lack any form of continual, appropriate feedback to re-establish this connection and offer a feeling of control. We suggest that auditory feedback can be used to support the driver in this context. A preliminary field study that explored how drivers respond to existing auditory feedback in manual vehicles was first undertaken. We then designed a set of abstract, synthesised sounds presented spatially around the driver, known as Spatial Earcons, that represented different primary driving sounds e.g. acceleration. To evaluate their effectiveness, we undertook a driving simulator study in an outdoor setting using a real vehicle. Spatial Earcons performed as well as Existing Vehicle Sounds during automated and manual driving scenarios. Subjective responses suggested Spatial Earcons produced an engaging driving experience. This paper argues that entirely new synthesised primary driving sounds, such as Spatial Earcons, can be designed for automated vehicles to replace Existing Vehicle Sounds. This creates new possibilities for presenting primary driving information in automated vehicles using auditory feedback, in order to re-establish a connection between driver and vehicle.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {36},
numpages = {24},
keywords = {Auditory Displays, Auditory Feedback, Automated Vehicles, Driving Simulator, Earcons, Existing Vehicle Sounds, Handover of Control}
}

@article{10.1145/3011286.3011302,
author = {Echeverr\'{\i}a, Jorge},
title = {Research on Augmenting the MDD Process with Variability Modeling},
year = {2017},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/3011286.3011302},
doi = {10.1145/3011286.3011302},
abstract = {Software Product Lines (SPLs) have proven to be successful at reducing the costs and time to market of product development through the planned reuse of software components into products within the same scope. SPL adoption has been typically regarded to follow a proactive approach, although recent surveys show that most of the SPLs are planned following reactive approaches. It seems necessary to refocus SPL engineering research, methodologies and tools for existing systems into SPL. We believe that systems following a Model Driven Development (MDD) approach can highly benefit from these re-engineering efforts, in order to enable them to manage variability. The aim of this research is to analyze how to improve the MDD process with variability modeling in real industrial environments. Nowadays, we have performed three empirical studies related to variability modeling in MDD approaches. These studies are the following: (1) an usability evaluation of a MDD approach with variability modeling, (2) comprehensibility of variability in model fragments for product configuration and (3) an evaluation about bug-fixing in a MDD-SPL tool},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–6},
numpages = {6},
keywords = {Model Driven Development, Usability Evaluation, Variability Modeling}
}

@article{10.1007/s10664-014-9357-1,
author = {B\'{e}can, Guillaume and Acher, Mathieu and Baudry, Benoit and Nasr, Sana Ben},
title = {Breathing ontological knowledge into feature model synthesis: an empirical study},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9357-1},
doi = {10.1007/s10664-014-9357-1},
abstract = {Feature Models (FMs) are a popular formalism for modeling and reasoning about the configurations of a software product line. As the manual construction of an FM is time-consuming and error-prone, management operations have been developed for reverse engineering, merging, slicing, or refactoring FMs from a set of configurations/dependencies. Yet the synthesis of meaningless ontological relations in the FM --- as defined by its feature hierarchy and feature groups --- may arise and cause severe difficulties when reading, maintaining or exploiting it. Numerous synthesis techniques and tools have been proposed, but only a few consider both configuration and ontological semantics of an FM. There are also few empirical studies investigating ontological aspects when synthesizing FMs. In this article, we define a generic, ontologic-aware synthesis procedure that computes the likely siblings or parent candidates for a given feature. We develop six heuristics for clustering and weighting the logical, syntactical and semantical relationships between feature names. We then perform an empirical evaluation on hundreds of FMs, coming from the SPLOT repository and Wikipedia. We provide evidence that a fully automated synthesis (i.e., without any user intervention) is likely to produce FMs far from the ground truths. As the role of the user is crucial, we empirically analyze the strengths and weaknesses of heuristics for computing ranking lists and different kinds of clusters. We show that a hybrid approach mixing logical and ontological techniques outperforms state-of-the-art solutions. We believe our approach, environment, and empirical results support researchers and practitioners working on reverse engineering and management of FMs.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1794–1841},
numpages = {48},
keywords = {Feature model, Model management, Refactoring, Reverse engineering, Software product lines, Variability}
}

@inproceedings{10.5555/2818754.2818780,
author = {Ben-David, Shoham and Sterin, Baruch and Atlee, Joanne M. and Beidu, Sandy},
title = {Symbolic model checking of product-line requirements using SAT-based methods},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Product line (PL) engineering promotes the development of families of related products, where individual products are differentiated by which optional features they include. Modelling and analyzing requirements models of PLs allows for early detection and correction of requirements errors -- including unintended feature interactions, which are a serious problem in feature-rich systems. A key challenge in analyzing PL requirements is the efficient verification of the product family, given that the number of products is too large to be verified one at a time. Recently, it has been shown how the high-level design of an entire PL, that includes all possible products, can be compactly represented as a single model in the SMV language, and model checked using the NuSMV tool. The implementation in NuSMV uses BDDs, a method that has been outperformed by SAT-based algorithms.In this paper we develop PL model checking using two leading SAT-based symbolic model checking algorithms: IMC and IC3. We describe the algorithms, prove their correctness, and report on our implementation. Evaluating our methods on three PL models from the literature, we demonstrate an improvement of up to 3 orders of magnitude over the existing BDD-based method.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {189–199},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/1404946.1404951,
author = {Anthonysamy, Pauline and Som\'{e}, St\'{e}phane S.},
title = {Aspect-oriented use case modeling for software product lines},
year = {2008},
isbn = {9781605581439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1404946.1404951},
doi = {10.1145/1404946.1404951},
abstract = {Software Product Line Development advocates software reuse by modeling common and variable artefacts separately across members of a family of products. Aspect-Oriented Software Development aims at separation of concerns with "aspects" to increase modularity, reusability, maintainability and ease of evolution. In this paper, we apply an aspect-oriented use case modeling approach to product line system modeling. A use case specification captures stakeholders concerns as interactions between a system and its actors. We adapt our previous work with the introduction of a «variability» relationship for the expression of variabilities. This relationship is used to model variable and common behaviours across a family of products as use cases. A variability composition mechanism enables building of executable behaviour models for each member of a product line family by integrating common elements with the applicable variable elements.},
booktitle = {Proceedings of the 2008 AOSD Workshop on Early Aspects},
articleno = {5},
numpages = {8},
keywords = {Petri nets, UML, concerns, software product lines, use cases},
location = {Brussels, Belgium},
series = {EA '08}
}

@article{10.1145/2532780.2532789,
author = {Graziotin, Daniel and Jedlitschka, Andreas},
title = {Recent developments in product-focused software process improvement: PROFES 2013 conference report},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2532780.2532789},
doi = {10.1145/2532780.2532789},
abstract = {This report summarizes the presentations and discussions that happened at PROFES 2013, the 14th International Conference on Product- Focused Software Process Improvement, which was held June 12-14, 2013 in Paphos, Cyprus. The main theme of PROFES is software process improvement (SPI) motivated by product, process, and service quality needs. PROFES 2013 addressed both quality engineering and management topics, divided into the areas of Decision Support in Software Engineering, Empirical Software Engineering, Managing Software Processes, Safety-Critical Software Engineering, Software Measurement, Software Process Improvement, and Software Maintenance.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {29–34},
numpages = {6},
keywords = {experimentation, human factors, management, measurement, performance, security, standardization}
}

@article{10.1016/j.infsof.2006.11.003,
author = {Niemel\"{a}, Eila and Immonen, Anne},
title = {Capturing quality requirements of product family architecture},
year = {2007},
issue_date = {November, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {11–12},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.11.003},
doi = {10.1016/j.infsof.2006.11.003},
abstract = {Software quality is one of the major issues with software intensive systems. Moreover, quality is a critical success factor in software product families exploiting shared architecture and common components in a set of products. Our contribution is the QRF (Quality Requirements of a software Family) method, which explicitly focuses on how quality requirements have to be defined, represented and transformed to architectural models. The method has been applied to two experiments; one in a laboratory environment and the other in industry. The use of the QRF method is exemplified by the Distribution Service Platform (DiSeP), the laboratory experiment. The lessons learned are also based on our experiences of applying the method in industrial settings.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1107–1120},
numpages = {14},
keywords = {Quality requirement, Software architecture, Software product family, Traceability}
}

@inproceedings{10.1145/1960502.1960508,
author = {Machado, Idarlan and Bonif\'{a}cio, Rodrigo and Alves, Vander and Turnes, Lucin\'{e}ia and Machado, Giselle},
title = {Managing variability in business processes: an aspect-oriented approach},
year = {2011},
isbn = {9781450306454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1960502.1960508},
doi = {10.1145/1960502.1960508},
abstract = {Business processes specify key activities in an organization, some of which can be automated. It is often the case that replication of activities across such processes occur and failure in identifying such replication results in organizational costs. To minimize this risk and optimize organizational resources, in this paper we characterize variability in business process and propose an approach to manage such a variability. The characterization of variability relies on the study of industrial-strength applications in the Human Resources domain. The management of variability is based on a compositional and parametric approach with Aspect-Orientation. It leverages and extends an existing tool to address variability in such domain},
booktitle = {Proceedings of the 2011 International Workshop on Early Aspects},
pages = {25–30},
numpages = {6},
keywords = {software product lines, composition, business processes, aspects},
location = {Porto de Galinhas, Brazil},
series = {EA '11}
}

@article{10.1007/s10009-012-0253-y,
author = {Schaefer, Ina and Rabiser, Rick and Clarke, Dave and Bettini, Lorenzo and Benavides, David and Botterweck, Goetz and Pathak, Animesh and Trujillo, Salvador and Villela, Karina},
title = {Software diversity: state of the art and perspectives},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-012-0253-y},
doi = {10.1007/s10009-012-0253-y},
abstract = {Diversity is prevalent in modern software systems to facilitate adapting the software to customer requirements or the execution environment. Diversity has an impact on all phases of the software development process. Appropriate means and organizational structures are required to deal with the additional complexity introduced by software variability. This introductory article to the special section "Software Diversity--Modeling, Analysis and Evolution" provides an overview of the current state of the art in diverse systems development and discusses challenges and potential solutions. The article covers requirements analysis, design, implementation, verification and validation, maintenance and evolution as well as organizational aspects. It also provides an overview of the articles which are part of this special section and addresses particular issues of diverse systems development.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {477–495},
numpages = {19},
keywords = {Software diversity, Software product lines, Variability}
}

@inproceedings{10.1145/1852786.1852810,
author = {Falessi, Davide and Cantone, Giovanni and Canfora, Gerardo},
title = {A comprehensive characterization of NLP techniques for identifying equivalent requirements},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852810},
doi = {10.1145/1852786.1852810},
abstract = {Though very important in software engineering, linking artifacts of the same type (clone detection) or of different types (traceability recovery) is extremely tedious, error-prone and requires significant effort. Past research focused on supporting analysts with mechanisms based on Natural Language Processing (NLP) to identify candidate links. Because a plethora of NLP techniques exists, and their performances vary among contexts, it is important to characterize them according to the provided level of support. The aim of this paper is to characterize a comprehensive set of NLP techniques according to the provided level of support to human analysts in detecting equivalent requirements. The characterization consists on a case study, featuring real requirements, in the context of an Italian company in the defense and aerospace domain. The major result from the case study is that simple NLP are more precise than complex ones.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {18},
numpages = {10},
keywords = {case study, natural language processing, requirements},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@article{10.1007/s11219-012-9193-8,
author = {Mets\"{a}, Jani and Maoz, Shahar and Katara, Mika and Mikkonen, Tommi},
title = {Using aspects for testing of embedded software: experiences from two industrial case studies},
year = {2014},
issue_date = {June      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9193-8},
doi = {10.1007/s11219-012-9193-8},
abstract = {Aspect-oriented software testing is emerging as an important alternative to conventional procedural and object-oriented testing techniques. This paper reports experiences from two case studies where aspects were used for the testing of embedded software in the context of an industrial application. In the first study, we used code-level aspects for testing non-functional properties. The methodology we used for deriving test aspect code was based on translating high-level requirements into test objectives, which were then implemented using test aspects in AspectC++. In the second study, we used high-level visual scenario-based models for the test specification, test generation, and aspect-based test execution. To specify scenario-based tests, we used a UML2-compliant variant of live sequence charts. To automatically generate test code from the models, a modified version of the S2A Compiler, outputting AspectC++ code, was used. Finally, to examine the results of the tests, we used the Tracer, a prototype tool for model-based trace visualization and exploration. The results of the two case studies show that aspects offer benefits over conventional techniques in the context of testing embedded software; these benefits are discussed in detail. Finally, towards the end of the paper, we also discuss the lessons learned, including the technological and other barriers to the future successful use of aspects in the testing of embedded software in industry.},
journal = {Software Quality Journal},
month = jun,
pages = {185–213},
numpages = {29},
keywords = {Aspect-oriented programming, Case studies, Embedded software, Software testing}
}

@article{10.1016/j.infsof.2012.12.001,
author = {Wnuk, Krzysztof and Gorschek, Tony and Zahda, Showayb},
title = {Obsolete software requirements},
year = {2013},
issue_date = {June, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {6},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.12.001},
doi = {10.1016/j.infsof.2012.12.001},
abstract = {Context: Coping with rapid requirements change is crucial for staying competitive in the software business. Frequently changing customer needs and fierce competition are typical drivers of rapid requirements evolution resulting in requirements obsolescence even before project completion. Objective: Although the obsolete requirements phenomenon and the implications of not addressing them are known, there is a lack of empirical research dedicated to understanding the nature of obsolete software requirements and their role in requirements management. Method: In this paper, we report results from an empirical investigation with 219 respondents aimed at investigating the phenomenon of obsolete software requirements. Results: Our results contain, but are not limited to, defining the phenomenon of obsolete software requirements, investigating how they are handled in industry today and their potential impact. Conclusion: We conclude that obsolete software requirements constitute a significant challenge for companies developing software intensive products, in particular in large projects, and that companies rarely have processes for handling obsolete software requirements. Further, our results call for future research in creating automated methods for obsolete software requirements identification and management, methods that could enable efficient obsolete software requirements management in large projects.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {921–940},
numpages = {20},
keywords = {Change impact analysis, Empirical study, Market driven requirements engineering, Obsolete requirements, Requirements management, Survey}
}

@article{10.1145/2413038.2382768,
author = {Brahmasani, Siva and Selvakumar, Subramanian and Sivasankar, E.},
title = {Prevention of XSS attacks using STCD: Server side tagging and client side differentiation},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2413038.2382768},
doi = {10.1145/2413038.2382768},
abstract = {Variability (the ability of a software system or software artifact to be adapted for use in a specific context) is reflected in and facilitated through the software architecture. The Second International Workshop on Variability in Software Architecture (VARSA) was held in conjunction with the Joint 10th Working IEEE/IFIP Conference on Software Architecture &amp; 6th European Conference on Software Architecture 2012 in Helsinki, Finland. The workshop aimed at exploring current and emerging methods, languages, notations technologies and tools to model, implement, and manage variability in the software architecture. It featured one industrial talk, five research paper presentations, and three working group discussions. Working groups discussed topics that emerged during the workshop. This report summarizes the themes of the workshop and presents the results of the working group discussions.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {46–49},
numpages = {4},
keywords = {VARSA, software architecture, variability}
}

@article{10.1145/2382756.2382768,
author = {Brahmasani, Siva and Selvakumar, Subramanian and Sivasankar, E.},
title = {Prevention of XSS attacks using STCD: Server side tagging and client side differentiation},
year = {2013},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2382756.2382768},
doi = {10.1145/2382756.2382768},
abstract = {Variability (the ability of a software system or software artifact to be adapted for use in a specific context) is reflected in and facilitated through the software architecture. The Second International Workshop on Variability in Software Architecture (VARSA) was held in conjunction with the Joint 10th Working IEEE/IFIP Conference on Software Architecture &amp; 6th European Conference on Software Architecture 2012 in Helsinki, Finland. The workshop aimed at exploring current and emerging methods, languages, notations technologies and tools to model, implement, and manage variability in the software architecture. It featured one industrial talk, five research paper presentations, and three working group discussions. Working groups discussed topics that emerged during the workshop. This report summarizes the themes of the workshop and presents the results of the working group discussions.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–9},
numpages = {9},
keywords = {VARSA, software architecture, variability}
}

@inproceedings{10.1145/2351676.2351693,
author = {Rabiser, Rick and Gr\"{u}nbacher, Paul and Lehofer, Martin},
title = {A qualitative study on user guidance capabilities in product configuration tools},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351693},
doi = {10.1145/2351676.2351693},
abstract = {Software systems are nowadays often configured by sales people, domain experts, or even customers instead of engineers. Configuration tools communicate the systems' variability to these end users and provide guidance for selecting and customizing the available features. However, even if a configuration tool creates technically correct systems, addressing the specific needs of business-oriented users remains challenging. We analyze existing configuration tools to identify key capabilities for guiding end users and discuss these capabilities using the cognitive dimensions of notations framework. We present an implementation of the capabilities in our configuration tool DOPLER CW. We performed a qualitative investigation on the usefulness of the tool's capabilities for user guidance in product configuration by involving nine business-oriented experts of two industry partners from the domain of industrial automation. We present key results and derive general implications for tool developers.},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {110–119},
numpages = {10},
keywords = {Configuration tools, cognitive dimensions of notations, end user guidance, qualitative study},
location = {Essen, Germany},
series = {ASE '12}
}

@article{10.1155/2021/5089236,
author = {Chen, Yu and Tang, Zhong and Ding, Baiyuan},
title = {Research on the Construction of Intelligent Community Emergency Service Platform Based on Convolutional Neural Network},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/5089236},
doi = {10.1155/2021/5089236},
abstract = {Aiming at the shortcomings of the existing community emergency service platform, such as single function, poor scalability, and strong subjectivity, an intelligent community emergency service platform based on convolutional neural network was constructed. Firstly, the requirements analysis of the emergency service platform was carried out, and the functional demand of the emergency service platform was analyzed from the aspects of community environment, safety, infrastructure, health management, emergency response, and so on. Secondly, through logistics network, big data, cloud computing, artificial intelligence, and all kinds of applications, the intelligent community emergency service platform was designed. Finally, a semantic matching emergency question answering system based on convolutional neural network was developed to provide key technical support for the emergency preparation stage of intelligent community. The results show that the intelligent community emergency service platform plays an important role in preventing community emergency events and taking active and effective measures to ensure the health and safety of community residents.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@inproceedings{10.1145/2593882.2593895,
author = {Hatcliff, John and Wassyng, Alan and Kelly, Tim and Comar, Cyrille and Jones, Paul},
title = {Certifiably safe software-dependent systems: challenges and directions},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593895},
doi = {10.1145/2593882.2593895},
abstract = {The amount and impact of software-dependence in critical systems impinging on daily life is increasing rapidly. In many of these systems, inadequate software and systems engineering can lead to economic disaster, injuries or death. Society generally does not recognize the potential of losses from deficiencies of systems due to software until after some mishap occurs. Then there is an outcry, reflecting societal expectations; however, few know what it takes to achieve the expected safety and, in general, loss-prevention.  On the one hand there are unprecedented, exponential increases in size, inter-dependencies, intricacies, numbers and variety in the systems and distribution of development processes across organizations and cultures. On the other hand, industry's capability to verify and validate these systems has not kept up. Mere compliance with existing standards, techniques, and regulations cannot guarantee the safety properties of these systems. The gap between practice and capability is increasing rapidly.  This paper considers the future of software engineering as needed to support development and certification of safety-critical software-dependent systems. We identify a collection of challenges and document their current state, the desired state, gaps and barriers to reaching the desired state, and potential directions in software engineering research and education that could address the gaps and barriers.},
booktitle = {Future of Software Engineering Proceedings},
pages = {182–200},
numpages = {19},
keywords = {Certification, assurance, hazard analysis, requirements, safety, standards, validation, verification},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/3265845.3265856,
author = {Lin, Han Hong and Han, Ping Hsuan and Lu, Kuan Yin and Sun, Chia Hung and Lee, Pei Yi and Jan, Yao Fu and Lee, Amy Ming Sui and Sun, Wei Zen and Hung, Yi Ping},
title = {Stillness Moves: Exploring Body Weight-Transfer Learning in Physical Training for Tai-Chi Exercise},
year = {2018},
isbn = {9781450359818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265845.3265856},
doi = {10.1145/3265845.3265856},
abstract = {Body weight-transfer plays an important role in many exercises. The correlation of the body posture, movement, and weight-transfer will mutually affect the trainee to do well in performances such as Tai-Chi exercise. According to the traditional way of learning Tai-Chi, we proposed Stillness Moves, a physical training system for Tai-Chi, which captures and records users' skeleton movement and weight-transfer information for offering real-time and summary visual feedback. Based on above, we provide a gradual learning program in physical training, which combines body movement and weight-transfer learning. We evaluated our system and compared the performance without and with weight-transfer guidance in the user study. The result demonstrated that weight-transfer guidance is beneficial for trainee learning the Tai-Chi moves. For difficult moves, the trainee should learn the weight-transfer first, then, learning the body movement.},
booktitle = {Proceedings of the 1st International Workshop on Multimedia Content Analysis in Sports},
pages = {21–29},
numpages = {9},
keywords = {movement guidance, tai chi chuan, visualization, weight-transfer learning},
location = {Seoul, Republic of Korea},
series = {MMSports'18}
}

@inproceedings{10.1145/3344948.3344950,
author = {Njima, Mercy},
title = {Architecting for scale: the case for systematic software reuse in managing technical debt in start-ups},
year = {2019},
isbn = {9781450371421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344948.3344950},
doi = {10.1145/3344948.3344950},
abstract = {Most start-ups aspire to become non-start-ups someday. One would argue then that architecting for scale means doing it right the first time. However, start-ups usually start with a single idea in search of market fit. Taking time to design and implement a scalable architecture is time that is not spent responding to customer demands and is usually not a priority. This short-term vision may lead start-ups to accumulate technical debt. This work is geared towards understanding how start-ups find a viable trade-off between customer demands and long term goals. We conduct expert interviews at start-ups and scale-ups with the hypothesis that reusing software for the management of technical debt would allow a start-up to quickly respond to the demanding needs of the market in the long run.},
booktitle = {Proceedings of the 13th European Conference on Software Architecture - Volume 2},
pages = {55–58},
numpages = {4},
keywords = {exploratory study, software reuse, start-ups, technical debt},
location = {Paris, France},
series = {ECSA '19}
}

@inproceedings{10.1145/3239060.3239086,
author = {Lahmer, Marie and Glatz, Christiane and Seibold, Verena C. and Chuang, Lewis L.},
title = {Looming Auditory Collision Warnings for Semi-Automated Driving: An ERP Study},
year = {2018},
isbn = {9781450359467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239060.3239086},
doi = {10.1145/3239060.3239086},
abstract = {Looming sounds can be an ideal warning notification for emergency braking. This agrees with studies that have consistently demonstrated preferential brain processing for looming stimuli. This study investigates and demonstrates that looming sounds can similarly benefit emergency braking in managing a vehicle with adaptive cruise control (ACC). Specifically, looming auditory notifications induced the faster emergency braking times relative to a static auditory notification. Next, we compare the event-related potential (ERP) evoked by a looming notification, relative to its static equivalent. Looming notifications evoke a smaller fronto-central N2 amplitude than their static equivalents. Thus, we infer that looming sounds are consistent with the visual experience of an approaching collision and, hence, induced a corresponding performance benefit. Subjective ratings indicate no significant differences in the perceived workload across the notification conditions. Overall, this work suggests that auditory warnings should have congruent physical properties with the visual events that they warn for.},
booktitle = {Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {310–319},
numpages = {10},
keywords = {EEG, N2, adaptive-cruise control, auditory notifications, braking, event-related potential, looming},
location = {Toronto, ON, Canada},
series = {AutomotiveUI '18}
}

@article{10.1504/IJWET.2015.069359,
author = {Berkane, Mohamed Lamine and Seinturier, Lionel and Boufaida, Mahmoud},
title = {Using variability modelling and design patterns for self-adaptive system engineering: application to smart-home},
year = {2015},
issue_date = {May 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {1},
issn = {1476-1289},
url = {https://doi.org/10.1504/IJWET.2015.069359},
doi = {10.1504/IJWET.2015.069359},
abstract = {Adaptability is an increasingly important requirement for many systems, in particular for those that are deployed in dynamically changing environments. The purpose is to let the systems react and adapt autonomously to changing executing conditions without human intervention. Due to the large number of variability decisions e.g., user needs, environment characteristics and the current lack of reusable adaptation expertise, it becomes increasingly difficult to build a system that satisfies all the requirements and constraints that might arise during its lifetime. In this paper, we propose an approach for developing policies for self-adaptive systems at multiple levels of abstraction. This approach is the first that allows the combination of variability with feature model and reusability with design pattern into a single solution for product derivation that gives strong support to develop self-adaptive systems in a modular way. We demonstrate the feasibility of the proposed approach with a use case based on a smart home scenario.},
journal = {Int. J. Web Eng. Technol.},
month = may,
pages = {65–93},
numpages = {29}
}

@inproceedings{10.1145/3136014.3136027,
author = {Bari\v{s}i\'{c}, Ankica and Blouin, Dominique and Amaral, Vasco and Goul\~{a}o, Miguel},
title = {A requirements engineering approach for usability-driven DSL development},
year = {2017},
isbn = {9781450355254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136014.3136027},
doi = {10.1145/3136014.3136027},
abstract = {There is currently a lack of Requirements Engineering (RE) approaches applied to, or supporting, the development of a Domain-Specific Language (DSL) taking into account the environment in which it is to be used. We present a model-based RE approach to support DSL development with a focus on usability concerns. RDAL is a RE fragment language that can be complemented with other languages to support RE and design. USE-ME is a model driven approach for DSLs usability evaluation which is integrable with a DSL development approach. We combine RDAL and a new DSL, named DSSL, that we created for the specification of DSL-based systems. Integrated with this combination we add USE-ME to support usability evaluation. This combination of existing languages and tools provides a comprehensive RE approach for DSL development. We illustrate the approach with the development of the Gyro DSL for programming robots.},
booktitle = {Proceedings of the 10th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {115–128},
numpages = {14},
keywords = {Domain-Specific language, Requirements engineering, Usability evaluation},
location = {Vancouver, BC, Canada},
series = {SLE 2017}
}

@inproceedings{10.1145/3123514.3123524,
author = {Selfridge, Rod and Moffat, David and Reiss, Joshua D.},
title = {Physically Derived Sound Synthesis Model of a Propeller},
year = {2017},
isbn = {9781450353731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123514.3123524},
doi = {10.1145/3123514.3123524},
abstract = {A real-time sound synthesis model for propeller sounds is presented. Equations obtained from fluid dynamics and aerodynamics research are utilised to produce authentic propeller-powered aircraft sounds. The result is a physical model in which the geometries of the objects involved are used in sound synthesis calculations. The model operates in real-time making it ideal for integration within a game or virtual reality environment. Comparison with real propeller-powered aircraft sounds indicates that some aspects of real recordings are not replicated by our model. Listening tests suggest that our model performs as well as another synthesis method but is not as plausible as a real recording.},
booktitle = {Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences},
articleno = {16},
numpages = {8},
keywords = {Physical Model, Real-Time, Sound Synthesis},
location = {London, United Kingdom},
series = {AM '17}
}

@article{10.1016/j.infsof.2009.11.008,
author = {Ovaska, Eila and Evesti, Antti and Henttonen, Katja and Palviainen, Marko and Aho, Pekka},
title = {Knowledge based quality-driven architecture design and evaluation},
year = {2010},
issue_date = {June, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {6},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.008},
doi = {10.1016/j.infsof.2009.11.008},
abstract = {Modelling and evaluating quality properties of software is of high importance, especially when our every day life depends on the quality of services produced by systems and devices embedded into our surroundings. This paper contributes to the body of research in quality and model driven software engineering. It does so by introducing; (1) a quality aware software architecting approach and (2) a supporting tool chain. The novel approach with supporting tools enables the systematic development of high quality software by merging benefits of knowledge modelling and management, and model driven architecture design enhanced with domain-specific quality attributes. The whole design flow of software engineering is semi-automatic; specifying quality requirements, transforming quality requirements to architecture design, representing quality properties in architectural models, predicting quality fulfilment from architectural models, and finally, measuring quality aspects from implemented source code. The semi-automatic design flow is exemplified by the ongoing development of a secure middleware for peer-to-peer embedded systems.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {577–601},
numpages = {25},
keywords = {Evaluation, Model-driven development, Ontology, Quality attribute, Software architecture, Tool}
}

@inproceedings{10.1145/1967486.1967572,
author = {Souer, Jurriaan and Joor, Dirk-Jan},
title = {An approach to identify commonalities in web application engineering for a web content management system},
year = {2010},
isbn = {9781450304214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1967486.1967572},
doi = {10.1145/1967486.1967572},
abstract = {The process of Web applications engineering can be complex and time consuming. We argue that Web engineering based on a standardized platform with reusable components is a logical next step in the evolution of Web application development. One popular platform to create Web applications is called a Web Content Management Systems (WCMS) which allows organizations to develop Web applications in a time and resource efficient way. This paper presents a method to identify software commonalities in WCMS-based Web applications to improve the software product for future implementations based on feature modeling and e-business models. The resulting method provides insight in relevant e-business models and their corresponding functionalities. Moreover, this paper shows how these commonalities can be identified and how that could influence the software product line. The approach has been applied in a practical case study of a series of Web application engineering projects within the publishing vertical market. We have validated the approach with experts within the case study company and found that the approach is useful in aiding requirements engineers in the Web application engineering process and product managers in the software product management process.},
booktitle = {Proceedings of the 12th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {558–565},
numpages = {8},
keywords = {software product lines, web content management system, web engineering},
location = {Paris, France},
series = {iiWAS '10}
}

@book{10.5555/2911053,
author = {Mistrik, Ivan and Soley, Richard M. and Ali, Nour and Grundy, John and Tekinerdogan, Bedir},
title = {Software Quality Assurance: In Large Scale and Complex Software-intensive Systems},
year = {2015},
isbn = {0128023015},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Quality Assurance in Large Scale and Complex Software-intensive Systems presents novel and high-quality research related approaches that relate the quality of software architecture to system requirements, system architecture and enterprise-architecture, or software testing. Modern software has become complex and adaptable due to the emergence of globalization and new software technologies, devices and networks. These changes challenge both traditional software quality assurance techniques and software engineers to ensure software quality when building today (and tomorrows) adaptive, context-sensitive, and highly diverse applications. This edited volume presents state of the art techniques, methodologies, tools, best practices and guidelines for software quality assurance and offers guidance for future software engineering research and practice. Each contributed chapter considers the practical application of the topic through case studies, experiments, empirical validation, or systematic comparisons with other approaches already in practice. Topics of interest include, but are not limited, to: quality attributes of system/software architectures; aligning enterprise, system, and software architecture from the point of view of total quality; design decisions and their influence on the quality of system/software architecture; methods and processes for evaluating architecture quality; quality assessment of legacy systems and third party applications; lessons learned and empirical validation of theories and frameworks on architectural quality; empirical validation and testing for assessing architecture quality.Focused on quality assurance at all levels of software design and developmentCovers domain-specific software quality assurance issues e.g. for cloud, mobile, security, context-sensitive, mash-up and autonomic systemsExplains likely trade-offs from design decisions in the context of complex software system engineering and quality assuranceIncludes practical case studies of software quality assurance for complex, adaptive and context-critical systems}
}

@inproceedings{10.1145/2911451.2914709,
author = {Roegiest, Adam and Cormack, Gordon V.},
title = {Impact of Review-Set Selection on Human Assessment for Text Classification},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2914709},
doi = {10.1145/2911451.2914709},
abstract = {In a laboratory study, human assessors were significantly more likely to judge the same documents as relevant when they were presented for assessment within the context of documents selected using random or uncertainty sampling, as compared to relevance sampling. The effect is substantial and significant [0.54 vs. 0.42, p&lt;0.0002] across a population of documents including both relevant and non-relevant documents, for several definitions of ground truth. This result is in accord with Smucker and Jethani's SIGIR 2010 finding that documents were more likely to be judged relevant when assessed within low-precision versus high-precision ranked lists. Our study supports the notion that relevance is malleable, and that one should take care in assuming any labeling to be ground truth, whether for training, tuning, or evaluating text classifiers.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {861–864},
numpages = {4},
keywords = {assessor error, ediscovery, electronic discovery, evaluation, recall, supervised learning, user study},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@inproceedings{10.1145/1180995.1181063,
author = {Lin, Tao and Imamiya, Atsumi},
title = {Evaluating usability based on multimodal information: an empirical study},
year = {2006},
isbn = {159593541X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180995.1181063},
doi = {10.1145/1180995.1181063},
abstract = {New technologies are making it possible to provide an enriched view of interaction for researchers using multimodal information. This preliminary study explores the use of multiple information streams in usability evaluation. In the study, easy, medium and difficult versions of a game task were used to vary the levels of mental effort. Multimodal data streams during the three versions were analyzed, including eye tracking, pupil size, hand movement, heart rate variability (HRV) and subjectively reported data. Four findings indicate the potential value of usability evaluations based on multimodal information: First, subjective and physiological measures showed significant sensitivity to task difficulty. Second, different mental workload levels appeared to correlate with eye movement patterns, especially with a combined eye-hand movement measure. Third, HRV showed correlations with saccade speed. Finally, we present a new method using the ratio of eye fixations over mouse clicks to evaluate performance in more detail. These results warrant further investigations and take an initial step toward establishing usability evaluation methods based on multimodal information.},
booktitle = {Proceedings of the 8th International Conference on Multimodal Interfaces},
pages = {364–371},
numpages = {8},
keywords = {eye tracking, multimodal, physiological measures, usability},
location = {Banff, Alberta, Canada},
series = {ICMI '06}
}

@article{10.1016/j.jss.2014.01.021,
author = {Walraven, Stefan and Van Landuyt, Dimitri and Truyen, Eddy and Handekyn, Koen and Joosen, Wouter},
title = {Efficient customization of multi-tenant Software-as-a-Service applications with service lines},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.01.021},
doi = {10.1016/j.jss.2014.01.021},
abstract = {Application-level multi-tenancy is an architectural approach for Software-as-a-Service (SaaS) applications which enables high operational cost efficiency by sharing one application instance among multiple customer organizations (the so-called tenants). However, the focus on increased resource sharing typically results in a one-size-fits-all approach. In principle, the shared application instance satisfies only the requirements common to all tenants, without supporting potentially different and varying requirements of these tenants. As a consequence, multi-tenant SaaS applications are inherently limited in terms of flexibility and variability. This paper presents an integrated service engineering method, called service line engineering, that supports co-existing tenant-specific configurations and that facilitates the development and management of customizable, multi-tenant SaaS applications, without compromising scalability. Specifically, the method spans the design, implementation, configuration, composition, operations and maintenance of a SaaS application that bundles all variations that are based on a common core. We validate this work by illustrating the benefits of our method in the development of a real-world SaaS offering for document processing. We explicitly show that the effort to configure and compose an application variant for each individual tenant is significantly reduced, though at the expense of a higher initial development effort.},
journal = {J. Syst. Softw.},
month = may,
pages = {48–62},
numpages = {15},
keywords = {Multi-tenancy, SaaS, Variability}
}

@inproceedings{10.1145/2000259.2000263,
author = {Koziolek, Heiko},
title = {Sustainability evaluation of software architectures: a systematic review},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000263},
doi = {10.1145/2000259.2000263},
abstract = {Long-living software systems are sustainable if they can be cost-efficiently maintained and evolved over their entire life-cycle. The quality of software architectures determines sustainability to a large extent. Scenario-based software architecture evaluation methods can support sustainability analysis, but they are still reluctantly used in practice. They are also not integrated with architecture-level metrics when evaluating implemented systems, which limits their capabilities. Existing literature reviews for architecture evaluation focus on scenario-based methods, but do not provide a critical reflection of the applicability of such methods for sustainability evaluation. Our goal is to measure the sustainability of a software architecture both during early design using scenarios and during evolution using scenarios and metrics, which is highly relevant in practice. We thus provide a systematic literature review assessing scenario-based methods for sustainability support and categorize more than 40 architecture-level metrics according to several design principles. Our review identifies a need for further empirical research, for the integration of existing methods, and for the more efficient use of formal architectural models.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {3–12},
numpages = {10},
keywords = {architectural metric, evolution scenario, software architecture, survey, sustainability},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1109/MISE.2009.5069896,
author = {Acher, Mathieu and Lahire, Philippe and Moisan, Sabine and Rigault, Jean-Paul},
title = {Tackling high variability in video surveillance systems through a model transformation approach},
year = {2009},
isbn = {9781424437221},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MISE.2009.5069896},
doi = {10.1109/MISE.2009.5069896},
abstract = {This work explores how model-driven engineering techniques can support the configuration of systems in domains presenting multiple variability factors. Video surveillance is a good candidate for which we have an extensive experience. Ultimately, we wish to automatically generate a software component assembly from an application specification, using model to model transformations. The challenge is to cope with variability both at the specification and at the implementation levels. Our approach advocates a clear separation of concerns. More precisely, we propose two feature models, one for task specification and the other for software components. The first model can be transformed into one or several valid component configurations through step-wise specialization. This paper outlines our approach, focusing on the two feature models and their relations. We particularly insist on variability and constraint modeling in order to achieve the mapping from domain variability to software variability through model transformations.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Modeling in Software Engineering},
pages = {44–49},
numpages = {6},
series = {MISE '09}
}

@inproceedings{10.5555/645882.672395,
author = {Kang, Kyo Chul and Donohoe, Patrick and Koh, Eunman and Lee, Jaejoon and Lee, Kwanwoo},
title = {Using a Marketing and Product Plan as a Key Driver for Product Line Asset Development},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The product line engineering paradigm has emerged recently to address the need to minimize the development cost and the time to market in this highly competitive global market. Product line development consists of product line asset development and product development using the assets. Product line requirements are essential inputs to product line asset development. These inputs, although critical, are not sufficient to develop product line assets. A marketing and product plan, which includes plans on what features are to be packaged in products, how these features will be delivered to customers (e.g., feature binding time), and how the products will evolve in the future, also drives product line asset development; thus this paper explores design issues from the marketing perspective and presents key design drivers that are tightly coupled with the marketing strategy. An elevator control software example is used to illustrate how product line asset development is related to marketing and product plans.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {366–382},
numpages = {17},
series = {SPLC 2}
}

@article{10.1145/1824795.1824799,
author = {Mili, Hafedh and Tremblay, Guy and Jaoude, Guitta Bou and Lefebvre, \'{E}ric and Elabed, Lamia and Boussaidi, Ghizlane El},
title = {Business process modeling languages: Sorting through the alphabet soup},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/1824795.1824799},
doi = {10.1145/1824795.1824799},
abstract = {Requirements capture is arguably the most important step in software engineering, and yet the most difficult and the least formalized one [Phalp and Shepperd 2000]. Enterprises build information systems to support their business processes. Software engineering research has typically focused on the development process, starting with user requirements—if that—with business modeling often confused with software system modeling [Isoda 2001]. Researchers and practitioners in management information systems have long recognized that understanding the business processes that an information system must support is key to eliciting the needs of its users (see e.g., Eriksson and Penker 2000]), but lacked the tools to model such business processes or to relate such models to software requirements. Researchers and practitioners in business administration have long been interested in modeling the processes of organizations for the purposes of understanding, analyzing, and improving such processes [Hammer and Champy 1993], but their models were often too coarse to be of use to software engineers. The advent of ecommerce and workflow management systems, among other things, has led to a convergence of interests and tools, within the broad IT community, for modeling and enabling business processes. In this article we present an overview of business process modeling languages. We first propose a categorization of the various languages and then describe representative languages from each family.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {4},
numpages = {56},
keywords = {Process, business processes, process model reuse, process model verification, process modeling languages}
}

@article{10.1016/j.infsof.2007.10.003,
author = {Gorschek, Tony and Davis, Alan M.},
title = {Requirements engineering: In search of the dependent variables},
year = {2008},
issue_date = {January, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {50},
number = {1–2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.10.003},
doi = {10.1016/j.infsof.2007.10.003},
abstract = {When software development teams modify their requirements engineering process as an independent variable, they often examine the implications of these process changes by assessing the quality of the products of the requirements engineering process, e.g., a software requirements specification (SRS). Using the quality of the SRS as the dependent variable is flawed. As an alternative, this paper presents a framework of dependent variables that serves as a full range for requirements engineering quality assessment. In this framework, the quality of the SRS itself is just the first level. Other higher, and more significant levels, include whether the project was successful and whether the resulting product was successful. And still higher levels include whether or not the company was successful and whether there was a positive or negative impact on society as a whole.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {67–75},
numpages = {9},
keywords = {Dependent variables, Organizational perspective, Process improvement, Product focus, Product management, Project focus, Project management, Requirements engineering, Requirements process}
}

@article{10.1007/s10664-015-9410-8,
author = {Unterkalmsteiner, Michael and Gorschek, Tony and Feldt, Robert and Lavesson, Niklas},
title = {Large-scale information retrieval in software engineering - an experience report from industrial application},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9410-8},
doi = {10.1007/s10664-015-9410-8},
abstract = {Software Engineering activities are information intensive. Research proposes Information Retrieval (IR) techniques to support engineers in their daily tasks, such as establishing and maintaining traceability links, fault identification, and software maintenance. We describe an engineering task, test case selection, and illustrate our problem analysis and solution discovery process. The objective of the study is to gain an understanding of to what extent IR techniques (one potential solution) can be applied to test case selection and provide decision support in a large-scale, industrial setting. We analyze, in the context of the studied company, how test case selection is performed and design a series of experiments evaluating the performance of different IR techniques. Each experiment provides lessons learned from implementation, execution, and results, feeding to its successor. The three experiments led to the following observations: 1) there is a lack of research on scalable parameter optimization of IR techniques for software engineering problems; 2) scaling IR techniques to industry data is challenging, in particular for latent semantic analysis; 3) the IR context poses constraints on the empirical evaluation of IR techniques, requiring more research on developing valid statistical approaches. We believe that our experiences in conducting a series of IR experiments with industry grade data are valuable for peer researchers so that they can avoid the pitfalls that we have encountered. Furthermore, we identified challenges that need to be addressed in order to bridge the gap between laboratory IR experiments and real applications of IR in the industry.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {2324–2365},
numpages = {42},
keywords = {Data mining, Experiment, Information retrieval, Test case selection}
}

@inproceedings{10.1145/3196398.3196442,
author = {Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe},
title = {Data-driven search-based software engineering},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196442},
doi = {10.1145/3196398.3196442},
abstract = {This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {341–352},
numpages = {12},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.1007/s11219-013-9202-6,
author = {Nguyen, Tuong Huan and Vo, Bao Quoc and Lumpe, Markus and Grundy, John},
title = {KBRE: a framework for knowledge-based requirements engineering},
year = {2014},
issue_date = {March     2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9202-6},
doi = {10.1007/s11219-013-9202-6},
abstract = {Detecting inconsistencies is a critical part of requirements engineering (RE) and has been a topic of interest for several decades. Domain knowledge and semantics of requirements not only play important roles in elaborating requirements but are also a crucial way to detect conflicts among them. In this paper, we present a novel knowledge-based RE framework (KBRE) in which domain knowledge and semantics of requirements are central to elaboration, structuring, and management of captured requirements. Moreover, we also show how they facilitate the identification of requirements inconsistencies and other-related problems. In our KBRE model, description logic (DL) is used as the fundamental logical system for requirements analysis and reasoning. In addition, the application of DL in the form of Manchester OWL Syntax brings simplicity to the formalization of requirements while preserving sufficient expressive power. A tool has been developed and applied to an industrial use case to validate our approach.},
journal = {Software Quality Journal},
month = mar,
pages = {87–119},
numpages = {33},
keywords = {Description logics, Identification, Inconsistencies, Manchester OWL Syntax, Ontology, Requirements engineering}
}

@inproceedings{10.1007/978-3-319-35122-3_8,
author = {Peng, Zhenlian and Wang, Jian and He, Keqing and Li, Hongtao},
title = {An Approach for Prioritizing Software Features Based on Node Centrality in Probability Network},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_8},
doi = {10.1007/978-3-319-35122-3_8},
abstract = {Due to the increasing complexity of software products as well as the restriction of the development budget and time, requirements prioritization, i.e., selecting more crucial requirements to be designed and developed firstly, has become increasingly important in the software development lifetime. Considering the fact that a feature in a feature model can be viewed as a set of closely related requirements, feature prioritization will contribute to requirements prioritization to a large extent. Therefore, how to measure the priority of features within a feature model becomes an important issue in requirements analysis. In this paper, a software feature prioritization approach is proposed, which utilizes the dependencies between features to build a feature probability network and measures feature prioritization through the nodes centrality in the network. Experiments conducted on real world feature models show that the proposed approach can accurately prioritize features in feature models.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {106–121},
numpages = {16},
keywords = {Centrality, Feature model, Feature prioritization, Feature probability network},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1109/SBCARS.2015.11,
author = {Oliveira, Raphael Pereira de and Almeida, Eduardo Santana de},
title = {Requirements Evolution in Software Product Lines: An Empirical Study},
year = {2015},
isbn = {9781467396301},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SBCARS.2015.11},
doi = {10.1109/SBCARS.2015.11},
abstract = {The evolution of the requirements specification is a key activity for maintaining the goals of any software project and it has long been established and recognized by researchers and practitioners. Within Software Product Lines (SPL), this evolution is even more critical due to the necessity of dealing with common, variable, and product-specific requirements, not only for a single product but for the whole set of products. In this paper, we present the Feature-Driven Requirements Engineering Evolution (FeDRE2) approach, which provides support to evolve the requirements specification of SPL. The approach evolves the feature model and the use case specification from an SPL according to the user's needs. It also provides detailed guidelines on how to evolve chunks of features from a feature model and their related use case specifications. The evaluation of the approach is illustrated in an empirical study for evolving an SPL of mobile applications for emergency notifications. This empirical study was applied within 16 participants, 9 participants from Federal University of Bahia (Brazil) and 7 participants from Polytechnic University of Valencia (Spain). Evaluations concerning the perceived ease of use, perceived usefulness, effectiveness, and efficiency as regards requirements analysts using the approach are also presented. The results have shown that FeDRE2 was perceived as easy to learn and useful by the participants.},
booktitle = {Proceedings of the 2015 IX Brazilian Symposium on Components, Architectures and Reuse Software},
pages = {1–10},
numpages = {10},
keywords = {Empirical Study, Evolution, Requirements Engineering (RE), Software Product Lines (SPL)},
series = {SBCARS '15}
}

@inproceedings{10.1145/3474624.3476010,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Implementing Search-Based Software Engineering Approaches with Nautilus},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476010},
doi = {10.1145/3474624.3476010},
abstract = {Search-Based Software Engineering (SBSE) approaches adopt search-based techniques to solve Software Engineering (SE) optimization problems. Among these techniques, evolutionary algorithms are the most popular and successfully used, such as multi-objective evolutionary algorithms. However, some challenges still need to be addressed. Firstly, SE problems are complex and commonly impacted by many conflicting factors. In this context, the use of many-objective algorithms is necessary. Secondly, the users very often do not recognise the found solutions as feasible because these solutions are usually not generated considering the users’ needs and preferences. Thus, to deal properly with this situation, preference-based algorithms should be applied. Moreover, there are some practical issues regarding the choice of operators, evaluation of algorithms and visualization of solutions. Existing frameworks do not provide support to address these challenges. To overcome these limitations, we present Nautilus, an open-source Java web-platform tool that works with plugins to ease the addition of new problem instances, implementation of search operators and different multi and many-objective optimization algorithms, guided (or not) by human participation. This paper describes Nautilus-NRP, an extension implemented to address the Next Release Problem (NRP). NRP refers to the selection of requirements to be implemented in the next release of a software and is used to illustrate Nautilus’ main functionalities and how it can be extended to solve a SE problem. Link for the video: https://youtu.be/2dbwslTrvhg.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {303–308},
numpages = {6},
keywords = {many-objective optimization, next release problem, preference-based algorithms},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1007/s11219-016-9341-7,
author = {Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire and Zander, Justyna},
title = {Automatic generation of test system instances for configurable cyber-physical systems},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9341-7},
doi = {10.1007/s11219-016-9341-7},
abstract = {Cyber-physical systems (CPSs) are ubiquitous systems that integrate digital technologies with physical processes. These systems are becoming configurable to respond to the different needs that users demand. As a consequence, their variability is increasing, and they can be configured in many system variants. To ensure a systematic test execution of CPSs, a test system must be elaborated encapsulating several sources such as test cases or test oracles. Manually building a test system for each configuration is a non-systematic, time-consuming, and error-prone process. To overcome these problems, we designed a test system for testing CPSs and we analyzed the variability that it needed to test different configurations. Based on this analysis, we propose a methodology supported by a tool named ASTERYSCO that automatically generates simulation-based test system instances to test individual configurations of CPSs. To evaluate the proposed methodology, we selected different configurations of a configurable Unmanned Aerial Vehicle, and measured the time required to generate their test systems. On average, around 119 s were needed by our tool to generate the test system for 38 configurations. In addition, we compared the process of generating test system instances between the method we propose and a manual approach. Based on this comparison, we believe that the proposed tool allows a systematic method of generating test system instances. We believe that our approach permits an important step toward the full automation of testing in the field of configurable CPSs.},
journal = {Software Quality Journal},
month = sep,
pages = {1041–1083},
numpages = {43},
keywords = {Configurable cyber-physical systems, Test automation, Test system generation}
}

@article{10.1504/IJCAT.2015.070492,
author = {Kherissi, Farida and Meslati, Djamel and Tamzalit, Dalila},
title = {An approach based on extending the RUP for dealing with anticipated changes in ontogenetic software systems},
year = {2015},
issue_date = {July 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {51},
number = {4},
issn = {0952-8091},
url = {https://doi.org/10.1504/IJCAT.2015.070492},
doi = {10.1504/IJCAT.2015.070492},
abstract = {Software ontogeny refers to the ability of software to evolve dynamically in an autonomous way to meet the user needs and the anticipated and unanticipated changes of requirements. The evolution of an ontogenetic system has the particularity to be a continuous process that shapes them from the beginning of their creation. This feature does not match the current development methods, which consider that evolution is a sporadic process. Consequently, the platforms, tools and methodologies we use to develop software systems are not suitable and a huge amount of work is needed to adapt them to support the software ontogeny. In this paper, we propose a combination of three requirement engineering approaches: futures wheel method, goal-oriented requirement engineering and change cases, along with an extension of the rational unified process RUP order to modelling and handling anticipated changes in the hypothetical context of ontogenetic systems.},
journal = {Int. J. Comput. Appl. Technol.},
month = jul,
pages = {294–305},
numpages = {12}
}

@inproceedings{10.1145/2858036.2858372,
author = {Asenov, Dimitar and Hilliges, Otmar and M\"{u}ller, Peter},
title = {The Effect of Richer Visualizations on Code Comprehension},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858372},
doi = {10.1145/2858036.2858372},
abstract = {Researchers often introduce visual tools to programming environments in order to facilitate program comprehension, reduce navigation times, and help developers answer difficult questions. Syntax highlighting is the main visual lens through which developers perceive their code, and yet its effects and the effects of richer code presentations on code comprehension have not been evaluated systematically. We present a rigorous user study comparing mainstream syntax highlighting to two visually-enhanced presentations of code. Our results show that: (1) richer code visualizations reduce the time necessary to answer questions about code features, and (2) contrary to the subjective perception of developers, richer code visualizations do not lead to visual overload. Based on our results we outline practical recommendations for tool designers.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {5040–5045},
numpages = {6},
keywords = {code comprehension, code editor, programming, syntax highlighting, user study, visual programming},
location = {San Jose, California, USA},
series = {CHI '16}
}

@article{10.1145/1281421.1281444,
author = {Sawyer, Pete and Paech, Barbara and Heymans, Patrick},
title = {Working conference on requirements engineering: foundation for software quality (REFSQ'07)},
year = {2007},
issue_date = {July 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1281421.1281444},
doi = {10.1145/1281421.1281444},
abstract = {REFSQ'07 took place over the 11th and 12th June 2007 in the beautiful Norwegian city of Trondheim. It was attended by 47 people from 14 countries, who came to present and/or discuss 27 papers, the insights they offered and the issues they raised. During the course of the two days, a number of recurrent and important themes emerged. These were co-design of requirements and architecture, analyst-stakeholder-developer communication, requirements dependencies, the role of value in requirements engineering (RE) and the role and the progress made by research in RE.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {47–53},
numpages = {7},
keywords = {requirements, software quality}
}

@inproceedings{10.1109/MiSE.2019.00018,
author = {Sch\"{o}ttle, Matthias and Kienzle, J\"{o}rg},
title = {On the difficulties of raising the level of abstraction and facilitating reuse in software modelling: the case for signature extension},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MiSE.2019.00018},
doi = {10.1109/MiSE.2019.00018},
abstract = {Reuse is central to improving the software development process, increasing software quality and decreasing time-to-market. Hence it is of paramount importance that modelling languages provide features that enable the specification and modularization of reusable artefacts, as well as their subsequent reuse. In this paper we outline several difficulties caused by the finality of method signatures that make it hard to specify and use reusable artefacts encapsulating several variants. The difficulties are illustrated with a running example. To evaluate whether these difficulties can be observed at the programming level, we report on an empirical study conducted on the Java Platform API as well as present workarounds used in various programming languages to deal with the rigid nature of signatures. Finally, we outline signature extension as an approach to overcome these problems at the modelling level.},
booktitle = {Proceedings of the 11th International Workshop on Modelling in Software Engineerings},
pages = {71–77},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {MiSE '19}
}

@inproceedings{10.1145/1066677.1067012,
author = {Blake, M. Brian and Cleary, Kevin and Ranjan, Sohan R. and Ibanez, Luis and Gary, Kevin},
title = {Use case-driven component specification: a medical applications perspective to product line development},
year = {2005},
isbn = {1581139640},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066677.1067012},
doi = {10.1145/1066677.1067012},
abstract = {Modular and flexible software components can be useful for reuse across a class of domain-specific applications or product lines. By varying the composition of components suited to a particular product line, an assortment of applications can be developed to support differing operational needs. A top-down approach to the design components for a specific application may be effective, however a more evolutionary approach is needed to support the specification of components suited for a class of applications. In addition, such evolutionary approaches require support for the knowledge transfer that must occur from domain experts, who are not software experts, to skilled software engineers. By combining concepts from Software Product Line Development (SPLD) and other evolutionary design techniques, a new, use case-driven approach has been created called Component-Based Product Line Analysis and Design (C-PLAD). This approach was used to develop components in the domain of image-guided surgery applications.},
booktitle = {Proceedings of the 2005 ACM Symposium on Applied Computing},
pages = {1470–1477},
numpages = {8},
keywords = {component specifications, generation of component-based systems, medical domain, software lifecycle},
location = {Santa Fe, New Mexico},
series = {SAC '05}
}

@inproceedings{10.1145/3364641.3364649,
author = {da Mota Moura, Ana Maria and de Oliveira, Romeu Ferreira and Fernandes, Eduardo and de Lacerda Caetano, Lauro and Manoel, Luciene and do Prado Leite, Julio Cesar Sampaio},
title = {Improving Urban Mobility for the Visually Impaired using the Awareness Quality},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364649},
doi = {10.1145/3364641.3364649},
abstract = {Urban mobility for disabled people is a hard challenge. The technology of the Internet of Things (IoT) might empower these individuals by contributing to their safety and security. In the case of the visually impaired, smart canes are critical components for tackling the challenge of mobility for this population. These types of devices, when integrated into the IoT ecology, may augment their effectiveness. However, this integration is tricky, and we should plan it accordingly. We show how the use of quality requirements (NonFunctional Requirements) can contribute to the development of these types of systems. In this context, self-adaptive systems may improve the solution. To properly accomplish the expected benefits from self-adaptation, we demonstrate how the reuse of an awareness catalog contributes to the derivation of modular software architecture. Our demonstration focus on a real self-adaptive system, which was re-engineered to connect better the different parts of an IoT solution that supports urban mobility for the visually impaired.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {59–68},
numpages = {10},
keywords = {Awareness, Non-functional requirement, Self-Adaptive system},
location = {Fortaleza, Brazil},
series = {SBQS '19}
}

@inproceedings{10.5555/1558013.1558030,
author = {Morandini, Mirko and Penserini, Loris and Perini, Anna},
title = {Operational semantics of goal models in adaptive agents},
year = {2009},
isbn = {9780981738161},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Several agent-oriented software engineering methodologies address the emerging challenges posed by the increasing need of adaptive software. A common denominator of such methodologies is the paramount importance of the concept of goal model in order to understand the requirements of a software system. Goal models consist of goal graphs representing AND/OR-decomposition of abstract goals down to operationalisable leaf-level goals. Goal models are used primarily in the earlier phases of software engineering, for social modelling, requirements elicitation and analysis, to concretise abstract objectives, to detail them and to capture alternatives for their satisfaction.Although various agent programming languages incorporate the notion of (leaf-level) goal as a language construct, none of them natively support the definition of goal models. However, the semantic gap between goal models used at design-time and the concept of goal used at implementation and execution time represent a limitation especially in the development of self-adaptive and fault-tolerant systems. In such systems, design-time knowledge on goals and variability becomes relevant at run-time, to take autonomous decisions for achieving high level objectives correctly.Recently, unifying operational semantics for (leaf) goals have been proposed [15]. We extend this work to define an operational semantics for the behaviour of goals in goal models, maintaining the flexibility of using different goal types and conditions. We use a simple example to illustrate how the proposed approach effectively deals with the semantic gap between design-time goal models and run-time agent implementations.},
booktitle = {Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {129–136},
numpages = {8},
keywords = {agent programming, formal semantics, goal models, goals},
location = {Budapest, Hungary},
series = {AAMAS '09}
}

@book{10.5555/2671146,
author = {Mistrik, Ivan and Bahsoon, Rami and Kazman, Rick and Zhang, Yuanyuan},
title = {Economics-Driven Software Architecture},
year = {2014},
isbn = {0124104649},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Economics-driven Software Architecture presents a guide for engineers and architects who need to understand the economic impact of architecture design decisions: the long term and strategic viability, cost-effectiveness, and sustainability of applications and systems. Economics-driven software development can increase quality, productivity, and profitability, but comprehensive knowledge is needed to understand the architectural challenges involved in dealing with the development of large, architecturally challenging systems in an economic way. This book covers how to apply economic considerations during the software architecting activities of a project. Architecture-centric approaches to development and systematic evolution, where managing complexity, cost reduction, risk mitigation, evolvability, strategic planning and long-term value creation are among the major drivers for adopting such approaches. It assists the objective assessment of the lifetime costs and benefits of evolving systems, and the identification of legacy situations, where architecture or a component is indispensable but can no longer be evolved to meet changing needs at economic cost. Such consideration will form the scientific foundation for reasoning about the economics of nonfunctional requirements in the context of architectures and architecting. Familiarizes readers with essential considerations in economic-informed and value-driven software design and analysis Introduces techniques for making value-based software architecting decisions Provides readers a better understanding of the methods of economics-driven architecting}
}

@article{10.1145/3479160,
author = {Kozma-Spytek, Linda and Vogler, Christian},
title = {Factors Affecting the Accessibility of Voice Telephony for People with Hearing Loss: Audio Encoding, Network Impairments, Video and Environmental Noise},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1936-7228},
url = {https://doi.org/10.1145/3479160},
doi = {10.1145/3479160},
abstract = {This paper describes four studies with a total of 114 individuals with hearing loss and 12 hearing controls that investigate the impact of audio quality parameters on voice telecommunications. These studies were first informed by a survey of 439 individuals with hearing loss on their voice telecommunications experiences. While voice telephony was very important, with high usage of wireless mobile phones, respondents reported relatively low satisfaction with their hearing devices’ performance for telephone listening, noting that improved telephone audio quality was a significant need. The studies cover three categories of audio quality parameters: (1) narrowband (NB) versus wideband (WB) audio; (2) encoding audio at varying bit rates, from typical rates used in today's mobile networks to the highest quality supported by these audio codecs; and (3) absence of packet loss to worst-case packet loss in both mobile and VoIP networks. Additionally, NB versus WB audio was tested in auditory-only and audiovisual presentation modes and in quiet and noisy environments. With WB audio in a quiet environment, individuals with hearing loss exhibited better speech recognition, expended less perceived mental effort, and rated speech quality higher than with NB audio. WB audio provided a greater benefit when listening alone than when the visual channel also was available. The noisy environment significantly degraded performance for both presentation modes, but particularly for listening alone. Bit rate affected speech recognition for NB audio, and speech quality ratings for both NB and WB audio. Packet loss affected all of speech recognition, mental effort, and speech quality ratings. WB versus NB audio also affected hearing individuals, especially under packet loss. These results are discussed in terms of the practical steps they suggest for the implementation of telecommunications systems and related technical standards and policy considerations to improve the accessibility of voice telephony for people with hearing loss.},
journal = {ACM Trans. Access. Comput.},
month = oct,
articleno = {21},
numpages = {35},
keywords = {Voice telephony, telecommunications, hearing loss, hard of hearing, cochlear implant, hearing aid, wideband audio, narrowband audio, audio codec, bit rate, packet loss}
}

@inproceedings{10.1109/CGO.2009.33,
author = {Voronenko, Yevgen and de Mesmay, Fr\'{e}d\'{e}ric and P\"{u}schel, Markus},
title = {Computer Generation of General Size Linear Transform Libraries},
year = {2009},
isbn = {9780769535760},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CGO.2009.33},
doi = {10.1109/CGO.2009.33},
abstract = {The development of high-performance libraries has become extraordinarily difficult due to multiple processor cores, vector instruction sets, and deep memory hierarchies. Often, the library has to be reimplemented and reoptimized, when a new platform is released. In this paper we show how to automatically generate general input-size libraries for the domain of linear transforms. The input to our generator is a formal specification of the transform and the recursive algorithms the library should use; the output is a library that supports general input size, is vectorized and multithreaded, provides an adaptation mechanism for the memory hierarchy, and has excellent performance, comparable to or better than the best human-written libraries. Further, we show that our library generator enables various customizations; one example is the generation of Java libraries.},
booktitle = {Proceedings of the 7th Annual IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {102–113},
numpages = {12},
keywords = {DFT, Linear transform, SIMD vector instructions, automatic performance tuning, discrete Fourier transform, domain-specific language, multithreading, program generation},
series = {CGO '09}
}

@article{10.1016/j.infsof.2010.05.007,
author = {Mellado, Daniel and Fern\'{a}ndez-Medina, Eduardo and Piattini, Mario},
title = {Security requirements engineering framework for software product lines},
year = {2010},
issue_date = {October, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.05.007},
doi = {10.1016/j.infsof.2010.05.007},
abstract = {Context: The correct analysis and understanding of security requirements are important because they assist in the discovery of any security or requirement defects or mistakes during the early stages of development. Security requirements engineering is therefore both a central task and a critical success factor in product line development owing to the complexity and extensive nature of software product lines (SPL). However, most of the current SPL practices in requirements engineering do not adequately address security requirements engineering. Objective: The aim of this approach is to describe a holistic security requirements engineering framework with which to facilitate the development of secure SPLs and their derived products. It will conform with the most relevant security standards with regard to the management of security requirements, such as ISO/IEC 27001 and ISO/IEC 15408. Results: This framework is composed of: a security requirements engineering process for SPL (SREPPLine) driven by security standards; a Security Reference Meta Model to manage the variability of those SPL artefacts related to security requirements; and a tool (SREPPLineTool) which implements the meta-model and supports the process. Method: A complete explanation of the framework will be provided. The process will be formally specified with SPEM 2.0 and the repository will be formally specified with an XML grammar. The application of SREPPLine and SREPPLineTool will be illustrated through a description of a simple example as a preliminary validation. Conclusion: Although there have been several attempts to fill the gap between requirements engineering and SPL requirements engineering, no systematic approach with which to define security quality requirements and to manage their variability and their related security artefacts in SPL models is, as yet, available. The contribution of this work is that of providing a systematic approach for the management of the security requirements and their variability from the early stages of product line development in order to facilitate the conformance of SPL products with the most relevant security standards.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1094–1117},
numpages = {24},
keywords = {ISO 27001, Product lines, Requirements engineering, Security requirement, Security requirements engineering, Security software engineering}
}

@article{10.1177/1046878109334332,
author = {Sanders, Janet H. and Udoka, Silvanus J.},
title = {An Information Provision Framework for Performance-Based Interactive eLearning Application for Manufacturing},
year = {2010},
issue_date = {August    2010},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {41},
number = {4},
issn = {1046-8781},
url = {https://doi.org/10.1177/1046878109334332},
doi = {10.1177/1046878109334332},
abstract = {Fundamental concepts and definitions of electronic learning (eLearning) continue to emerge, and theories of eLearning that have been advanced thus far cover an array of academic perspectives including training and education, learning and knowledge, and technology and applications to specific market segments. Any study of the effectiveness and efficiency of eLearning, therefore, has to address a variety of issues, including the role of eLearning in knowledge and learning, its contribution to competent performance, its relationship to organizational transformation, and strategies for embedding it into other forms of electronic interaction. eLearning refers to a form of learning in which the instructor and student are physically separated by space or time, and the gap between the two is bridged through the use of online technologies. Virtual technology utilizes an interactive approach to computer-based learning by providing real-time feedback to the user. Surveys of manufacturing companies have verified their interest and enthusiasm in the potential of virtual technology for industrial applications; however, the companies noted that one of the barriers to investing in the technology is the need for a structured methodology to guide the application identification, as well as the model building and evaluation for this technology. The study referenced in this article addresses this need by providing a framework for the development of a virtual environment that provides information for manufacturing task completion. It builds upon extant research into the use of virtual reality for task completion as well as proposes a structure for virtual environment development.},
journal = {Simul. Gaming},
month = aug,
pages = {511–536},
numpages = {26},
keywords = {computer-based learning, eLearning, electronic interaction, electronic learning, industrial applications, information provision, knowledge, lean manufacturing, manufacturing task completion, organizational transformation, performance, real-time feedback, structured methodology, task completion, virtual environment, virtual reality, virtual technology, visualization}
}

@inproceedings{10.1145/3439961.3439975,
author = {Pald\^{e}s, Roberto Avila and Canedo, Edna Dias and Guimar\~{a}es, Fernando de Albuquerque and Calazans, Ang\'{e}lica Toffano Seidel},
title = {Functional Requirements Elicitation in IoT Systems: a follow-up study},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439975},
doi = {10.1145/3439961.3439975},
abstract = {As the Internet of Things (IoT) advances, specific views have been proposed for the entire software development cycle and also for Requirements Engineering (RE). The analysis of the use of RE techniques, tools, and models can contribute to obtain better results in this field. This paper presents a Systematic Mapping Study (SMS) to investigate techniques for Functional Requirements (FR) elicitation in IoT software systems, as well as gaps and limitations of current solutions. During the SMS, seventeen articles focused on FR in the IoT were found. The analysis was complemented with an input from the experience of practitioners who have dedicated to this topic, obtained through structured and semi-structured interviews. The results show that FR elicitation has started from the use of traditional techniques, but that these do not fully meet the specificities of the IoT. The majority of the models found are based on UML (Unified Modeling Language) and the most important techniques are based on scenarios. The tools that support these proposals are maturing or under development. In the conclusion, the study shows the advancements already achieved, as well as the challenges and opportunities that are still present.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {14},
numpages = {10},
keywords = {Functional Requirements Elicitation, Internet of Things, Software System, Systematic Mapping Study.},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@inproceedings{10.1145/2993236.2993249,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {A feature-based personalized recommender system for product-line configuration},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993249},
doi = {10.1145/2993236.2993249},
abstract = {Today’s competitive marketplace requires the industry to understand unique and particular needs of their customers. Product line practices enable companies to create individual products for every customer by providing an interdependent set of features. Users configure personalized products by consecutively selecting desired features based on their individual needs. However, as most features are interdependent, users must understand the impact of their gradual selections in order to make valid decisions. Thus, especially when dealing with large feature models, specialized assistance is needed to guide the users in configuring their product. Recently, recommender systems have proved to be an appropriate mean to assist users in finding information and making decisions. In this paper, we propose an advanced feature recommender system that provides personalized recommendations to users. In detail, we offer four main contributions: (i) We provide a recommender system that suggests relevant features to ease the decision-making process. (ii) Based on this system, we provide visual support to users that guides them through the decision-making process and allows them to focus on valid and relevant parts of the configuration space. (iii) We provide an interactive open-source configurator tool encompassing all those features. (iv) In order to demonstrate the performance of our approach, we compare three different recommender algorithms in two real case studies derived from business experience.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {120–131},
numpages = {12},
keywords = {Personalized Recommendations, Product-Line Configuration, Recommenders, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@article{10.1007/s10257-020-00499-9,
author = {Xiao, Zhongdong and Shu, Wenjun and Owusu, Abigail Osei},
title = {An analysis of product strategy in cloud transition considering SaaS customization},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {1},
issn = {1617-9846},
url = {https://doi.org/10.1007/s10257-020-00499-9},
doi = {10.1007/s10257-020-00499-9},
abstract = {When traditional enterprise software vendors adapt to software as a service (SaaS) practices and evolving to cloud service models, there is a major change occurring in the enterprise: a new hybrid product strategy consists of on-premises software and competitive customized SaaS. For the first time, we build a stylized model to reveal the influence of SaaS customization on the decisions of monopoly software vendor in the transition period. Increasing the customization efficiency of SaaS results in two possible structural regimes in the market. One is single on-premises software dominate the market if SaaS is customized at a low level and the other is hybrid products segment the market if SaaS is moderate-level customized. Surprisingly, software vendors with high customization proficiency should not allow SaaS products to dominate the market. It would benefit more from offering a competitive hybrid product strategy. Therefore, this paper does not recommend traditional software vendors to transform into pure cloud service providers. This key findings remain valid in the extended analysis of other customization technologies. Besides, the extension models show that both configuration and personalization customization technologies outperform the modification customization technology.},
journal = {Inf. Syst. E-Bus. Manag.},
month = mar,
pages = {281–311},
numpages = {31},
keywords = {Product strategy, Pricing, SaaS customization, Cloud computing, Market structure}
}

@article{10.1016/j.jss.2015.09.019,
author = {Vale, Tassio and Crnkovic, Ivica and de Almeida, Eduardo Santana and Silveira Neto, Paulo Anselmo da Mota and Cavalcanti, Yguarat\~{a} Cerqueira and Meira, Silvio Romero de Lemos},
title = {Twenty-eight years of component-based software engineering},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {111},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.09.019},
doi = {10.1016/j.jss.2015.09.019},
abstract = {We defined more precisely the identification of the gaps.We also defined more precisely the incentives for further research.In Section 4.3 we made explicit connection to the Fig. 15 and identified gaps.All pointed typos were fixed. The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research.},
journal = {J. Syst. Softw.},
month = jan,
pages = {128–148},
numpages = {21},
keywords = {Component-based software development, Component-based software engineering, Software component, Systematic mapping study}
}

@inproceedings{10.1145/1842752.1842812,
author = {Abbas, Nadeem and Andersson, Jesper and L\"{o}we, Welf},
title = {Autonomic Software Product Lines (ASPL)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842812},
doi = {10.1145/1842752.1842812},
abstract = {We describe ongoing work on a variability mechanism for Autonomic Software Product Lines (ASPL). The autonomic software product lines have self-management characteristics that make product line instances more resilient to context changes and some aspects of product line evolution. Instances sense the context, selects and bind the best component variants to variation-points at run-time. The variability mechanism we describe is composed of a profile guided dispatch based on off-line and on-line training processes. Together they form a simple, yet powerful variability mechanism that continuously learns, which variants to bind given the current context and system goals.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {324–331},
numpages = {8},
keywords = {MAPE-K, autonomic elements, context, goals, off-line training, on-line, variability, variants, variation-points},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1109/ICSE.2019.00090,
author = {Lillack, Max and St\u{a}nciulescu, \c{S}tefan and Hedman, Wilhelm and Berger, Thorsten and W\k{a}sowski, Andrzej},
title = {Intention-based integration of software variants},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00090},
doi = {10.1109/ICSE.2019.00090},
abstract = {Cloning is a simple way to create new variants of a system. While cheap at first, it increases maintenance cost in the long term. Eventually, the cloned variants need to be integrated into a configurable platform. Such an integration is challenging: it involves merging the usual code improvements between the variants, and also integrating the variable code (features) into the platform. Thus, variant integration differs from traditional software merging, which does not produce or organize configurable code, but creates a single system that cannot be configured into variants. In practice, variant integration requires fine-grained code edits, performed in an exploratory manner, in multiple iterations. Unfortunately, little tool support exists for integrating cloned variants.In this work, we show that fine-grained code edits needed for integration can be alleviated by a small set of integration intentions---domain-specific actions declared over code snippets controlling the integration. Developers can interactively explore the integration space by declaring (or revoking) intentions on code elements. We contribute the intentions (e.g., 'keep functionality' or 'keep as a configurable feature') and the IDE tool INCLINE, which implements the intentions and five editable views that visualize the integration process and allow declaring intentions producing a configurable integrated platform. In a series of experiments, we evaluated the completeness of the proposed intentions, the correctness and performance of INCLINE, and the benefits of using intentions for variant integration. The experiments show that INCLINE can handle complex integration tasks, that views help to navigate the code, and that it consistently reduces mistakes made by developers during variant integration.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {831–842},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/3063594,
author = {Cizmeci, Burak and Xu, Xiao and Chaudhari, Rahul and Bachhuber, Christoph and Alt, Nicolas and Steinbach, Eckehard},
title = {A Multiplexing Scheme for Multimodal Teleoperation},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3063594},
doi = {10.1145/3063594},
abstract = {This article proposes an application-layer multiplexing scheme for teleoperation systems with multimodal feedback (video, audio, and haptics). The available transmission resources are carefully allocated to avoid delay-jitter for the haptic signal potentially caused by the size and arrival time of the video and audio data. The multiplexing scheme gives high priority to the haptic signal and applies a preemptive-resume scheduling strategy to stream the audio and video data. The proposed approach estimates the available transmission rate in real time and adapts the video bitrate, data throughput, and force buffer size accordingly. Furthermore, the proposed scheme detects sudden transmission rate drops and applies congestion control to avoid abrupt delay increases and converge promptly to the altered transmission rate. The performance of the proposed scheme is measured objectively in terms of end-to-end signal latencies, packet rates, and peak signal-to-noise ratio (PSNR) for visual quality. Moreover, peak-delay and convergence time measurements are carried out to investigate the performance of the congestion control mode of the system.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {21},
numpages = {28},
keywords = {Haptics, congestion control, haptic compression and communication, human--robot interaction over communication networks, multiplexing, rate control, teleoperation}
}

@inproceedings{10.1145/2335484.2335506,
author = {Hirzel, Martin},
title = {Partition and compose: parallel complex event processing},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335506},
doi = {10.1145/2335484.2335506},
abstract = {Complex event processing uses patterns to detect composite events in streams of simple events. Typically, the events are logically partitioned by some key. For instance, the key can be the stock symbol in stock quotes, the author in tweets, the vehicle in transportation, or the patient in health-care. Composite event patterns often become meaningful only after partitioning. For instance, a pattern over stock quotes is typically meaningful over quotes for the same stock symbol. This paper proposes a pattern syntax and translation scheme organized around the notion of partitions. Besides making patterns meaningful, partitioning also benefits performance, since different keys can be processed in parallel. We have implemented partitioned parallel complex event processing as an extension to IBM's System S high-performance streaming platform. Our experiments with several benchmarks from finance and social media demonstrate processing speeds of up to 830,000 events per second, and substantial speedups for expensive patterns parallelized on multi-core machines as well as multi-machine clusters. Partitioning the event stream before detecting composite events makes event processing both more intuitive and parallel.},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {191–200},
numpages = {10},
keywords = {CEP, SPL, automata, composite events, parallelism, pattern matching, regular expressions, stream processing},
location = {Berlin, Germany},
series = {DEBS '12}
}

@inproceedings{10.1145/2993236.2993246,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Collet, Philippe and Alam, Omar},
title = {Delaying decisions in variable concern hierarchies},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993246},
doi = {10.1145/2993236.2993246},
abstract = {Concern-Oriented Reuse (CORE) proposes a new way of structuring model-driven software development, where models of the system are modularized by domains of abstraction within units of reuse called concerns. Within a CORE concern, models are further decomposed and modularized by features. This paper extends CORE with a technique that enables developers of high-level concerns to reuse lower-level concerns without unnecessarily committing to a specific feature selection. The developer can select the functionality that is minimally needed to continue development, and reexpose relevant alternative lower-level features of the reused concern in the reusing concern's interface. This effectively delays decision making about alternative functionality until the higher-level reuse context, where more detailed requirements are known and further decisions can be made. The paper describes the algorithms for composing the variation (i.e., feature and impact models), customization, and usage interfaces of a concern, as well as the concern's realization models and finally an entire concern hierarchy, as is necessary to support delayed decision making in CORE.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {93–103},
numpages = {11},
keywords = {Delaying of Decisions, Model Interfaces, Model Reuse, Model-Driven Engineering, Reuse Hierarchies},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@article{10.1016/j.jss.2015.05.006,
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah},
title = {Feature extraction approaches from natural language requirements for reuse in software product lines},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.05.006},
doi = {10.1016/j.jss.2015.05.006},
abstract = {Hybrid NLP approaches were more common for extracting textual requirements.There is a mixture of automated and semi-automated approaches from IR and data mining.Support tools were not made available to the public.Not all studies use software metrics in conjunction with experiments and case studies.Reconfirm practitioners guidelines' absence from selected studies (Alves et al., 2010). Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies.},
journal = {J. Syst. Softw.},
month = aug,
pages = {132–149},
numpages = {18},
keywords = {Feature extractions, Natural language requirements, Requirements reuse, Software product lines, Systematic literature review}
}

@article{10.1016/j.future.2014.12.002,
author = {Weinreich, Rainer and Groher, Iris and Miesbauer, Cornelia},
title = {An expert survey on kinds, influence factors and documentation of design decisions in practice},
year = {2015},
issue_date = {June 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2014.12.002},
doi = {10.1016/j.future.2014.12.002},
abstract = {Support for capturing architectural knowledge has been identified as an important research challenge. As the basis for an approach to recovering design decisions and capturing their rationale, we performed an expert survey in practice to gain insights into the different kinds, influence factors, and sources for design decisions and also into how they are currently captured in practice. The survey was conducted with 25 software architects, software team leads, and senior developers from 22 different companies in 10 different countries with more than 13 years of experience in software development on average. The survey confirms earlier work by other authors on design decision classification and influence factors, and also identifies additional kinds of decisions and influence factors not mentioned in previous work. In addition, we gained insight into the practice of capturing, the relative importance of different decisions and influence factors, and into potential sources for recovering decisions. We present results of a qualitative expert survey on design decisions in practice.We examine design decision classification, documentation, and influence factors.We collect architects' experiences in decision making and documentation.We provide recommendations for potential improvements and research directions based on the results of our study.Results are compared to literature and similar studies.},
journal = {Future Gener. Comput. Syst.},
month = jun,
pages = {145–160},
numpages = {16},
keywords = {Design decision classification, Design decision documentation, Design decision influence factors, Design decisions, Software architecture knowledge management}
}

@article{10.1016/j.cl.2018.01.003,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {Personalized recommender systems for product-line configuration processes},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.003},
doi = {10.1016/j.cl.2018.01.003},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {451–471},
numpages = {21},
keywords = {Product lines, Feature model, Product-line configuration, Recommender systems, Personalized recommendations}
}

@article{10.1016/j.infsof.2017.03.004,
author = {Dey, Sangeeta and Lee, Seok-Won},
title = {REASSURE},
year = {2017},
issue_date = {July 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {87},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.03.004},
doi = {10.1016/j.infsof.2017.03.004},
abstract = {ContextSocio-technical systems are expected to understand the dynamics of the execution environment and behave accordingly. Significant work has been done on formalizing and modeling requirements of such adaptive systems. However, not enough attention is paid on eliciting requirements from users and introducing flexibility in the system behavior at an early phase of requirements engineering. Most of the work is based on an assumption that general users cognitive level would be able to support the inherent complexity of variability acquisition. ObjectiveOur main focus is on providing help to the users with ordinary cognitive level to express their expectations from the complex system considering various contexts. This work also helps the designers to explore the design variability based on the general users preferences. MethodWe explore the idea of using a cognitive technique Repertory Grid (RG) to acquire knowledge from users and experts along multiple dimensions of problem and design space. We propose REASSURE methodology which guides requirements engineers to explore the intentional and design variability in an organized way. We also provide a tool support to analyze the knowledge captured in multiple repertory grid files and detect potential conflicts in the intentional variability. Finally, we evaluate the proposed idea by performing an empirical study using smart home system domain. ResultsThe result of our study shows that a greater number of requirements can be elicited after applying our approach. With the help of the provided tool support, it is even possible to detect a greater number of conflicts in users requirements than the traditional practices. ConclusionWe envision RG as a technique to filter design options based on the intentional variability in various contexts. The promising results of empirical study open up new research questions: how to elicit requirements from multiple stakeholders and reach consensus for multi-dimensional problem domain.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {160–179},
numpages = {20},
keywords = {Adaptive systems, Repertory grid, Requirements elicitation, Socio-technical systems}
}

@article{10.1016/j.jss.2012.10.013,
author = {Nakagawa, Elisa Y. and Antonino, Pablo O. and Becker, Martin and Maldonado, Jos\'{e} C. and Storf, Holger and Villela, Karina B. and Rombach, Dieter},
title = {Relevance and perspectives of AAL in Brazil},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.10.013},
doi = {10.1016/j.jss.2012.10.013},
abstract = {Population aging has been taking place in many countries across the globe and more recently in emerging countries. In this context, Ambient Assisted Living (AAL) has become one focus of attention, including methods, products, services, and AAL software systems that support the everyday lives of elderly people, promoting mainly their independence and dignity. From the perspective of computer science, efforts are already being dedicated to adequately developing AAL systems. However, in spite of its relevance, AAL has not been properly investigated in emerging countries, including Brazil. Thus, the contribution of this paper is to present the main perspectives of research in AAL, in particular in the area of software engineering, considering that the Brazilian population is also subject to the aging process. The main intention of this paper is to raise the interest of Brazilian researchers, as well as government and industry, for this important area.},
journal = {J. Syst. Softw.},
month = apr,
pages = {985–996},
numpages = {12},
keywords = {AAL platform, Ambient Assisted Living (AAL), Population aging, Reference architecture}
}

@inproceedings{10.1145/3290605.3300713,
author = {Memoli, Gianluca and Chisari, Letizia and Eccles, Jonathan P. and Caleap, Mihai and Drinkwater, Bruce W. and Subramanian, Sriram},
title = {VARI-SOUND: A Varifocal Lens for Sound},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300713},
doi = {10.1145/3290605.3300713},
abstract = {Centuries of development in optics have given us passive devices (i.e. lenses, mirrors and filters) to enrich audience immersivity with light effects, but there is nothing similar for sound. Beam-forming in concert halls and outdoor gigs still requires a large number of speakers, while headphones are still the state-of-the-art for personalized audio immersivity in VR. In this work, we show how 3D printed acoustic meta-surfaces, assembled into the equivalent of optical systems, may offer a different solution. We demonstrate how to build them and how to use simple design tools, like the thin-lens equation, also for sound. We present some key acoustic devices, like a "collimator", to transform a standard computer speaker into an acoustic "spotlight"; and a "magnifying glass", to create sound sources coming from distinct locations than the speaker itself. Finally, we demonstrate an acoustic varifocal lens, discussing applications equivalent to auto-focus cameras and VR headsets and the limitations of the technology.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {3d printing, fabrication, metamaterials, microstructures, spatial audio},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@article{10.1145/2088883.2088900,
author = {Tekinerdogan, Bedir and Cetin, Semih and Babar, Muhammad Ali and Lago, Patricia and M\"{a}ki\"{o}, Juho},
title = {Architecting in global software engineering},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2088883.2088900},
doi = {10.1145/2088883.2088900},
abstract = {This paper summarizes the results of the First Workshop on Arc-hitecting in Global Software Engineering (GSE), which was or-ganized in conjunction with the 6th International Conference on Global Software Engineering (ICGSE 2011). The workshop aimed to bring together researchers and practitioners for defining and advancing the state-of-the-art and state-of-the practice in architecture design of global software development systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–7},
numpages = {7},
keywords = {global software engineering, software architecture, workshop}
}

@inproceedings{10.1145/2701319.2701329,
author = {Krieter, Sebastian and Schr\"{o}ter, Reimar and Fenske, Wolfram and Saake, Gunter},
title = {Use-Case-Specific Source-Code Documentation for Feature-Oriented Programming},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701329},
doi = {10.1145/2701319.2701329},
abstract = {Source-code documentation is essential to efficiently develop and maintain large software products. Documentation is equally important for software product lines (SPLs), which represent a set of different products with a common code base. Unfortunately, proper support for documenting the source code of an SPL is currently lacking, because source code variability is not considered by current documentation tools. We introduce a method to provide source-code documentation for feature-oriented programming and aim to support developers who implement, maintain, and use SPLs. We identify multiple use cases for developers working with SPLs and propose four different documentation types (meta, product, feature, and context) that fulfill the information requirements of these use cases. Furthermore, we design an algorithm that enables developers to create tailor-made documentation for each use case. Our method is based on the documentation tool Javadoc and allows developers to easily write documentation comments that contain little overhead or redundancy. To demonstrate the efficiency of our method, we present a prototypical implementation and evaluate our method with regard to documentation effort for the SPL developers by documenting two small SPLs.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {27–34},
numpages = {8},
keywords = {API Documentation, Feature-Oriented Programming, Software Product Lines, Source Code Documentation},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@article{10.1145/3229048,
author = {Zheng, Yongjie and Cu, Cuong and Taylor, Richard N.},
title = {Maintaining Architecture-Implementation Conformance to Support Architecture Centrality: From Single System to Product Line Development},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3229048},
doi = {10.1145/3229048},
abstract = {Architecture-centric development addresses the increasing complexity and variability of software systems by focusing on architectural models, which are generally easier to understand and manipulate than source code. It requires a mechanism that can maintain architecture-implementation conformance during architectural development and evolution. The challenge is twofold. There is an abstraction gap between software architecture and implementation, and both may evolve. Existing approaches are deficient in support for both change mapping and product line architecture. This article presents a novel approach named 1.x-way mapping and its extension, 1.x-line mapping to support architecture-implementation mapping in single system development and in product line development, respectively. They specifically address mapping architecture changes to code, maintaining variability conformance between product line architecture and code, and tracing architectural implementation. We built software tools named xMapper and xLineMapper to realize the two approaches, and conducted case studies with two existing open-source systems to evaluate the approaches. The result shows that our approaches are applicable to the implementation of a real software system and are capable of maintaining architecture-implementation conformance during system evolution.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {8},
numpages = {52},
keywords = {Architecture-implementation mapping, architectural evolution, architecture-centric development, architecture-centric feature traceability, variability conformance}
}

@inproceedings{10.1145/2745802.2745815,
author = {Zhou, You and Zhang, He and Huang, Xin and Yang, Song and Babar, Muhammad Ali and Tang, Hao},
title = {Quality assessment of systematic reviews in software engineering: a tertiary study},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745815},
doi = {10.1145/2745802.2745815},
abstract = {Context: The quality of an Systematic Literature Review (SLR) is as good as the quality of the reviewed papers. Hence, it is vital to rigorously assess the papers included in an SLR. There has been no tertiary study aimed at reporting the state of the practice of quality assessment used in SLRs in Software Engineering (SE).Objective: We aimed to study the practices of quality assessment of the papers included in SLRs in SE.Method: We conducted a tertiary study of the SLRs that have performed quality assessment of the reviewed papers.Results: We identified and analyzed different aspects of the quality assessment of the papers included in 127 SLRs.Conclusion: Researchers use a variety of strategies for quality assessment of the papers reviewed, but report little about the justification for the used criteria. The focus is creditability but not relevance aspect of the papers. Appropriate guidelines are required for devising quality assessment strategies.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {14},
numpages = {14},
keywords = {quality assessment, software engineering, systematic (literature) review},
location = {Nanjing, China},
series = {EASE '15}
}

@article{10.1016/j.infsof.2017.02.002,
author = {Pessoa, Leonardo and Fernandes, Paula and Castro, Thiago and Alves, Vander and Rodrigues, Genana N. and Carvalho, Hervaldo},
title = {Building reliable and maintainable Dynamic Software Product Lines},
year = {2017},
issue_date = {June 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {86},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.02.002},
doi = {10.1016/j.infsof.2017.02.002},
abstract = {Context: Dependability is a key requirement, especially in safety-critical applications. Many of these applications have changing context and configurations at runtime to achieve functional and quality goals and can be realized as Dynamic Software Product Lines (DSPLs). DSPL constitutes an emerging but promising research area. Nevertheless, ensuring dependability in DSPLs remains insufficiently explored, especially in terms of reliability and maintainability. This compromises quality assurance and applicability of DSPLs in safety-critical domains, such as Body Sensor Network (BSN).Objective: To address this issue, we propose an approach to developing reliable and maintainable DSPLs in the context of the BSN domain.Method: Adaptation plans are instances of a Domain Specific Language (DSL) describing reliability goals and adaptability at runtime. These instances are automatically checked for reliability goal satisfiability before being deployed and interpreted at runtime to provide more suitable adaptation goals complying with evolving needs perceived by a domain specialist.Results: The approach is evaluated in the BSN domain. Results show that reliability and maintainability could be provided with execution and reconfiguration times of around 30ms, notification and adaptation plan update time over the network around 5s, and space consumption around 5 MB.Conclusion: The method is feasible at reasonable cost. The incurred benefits are reliable vital signal monitoring for the patientthus providing early detection of serious health issues and the possibility of proactive treatmentand a maintainable infrastructure allowing medical DSL instance update to suit the needs of the domain specialist and ultimately of the patient.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {54–70},
numpages = {17},
keywords = {Adaptiveness, Body Sensor Network, Context-awareness, Dynamic Software Product Lines, Maintainability, Reliability}
}

@inproceedings{10.1145/3308561.3353796,
author = {Kozma-Spytek, Linda and Tucker, Paula and Vogler, Christian},
title = {Voice Telephony for Individuals with Hearing Loss: The Effects of Audio Bandwidth, Bit Rate and Packet Loss},
year = {2019},
isbn = {9781450366762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3353796},
doi = {10.1145/3308561.3353796},
abstract = {This paper describes three studies conducted with a total of 114 individuals with hearing loss and 12 hearing controls, with the goal of investigating the impact of audio quality parameters on the accessibility of voice telecommunications. Three categories of parameters are covered: (1) narrowband (NB) versus wideband (WB) audio; (2) encoding audio at varying bit rates, ranging from typical rates used in today's telecom networks to the highest quality supported by these audio codecs; and (3) absence of packet loss to worst-case packet loss in VoIP telephony. With WB audio, individuals with hearing loss exhibit better speech recognition, expend less perceived mental effort, and rate speech quality higher than with NB audio. Bit rate affects speech recognition for NB audio, and speech quality ratings for both NB and WB audio. Packet loss affects all of speech recognition, mental effort, and speech quality ratings. WB versus NB audio also affects hearing individuals, especially under packet loss.},
booktitle = {Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {3–15},
numpages = {13},
keywords = {audio codec, bit rate, cochlear implant, hard of hearing, hearing aid, hearing loss, narrowband audio, packet loss, telecommunications, voice telephony, wideband audio},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19}
}

@inproceedings{10.1145/3323771.3323789,
author = {Kawano, Atsuko and Motoyama, Yuji and Aoyama, Mikio},
title = {A LX (Learner eXperience)-Based Evaluation Method of the Education and Training Programs for Professional Software Engineers},
year = {2019},
isbn = {9781450366397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323771.3323789},
doi = {10.1145/3323771.3323789},
abstract = {We propose a new design methodology to maximize the training effect in a corporate education and training for professional software engineers. Conventionally, the education and training programs have been designed in a top-down manner based on the long-term strategy on the business and engineering resources development. However, to draw out the learners' high performance from the education and training programs, we need to have an empathy with the learners, and to analyze their expectations and emotions in order to motivate them. Therefore, this paper proposes the learner-centered design methodology of the corporate education and training programs inspired by the design thinking and lean start-up concepts. We define the learning processes in the education and training programs as LX (Learner eXperience), and propose LJM (Learning Journey Map) as the LX evaluation method as an extension of CJM (Customer Journey Map) in UX (User eXperience) design. The LJM enables to evaluate training effect and communicate with stakeholders in the training design expressing the LX quantitatively in a visual form. We applied the proposed design methodology to the education and training programs for professional software engineers in a company to evaluate LX and elicit learner requirements to the programs. We applied the proposed LJM to the education and training program of two levels of the whole program and its LUs (Learning Units), and identified problems in the LX. From the empirical study, we confirm the effectiveness of the proposed methodology.},
booktitle = {Proceedings of the 2019 7th International Conference on Information and Education Technology},
pages = {151–159},
numpages = {9},
keywords = {Corporate education and training program, Design thinking, Journey map, LX (Learner eXperience), Lean start-up, Professional software engineer, UX (User eXperience)},
location = {Aizu-Wakamatsu, Japan},
series = {ICIET 2019}
}

@article{10.1007/s10845-011-0544-2,
author = {Yang, Dong and Dong, Ming},
title = {Applying constraint satisfaction approach to solve product configuration problems with cardinality-based configuration rules},
year = {2013},
issue_date = {February  2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {1},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-011-0544-2},
doi = {10.1007/s10845-011-0544-2},
abstract = {In this paper, the product configuration problems that are characterized by cardinality-based configuration rules are dealt with. Novel configuration rules including FI and EI rules are presented to clarify the semantics of inclusion rules when cardinalities and hierarchies of products are encountered. Then, a configuration graph is proposed to visualize structural rules and configuration rules in product configuration problem. An encoding approach is elaborated to transform the configuration graph as a CSP (Constraint Satisfaction Problem). As a consequence, existing CSP solver, i.e. JCL (Java Constraint Library), is employed to implement the configuration system for product configuration problem with cardinality-related configuration rules. A case study of a bus configuration is used throughout this paper to illustrate the effectiveness of the presented approach.},
journal = {J. Intell. Manuf.},
month = feb,
pages = {99–111},
numpages = {13},
keywords = {Constraint satisfaction, Mass customization, Product configuration}
}

@inproceedings{10.1145/1961258.1961282,
author = {Espinoza, Angelina and D\'{\i}az, Jessica},
title = {A traceability semantics approach for supporting product value analysis},
year = {2010},
isbn = {9781450302814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1961258.1961282},
doi = {10.1145/1961258.1961282},
abstract = {Product requirements prioritization approaches identify the most valuable requirements according to customer value, requirements risk, volatility, cost or other market parameters. However, it is still a challenge in Value-Based Engineering to manage the requirement values for incrementing the product value. The analysis of the product value requires a better understanding of interdependencies among various artifacts of the software development lifecycle, particularly, requires to get a better understanding of how the value chain is preserved from the problem space to the solution space. In this position paper our aim is to study the feasibility of using traceability as the backbone to preserve the value chain between the prioritized product requirements and the product architecture. We propose traceability semantics to address this aim. Some initial findings from our research are presented.},
booktitle = {Proceedings of the 11th International Conference on Product Focused Software},
pages = {102–104},
numpages = {3},
keywords = {product architecture, product features, product value, product value chain, traceability semantics},
location = {Limerick, Ireland},
series = {PROFES '10}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00014,
author = {Idowu, Samuel and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Asset management in machine learning: a survey},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00014},
doi = {10.1109/ICSE-SEIP52600.2021.00014},
abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {51–60},
numpages = {10},
keywords = {SE4AI, asset management, machine learning},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@article{10.1016/j.specom.2019.10.003,
author = {Stasak, Brian and Epps, Julien and Goecke, Roland},
title = {Automatic depression classification based on affective read sentences: Opportunities for text-dependent analysis},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2019.10.003},
doi = {10.1016/j.specom.2019.10.003},
journal = {Speech Commun.},
month = dec,
pages = {1–14},
numpages = {14},
keywords = {Digital phenotyping, Digital medicine, Paralinguistics, Machine learning, Speech elicitation, Valence}
}

@article{10.1016/j.dss.2012.10.005,
author = {Van Valkenhoef, Gert and Tervonen, Tommi and Zwinkels, Tijs and De Brock, Bert and Hillege, Hans},
title = {ADDIS: A decision support system for evidence-based medicine},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {2},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2012.10.005},
doi = {10.1016/j.dss.2012.10.005},
abstract = {Clinical trials are the main source of information for the efficacy and safety evaluation of medical treatments. Although they are of pivotal importance in evidence-based medicine, there is a lack of usable information systems providing data-analysis and decision support capabilities for aggregate clinical trial results. This is partly caused by unavailability (i) of trial data in a structured format suitable for re-analysis, and (ii) of a complete data model for aggregate level results. In this paper, we develop a unifying data model that enables the development of evidence-based decision support in the absence of a complete data model. We describe the supported decision processes and show how these are implemented in the open source ADDIS software. ADDIS enables semi-automated construction of meta-analyses, network meta-analyses and benefit-risk decision models, and provides visualization of all results.},
journal = {Decis. Support Syst.},
month = may,
pages = {459–475},
numpages = {17},
keywords = {ADE, ADR, ADaM, AMIA, ANSI, ATC, BRIDG, CDASH, CDISC, CDMS, CHMP, CPOE, CRF, CRO, CTIS, CTMS, Clinical trial, DB, DED, DIS, DOI, DSS, Data model, Decision analysis, EAV, EBM, EDC, EHR, EMA, EPAR, Evidence synthesis, Evidence-based medicine, FDA, FDAAA, GCP, GUI, HL7, HSDB, ICD, ICMJE, ICTRP, JAMA, LAB, MCDA, MeSH, MedDRA, NCI, NDA, NIHUS, OBX, OCRe, ODM, OWL, PIM, PMDA, PRM, PhRMA, QRD, RIM, SDTM, SEND, SMAA, SNOMEDCT, SPL, SmPC, TDM, UMLS, WHO, caBIG, eCRF, eLab, ePRO}
}

@article{10.1016/j.csi.2013.07.006,
author = {Stallinger, Fritz and Neumann, Robert},
title = {Enhancing ISO/IEC 15288 with reuse and product management: An add-on process reference model},
year = {2013},
issue_date = {November, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {36},
number = {1},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2013.07.006},
doi = {10.1016/j.csi.2013.07.006},
abstract = {To support the transformation of system engineering from the project-based development of highly customer-specific solutions to the reuse and customization of 'system products', we integrate a process reference model for reuse- and product-oriented industrial engineering and a process reference model extending ISO/IEC 12207 on software life cycle processes with software- and system-level product management. We synthesize the key process elements of both models to enhance ISO/IEC 15288 on system life cycle processes with product- and reuse-oriented engineering and product management practices as an integrated framework for process assessment and improvement in contexts where systems are developed and evolved as products.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {21–32},
numpages = {12},
keywords = {ISO/IEC 15288, Process reference model, Product management, Reuse, Systems engineering}
}

@article{10.1145/2445560.2445564,
author = {Ullah, Azmat and Lai, Richard},
title = {A Systematic Review of Business and Information Technology Alignment},
year = {2013},
issue_date = {April 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2445560.2445564},
doi = {10.1145/2445560.2445564},
abstract = {Business organizations have become heavily dependent on information technology (IT) services. The process of alignment is defined as the mutual synchronization of business goals and IT services. However, achieving mature alignment between business and IT is difficult due to the rapid changes in the business and IT environments. This article provides a systematic review of studies on the alignment of business and IT. The research articles reviewed are based on topics of alignment, the definition of alignment, history, alignment challenges, phases of alignment, alignment measurement approaches, the importance of alignment in business industries, how software engineering helps in better alignment, and the role of the business environment in aligning business with IT. It aims to present a thorough understanding of business-IT alignment and to provide a list of future research directions regarding alignment. To perform the systematic review, we used the guidelines developed by Kitchenham for reviewing the available research papers relevant to our topic.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {4},
numpages = {30},
keywords = {IT alignment, IT issues, IT support, Systematic review, alignment measurement, alignment phases, business, business environment modeling, business issues, literature}
}

@article{10.1145/2993231.2993234,
author = {Vilela, Jessyka and Goncalves, Enyo and Holanda, Ana Carla and Castro, Jaelson and Figueiredo, Bruno},
title = {A retrospective analysis of SAC requirements: engineering track},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/2993231.2993234},
doi = {10.1145/2993231.2993234},
abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: In a previous paper, we investigated how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. In this paper, we extended the analysis including the papers of the last edition (2016) and a brief resume of all papers published in the nine editions of SAC-RE track. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 86 papers over the 9 previous SAC RETrack editions, which were analyzed and discussed.},
journal = {SIGAPP Appl. Comput. Rev.},
month = aug,
pages = {26–41},
numpages = {16},
keywords = {SAC, relevance, requirements engineering, retrospective, scoping study, symposium on applied computing, systematic mapping study, trends}
}

@inproceedings{10.1145/1944892.1944898,
author = {Cavalcanti, Yguarat\~{a} Cerqueira and do Carmo Machado, Ivan and da Mota, Paulo Anselmo and Neto, Silveira and Lobato, Luanna Lopes and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {Towards metamodel support for variability and traceability in software product lines},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944898},
doi = {10.1145/1944892.1944898},
abstract = {In Software Product Lines (SPL), where a greater variety of products are derived from a common platform and constantly changed and evolved, it is important to manage the SPL variability and the traceability among its artifacts. This paper presents a metamodel which aims to coordinate SPL activities, by managing different SPL phases and their responsibles, and to maintain the traceability and variability among different artifacts. The metamodel was built for a SPL project in a private company working in the medical information management domain, which includes four products encompassing 102 different modules and 840 features. The metamodel is divided into five sub-models: project and risk management, scoping, requirements and testing. It is represented in the UML notation. Organizations using this metamodel as basis for their approaches, can easily understand the relationships between the SPL assets, communicate to the stakeholders, and facilitate the evolution and maintenance of the SPL. The metamodel can also be adapted to the single system development context.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {49–57},
numpages = {9},
keywords = {design, metamodel, software engineering, software product lines, variability},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@article{10.1007/s10270-018-00704-x,
author = {Rodrigues, V\'{\i}tor and Donetti, Simone and Damiani, Ferruccio},
title = {Certifying delta-oriented programs},
year = {2019},
issue_date = {October   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-00704-x},
doi = {10.1007/s10270-018-00704-x},
abstract = {A major design concern in modern software development frameworks is to ensure that mechanisms for updating code running on remote devices comply with given safety specifications. This paper presents a delta-oriented approach for implementing product lines where software reuse is achieved at the three levels of state-diagram modeling, C/$$text {C}^{_{_{_{++}}}} $$C++source code and binary code. A safety specification is expressed on the properties of reusable software libraries that can be dynamically loaded at run time after an over-the-air update. The compilation of delta-engineered code is certified using the framework of proof-carrying code in order to guarantee safety of software updates on remote devices. An empirical evaluation of the computational cost associated with formal safety checks is done by means of experimentation.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {2875–2906},
numpages = {32},
keywords = {Delta-oriented programming, Model-driven development, Proof-carrying code, Runtime systems, Safety properties}
}

@article{10.1016/j.cad.2013.08.006,
author = {Davia, Miguel and Jimeno-Morenilla, Antonio and Salas, Faustino},
title = {Footwear bio-modelling: An industrial approach},
year = {2013},
issue_date = {December, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {45},
number = {12},
issn = {0010-4485},
url = {https://doi.org/10.1016/j.cad.2013.08.006},
doi = {10.1016/j.cad.2013.08.006},
abstract = {There is a growing need within the footwear sector to customise the design of the last from which a specific footwear style is to be produced. This customisation is necessary for user comfort and health reasons, as the user needs to wear a suitable shoe. For this purpose, a relationship must be established between the user foot and the last with which the style will be made; up until now, no model has existed that integrates both elements. On the one hand, traditional customised footwear manufacturing techniques are based on purely artisanal procedures which make the process arduous and complex; on the other hand, geometric models proposed by different authors present the impossibility of implementing them in an industrial environment with limited resources for the acquisition of morphometric and structural data for the foot, apart from the fact that they do not prove to be sufficiently accurate given the non-similarity of the foot and last. In this paper, two interrelated geometric models are defined, the first, a bio-deformable foot model and the second, a deformable last model. The experiments completed show the goodness of the model, with it obtaining satisfactory results in terms of comfort, efficiency and precision, which make it viable for use in the sector.},
journal = {Comput. Aided Des.},
month = dec,
pages = {1575–1590},
numpages = {16},
keywords = {Footwear production, Deformation techniques, Customised footwear}
}

@article{10.1504/ijwgs.2019.100837,
author = {Bani-Ismail, Basel and Baghdadi, Youcef},
title = {Migrating two legacy systems to SOA: a new approach for service selection based on data flow diagram},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {15},
number = {3},
issn = {1741-1106},
url = {https://doi.org/10.1504/ijwgs.2019.100837},
doi = {10.1504/ijwgs.2019.100837},
abstract = {There are many service identification methods (SIMs) to simplify service identification in SOA lifecycle. These SIMs vary in terms of their features (e.g., input artefact, technique). Due to this diversity, few evaluation frameworks have been proposed to guide organisations in selecting a suitable SIM based on their available input artefacts (e.g., source code, business process). This research concerns with SIMs that consider data flow diagram (DFD) as an input artefact, in order to migrate two legacy systems, modelled with DFD, to SOA. Only two SIMs are found in the literature to identify services based on DFD. However, these SIMs do not provide a way to select among the services identified to be implemented as web services. Therefore, this paper aims to bridge this gap by proposing a new approach for service selection based on DFD to assist organisations in speeding up the process of migrating their legacy systems to SOA.},
journal = {Int. J. Web Grid Serv.},
month = jan,
pages = {251–281},
numpages = {30},
keywords = {service-oriented architecture, SOA, service identification, service identification method, SIM, service selection, service quality, evaluation framework, data flow diagram, DFD}
}

@article{10.4018/IJWP.2011070104,
author = {Guelfi, Nicolas and Pruski, C\'{e}dric and Reynaud, Chantal},
title = {Adaptive Ontology-Based Web Information Retrieval: The TARGET Framework},
year = {2011},
issue_date = {July 2011},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {3},
issn = {1938-0194},
url = {https://doi.org/10.4018/IJWP.2011070104},
doi = {10.4018/IJWP.2011070104},
abstract = {Finding relevant information on the Web is difficult for most users. Although Web search applications are improving, they must be more "intelligent" to adapt to the search domains targeted by queries, the evolution of these domains, and users' characteristics. In this paper, the authors present the TARGET framework for Web Information Retrieval. The proposed approach relies on the use of ontologies of a particular nature, called adaptive ontologies, for representing both the search domain and a user's profile. Unlike existing approaches on ontologies, the authors make adaptive ontologies adapt semi-automatically to the evolution of the modeled domain. The ontologies and their properties are exploited for domain specific Web search purposes. The authors propose graph-based data structures for enriching Web data in semantics, as well as define an automatic query expansion technique to adapt a query to users' real needs. The enriched query is evaluated on the previously defined graph-based data structures representing a set of Web pages returned by a usual search engine in order to extract the most relevant information according to user needs. The overall TARGET framework is formalized using first-order logic and fully tool supported.},
journal = {Int. J. Web Portals},
month = jul,
pages = {41–58},
numpages = {18}
}

@inproceedings{10.1145/2833258.2833284,
author = {Bien, Ngo Huy and Thu, Tran Dan},
title = {Graphical User Interface Variability Architecture Pattern},
year = {2015},
isbn = {9781450338431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2833258.2833284},
doi = {10.1145/2833258.2833284},
abstract = {Designing software applications for multiple tenants is challenging. The task is even harder when designing pure multi-tenancy applications that must support different customers using a single codebase and data store. One of the most common problems when developing these systems is to support different graphical user interface not only for different users but also tenants. This critical requirement also applies to context-aware applications in which different graphical user interface should be presented to each user according to the user's context or software components and platforms that should allow developers easily to create different looks and feel for their applications. In this paper, we propose an architecture pattern for modeling graphical user interface that support different customizations and configurations. The modularity, the reusability and the maintainability of the pattern were evaluated by qualitative analysis based on well-known patterns used in the proposed architecture pattern. Real world systems were built to validate the applicability, the correctness, the security and the performance of the pattern. We believe that our pattern will be useful for software providers as well as normal organizations when building software components or software systems for different customers or creating multi-tenancy applications in a cloud-based environment or building context-aware applications.},
booktitle = {Proceedings of the 6th International Symposium on Information and Communication Technology},
pages = {304–311},
numpages = {8},
keywords = {Graphical user interface, PaaS, SaaS, architecture pattern, enterprise systems, multi-tenancy, software architecture, software variability, variability pattern},
location = {Hue City, Viet Nam},
series = {SoICT '15}
}

@inproceedings{10.1145/336512.336546,
author = {Lamsweerde, Axel van},
title = {Formal specification: a roadmap},
year = {2000},
isbn = {1581132530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336512.336546},
doi = {10.1145/336512.336546},
booktitle = {Proceedings of the Conference on The Future of Software Engineering},
pages = {147–159},
numpages = {13},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@inproceedings{10.5555/2819303.2819317,
author = {Bastos, Jonatas Ferreira and da Mota Silveira Neto, Paulo Anselmo and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {Software product lines adoption: an industrial case study (keynote)},
year = {2015},
publisher = {IEEE Press},
abstract = {The benefits of applying a software product lines (SPL) approach have encouraged small and medium-sized enterprises (SMEs) for its adoption. However, additional systematic empirical investigations are necessary in order to provide more information about the transfer and application of SPL adoption in SMEs. In this sense, this study aims to understanding on the SPL adoption in the context of small to medium-sized organization, by documenting empirical evidence observed during the transition from single-system to adopt a SPL approach. A case study was undertaken to analyze the important aspects, such as: adoption strategies, organizational structures, barriers and maturity level. It provides a better understanding on the adoption of SPL in SMEs considering different points of view and also provides new findings which sets outlines for future research.},
booktitle = {Proceedings of the Third International Workshop on Conducting Empirical Studies in Industry},
pages = {35–42},
numpages = {8},
location = {Florence, Italy},
series = {CESI '15}
}

@article{10.1016/j.infsof.2011.06.002,
author = {Breivold, Hongyu Pei and Crnkovic, Ivica and Larsson, Magnus},
title = {A systematic review of software architecture evolution research},
year = {2012},
issue_date = {January, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.06.002},
doi = {10.1016/j.infsof.2011.06.002},
abstract = {Context: Software evolvability describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for making strategic decisions, and increasing economic value of software. For long-lived systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. For this reason, many research studies have been proposed in this area both by researchers and industry practitioners. These studies comprise a spectrum of particular techniques and practices, covering various activities in software lifecycle. However, no systematic review has been conducted previously to provide an extensive overview of software architecture evolvability research. Objective: In this work, we present such a systematic review of architecting for software evolvability. The objective of this review is to obtain an overview of the existing approaches in analyzing and improving software evolvability at architectural level, and investigate impacts on research and practice. Method: The identification of the primary studies in this review was based on a pre-defined search strategy and a multi-step selection process. Results: Based on research topics in these studies, we have identified five main categories of themes: (i) techniques supporting quality consideration during software architecture design, (ii) architectural quality evaluation, (iii) economic valuation, (iv) architectural knowledge management, and (v) modeling techniques. A comprehensive overview of these categories and related studies is presented. Conclusion: The findings of this review also reveal suggestions for further research and practice, such as (i) it is necessary to establish a theoretical foundation for software evolution research due to the fact that the expertise in this area is still built on the basis of case studies instead of generalized knowledge; (ii) it is necessary to combine appropriate techniques to address the multifaceted perspectives of software evolvability due to the fact that each technique has its specific focus and context for which it is appropriate in the entire software lifecycle.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {16–40},
numpages = {25},
keywords = {Architecture analysis, Architecture evolution, Evolvability analysis, Software architecture, Software evolvability, Systematic review}
}

@article{10.1016/j.jss.2017.09.033,
author = {Badampudi, Deepika and Wnuk, Krzysztof and Wohlin, Claes and Franke, Ulrik and Smite, Darja and Cicchetti, Antonio},
title = {A decision-making process-line for selection of software asset origins and components},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.09.033},
doi = {10.1016/j.jss.2017.09.033},
abstract = {Presents a process-line for selecting software asset origins and components.Process-line helps decision-makers to build their decisions-making process.The process-line is evaluated through five case studies in three companies.The practitioners did not perceive any activity to be missing in the process-line.A sub-set of activities were followed by the companies without any specific order. Selecting sourcing options for software assets and components is an important process that helps companies to gain and keep their competitive advantage. The sourcing options include: in-house, COTS, open source and outsourcing. The objective of this paper is to further refine, extend and validate a solution presented in our previous work. The refinement includes a set of decision-making activities, which are described in the form of a process-line that can be used by decision-makers to build their specific decision-making process. We conducted five case studies in three companies to validate the coverage of the set of decision-making activities. The solution in our previous work was validated in two cases in the first two companies. In the validation, it was observed that no activity in the proposed set was perceived to be missing, although not all activities were conducted and the activities that were conducted were not executed in a specific order. Therefore, the refinement of the solution into a process-line approach increases the flexibility and hence it is better in capturing the differences in the decision-making processes observed in the case studies. The applicability of the process-line was then validated in three case studies in a third company.},
journal = {J. Syst. Softw.},
month = jan,
pages = {88–104},
numpages = {17},
keywords = {Case study, Component-based software engineering, Decision-making}
}

@inproceedings{10.5555/1766311.1766410,
author = {Bekiaris, Evangelos and Panou, Maria and Mousadakou, Adriani},
title = {Elderly and disabled travelers needs in infomobility services},
year = {2007},
isbn = {9783540732785},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Within ASK-IT project, an extensive survey of the needs of elderly and disabled travelers using infomobility services has been performed. More specifically, 39 past and on-going research projects have been reviewed, having a common aim to ASK-IT. The user needs that have emerged after the testing of the developed systems are highlighted in this document for the transport and tourism areas. Results do not refer only to the visual and acoustical HMI of systems and services for information provision while traveling, but also to the content and the design aspects of the HMI, in order to satisfy accessibility.},
booktitle = {Proceedings of the 4th International Conference on Universal Access in Human Computer Interaction: Coping with Diversity},
pages = {853–860},
numpages = {8},
keywords = {HMI, accessibility, disabled, elderly, infomobility, needs},
location = {Beijing, China},
series = {UAHCI'07}
}

@inproceedings{10.1007/978-3-319-35122-3_16,
author = {Braga, Rosana T. and Feloni, Daniel and Pacini, Karen and Filho, Domenico Schettini and Gottardi, Thiago},
title = {AIRES: An Architecture to Improve Software Reuse},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_16},
doi = {10.1007/978-3-319-35122-3_16},
abstract = {Among the several challenges still faced by Software Engineering, software reuse can be listed as a potential solution towards improving productivity and quality, through the utilization of previously produced artifacts that can leverage development activities. Among these artifacts we can mention not only code, but also requirements' documents, analysis and design models, test cases, documentation, and even development processes that achieved success in the past and could be reused again and again. However, the diversity of methods, processes and tools for software engineering make it difficult to turn reuse into a systematic activity. Considering this context, the present paper aims at presenting an architectural model that encompasses the main elements needed to support software reuse in a large scale. This model, named AIRES, allows reuse to be realized intrinsically to the development process life cycle, providing mechanisms to facilitate a variety of processes and artifacts representation and a Service-Oriented Architecture SOA to make assets available to other software engineering environments or tools. The AIRES model is being implemented using open source platforms and will be available within the cloud.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {231–246},
numpages = {16},
keywords = {Reuse environments, Reuse tools, Software reuse},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/3053600.3053619,
author = {Mangels, Tatiana and Murarasu, Alin and Oden, Forest and Fishkin, Alexey and Becker, Daniel},
title = {Efficient Analysis at Edge},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053619},
doi = {10.1145/3053600.3053619},
abstract = {Digitalization changes traditional business models by using digital technologies to improve existing offerings and to create new offerings. Current technological trends such as artificial intelligence, autonomous systems, and predictive maintenance are ideal candidate technologies to enable digitalization use cases. Often, these technologies rely on the availability of large amounts of data and the capability to process these data efficiently. In contrast to consumer markets, industrial products must fulfill higher non-functional requirements such as fast response times, 24/7 availability and stability, real-time processing, safety, or security requirements. As a consequence, processing capabilities -- ranging from multicore and manycores to even high end parallel clusters -- have to be exploited to achieve necessary performance and stability needs. In this paper, we introduce a Distributed Multicore Monitoring Framework (MoMo) which is a reference monitoring solution developed at Siemens Corporate Technology. It can be used to easily build efficient and stable diagnostic solutions which can help to understand the correctness, availability, reliability, and performance of large-scale distributed systems based on live data. Due to its small footprint MoMo can be used to analyze data directly at the data source which, for instance, can significantly reduce the network load. While MoMo's efficiency comes from the usage of multicore processors (CPUs) for running analysis in parallel, its usability is guaranteed by its capability to easily integrate with other monitoring frameworks and its usage of SPL - a domain-specific language which allows user to easily define diagnostic algorithms.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {85–90},
numpages = {6},
keywords = {data analysis, monitoring, parallel computing},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@article{10.1007/s10664-020-09932-6,
author = {Bagheri, Hamid and Wang, Jianghao and Aerts, Jarod and Ghorbani, Negar and Malek, Sam},
title = {Flair: efficient analysis of Android inter-component vulnerabilities in response to incremental changes},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09932-6},
doi = {10.1007/s10664-020-09932-6},
abstract = {Inter-component communication (ICC) among Android apps is shown to be the source of many security vulnerabilities. Prior research has developed compositional analyses to detect the existence of ICC vulnerabilities in a set of installed apps. However, they all lack the ability to efficiently respond to incremental system changes—such as adding/removing apps. Every time the system changes, the entire analysis has to be repeated, making them too expensive for practical use, given the frequency with which apps are updated, installed, and removed on a typical Android device. This paper presents a novel technique, dubbed FLAIR, for efficient, yet formally precise, security analysis of Android apps in response to incremental system changes. Leveraging the fact that the changes are likely to impact only a small fraction of the prior analysis results, FLAIR recomputes the analysis only where required, thereby greatly improving analysis performance without sacrificing the soundness and completeness thereof. Our experimental results using numerous collections of real-world apps corroborate that FLAIR can provide an order of magnitude speedup over prior techniques.},
journal = {Empirical Softw. Engg.},
month = may,
numpages = {37},
keywords = {Android analysis, Evolving software, Relational logic}
}

@inproceedings{10.1145/3194133.3194136,
author = {da Silva, Jo\~{a}o Pablo S. and Ecar, Miguel and Pimenta, Marcelo S. and Guedes, Gilleanes T. A. and Franz, Luiz Paulo and Marchezan, Luciano},
title = {A systematic literature review of UML-based domain-specific modeling languages for self-adaptive systems},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194136},
doi = {10.1145/3194133.3194136},
abstract = {Self-adaptive Systems (SaSs) operate under uncertainty conditions and have intrinsic properties that make their modeling a non-trivial activity. This complexity can be minimized by using Domain-Specific Modeling Languages (DSMLs), which may be created by extending Unified Modeling Language (UML). In face of this, we propose investigating how the UML has been customized to create DSMLs that provide proper support for SaSs modeling. To achieve this, we performed a Systematic Literature Review (SRL) by retrieving studies with snowballing technique, selecting studies according to inclusion and exclusion criteria, and extracting and analyzing data to answer our research questions. As the outcome, we retrieved 786 studies and selected 16 primary studies published between 2005 and 2017. The results reveal that the class diagram has been customized through the profile-based mechanism to provide proper support to analysis and design of context-awareness and self-adaptiveness properties.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {87–93},
numpages = {7},
keywords = {domain-specific modeling language (DSML), self-adaptive systems (SaS), snowballing technique, systematic literature review (SLR), unified modeling language (UML)},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@inproceedings{10.5555/2819009.2819038,
author = {Schroeder, Jan and Holzner, Daniela and Berger, Christian and Hoel, Carl-Johan and Laine, Leo and Magnusson, Anders},
title = {Design and evaluation of a customizable multi-domain reference architecture on top of product lines of self-driving heavy vehicles: an industrial case study},
year = {2015},
publisher = {IEEE Press},
abstract = {Self-driving vehicles for commercial use cases like logistics or overcast mines increase their owners' economic competitiveness. Volvo maintains, evolves, and distributes a vehicle control product line for different brands like Volvo Trucks, Renault, and Mack in more than 190 markets world-wide. From the different application domains of their customers originates the need for a multi-domain reference architecture concerned with transport mission planning, execution, and tracking on top of the vehicle control product line. This industrial case study is the first of its kind reporting about the systematic process to design such a reference architecture involving all relevant external and internal stakeholders, development documents, low-level artifacts, and literature. Quantitative and qualitative metrics were applied to evaluate non-functional requirements on the reference architecture level before a concrete variant was evaluated using a Volvo FMX truck in an exemplary construction site setting.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {189–198},
numpages = {10},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.5555/2350776.2350779,
author = {Preuveneers, Davy and Novais, Paulo},
title = {A survey of software engineering best practices for the development of smart applications in Ambient Intelligence},
year = {2012},
issue_date = {August 2012},
publisher = {IOS Press},
address = {NLD},
volume = {4},
number = {3},
issn = {1876-1364},
abstract = {Over the past decade, the world of Ambient Intelligence and smart environments has brought us a wide variety of novel applications with potential for exhibiting sophisticated intelligent behavior. These applications are called smart because they can sense, anticipate and adapt themselves to the context, desires and intentions of their users. However, the unpredictability of human behavior, the unanticipated circumstances of execution and a growing heterogeneity of future operational environments impose significant development challenges if these innovative applications are to gain wide acceptance in our daily life. Therefore, applying well-established software engineering methodologies is a fundamental prerequisite for delivering high quality and robust smart software applications.This survey provides a illustrative presentation of software engineering best practices in the area of smart environments and Ambient Intelligence. The aim of this survey is not to explore and compare every possible related work in detail, but to offer insights into the latest developments in this domain and to further the research into the successful design, development and evaluation of Ambient Intelligence frameworks and applications.(This research is partially funded by the Interuniversity Attraction Poles Programme Belgian State, Belgian Science Policy, and by the Research Fund KU Leuven.)},
journal = {J. Ambient Intell. Smart Environ.},
month = aug,
pages = {149–162},
numpages = {14},
keywords = {Ambient Intelligence, Software engineering, smart applications, survey}
}

@inproceedings{10.1145/2593882.2593886,
author = {Garlan, David},
title = {Software architecture: a travelogue},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593886},
doi = {10.1145/2593882.2593886},
abstract = {Over the past two and a half decades software architecture has emerged as an important subfield of software engineering. During that time there has been considerable progress in developing the technological and methodological base for treating architectural design as an engineering discipline. However, much still remains to be done to achieve that. Moreover, the changing face of technology raises a number of challenges for software architecture. This travelogue recounts the history of the field, its current state of practice and research, and speculates on some of the important emerging trends, challenges, and aspirations.},
booktitle = {Future of Software Engineering Proceedings},
pages = {29–39},
numpages = {11},
keywords = {Software architecture, architecture and agility, architecture description languages, architecture styles, architecture trends, software frame-works, software product lines},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/3362789.3362837,
author = {V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco J. and Ther\'{o}n, Roberto},
title = {Capturing high-level requirements of information dashboards' components through meta-modeling},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362837},
doi = {10.1145/3362789.3362837},
abstract = {Information dashboards are increasing their sophistication to match new necessities and adapt to the high quantities of generated data nowadays. These tools support visual analysis, knowledge generation, and thus, are crucial systems to assist decision-making processes. However, the design and development processes are complex, because several perspectives and components can be involved. Tailoring capabilities are focused on providing individualized dashboards without affecting the time-to-market through the decrease of the development processes' time. Among the methods used to configure these tools, the software product lines paradigm and model-driven development can be found. These paradigms benefit from the study of the target domain and the abstraction of features, obtaining high-level models that can be instantiated into concrete models. This paper presents a dashboard meta-model that aims to be applicable to any dashboard. Through domain engineering, different features of these tools are identified and arranged into abstract structures and relationships to gain a better understanding of the domain. The goal of the meta-model is to obtain a framework for instantiating any dashboard to adapt them to different contexts and user profiles. One of the contexts in which dashboards are gaining relevance is Learning Analytics, as learning dashboards are powerful tools for assisting teachers and students in their learning activities. To illustrate the instantiation process of the presented meta-model, a small example within this relevant context (Learning Analytics) is also provided.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {815–821},
numpages = {7},
keywords = {Domain engineering, High-level requirements, Information Dashboards, Meta-modeling},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@inproceedings{10.1145/2610384.2610411,
author = {Galindo, Jos\'{e} A. and Alf\'{e}rez, Mauricio and Acher, Mathieu and Baudry, Benoit and Benavides, David},
title = {A variability-based testing approach for synthesizing video sequences},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610411},
doi = {10.1145/2610384.2610411},
abstract = {A key problem when developing video processing software is the difficulty to test different input combinations. In this paper, we present VANE, a variability-based testing approach to derive video sequence variants. The ideas of VANE are i) to encode in a variability model what can vary within a video sequence; ii) to exploit the variability model to generate testable configurations; iii) to synthesize variants of video sequences corresponding to configurations. VANE computes T-wise covering sets while optimizing a function over attributes. Also, we present a preliminary validation of the scalability and practicality of VANE in the context of an industrial project involving the test of video processing algorithms.},
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {293–303},
numpages = {11},
keywords = {Combinatorial testing, Variability, Video analysis},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1007/978-3-030-64330-0_9,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Rosado, David G. and S\'{a}nchez, Luis Enrique and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Gasca, Rafael M. and Fern\'{a}ndez-Medina, Eduardo},
title = {Definition and Verification of Security Configurations of Cyber-Physical Systems},
year = {2020},
isbn = {978-3-030-64329-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64330-0_9},
doi = {10.1007/978-3-030-64330-0_9},
abstract = {The proliferation of Cyber-Physical Systems (CPSs) is raising serious security challenges. These are complex systems, integrating physical elements into automated networked systems, often containing a variety of devices, such as sensors and actuators, and requiring complex management and data storage. This makes the construction of secure CPSs a challenge, requiring not only an adequate specification of security requirements and needs related to the business domain but also an adaptation and concretion of these requirements to define a security configuration of the CPS where all its components are related. Derived from the complexity of the CPS, their configurations can be incorrect according to the requirements, and must be verified. In this paper, we propose a grammar for specifying business domain security requirements based on the CPS components. This will allow the definition of security requirements that, through a defined security feature model, will result in a configuration of services and security properties of the CPS, whose correctness can be verified. For this last stage, we have created a catalogue of feature models supported by a tool that allows the automatic verification of security configurations. To illustrate the results, the proposal has been applied to automated verification of requirements in a hydroponic system scenario.},
booktitle = {Computer Security: ESORICS 2020 International Workshops, CyberICPS, SECPRE, and ADIoT, Guildford, UK, September 14–18, 2020, Revised Selected Papers},
pages = {135–155},
numpages = {21},
keywords = {Cyber-physical system, CPS, Security, Requirement, Feature model, Configuration, Verification}
}

@inproceedings{10.1007/978-3-030-27455-9_4,
author = {Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Farah, Paulo Roberto and Vergilio, Silvia Regina and Guizzo, Giovani},
title = {A Review of Ten Years of the Symposium on Search-Based Software Engineering},
year = {2019},
isbn = {978-3-030-27454-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27455-9_4},
doi = {10.1007/978-3-030-27455-9_4},
abstract = {The year 2018 marked the tenth anniversary of the Symposium on Search Based Software Engineering (SSBSE). In order to better understand the characteristics and evolution of papers published in SSBSE, this work reports results from a mapping study targeting the ten proceedings of SSBSE. Our goal is to identify and to analyze authorship collaborations, the impact and relevance of SSBSE in terms of citations, the software engineering areas commonly studied as well as the new problems recently solved, the computational intelligence techniques preferred by authors and the rigour of experiments conducted in the papers. Besides this analysis, we list some recommendations to new authors who envisage to publish their work in SSBSE. Despite of existing mapping studies on SBSE, our contribution in this work is to provide information to researchers and practitioners willing to enter the SBSE field, being a source of information to strengthen the symposium, guide new studies, and motivate new collaboration among research groups.},
booktitle = {Search-Based Software Engineering: 11th International Symposium, SSBSE 2019, Tallinn, Estonia, August 31 – September 1, 2019, Proceedings},
pages = {42–57},
numpages = {16},
keywords = {Systematic mapping, SBSE, Bibliometric analysis},
location = {Tallinn, Estonia}
}

@inproceedings{10.5555/2486788.2487064,
author = {Cooper, Kendra M. L. and Scacchi, Walt and Wang, Alf Inge},
title = {3rd international workshop on games and software engineering: engineering computer games to enable positive, progressive change (GAS 2013)},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {We present a summary of the 3rd ICSE Workshop on Games and Software Engineering: Engineering Computer Games to Enable Positive, Progressive Change in this article. The full day workshop is planned to include a keynote speaker, panel discussion, and paper presentations on game software engineering topics related to requirements specification and verification, software engineering education, re-use, and infrastructure. An overview of the accepted papers is included in this summary.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1521–1522},
numpages = {2},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1007/s00607-013-0338-9,
author = {Bertolino, Antonia and Inverardi, Paola and Muccini, Henry},
title = {Software architecture-based analysis and testing: a look into achievements and future challenges},
year = {2013},
issue_date = {August    2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {95},
number = {8},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-013-0338-9},
doi = {10.1007/s00607-013-0338-9},
journal = {Computing},
month = aug,
pages = {633–648},
numpages = {16}
}

@article{10.1016/j.jnca.2015.07.007,
author = {Yongsiriwit, Karn and Assy, Nour and Gaaloul, Walid},
title = {A semantic framework for configurable business process as a service in the cloud},
year = {2016},
issue_date = {January 2016},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {59},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2015.07.007},
doi = {10.1016/j.jnca.2015.07.007},
abstract = {With the advent of Cloud Computing, new opportunities for Business Process Outsourcing services have emerged. Business Process as a Service (BPaaS), a new cloud service model, has recently gained a great importance for outsourcing cloud-based business processes constructed for multi-tenancy. In such a multi-tenant environment, using configurable business process models enables the sharing of a reference process among different tenants that can be customized according to specific needs. With a large choice of configurable process modeling languages, different providers may deliver configurable processes with common functionalities but different representations which makes the process discovery and configuration a manual tedious task. This in turn creates cloud silos and vendors lock-in with non-reusable configurable BPaaS models. Therefore, with the aim of enabling the interoperability between multiple BPaaS providers, we propose in this paper a semantic framework for BPaaS configurable models. Taking advantage of Semantic Web technologies and data mining techniques, our framework allows for (1) an ontology-based high level abstract representation of BPaaS configurable models enriched with configuration guidelines and (2) an automated approach for extracting the configuration guidelines from existing process repositories. To show the feasibility and effectiveness of our approach, we extend Signavio with our semantic framework and conduct experiments on a dataset from SAP reference model.},
journal = {J. Netw. Comput. Appl.},
month = jan,
pages = {168–184},
numpages = {17},
keywords = {BPaaS, Business Process as a Service, Cloud Computing, Configurable process model, Green IT, Semantic technology}
}

@article{10.1016/j.jss.2012.09.033,
author = {Hurtado, Julio Ariel and Bastarrica, Mar\'{\i}A Cecilia and Ochoa, Sergio F. and Simmonds, Jocelyn},
title = {MDE software process lines in small companies},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.09.033},
doi = {10.1016/j.jss.2012.09.033},
abstract = {Software organizations specify their software processes so that process knowledge can be systematically reused across projects. However, different projects may require different processes. Defining a separate process for each potential project context is expensive and error-prone, since these processes must simultaneously evolve in a consistent manner. Moreover, an organization cannot envision all possible project contexts in advance because several variables may be involved, and these may also be combined in different ways. This problem is even worse in small companies since they usually cannot afford to define more than one process. Software process lines are a specific type of software product lines, in the software process domain. A benefit of software process lines is that they allow software process customization with respect to a context. In this article we propose a model-driven approach for software process lines specification and configuration. The article also presents two industrial case studies carried out at two small Chilean software development companies. Both companies have benefited from applying our approach to their processes: new projects are now developed using custom processes, process knowledge is systematically reused, and the total time required to customize a process is much shorter than before.},
journal = {J. Syst. Softw.},
month = may,
pages = {1153–1171},
numpages = {19},
keywords = {Model-driven engineering, Process asset reuse, Software process lines}
}

@inproceedings{10.1145/3451471.3451478,
author = {Gunawan, Fandi and K. Budiardjo, Eko},
title = {&nbsp;A Quest of Software Process Improvements in DevOps and Kanban: A Case Study in Small Software Company},
year = {2021},
isbn = {9781450388955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3451471.3451478},
doi = {10.1145/3451471.3451478},
abstract = {A good software process improves software products. In the case of a small software company, software development is a matter of survivability due to its limited resources to develop software. XYZ Company is a very small software company that adopted Kanban and DevOps and faced software delivery delays. It is necessary to recommend the software process improvements to solve this problem. Software process improvements are the outcomes of measurement and analysis of maturity levels using the ISO 29110 framework in a qualitative study. They are then analyzed using the Lean Six Sigma tools, namely gap analysis, root cause analysis, and Pareto analysis. Delphi method validated them and resulted in 18 improvement recommendations within four domains, namely (a) product, (b) people, (c) technology, and (d) process. The improvements span across two main processes within software development, namely (a) Project Management (PM) and (b) Software Implementation (SI). The XYZ Company or any agile-based software company could adopt the 18 improvement recommendations to enhance the software process and quality.},
booktitle = {Proceedings of the 2021 4th International Conference on Software Engineering and Information Management},
pages = {39–45},
numpages = {7},
keywords = {DevOps, ISO 29110, Kanban, SPI, Software Process Improvement, agile, small software company},
location = {Yokohama, Japan},
series = {ICSIM '21}
}

@article{10.5555/2747013.2747140,
author = {Chen, Bihuan and Peng, Xin and Yu, Yijun and Zhao, Wenyun},
title = {Uncertainty handling in goal-driven self-optimization - Limiting the negative effect on adaptation},
year = {2014},
issue_date = {April 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {90},
number = {C},
issn = {0164-1212},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose techniques to handle contribution uncertainty and effect uncertainty in goal-driven self-optimization.We integrate these uncertainty handling techniques with preference uncertainty handling to a goal-driven self-optimization framework.We demonstrate the effectiveness of our approach with an evaluation on an online shopping system. Goal-driven self-optimization through feedback loops has shown effectiveness in reducing oscillating utilities due to a large number of uncertain factors in the runtime environments. However, such self-optimization is less satisfactory when there contains uncertainty in the predefined requirements goal models, such as imprecise contributions and unknown quality preferences, or during the switches of goal solutions, such as lack of understanding about the time for the adaptation actions to take effect. In this paper, we propose to handle such uncertainty in goal-driven self-optimization without interrupting the services. Taking the monitored quality values as the feedback, and the estimated earned value as the global indicator of self-optimization, our approach dynamically updates the quantitative contributions from alternative functionalities to quality requirements, tunes the preferences of relevant quality requirements, and determines a proper timing delay for the last adaptation action to take effect. After applying these runtime measures to limit the negative effect of the uncertainty in goal models and their suggested switches, an experimental study on a real-life online shopping system shows the improvements over goal-driven self-optimization approaches without uncertainty handling.},
journal = {J. Syst. Softw.},
month = apr,
pages = {114–127},
numpages = {14},
keywords = {Goal-driven self-optimization, Requirements goal models, Uncertainty}
}

@article{10.1016/j.infsof.2016.11.009,
author = {Mariani, Thain\'{a} and Vergilio, Silvia Regina},
title = {A systematic review on search-based refactoring},
year = {2017},
issue_date = {March 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {83},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.11.009},
doi = {10.1016/j.infsof.2016.11.009},
abstract = {Context: To find the best sequence of refactorings to be applied in a software artifact is an optimization problem that can be solved using search techniques, in the field called Search-Based Refactoring (SBR). Over the last years, the field has gained importance, and many SBR approaches have appeared, arousing research interest.Objective: The objective of this paper is to provide an overview of existing SBR approaches, by presenting their common characteristics, and to identify trends and research opportunities.Method: A systematic review was conducted following a plan that includes the definition of research questions, selection criteria, a search string, and selection of search engines. 71 primary studies were selected, published in the last sixteen years. They were classified considering dimensions related to the main SBR elements, such as addressed artifacts, encoding, search technique, used metrics, available tools, and conducted evaluation.Results: Some results show that code is the most addressed artifact, and evolutionary algorithms are the most employed search technique. Furthermore, most times, the generated solution is a sequence of refactorings. In this respect, the refactorings considered are usually the ones of the Fowler's Catalog. Some trends and opportunities for future research include the use of models as artifacts, the use of many objectives, the study of the bad smells effect, and the use of hyper-heuristics.Conclusions: We have found many SBR approaches, most of them published recently. The approaches are presented, analyzed, and grouped following a classification scheme. The paper contributes to the SBR field as we identify a range of possibilities that serve as a basis to motivate future researches.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {14–34},
numpages = {21},
keywords = {Evolutionary algorithms, Refactoring, Search-based software engineering}
}

@article{10.1016/j.jss.2016.02.010,
author = {Alegre, Unai and Augusto, Juan Carlos and Clark, Tony},
title = {Engineering context-aware systems and applications},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.02.010},
doi = {10.1016/j.jss.2016.02.010},
abstract = {Survey the efforts of the community in order to encourage a Context-Aware Systems Engineering process.Analysis of the state-of-the-art engineering techniques applied in the most common development stages.A study of existing methodologies within these development stages.The main challenges remaining open in the Context-Aware Computing field. Context-awareness is an essential component of systems developed in areas like Intelligent Environments, Pervasive &amp; Ubiquitous Computing and Ambient Intelligence. In these emerging fields, there is a need for computerized systems to have a higher understanding of the situations in which to provide services or functionalities, to adapt accordingly. The literature shows that researchers modify existing engineering methods in order to better fit the needs of context-aware computing. These efforts are typically disconnected from each other and generally focus on solving specific development issues. We encourage the creation of a more holistic and unified engineering process that is tailored for the demands of these systems. For this purpose, we study the state-of-the-art in the development of context-aware systems, focusing on: (A) Methodologies for developing context-aware systems, analyzing the reasons behind their lack of adoption and features that the community wish they can use; (B) Context-aware system engineering challenges and techniques applied during the most common development stages; (C) Context-aware systems conceptualization.},
journal = {J. Syst. Softw.},
month = jul,
pages = {55–83},
numpages = {29},
keywords = {Ambient Intelligence, Context-Aware Systems Engineering, Context-aware computing, Context-awareness, Context-sensitive, Intelligent Environments, Pervasive &amp; Ubiquitous Computing, Sentient computing, Software engineering}
}

@inbook{10.1145/3191315.3191316,
author = {Kifer, Michael and Liu, Yanhong Annie},
title = {Preface},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191316},
abstract = {The idea of this book grew out of a symposium that was held at Stony Brook in September 2012 in celebration of David S. Warren's fundamental contributions to Computer Science and the area of Logic Programming in particular.Logic Programming (LP) is at the nexus of Knowledge Representation, Artificial Intelligence, Mathematical Logic, Databases, and Programming Languages. It is fascinating and intellectually stimulating due to the fundamental interplay among theory, systems, and applications brought about by logic. Logic programs are more declarative in the sense that they strive to be logical specifications of "what" to do rather than "how" to do it, and thus they are high-level and easier to understand and maintain. Yet, without being given an actual algorithm, LP systems implement the logical specifications automatically.Several books cover the basics of LP but focus mostly on the Prolog language with its incomplete control strategy and non-logical features. At the same time, there is generally a lack of accessible yet comprehensive collections of articles covering the key aspects in declarative LP. These aspects include, among others, well-founded vs. stable model semantics for negation, constraints, object-oriented LP, updates, probabilistic LP, and evaluation methods, including top-down vs. bottom-up, and tabling.For systems, the situation is even less satisfactory, lacking accessible literature that can help train the new crop of developers, practitioners, and researchers. There are a few guides on Warren's Abstract Machine (WAM), which underlies most implementations of Prolog, but very little exists on what is needed for constructing a state-of-the-art declarative LP inference engine. Contrast this with the literature on, say, Compilers, where one can first study a book on the general principles and algorithms and then dive in the particulars of a specific compiler. Such resources greatly facilitate the ability to start making meaningful contributions quickly. There is also a dearth of articles about systems that support truly declarative languages, especially those that tie into first-order logic, mathematical programming, and constraint solving.LP helps solve challenging problems in a wide range of application areas, but in-depth analysis of their connection with LP language abstractions and LP implementation methods is lacking. Also, rare are surveys of challenging application areas of LP, such as Bioinformatics, Natural Language Processing, Verification, and Planning.The goal of this book is to help fill in the previously mentioned void in the LP literature. It offers a number of overviews on key aspects of LP that are suitable for researchers and practitioners as well as graduate students. The following chapters in theory, systems, and applications of LP are included.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {xvii–xx}
}

@inproceedings{10.1145/985921.986030,
author = {Kallinen, Kari},
title = {The effects of background music on using a pocket computer in a cafeteria: immersion, emotional responses, and social richness of medium},
year = {2004},
isbn = {1581137036},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/985921.986030},
doi = {10.1145/985921.986030},
abstract = {The focus of the present paper was to examine the effects of background music on using a pocket computer (i.e., reading entertainment news and making notes) in a noisy cafeteria environment. Music listening, as compared to using PDA without listening to music, prompted higher overall user satisfaction and immersion in media use, less boredom and more pleasure, and higher perceived social richness of the medium in terms of personality, liveliness, and emotionality. It was also found that PDA user experience and personality (i.e., impulsive-sensation seeking [ImpSS]) moderated some of these responses. The results are of importance given that the modern technology make it possible (1) to use computers in various everyday environments (e.g., in cafeterias and on business trips), and (2) to adapt the information and/or interfaces to fit the individual characteristics of the user.},
booktitle = {CHI '04 Extended Abstracts on Human Factors in Computing Systems},
pages = {1227–1230},
numpages = {4},
keywords = {PDA, background music, emotion, immersion, pocket computer, reading, social richness of medium},
location = {Vienna, Austria},
series = {CHI EA '04}
}

@book{10.5555/2974976,
author = {Douglass, Bruce Powel},
title = {Agile Systems Engineering},
year = {2015},
isbn = {9780128023495},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Agile Systems Engineering presents a vision of systems engineering where precise specification of requirements, structure, and behavior meet larger concerns as such as safety, security, reliability, and performance in an agile engineering context. World-renown author and speaker Dr. Bruce Powel Douglass incorporates agile methods and model-based systems engineering (MBSE) to define the properties of entire systems while avoiding errors that can occur when using traditional textual specifications. Dr. Douglass covers the lifecycle of systems development, including requirements, analysis, design, and the handoff to specific engineering disciplines. Throughout, Dr. Douglass couples agile methods with SysML and MBSE to arm system engineers with the conceptual and methodological tools they need to avoid specification defects and improve system quality while simultaneously reducing the effort and cost of systems engineering. Identifies how the concepts and techniques of agile methods can be effectively applied in systems engineering context Shows how to perform model-based functional analysis and tie these analyses back to system requirements and stakeholder needs, and forward to system architecture and interface definition Provides a means by which the quality and correctness of systems engineering data can be assured (before the entire system is built!) Explains agile system architectural specification and allocation of functionality to system components Details how to transition engineering specification data to downstream engineers with no loss of fidelity Includes detailed examples from across industries taken through their stages, including the "Waldo" industrial exoskeleton as a complex system Table of Contents What is Model-Based Systems Engineering What are Agile Methods and Why Should I Care The importance of Agile methods Agile Stakeholder Requirements Engineering Agile Systems Requirements Definition and Analysis System Architectural Analysis and Trade Studies Agile Systems Architectural Design The Handoff to Downstream Engineering Appendix A. T-Wrecks Stakeholder Requirements Appendix B. T-Wrecks System Requirements}
}

@article{10.1007/s10270-016-0523-3,
author = {Liebel, Grischa and Marko, Nadja and Tichy, Matthias and Leitner, Andrea and Hansson, J\"{o}rgen},
title = {Model-based engineering in the embedded systems domain: an industrial survey on the state-of-practice},
year = {2018},
issue_date = {February  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0523-3},
doi = {10.1007/s10270-016-0523-3},
abstract = {Model-based engineering (MBE) aims at increasing the effectiveness of engineering by using models as important artifacts in the development process. While empirical studies on the use and the effects of MBE in industry exist, only few of them target the embedded systems domain. We contribute to the body of knowledge with an empirical study on the use and the assessment of MBE in that particular domain. The goal of this study is to assess the current state-of-practice and the challenges the embedded systems domain is facing due to shortcomings with MBE. We collected quantitative data from 113 subjects, mostly professionals working with MBE, using an online survey. The collected data spans different aspects of MBE, such as the used modeling languages, tools, notations, effects of MBE introduction, or shortcomings of MBE. Our main findings are that MBE is used by a majority of all participants in the embedded systems domain, mainly for simulation, code generation, and documentation. Reported positive effects of MBE are higher quality and improved reusability. Main shortcomings are interoperability difficulties between MBE tools, high training effort for developers and usability issues. Our study offers valuable insights into the current industrial practice and can guide future research in the fields of systems modeling and embedded systems.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {91–113},
numpages = {23},
keywords = {Embedded systems, Empirical study, Industry, Model-based engineering, Model-driven engineering, Modeling, State-of-practice}
}

@inproceedings{10.1109/WI.2006.173,
author = {Zhou, Jiehan and Niemela, Eila},
title = {Toward Semantic QoS Aware Web Services: Issues, Related Studies and Experience},
year = {2006},
isbn = {0769527477},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2006.173},
doi = {10.1109/WI.2006.173},
abstract = {Semantic QoSaware Web services incorporating the emerging Web services in the QoSaware system development are promoting ServiceOriented Software Engineering (SOSE). To identify the steps toward semantic quality of service (QoS)aware Web services, this paper examines previous studies related to semantic QoSaware Web services, including QoSaware Web service architectures, QoS classification, QoS ontology, QoS specification languages, and Web service creation tools. Moreover, a case study is presented to discuss the gaps between our current quality driven software development approach and the semantic QoSaware Web services.},
booktitle = {Proceedings of the 2006 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {553–557},
numpages = {5},
series = {WI '06}
}

@article{10.1016/j.jss.2016.06.068,
author = {Gholami, Mahdi Fahmideh and Daneshgar, Farhad and Low, Graham and Beydoun, Ghassan},
title = {Cloud migration process-A survey, evaluation framework, and open challenges},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.068},
doi = {10.1016/j.jss.2016.06.068},
abstract = {The relevant approaches for migrating legacy applications to the cloud are surveyed.An extensive analysis of existing approaches on the basis of a set of important criteria/features.Important cloud migration activities, techniques, and concerns that need to be properly addressed in a typical cloud migration process are delineated.Existing open issues and future research opportunities on the cloud migration research area are discussed. Moving mission-oriented enterprise software applications to cloud environments is a crucial IT task and requires a systematic approach. The foci of this paper is to provide a detailed review of extant cloud migration approaches from the perspective of the process model. To this aim, an evaluation framework is proposed and used to appraise and compare existing approaches for highlighting their features, similarities, and key differences. The survey distills the status quo and makes a rich inventory of important activities, recommendations, techniques, and concerns that are common in a typical cloud migration process in one place. This enables both academia and practitioners in the cloud computing community to get an overarching view of the process of the legacy application migration to the cloud. Furthermore, the survey identifies a number challenges that have not been yet addressed by existing approaches, developing opportunities for further research endeavours.},
journal = {J. Syst. Softw.},
month = oct,
pages = {31–69},
numpages = {39},
keywords = {Cloud computing, Cloud migration, Evaluation framework, Legacy application, Migration methodology, Process model}
}

@article{10.1007/s10664-020-09884-x,
author = {Fischer, Stefan and Michelon, Gabriela Karoline and Ramler, Rudolf and Linsbauer, Lukas and Egyed, Alexander},
title = {Automated test reuse for highly configurable software},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09884-x},
doi = {10.1007/s10664-020-09884-x},
abstract = {Dealing with highly configurable systems is generally very complex. Researchers and practitioners have conceived hundreds of different analysis techniques to deal with different aspects of configurable systems. One large focal point is the testing of configurable software. This is challenging due to the large number of possible configurations. Moreover, tests themselves are rarely configurable and instead built for specific configurations. However, existing tests need to be adapted to run on a different configuration. In this paper, we report on an experiment about automatically reusing existing tests in configurable systems. We used manually developed tests for specific configurations of three configurable systems and investigated how changing the configuration affects the tests. Subsequently, we employed an approach for automated reuse to generate new test variants (by reusing from existing ones) for combinations of previous configurations and compared their results to the ones from existing tests. Our results showed that we could directly reuse some tests for different configurations. Nonetheless, our automatically generated test variants generally yielded better results. Our generated tests had a higher or equal success rate to the existing tests in most cases. Even in the cases the success rate was equal, our generated tests generally had higher code coverage.},
journal = {Empirical Softw. Engg.},
month = nov,
pages = {5295–5332},
numpages = {38},
keywords = {Variability, Configurable software, Clone-and-own, Reuse, Testing}
}

@inproceedings{10.5555/2337223.2337513,
author = {Orso, Alessandro and Reussner, Ralf},
title = {Summary of the ICSE 2012 workshops},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {The workshops of ICSE 2012 provide a forum for researchers and practitioners to exchange and discuss scientific ideas before they have matured to warrant conference or journal publication. ICSE Workshops also serve as incubators for scientific communities that form and share a particular research agenda.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1651–1653},
numpages = {3},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@article{10.1016/j.future.2018.12.025,
author = {Cao, Yang and Lung, Chung-Horng and Ajila, Samuel A. and Li, Xiaolin},
title = {Support mechanisms for cloud configuration using XML filtering techniques: A case study in SaaS},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {95},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.12.025},
doi = {10.1016/j.future.2018.12.025},
journal = {Future Gener. Comput. Syst.},
month = jun,
pages = {52–67},
numpages = {16},
keywords = {Cloud Computing, Software-as-a-Service, Multi-Tenancy, Feature Modeling, XML Filtering, Yfilter}
}

@inproceedings{10.5555/2814058.3252435,
author = {Siqueira, Sean W. M. and Carvalho, Sergio T.},
title = {Session details: Main Track - Software Requirements, Architecture and Design for Information Systems},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
location = {Goiania, Goias, Brazil},
series = {SBSI '15}
}

@article{10.1145/1988997.1989008,
author = {Rahmani, M.},
title = {Software modeling &amp; design: UML, use cases, patterns, &amp; software architectures by Hassan Gomaa},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1988997.1989008},
doi = {10.1145/1988997.1989008},
journal = {SIGSOFT Softw. Eng. Notes},
month = aug,
pages = {35},
numpages = {1}
}

@article{10.1007/s11219-019-09489-8,
author = {Al\'{e}groth, Emil and Gorschek, Tony and Petersen, Kai and Mattsson, Michael},
title = {Characteristics that affect preference of decision models for asset selection: an industrial questionnaire survey},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09489-8},
doi = {10.1007/s11219-019-09489-8},
abstract = {Modern software development relies on a combination of development and re-use of technical asset, e.g., software components, libraries, and APIs. In the past, re-use was mostly conducted with internal assets but today external; open source, customer off-the-shelf (COTS), and assets developed through outsourcing are also common. This access to more asset alternatives presents new challenges regarding what assets to optimally chose and how to make this decision. To support decision-makers, decision theory has been used to develop decision models for asset selection. However, very little industrial data has been presented in literature about the usefulness, or even perceived usefulness, of these models. Additionally, only limited information has been presented about what model characteristics determine practitioner preference toward one model over another. The objective of this work is to evaluate what characteristics of decision models for asset selection determine industrial practitioner preference of a model when given the choice of a decision model of high precision or a model with high speed. An industrial questionnaire survey is performed where a total of 33 practitioners, of varying roles, from 18 companies are tasked to compare two decision models for asset selection. Textual analysis and formal and descriptive statistics are then applied on the survey responses to answer the study’s research questions. The study shows that the practitioners had clear preference toward the decision model that emphasized speed over the one that emphasized decision precision. This conclusion was determined to be because one of the models was perceived faster, had lower complexity, was more flexible in use for different decisions, and was more agile on how it could be used in operation, its emphasis on people, its emphasis on “good enough” precision and ability to fail fast if a decision was a failure. Hence, we found seven characteristics that the practitioners considered important for their acceptance of the model. Industrial practitioner preference, which relates to acceptance, of decision models for asset selection is dependent on multiple characteristics that must be considered when developing a model for different types of decisions such as operational day-to-day decisions as well as more critical tactical or strategic decisions. The main contribution of this work are the seven identified characteristics that can serve as industrial requirements for future research on decision models for asset selection.},
journal = {Software Quality Journal},
month = dec,
pages = {1675–1707},
numpages = {33},
keywords = {Decision models, Characteristics, Industrial study, Survey, Model comparison}
}

@article{10.1007/s10257-014-0251-6,
author = {Lee, Chien-Hsiang and Hwang, San-Yih and Yen, I-Ling and Yu, Tao-Kang},
title = {A service pattern model for service composition with flexible functionality},
year = {2015},
issue_date = {May       2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {2},
issn = {1617-9846},
url = {https://doi.org/10.1007/s10257-014-0251-6},
doi = {10.1007/s10257-014-0251-6},
abstract = {A key feature with service-oriented-architecture is to allow flexible composition of services into a business process. Although previous works related to service composition have paved the way for automatic composition, the techniques have limited applicability when it comes to composing complex workflows based on functional requirements, partly due to the large search space of the available services. In this paper, we propose a novel concept, the prospect service. Unlike existing abstract services which possess fixed service interfaces, a prospect service has a flexible interface to allow functional flexibility. Furthermore, we define a meta-model to specify service patterns with prospect services and adaptable workflow constructs to model flexible and adaptable process templates. An automated instantiation method is introduced to instantiate concrete processes with different functionalities from a service pattern. Since the search space for automatically instantiating a process from a service pattern is greatly reduced compared to that for automatically composing a process from scratch, the proposed approach significantly improve the feasibility of automated composition. Empirical study of the service pattern shows that the use of the proposed model significantly outperforms manual composition in terms of composition time and accuracy, and simulation results demonstrate that the proposed automated instantiation method is efficient.},
journal = {Inf. Syst. E-Bus. Manag.},
month = may,
pages = {235–265},
numpages = {31},
keywords = {Meta-model, Service pattern, Variability modeling, Web service composition}
}

@article{10.1016/j.eswa.2012.08.026,
author = {Ognjanovi\'{c}, Ivana and Ga\v{s}Evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {A stratified framework for handling conditional preferences: An extension of the analytic hierarchy process},
year = {2013},
issue_date = {March, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.08.026},
doi = {10.1016/j.eswa.2012.08.026},
abstract = {Representing and reasoning over different forms of preferences is of crucial importance to many different fields, especially where numerical comparisons need to be made between critical options. Focusing on the well-known Analytical Hierarchical Process (AHP) method, we propose a two-layered framework for addressing different kinds of conditional preferences which include partial information over preferences and preferences of a lexicographic kind. The proposed formal two-layered framework, called CS-AHP, provides the means for representing and reasoning over conditional preferences. The framework can also effectively order decision outcomes based on conditional preferences in a way that is consistent with well-formed preferences. Finally, the framework provides an estimation of the potential number of violations and inconsistencies within the preferences. We provide and report extensive performance analysis for the proposed framework from three different perspectives, namely time-complexity, simulated decision making scenarios, and handling cyclic and partially defined preferences.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1094–1115},
numpages = {22},
keywords = {AHP method, Comparative preferences, Conditional preferences, Lexicographic order, S-AHP method, Well-formed preferences}
}

@inproceedings{10.5555/2486788.2486923,
author = {Bellomo, Stephany and Nord, Robert L. and Ozkaya, Ipek},
title = {A study of enabling factors for rapid fielding: combined practices to balance speed and stability},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Agile projects are showing greater promise in rapid fielding as compared to waterfall projects. However, there is a lack of clarity regarding what really constitutes and contributes to success. We interviewed project teams with incremental development lifecycles, from five government and commercial organizations, to gain a better understanding of success and failure factors for rapid fielding on their projects. A key area we explored involves how Agile projects deal with the pressure to rapidly deliver high-value capability, while maintaining project speed (delivering functionality to the users quickly) and product stability (providing reliable and flexible product architecture). For example, due to schedule pressure we often see a pattern of high initial velocity for weeks or months, followed by a slowing of velocity due to stability issues. Business stakeholders find this to be disruptive as the rate of capability delivery slows while the team addresses stability problems. We found that experienced practitioners, when faced with these challenges, do not apply Agile practices alone. Instead they combine practicesAgile, architecture, or otherin creative ways to respond quickly to unanticipated stability problems. In this paper, we summarize the practices practitioners we interviewed from Agile projects found most valuable and provide an overarching scenario that provides insight into how and why these practices emerge.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {982–991},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.4018/jismd.2013070102,
author = {Reinhartz-Berger, Iris},
title = {Representation of Situational Methods: Incorporating ISO/IEC 24744 into a Domain-Based Framework},
year = {2013},
issue_date = {July 2013},
publisher = {IGI Global},
address = {USA},
volume = {4},
number = {3},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2013070102},
doi = {10.4018/jismd.2013070102},
abstract = {Method Engineering ME and Situational Method Engineering SME aim at providing effective solutions for building and supporting evolution of software and information systems development methods. For this purpose, method components are specified and composed into general-purpose development methods or situational methods, i.e., development methods that best fit the characteristics of a given project and its environment. Recently ISO/IEC 24744 has emerged for defining a metamodel and a notation for development methods. However, this standard lacks a systematic support for situational classification and maintenance. In this work, the authors suggest incorporating ISO/IEC 24744 metamodel into a domain-based framework, called Application-based DOmain Modeling for Method Engineering ADOM-ME, which supports specifying both general-purpose and situational methods in a single, simple, accessible, and scalable frame of reference. An exploratory study on the usability of ADOM-ME indicates that the approach can be utilized by information systems students that represent non-experienced method engineers and software developers.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = jul,
pages = {32–49},
numpages = {18},
keywords = {Development Methods, ISO/IEC 24744, Metamodeling, Method Engineering ME, Methodology, Situational Method Engineering SME}
}

@inproceedings{10.1007/978-3-319-05843-6_15,
author = {Alebrahim, Azadeh and Faβbender, Stephan and Heisel, Maritta and Meis, Rene},
title = {Problem-Based Requirements Interaction Analysis},
year = {2014},
isbn = {9783319058429},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-05843-6_15},
doi = {10.1007/978-3-319-05843-6_15},
abstract = {[Context] The ability to address the diverse interests of different stakeholders in a software project in a coherent way is one fundamental software quality. These diverse and maybe conflicting interests are reflected by the requirements of each stakeholder. [Problem] Thus, it is likely that aggregated requirements for a software system contain interactions. To avoid unwanted interactions and improve software quality, we propose a structured method consisting of three phases to find such interactions. [Principal ideas] For our method, we use problem diagrams, which describe requirements in a structured way. The information represented in the problem diagrams is translated into a formal Z model. Then we reduce the number of combinations of requirements, which might conflict. [Contribution] The reduction of requirements interaction candidates is crucial to lower the effort of the in depth interaction analysis. For validation of our method, we use a real-life example in the domain of smart grid.},
booktitle = {Proceedings of the 20th International Working Conference on Requirements Engineering: Foundation for Software Quality - Volume 8396},
pages = {200–215},
numpages = {16},
keywords = {Requirements interactions, Z notation, feature interaction, problem frames},
location = {Essen, Germany},
series = {REFSQ 2014}
}

@article{10.1007/s10664-009-9121-0,
author = {Falessi, Davide and Babar, Muhammad Ali and Cantone, Giovanni and Kruchten, Philippe},
title = {Applying empirical software engineering to software architecture: challenges and lessons learned},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-009-9121-0},
doi = {10.1007/s10664-009-9121-0},
abstract = {In the last 15 years, software architecture has emerged as an important software engineering field for managing the development and maintenance of large, software-intensive systems. Software architecture community has developed numerous methods, techniques, and tools to support the architecture process (analysis, design, and review). Historically, most advances in software architecture have been driven by talented people and industrial experience, but there is now a growing need to systematically gather empirical evidence about the advantages or otherwise of tools and methods rather than just rely on promotional anecdotes or rhetoric. The aim of this paper is to promote and facilitate the application of the empirical paradigm to software architecture. To this end, we describe the challenges and lessons learned when assessing software architecture research that used controlled experiments, replications, expert opinion, systematic literature reviews, observational studies, and surveys. Our research will support the emergence of a body of knowledge consisting of the more widely-accepted and well-formed software architecture theories.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {250–276},
numpages = {27},
keywords = {Empirical software engineering, Software architecture}
}

@article{10.1145/1366546.1366547,
author = {G\'{e}rard, S\'{e}bastien and Feiler, Peter and Rolland, Jean-Francois and Filali, Mamoun and Reiser, Mark-Oliver and Delanote, Didier and Berbers, Yolande and Pautet, Laurent and Perseil, Isabelle},
title = {UML&amp;AADL '2007 grand challenges},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/1366546.1366547},
doi = {10.1145/1366546.1366547},
abstract = {On today's sharply competitive industrial market, engineers must focus on their core competencies to produce ever more innovative products, while also reducing development times and costs. This has further heightened the complexity of the development process. At the same time, industrial systems, and specifically real-time embedded systems, have become increasingly software-intensive. New software development approaches and methods must therefore be found to free engineers from the even more complex technical constraints of development and to enable them to concentrate on their core business specialties. One emerging solution is to foster model-based development by defining modeling artifacts well-suited to their domain concerns instead of asking them to write code. However, model-driven approaches will be solutions to the previous issues only if models evolves from a contemplative role to a productive role within the development processes. In this context, model transformation is a key design paradigm that will foster this revolution. This paper is the result of discussions and exchanges that took place within the second edition of the workshop "UML&amp;AADL" (http://www.artist-embedded.org/artist/Topics.html) that-was hold in 2007 in Auckland, New Zealand, in conjunction with the ICECCS07 conference. The purpose of this workshop was to gather people of both communities from UML (including its domain specific extensions, with a focus on MARTE) and AADL (including its annexes) in order to foster sharing of results and experiments. More specially this year, the focus was on how both standards do subscribe to the model driven engineering paradigm, or to be more precise, how MDE may ease and foster the usage of both sets of standards for developing real-time embedded systems. This paper will show that, even if the work is not yet finished, the current results seems to be already very promising.},
journal = {SIGBED Rev.},
month = oct,
articleno = {1},
numpages = {1},
keywords = {AADL, ADL, MARTE, MDA, MDD, MDE, TLA+, UML, embedded, real-time, xUML}
}

@article{10.1016/j.infsof.2016.03.001,
author = {Bass, Julian M.},
title = {Artefacts and agile method tailoring in large-scale offshore software development programmes},
year = {2016},
issue_date = {July 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {75},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.03.001},
doi = {10.1016/j.infsof.2016.03.001},
abstract = {Context: Large-scale offshore software development programmes are complex, with challenging deadlines and a high risk of failure. Agile methods are being adopted, despite the challenges of coordinating multiple development teams. Agile processes are tailored to support team coordination. Artefacts are tangible products of the software development process, intended to ensure consistency in the approach of teams on the same development programme.Objective: This study aims to increase understanding of how development processes are tailored to meet the needs of large-scale offshore software development programmes, by focusing on artefact inventories used in the development process.Method: A grounded theory approach using 46 practitioner interviews, supplemented with documentary sources and observations, in nine international companies was adopted. The grounded theory concepts of open coding, memoing, constant comparison and saturation were used in data analysis.Results: The study has identified 25 artefacts, organised into five categories: feature, sprint, release, product and corporate governance. It was discovered that conventional agile artefacts are enriched with artefacts associated with plan-based methods in order to provide governance. The empirical evidence collected in the study has been used to identify a primary owner of each artefact and map each artefact to specific activities within each of the agile roles.Conclusion: The development programmes in this study create agile and plan-based artefacts to improve compliance with enterprise quality standards and technology strategies, whilst also mitigating risk of failure. Management of these additional artefacts is currently improvised because agile development processes lack corresponding ceremonies.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {1–16},
numpages = {16},
keywords = {Software development artefacts, Scrum, Process tailoring, Outsourced, Offshore, Large-scale, Grounded theory, Enterprise, Agile software development}
}

@article{10.1007/s11219-017-9386-2,
author = {Gurbuz, Havva Gulay and Tekinerdogan, Bedir},
title = {Model-based testing for software safety: a systematic mapping study},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9386-2},
doi = {10.1007/s11219-017-9386-2},
abstract = {Testing safety-critical systems is crucial since a failure or malfunction may result in death or serious injuries to people, equipment, or environment. An important challenge in testing is the derivation of test cases that can identify the potential faults. Model-based testing adopts models of a system under test and/or its environment to derive test artifacts. This paper aims to provide a systematic mapping study to identify, analyze, and describe the state-of-the-art advances in model-based testing for software safety. The systematic mapping study is conducted as a multi-phase study selection process using the published literature in major software engineering journals and conference proceedings. We reviewed 751 papers and 36 of them have been selected as primary studies to answer our research questions. Based on the analysis of the data extraction process, we discuss the primary trends and approaches and present the identified obstacles. This study shows that model-based testing can provide important benefits for software safety testing. Several solution directions have been identified, but further research is critical for reliable model-based testing approach for safety.},
journal = {Software Quality Journal},
month = dec,
pages = {1327–1372},
numpages = {46},
keywords = {Model-based testing, Model-driven testing, Software safety, Systematic mapping study}
}

@article{10.1007/s10270-017-0648-z,
author = {Aysolmaz, Banu and Schunselaar, Dennis M. and Reijers, Hajo A. and Yaldiz, Ali},
title = {Selecting a process variant modeling approach: guidelines and application},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0648-z},
doi = {10.1007/s10270-017-0648-z},
abstract = {Various modeling approaches have been introduced to manage process diversity in a business context. For practitioners, it is difficult to select an approach suitable for the needs and limitations of their organization due to the limited number of examples and guidelines. In this paper, we report on an action research study to perform a comparative process variant modeling application in a process management consultancy company. This company experienced difficulties in maintaining and reusing process definitions of their customers. We describe how the requirements were determined and led to the selection of two specific approaches, the Decomposition Driven Method and the Provop approach. We comparatively evaluated the suitability of these approaches to develop variant models for six software project management processes of five customers. This study contributes to the field by presenting an industrial case for process variant modeling, reporting in-depth, real-life applications of two approaches, applying the approaches for hierarchical processes, and presenting guidelines for choosing an approach under comparable conditions.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1155–1178},
numpages = {24},
keywords = {Business process modeling, Decomposition driven method, Process variant modeling, Provop}
}

@inproceedings{10.1145/1370828.1370831,
author = {Conejero, Jos\'{e} M. and Hern\'{a}ndez, Juan},
title = {Analysis of crosscutting features in software product lines},
year = {2008},
isbn = {9781605580326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370828.1370831},
doi = {10.1145/1370828.1370831},
abstract = {Software Product Lines has emerged as a new technology to develop software product families related to a particular domain. The software products developed by this methodology are based on the combination of a set of common and variable assets. However, in order to combine these assets to build different products, coupling between common and variable parts must be highly reduced. In that sense, crosscutting features make evolution and adaptability of software difficult. In this paper we propose a framework to identify crosscutting features at early stages in order to use aspect-oriented techniques to modularize them and reduce their dependencies. This framework is based on a crosscutting pattern and uses traceability matrices to perform the analysis of crosscutting. Finally, applicability of the framework is shown by identifying crosscutting features in the Arcade Game Maker product line.},
booktitle = {Proceedings of the 13th International Workshop on Early Aspects},
pages = {3–10},
numpages = {8},
keywords = {crosscutting features, software product lines},
location = {Leipzig, Germany},
series = {EA '08}
}

@inproceedings{10.1145/1370062.1370078,
author = {Espinoza, Huascar and Servat, David and G\'{e}rard, S\'{e}bastien},
title = {Leveraging analysis-aided design decision knowledge in UML-based development of embedded systems},
year = {2008},
isbn = {9781605580388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370062.1370078},
doi = {10.1145/1370062.1370078},
abstract = {Many important works have been carried out to provide modeling languages (e.g., UML, SDL) with expressiveness to support embedded system design, validation and verification. A fundamental shortcoming in current model-driven approaches is the inability to explicitly capture design decisions and trade-offs between different non-functional parameters, among which timeliness, memory usage, and power consumption are of primary interest. This paper highlights technical limitations in UML to specify complex non-functional evaluation scenarios of candidate architectures, and outlines our current work to provide straightforward solutions.},
booktitle = {Proceedings of the 3rd International Workshop on Sharing and Reusing Architectural Knowledge},
pages = {55–62},
numpages = {8},
keywords = {UML, design space exploration, embedded systems, model-driven engineering, trade-off analysis},
location = {Leipzig, Germany},
series = {SHARK '08}
}

@article{10.1016/j.jss.2016.03.038,
author = {Deb, Novarun and Chaki, Nabendu and Ghose, Aditya},
title = {Extracting finite state models from i* models},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.03.038},
doi = {10.1016/j.jss.2016.03.038},
abstract = {The Naive Algorithm (NA) extracts all possible finite state models from i* models.We observe an explosion in the finite state model space.The Semantic Implosion Algorithm (SIA) is a solution to this explosion problem.NA and SIA are extensively simulated on different categories of i* models.SIA drastically reduces the model space growth from O(1020) (for NA) to O(103). i* models are inherently sequence agnostic. This makes the process of cross-checking i* models against temporal properties quite impossible. There is an immediate industrial need to bridge the gap between such a sequence agnostic model and a standardized model verifier so that model checking can be performed in the requirement analysis phase itself. In this paper, we first spell out the Naive Algorithm that generates all possible finite state models corresponding to a given i* model. The growth of the finite state model space can be mapped to the problem of finding the number of possible paths between the Least Upper Bound (LUB) and the Greatest Lower Bound (GLB) of a k-dimensional hypercube lattice structure. The mathematics for doing a quantitative analysis of the space growth has also been presented. The Naive Algorithm has its main drawback in the hyperexponential growth of the model space. The Semantic Implosion Algorithm is proposed as a solution to the hyperexponential problem. This algorithm exploits the temporal information embedded within the i* model of an enterprise to reduce the rate of growth of the finite state model space. A comparative quantitative analysis between the two approaches concludes the superiority of the Semantic Implosion Algorithm.},
journal = {J. Syst. Softw.},
month = nov,
pages = {265–280},
numpages = {16},
keywords = {Model checking, Model transformation, i* model}
}

@inproceedings{10.1145/1370916.1370921,
author = {Bastida, Leire and Nieto, Francisco Javier and Tola, Roberto},
title = {Context-aware service composition: a methodology and a case study},
year = {2008},
isbn = {9781605580296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370916.1370921},
doi = {10.1145/1370916.1370921},
abstract = {The need for defining flexible and dynamic composed services that can adapt to the execution environment in an easy and quick way brings a new challenge to service centric system development and use. In this article, we present an innovative, easy-to-handle solution that supports the development of self-configuring and context-aware compositions from design time to implementation level. We discuss the steps that the service integrators should follow to create context-aware service compositions and also introduce a composition platform that supports the lifecycle of dynamic compositions both at design-time and at runtime. In order to demonstrate that our approach enables the development of context-aware service composition, we focus on a real case study in the automotive and telecommunication domains, where the evaluation results may be useful to understand, supervise and improve our approach to support dynamic compositions.},
booktitle = {Proceedings of the 2nd International Workshop on Systems Development in SOA Environments},
pages = {19–24},
numpages = {6},
keywords = {dynamic composition, service centric systems},
location = {Leipzig, Germany},
series = {SDSOA '08}
}

@article{10.1016/j.jss.2018.06.027,
author = {Merino, L. and Ghafari, M. and Anslow, C. and Nierstrasz, O.},
title = {A systematic literature review of software visualization evaluation},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.06.027},
doi = {10.1016/j.jss.2018.06.027},
journal = {J. Syst. Softw.},
month = oct,
pages = {165–180},
numpages = {16},
keywords = {Software visualisation, Evaluation, Literature review}
}

@article{10.1016/j.future.2015.03.006,
author = {Garc\'{\i}a-Gal\'{a}n, Jes\'{u}s and Trinidad, Pablo and Rana, Omer F. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated configuration support for infrastructure migration to the cloud},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.03.006},
doi = {10.1016/j.future.2015.03.006},
abstract = {With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configuration-in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy. We support the decision making in migration planning to the cloud.We use Feature Models to describe the configuration space of an IaaS.We automate the search of the most suitable IaaS configuration.Our approach improves the results of commercial applications on Amazon EC2.},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {200–212},
numpages = {13},
keywords = {Automated analysis, Cloud migration, EC2, Feature model, IaaS}
}

@article{10.1007/s10270-015-0498-5,
author = {Rodrigues, Taniro and Delicato, Fl\'{a}via C. and Batista, Thais and Pires, Paulo F. and Pirmez, Luci},
title = {An approach based on the domain perspective to develop WSAN applications},
year = {2017},
issue_date = {October   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0498-5},
doi = {10.1007/s10270-015-0498-5},
abstract = {As wireless sensor and actuator networks (WSANs) can be used in many different domains, WSAN applications have to be built from two viewpoints: domain and network. These different viewpoints create a gap between the abstractions handled by the application developers, namely the domain and network experts. Furthermore, there is a coupling between the application logic and the underlying sensor platform, which results in platform-dependent projects and source codes difficult to maintain, modify, and reuse. Consequently, the process of developing an application becomes cumbersome. In this paper, we propose a model-driven architecture (MDA) approach for WSAN application development. Our approach aims to facilitate the task of the developers by: (1) enabling application design through high abstraction level models; (2) providing a specific methodology for developing WSAN applications; and (3) offering an MDA infrastructure composed of PIM, PSM, and transformation programs to support this process. Our approach allows the direct contribution of domain experts in the development of WSAN applications, without requiring specific knowledge of programming WSAN platforms. In addition, it allows network experts to focus on the specific characteristics of their area of expertise without the need of knowing each specific application domain.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {949–977},
numpages = {29},
keywords = {Abstraction, Architecture, Code generation, Domain-specific language, Model-driven architecture, UML profile, WSAN applications}
}

@article{10.1016/j.csi.2013.04.002,
author = {Garz\'{a}S, Javier and Pino, Francisco J. and Piattini, Mario and Fern\'{a}Ndez, Carlos Manuel},
title = {A maturity model for the Spanish software industry based on ISO standards},
year = {2013},
issue_date = {November, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {35},
number = {6},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2013.04.002},
doi = {10.1016/j.csi.2013.04.002},
abstract = {Many organizations are implementing process improvement models, seeking to increase their organizational maturity for software development. However, implementing traditional maturity models involves a large investment (as regards money, time and resources) which is beyond the reach of vast majority of small organizations. This paper presents the use and adaptation of some ISO models in the creation of an organizational maturity model for the Spanish software industry. This model was used satisfactorily to (i) improve the software processes of several Spanish small firms, and (ii) obtain an organizational maturity certification for software development, granted by the Spanish Association for Standardization and Certification.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {616–628},
numpages = {13},
keywords = {AENOR, Certification, ISO/IEC 12207, ISO/IEC 15504, Maturity levels, Maturity model}
}

@article{10.1007/s10664-018-9597-6,
author = {Quirchmayr, Thomas and Paech, Barbara and Kohl, Roland and Karey, Hannes and Kasdepke, Gunar},
title = {Semi-automatic rule-based domain terminology and software feature-relevant information extraction from natural language user manuals},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9597-6},
doi = {10.1007/s10664-018-9597-6},
abstract = {Mature software systems comprise a vast number of heterogeneous system capabilities which are usually requested by different groups of stakeholders and which evolve over time. Software features describe and bundle low level capabilities logically on an abstract level and thus provide a structured and comprehensive overview of the entire capabilities of a software system. Software features are often not explicitly managed. Quite the contrary, feature-relevant information is often spread across several software engineering artifacts (e.g., user manual, issue tracking systems). It requires huge manual effort to identify and extract feature-relevant information from these artifacts in order to make feature knowledge explicit. In this paper we present a two-step-approach to extract feature-relevant information from a user manual: First we semi-automatically extract a domain terminology from a natural language user manual based on linguistic patterns. Then, we apply natural language processing techniques based on the extracted domain terminology and structural sentence information. Our approach is able to extract atomic feature-relevant information with an F1-score of at least 92.00%. We describe the implementation of the approach as well as evaluations based on example sections of a user manual taken from industry.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {3630–3683},
numpages = {54},
keywords = {Atomic information extraction, NLP, Software feature, Terminology extraction}
}

@article{10.1007/s10270-019-00723-2,
author = {Daun, Marian and Weyer, Thorsten and Pohl, Klaus},
title = {Improving manual reviews in function-centered engineering of embedded systems using a dedicated review model},
year = {2019},
issue_date = {Dec 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {6},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-019-00723-2},
doi = {10.1007/s10270-019-00723-2},
abstract = {In model-based engineering of embedded systems, manual validation activities such as reviews and inspections are needed to ensure that the system under development satisfies the stakeholder intentions. During the engineering process, changes in the stakeholder intentions typically trigger revisions of already developed and documented engineering artifacts including requirements and design specifications. In practice, changes in stakeholder intentions are often not immediately perceived and not properly documented. Moreover, they are quite often not consistently incorporated into all relevant engineering artifacts. In industry, typically manual reviews are executed to ensure that the relevant stakeholder intentions are adequately considered in the engineering artifacts. In this article, we introduce a dedicated review model to aid the reviewer in conducting manual reviews of behavioral requirements and functional design specification—two core artifacts in function-centered engineering of embedded software. To investigate whether the proposed solution is beneficial we conducted controlled experiments showing that the use of the dedicated review model can significantly increase the effectiveness and efficiency of manual reviews. Additionally, the use of the dedicated review model leads to significantly more confident decisions of the reviewers and is perceived by the reviewers as significantly more supportive compared with reviews without the dedicated review model.},
journal = {Softw. Syst. Model.},
month = dec,
pages = {3421–3459},
numpages = {39},
keywords = {Behavioral requirements, Embedded software, Functional design, Review model, Requirements engineering, Perspective-based review, Model transformations}
}

@article{10.1007/s00607-016-0497-6,
author = {Park, Joonseok and An, Youngmin and Kang, Taejun and Yeom, Keunhyuk},
title = {Virtual cloud bank: consumer-centric service recommendation process and architectural perspective for cloud service brokers},
year = {2016},
issue_date = {November  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {98},
number = {11},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-016-0497-6},
doi = {10.1007/s00607-016-0497-6},
abstract = {A paradigm shift from a PC-centric to a cloud computing environment has occurred because of rapid and continuous improvements in the IT environments. With the expansion of cloud computing, various types of cloud services are emerging, collectively known as XaaS (that is, "everything as a service"), including Infrastructure as a Service, Platform as a Service, and Software as a Service. Therefore, an intermediation entity known as a CSB (cloud service broker) is required that interrelates cloud service providers and cloud consumers. However, CSBs remain an active research area in industry and academia. With the diversification of cloud services and the emergence of services with similar functions, the role of the CSB to recommend cloud services that meet the requirements of various consumers has become increasingly important. In this paper, to allow CSBs to provide appropriate cloud services from consumer-centric perspectives, we propose a CSB named the virtual cloud bank that includes a process for recommending cloud services and architectural aspects. Our proposed approach can be utilized as a reference model for a consumer-centric CBS for provisioning cloud services. Therefore, if CSB developers apply and extend the proposed well-defined architectural elements and recommendation approach, this is expected to help accelerate the growth of the cloud computing market.},
journal = {Computing},
month = nov,
pages = {1153–1184},
numpages = {32},
keywords = {68N30, CSB architecture, Cloud service brokerage, Cloud service recommendation, Service recommendation process, Virtual cloud bank}
}

@article{10.1007/s10270-020-00840-3,
author = {Akiki, Pierre A. and Maalouf, Hoda W.},
title = {CHECKSUM: tracking changes and measuring contributions in cooperative systems modeling},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00840-3},
doi = {10.1007/s10270-020-00840-3},
abstract = {Models are often used to represent various types of systems. This is especially true for software systems, where cooperating teams create models using a modeling language (e.g., UML). In cooperative modeling scenarios, it is useful to identify contributions and changes performed by individuals and teams. This paper presents a technique called CHECKSUM, which monitors the cooperative work done on models and maintains an immutable changelog. CHECKSUM uses its changelog to measure contributions based on points, time, and quality, and to enable the auditing of a model’s change-history. This paper also presents GEneric Meta-Model (GEMM). The latter unifies the underlying representation of different types of models that follow varying visualization patterns including box and line, container, and interleaving. GEMM enables CHECKSUM to support an extensible variety of model types. We developed a prototype tool that realizes CHECKSUM’s concepts and integrates it into two existing modeling tools. We conducted two studies to evaluate CHECKSUM from two perspectives: technical and user. The studies yielded positive results concerning various qualities including integrability into existing tools, effectiveness, efficiency, usability, and usefulness.},
journal = {Softw. Syst. Model.},
month = aug,
pages = {1079–1122},
numpages = {44},
keywords = {Models, Diagrams, Changes, Contributions, Cooperative work, Design tools and techniques}
}

@article{10.1145/2757308.2757312,
author = {Cheng, Betty H.C.},
title = {A Review of Dr. Robert France's Contributions and Impact on Model-Driven Engineering and Software Engineering: Robert B. France (1960-2015)},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/2757308.2757312},
doi = {10.1145/2757308.2757312},
abstract = {Robert B. France was one of the pioneers in model-driven engineering and instrumental in shaping the field of model-driven engineering from two complementary dimensions: research and building collaborative communities. In both dimensions, an overarching quality is the notion of inclusiveness and bridge building. Dr. France built bridges between research communities, between researchers and practitioners, between individual researchers, and between senior and junior researchers. This article describes a few highlights from both dimensions, focusing on the "bridging" aspects in each},
journal = {SIGSOFT Softw. Eng. Notes},
month = jun,
pages = {23–31},
numpages = {9}
}

@inproceedings{10.1145/3291801.3291821,
author = {Xu, Zheng and Guan, Ti and Li, Huicong and Ma, Qiang and Geng, Yujie and Zhang, Pei and Wang, Yanling},
title = {Decoupling Design Based on Power Dispatching Management Cloud Computing Platform},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291821},
doi = {10.1145/3291801.3291821},
abstract = {The current power dispatching management(OMS) cloud platform has strong coupling between various functional modules, and the modification or stop of some modules may affect the operation of the entire system. In order to solve this problem, this paper proposes the decoupling measures for the existing OMS cloud platform to effectively reduce the coupling between functional modules. This paper introduces the technology of OMS cloud platform decoupling, the design principle and overall architecture of OMS cloud platform decoupling, and the design structure of IaaS, PaaS and SaaS layers. The test of the decoupled system is introduced. The decoupled system can realize independent deployment of business functions and independent upgrade maintenance, and improve the dynamic scalability and maintainability of the system.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {206–211},
numpages = {6},
keywords = {Cloud Platform, Decoupling, Power Dispatching Management System},
location = {Weihai, China},
series = {ICBDR '18}
}

@article{10.1016/j.jss.2016.07.038,
author = {Oliveira, Raphael Pereira de and Santos, Alcemir Rodrigues and Almeida, Eduardo Santana de and Gomes, Gecynalda Soares da Silva},
title = {Evaluating Lehmans Laws of software evolution within software product lines industrial projects},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.07.038},
doi = {10.1016/j.jss.2016.07.038},
abstract = {We performed two empirical evaluations within industrial Software Product Lines projects.We applied a statistical test to evaluated Lehmans Laws according to the available industrial data.We performed a complete descriptive statistics analysis.Most of the Lehmans Laws of software evolution holds for Software Product Lines.There is a need for systematizing the evolution within Software Product Lines. The evolution of a single system is a task where we deal with the modification of a single product. Lehmans Laws of software evolution were broadly evaluated within this type of system and the results shown that these single systems evolve according to his stated laws over time. However, considering Software Product Lines (SPL), we need to deal with the modification of several products which include common, variable, and product specific assets. Because of the several assets within SPL, each stated law may have a different behavior for each asset kind. Nonetheless, we do not know if all of the stated laws are still valid for SPL since they were partially evaluated in this context. Thus, this paper details an empirical investigation where Lehmans Laws (LL) of Software Evolution were used in two SPL industrial projects to understand how the SPL assets evolve over time. These projects are related to an application in the medical domain and another in the financial domain, developed by medium-size companies in Brazil. They contain a total of 71 modules and a total of 71.442 bug requests in their tracking system, gathered along the total of more than 10 years. We employed two techniques - the KPSS Test and linear regression analysis, to assess the relationship between LL and SPL assets. Results showed that one law was completely supported (conservation of organizational stability) for all assets within both empirical studies. Two laws were partially supported for both studies depending on the asset type (continuous growth and conservation of familiarity). Finally, the remaining laws had differences among their results for all assets (continuous change, increasing complexity, and declining quality).},
journal = {J. Syst. Softw.},
month = sep,
pages = {347–365},
numpages = {19},
keywords = {Empirical study, Lehmans Laws of software evolution, Software evolution, Software product lines}
}

@article{10.1145/2579281.2579283,
author = {Doernhoefer, Mark},
title = {Surfing the net for software engineering notes},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579283},
doi = {10.1145/2579281.2579283},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {15–24},
numpages = {10}
}

@inproceedings{10.1145/2656075.2656101,
author = {Assare, Omid and Gupta, Rajesh},
title = {Timing analysis of erroneous systems},
year = {2014},
isbn = {9781450330510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656075.2656101},
doi = {10.1145/2656075.2656101},
abstract = {Erroneous systems allow timing errors to occur during execution, but use measures to ensure continued operation through changes in operating parameters (voltage and frequency), error correction at various levels of the system, or ensuring controlled occurrence of errors to perform approximate computing. In this paper, we are interested in characterization of error behavior at the level of instructions and programs. We propose Inter- and Intra-Program Variation as measures of error rate variability in different programs and among instructions of a program, respectively. We also characterize the error rate variation caused by the program input data and show that it is comparable to other sources of variability such as process variation. Finally, we present an analysis of the physical location of errors in hardware, identify regions in which most of the errors occur, and how different programs change the distribution of errors among these regions. In order to enable reliable timing analysis of large programs, we propose Clustered Timing Model (CTM), a high level timing model based on clustering functionally similar timing paths of the processor, and develop a CTM for LEON3, a representative in-order RISC processor. The accuracy of the model is verified using our variation-aware timing analysis framework with an average error of 3.9% (max. 6.7%) across a wide range of voltage-temperature corners.},
booktitle = {Proceedings of the 2014 International Conference on Hardware/Software Codesign and System Synthesis},
articleno = {7},
numpages = {10},
keywords = {delay faults, dynamic error estimation, erroneous systems, process variation, software error behavior, variability, variation-aware timing analysis},
location = {New Delhi, India},
series = {CODES '14}
}

@inproceedings{10.5555/1783789.1783812,
author = {Lin, Tao and Imamiya, Atsumi and Hu, Wanhua and Omata, Masaki},
title = {Combined user physical, physiological and subjective measures for assessing user cost},
year = {2006},
isbn = {9783540710240},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {New technologies are making it possible to provide an enriched view of interaction for researchers using multimodal information. This preliminary study explores the use of multimodal information streams in evaluating user cost. In the study, easy, medium and difficult versions of a game task were used to vary the levels of the cost to user. Multimodal data streams during the three versions were analyzed, including eye tracking, pupil size, hand movement, heart rate variability (HRV) and subjectively reported data. Three findings indicate the potential value of multimodal information in evaluating usability: First, subjective and physiological measures showed significant sensitivity to task difficulty. Second, different user cost levels appeared to correlate with eye movement patterns, especially with a combined eye-hand measure. Third, HRV showed correlations with saccade speed. These results warrant further investigations and take an initial step toward establishing usability evaluation methods based on multimodal information.},
booktitle = {Proceedings of the 9th Conference on User Interfaces for All},
pages = {304–316},
numpages = {13},
location = {K\"{o}nigswinter, Germany},
series = {ERCIM'06}
}

@article{10.1145/2629395,
author = {Jain, Radhika and Cao, Lan and Mohan, Kannan and Ramesh, Balasubramaniam},
title = {Situated Boundary Spanning: An Empirical Investigation of Requirements Engineering Practices in Product Family Development},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629395},
doi = {10.1145/2629395},
abstract = {Requirements Engineering (RE) faces considerable challenges that are often related to boundaries between various stakeholders involved in the software development process. These challenges may be addressed by boundary spanning practices. We examine how boundary spanning can be adapted to address RE challenges in Product Family Development (PFD), a context that involves complex RE. We study two different development approaches, namely, conventional and agile PFD, because these present considerably different challenges. Our findings from a multisite case study present boundary spanning as a solution to improve the quality of RE processes and highlight interesting differences in how boundary spanner roles and boundary objects are adapted in conventional and agile PFD.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {16},
numpages = {29},
keywords = {Boundary spanning, agile development, product family development, requirements engineering}
}

@article{10.1145/3149485.3149524,
author = {Galster, Matthias and Weyns, Danny and Goedicke, Michael and Zdun, Uwe and Cunha, J\'{a}come and Chavarriaga, Jaime},
title = {Variability and Complexity in Software Design: Towards Quality through Modeling and Testing},
year = {2018},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3149485.3149524},
doi = {10.1145/3149485.3149524},
abstract = {Today's software systems must accommodate a wide range of usage and deployment scenarios. The increasing size and heterogeneity of software-intensive systems, dynamic and critical operating conditions, fast moving and highly competitive markets, and increasingly powerful and versatile hardware makes it more and more difficult to handle the additional complexity in design caused by variability. This paper reports results of the Second International Workshop on Variability and Complexity in Software Design. It also outlines directions the field might move in the future.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {35–37},
numpages = {3},
keywords = {Variability, complexity, software design}
}

@inproceedings{10.1145/2896941.2896945,
author = {Smiley, Karen and Harding, Jeff and Patel, Pankesh},
title = {From ideas to implementations: closing the gaps between technical experts and software solutions},
year = {2016},
isbn = {9781450341578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896941.2896945},
doi = {10.1145/2896941.2896945},
abstract = {Rapid delivery strategies strive to balance critical performance qualities vs. reducing the time between an idea and deployment of a software implementation of that idea. For industrial software solutions that encapsulate expertise in deliverable components, technical SMEs (Subject Matter Experts) with ideas and knowledge have traditionally partnered as requirements providers with software development teams. These human processes are not optimally fast, are vulnerable to errors in translating or interpreting requirements, and do not scale when software teams need to integrate the knowledge of many SMEs into multiple software solutions and deployments. To address these limitations, ABB has pursued an industrial research initiative for innovative SME toolsets with focus on two goals: to accelerate the creation, evolution, reuse, and delivery of expert algorithms, and to streamline the deployment of these algorithms into releases and fielded solutions. The vision underpinning the initiative is to empower technical SMEs as "end-user developers" to convert their knowledge into reusable software solution components without having to learn, perform, or partner on traditional software development, integration, or deployment. In this paper, we summarize our experiences and lessons learned to date from this initiative, key continuing challenges, and some positional thoughts on how end-user development by technical SMEs aligns with emerging approaches for rapid delivery and evolution.},
booktitle = {Proceedings of the International Workshop on Continuous Software Evolution and Delivery},
pages = {1–4},
numpages = {4},
keywords = {continuous deployment, end-user programming, end-user software engineering, industrial analytics, subject matter expert, visual programming environments},
location = {Austin, Texas},
series = {CSED '16}
}

@inproceedings{10.1145/2994310.2994327,
author = {Mattila, Anna-Liisa and Ihantola, Petri and Kilamo, Terhi and Luoto, Antti and Nurminen, Mikko and V\"{a}\"{a}t\"{a}j\"{a}, Heli},
title = {Software visualization today: systematic literature review},
year = {2016},
isbn = {9781450343671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2994310.2994327},
doi = {10.1145/2994310.2994327},
abstract = {Software visualization means visualizing various aspects and artifacts related to software. By this definition a wide range of different software engineering aspects from program comprehension to understanding software process and usage are covered. This paper presents the results of systematic literature review spanning six years of software visualization literature. The main result shows that the most studied topics in the past six years are related to software structure, behavior and evolution. Software process and usage are addressed only in few studies. In the future studying the adoption of software visualization tools in industry context would be beneficial.},
booktitle = {Proceedings of the 20th International Academic Mindtrek Conference},
pages = {262–271},
numpages = {10},
keywords = {human-centered computing, software visualization, systematic literature review},
location = {Tampere, Finland},
series = {AcademicMindtrek '16}
}

@inproceedings{10.1145/375212.375277,
author = {Savolainen, Juha and Kuusela, Juha},
title = {Violatility analysis framework for product lines},
year = {2001},
isbn = {1581133588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375212.375277},
doi = {10.1145/375212.375277},
abstract = {Evolution of a software intensive system is unavoidable. In fact, evolution can be seen as a part of reuse process. During the evolution of the software asset, the major part of the system functionality is normally reused. So the key issue is to identify the volatile parts of the domain requirements. Additionally, there is promise that tailored tool support may help supporting evolution in software intensive systems. In this paper, we describe the volatility analysis method for product lines. This highly practical method has been used in multiple domains and is able to express and estimate common types of evolutional characteristics. The method is able to represent volatility in multiple levels and has capacity to tie the volatility estimation to one product line member specification. We  also briefly describe current tool support for the method. The main contribution of this paper is a volatility analysis framework that can be used to describe how requirements are estimated to evolve in the future. The method is based on the definition hierarchy framework.},
booktitle = {Proceedings of the 2001 Symposium on Software Reusability: Putting Software Reuse in Context},
pages = {133–141},
numpages = {9},
keywords = {commonality, domain analysis, evolution, product line, requirements engineering, variability, volatility analysis},
location = {Toronto, Ontario, Canada},
series = {SSR '01}
}

@article{10.4018/jdm.2011100101,
author = {Reinhartz-Berger, Iris and Aharoni, Anat},
title = {Semi-Automatic Composition of Situational Methods},
year = {2011},
issue_date = {October 2011},
publisher = {IGI Global},
address = {USA},
volume = {22},
number = {4},
issn = {1063-8016},
url = {https://doi.org/10.4018/jdm.2011100101},
doi = {10.4018/jdm.2011100101},
abstract = {Situational methods are approaches to the development of software systems that are designed and constructed to fit particular circumstances that often refer to project characteristics. One common way to create situational methods is to reuse method components, which are the building blocks of development methods. For this purpose, method components must be stored in a method base, and then retrieved and composed specifically for the situation in hand. Most approaches in the field of situational method engineering require the expertise of method engineers to support the retrieval and composition of method components. Furthermore, this is usually done in an ad-hoc manner and for pre-defined situations. In this paper, the authors propose an approach, supported by a tool that creates situational methods semi-automatically. This approach refers to structural and behavioral considerations and a wide variety of characteristics when comparing method components and composing them into situational methods. The resultant situational methods are stored in the method base for future usage and composition. Based on an experimental study of the approach, the authors show that it provides correct and suitable draft situational methods, which human evaluators have assessed as relevant for the given situations.},
journal = {J. Database Manage.},
month = oct,
pages = {1–29},
numpages = {29},
keywords = {Development Methods, ISO/IEC 24744, Metamodeling, Semi-Automatic Composition, Situational Method Engineering}
}

@inproceedings{10.5555/1813962.1813999,
author = {Gadelha, Bruno and Nunes, Ingrid and Fuks, Hugo and De Lucena, Carlos J. P.},
title = {An approach for developing groupware product lines based on the 3C collaboration model},
year = {2009},
isbn = {3642042155},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software Product Lines (SPLs) are a new software engineering technology that aims at promoting reduced time and costs in the development of system families by the exploitation of applications commonalities. Given that different Groupware applications typically share a lot of functionalities, Groupware Product Lines (GPLs) have emerged to incorporate SPL benefits to the Groupware development. In this paper, we propose an approach for developing GPLs, which incorporates SPL techniques to allow the derivation of customized groupware according to specific contexts and the systematic reuse of software assets. Our approach is based on the 3C Collaboration Model that allows identifying collaboration needs and guiding the user to select appropriate features according to their collaboration purpose. A GPL of Learning Object repositories, named FLOCOS GPL, is used to illustrate the proposed approach.},
booktitle = {Proceedings of the 15th International Conference on Groupware: Design, Implementation, and Use},
pages = {328–343},
numpages = {16},
keywords = {groupware development, learning objects, software product lines},
location = {Peso da R\'{e}gua, Douro, Portugal},
series = {CRIWG'09}
}

@article{10.1016/j.datak.2014.07.003,
author = {Bre\ss{}, Sebastian and Siegmund, Norbert and Heimel, Max and Saecker, Michael and Lauer, Tobias and Bellatreche, Ladjel and Saake, Gunter},
title = {Load-aware inter-co-processor parallelism in database query processing},
year = {2014},
issue_date = {September 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2014.07.003},
doi = {10.1016/j.datak.2014.07.003},
abstract = {For a decade, the database community has been exploring graphics processing units and other co-processors to accelerate query processing. While the developed algorithms often outperform their CPU counterparts, it is not beneficial to keep processing devices idle while overutilizing others. Therefore, an approach is needed that efficiently distributes a workload on available (co-)processors while providing accurate performance estimates for the query optimizer. In this paper, we contribute heuristics that optimize query processing for response time and throughput simultaneously via inter-device parallelism. Our empirical evaluation reveals that the new approach achieves speedups up to 1.85 compared to state-of-the-art approaches while preserving accurate performance estimations. In a further series of experiments, we evaluate our approach on two new use cases: joining and sorting. Furthermore, we use a simulation to assess the performance of our approach for systems with multiple co-processors and derive some general rules that impact performance in those systems. Contribute heuristics to enhance performance by exploiting inter-device parallelismHeuristics consider load and speed on (co-)processors.Extensive evaluation on four use cases: aggregation, selection, sort, and joinAssess the performance of best heuristic for systems with multiple co-processorsDiscuss how operator-stream-based scheduling can be used in a query processor},
journal = {Data Knowl. Eng.},
month = sep,
pages = {60–79},
numpages = {20},
keywords = {Co-processing, Query optimization, Query processing}
}

@article{10.1504/IJWMC.2014.063054,
author = {Siala, Fatma and Ghedira, Khaled},
title = {How to select dynamically a QoS-driven composite web service by a multi-agent system using CBR method},
year = {2014},
issue_date = {July 2014},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {4},
issn = {1741-1084},
url = {https://doi.org/10.1504/IJWMC.2014.063054},
doi = {10.1504/IJWMC.2014.063054},
abstract = {Service-oriented architecture permits the composition of web services provided with different Quality of Service QoS levels. In a given composition, finding the set of services that optimises some QoS attributes under its constraints is a problem that needs to be solved. Our aim is to propose an intelligent approach to the selection of a Composite Web Service CWS based on QoS. This paper reports the authors' recent research on addressing the issue. An overview on the previously proposed approaches is presented. These approaches correspond to several improvements of an existing multi-agent one, which is well-cited in the specialised literature. Each framework, implemented on JADE Java Agent Development framework, improves another one in terms of CPU time and/or QoS score, to reach a new agent-based and scalable framework. The last framework utilises the agents' ability of negotiation, interaction and cooperation in order to facilitate the selection of composite web services. By using CBR method, the agents can memorise QoS scores and availability. The improvements are related not only to the CPU time but also to the Composite QoS CQoS value, while operating in a dynamic environment and taking into account user preferences.},
journal = {Int. J. Wire. Mob. Comput.},
month = jul,
pages = {327–347},
numpages = {21}
}

@article{10.1007/s10515-012-0101-z,
author = {Hall, Robert J.},
title = {Editorial: analysis in software engineering},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-012-0101-z},
doi = {10.1007/s10515-012-0101-z},
journal = {Automated Software Engg.},
month = sep,
pages = {231–232},
numpages = {2}
}

@article{10.1016/j.jss.2012.03.071,
author = {Poort, Eltjo R. and Van Vliet, Hans},
title = {RCDA: Architecting as a risk- and cost management discipline},
year = {2012},
issue_date = {September, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {9},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.03.071},
doi = {10.1016/j.jss.2012.03.071},
abstract = {We propose to view architecting as a risk- and cost management discipline. This point of view helps architects identify the key concerns to address in their decision making, by providing a simple, relatively objective way to assess architectural significance. It also helps business stakeholders to align the architect's activities and results with their own goals. We examine the consequences of this point of view on the architecture process. The point of view is the basis of RCDA, the Risk- and Cost Driven Architecture approach. So far, more than 150 architects have received RCDA training. For a majority of the trainees, RCDA has a significant positive impact on their architecting work.},
journal = {J. Syst. Softw.},
month = sep,
pages = {1995–2013},
numpages = {19},
keywords = {Cost management, Risk Management, Software architecture}
}

@article{10.1016/j.rcim.2016.01.008,
author = {Luo, Hao and Wang, Kai and Kong, Xiang T.R. and Lu, Shaoping and Qu, Ting},
title = {Synchronized production and logistics via ubiquitous computing technology},
year = {2017},
issue_date = {June 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {45},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2016.01.008},
doi = {10.1016/j.rcim.2016.01.008},
abstract = {The integration of manufacturing and logistics has drawn widespread research attentions in recent years. This paper focuses on the Synchronized Production and Logistics (SPL), which is operational level integration. SPL\'{z}is defined as synchronizing the processing, moving and storing of raw material, WIP and finished product within one manufacturing unit by high level information sharing and joint scheduling to achieve synergic decision, execution and overall performance improvement. Through analysing the requirements and challenges in real life industry, the ubiquitous computing is adopted as an enabling technology and an Ubi-SPL (Synchronized Production and Logistics via Ubiquitous Technology) framework is proposed. This framework is consists of four layers, which creates a close decision-execution loop by linking the frontline real time data, user feedback and optimized decision together. A real life case study of applying Ubi-SPL solution in a chemical industry has been conducted. The implementation results show that the proposed Ubi-SPL solution can significantly improve the overall performance in both production and logistics service. This paper focuses on the Synchronized Production and Logistics.The ubiquitous computing is adopted as an enabling technology.Synchronized Production &amp; Logistics via Ubiquitous Technics framework is proposed.A real life case study in a chemical industry has been conducted.},
journal = {Robot. Comput.-Integr. Manuf.},
month = jun,
pages = {99–115},
numpages = {17},
keywords = {RFID, Synchronized production and logistics, Ubiquitous manufacturing}
}

@inproceedings{10.5555/645882.672257,
author = {Thiel, Steffen and Hein, Andreas},
title = {Systematic Integration of Variability into Product Line Architecture Design},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product lines consider related products, their commonalities and their differences. The differences between the single products are also referred to as variability. Consequently, variability is inherent in every product line and makes a key difference as compared to single systems. While, on the requirements level, the methods for analyzing product line variability are understood today, their transition to architecture remains vague. Bringing variability to architecture as an "add-on" is just a provisional solution and forebodes the risk of violating other intentions. This paper presents a systematic approach to integrate variability with product line architecture design. In particular, it promotes variability as an architectural driver, embeds variability requirements in the architecture design framework "Quality-Driven Software Architecting" (QUASAR), and gives guidelines and examples for documenting variability in architectural views.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {130–153},
numpages = {24},
series = {SPLC 2}
}

@article{10.1145/347823.347831,
author = {Nance, Richard E. and Overstreet, C. Michael and Page, Ernest H.},
title = {Redundancy in model specifications for discrete event simulation},
year = {1999},
issue_date = {July 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/347823.347831},
doi = {10.1145/347823.347831},
abstract = {Although redundancy in model specification generally has negative connotations, we offer arguments for revising those convictions. Defining “representational redundancy” as the inclusion of any symbols not required to fulfill the study objectives, we cite several sources of redundancy, classified as accidental or intentional, that contribute positively to the model development tasks. Comparative benefits and detriments are discussed briefly. Focusing on the most interesting source of redundancy‐that which is intentionally induced by a modeling methodology—we demonstrate that automated elimination of redundancy can actually improve model execution time. Using four models drawn from the literature that are easily understood, but which represent some differences in size and complexity, the direct graphical representations shows improvements over a  base case ranging from 27.3 percent to 68.1 percent in execution time. Further, increasing improvement is realized with increasing model size and complexity. These results are encouraging because they suggest that modeling methodologies with automated model diagnosis can significantly reduce both execution and developments time and cost.},
journal = {ACM Trans. Model. Comput. Simul.},
month = jul,
pages = {254–281},
numpages = {28},
keywords = {discrete event simulation, model analysis, model development environment, uses of redundancy}
}

@inproceedings{10.1145/3426425.3426947,
author = {Coulon, Fabien and Auvolat, Alex and Combemale, Benoit and Bromberg, Y\'{e}rom-David and Ta\"{\i}ani, Fran\c{c}ois and Barais, Olivier and Plouzeau, No\"{e}l},
title = {Modular and distributed IDE},
year = {2020},
isbn = {9781450381765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426425.3426947},
doi = {10.1145/3426425.3426947},
abstract = {Integrated Development Environments (IDEs) are indispensable companions to programming languages. They are increasingly turning towards Web-based infrastructure. The rise of a protocol such as the Language Server Protocol (LSP) that standardizes the separation between a language-agnostic IDE, and a language server that provides all language services (e.g., auto completion, compiler...) has allowed the emergence of high quality generic Web components to build the IDE part that runs in the browser. However, all language services require different computing capacities and response times to guarantee a user-friendly experience within the IDE. The monolithic distribution of all language services prevents to leverage on the available execution platforms (e.g., local platform, application server, cloud). In contrast with the current approaches that provide IDEs in the form of a monolithic client-server architecture, we explore in this paper the modularization of all language services to support their individual deployment and dynamic adaptation within an IDE. We evaluate the performance impact of the distribution of the language services across the available execution platforms on four EMF-based languages, and demonstrate the benefit of a custom distribution.},
booktitle = {Proceedings of the 13th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {270–282},
numpages = {13},
keywords = {Generative approach, IDE, Microservice},
location = {Virtual, USA},
series = {SLE 2020}
}

@inproceedings{10.5555/1885639.1885669,
author = {Furtado, Andre W. B. and Santos, Andre L. M. and Ramalho, Geber L.},
title = {Streamlining domain analysis for digital games product lines},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Digital games and their development process are quite peculiar when compared to other software in general. However, current domain engineering processes do not addresses such peculiarities and, not surprisingly, successful cases of software product lines (SPLs) for digital games cannot be found in the literature nor the industry. With such a motivation, this paper focuses on streamlining and enriching the Domain Analysis process for SPLs targeted at digital games. Guidelines are provided for making Domain Analysis tasks aware of digital games peculiarities, in order to tackle the challenges of and benefit from the unique characteristics of such a macro-domain. A case study for an SPL aimed at arcade-based games is also presented to illustrate and evaluate the proposed guidelines.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {316–330},
numpages = {15},
keywords = {digital games development, domain analysis, software product lines},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1145/3131608,
author = {Oulasvirta, Antti and Feit, Anna and L\"{a}hteenlahti, Perttu and Karrenbauer, Andreas},
title = {Computational Support for Functionality Selection in Interaction Design},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3131608},
doi = {10.1145/3131608},
abstract = {Designing interactive technology entails several objectives, one of which is identifying and selecting appropriate functionality. Given candidate functionalities such as “print,” “bookmark,” and “share,” a designer has to choose which functionalities to include and which to leave out. Such choices critically affect the acceptability, productivity, usability, and experience of the design. However, designers may overlook reasonable designs because there is an exponential number of functionality sets and multiple factors to consider. This article is the first to formally define this problem and propose an algorithmic method to support designers to explore alternative functionality sets in early stage design. Based on interviews of professional designers, we mathematically define the task of identifying functionality sets that strike the best balance among four objectives: usefulness, satisfaction, ease of use, and profitability. We develop an integer linear programming solution that can efficiently solve very large instances (set size over 1,300) on a regular computer. Further, we build on techniques of robust optimization to search for diverse and surprising functionality designs. Empirical results from a controlled study and field deployment are encouraging. Most designers rated computationally created sets to be of the comparable or superior quality than their own. Designers reported gaining better understanding of available functionalities and the design space.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = oct,
articleno = {34},
numpages = {30},
keywords = {Functionality selection, computer-supported design, creativity, design tools, integer linear programming, interaction design, optimization methods, user-centered design}
}

@inproceedings{10.1145/1185448.1185468,
author = {Hunt, John M. and McGregor, John D.},
title = {A series of choices variability in the development process},
year = {2006},
isbn = {1595933158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1185448.1185468},
doi = {10.1145/1185448.1185468},
abstract = {Software variability is "the ability of a software artifact to vary its behavior at some point in its life cycle" [12]. Almost every software artifact requires some type of variability. While variability is endemic to the creation of software it is rarely the direct focus of study. In addition, software systems have shown an increasing amount of variability in recent years. This work provides an analysis of the decisions involved in providing variability at a specific point in a product. A classification scheme and related choice model is provided that describes the decisions related to variability, making them more explicit and quantifiable.},
booktitle = {Proceedings of the 44th Annual ACM Southeast Conference},
pages = {85–90},
numpages = {6},
keywords = {modeling},
location = {Melbourne, Florida},
series = {ACMSE '06}
}

@inproceedings{10.1007/11751595_18,
author = {Kim, Haeng-Kon},
title = {Applying product line to the embedded systems},
year = {2006},
isbn = {3540340750},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11751595_18},
doi = {10.1007/11751595_18},
abstract = {For software intensive systems, a reuse-driven product line approach will potentially reduce time-to-market, and improve product quality while reducing uncertainty on cost and sc11edule estimates. Product lines raise reuse to the level of design frameworks, not simply code or component reuse. They capture commonality and adaptability, through domain and variability analyzes, to be able to create new products easily by instantiating prefabricated components, adapting their design parameters, and leveraging from established testing suites. In this paper, we examine software technology and infrastructure (process) supporting product lines more directly to embedded systems. We also present evaluation criteria for the development of a product line and give an overview of the current state of practices in the embedded software area. A product line architecture that brings about a balance between sub-domains and their most important properties is an investment that must be looked after. However, the sub-domains need flexibility to use, change and manage their own technologies, and evolve separately, but in a controlled way.},
booktitle = {Proceedings of the 2006 International Conference on Computational Science and Its Applications - Volume Part III},
pages = {163–171},
numpages = {9},
location = {Glasgow, UK},
series = {ICCSA'06}
}

@article{10.5555/2873826.2874006,
author = {Hervieu, Aymeric and Marijan, Dusica and Gotlieb, Arnaud and Baudry, Benoit},
title = {Practical minimization of pairwise-covering test configurations using constraint programming},
year = {2016},
issue_date = {March 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {71},
number = {C},
issn = {0950-5849},
abstract = {Context: Testing highly-configurable software systems is challenging due to a large number of test configurations that have to be carefully selected in order to reduce the testing effort as much as possible, while maintaining high software quality. Finding the smallest set of valid test configurations that ensure sufficient coverage of the system's feature interactions is thus the objective of validation engineers, especially when the execution of test configurations is costly or time-consuming. However, this problem is NP-hard in general and approximation algorithms have often been used to address it in practice.Objective: In this paper, we explore an alternative exact approach based on constraint programming that will allow engineers to increase the effectiveness of configuration testing while keeping the number of configurations as low as possible.Method: Our approach consists in using a (time-aware) minimization algorithm based on constraint programming. Given the amount of time, our solution generates a minimized set of valid test configurations that ensure coverage of all pairs of feature values (a.k.a. pairwise coverage). The approach has been implemented in a tool called PACOGEN.Results: PACOGEN was evaluated on 224 feature models in comparison with the two existing tools that are based on a greedy algorithm. For 79% of 224 feature models, PACOGEN generated up to 60% fewer test configurations than the competitor tools. We further evaluated PACOGEN in the case study of an industrial video conferencing product line with a feature model of 169 features, and found 60% fewer configurations compared with the manual approach followed by test engineers. The set of test configurations generated by PACOGEN decreased the time required by test engineers in manual test configuration by 85%, increasing the feature-pairs coverage at the same time.Conclusion: Our experimental evaluation concluded that optimal time-aware minimization of pairwise-covering test configurations is efficiently addressed using constraint programming techniques.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {129–146},
numpages = {18},
keywords = {Constraint programming, Highly-configurable software systems, Variability testing}
}

@article{10.1007/s12650-020-00647-w,
author = {Chotisarn, Noptanit and Merino, Leonel and Zheng, Xu and Lonapalawong, Supaporn and Zhang, Tianye and Xu, Mingliang and Chen, Wei},
title = {A systematic literature review of modern software visualization},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {4},
issn = {1343-8875},
url = {https://doi.org/10.1007/s12650-020-00647-w},
doi = {10.1007/s12650-020-00647-w},
journal = {J. Vis.},
month = aug,
pages = {539–558},
numpages = {20},
keywords = {Software visualization, Systematic literature review, Information visualization}
}

@article{10.1016/j.cie.2010.12.010,
author = {Berger, Thierry and Sallez, Yves and Raileanu, Silviu and Tahon, Christian and Trentesaux, Damien and Borangiu, Theodor},
title = {Personal Rapid Transit in an open-control framework},
year = {2011},
issue_date = {September, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {61},
number = {2},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2010.12.010},
doi = {10.1016/j.cie.2010.12.010},
abstract = {Over the last decade, authorities have begun inquiring about the use of safe, comfortable, ecological vehicles for operation in an urban context as an alternative to private cars. Several on-demand transport projects have emerged with new automated vehicles known as cybercars or Personal Rapid Transit (PRT). Our state-of-the-art survey of the literature about automated On-Demand Transport (ODT) control solutions highlighted the desirability of a decentralized approach, although centralized approaches do have some advantages. In order to benefit from the advantages of both centralized/hierarchical and decentralized/heterarchical control approaches, we propose a new concept of control: open-control. In this paper, the context is intelligent transportation, where vehicles (e.g., PRTs) can be seen as autonomous decisional entities that are part of a transport system. In this context, the open-control concept is used to support two solutions to PRT routing with uncertainty and perturbations. This open-control concept, developed in our lab, exhibits the traditional explicit control, as well as an innovative type of control called implicit control, which allows system entities to be influenced via an Optimization Mechanism (OM). After introducing the open-control paradigm, we illustrate two applications of the implicit control of a PRT fleet, one based on a stigmergic method and the second based on an embedded version of the Dijkstra's algorithm. We present a real implementation of the second approach applied to an experimental PRT network. We describe our experimental platform for PRT control and report our first experimental results. These experiments clearly show the reactivity of the control faced with unpredictable events, such as path perturbation or dynamic insertion of PRT in the network.},
journal = {Comput. Ind. Eng.},
month = sep,
pages = {300–312},
numpages = {13},
keywords = {Dijkstra's algorithm, Intelligent transportation, Open-control, Personal Rapid Transit, Stigmergy}
}

@inproceedings{10.1007/978-3-030-30856-8_2,
author = {Andersson, Jesper and Grassi, Vincenzo and Mirandola, Raffaela and Perez-Palacin, Diego},
title = {A Distilled Characterization of Resilience and Its Embraced Properties Based on State-Spaces},
year = {2019},
isbn = {978-3-030-30855-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30856-8_2},
doi = {10.1007/978-3-030-30856-8_2},
abstract = {In recent years, we have observed the increasing interest in the system property resilience. We ascribe this increasing interest to the rapidly growing number of deployed, complex, socio-technical systems, which are facing uncertainty about changes they are expected to experience during their life-cycle and ways to deal with them. This paper contributes to current resilience research by focusing on the different definitions given for this system property, highlighting the risk that, using different terms in different communities, this contributes to create a “tower of Babel” problem, with the consequent difficulty in exchanging ideas and working together towards a common goal. We adopt an extended definition of dependability to define resilience. Based on that, we identify features of resilient systems, capture properties falling under the resilience umbrella, and define a conceptual framework for resilience characterization including how changes affect the system, strategies to design resilience, and discuss metrics for quantifying resilience at design and runtime.},
booktitle = {Software Engineering for Resilient Systems: 11th International Workshop, SERENE 2019, Naples, Italy, September 17, 2019, Proceedings},
pages = {11–25},
numpages = {15},
keywords = {Resilience, Conceptual framework, Strategies and metrics},
location = {Naples, Italy}
}

@article{10.1016/j.cie.2016.09.023,
author = {Addo-Tenkorang, Richard and Helo, Petri T.},
title = {Big data applications in operations/supply-chain management},
year = {2016},
issue_date = {November 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {101},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2016.09.023},
doi = {10.1016/j.cie.2016.09.023},
abstract = {Harnessing optimum value from industrial data increased in the last two decades.A detailed review of "big data" application in operations/SC management processes.Proposed (Value-adding - V5) framework for operation/SC management. PurposeBig data is increasingly becoming a major organizational enterprise force to reckon with in this global era for all sizes of industries. It is a trending new enterprise system or platform which seemingly offers more features for acquiring, storing and analysing voluminous generated data from various sources to obtain value-additions. However, current research reveals that there is limited agreement regarding the performance of "big data." Therefore, this paper attempts to thoroughly investigate "big data," its application and analysis in operations or supply-chain management, as well as the trends and perspectives in this research area. This paper is organized in the form of a literature review, discussing the main issues of "big data" and its extension into "big data II"/IoT-value-adding perspectives by proposing a value-adding framework. Methodology/research approachThe research approach employed is a comprehensive literature review. About 100 or more peer-reviewed journal articles/conference proceedings as well as industrial white papers are reviewed. Harzing Publish or Perish software was employed to investigate and critically analyse the trends and perspectives of "big data" applications between 2010 and 2015. Findings/resultsThe four main attributes or factors identified with "big data" include - big data development sources (Variety - V1), big data acquisition (Velocity - V2), big data storage (Volume - V3), and finally big data analysis (Veracity - V4). However, the study of "big data" has evolved and expanded a lot based on its application and implementation processes in specific industries in order to create value (Value-adding - V5) - "Big Data cloud computing perspective/Internet of Things (IoT)". Hence, the four Vs of "big data" is now expanded into five Vs. Originality/value of researchThis paper presents original literature review research discussing "big data" issues, trends and perspectives in operations/supply-chain management in order to propose "Big data II" (IoT - Value-adding) framework. This proposed framework is supposed or assumed to be an extension of "big data" in a value-adding perspective, thus proposing that "big data" be explored thoroughly in order to enable industrial managers and businesses executives to make pre-informed strategic operational and management decisions for increased return-on-investment (ROI). It could also empower organizations with a value-adding stream of information to have a competitive edge over their competitors.},
journal = {Comput. Ind. Eng.},
month = nov,
pages = {528–543},
numpages = {16},
keywords = {Big data - applications and analysis, Cloud computing, Internet of Things (IoT), Master database management, Operations/supply-chain management}
}

@article{10.1007/s10664-015-9418-0,
author = {Li, Yan and Yue, Tao and Ali, Shaukat and Zhang, Li},
title = {Zen-ReqOptimizer: a search-based approach for requirements assignment optimization},
year = {2017},
issue_date = {February  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9418-0},
doi = {10.1007/s10664-015-9418-0},
abstract = {At early phases of a product development lifecycle of large scale Cyber-Physical Systems (CPSs), a large number of requirements need to be assigned to stakeholders from different organizations or departments of the same organization for review, clarification and checking their conformance to standards and regulations. These requirements have various characteristics such as extents of importance to the organization, complexity, and dependencies between each other, thereby requiring different effort (workload) to review and clarify. While working with our industrial partners in the domain of CPSs, we discovered an optimization problem, where an optimal solution is required for assigning requirements to various stakeholders by maximizing their familiarity to assigned requirements, meanwhile balancing the overall workload of each stakeholder. In this direction, we propose a fitness function that takes into account all the above-mentioned factors to guide a search algorithm to find an optimal solution. As a pilot experiment, we first investigated four commonly applied search algorithms (i.e., GA, (1 + 1) EA, AVM, RS) together with the proposed fitness function and results show that (1 + 1) EA performs significantly better than the other algorithms. Since our optimization problem is multi-objective, we further empirically evaluated the performance of the fitness function with six multi-objective search algorithms (CellDE, MOCell, NSGA-II, PAES, SMPSO, SPEA2) together with (1 + 1) EA (the best in the pilot study) and RS (as the baseline) in terms of finding an optimal solution using an real-world case study and 120 artificial problems of varying complexity. Results show that both for the real-world case study and the artificial problems (1 + 1) EA achieved the best performance for each single objective and NSGA-II achieved the best performance for the overall fitness. NSGA-II has the ability to solve a wide range of problems without having their performance degraded significantly and (1 + 1) EA is not fit for problems with less than 250 requirements Therefore we recommend that, if a project manager is interested in a particular objective then (1 + 1) EA should be used; otherwise, NSGA-II should be applied to obtain optimal solutions when putting the overall fitness as the first priority.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {175–234},
numpages = {60},
keywords = {Optimization and empirical evaluation, Requirements assignment, Search based software engineering}
}

@article{10.1016/j.infsof.2019.05.009,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh and Marston, Chad},
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.009},
doi = {10.1016/j.infsof.2019.05.009},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {131–145},
numpages = {15},
keywords = {Big data, Machine learning, Business-to-business, User trust, Case study}
}

@article{10.1016/j.infsof.2016.11.007,
author = {Ouni, Ali and Kula, Raula Gaikovina and Kessentini, Marouane and Ishio, Takashi and German, Daniel M. and Inoue, Katsuro},
title = {Search-based software library recommendation using multi-objective optimization},
year = {2017},
issue_date = {March 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {83},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.11.007},
doi = {10.1016/j.infsof.2016.11.007},
abstract = {Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers.Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find "useful" third-party libraries to the implementation of their software systems.Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries.Results: We evaluated our approach on 6083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92%, precision of 51% and recall of 68%, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5.Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {55–75},
numpages = {21},
keywords = {Multi-objective optimization, Search-based software engineering, Software library, Software reuse}
}

@inproceedings{10.5555/2487336.2487350,
author = {Pascual, Gustavo G. and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Run-time adaptation of mobile applications using genetic algorithms},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {Mobile applications run in environments where the context is continuously changing. Therefore, it is necessary to provide support for the run-time adaptation of these applications. This support is usually achieved by middleware platforms that offer a context-aware dynamic reconfiguration service. However, the main shortcoming of existing approaches is that both the list of possible configurations and the plans to adapt the application to a new configuration are usually specified at design-time. In this paper we present an approach that allows the automatic generation at run-time of application configurations and of reconfiguration plans. Moreover, the generated configurations are optimal regarding the provided functionality and, more importantly, without exceeding the available resources (e.g. battery). This is performed by: (1) having the information about the application variability available at runtime using feature models, and (2) using a genetic algorithm that allows generating an optimal configuration at runtime. We have specified a case study and evaluated our approach, and the results show that it is efficient enough as to be used on mobile devices without introducing an excessive overhead.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {73–82},
numpages = {10},
location = {San Francisco, CA, USA},
series = {SEAMS '13}
}

@inproceedings{10.1145/2483760.2492396,
author = {Th\"{u}m, Thomas},
title = {Product-line verification with feature-oriented contracts},
year = {2013},
isbn = {9781450321594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2483760.2492396},
doi = {10.1145/2483760.2492396},
abstract = {Software product lines allow programmers to reuse code across similar software products. Software products are decomposed into separate modules representing user-visible features. Based on a selection of desired features, a customized software product can be generated automatically. However, these reuse mechanisms challenge existing techniques for specification and verification of software. Specifying and verifying each product involves redundant steps, and is often infeasible. We discuss how method contracts (i.e., preconditions and postconditions) can be used to efficiently specify and verify product lines.},
booktitle = {Proceedings of the 2013 International Symposium on Software Testing and Analysis},
pages = {374–377},
numpages = {4},
keywords = {Java Modeling Language, Software product lines, design by contract, feature-oriented programming, verification},
location = {Lugano, Switzerland},
series = {ISSTA 2013}
}

@inproceedings{10.5555/1964571.1964592,
author = {Gonz\'{a}lez, Sebasti\'{a}n and Cardozo, Nicol\'{a}s and Mens, Kim and C\'{a}diz, Alfredo and Libbrecht, Jean-Christophe and Goffaux, Julien},
title = {Subjective-C: bringing context to mobile platform programming},
year = {2010},
isbn = {9783642194399},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Thanks to steady advances in hardware, mobile computing platforms are nowadays much more connected to their physical and logical environment than ever before. To ease the construction of adaptable applications that are smarter with respect to their execution environment, the context-oriented programming paradigm has emerged. However, up until now there has been no proof that this emerging paradigm can be implemented and used effectively on mobile devices, probably the kind of platform which is most subject to dynamically changing contexts. In this paper we study how to effectively realise core context-oriented abstractions on top of Objective-C, a mainstream language for mobile device programming. The result is Subjective-C, a language which goes beyond existing context-oriented languages by providing a rich encoding of context interdependencies. Our initial validation cases and efficiency benchmarks make us confident that context-oriented programming can become mainstream in mobile application development.},
booktitle = {Proceedings of the Third International Conference on Software Language Engineering},
pages = {246–265},
numpages = {20},
location = {Eindhoven, The Netherlands},
series = {SLE'10}
}

@article{10.1016/j.jss.2014.11.051,
author = {Albuquerque, Diego and Cafeo, Bruno and Garcia, Alessandro and Barbosa, Simone and Abrah\~{a}o, Silvia and Ribeiro, Ant\'{o}nio},
title = {Quantifying usability of domain-specific languages},
year = {2015},
issue_date = {March 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {101},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.11.051},
doi = {10.1016/j.jss.2014.11.051},
abstract = {A usability measurement framework for DSLs was developed based on the cognitive dimensions of notations.With our proposed metrics was possible to identify DSL usability limitations.Our metrics suite revealed specific DSL features favoring maintenance tasks.Eight critical DSL usability dimensions based on the CDN framework were analyzed in this study. A domain-specific language (DSL) aims to support software development by offering abstractions to a particular domain. It is expected that DSLs improve the maintainability of artifacts otherwise produced with general-purpose languages. However, the maintainability of the DSL artifacts and, hence, their adoption in mainstream development, is largely dependent on the usability of the language itself. Unfortunately, it is often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify beforehand by DSL stakeholders. There is even less support on how to quantitatively evaluate the usability of DSLs used in maintenance tasks. In this context, this paper reports a study to compare the usability of textual DSLs under the perspective of software maintenance. A usability measurement framework was developed based on the cognitive dimensions of notations. The framework was evaluated both qualitatively and quantitatively using two DSLs in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify DSL usability limitations, (2) to reveal specific DSL features favoring maintenance tasks, and (3) to successfully analyze eight critical DSL usability dimensions.},
journal = {J. Syst. Softw.},
month = mar,
pages = {245–259},
numpages = {15},
keywords = {DSL, Metrics, Usability}
}

@inproceedings{10.1145/2000259.2000285,
author = {Galster, Matthias and Avgeriou, Paris},
title = {Empirically-grounded reference architectures: a proposal},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000285},
doi = {10.1145/2000259.2000285},
abstract = {A reference architecture describes core elements of the software architecture for systems that stem from the same domain. A reference architecture ensures interoperability of systems through standardization. It also facilitates the instantiation of new concrete architectures. However, we currently lack procedures for systematically designing reference architectures that are empirically-grounded. Being empirically-grounded would increase the validity and reusability of a reference architecture. We therefore present an approach which helps systematically design reference architectures. Our approach consists of six steps performed by the software architect and domain experts. It helps design reference architectures either from scratch, or based on existing architecture artifacts. We also illustrate how our approach could be applied to the design of two existing reference architectures found in literature.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {153–158},
numpages = {6},
keywords = {design process, empirically-grounded, reference architecture, software architecture},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.5555/648114.748901,
author = {Northrop, Linda M. and Bachmann, Felix and Due\~{n}as, Juan C.},
title = {Report on Discussion Sessions "Diversity Solutions" and "Light-Weight Processes"},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This document reports about the papers presented in the sessions "Diversity solutions" and "Light-weight processes", as well as the discussion session. The main results, conclusions and open points in the sessions are included.},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {258–263},
numpages = {6},
series = {PFE '01}
}

@inproceedings{10.1007/978-3-642-33944-8_6,
author = {Trejo, Natalia and Casas, Sandra and Hallar, Karim},
title = {A feature-oriented WSDL extension for describing grid services},
year = {2011},
isbn = {9783642339431},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33944-8_6},
doi = {10.1007/978-3-642-33944-8_6},
abstract = {Grid computing and Feature-oriented Development Software are emerging technologies, which can be combined to analyze, model, and specify Grid services. In a Grid environment, there are a large number of similar resources provided by different parties, that may provide the same functionality, but different Quality of Service (QoS) measures. A feature-based approach is presented to optimize the development of Grid services and Grid service composition. WSDL specification is extended to contain useful description of both functional and non-functional characteristics by mean Design by Contract technique. In this way, Grid users can specify their QoS expectations and select suitable resources and use them for their Grid workflow at design time before its execution on the Grid.},
booktitle = {Proceedings of the Second International Conference on Human-Computer Interaction, Tourism and Cultural Heritage},
pages = {64–72},
numpages = {9},
keywords = {QoS attributes, design by contract, feature-oriented software development, grid service, grid service composition},
location = {C\'{o}rdoba, Argentina},
series = {HCITOCH'11}
}

@inproceedings{10.1145/1023833.1023847,
author = {Beltrame, Giovanni and Palermo, Gianluca and Sciuto, Donatella and Silvano, Cristina},
title = {Plug-in of power models in the StepNP exploration platform: analysis of power/performance trade-offs},
year = {2004},
isbn = {1581138903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1023833.1023847},
doi = {10.1145/1023833.1023847},
abstract = {In this paper, we propose a power/performance estimation layer designed for StepNP, a system-level architecture simulation and exploration platform for Network Processors and Multi-Processor Systems-on-Chip (MP-SoCs). The first goal of our work is to plug-in PIRATE, a parameterizable Network on-Chip in the StepNP platform, to support a fast exploration of on-chip interconnection networks. Up to now, StepNP does not provide any energy profiling, so our second goal is to dynamically plug-in power models of the different system components to provide power estimates quickly. The proposed power/performance exploration framework is based on a power characterization methodology and a system-level simulator to dynamically profile the given network application. This framework is intended to be used at different levels of the design, considering several levels of accuracy and taking full advantage of the StepNP performance profiling features. Experimental results are provided for the exploration of an ARM-based MP-SOC including a configurable NoC-IP executing an IPv4 forwarding application.},
booktitle = {Proceedings of the 2004 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems},
pages = {85–92},
numpages = {8},
keywords = {low-power design, multiprocessor, network on chip, platform based design},
location = {Washington DC, USA},
series = {CASES '04}
}

@inproceedings{10.5555/1926458.1926464,
author = {Saxena, Tripti and Karsai, Gabor},
title = {MDE-based approach for generalizing design space exploration},
year = {2010},
isbn = {3642161448},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Design Space Exploration (DSE) is the exploration of design alternatives before the implementation. Existing DSE frameworks are domain-specific where the representation, evaluation method as well as exploration algorithm are tightly coupled with domain-dependent assumptions. Although the tasks involved in DSE are similar, the inflexibility of the existing frameworks restricts their reuse for solving DSE problems from other domains.This paper presents an MDE-based approach for generalizing DSE techniques. The framework supports a reconfigurable representation of a design space, which is decoupled from exploration algorithm. The framework can be configured to solve DSE problems from different domains and enables the designer to experiment with different approaches to solve the same problem with minimum effort. The main contributions of this framework are: (1) rapid modeling of DSE problems, (2) reuse of previously defined artifacts, (3) multiple solver support and (4) a tool for scalability study.},
booktitle = {Proceedings of the 13th International Conference on Model Driven Engineering Languages and Systems: Part I},
pages = {46–60},
numpages = {15},
keywords = {design space exploration, domain-specific modeling languages},
location = {Oslo, Norway},
series = {MODELS'10}
}

@article{10.1016/j.jss.2009.08.032,
author = {Tang, Antony and Avgeriou, Paris and Jansen, Anton and Capilla, Rafael and Ali Babar, Muhammad},
title = {A comparative study of architecture knowledge management tools},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.08.032},
doi = {10.1016/j.jss.2009.08.032},
abstract = {Recent research suggests that architectural knowledge, such as design decisions, is important and should be recorded alongside the architecture description. Different approaches have emerged to support such architectural knowledge (AK) management activities. However, there are different notions of and emphasis on what and how architectural activities should be supported. This is reflected in the design and implementation of existing AK tools. To understand the current status of software architecture knowledge engineering and future research trends, this paper compares five architectural knowledge management tools and the support they provide in the architecture life-cycle. The comparison is based on an evaluation framework defined by a set of 10 criteria. The results of the comparison provide insights into the current focus of architectural knowledge management support, their advantages, deficiencies, and conformance to the current architectural description standard. Based on the outcome of this comparison a research agenda is proposed for future work on AK tools.},
journal = {J. Syst. Softw.},
month = mar,
pages = {352–370},
numpages = {19},
keywords = {Architectural design, Architectural knowledge management tool, Design rationale}
}

@article{10.5555/2594638.2594643,
author = {Hotz, Lothar and Wolter, Katharina},
title = {Beyond physical product configuration --Configuration in unusual domains},
year = {2013},
issue_date = {January 2013},
publisher = {IOS Press},
address = {NLD},
volume = {26},
number = {1},
issn = {0921-7126},
abstract = {Configuration technologies are typically applied in domains with physical products. In this article, we determine characteristics of configuration technologies that are used to compose non-pure physical products. Starting from two case studies software-intensive systems and scene interpretation where we successfully applied configuration, we determine some characteristics of knowledge representation languages and configuration systems that enable to solve configuration tasks in domains beyond pure physical products. As such, the article provides thinking outside the box of physical product configuration.},
journal = {AI Commun.},
month = jan,
pages = {39–66},
numpages = {28},
keywords = {Knowledge Representation, Knowledge-Based Configuration, Scene Interpretation, Software-Intensive Systems}
}

@article{10.4018/jossp.2011010101,
author = {Raza, Arif and Capretz, Luiz Fernando and Ahmed, Faheem},
title = {An Empirical Study of Open Source Software Usability: The Industrial Perspective},
year = {2011},
issue_date = {January 2011},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/jossp.2011010101},
doi = {10.4018/jossp.2011010101},
abstract = {Recent years have seen a sharp increase in the use of open source projects by common novice users; Open Source Software OSS is thus no longer a reserved arena for software developers and computer gurus. Although user-centered designs are gaining popularity in OSS, usability is still not considered one of the prime objectives in many design scenarios. This paper analyzes industry users' perception of usability factors, including understandability, learnability, operability, and attractiveness on OSS usability. The research model of this empirical study establishes the relationship between the key usability factors and OSS usability from industrial perspective. In order to conduct the study, a data set of 105 industry users is included. The results of the empirical investigation indicate the significance of the key factors for OSS usability.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {1–16},
numpages = {16},
keywords = {Empirical Study, Industry, Open Source Software OSS, Usability, Users}
}

@article{10.1007/s10664-014-9318-8,
author = {Siegmund, Janet and Schumann, Jana},
title = {Confounding parameters on program comprehension: a literature survey},
year = {2015},
issue_date = {August    2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9318-8},
doi = {10.1007/s10664-014-9318-8},
abstract = {Program comprehension is an important human factor in software engineering. To measure and evaluate program comprehension, researchers typically conduct experiments. However, designing experiments requires considerable effort, because confounding parameters need to be controlled for. Our aim is to support researchers in identifying relevant confounding parameters and select appropriate techniques to control their influence. To this end, we conducted a literature survey of 13 journals and conferences over a time span of 10 years. As result, we created a catalog of 39 confounding parameters, including an overview of measurement and control techniques. With the catalog, we give experimenters a tool to design reliable and valid experiments.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1159–1192},
numpages = {34},
keywords = {Confounding parameters, Controlled experiment, Empirical research, Program comprehension}
}

@inproceedings{10.1145/2897053.2897062,
author = {McGee, Ethan T. and McGregor, John D.},
title = {Using dynamic adaptive systems in safety-critical domains},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897062},
doi = {10.1145/2897053.2897062},
abstract = {The development of safety-critical Cyber-Physical Systems (CPS) is expanding due to the Internet of Things' promise to make high-integrity applications and services part of everyday life. This expansion is seen in the dependencies some connected vehicles have on cloud services that provide guidance and accident avoidance / detection features. Such systems are safety-critical since failure could result in serious injury or death. Due to the severe consequences of failure, fault-tolerance, reliability and dependability should be primary driving qualities in the design and development of these systems. However, the cost of the analysis, evaluation and certification activities needed to ensure that the possibility of failure has been sufficiently mitigated is significantly higher than the cost of developing traditional software.Our group is exploring the addition of dynamic adaptive capabilities to safety-critical systems. We postulate that dynamic adaptivity could provide several enhancements to safety-critical systems. It would allow systems to reason about the environment within which they are sited and about their internal operation enabling decision making that is context-specific and appropriately prioritized. However, the addition of adaptivity with the associated overhead of reasoning is not without drawbacks particularly when hard real-time safety-critical systems are involved. In this brief position paper, we explore some of the questions and concerns that are raised when dynamic adaptive behavior is introduced into safety-critical systems as well as ways that the Architecture Analysis &amp; Design Language (AADL) can be used to model / analyze such systems.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {115–121},
numpages = {7},
keywords = {dynamic adaptive systems, dynamic software product lines, safety critical systems, software product lines},
location = {Austin, Texas},
series = {SEAMS '16}
}

@article{10.1145/2815169.2815171,
author = {Medina, Jonathas Leontino and Cagnin, Maria Istela and Paiva, D\'{e}bora Maria Barroso},
title = {Investigating accessibility on web-based maps},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/2815169.2815171},
doi = {10.1145/2815169.2815171},
abstract = {This paper presents results of an accessibility evaluation carried out with web-based map applications. Three points of view were considered: experts on accessibility, evaluation tools and final users (partially or totally blind people). The document WCAG 2.0 (Web Content Accessibility Guidelines) provided us with guidelines for evaluation and GQM (Goal, Question and Metric) approach was used to define and set measurable goals. A number of problems was identified and none of the evaluated applications entirely meet the analyzed criteria.},
journal = {SIGAPP Appl. Comput. Rev.},
month = aug,
pages = {17–26},
numpages = {10},
keywords = {WCAG 2.0, accessibility evaluation, web accessibility, web-based maps}
}

@inproceedings{10.1145/1031607.1031622,
author = {\v{C}ubraniundefined, Davor and Murphy, Gail C. and Singer, Janice and Booth, Kellogg S.},
title = {Learning from project history: a case study for software development},
year = {2004},
isbn = {1581138105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031607.1031622},
doi = {10.1145/1031607.1031622},
abstract = {The lack of lightweight communication channels and other technical and sociological difficulties make it hard for new members of a non-collocated software development team to learn effectively from their more experienced colleagues while they are coming up-to-speed on a project. To address this situation, we have developed a tool, named Hipikat, that provides developers with efficient and effective access to the group memory for a software development project that is implicitly formed by all of the artifacts produced during the development. This &lt;i&gt;project memory&lt;/i&gt; is built automatically with little or no change to existing work practices. We report an exploratory case study evaluating whether software developers who are new to a project can benefit from the artifacts that Hipikat recommends from the project memory. To assess the appropriateness of the recommendations, we investigated when and how developers queried the project memory, how the evaluated the recommended artifacts, and the process by which they utilized the artifacts. We found that newcomers did use the recommendations and their final solutions exploited the recommended artifacts, although most of the Hipikat queries came in the early stages of a change task. We describe the case study, present qualitative observations, and suggest implications of using project memory as a learning aid for project newcomers.},
booktitle = {Proceedings of the 2004 ACM Conference on Computer Supported Cooperative Work},
pages = {82–91},
numpages = {10},
keywords = {project memory, recommender system, software artifacts, software development teams, user studies},
location = {Chicago, Illinois, USA},
series = {CSCW '04}
}

@article{10.1109/TASLP.2015.2442415,
author = {Fujioka, Toyota and Nagata, Yoshifumi and Abe, Masato},
title = {High-precision harmonic distortion level measurement of a loudspeaker using adaptive filters in a noisy environment},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2442415},
doi = {10.1109/TASLP.2015.2442415},
abstract = {The harmonic distortion level (HDL), an important criterion used to evaluate loudspeaker performance, can usually be measured using spectral analysis. However, spectral analysis entails great computational complexity. Furthermore, long measurement times are necessary to measure HDL accurately in a noisy environment. Therefore, we proposed the HDL measurement technique for a loudspeaker using an adaptive filter. This measurement technique can measure HDL as accurately as spectral analysis using fast Fourier transform (FFT). Furthermore, its computational complexity is much less than that of spectral analysis. Nevertheless, this measurement technique requires a long measurement time to assess the convergence of filter coefficients. This paper presents a description of the new HDL measurement technique of a loudspeaker using plural adaptive filters. The proposed measurement technique can measure HDL more accurately than spectral analysis in a noisy environment. We evaluated the performance of the proposed measurement technique using computer simulations. Results based on computer simulations show that the proposed measurement technique is more effective than spectral analysis in an acoustic environment with background noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1613–1622},
numpages = {10},
keywords = {adaptive filter, distortion level measurement, harmonic distortion, loudspeaker}
}

@article{10.1007/s00165-021-00563-2,
author = {Cordy, Maxime and Lazreg, Sami and Papadakis, Mike and Legay, Axel},
title = {Statistical model checking for variability-intensive systems: applications to bug detection and minimization},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00563-2},
doi = {10.1007/s00165-021-00563-2},
abstract = {We propose a new Statistical Model Checking (SMC) method to
identify bugs in variability-intensive systems (VIS). The
state-space of such systems is exponential in the number of
variants, which makes the verification problem harder than for
classical systems. To reduce verification time, we propose to
combine SMC with featured transition systems (FTS)—a
model that represents jointly the state spaces of all variants. Our
new methods allow the sampling of executions from one or more
(potentially all) variants. We investigate their utility in two
complementary use cases. The first case considers the problem of
finding all variants that violate a given property expressed in
Linear-Time Logic (LTL) within a given simulation budget. To achieve
this, we perform random walks in the featured transition system
seeking accepting lassos. We show that our method allows us to find
bugs much faster (up to 16 times according to our experiments) than
exhaustive methods. As any simulation-based approach, however, the
risk of Type-1 error exists. We provide a lower bound and an upper
bound for the number of simulations to perform to achieve the
desired level of confidence. Our empirical study involving 59
properties over three case studies reveals that our method manages
to discover all variants violating 41 of the properties.
This indicates that SMC can act as a coarse-grained
analysis method to quickly identify the set of buggy variants.
The second case complements the first one. In case the
coarse-grained analysis reveals that no variant can guarantee to
satisfy an intended property in all their executions, one should
identify the variant that minimizes the probability of violating
this property. Thus, we propose a fine-grained SMC method that
quickly identifies promising variants and accurately estimates their
violation probability. We evaluate different selection strategies
and reveal that a genetic algorithm combined with elitist selection
yields the best results.},
journal = {Form. Asp. Comput.},
month = dec,
pages = {1147–1172},
numpages = {26},
keywords = {Statistical model checking, Variability, Verification, Simulation, Sampling}
}

@inbook{10.5555/1985596.1985604,
author = {Brennan, Shane and Fritsch, Serena and Liu, Yu and Sterritt, Ashley and Fox, Jorge and Linehan, \'{E}amonn and Driver, Cormac and Meier, Ren\'{e} and Cahill, Vinny and Harrison, William and Clarke, Siobh\'{a}n},
title = {A framework for flexible and dependable service-oriented embedded systems},
year = {2010},
isbn = {364217244X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The continued development and deployment of distributed, real-time embedded systems technologies in recent years has resulted in a multitude of ecosystems in which service-oriented embedded systems can now be realised. Such ecosystems are often exposed to dynamic changes in user requirements, environmental conditions and network topologies that require service-oriented embedded systems to evolve at runtime. This paper presents a framework for service-oriented embedded systems that can dynamically adapt to changing conditions at runtime. Supported by model-driven development techniques, the framework facilitates lightweight dynamic service composition in embedded systems while predicting the temporal nature of unforeseen service assemblies and coping with adverse feature interactions following dynamic service composition. This minimises the complexity of evolving software where services are deployed dynamically and ultimately, enables flexible and dependable service-oriented embedded systems.},
booktitle = {Architecting Dependable Systems VII},
pages = {123–145},
numpages = {23}
}

@inproceedings{10.1145/2060329.2060362,
author = {Saxena, Tripti and Karsai, Gabor},
title = {The GDSE framework: a meta-tool for automated design space exploration},
year = {2010},
isbn = {9781450305495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2060329.2060362},
doi = {10.1145/2060329.2060362},
abstract = {Existing Design Space Exploration (DSE) frameworks are tailored specifically to a particular problem domain and cannot be easily re-used between domains. Typically these frameworks translate the DSE problem to a single formulation of the problem (e.g. ILP or CSP) and then solve it to retrieve satisfying alternatives. In order to compare the efficiency of different formulations/techniques on a given problem, the domain-expert has to manually reformulate the problem in another constraint language, which is time-consuming. In order to overcome this lack of reusability and flexibility in the current frameworks, we present here the Generic Design Space Exploration (GDSE) framework that allows the designer to solve DSE problems from different domains. Rather than using one strict formulation of the design problem, the framework supports a higher level formulation that can be mapped to different low level encodings. The main contributions of this framework are: 1) a generic representation which can be used to express any DSE problem, and 2) a flexible exploration technique which supports several exploration techniques.},
booktitle = {Proceedings of the 10th Workshop on Domain-Specific Modeling},
articleno = {15},
numpages = {6},
location = {Reno, Nevada},
series = {DSM '10}
}

@article{10.1007/s11042-018-5980-y,
author = {Krismayer, Thomas and Schedl, Markus and Knees, Peter and Rabiser, Rick},
title = {Predicting user demographics from music listening information},
year = {2019},
issue_date = {Feb 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {3},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-5980-y},
doi = {10.1007/s11042-018-5980-y},
abstract = {Online activities such as social networking, online shopping, and consuming multi-media create digital traces, which are often analyzed and used to improve user experience and increase revenue, e. g., through better-fitting recommendations and more targeted marketing. Analyses of digital traces typically aim to find user traits such as age, gender, and nationality to derive common preferences. We investigate to which extent the music listening habits of users of the social music platform Last.fm can be used to predict their age, gender, and nationality. We propose a feature modeling approach building on Term Frequency-Inverse Document Frequency (TF-IDF) for artist listening information and artist tags combined with additionally extracted features. We show that we can substantially outperform a baseline majority voting approach and can compete with existing approaches. Further, regarding prediction accuracy vs. available listening data we show that even one single listening event per user is enough to outperform the baseline in all prediction tasks. We also compare the performance of our algorithm for different user groups and discuss possible prediction errors and how to mitigate them. We conclude that personal information can be derived from music listening information, which indeed can help better tailoring recommendations, as we illustrate with the use case of a music recommender system that can directly utilize the user attributes predicted by our algorithm to increase the quality of it's recommendations.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {2897–2920},
numpages = {24},
keywords = {Digital user traces, Music listening habits, User demographics, User trait prediction}
}

@article{10.1007/s10664-013-9263-y,
author = {Bjarnason, Elizabeth and Runeson, Per and Borg, Markus and Unterkalmsteiner, Michael and Engstr\"{o}m, Emelie and Regnell, Bj\"{o}rn and Sabaliauskaite, Giedre and Loconsole, Annabella and Gorschek, Tony and Feldt, Robert},
title = {Challenges and practices in aligning requirements with verification and validation: a case study of six companies},
year = {2014},
issue_date = {December  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9263-y},
doi = {10.1007/s10664-013-9263-y},
abstract = {Weak alignment of requirements engineering (RE) with verification and validation (VV) may lead to problems in delivering the required products in time with the right quality. For example, weak communication of requirements changes to testers may result in lack of verification of new requirements and incorrect verification of old invalid requirements, leading to software quality problems, wasted effort and delays. However, despite the serious implications of weak alignment research and practice both tend to focus on one or the other of RE or VV rather than on the alignment of the two. We have performed a multi-unit case study to gain insight into issues around aligning RE and VV by interviewing 30 practitioners from 6 software developing companies, involving 10 researchers in a flexible research process for case studies. The results describe current industry challenges and practices in aligning RE with VV, ranging from quality of the individual RE and VV activities, through tracing and tools, to change control and sharing a common understanding at strategy, goal and design level. The study identified that human aspects are central, i.e. cooperation and communication, and that requirements engineering practices are a critical basis for alignment. Further, the size of an organisation and its motivation for applying alignment practices, e.g. external enforcement of traceability, are variation factors that play a key role in achieving alignment. Our results provide a strategic roadmap for practitioners improvement work to address alignment challenges. Furthermore, the study provides a foundation for continued research to improve the alignment of RE with VV.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {1809–1855},
numpages = {47},
keywords = {Alignment, Case study, Requirements engineering, Testing, Validation, Verification}
}

@article{10.1145/99902.99903,
author = {Kamada, Tomihisa and Kawai, Satoru},
title = {A general framework for visualizing abstract objects and relations},
year = {1991},
issue_date = {Jan. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/99902.99903},
doi = {10.1145/99902.99903},
abstract = {Pictorial representations significantly enhance our ability to understand complicated relations and structures, which means that information systems strongly require user interfaces that support the visualization of many kinds of information with a wide variety of graphical forms. At present, however, these difficult visualization problems have not been solved. We present a visualization framework for translating abstract objects and relations, typically represented in textual forms, into pictorial representations, and describe a general visualization interface based on this framework. In our framework, abstract objects and relations are mapped to graphical objects and relations by user-defined mapping rules. The kernel of our visualization process is to determine a layout of  graphical objects under geometric constraints. A constraint-based object layout system named COOL has been developed to handle this layout problem. COOL introduces the concept of rigidity of constraints in order to reasonably handle, a set of conflicting constraints by use of the least squares method. As applications of our system, we show the generation of kinship diagrams, list diagrams, Nassi-Shneiderman diagrams, and entity-relationship diagrams.},
journal = {ACM Trans. Graph.},
month = jan,
pages = {1–39},
numpages = {39}
}

@article{10.1016/j.is.2006.09.003,
author = {Oberweis, Andreas and Pankratius, Victor and Stucky, Wolffried},
title = {Product lines for digital information products},
year = {2007},
issue_date = {September, 2007},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {32},
number = {6},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2006.09.003},
doi = {10.1016/j.is.2006.09.003},
abstract = {The growth of the Web has fueled the creation, storage, and exchange of digital information products (DIPs), whose main purpose is the delivery of information, entertainment, education, or training. Very often, after their initial creation, the growing amount of content also leads to a more complicated maintenance, since updates typically occur rather often and the potential variability of modifications is not limited in advance. Moreover, commonalities between different parts of similar information products are not exploited, which often leads to redundancy. At the moment, there is hardly any attempt to compose information products from different sources and to produce more complex information products in a coordinated way. To help remedy this situation, this paper introduces the Product Lines for digitAl iNformation producTs (PLANT) approach, which applies the concept of software product lines to DIPs. The PLANT approach explicitly manages the commonalities of similar DIPs by defining common requirements, limiting variability in advance, as well as planning and coordinating reuse. This article focuses on the modeling of such product lines. In particular, the developed general concepts will be exemplified throughout the paper in the area of e-learning, in which DIPs play an important role. The application of PLANT in other areas and an implemented tool that supports the creation of information products in a product line are outlined as well.},
journal = {Inf. Syst.},
month = sep,
pages = {909–939},
numpages = {31},
keywords = {Digital information products, Digital products, Feature models, Software product lines, Workflow management, e-learning}
}

@inproceedings{10.1145/3430984.3431022,
author = {Virk, Jitender Singh and Bathula, Deepti R.},
title = {Domain-Specific, Semi-Supervised Transfer Learning for Medical Imaging},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431022},
doi = {10.1145/3430984.3431022},
abstract = {Limited availability of annotated medical imaging data poses a challenge for deep learning algorithms. Although transfer learning minimizes this hurdle in general, knowledge transfer across disparate domains is shown to be less effective. On the other hand, smaller architectures were found to be more compelling in learning better features. Consequently, we propose a lightweight architecture that uses mixed asymmetric kernels (MAKNet) to reduce the number of parameters significantly. Additionally, we train the proposed architecture using semi-supervised learning to provide pseudo-labels for a large medical dataset to assist with transfer learning. The proposed MAKNet provides better classification performance with fewer parameters than popular architectures. Experimental results also highlight the importance of domain-specific knowledge for effective transfer learning. Additionally, we interrogate the proposed network with integrated gradients and perturbation methods to establish the superior quality of the learned features.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {145–153},
numpages = {9},
keywords = {CT scans, Neural networks, Semi-supervised learning, domain-specific, image perturbations, integrated gradients, mixed-kernels neural networks, pseudo-labelling, transfer learning},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@inproceedings{10.1145/3022636.3022649,
author = {Syed, Madiha H. and Fernandez, Eduardo B. and Ilyas, Mohammad},
title = {A Pattern for Fog Computing},
year = {2016},
isbn = {9781450342001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022636.3022649},
doi = {10.1145/3022636.3022649},
abstract = {Fog Computing is a new variety of the cloud computing paradigm that brings virtualized cloud services to the edge of the network to control the devices in the IoT. We present a pattern for fog computing which describes its architecture, including its computing, storage and networking services. Fog computing is implemented as an intermediate platform between end devices and cloud computing data centers. The recent popularity of the Internet of Things (IoT) has made fog computing a necessity to handle a variety of devices. It has been recognized as an important platform to provide efficient, location aware, close to the edge, cloud services. Our model includes most of the functionality found in current fog architectures.},
booktitle = {Proceedings of the 10th Travelling Conference on Pattern Languages of Programs},
articleno = {13},
numpages = {10},
keywords = {Cloud computing, Fog computing, Internet of Things, Patterns, Security, Software architecture, architecture patterns, edge computing},
location = {Leerdam, AA, Netherlands},
series = {VikingPLoP '16}
}

@article{10.1145/2597999,
author = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
title = {Adaptive Model-Driven User Interface Development Systems},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2597999},
doi = {10.1145/2597999},
abstract = {Adaptive user interfaces (UIs) were introduced to address some of the usability problems that plague many software applications. Model-driven engineering formed the basis for most of the systems targeting the development of such UIs. An overview of these systems is presented and a set of criteria is established to evaluate the strengths and shortcomings of the state of the art, which is categorized under architectures, techniques, and tools. A summary of the evaluation is presented in tables that visually illustrate the fulfillment of each criterion by each system. The evaluation identified several gaps in the existing art and highlighted the areas of promising improvement.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {9},
numpages = {33},
keywords = {Adaptive user interfaces, model-driven engineering}
}

@article{10.1016/j.eswa.2020.113808,
author = {Mohsin, Hufsa and Shi, Chongyang},
title = {SPBC: A self-paced learning model for bug classification from historical repositories of open-source software},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113808},
doi = {10.1016/j.eswa.2020.113808},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {15},
keywords = {Bug triaging, Defect localization, Self-paced learning, Bug report analysis, Bug classification}
}

@article{10.1145/1516533.1516538,
author = {Salehie, Mazeiar and Tahvildari, Ladan},
title = {Self-adaptive software: Landscape and research challenges},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/1516533.1516538},
doi = {10.1145/1516533.1516538},
abstract = {Software systems dealing with distributed applications in changing environments normally require human supervision to continue operation in all conditions. These (re-)configuring, troubleshooting, and in general maintenance tasks lead to costly and time-consuming procedures during the operating phase. These problems are primarily due to the open-loop structure often followed in software development. Therefore, there is a high demand for management complexity reduction, management automation, robustness, and achieving all of the desired quality requirements within a reasonable cost and time range during operation. Self-adaptive software is a response to these demands; it is a closed-loop system with a feedback loop aiming to adjust itself to changes during its operation. These changes may stem from the software system's self (internal causes, e.g., failure) or context (external events, e.g., increasing requests from users). Such a system is required to monitor itself and its context, detect significant changes, decide how to react, and act to execute such decisions. These processes depend on adaptation properties (called self-* properties), domain characteristics (context information or models), and preferences of stakeholders. Noting these requirements, it is widely believed that new models and frameworks are needed to design self-adaptive software. This survey article presents a taxonomy, based on concerns of adaptation, that is, how, what, when and where, towards providing a unified view of this emerging area. Moreover, as adaptive systems are encountered in many disciplines, it is imperative to learn from the theories and models developed in these other areas. This survey article presents a landscape of research in self-adaptive software by highlighting relevant disciplines and some prominent research projects. This landscape helps to identify the underlying research gaps and elaborates on the corresponding challenges.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = may,
articleno = {14},
numpages = {42},
keywords = {Adaptation processes, research challenges, self-adaptive software, self-properties, survey}
}

@inproceedings{10.1145/3458359.3458363,
author = {Tang, Ke and Xu Ding, Jing and Jian Lei, Mei and Zeng, Zhen and Di Guo, Jun and Ling Zeng, Qiu},
title = {An Electromagnetic Immunity Test Scheme for Vehicle Audio System},
year = {2021},
isbn = {9781450389020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458359.3458363},
doi = {10.1145/3458359.3458363},
abstract = {With the emergence of in-vehicle electronic products and the rapid development of Telematics, the electromagnetic environment faced by vehicles has become more complex than ever, and the electromagnetic immunity of in-vehicle electronic products has a crucial impact on the driver's user experience and even road safety. So electromagnetic susceptibility (EMS) testing has become an urgent issue for the industry to face and solve. Besides EMS test, acoustic measurement should be paid attention to in-vehicle multimedia equipment audio performance. In this paper, an electromagnetic compatibility (EMC) test scheme for vehicle level audio quality is proposed. The EMC test platform is verified, the audio anti-interference test system is placed in an AF shielding box to test the different reverberation effects of the information received by human ear, and the Perceptual Objective Hearing Quality Assessment (POLQA) is introduced. The EMS test results show that it is feasible to use the MOS values of POLQA to evaluate the whole vehicle audio electromagnetic immunity.},
booktitle = {Proceedings of the 2021 10th International Conference on Informatics, Environment, Energy and Applications},
pages = {11–16},
numpages = {6},
keywords = {Audio quality, Automotive EMC, Electric vehicle, Vehicle acoustical environments},
location = {Xi'an, China},
series = {IEEA '21}
}

@article{10.3233/AIS-170463,
author = {Gil, Miriam and Pelechano, Vicente},
title = {Self-adaptive unobtrusive interactions of mobile computing systems},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {9},
number = {6},
issn = {1876-1364},
url = {https://doi.org/10.3233/AIS-170463},
doi = {10.3233/AIS-170463},
abstract = {In Pervasive Computing environments, people are surrounded by a lot of embedded services. Since pervasive devices, such as mobile devices, have become a key part of our everyday life, they enable users to always be connected to the environment, making demands on one of the most valuable resources of users: human attention. A&nbsp;challenge of the mobile computing systems is regulating the request for users’ attention. In other words, service interactions should behave in a considerate manner by taking into account the degree to which each service intrudes on the user’s mind (i.e., the degree of obtrusiveness). The main goal of this paper is to introduce self-adaptive capabilities in mobile computing systems in order to provide non-disturbing interactions. We achieve this by means of an software infrastructure that automatically adapts the service interaction obtrusiveness according to the user’s context. This infrastructure works from a set of high-level models that define the unobtrusive adaptation behavior and its implication with the interaction resources in a technology-independent way. Our infrastructure has been validated through several experiments to assess its correctness, performance, and the achieved user experience through a user study.},
journal = {J. Ambient Intell. Smart Environ.},
month = jan,
pages = {659–688},
numpages = {30},
keywords = {Interaction adaptation, self-adaptation, pervasive computing, unobtrusiveness, mobile computing}
}

@article{10.1007/s00607-020-00874-x,
author = {Andersson, Jesper and Grassi, Vincenzo and Mirandola, Raffaela and Perez-Palacin, Diego},
title = {A conceptual framework for resilience: fundamental definitions, strategies and metrics},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {103},
number = {4},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-020-00874-x},
doi = {10.1007/s00607-020-00874-x},
abstract = {The resilience system property has become more and more relevant, mainly because of the increasing dependance on a rapidly growing number of software-intensive, complex, socio-technical systems, which are facing uncertainty about changes they are expected to experience during their life-cycle and ways to deal with them. Methodologies for the systematic design and validation of resilience for such systems are thus highly necessary, and require contributions from several different fields. This paper contributes to current resilience research by providing a conceptual framework intended to serve as a common ground for the development of such methodologies. Its main points are: the identification of the main categories of changes a system should face; a clear definition of the different facets of resilience one could want to achieve, expressed in terms of the system dynamics; a mapping of each of these facets to design strategies that are better suited to achieve it; and the corresponding identification of possible metrics that can be used to assess its achievement.},
journal = {Computing},
month = apr,
pages = {559–588},
numpages = {30},
keywords = {Resilience, Conceptual framework, Strategies and metrics, 68U01, 68N30, 68U99}
}

@inproceedings{10.1145/3338286.3340139,
author = {Huang, Xun-Yi and Cherng, Fu-Yin and King, Jung-Tai and Lin, Wen-Chieh},
title = {EEG-based Measures of Auditory Saliency in a Complex Context},
year = {2019},
isbn = {9781450368254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338286.3340139},
doi = {10.1145/3338286.3340139},
abstract = {Auditory saliency is an important mechanism that helps humans extract relevant information from environments. Audio notifications of mobile devices with high saliency can increase users' receptivity, yet overly high saliency could cause annoyance. Accurately measuring auditory saliency of a notification is critical for evaluating its usability. Previous studies adopted behavioral methods. However, their results may not accurately reflect auditory saliency as humans' perception of auditory saliency often involves complicated cognitive processes. Thus, we propose an electroencephalography (EEG)-based approach that can complement behavioral studies to provide a more nuanced analysis of auditory saliency. We evaluated our method by conducting an EEG experiment that measured the mismatch negativity and P3a of the sounds in realistic scenarios. We also conducted a behavioral experiment to link the EEG-based method with the behavioral method. The results suggested that EEG can provide detailed information about how human perceive auditory saliency and complement the behavioral measures.},
booktitle = {Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {28},
numpages = {11},
keywords = {Auditory Saliency, Brain-computer Interface, Notification},
location = {Taipei, Taiwan},
series = {MobileHCI '19}
}

@article{10.5555/1349897.1350159,
author = {Tekinerdogan, Bedir and Sozer, Hasan and Aksit, Mehmet},
title = {Software architecture reliability analysis using failure scenarios},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {4},
issn = {0164-1212},
abstract = {With the increasing size and complexity of software in embedded systems, software has now become a primary threat for the reliability. Several mature conventional reliability engineering techniques exist in literature but traditionally these have primarily addressed failures in hardware components and usually assume the availability of a running system. Software architecture analysis methods aim to analyze the quality of software-intensive system early at the software architecture design level and before a system is implemented. We propose a Software Architecture Reliability Analysis Approach (SARAH) that benefits from mature reliability engineering techniques and scenario-based software architecture analysis to provide an early software reliability analysis at the architecture design level. SARAH defines the notion of failure scenario model that is based on the Failure Modes and Effects Analysis method (FMEA) in the reliability engineering domain. The failure scenario model is applied to represent so-called failure scenarios that are utilized to derive fault tree sets (FTS). Fault tree sets are utilized to provide a severity analysis for the overall software architecture and the individual architectural elements. Despite conventional reliability analysis techniques which prioritize failures based on criteria such as safety concerns, in SARAH failure scenarios are prioritized based on severity from the end-user perspective. SARAH results in a failure analysis report that can be utilized to identify architectural tactics for improving the reliability of the software architecture. The approach is illustrated using an industrial case for analyzing reliability of the software architecture of the next release of a Digital TV.},
journal = {J. Syst. Softw.},
month = apr,
pages = {558–575},
numpages = {18},
keywords = {FMEA, Fault trees, Reliability analysis, Scenario-based architectural evaluation}
}

@inproceedings{10.1145/2541940.2541962,
author = {Zahedi, Seyed Majid and Lee, Benjamin C.},
title = {REF: resource elasticity fairness with sharing incentives for multiprocessors},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541962},
doi = {10.1145/2541940.2541962},
abstract = {With the democratization of cloud and datacenter computing, users increasingly share large hardware platforms. In this setting, architects encounter two challenges: sharing fairly and sharing multiple resources. Drawing on economic game-theory, we rethink fairness in computer architecture. A fair allocation must provide sharing incentives (SI), envy-freeness (EF), and Pareto efficiency (PE).We show that Cobb-Douglas utility functions are well suited to modeling user preferences for cache capacity and memory bandwidth. And we present an allocation mechanism that uses Cobb-Douglas preferences to determine each user's fair share of the hardware. This mechanism provably guarantees SI, EF, and PE, as well as strategy-proofness in the large (SPL). And it does so with modest performance penalties, less than 10% throughput loss, relative to an unfair mechanism.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {145–160},
numpages = {16},
keywords = {economic mechanisms, fair sharing, game theory, multiprocessor architectures},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@article{10.1007/s10009-016-0443-0,
author = {Boukhari, Ily\`{e}s and Jean, St\'{e}phane and Ait-Sadoune, Idir and Bellatreche, Ladjel},
title = {The role of user requirements in data repository design},
year = {2018},
issue_date = {February  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {1},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-016-0443-0},
doi = {10.1007/s10009-016-0443-0},
abstract = {Requirements engineering plays a crucial role in the development process of an information system as it aims at providing a complete and accurate requirement specification. In the life cycle of a Data Repository ($${mathcal {D}}{mathcal {R}}$$DR) such as a database or a data warehouse, the requirements are mainly used to define the conceptual model once they have been identified from the informal specification. In this paper, we study the interest of requirements in the other phases of the $${mathcal {D}}{mathcal {R}}$$DR life cycle. As the data integration problem, handled in the Extract, Transform, Load (ETL) phase, comes from the heterogeneity of requirements, we introduce a requirement integration framework based on ontologies and a generic model to unify the used vocabularies and requirement languages. Then we propose an approach to check the consistency of the requirements, w.r.t. the integrity constraints defined on the logical schema using the formal B method. We also show that requirements help define appropriate access structures such as indexes and materialized views to optimize SQL queries of a $${mathcal {D}}{mathcal {R}}$$DR. Our approach is based on transformation rules that identify important queries that will be executed on a $${mathcal {D}}{mathcal {R}}$$DR directly from the requirements. The experiments conducted on the Star Schema Benchmark (SSB) confirm the interest of this approach for the selection of different optimization structures. Finally, we present the OntoReqTool that implements the previous functionality on top of the OntoDB/OntoQL platform.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = feb,
pages = {19–34},
numpages = {16},
keywords = {B method, Data warehouse, Database, Ontology, Query optimization, Requirements engineering}
}

@inproceedings{10.5555/2394450.2394490,
author = {Her, Jin Sun and Choi, Si Won and Cheun, Du Wan and Bae, Jeong Seop and Kim, Soo Dong},
title = {A component-based process for developing automotive ECU software},
year = {2007},
isbn = {3540734597},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software plays a vital role in operating modern automobiles, and it is a key element in providing innovative features such as Collision Prevention System. There are two essential issues to be resolved; managing software complexity, and reducing software cost and time-to-market. A key solution to these two issues is to maximize reusing components in building various Electronic Control Units (ECUs). Component-based development (CBD) is regarded as an effective reuse technology. However, current CBD methodologies do not effectively support developing reusable automotive components and ECUs. Hence, in this paper, we first define variability types and variation points for ECUs. Based on the variability types, we propose a component-based development process for developing ECUs. To assess the applicability of the proposed CBD process, we present the case study of developing an innovative automotive ECU for Automatic Parking System (APS).},
booktitle = {Proceedings of the 8th International Conference on Product-Focused Software Process Improvement},
pages = {358–373},
numpages = {16},
location = {Riga, Latvia},
series = {PROFES'07}
}

@inproceedings{10.1145/3384419.3430727,
author = {Sun, Ke and Chen, Chen and Zhang, Xinyu},
title = {"Alexa, stop spying on me!": speech privacy protection against voice assistants},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430727},
doi = {10.1145/3384419.3430727},
abstract = {Voice assistants (VAs) are becoming highly popular recently as a general means of interacting with the Internet of Things. However, the use of always-on microphones on VAs imposes a looming threat on users' privacy. In this paper, we propose MicShield, the first system that serves as a companion device to enforce privacy preservation on VAs. MicShield introduces a novel selective jamming mechanism, which obfuscates the user's private speech while passing legitimate voice commands to the VAs. It achieves this by using a phoneme level jamming control pipeline. Our implementation and experiments demonstrate that MicShield can effectively protect a user's private speech, without affecting the VA's responsiveness.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {298–311},
numpages = {14},
keywords = {privacy protection, selective jamming, voice assistant},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@inproceedings{10.1145/1882291.1882309,
author = {Ramasubbu, Narayan and Balan, Rajesh Krishna},
title = {Evolution of a bluetooth test application product line: a case study},
year = {2010},
isbn = {9781605587912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882291.1882309},
doi = {10.1145/1882291.1882309},
abstract = {In this paper, we study the decision making process involved in the five year lifecycle of a Bluetooth software product produced by a large, multi-national test and measurement firm. In this environment, customer change requests either have to be added as a standard feature in the product, or developed as a special customized version of the product. We first discuss the influential factors, such as evolving standards, market share, installed-base, and complexity, which collectively determined how the firm responded to product change requests. We then develop a predictive decision model to test the collective impact of these factors on determining whether to standardize or customize a customer's change request. Finally, we develop and test a customization cost estimation model, for use by software product teams, which specifically accounts for factors unique to the customization stage of a product lifecycle.},
booktitle = {Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {107–116},
numpages = {10},
keywords = {complexity, product development, product life cycle, software engineering economics, software evolution, software process},
location = {Santa Fe, New Mexico, USA},
series = {FSE '10}
}

@inproceedings{10.1145/2993236.2993248,
author = {Kowal, Matthias and Ananieva, Sofia and Th\"{u}m, Thomas},
title = {Explaining anomalies in feature models},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993248},
doi = {10.1145/2993236.2993248},
abstract = {The development of variable software, in general, and feature models, in particular, is an error-prone and time-consuming task. It gets increasingly more challenging with industrial-size models containing hundreds or thousands of features and constraints. Each change may lead to anomalies in the feature model such as making some features impossible to select. While the detection of anomalies is well-researched, giving explanations is still a challenge. Explanations must be as accurate and understandable as possible to support the developer in repairing the source of an error. We propose an efficient and generic algorithm for explaining different anomalies in feature models. Additionally, we achieve a benefit for the developer by computing short explanations expressed in a user-friendly manner and by emphasizing specific parts in explanations that are more likely to be the cause of an anomaly. We provide an open-source implementation in FeatureIDE and show its scalability for industrial-size feature models.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {132–143},
numpages = {12},
keywords = {Anomalies, Explanations, Feature Models, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@article{10.1007/s11227-020-03268-0,
author = {Dehraj, Pooja and Sharma, Arun},
title = {A review on architecture and models for autonomic software systems},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {1},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-020-03268-0},
doi = {10.1007/s11227-020-03268-0},
abstract = {Autonomic computing was the term coined by IBM in 2001. The term autonomic computing was used to define the self-adaptable nature of the human body. According to IBM, the same self-adaptable feature was the need to be incorporated in the software systems. Autonomic computing is the combination of few self-capabilities such as self-configuration, self-healing, self-optimization, self-protection, self-awareness, etc. So, autonomic computing approach was then used to develop autonomic software systems. This approach makes the computing systems self-adaptable and self-decision-making support systems for various activities. It also helps to reduce the human intervention in the software management process. Though, the implementation of autonomic self-capabilities may increase the software complexity, which further requires human intervention for the software maintenance-related specific tasks. Still, IT industries are approaching to develop autonomic features in their existing architecture or developing new self-adaptable software systems. Autonomic computing has its importance for providing a bridge for handling and managing the run-time computation-based issues/exceptions of the software. So, the discussion of this solution has become a necessity for making the vision of autonomic decision making more clear and understandable for researchers and developers for the improvement in an autonomic area. The paper provides an insight vision of the autonomic decision-making concept and its importance for the various purposes such as intrusion detection, cloud-based data security, wireless sensor network, Internet of Things, Big Data and many other areas where management cannot be handled by a human in real time. To assess the degree of autonomic feature, there is another term used which is known as autonomicity. The paper also discusses some solutions suggested and implemented by different researchers during their studies for estimating the system’s autonomicity level. These solutions will help in comparing different autonomic applications based on the autonomic features implemented in each application. This paper is an attempt to provide better understandability in the autonomic computational field.},
journal = {J. Supercomput.},
month = jan,
pages = {388–417},
numpages = {30},
keywords = {Autonomic computing, Self-configuration, Self-healing, Self-optimization, Self-protection, System autonomicity level}
}

@inproceedings{10.5555/3108244.3108251,
author = {Weissnegger, Ralph and Pistauer, Markus and Schachner, Martin and Kreiner, Christian and R\"{o}mer, Kay and Steger, Christian},
title = {SaVeSoC: safety aware virtual prototype generation and evaluation of a system on chip},
year = {2017},
isbn = {9781510838260},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {The electrification of today's vehicles and the high amount of new assistance features imply more and more complex systems. The sensing and controlling of these systems is the work of the highly distributed and connected electronic control units. To keep pace with the fast growing automotive market, reusability of components and features is today the key to reduce costs and time-to-market. Especially when systems are safety-critical and demand reliability, new methods and tools are thus essential to support the reusability aspect in the development process. A model-based approach, in conjunction, moreover helps to communicate between different stakeholders, provides different views and serves as a central storage of information. Through applying reliability analysis and simulation-based verification methods on our hardware model and furthermore automatic generation of a first virtual prototype, we are able to reduce the tools involved, thus resulting in correctness, completeness and consistency of the entire system.},
booktitle = {Proceedings of the Symposium on Model-Driven Approaches for Simulation Engineering},
articleno = {7},
numpages = {12},
keywords = {ISO26262, UML, cyber-physical system, functional safety, virtual prototyping},
location = {Virginia Beach, Virginia},
series = {Mod4Sim '17}
}

@article{10.1145/1880050.1880057,
author = {Driver, Cormac and Reilly, Sean and Linehan, \'{E}amonn and Cahill, Vinny and Clarke, Siobh\'{a}n},
title = {Managing embedded systems complexity with aspect-oriented model-driven engineering},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/1880050.1880057},
doi = {10.1145/1880050.1880057},
abstract = {Model-driven engineering addresses issues of platform heterogeneity and code quality through the use of high-level system models and subsequent automatic transformations. Adoption of the model-driven software engineering paradigm for embedded systems necessitates specification of appropriate models of often complex systems. Modern embedded systems are typically composed of multiple functional and nonfunctional concerns, with the nonfunctional concerns (e.g., timing and performance) typically affecting the design and implementation of the functional concerns. The presence of crosscutting concerns makes specification of adequate platform-independent models a significant challenge. Aspect-oriented software development is a separation of concerns technique that decomposes systems into distinct features with minimal overlap. In this article, we illustrate how Theme/UML, an aspect-oriented modeling approach, can be used to separate embedded systems concerns and reduce complexity in design. We also present Model-Driven Theme/UML, a toolset for model-driven engineering of embedded systems that supports modularised design with Theme/UML and automatic transformations to composed models and source code.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = jan,
articleno = {21},
numpages = {26},
keywords = {Aspect-Oriented Software Deve- lopment, Model-Driven Engineering, Nonfunctional requirements, Theme/UML, code generation, model transformation, separation of concerns}
}

@article{10.1016/j.infsof.2017.09.009,
author = {Irshad, Mohsin and Petersen, Kai and Poulding, Simon},
title = {A systematic literature review of software requirements reuse approaches},
year = {2018},
issue_date = {January 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {93},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.09.009},
doi = {10.1016/j.infsof.2017.09.009},
abstract = {ContextEarly software reuse is considered as the most beneficial form of software reuse. Hence, previous research has focused on supporting the reuse of software requirements. ObjectiveThis study aims to identify and investigate the current state of the art with respect to (a) what requirement reuse approaches have been proposed, (b) the methods used to evaluate the approaches, (c) the characteristics of the approaches, and (d) the quality of empirical studies on requirements reuse with respect to rigor and relevance. MethodWe conducted a systematic review and a combination of snowball sampling and database search have been used to identify the studies. The rigor and relevance scoring rubric has been used to assess the quality of the empirical studies. Multiple researchers have been involved in each step to increase the reliability of the study. ResultsSixty-nine studies were identified that describe requirements reuse approaches. The majority of the approaches used structuring and matching of requirements as a method to support requirements reuse and text-based artefacts were commonly used as an input to these approaches. Further evaluation of the studies revealed that the majority of the approaches are not validated in the industry. The subset of empirical studies (22 in total) was analyzed for rigor and relevance and two studies achieved the maximum score for rigor and relevance based on the rubric. It was found that mostly text-based requirements reuse approaches were validated in the industry. ConclusionFrom the review, it was found that a number of approaches already exist in literature, but many approaches are not validated in industry. The evaluation of rigor and relevance of empirical studies show that these do not contain details of context, validity threats, and the industrial settings, thus highlighting the need for the industrial evaluation of the approaches.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {223–245},
numpages = {23},
keywords = {Artefact reuse, Relevance, Requirements reuse, Reusability, Rigor, Software requirements}
}

@article{10.1016/j.infsof.2009.04.004,
author = {Mohagheghi, Parastoo and Dehlen, Vegard and Neple, Tor},
title = {Definitions and approaches to model quality in model-based software development - A review of literature},
year = {2009},
issue_date = {December, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {12},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.04.004},
doi = {10.1016/j.infsof.2009.04.004},
abstract = {More attention is paid to the quality of models along with the growing importance of modelling in software development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {1646–1669},
numpages = {24},
keywords = {Model quality, Model-driven development, Modelling, Systematic review, UML}
}

@inproceedings{10.1007/978-3-642-39031-9_24,
author = {Pascual, Gustavo G. and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Run-Time support to manage architectural variability specified with CVL},
year = {2013},
isbn = {9783642390302},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39031-9_24},
doi = {10.1007/978-3-642-39031-9_24},
abstract = {The execution context in which pervasive systems or mobile computing run changes continuously. Hence, applications for these systems should be adapted at run-time according to the current context. In order to implement a context-aware dynamic reconfiguration service, most approaches usually require to model at design-time both the list of all possible configurations and the plans to switch among them. In this paper we present an alternative approach for the automatic run-time generation of application configurations and the reconfiguration plans. The generated configurations are optimal regarding different criteria, such as functionality or resource consumption (e.g. battery or memory). This is achieved by: (1) modelling architectural variability at design-time using Common Variability Language (CVL), and (2) using a genetic algorithm that finds at run-time nearly-optimal configurations using the information provided by the variability model. We also specify a case study and we use it to evaluate our approach, showing that it is efficient and suitable for devices with scarce resources.},
booktitle = {Proceedings of the 7th European Conference on Software Architecture},
pages = {282–298},
numpages = {17},
keywords = {CVL, architectural variability, context, dynamic reconfiguration, genetic algorithm, pervasive systems},
location = {Montpellier, France},
series = {ECSA'13}
}

@inproceedings{10.1145/3486609.3487197,
author = {Ataei, Parisa and Khan, Fariba and Walkingshaw, Eric},
title = {A variational database management system},
year = {2021},
isbn = {9781450391122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486609.3487197},
doi = {10.1145/3486609.3487197},
abstract = {Many problems require working with data that varies in its structure and content. Current approaches, such as schema evolution or data integration tools, are highly tailored to specific kinds of variation in databases. While these approaches work well in their roles, they do not address all kinds of variation and do address the interaction of different kinds of variation in databases. In this paper, we define a framework for capturing variation as a generic and orthogonal con- cern in relational databases. We define variational schemas, variational databases, and variational queries for capturing variation in the structure, content, and information needs of relational databases, respectively. We define a type system that ensures variational queries are consistent with respect to a variational schema. Finally, we design and implement a variational database management system as an abstraction layer over a traditional relational database management system. Using previously developed use cases, we show the feasibility of our framework and demonstrate the performance of different approaches used in our system},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {29–42},
numpages = {14},
keywords = {choice calculus, relational databases, software product lines, type systems, variation, variational data},
location = {Chicago, IL, USA},
series = {GPCE 2021}
}

@article{10.1016/j.cose.2014.09.003,
author = {Mesquida, Antoni Llu\'{\i}s and Mas, Antonia},
title = {Implementing information security best practices on software lifecycle processes},
year = {2015},
issue_date = {February 2015},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {48},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2014.09.003},
doi = {10.1016/j.cose.2014.09.003},
abstract = {The ISO/IEC 15504 international standard can be aligned with the ISO/IEC 27000 information security management framework. During the research conducted all the existing relations between ISO/IEC 15504-5 software development base practices and ISO/IEC 27002 security controls have been analysed and the ISO/IEC 15504 Security Extension has been developed. This extension details the changes that software companies should make in the software lifecycle processes for the successful implementation of the related security controls. To attain our research objectives, we evaluate the ISO/IEC 15504 Security Extension through case studies in a sample of software development organizations. This study follows the design science research paradigm that is based on constructive research. ISO/IEC 15504-5 processes can be adapted to deploy ISO/IEC 27002 controls on them.Relations between ISO/IEC 15504-5 and ISO/IEC 27002 security controls are analysed.From these relations, the ISO/IEC 15504 Security Extension has been developed.The Design Science Research paradigm has been followed during its development.The ISO/IEC 15504 Security Extension has been validated in industry.},
journal = {Comput. Secur.},
month = feb,
pages = {19–34},
numpages = {16},
keywords = {ISO/IEC 15504 (SPICE), ISO/IEC 27002, Information security management systems, Security extension, Software process improvement (SPI)}
}

@article{10.1007/s10515-005-2648-4,
author = {Padmanabhan, Prasanna and Lutz, Robyn R.},
title = {Tool-Supported Verification of Product Line Requirements},
year = {2005},
issue_date = {October   2005},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {12},
number = {4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-005-2648-4},
doi = {10.1007/s10515-005-2648-4},
abstract = {A recurring difficulty for organizations that employ a product-line approach to development is that when a new product is added to an existing product line, there is currently no automated way to verify the completeness and consistency of the new product's requirements in terms of the product line. In this paper we address the issue of requirements verification for product lines. We have implemented our approach in a requirements engineering tool called DECIMAL (DECIsion Modeling AppLication). DECIMAL is a requirements verification tool with a rich graphical user interface that automatically checks for completeness and consistency between a new product and the product line to which it belongs. The verification uses an SQL database server as the underlying analysis engine. The paper describes the tool and evaluates it in two applications: a virtual-reality, positional device-driver product line and the feature-interaction resolution problem.},
journal = {Automated Software Engg.},
month = oct,
pages = {447–465},
numpages = {19},
keywords = {consistency checking, dependency constraints, feature-interaction resolution, product family, product line, requirements verification, variability}
}

@article{10.1007/s10664-014-9322-z,
author = {Bass, Julian M.},
title = {How product owner teams scale agile methods to large distributed enterprises},
year = {2015},
issue_date = {December  2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9322-z},
doi = {10.1007/s10664-014-9322-z},
abstract = {Software development teams in large scale offshore enterprise development programmes are often under intense pressure to deliver high quality software within challenging time contraints. Project failures can attract adverse publicity and damage corporate reputations. Agile methods have been advocated to reduce project risks, improving both productivity and product quality. This article uses practitioner descriptions of agile method tailoring to explore large scale offshore enterprise development programmes with a focus on product owner role tailoring, where the product owner identifies and prioritises customer requirements. In globalised projects, the product owner must reconcile competing business interests, whilst generating and then prioritising large numbers of requirements for numerous development teams. The study comprises eight international companies, based in London, Bangalore and Delhi. Interviews with 46 practitioners were conducted between February 2010 and May 2012. Grounded theory was used to identify that product owners form into teams. The main contribution of this research is to describe the nine product owner team functions identified: groom, prioritiser, release master, technical architect, governor, communicator, traveller, intermediary and risk assessor. These product owner functions arbitrate between conflicting customer requirements, approve release schedules, disseminate architectural design decisions, provide technical governance and propogate information across teams. The functions identified in this research are mapped to a scrum of scrums process, and a taxonomy of the functions shows how focusing on either decision-making or information dissemination in each helps to tailor agile methods to large scale offshore enterprise development programmes.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {1525–1557},
numpages = {33},
keywords = {Agile software development, Grounded theory, Large scale offshore enterprise development programmes, Product owner, Product owner teams, Scrum}
}

@inproceedings{10.1007/978-3-030-89370-5_19,
author = {Luo, Chao and Bi, Sheng and Dong, Min and Nie, Hongxu},
title = {RGB-D Based Visual Navigation Using Direction Estimation Module},
year = {2021},
isbn = {978-3-030-89369-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89370-5_19},
doi = {10.1007/978-3-030-89370-5_19},
abstract = {Target-driven visual navigation without mapping works to solve navigation problems that given a target object, mobile robots can navigate to the target object. Recently, visual navigation has been researched and improved largely by learning-based methods. However, their methods lack depth information and spatial perception, using only single RGB images. To overcome these problems, two methods are presented in this paper. Firstly, we encode visual features of objects by dynamic graph convolutional network and extract 3D spatial features for objects by 3D geometry, a high level visual feature for agent to easily understand object relationship. Secondly, as human beings, they solve this problem in two steps, first exploring a new environment to find the target object and second planning a path to arrive. Inspired by the way of humans navigation, we propose direction estimation module (DEM) based on RGB-D images. DEM provides direction estimation of the target object to our learning model by a wheel odometry. Given a target object, first stage, our agent explores an unseen scene to detect the target object. Second stage, when detected the target object, we can estimate current location of the target object by 3D geometry, after that, each step of the agent, DEM will estimate new location of target object, and give direction information of the target object from a first-view image. It can guide our agent to navigate to the target object. Our experiment results outperforms the result of state of the art method in the artificial environment AI2-Thor.},
booktitle = {PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III},
pages = {252–264},
numpages = {13},
keywords = {Visual navigation, Mobile robot, Direction estimation module, Reinforcement learning},
location = {Hanoi, Vietnam}
}

@article{10.1007/s10270-020-00831-4,
author = {Klikovits, Stefan and Buchs, Didier},
title = {Pragmatic reuse for DSML development: Composing a DSL for hybrid CPS modeling},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00831-4},
doi = {10.1007/s10270-020-00831-4},
abstract = {By bridging the semantic gap, domain-specific language (DSLs) serve an important role in the conquest to allow domain experts to model their systems themselves. In this publication we present a case study of the development of the Continuous REactive SysTems language (CREST), a DSL for hybrid systems modeling. The language focuses on the representation of continuous resource flows such as water, electricity, light or heat. Our methodology follows a very pragmatic approach, combining the syntactic and semantic principles of well-known modeling means such as hybrid automata, data-flow languages and architecture description languages into a coherent language. The borrowed aspects have been carefully combined and formalised in a well-defined operational semantics. The DSL provides two concrete syntaxes: CREST diagrams, a graphical language that is easily understandable and serves as a model basis, and crestdsl, an internal DSL implementation that supports rapid prototyping—both are geared towards usability and clarity. We present the DSL’s semantics, which thoroughly connect the various language concerns into an executable formalism that enables sound simulation and formal verification in crestdsl, and discuss the lessons learned throughout the project.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {837–866},
numpages = {30},
keywords = {Cyber-physical systems, Domain-specific language, Modeling, Simulation, Verification}
}

@inproceedings{10.1145/2110147.2110152,
author = {Boucher, Quentin and Perrouin, Gilles and Heymans, Patrick},
title = {Deriving configuration interfaces from feature models: a vision paper},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110152},
doi = {10.1145/2110147.2110152},
abstract = {In software product lines, feature models are the de-facto standard for representing variability as well as for configuring products. Yet, configuration relying on feature models faces two issues: i) it assumes knowledge of the underlying formalism, which may not be true for end users and ii) it does not take advantage of advanced user-interface controls, leading to usability and integration problems with other parts of the user interface. To address these issues, our research focuses on the generation of configuration interfaces based on variability models, both from the visual and behavioral perspectives. We tackle visual issues by generating abstract user-interfaces from feature models. Regarding configuration behavior, in particular the configuration sequence, we plan to use feature configuration workflows, variability-aware models that exhibit similar characteristics as of task, user, discourse and business models found in the in the human-computer interaction community. This paper discusses the main challenges and possible solutions to realize our vision.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {37–44},
numpages = {8},
keywords = {configuration interfaces, feature configuration workflows, software product lines},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@article{10.1007/s10515-012-0117-4,
author = {N\"{o}hrer, Alexander and Egyed, Alexander},
title = {C2O configurator: a tool for guided decision-making},
year = {2013},
issue_date = {June      2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-012-0117-4},
doi = {10.1007/s10515-012-0117-4},
abstract = {Decision models are widely used in software engineering to describe and restrict decision-making (e.g., deriving a product from a product-line). Since decisions are typically interdependent, it is often neither obvious which decisions have the most significant impact nor which decisions might ultimately conflict. Unfortunately, the current state-of-the-art provides little support for dealing with such situations. On the one hand, some conflicts can be avoided by providing more freedom in which order decisions are made (i.e., most important decisions first). On the other hand, conflicts are unavoidable at times, and living with conflicts may be preferable over forcing the user to fix them right away--particularly because fixing conflicts becomes easier as more is known about a user's intentions. This paper introduces the C2O (Configurator 2.0) tool for guided decision-making. The tool allows the user to answer questions in an arbitrary order--with and without the presence of inconsistencies. While giving users those freedoms, it still supports and guides them by (i) rearranging the order of questions according to their potential to minimize user input, (ii) providing guidance to avoid follow-on conflicts, and (iii) supporting users in fixing conflicts at a later time.},
journal = {Automated Software Engg.},
month = jun,
pages = {265–296},
numpages = {32}
}

@article{10.1109/TASLP.2016.2593803,
author = {Carini, Alberto and Cecchi, Stefania and Romoli, Laura and Carini, Alberto and Cecchi, Stefania and Romoli, Laura},
title = {Robust Room Impulse Response Measurement Using Perfect Sequences for Legendre Nonlinear Filters},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2593803},
doi = {10.1109/TASLP.2016.2593803},
abstract = {The paper proposes a novel approach for measuring the room impulse response that is robust toward the nonlinearities affecting the power amplifier or the loudspeaker. The approach is implemented by modeling the acoustic path as a Legendre nonlinear filter and by measuring the first-order kernel using perfect periodic sequences and the cross-correlation method. Perfect sequences for Legendre filters are periodic sequences that guarantee the orthogonality of the Legendre basis functions over a period. They ensure the robustness of the first kernel measurement toward nonlinear distortions. The paper also explains how perfect periodic sequences for Legendre filters that are suitable for room impulse response identification can be developed. Experimental results involving both simulated and real environments illustrate the effectiveness and the robustness of the proposed approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1969–1982},
numpages = {14}
}

@inproceedings{10.1145/2110147.2110164,
author = {Hubaux, Arnaud and Xiong, Yingfei and Czarnecki, Krzysztof},
title = {A user survey of configuration challenges in Linux and eCos},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110164},
doi = {10.1145/2110147.2110164},
abstract = {Operating systems expose sophisticated configurability to handle variability in hardware platforms like mobile devices, desktops, and servers. The variability model of an operating system kernel like Linux contains thousands of options guarded by hundreds of complex constraints. To guide users throughout the configuration and ensure the validity of their decisions, specialized tools known as configurators have been developed. Despite these tools, configuration still remains a difficult and challenging process. To better understand the challenges faced by users during configuration, we conducted two surveys, one among Linux users and another among eCos users. This paper presents the results of the surveys along three dimensions: configuration practice; user guidance; and language expressiveness. We hope that these results will help researchers and tool builders focus their efforts to improve tool support for software configuration.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {149–155},
numpages = {7},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.1145/800088.802831,
author = {Snodgrass, Richard},
title = {A sophisticated microcomputer user interface},
year = {1980},
isbn = {0897910249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800088.802831},
doi = {10.1145/800088.802831},
abstract = {The design and implementation of a menu-oriented interface for personal computers is discussed. Factors pertaining to the cognitive limitations of users are examined and their impact on the design of the system is described. The major attributes of the system are (1) all communication between the operator and the computer is through menus or forms (which are analogous to hard copy documents); (2) extensive help is available at all times; (3) the interface can adapt to the experience of the user; (4) the display processing time is short; and (5) an external data format exists that completely defines the interface. The various components of the interface are discussed in detail, followed by a discussion of the implementation.},
booktitle = {Proceedings of the 3rd ACM SIGSMALL Symposium and the First SIGPC Symposium on Small Systems},
pages = {97–107},
numpages = {11},
location = {Palo Alto, California, USA},
series = {SIGSMALL '80}
}

@inproceedings{10.5555/2663546.2663560,
author = {Pascual, Gustavo G. and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Run-time adaptation of mobile applications using genetic algorithms},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {Mobile applications run in environments where the context is continuously changing. Therefore, it is necessary to provide support for the run-time adaptation of these applications. This support is usually achieved by middleware platforms that offer a context-aware dynamic reconfiguration service. However, the main shortcoming of existing approaches is that both the list of possible configurations and the plans to adapt the application to a new configuration are usually specified at design-time. In this paper we present an approach that allows the automatic generation at run-time of application configurations and of reconfiguration plans. Moreover, the generated configurations are optimal regarding the provided functionality and, more importantly, without exceeding the available resources (e.g. battery). This is performed by: (1) having the information about the application variability available at runtime using feature models, and (2) using a genetic algorithm that allows generating an optimal configuration at runtime. We have specified a case study and evaluated our approach, and the results show that it is efficient enough as to be used on mobile devices without introducing an excessive overhead.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {73–82},
numpages = {10},
keywords = {autonomic computing, context, dynamic reconfiguration, feature models, genetic algorithms, middleware},
location = {San Francisco, California},
series = {SEAMS '13}
}

@article{10.5555/2773202.2773211,
author = {\v{S}tuikys, Vytautas and Dama\v{s}evi\v{c}ius, Robertas},
title = {Equivalent Transformations of Heterogeneous Meta-Programs},
year = {2013},
issue_date = {April 2013},
publisher = {IOS Press},
address = {NLD},
volume = {24},
number = {2},
issn = {0868-4952},
abstract = {We consider a generalization of heterogeneous meta-programs by (1) introducing an extra level of abstraction within the meta-program structure, and (2) meta-program transformations. We define basic terms, formalize transformation tasks, consider properties of meta-program transformations and rules to manage complexity through the following transformation processes: (1) reverse transformation, when a correct one-stage meta-program M1 is transformed into the equivalent two-stage meta-meta-program M2; (2) two-stage forward transformations, when M2 is transformed into a set of meta-programs, and each meta-program is transformed into a set of target programs. The results are as follows: (a) formalization of the transformation processes within the heterogeneous meta-programming paradigm; (b) introduction and approval of equivalent transformations of meta-programs into meta-meta-programs and vice versa; (c) introduction of metrics to evaluate complexity of meta-specifications. The results are approved by examples, theoretical reasoning and experiments.},
journal = {Informatica},
month = apr,
pages = {315–337},
numpages = {23},
keywords = {Generalization, Meta-Program Complexity, Meta-Programming, Transformation}
}

@inproceedings{10.5555/1768904.1768905,
author = {Sawyer, Pete and Paech, Barbara and Heymans, Patrick},
title = {REFSQ 2007 international working conference on requirements engineering: foundation for software quality},
year = {2007},
isbn = {9783540730309},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The 13th Working Conference on Requirements Engineering: Foundation for Software Quality (REFSQ'07) will take place in the beautiful city of Trondheim, Norway on the 11th and 12th June 2007. As with most previous years, REFSQ'07 is affiliated with CAiSE. However, REFSQ'07 is significantly larger than in previous years, both in terms of the number of submissions and the size of the programme. 27 papers will be presented, plus a keynote address by Klaus Pohl, and parallel sessions will be necessary to make the programme possible within two days. However, the essential highly interactive and participatory nature of the REFSQ 'brand' will be retained.},
booktitle = {Proceedings of the 13th International Working Conference on Requirements Engineering: Foundation for Software Quality},
pages = {1–17},
numpages = {17},
location = {Trondheim, Norway},
series = {REFSQ'07}
}

@inproceedings{10.1145/3147704.3147731,
author = {Sauermann, Victor and Frey, Frank},
title = {Architecture Management in Software Development Organizations},
year = {2017},
isbn = {9781450348485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147704.3147731},
doi = {10.1145/3147704.3147731},
abstract = {As long as a software development organization drives several development projects in parallel the enterprise architecture gets more and more complex. Every initiative and every project has its own goals and timelines. This context implies a great challenge steering and managing the architecture to achieve more homogeneity, evolve according to new technology stacks and still pay credit to a necessary independence between projects. Short-term goals of the projects need to be balanced out with mid- and long-term objectives of the company. Therefore, architecture management measures should be considered, planned, and realized. This paper describes eight high-level architecture management patterns as starting point for central architecture teams, chiefs of architects, lead and principal software architects or CTOs.},
booktitle = {Proceedings of the 22nd European Conference on Pattern Languages of Programs},
articleno = {24},
numpages = {14},
keywords = {Architecture management, Standardization},
location = {Irsee, Germany},
series = {EuroPLoP '17}
}

@article{10.1007/s10664-021-09956-6,
author = {Veizaga, Alvaro and Alferez, Mauricio and Torre, Damiano and Sabetzadeh, Mehrdad and Briand, Lionel},
title = {On systematically building a controlled natural language for functional requirements},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09956-6},
doi = {10.1007/s10664-021-09956-6},
abstract = {Natural language (NL) is pervasive in software requirements specifications (SRSs). However, despite its popularity and widespread use, NL is highly prone to quality issues such as vagueness, ambiguity, and incompleteness. Controlled natural languages (CNLs) have been proposed as a way to prevent quality problems in requirements documents, while maintaining the flexibility to write and communicate requirements in an intuitive and universally understood manner. In collaboration with an industrial partner from the financial domain, we systematically develop and evaluate a CNL, named Rimay, intended at helping analysts write functional requirements. We rely on Grounded Theory for building Rimay and follow well-known guidelines for conducting and reporting industrial case study research. Our main contributions are: (1) a qualitative methodology to systematically define a CNL for functional requirements; this methodology is intended to be general for use across information-system domains, (2) a CNL grammar to represent functional requirements; this grammar is derived from our experience in the financial domain, but should be applicable, possibly with adaptations, to other information-system domains, and (3) an empirical evaluation of our CNL (Rimay) through an industrial case study. Our contributions draw on 15 representative SRSs, collectively containing 3215 NL requirements statements from the financial domain. Our evaluation shows that Rimay is expressive enough to capture, on average, 88% (405 out of 460) of the NL requirements statements in four previously unseen SRSs from the financial domain.},
journal = {Empirical Softw. Engg.},
month = jul,
numpages = {53},
keywords = {Natural language requirements, Functional requirements, Controlled natural language, Qualitative study, Case study research}
}

@article{10.1145/3482968,
author = {Tang, Yan and Cui, Weilong and Su, Jianwen},
title = {A Query Language for Workflow Logs},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3482968},
doi = {10.1145/3482968},
abstract = {A business process (workflow) is an assembly of tasks to accomplish a business goal. Real-world workflow models often demanded to change due to new laws and policies, changes in the environment, and so on. To understand the inner workings of a business process to facilitate changes, workflow logs have the potential to enable inspecting, monitoring, diagnosing, analyzing, and improving the design of a complex workflow. Querying workflow logs, however, is still mostly an ad hoc practice by workflow managers. In this article, we focus on the problem of querying workflow log concerning both control flow and dataflow properties. We develop a query language based on “incident patterns” to allow the user to directly query workflow logs instead of having to transform such queries into database operations. We provide the formal semantics and a query evaluation algorithm of our language. By deriving an accurate cost model, we develop an optimization mechanism to accelerate query evaluation. Our experiment results demonstrate the effectiveness of the optimization and achieves up to 50\texttimes{} speedup over an adaption of existing evaluation method.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {16},
numpages = {28},
keywords = {Business workflow, query languages, log}
}

@article{10.1145/3280988,
author = {Razzaq, Abdul and Wasala, Asanka and Exton, Chris and Buckley, Jim},
title = {The State of Empirical Evaluation in Static Feature Location},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3280988},
doi = {10.1145/3280988},
abstract = {Feature location (FL) is the task of finding the source code that implements a specific, user-observable functionality in a software system. It plays a key role in many software maintenance tasks and a wide variety of Feature Location Techniques (FLTs), which rely on source code structure or textual analysis, have been proposed by researchers. As FLTs evolve and more novel FLTs are introduced, it is important to perform comparison studies to investigate “Which are the best FLTs?” However, an initial reading of the literature suggests that performing such comparisons would be an arduous process, based on the large number of techniques to be compared, the heterogeneous nature of the empirical designs, and the lack of transparency in the literature. This article presents a systematic review of 170 FLT articles, published between the years 2000 and 2015. Results of the systematic review indicate that 95% of the articles studied are directed towards novelty, in that they propose a novel FLT. Sixty-nine percent of these novel FLTs are evaluated through standard empirical methods but, of those, only 9% use baseline technique(s) in their evaluations to allow cross comparison with other techniques. The heterogeneity of empirical evaluation is also clearly apparent: altogether, over 60 different FLT evaluation metrics are used across the 170 articles, 272 subject systems have been used, and 235 different benchmarks employed. The review also identifies numerous user input formats as contributing to the heterogeneity. Analysis of the existing research also suggests that only 27% of the FLTs presented might be reproduced from the published material. These findings suggest that comparison across the existing body of FLT evaluations is very difficult. We conclude by providing guidelines for empirical evaluation of FLTs that may ultimately help to standardise empirical research in the field, cognisant of FLTs with different goals, leveraging common practices in existing empirical evaluations and allied with rationalisations. This is seen as a step towards standardising evaluation in the field, thus facilitating comparison across FLTs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {2},
numpages = {58},
keywords = {Feature location, bug location, concept location, requirement traceability}
}

@article{10.1007/s11219-016-9322-x,
author = {Engstr\"{o}m, Emelie and Petersen, Kai and Ali, Nauman Bin and Bjarnason, Elizabeth},
title = {SERP-test: a taxonomy for supporting industry---academia communication},
year = {2017},
issue_date = {December  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9322-x},
doi = {10.1007/s11219-016-9322-x},
abstract = {This paper presents the construction and evaluation of SERP-test, a taxonomy aimed to improve communication between researchers and practitioners in the area of software testing. SERP-test can be utilized for direct communication in industry academia collaborations. It may also facilitate indirect communication between practitioners adopting software engineering research and researchers who are striving for industry relevance. SERP-test was constructed through a systematic and goal-oriented approach which included literature reviews and interviews with practitioners and researchers. SERP-test was evaluated through an online survey and by utilizing it in an industry---academia collaboration project. SERP-test comprises four facets along which both research contributions and practical challenges may be classified: Intervention, Scope, Effect target and Context constraints. This paper explains the available categories for each of these facets (i.e., their definitions and rationales) and presents examples of categorized entities. Several tasks may benefit from SERP-test, such as formulating research goals from a problem perspective, describing practical challenges in a researchable fashion, analyzing primary studies in a literature review, or identifying relevant points of comparison and generalization of research.},
journal = {Software Quality Journal},
month = dec,
pages = {1269–1305},
numpages = {37},
keywords = {Classification, Context, Effect target, Industry relevance, Intervention, Methodology, SERP-test, Scope, Software testing, Taxonomy}
}

@article{10.1145/1050849.1057988,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Frontmatter (TOC, Letters, Open Source Software (OSS) Patent Search Engine, Calendar of Events, Workshop and Conference Information)},
year = {2005},
issue_date = {March 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1050849.1057988},
doi = {10.1145/1050849.1057988},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {0},
numpages = {19}
}

@inproceedings{10.1007/11497455_15,
author = {Cer\'{o}n, Rodrigo and Due\~{n}as, Juan C. and Serrano, Enrique and Capilla, Rafael},
title = {A meta-model for requirements engineering in system family context for software process improvement using CMMI},
year = {2005},
isbn = {3540262008},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11497455_15},
doi = {10.1007/11497455_15},
abstract = {Software industries are pursuing the development of software intensive systems with a greater degree of re-use, reduction of costs, and shorter time to market. One of the successful approaches taken is based on the development of sets of similar systems where development efforts are shared. This approach is known as System Families. This article discusses an important issue in system family engineering activities: requirements modelling in system family context. The requirements must contain both the common and variable parts. Also, functional and non-functional aspects have to be considered in system family approach. Besides, an organization framework must be taken into account for requirements management. Some meta-models for these issues in system family are proposed and discussed. Based on the proposed model, a process for requirements management and development according to CMMI practices has been created.},
booktitle = {Proceedings of the 6th International Conference on Product Focused Software Process Improvement},
pages = {173–188},
numpages = {16},
location = {Oulu, Finland},
series = {PROFES'05}
}

@inproceedings{10.1145/168642.168664,
author = {Stadelmann, Marc},
title = {A spreadsheet based on constraints},
year = {1993},
isbn = {089791628X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/168642.168664},
doi = {10.1145/168642.168664},
booktitle = {Proceedings of the 6th Annual ACM Symposium on User Interface Software and Technology},
pages = {217–224},
numpages = {8},
keywords = {APL, constraint satisfaction, constraints, matrices, spreadsheets, user-interface, vectors},
location = {Atlanta, Georgia, USA},
series = {UIST '93}
}

@inproceedings{10.5555/1927661.1927706,
author = {Duan, Jinan and Zhu, Qunxiong and Guan, Zhong},
title = {A requirement-driven approach to enterprise application development},
year = {2010},
isbn = {3642165141},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The requirements changes are the root causes of evolution of enterprise applications. How to effectively develop enterprise application with the frequently changing requirements is still a challenge to software engineering. The two main aspects are how to capture requirements changes and then how to reflect them to the applications. Use cases and refactoring are excellent tools to capture functional requirements and to change object-oriented software gradually. This paper presents a requirement-driven approach to enterprise application development. The approach uses refined use cases to capture the requirements and to build domain models, controller logics and views. It transforms requirement changes into the refactorings of refined use cases, thus it can propagate the modification to the application. With rapidly continuous iterations, this approach tries to give a solution to the problems of enterprise applications development.},
booktitle = {Proceedings of the 2010 International Conference on Web Information Systems and Mining},
pages = {295–302},
numpages = {8},
keywords = {enterprise application, refactoring, refined use cases, requirement-driven},
location = {Sanya, China},
series = {WISM'10}
}

@inproceedings{10.1145/1858378.1858445,
author = {Krishnamurthy, Vallidevi and Babu, Chitra},
title = {Effective self adaptation by integrating adaptive framework with architectural patterns},
year = {2010},
isbn = {9781450301947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858378.1858445},
doi = {10.1145/1858378.1858445},
abstract = {As software development becomes large-scale, the major focus of the developers has shifted from algorithms and data structures to architectures of the software systems. Architecture provides a bridge between requirements and implementation. Until recently, mechanism for self adaptation is largely in the form of programming language features embedded in code, thus prohibiting reusability and modifiability. This paper advocates the use of an external mechanism which monitors the running system and updates the architectural properties based on the changes in the system. Haphazard changes made in the architecture might make the system unstable. In order to address this issue, this paper proposes an approach for dynamic self adaptation by integrating the adaptive framework with architectural patterns. Since the architectural patterns have some predefined and proven structure, they make the system amenable for easy adoption. The changes in the architecture are mapped back to the running system. The external mechanism thus monitors and makes changes to the running system dynamically. The proposed approach has been tested by applying it to a sample e-Shopping application case study.},
booktitle = {Proceedings of the 1st Amrita ACM-W Celebration on Women in Computing in India},
articleno = {67},
numpages = {4},
keywords = {architecture description language, dynamic self adaptation, external control mechanism, patterns, software architecture},
location = {Coimbatore, India},
series = {A2CWiC '10}
}

@article{10.1145/1095430.1095436,
author = {Dalal, Siddhartha R. and Jain, Ashish and Poore, Jesse},
title = {Advances in model-based software testing (A-MOST)},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1095430.1095436},
doi = {10.1145/1095430.1095436},
abstract = {This is a summary of the Advances in Model-Based Software Testing (A-MOST) workshop held on May 15-16, 2005, in St. Louis, Missouri. The workshop had approximately 40 participants. The goals of this workshop were to offer a comprehensive overview of model-based testing to the ICSE community, and bring the researchers and practitioners together to discuss advances, applications, and the complex problems yet to be solved in model-based testing. The workshop was lively and maintained participant interest for the entire two-day program. A brief summary based upon agenda items is provided. Details on the workshops are archived at http://aetgweb.argreenhouse.com.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–3},
numpages = {3}
}

@inproceedings{10.1145/1985793.1985856,
author = {She, Steven and Lotufo, Rafael and Berger, Thorsten and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {Reverse engineering feature models},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985856},
doi = {10.1145/1985793.1985856},
abstract = {Feature models describe the common and variable characteristics of a product line. Their advantages are well recognized in product line methods. Unfortunately, creating a feature model for an existing project is time-consuming and requires substantial effort from a modeler.We present procedures for reverse engineering feature models based on a crucial heuristic for identifying parents - the major challenge of this task. We also automatically recover constructs such as feature groups, mandatory features, and implies/excludes edges. We evaluate the technique on two large-scale software product lines with existing reference feature models--the Linux and eCos kernels--and FreeBSD, a project without a feature model. Our heuristic is effective across all three projects by ranking the correct parent among the top results for a vast majority of features. The procedures effectively reduce the information a modeler has to consider from thousands of choices to typically five or less.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {461–470},
numpages = {10},
keywords = {feature models, feature similarity, variability modeling},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@article{10.1007/s10270-019-00735-y,
author = {Wolny, Sabine and Mazak, Alexandra and Carpella, Christine and Geist, Verena and Wimmer, Manuel},
title = {Thirteen years of SysML: a systematic mapping study},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-019-00735-y},
doi = {10.1007/s10270-019-00735-y},
abstract = {The OMG standard Systems Modeling Language (SysML) has been on the market for about thirteen years. This standard is an extended subset of UML providing a graphical modeling language for designing complex systems by considering software as well as hardware parts. Over the period of thirteen years, many publications have covered various aspects of SysML in different research fields. The aim of this paper is to conduct a systematic mapping study about SysML to identify the different categories of papers, (i) to get an overview of existing research topics and groups, (ii) to identify whether there are any publication trends, and (iii) to uncover possible missing links. We followed the guidelines for conducting a systematic mapping study by Petersen et al. (Inf Softw Technol 64:1–18, 2015) to analyze SysML publications from 2005 to 2017. Our analysis revealed the following main findings: (i) there is a growing scientific interest in SysML in the last years particularly in the research field of Software Engineering, (ii) SysML is mostly used in the design or validation phase, rather than in the implementation phase, (iii) the most commonly used diagram types are the SysML-specific requirement diagram, parametric diagram, and block diagram, together with the activity diagram and state machine diagram known from UML, (iv) SysML is a specific UML profile mostly used in systems engineering; however, the language has to be customized to accommodate domain-specific aspects, (v) related to collaborations for SysML research over the world, there are more individual research groups than large international networks. This study provides a solid basis for classifying existing approaches for SysML. Researchers can use our results (i) for identifying open research issues, (ii) for a better understanding of the state of the art, and (iii) as a reference for finding specific approaches about SysML.},
journal = {Softw. Syst. Model.},
month = jan,
pages = {111–169},
numpages = {59},
keywords = {SysML, Systematic mapping study, Systems engineering}
}

@article{10.1016/j.infsof.2012.07.007,
author = {Wendler, Roy},
title = {The maturity of maturity model research: A systematic mapping study},
year = {2012},
issue_date = {December, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {12},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.007},
doi = {10.1016/j.infsof.2012.07.007},
abstract = {Context: Maturity models offer organizations a simple but effective possibility to measure the quality of their processes. Emerged out of software engineering, the application fields have widened and maturity model research is becoming more important. During the last two decades the publication amount steadily rose as well. Until today, no studies have been available summarizing the activities and results of the field of maturity model research. Objective: The objective of this paper is to structure and analyze the available literature of the field of maturity model research to identify the state-of-the-art research as well as research gaps. Method: A systematic mapping study was conducted. It included relevant publications of journals and IS conferences. Mapping studies are a suitable method for structuring a broad research field concerning research questions about contents, methods, and trends in the available publications. Results: The mapping of 237 articles showed that current maturity model research is applicable to more than 20 domains, heavily dominated by software development and software engineering. The study revealed that most publications deal with the development of maturity models and empirical studies. Theoretical reflective publications are scarce. Furthermore, the relation between conceptual and design-oriented maturity model development was analyzed, indicating that there is still a gap in evaluating and validating developed maturity models. Finally, a comprehensive research framework was derived from the study results and implications for further research are given. Conclusion: The mapping study delivers the first systematic summary of maturity model research. The categorization of available publications helps researchers gain an overview of the state-of-the-art research and current research gaps. The proposed research framework supports researchers categorizing their own projects. In addition, practitioners planning to use a maturity model may use the study as starting point to identify which maturity models are suitable for their domain and where limitations exist.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {1317–1339},
numpages = {23},
keywords = {Design-oriented research, Maturity models, Software management, Systematic mapping study}
}

@inproceedings{10.5555/1765571.1765583,
author = {Liu, Shih-Hsi and Bryant, Barrett R. and Auguston, Mikhail and Gray, Jeff and Raje, Rajeev and Tuceryan, Mihran},
title = {A component-based approach for constructing high-confidence distributed real-time and embedded systems},
year = {2005},
isbn = {9783540711551},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In applying Component-Based Software Engineering (CBSE) techniques to the domain of Distributed Real-time and Embedded (DRE) Systems, there are five critical challenges: 1) discovery of relevant components and resources, 2) specification and modeling of components, 3) exploration and elimination of design assembly options, 4) automated generation of heterogeneous component bridges, and 5) validation of context-related embedded systems. To address these challenges, this paper introduces four core techniques to facilitate high-confidence DRE system construction from components: 1) A component and resource discovery technique promotes component searching based on rich and precise descriptions of components and context; 2) A timed colored Petri Net-based modeling toolkit enables design and analysis on DRE systems, as well as reduces unnecessary later work by eliminating infeasible design options; 3) A formal specification language describes all specifications consistently and automatically generates component bridges for seamless system integration; and 4) A grammar-based formalism specifies context behaviors and validates integrated systems using sufficient context-related test cases. The success of these ongoing techniques may not only accelerate the software development pace and reduce unnecessary development cost, but also facilitate high-confidence DRE system construction using different formalisms over the entire software life-cycle.},
booktitle = {Proceedings of the 12th Monterey Conference on Reliable Systems on Unreliable Networked Platforms},
pages = {225–247},
numpages = {23},
location = {Laguna Beach, CA, 2005}
}

@article{10.1016/j.patcog.2011.12.019,
author = {Graves, Daniel and Noppen, Joost and Pedrycz, Witold},
title = {Clustering with proximity knowledge and relational knowledge},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {7},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2011.12.019},
doi = {10.1016/j.patcog.2011.12.019},
abstract = {In this article, a proximity fuzzy framework for clustering relational data is presented, where the relationships between the entities of the data are given in terms of proximity values. We offer a comprehensive and in-depth comparison of our clustering framework with proximity relational knowledge to clustering with distance relational knowledge, such as the well known relational Fuzzy C-Means (FCM). We conclude that proximity can provide a richer description of the relationships among the data and this offers a significant advantage when realizing clustering. We further motivate clustering relational proximity data and provide both synthetic and real-world experiments to demonstrate both the usefulness and advantage offered by clustering proximity data. Finally, a case study of relational clustering is introduced where we apply proximity fuzzy clustering to the problem of clustering a set of trees derived from software requirements engineering. The relationships between trees are based on the degree of closeness in both the location of the nodes in the trees and the semantics associated with the type of connections between the nodes.},
journal = {Pattern Recogn.},
month = jul,
pages = {2633–2644},
numpages = {12},
keywords = {Fuzzy clustering, Knowledge representation, Proximity, Relational clustering, Software requirements}
}

@article{10.1016/j.jss.2009.11.709,
author = {Zhang, Pengcheng and Muccini, Henry and Li, Bixin},
title = {A classification and comparison of model checking software architecture techniques},
year = {2010},
issue_date = {May, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.11.709},
doi = {10.1016/j.jss.2009.11.709},
abstract = {Software architecture specifications are used for many different purposes, such as documenting architectural decisions, predicting architectural qualities before the system is implemented, and guiding the design and coding process. In these contexts, assessing the architectural model as early as possible becomes a relevant challenge. Various analysis techniques have been proposed for testing, model checking, and evaluating performance based on architectural models. Among them, model checking is an exhaustive and automatic verification technique, used to verify whether an architectural specification conforms to expected properties. While model checking is being extensively applied to software architectures, little work has been done to comprehensively enumerate and classify these different techniques. The goal of this paper is to investigate the state-of-the-art in model checking software architectures. For this purpose, we first define the main activities in a model checking software architecture process. Then, we define a classification and comparison framework and compare model checking software architecture techniques according to it.},
journal = {J. Syst. Softw.},
month = may,
pages = {723–744},
numpages = {22},
keywords = {Model checking, Software architecture}
}

@article{10.1016/j.future.2019.04.015,
author = {Zalila, Faiez and Challita, St\'{e}phanie and Merle, Philippe},
title = {Model-driven cloud resource management with OCCIware},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {99},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.04.015},
doi = {10.1016/j.future.2019.04.015},
journal = {Future Gener. Comput. Syst.},
month = oct,
pages = {260–277},
numpages = {18},
keywords = {Cloud computing, Service computing, Model-driven engineering (MDE), Meta modeling, Models@runtime, Software standards, Computer-aided software engineering, Distributed information systems, Modeling environments}
}

@article{10.1145/1543405.1543432,
author = {Baillargeon, Robert and France, Robert and Zschaler, Steffen and Rumpe, Bernhard and V\"{o}lkel, Steven and Georg, Geri},
title = {Workshop on modeling in software engineering at ICSE 2009},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1543405.1543432},
doi = {10.1145/1543405.1543432},
abstract = {The Modeling in Software Engineering (MiSE) workshop series provides a forum for discussing the challenges associated with modeling software and with incorporating modeling practices into the software development process. The main goal of the series is to further promote cross-fertilization between the modeling communities (e.g., MODELS) and software-engineering communities.In particular, the workshop provides a medium to exchange innovative technical ideas and experiences related to modeling. The 2009 MiSE workshop provided a venue for presentation and discussion of eleven papers in the five areas of model evolution, domain specific languages, verification and validation, model transformation and state-of-the-art modeling usage in software development. These papers represent a 44% acceptance rate to the workshop. Three posters were also accepted. This report summarizes the discussions and conclusions of the workshop.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {34–37},
numpages = {4},
keywords = {MiSE workshop, software modeling}
}

@article{10.1016/j.jss.2016.12.016,
author = {Landwehr, Carl and Ludewig, Jochen and Meersman, Robert and Parnas, David Lorge and Shoval, Peretz and Wand, Yair and Weiss, David and Weyuker, Elaine},
title = {Software Systems Engineering programmes a capability approach},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {125},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.12.016},
doi = {10.1016/j.jss.2016.12.016},
abstract = {Discusses undergraduate programmes that prepare graduates for a career building software intensive systems.Presents detailed description of capabilities that graduates should have acquired.Derived from historical discussions of the field.Explains differences between Science programmes and Engineering programmes.Presents a broad set of specialized programs. This paper discusses third-level educational programmes that are intended to prepare their graduates for a career building systems in which software plays a major role. Such programmes are modelled on traditional Engineering programmes but have been tailored to applications that depend heavily on software. Rather than describe knowledge that should be taught, we describe capabilities that students should acquire in these programmes. The paper begins with some historical observations about the software development field.},
journal = {J. Syst. Softw.},
month = mar,
pages = {354–364},
numpages = {11},
keywords = {Education, Engineering, Information systems, Software design, Software development, Software documentation, Software education}
}

@inproceedings{10.5555/2041790.2041811,
author = {Gamez, Nadia and Fuentes, Lidia and Arag\"{u}ez, Miguel A.},
title = {Autonomic computing driven by feature models and architecture in FamiWare},
year = {2011},
isbn = {9783642237973},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A wireless sensor network is an example of a system that should be able to adapt its sensor nodes to some context changes with minimum human intervention. This means that the architecture of the middleware for sensors must encapsulate a dynamic mechanism to allow reconfiguration. We present a novel approach to achieve self-adaptation based on software product lines and on the autonomic computing paradigm for the FamiWare middleware. FamiWare uses feature models to represent the potential middleware configurations at runtime. Each configuration is automatically mapped to the corresponding architectural representation of a specific middleware product. Following the autonomic computing principles, FamiWare defines a reconfiguration mechanism that switches from one architectural configuration to another by means of executing a plan. This is possible thanks to the loosely coupled architecture of FamiWare based on an event-based publish and subscribe mechanism. We evaluate our work by showing that the resource consumption and the overhead are not so critical compared with the benefits of providing this self-adaptation mechanism.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture},
pages = {164–179},
numpages = {16},
keywords = {autonomic computing, event-based architectures, feature models, middleware, models at runtime, product lines architectures},
location = {Essen, Germany},
series = {ECSA'11}
}

@article{10.1016/j.scico.2010.10.007,
author = {Olszak, Andrzej and N\o{}rregaard J\o{}rgensen, Bo},
title = {Remodularizing Java programs for improved locality of feature implementations in source code},
year = {2012},
issue_date = {March, 2012},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {77},
number = {3},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2010.10.007},
doi = {10.1016/j.scico.2010.10.007},
abstract = {Explicit traceability between features and source code is known to help programmers to understand and modify programs during maintenance tasks. However, the complex relations between features and their implementations are not evident from the source code of object-oriented Java programs. Consequently, the implementations of individual features are difficult to locate, comprehend, and modify in isolation. In this paper, we present a novel remodularization approach that improves the representation of features in the source code of Java programs. Both forward and reverse restructurings are supported through on-demand bidirectional restructuring between feature-oriented and object-oriented decompositions. The approach includes a feature location phase based on tracing of program execution, a feature representation phase that reallocates classes into a new package structure based on single-feature and multi-feature packages, and an annotation-based reverse transformation of code. Case studies performed on two open-source projects indicate that our approach requires relatively little manual effort and reduces tangling and scattering of feature implementations in the source code.},
journal = {Sci. Comput. Program.},
month = mar,
pages = {131–151},
numpages = {21},
keywords = {Feature location, Features, Fragile decomposition problem, Remodularization}
}

@inproceedings{10.1145/1629716.1629736,
author = {Tekinerdoundefinedan, Bedir and Aktekin, Namik},
title = {Interaction-based feature-driven model-transformations for generating e-forms},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629736},
doi = {10.1145/1629716.1629736},
abstract = {One of the basic pillars in Model-Driven Software Development (MDSD) is defined by model transformations and likewise several useful approaches have been proposed in this context. In parallel, domain modeling plays an essential role in MDSD to support the definition of concepts in the domain, and support the model transformation process. In this paper, we will discuss the results of an e-government project for the generation of e-forms from feature models. Very often existing model transformation practices seem to largely adopt a closed world assumption whereby the transformation definitions of models are defined beforehand and interaction with the user at run-time is largely omitted. Our study shows the need for a more interactive approach in model transformations in which e-forms are generated after interaction with the end-user. To show the case we illustrate three different approaches for generation in increasing complexity: (1) offline model transformation without interaction (2) model transformation with initial interaction (3) model-transformation with run-time interaction.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {103–108},
numpages = {6},
keywords = {e-government, feature-oriented modeling, model-driven software development},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.1007/s10270-013-0343-7,
author = {Kusel, A. and Sch\"{o}nb\"{o}ck, J. and Wimmer, M. and Kappel, G. and Retschitzegger, W. and Schwinger, W.},
title = {Reuse in model-to-model transformation languages: are we there yet?},
year = {2015},
issue_date = {May       2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-013-0343-7},
doi = {10.1007/s10270-013-0343-7},
abstract = {In the area of model-driven engineering, model transformations are proposed as the technique to systematically manipulate models. For increasing development productivity as well as quality of model transformations, reuse mechanisms are indispensable. Although numerous mechanisms have been proposed, no systematic comparison exists, making it unclear, which reuse mechanisms may be best employed in a certain situation. Thus, this paper provides an in-depth comparison of reuse mechanisms in model-to-model transformation languages and categorizes them along their intended scope of application. Finally, current barriers and facilitators to model transformation reuse are discussed.},
journal = {Softw. Syst. Model.},
month = may,
pages = {537–572},
numpages = {36},
keywords = {Model transformations, Model-driven engineering, Reuse mechanisms, Survey}
}

@article{10.1016/j.future.2014.08.015,
author = {Pascual, Gustavo G. and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Self-adaptation of mobile systems driven by the Common Variability Language},
year = {2015},
issue_date = {June 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2014.08.015},
doi = {10.1016/j.future.2014.08.015},
abstract = {The execution context in which pervasive systems or mobile computing run changes continually. Hence, applications for these systems require support for self-adaptation to the continual context changes. Most of the approaches for self-adaptive systems implement a reconfiguration service that receives as input the list of all possible configurations and the plans to switch between them. In this paper we present an alternative approach for the automatic generation of application configurations and the reconfiguration plans at runtime. With our approach, the generated configurations are optimal as regards different criteria, such as functionality or resource consumption (e.g. battery or memory). This is achieved by: (1) modelling architectural variability at design-time using the Common Variability Language (CVL), and (2) using a genetic algorithm that finds nearly-optimal configurations at run-time using the information provided by the variability model. We also specify a case study and we use it to evaluate our approach, showing that it is efficient and suitable for devices with scarce resources. We specify an approach for the dynamic reconfiguration of mobile applications.We model a mobile application with variability which can be reconfigured at runtime.We simulate the execution of the mobile application when our dynamic reconfiguration service is applied and not applied, respectively.We measure the battery life as well as the overall utility of the application perceived by the user.Applying our dynamic reconfiguration, the battery life is incremented by 45.9% and the utility is incremented by 10.31%.},
journal = {Future Gener. Comput. Syst.},
month = jun,
pages = {127–144},
numpages = {18},
keywords = {Architectural variability, CVL, Context, Dynamic reconfiguration, Genetic algorithm, Pervasive systems}
}

@article{10.1016/j.infsof.2009.04.001,
author = {Nicol\'{a}s, Joaqu\'{\i}n and Toval, Ambrosio},
title = {On the generation of requirements specifications from software engineering models: A systematic literature review},
year = {2009},
issue_date = {September, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.04.001},
doi = {10.1016/j.infsof.2009.04.001},
abstract = {System and software requirements documents play a crucial role in software engineering in that they must both communicate requirements to clients in an understandable manner and define requirements in precise detail for system developers. The benefits of both lists of textual requirements (usually written in natural language) and software engineering models (usually specified in graphical form) can be brought together by combining the two approaches in the specification of system and software requirements documents. If, moreover, textual requirements are generated from models in an automatic or closely monitored form, the effort of specifying those requirements is reduced and the completeness of the specification and the management of the requirements traceability are improved. This paper presents a systematic review of the literature related to the generation of textual requirements specifications from software engineering models.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {1291–1307},
numpages = {17},
keywords = {Requirements document generation from software engineering model, Specification generation from software engineering model, Systematic literature review, Textual requirements generation from software engineering model}
}

@inproceedings{10.1007/978-3-642-40276-0_8,
author = {Corral, Luis and Sillitti, Alberto and Succi, Giancarlo},
title = {Agile Software Development Processes for Mobile Systems: Accomplishment, Evidence and Evolution},
year = {2013},
isbn = {9783642402753},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-40276-0_8},
doi = {10.1007/978-3-642-40276-0_8},
abstract = {Mobile software applications have to cope with a particular execution environment that includes limited resources, high autonomy requirements, market regulations, and many other constraints. To provide a software development process that responds to these challenges, several methodologies proposed the adoption of Agile practices; however, it is not clear how a software development process would help to solve all the issues present in the mobile domain. Moreover, the rapid evolution of the mobile environment questions several of the premises upon which the proposed methodologies were designed. In this paper, we present a review on Agile software development processes for mobile applications and their implementations, with the objective of knowing the contribution of Agile methods to address the needs of the mobile software in a real production environment. In addition, we aim to put up to date the discussion about what are the best practices that facilitate the creation of high quality software products in the current mobile domain.},
booktitle = {Proceedings of the 10th International Conference on Mobile Web Information Systems - Volume 8093},
pages = {90–106},
numpages = {17},
keywords = {Agile, Development, Mobile, Process, Quality},
location = {Paphos, Cyprus},
series = {MobiWIS 2013}
}

@article{10.1145/375360.375367,
author = {Weihe, Karsten},
title = {A software engineering perspective on algorithmics},
year = {2001},
issue_date = {March 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/375360.375367},
doi = {10.1145/375360.375367},
abstract = {An algorithm component is an implementation of an algorithm which is not intended to be a stand-alone module, but to perform a specific task within a large software package or even within several distinct software packages. Therefore, the design of algorithm components must also incorporate software-engineering aspects. A key design goal is adaptability. This goal is important for maintenance throughout a project, prototypical development, and reuse in new, unforseen contexts. From a theoretical viewpoint most algorithms apply  to a range of possible use scenarios. Ideally, each algorithm is implemented by one algorithm component, which is easily, safely, and efficiently adaptable to all of these contexts.Various techniques have been developed for the design and implementation of algorithm components. However, a common basis for systematic, detailed evaluations and comparisons in view of the real practical needs is still missing. Basically, this means a set of concrete criteria, which specify what sort of adaptability is really required in practice, and which are well-justified by convincing, representative use scenarios.This paper is intended to be a first “milestone” on the way towards such a system of criteria. We will present a set of concrete goals, which are general and problem-independent and might appear ubiquitously in the algorithmic realm. These goals are illustrated, motivated, and justified by an extensive requirements analysis for a particular algorithm from a     particular algorithmic domain: Dijkstra's algorithm for shortest paths in networks.Clearly, the field of algorithmics might be too versatile to allow a comprehensive, yet concise set of precise, justified criteria. Even a domain as restricted as graph and network algorithms includes aspects that are not fully understood. The analysis will include a discussion of the limits of the case study and the scope of the goals. The case study was chosen because it seems to be close to the “boderline” between the aspects that are well understood and the aspects that are not. Hence, this example may well serve as an“acid test” for programming techniques in view of the state of the art.},
journal = {ACM Comput. Surv.},
month = mar,
pages = {89–134},
numpages = {46},
keywords = {algorithm engineering}
}

@inproceedings{10.1109/MISE.2007.3,
author = {Ubayashi, Naoyasu and Sano, Shinji and Otsubo, Genya},
title = {A Reflective Aspect-Oriented Model Editor Based on Metamodel Extension},
year = {2007},
isbn = {0769529534},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MISE.2007.3},
doi = {10.1109/MISE.2007.3},
abstract = {AspectM, an aspect-oriented modeling language, provides not only basic modeling constructs but also an extension mechanism called metamodel access protocol (MMAP) that allows a modeler to modify the metamodel. MMAP consists of metamodel extension points, extension operations, and primitive predicates for defining pointcut designators. In this paper, a reflective model editor for supporting MMAP is proposed. A new modeling construct can be introduced by extending the metamodel. This mechanism, a kind of edit-time structural reflection, enables a modeler to represent domain-specific crosscutting concerns.},
booktitle = {Proceedings of the International Workshop on Modeling in Software Engineering},
pages = {12},
series = {MISE '07}
}

@article{10.1145/3093895,
author = {Longo, Antonella and Zappatore, Marco and Bochicchio, Mario and Navathe, Shamkant B.},
title = {Crowd-Sourced Data Collection for Urban Monitoring via Mobile Sensors},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3093895},
doi = {10.1145/3093895},
abstract = {A considerable amount of research has addressed Internet of Things and connected communities. It is possible to exploit the sensing capabilities of connected communities, by leveraging the continuously growing use of cloud computing solutions and mobile devices. The pervasiveness of mobile sensors also enables the Mobile Crowd Sensing (MCS) paradigm, which aims at using mobile-embedded sensors to extend monitoring of multiple (environmental) phenomena in expansive urban areas. In this article, we discuss our approach with a cloud-based platform to pave the way for applying crowd sensing in urban scenarios. We have implemented a complete solution for environmental monitoring of several pollutants, like noise, air, electromagnetic fields, and so on in an urban area based on this paradigm. Through extensive experimentation, specifically on noise pollution, we show how the proposed infrastructure exhibits the ability to collect data from connected communities, and enables a seamless support of services needed for improving citizens’ quality of life and eventually helps city decision makers in urban planning.},
journal = {ACM Trans. Internet Technol.},
month = oct,
articleno = {5},
numpages = {21},
keywords = {Mobile crowed sensing, smart cities, social sensing}
}

@article{10.1007/s00371-018-1475-0,
author = {Yang, Chunlei and Pu, Jiexin and Dong, Yongsheng and Xie, Guo-Sen and Si, Yanna and Liu, Zhonghua},
title = {Scene classification-oriented saliency detection via the modularized prescription},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {4},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-018-1475-0},
doi = {10.1007/s00371-018-1475-0},
abstract = {Saliency detection technology has been greatly developed and applied in recent years. However, the performance of current methods is not satisfactory in complex scenes. One of the reasons is that the performance improvement is often carried out through utilizing complicated mathematical models and involving multiple features rather than classifying the scene complexity and respectively detecting saliency. To break this unified detection schema for generating better results, we propose a method of scene classification-oriented saliency detection via the modularized prescription in this paper. Different scenes are described by a scene complexity expression model, and they are analyzed and discriminately detected by different pipelines. This process seems like that doctors can tailor the treatment prescriptions when they meet different symptoms. Moreover, two SVM-based classifiers are trained for scene classification and sky region identification, and the proposed sky region discrimination and erase model can be used to efficiently decrease the saliency interference by the high luminance of the background sky regions. Experimental results demonstrate the effectiveness and superiority of the proposed method in both higher precision and better smoothness, especially for detecting in structure complex scenes.},
journal = {Vis. Comput.},
month = apr,
pages = {473–488},
numpages = {16},
keywords = {Modularized prescription, Saliency detection, Scene classification, Scene complexity expression, Support Vector Machine (SVM)}
}

@article{10.1145/2729974,
author = {Mkaouer, Wiem and Kessentini, Marouane and Shaout, Adnan and Koligheu, Patrice and Bechikh, Slim and Deb, Kalyanmoy and Ouni, Ali},
title = {Many-Objective Software Remodularization Using NSGA-III},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2729974},
doi = {10.1145/2729974},
abstract = {Software systems nowadays are complex and difficult to maintain due to continuous changes and bad design choices. To handle the complexity of systems, software products are, in general, decomposed in terms of packages/modules containing classes that are dependent. However, it is challenging to automatically remodularize systems to improve their maintainability. The majority of existing remodularization work mainly satisfy one objective which is improving the structure of packages by optimizing coupling and cohesion. In addition, most of existing studies are limited to only few operation types such as move class and split packages. Many other objectives, such as the design semantics, reducing the number of changes and maximizing the consistency with development change history, are important to improve the quality of the software by remodularizing it. In this article, we propose a novel many-objective search-based approach using NSGA-III. The process aims at finding the optimal remodularization solutions that improve the structure of packages, minimize the number of changes, preserve semantics coherence, and reuse the history of changes. We evaluate the efficiency of our approach using four different open-source systems and one automotive industry project, provided by our industrial partner, through a quantitative and qualitative study conducted with software engineers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {17},
numpages = {45},
keywords = {Search-based software engineering, remodularization, software maintenance, software quality}
}

@article{10.1016/j.micpro.2021.104015,
author = {Zhang, Chunmei and Wu, Fengling and Zhang, Chengming},
title = {RETRACTED: Pentium microprocessor system structure and principal of political teaching and teaching material improvement},
year = {2021},
issue_date = {Jun 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {83},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2021.104015},
doi = {10.1016/j.micpro.2021.104015},
journal = {Microprocess. Microsyst.},
month = jun,
numpages = {7}
}

@article{10.1007/s00779-010-0356-y,
author = {L\'{e}zoray, Jean-Baptiste and Segarra, Maria-Teresa and Phung-Khac, An and Th\'{e}paut, Andr\'{e} and Gilliot, Jean-Marie and Beugnard, Antoine},
title = {A design process enabling adaptation in pervasive heterogeneous contexts},
year = {2011},
issue_date = {April     2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {4},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-010-0356-y},
doi = {10.1007/s00779-010-0356-y},
abstract = {In the next decades, the growth in population aging will cause important problems to most industrialized countries. To tackle this issue, Ambient Assistive Living (AAL) systems can reinforce the well-being of elderly people, by providing emergency, autonomy enhancement, and comfort services. These services will postpone the need of a medicalized environment and will allow the elderly to stay longer at home. However, each elderly has specific needs and a deployment environment of such services is likely unique. Furthermore, the needs evolve over time, and so does the deployment environment of the system. In this paper, we propose the use of a model-based development method, the adaptive medium approach, to enable dynamic adaptation of AAL systems. We also propose improvements to make it more suited to the AAL domain, such as considering heterogeneity and a composition model. The paper includes an evaluation of the prototype implementing the approach, and a comparison with related work.},
journal = {Personal Ubiquitous Comput.},
month = apr,
pages = {353–363},
numpages = {11},
keywords = {AAL, Adaptive medium approach, Dynamic adaptation, Heterogeneity, Model-driven engineering}
}

@article{10.1016/j.infsof.2016.03.008,
author = {Bjarnason, Elizabeth and Unterkalmsteiner, Michael and Borg, Markus and Engstr\"{o}m, Emelie},
title = {A multi-case study of agile requirements engineering and the use of test cases as requirements},
year = {2016},
issue_date = {September 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {77},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.03.008},
doi = {10.1016/j.infsof.2016.03.008},
abstract = {ContextIt is an enigma that agile projects can succeed 'without requirements' when weak requirements engineering is a known cause for project failures. While agile development projects often manage well without extensive requirements test cases are commonly viewed as requirements and detailed requirements are documented as test cases. ObjectiveWe have investigated this agile practice of using test cases as requirements to understand how test cases can support the main requirements activities, and how this practice varies. MethodWe performed an iterative case study at three companies and collected data through 14 interviews and two focus groups. ResultsThe use of test cases as requirements poses both benefits and challenges when eliciting, validating, verifying, and managing requirements, and when used as a documented agreement. We have identified five variants of the test-cases-as-requirements practice, namely de facto, behaviour-driven, story-test driven, stand-alone strict and stand-alone manual for which the application of the practice varies concerning the time frame of requirements documentation, the requirements format, the extent to which the test cases are a machine executable specification and the use of tools which provide specific support for the practice of using test cases as requirements. ConclusionsThe findings provide empirical insight into how agile development projects manage and communicate requirements. The identified variants of the practice of using test cases as requirements can be used to perform in-depth investigations into agile requirements engineering. Practitioners can use the provided recommendations as a guide in designing and improving their agile requirements practices based on project characteristics such as number of stakeholders and rate of change.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {61–79},
numpages = {19},
keywords = {Acceptance test, Agile development, Behaviour-driven development, Case study, Empirical software engineering, Requirements, Test-driven development, Test-first development, Testing}
}

@inproceedings{10.1109/ICSSP.2019.00028,
author = {Alam, Omar},
title = {Towards an agile concern-driven development process},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSSP.2019.00028},
doi = {10.1109/ICSSP.2019.00028},
abstract = {This paper proposes an Agile Concern-Driven Development (Agile CDD) process, a software development process that uses concerns as its primary artifact and applies agile practices. Whereas classical Model-Driven Engineering (MDE) methodologies focus on models that are built from scratch with little support for reuse, Agile CDD is a reuse-focused development process in which an application is built incrementally by repeatedly reusing other existing concerns. In Agile CDD, a modeler would use a modelling language that is appropriate for the current development phase and for the problem domain. Model transformations would then be applied to produce the initial set of models for the next phase. The process will continue until an execute model is produced. In each phase, the modeller should consult a repository of reusable concerns to identify and reuse concerns. Changing requirements are welcome and incomplete implementations are moved to the next iteration by delaying design decisions.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {155–159},
numpages = {5},
location = {Montreal, Quebec, Canada},
series = {ICSSP '19}
}

@article{10.1016/j.infsof.2019.04.003,
author = {Rodrigues, Gabriel S. and Guimar\~{a}es, Felipe P. and Rodrigues, Gena\'{\i}na N. and Knauss, Alessia and de Ara\'{u}jo, Jo\~{a}o Paulo C. and Andrade, Hugo and Ali, Raian},
title = {GoalD: A Goal-Driven deployment framework for dynamic and heterogeneous computing environments},
year = {2019},
issue_date = {Jul 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {111},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.04.003},
doi = {10.1016/j.infsof.2019.04.003},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {159–176},
numpages = {18},
keywords = {Autonomous deployment, Contextual goal modelling, Heterogeneous computational resources, Deployment planning}
}

@inproceedings{10.1007/11554844_5,
author = {Eriksson, Magnus and B\"{o}rstler, J\"{u}rgen and Borg, Kjell},
title = {The PLUSS approach: domain modeling with features, use cases and use case realizations},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_5},
doi = {10.1007/11554844_5},
abstract = {This paper describes a product line use case modeling approach tailored towards organizations developing and maintaining extremely long lived software intensive systems. We refer to the approach as the PLUSS approach, Product Line Use case modeling for Systems and Software engineering . An industrial case study is presented where PLUSS is applied and evaluated in the target domain. Based on the case study data we draw the conclusion that PLUSS performs better than modeling according to the styles and guidelines specified by the IBM-Rational Unified Process (RUP) in the current industrial context.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {33–44},
numpages = {12},
location = {Rennes, France},
series = {SPLC'05}
}

@article{10.1016/j.jss.2013.11.1121,
author = {Sutcliffe, Alistair and Papamargaritis, George},
title = {End-user development by application-domain configuration},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.11.1121},
doi = {10.1016/j.jss.2013.11.1121},
abstract = {An application generator/tailoring tool aimed at end users is described. It employs conceptual models of problem domains to drive configuration of an application generator suitable for a related set of applications, such as reservation and resource allocation. The tool supports a two-phase approach of configuring the general architecture for a domain, such as reservation-booking problems, then customisation and generation of specific applications. The tool also provides customisable natural language-style queries for spatial and temporal terms. Development and use of the tool to generate two applications, service engineer call allocation, and airline seat reservation, are reported with a specification exercise to configure the generic architecture to a new problem domain for monitoring-sensing applications. The application generator/tailoring tool is evaluated with novice end users and experts to demonstrate its effectiveness.},
journal = {J. Syst. Softw.},
month = may,
pages = {85–99},
numpages = {15},
keywords = {Application generation, Domain-oriented design, End-user development}
}

@inproceedings{10.1145/3397481.3450678,
author = {Reyes, Guillermo and Alles, Alexandra},
title = {Multi-modal Multi-scale Attention Guidance in Cyber-Physical Environments},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397481.3450678},
doi = {10.1145/3397481.3450678},
abstract = {This work proposes a new method for guiding a user’s attention towards objects of interest in a cyber-physical environment (CPE). CPEs are environments that contain several computing systems that interact with each other and with the physical world. These environments contain several sensors (cameras, eye trackers, etc.) and output devices (lamps, screens, speakers, etc.). These devices can be used to first track the user’s position, orientation, and focus of attention to then find the most suitable output device to guide the user’s attention towards a target object. We argue that the most suitable device in this context is the one that attracts attention closest to the target and is salient enough to capture the user’s attention. The method is implemented as a function which estimates the ”closeness” and ”salience” of each visual and auditive output device in the environment. Some parameters of this method are then evaluated through a user study in the context of a virtual reality supermarket. The results show that multi-modal guidance can lead to better guiding performance. However, this depends on the set parameters.},
booktitle = {Proceedings of the 26th International Conference on Intelligent User Interfaces},
pages = {356–365},
numpages = {10},
keywords = {Attention, Attention Guidance, Cyber-Physical Environments, Intelligent Environments, Multi-modal, Multi-scale},
location = {College Station, TX, USA},
series = {IUI '21}
}

@article{10.1007/s00766-013-0194-3,
author = {Reddivari, Sandeep and Rad, Shirin and Bhowmik, Tanmay and Cain, Nisreen and Niu, Nan},
title = {Visual requirements analytics: a framework and case study},
year = {2014},
issue_date = {September 2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0194-3},
doi = {10.1007/s00766-013-0194-3},
abstract = {For many software projects, keeping requirements on track needs an effective and efficient path from data to decision. Visual analytics creates such a path that enables the human to extract insights by interacting with the relevant information. While various requirements visualization techniques exist, few have produced end-to-end value to practitioners. In this paper, we advance the literature on visual requirements analytics by characterizing its key components and relationships in a framework. We follow the goal---question---metric paradigm to define the framework by teasing out five conceptual goals (user, data, model, visualization, and knowledge), their specific operationalizations, and their interconnections. The framework allows us to not only assess existing approaches, but also create tool enhancements in a principled manner. We evaluate our enhanced tool support through a case study where massive, heterogeneous, and dynamic requirements are processed, visualized, and analyzed. Working together with practitioners on a contemporary software project within its real-life context leads to the main finding that visual analytics can help tackle both open-ended visual exploration tasks and well-structured visual exploitation tasks in requirements engineering. In addition, the study helps the practitioners to reach actionable decisions in a wide range of areas relating to their project, ranging from theme and outlier identification, over requirements tracing, to risk assessment. Overall, our work illuminates how the data-to-decision analytical capabilities could be improved by the increased interactivity of requirements visualization.},
journal = {Requir. Eng.},
month = sep,
pages = {257–279},
numpages = {23},
keywords = {Exploratory case study, Requirements engineering visualization, Requirements management, Risk assessment, Visual analytical reasoning, Visual analytics}
}

@article{10.1007/s11334-014-0236-0,
author = {Eito-Brun, Ricardo},
title = {Mapping of improvement models as a risk reduction strategy: a theoretical comparison for the aerospace industry},
year = {2014},
issue_date = {December  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-014-0236-0},
doi = {10.1007/s11334-014-0236-0},
abstract = {Organizations involved on process improvement programs need to deal with different process improvement and assessment models. As not all the process improvement and assessment models have an equivalent scope, the selection of a particular model to guide the improvement strategy may result in a partial, constrained view of the areas where the organization may obtain competitive advantages. As a mitigation strategy, organizations should have a detailed understanding of the differences in the scope of the available models. Whatever the model they adopt, companies should be aware of relevant areas that may be missed or treated with more or less detail in the models under consideration. In addition, the need of dealing with different assessment models is usually found in second- and third-party assessments, when prospects or potential contractors decide to conduct an assessment of the subcontractor's capabilities using a model that may not be the same as the reference model selected by the target subcontractor. In these situations, companies are at risks of overlooking relevant processes and practices. This paper describes a case study developed for the aerospace industry, based on the mapping of two assessment models widely deployed in this activity sector: CMMI-DEV and SPICE for Space, a variant of ISO/IEC 15504. A detailed gap analysis is provided identifying those aspects that should be considered both as potential improvement areas and as sources of risks. An extended assessment activity methodology is proposed that considers the results of model traceability analysis as a key factor for conducting the assessments.},
journal = {Innov. Syst. Softw. Eng.},
month = dec,
pages = {283–295},
numpages = {13},
keywords = {Assessment and improvement models, CMMI-DEV, Model comparison, Risk identification, S4S, SPICE for Space}
}

@article{10.1007/s00034-018-1002-6,
author = {Chen, Chengying and Chen, Liming},
title = {A 79-dB SNR 1.1-mW Fully Integrated Hearing Aid SoC},
year = {2019},
issue_date = {July      2019},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {38},
number = {7},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-018-1002-6},
doi = {10.1007/s00034-018-1002-6},
abstract = {For low-power hearing aid device application, a fully integrated optimized hearing aid SoC structure is proposed in this paper. The SoC consists of high-resolution, low-power analog front-end (AFE), time-division-multiplexed power-on-reset circuit, charge pump, digital signal processing (DSP) platform, and Class-D amplifier. A novel peak-statistical algorithm is proposed to track signal amplitude and adjust automatic gain control loop gain precisely. A comparative DWA is applied to break the correlation of in-band tone and sequential selection scheme, which realizes second-order noise shaping and suppresses harmonic effectively. The SoC has been implemented with 0.13 µm CMOS process. By measurement, it shows that the peak signal-to-noise ratio (SNR) of AFE is 82.6 dB and peak SNR of Class-D amplifier is 79.8 dB. Also, three main algorithms of wide dynamic range compression, noise reduction, and feedback cancelation are executed through DSP platform. With 1 V supply voltage, total SoC power is 1.1 mW and core area is 9.3 mm2. Based on our SoC, a hearing aid device prototype is produced that shows its great potential for mass manufacture in the future.},
journal = {Circuits Syst. Signal Process.},
month = jul,
pages = {2893–2909},
numpages = {17},
keywords = {Analog front-end, DSP, Hearing aid, Low power}
}

@article{10.1145/1022494.1022576,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Backmatter (Report abstracts, Paper abstracts, Calendar of Future Events, Call for Participation, Keynotes, Workshops, Tutorials)},
year = {2004},
issue_date = {September 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1022494.1022576},
doi = {10.1145/1022494.1022576},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {42},
numpages = {16}
}

@article{10.1155/2021/4513610,
author = {Chen, Ling-qing and Wu, Mei-ting and Pan, Li-fang and Zheng, Ru-bin and Liu, KunHong},
title = {Grade Prediction in Blended Learning Using Multisource Data},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/4513610},
doi = {10.1155/2021/4513610},
abstract = {Today, blended learning is widely carried out in many colleges. Different online learning platforms have accumulated a large number of fine granularity records of students’ learning behavior, which provides us with an excellent opportunity to analyze students’ learning behavior. In this paper, based on the behavior log data in four consecutive years of blended learning in a college’s programming course, we propose a novel multiclassification frame to predict students’ learning outcomes. First, the data obtained from diverse platforms, i.e., MOOC, Cnblogs, Programming Teaching Assistant (PTA) system, and Rain Classroom, are integrated and preprocessed. Second, a novel error-correcting output codes (ECOC) multiclassification framework, based on genetic algorithm (GA) and ternary bitwise calculator, is designed to effectively predict the grade levels of students by optimizing the code-matrix, feature subset, and binary classifiers of ECOC. Experimental results show that the proposed algorithm in this paper significantly outperforms other alternatives in predicting students’ grades. In addition, the performance of the algorithm can be further improved by adding the grades of prerequisite courses.},
journal = {Sci. Program.},
month = jan,
numpages = {15}
}

@article{10.1007/s10270-009-0142-3,
author = {Goknil, Arda and Kurtev, Ivan and Berg, Klaas and Veldhuis, Jan-Willem},
title = {Semantics of trace relations in requirements models for consistency checking and inferencing},
year = {2011},
issue_date = {February  2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-009-0142-3},
doi = {10.1007/s10270-009-0142-3},
abstract = {Requirements traceability is the ability to relate requirements back to stakeholders and forward to corresponding design artifacts, code, and test cases. Although considerable research has been devoted to relating requirements in both forward and backward directions, less attention has been paid to relating requirements with other requirements. Relations between requirements influence a number of activities during software development such as consistency checking and change management. In most approaches and tools, there is a lack of precise definition of requirements relations. In this respect, deficient results may be produced. In this paper, we aim at formal definitions of the relation types in order to enable reasoning about requirements relations. We give a requirements metamodel with commonly used relation types. The semantics of the relations is provided with a formalization in first-order logic. We use the formalization for consistency checking of relations and for inferring new relations. A tool has been built to support both reasoning activities. We illustrate our approach in an example which shows that the formal semantics of relation types enables new relations to be inferred and contradicting relations in requirements documents to be determined. The application of requirements reasoning based on formal semantics resolves many of the deficiencies observed in other approaches. Our tool supports better understanding of dependencies between requirements.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {31–54},
numpages = {24},
keywords = {Consistency checking, Inferencing, Reasoning, Requirements metamodel, Requirements traceability}
}

@article{10.1007/s10791-019-09361-0,
author = {Zhang, Haotian and Cormack, Gordon V. and Grossman, Maura R. and Smucker, Mark D.},
title = {Evaluating sentence-level relevance feedback for high-recall information retrieval},
year = {2020},
issue_date = {Feb 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1386-4564},
url = {https://doi.org/10.1007/s10791-019-09361-0},
doi = {10.1007/s10791-019-09361-0},
abstract = {This study uses a novel simulation framework to evaluate whether the time and effort necessary to achieve high recall using active learning is reduced by presenting the reviewer with isolated sentences, as opposed to full documents, for relevance feedback. Under the weak assumption that more time and effort is required to review an entire document than a single sentence, simulation results indicate that the use of isolated sentences for relevance feedback can yield comparable accuracy and higher efficiency, relative to the state-of-the-art baseline model implementation (BMI) of the AutoTAR continuous active learning (“CAL”) method employed in the TREC 2015 and 2016 Total Recall Track.},
journal = {Inf. Retr.},
month = feb,
pages = {1–26},
numpages = {26},
keywords = {Continuous active learning, CAL, Technology-assisted review, TAR, Total Recall, Relevance feedback}
}

@inproceedings{10.1007/978-3-030-58957-8_7,
author = {Hendriksen, Lone S. and Hansen, Henning Sten and Stenseng, Lars},
title = {Data Acquisition for Integrated Coastal Zone Management and Planning in a Climate Change Perspective},
year = {2020},
isbn = {978-3-030-58956-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58957-8_7},
doi = {10.1007/978-3-030-58957-8_7},
abstract = {The coastal zones are characterised with high populations densities and sensitive ecosystems which requires efficient planning and management. The ongoing climate change will make this task more challenging with sea level rise and increased storminess. Digital governance is an efficient instrument to cope with climate change in the coastal zone but requires up-to-date and high-quality information. The current research has aimed at providing an overview of the currently available data sources, gaps in data availability, and new approaches to data collection in a very dynamic coastal zone.},
booktitle = {Electronic Government and the Information Systems Perspective: 9th International Conference, EGOVIS 2020, Bratislava, Slovakia, September 14–17, 2020, Proceedings},
pages = {91–105},
numpages = {15},
keywords = {Coastal zone planning, Spatial data infrastructure, Climate change, UAV, Remote sensing, LiDAR, Mobile mapping},
location = {Bratislava, Slovakia}
}

@article{10.4018/jsse.2010102004,
author = {Nhlabatsi, Armstrong and Nuseibeh, Bashar and Yu, Yijun},
title = {Security Requirements Engineering for Evolving Software Systems: A Survey},
year = {2010},
issue_date = {January 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {1},
issn = {1947-3036},
url = {https://doi.org/10.4018/jsse.2010102004},
doi = {10.4018/jsse.2010102004},
abstract = {Long-lived software systems often undergo evolution over an extended period. Evolution of these systems is inevitable as they need to continue to satisfy changing business needs, new regulations and standards, and introduction of novel technologies. Such evolution may involve changes that add, remove, or modify features; or that migrate the system from one operating platform to another. These changes may result in requirements that were satisfied in a previous release of a system not being satisfied in subsequent versions. When evolutionary changes violate security requirements, a system may be left vulnerable to attacks. In this article we review current approaches to security requirements engineering and conclude that they lack explicit support for managing the effects of software evolution. We then suggest that a cross fertilization of the areas of software evolution and security engineering would address the problem of maintaining compliance to security requirements of software systems as they evolve.},
journal = {Int. J. Secur. Softw. Eng.},
month = jan,
pages = {54–73},
numpages = {20},
keywords = {Entailment Relation, Security Requirements Engineering, Software Evolution}
}

@article{10.1007/s10515-014-0144-4,
author = {Paydar, Samad and Kahani, Mohsen},
title = {A semantic web enabled approach to reuse functional requirements models in web engineering},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0144-4},
doi = {10.1007/s10515-014-0144-4},
abstract = {Web engineering has emerged as a new software discipline to specifically address the challenges and complexities of developing high quality web applications. A main theme in different web engineering methodologies is to employ model driven development approaches. This increases the level of abstraction and formalism to the extent that machines can better involve in the development process and provide more automation, e.g. automatic code generation from the models. Despite their benefits, a main problem of these model-driven methodologies is that developing each new web application implies creating a probably large number of models from scratch. Hence, model reuse can be considered as the main solution to this problem. In this paper, a semantic web enabled approach is proposed for reusing models, specifically functional requirements models. It takes the brief description of the functional requirements of a new web application in terms of UML use case diagram, and semi-automatically generates the draft of the corresponding detailed description in terms of a set of UML activity diagrams. This is performed by utilizing a repository which contains semantic representation of the models of the previous web applications. The proposed approach is based on novel algorithms for annotating activity diagrams, measuring similarity of use cases, and adapting activity diagrams. The experimental evaluations demonstrate that the proposed approach is promising, and it has good precision and effectiveness.},
journal = {Automated Software Engg.},
month = jun,
pages = {241–288},
numpages = {48},
keywords = {Adaptation, Annotation, Reuse, Semantic web, Use case similarity, Web engineering}
}

@inproceedings{10.1145/2915970.2915988,
author = {Santos, Ronnie E. S. and da Silva, Fabio Q. B. and de Magalh\~{a}es, Cleyton V. C.},
title = {Benefits and limitations of job rotation in software organizations: a systematic literature review},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2915988},
doi = {10.1145/2915970.2915988},
abstract = {Context. Job Rotation is an organizational practice whereby individuals are regularly moved among jobs or projects in the same organization. Goal: To identify and discuss evidence about job rotation, in order to understand the use, the benefits, and the limitations of this practice in software organizations. Method: A systematic literature review protocol was used to identify and select empirical studies previously published in the software engineering literature, and then coding techniques were used to analyse and synthesize their findings. Results: This review identified 18 empirical papers presenting evidence of 17 distinct studies about job rotation in software engineering. These studies revealed that in software organizations job rotation has been used to enhance communication, organizational understanding, knowledge exchange, and task variety. However, its use also requires extra effort and sometimes complex planning. Conclusion: The research about job rotation in software engineering is restricted, with only one study focusing on this topic and 16 presenting non-intentional evidence about the theme. Our review synthesized evidence that could inform research and practice. However, due to the specific nature of software development tasks and jobs, empirical evidence is still needed to guide the effective application of job rotation in practice.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {16},
numpages = {12},
keywords = {SLR, job rotation, software engineering},
location = {Limerick, Ireland},
series = {EASE '16}
}

@article{10.1007/s10845-014-0917-4,
author = {Alotaibi, Youseef},
title = {Business process modelling challenges and solutions: a literature review},
year = {2016},
issue_date = {August    2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {4},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-014-0917-4},
doi = {10.1007/s10845-014-0917-4},
abstract = {We have presented a review of the challenges facing business PM. These challenges are categorized into three challenges: (1) between business and IT, difficulty of deriving IT goals from business goals challenges; (2) security issues on business PM challenges; and (3) managing customer power, the rapidly changing business environment and business process (BP) challenges. Also, it presents the limitations of existing business PM frameworks. For example, in the first challenge, the existing literature is limited because they fail to capture the real business environment. Also, it is hard for IT analysts to understand BPs. In the second challenges, the existing methods of IS development fail to successfully integrate security during all development process stages and only deal with specific security requirements, goals and constraints. In the third challenges, no research has been conducted in the area of separating customers into different priority groups to provide services according to their required delivery time, payment history and feedback. Finally, we outline possible further research directions in the business PM domain. A systematic literature review method was used. Our review reports on academic publications on business PM challenges over the 13 years from 2000 to 2012. There are 31 journals as well as the IEEE and ACM databases being searched to identify relevant papers. Our systematic literature review results in that there are 53 journal papers as being the most relevant to our topic. In conclusion, it is not easy to create a good business PM. However, the research have to pay much attention on the area of creating successful business PM by creating secure business PM, manage customer power and create business PM where IT goals can be easily derived from business goals.},
journal = {J. Intell. Manuf.},
month = aug,
pages = {701–723},
numpages = {23},
keywords = {Alignment, Business PM challenges, Business Process Management (BPM), Business Process Modelling (PM), Business process (BP), Customer power, Literature review, Security}
}

@article{10.1016/j.jss.2019.02.019,
author = {Mian, Zhibao and Bottaci, Leonardo and Papadopoulos, Yiannis and Mahmud, Nidhal},
title = {Model transformation for analyzing dependability of AADL model by using HiP-HOPS},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.019},
doi = {10.1016/j.jss.2019.02.019},
journal = {J. Syst. Softw.},
month = may,
pages = {258–282},
numpages = {25},
keywords = {Dependability modeling, Dependability analysis, AADL, HiP-HOPS, Model transformation}
}

@inproceedings{10.5555/3320516.3320882,
author = {Gutenschwager, Kai and Theile, Marcel and Wilhelm, Bastian and Rabe, Markus},
title = {Comparison of approaches to encrypt data for supply chain simulation applications in cloud environments},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {A main characteristic in the field of supply chain simulation is that typically several independent organizations are involved. However, most simulation studies only consider a very limited number of different organizations. One main reason is that suppliers usually do not want to publish sensitive data, such as resource capacities or cost rates to their customer. In this paper we present an overall architecture to tackle this problem of multi-organizational simulation studies storing all relevant data at an independent third party, which also carries out all experiments and distributes the results as a cloud service. In order to define scenarios, different rights to use the provided data can be granted to other participants. The necessary user-specific encryption of datasets needs to be established on the underlying data base structure. In this paper we focus on three approaches for storing and encrypting data along with a detailed comparison in terms of performance.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {3084–3095},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@article{10.5555/3288647.3288713,
author = {Rabiser, Rick and Thanhofer-Pilisch, J\"{u}rgen and Vierhauser, Michael and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Developing and evolving a DSL-based approach for runtime monitoring of systems of systems},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {4},
issn = {0928-8910},
abstract = {Complex software-intensive systems are often described as systems of systems (SoS) due to their heterogeneous architectural elements. As SoS behavior is often only understandable during operation, runtime monitoring is needed to detect deviations from requirements. Today, while diverse monitoring approaches exist, most do not provide what is needed to monitor SoS, e.g., support for dynamically defining and deploying diverse checks across multiple systems. In this paper we report on our experiences of developing, applying, and evolving an approach for monitoring an SoS in the domain of industrial automation software, that is based on a domain-specific language (DSL). We first describe our initial approach to dynamically define and check constraints in SoS at runtime and then motivate and describe its evolution based on requirements elicited in an industry collaboration project. We furthermore describe solutions we have developed to support the evolution of our approach, i.e., a code generation approach and a framework to automate testing the DSL after changes. We evaluate the expressiveness and scalability of our new DSL-based approach using an industrial SoS. We also discuss lessons we learned. Our results show that while developing a DSL-based approach is a good solution to support industrial users, one must prepare the approach for evolution, by making it extensible and adaptable to future scenarios. Particularly, support for automated (re-)generation of tools and code after changes and automated testing are essential.},
journal = {Automated Software Engg.},
month = dec,
pages = {875–915},
numpages = {41},
keywords = {Constraint checking, DSL evolution, Domain-specific languages, Requirements monitoring, Systems of systems}
}

@article{10.1016/j.jss.2012.04.075,
author = {Pacheco, Carla and Garcia, Ivan},
title = {A systematic literature review of stakeholder identification methods in requirements elicitation},
year = {2012},
issue_date = {September, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {9},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.04.075},
doi = {10.1016/j.jss.2012.04.075},
abstract = {This paper presents a systematic review of relevant published studies related to topics in Requirements Engineering, specifically, concerning stakeholder identification methods in requirements elicitation, dated from 1984 to 2011. Addressing four specific research questions, this systematic literature review shows the following evidence gathered from these studies: current status of stakeholder identification in software requirement elicitation, the best practices recommended for its performance, consequences of incorrect identification in requirements quality, and, aspects which need to be improved. Our findings suggest that the analyzed approaches still have serious limitations in terms of covering all aspects of stakeholder identification as an important part of requirements elicitation. However, through correctly identifying and understanding the stakeholders, it is possible to develop high quality software.},
journal = {J. Syst. Softw.},
month = sep,
pages = {2171–2181},
numpages = {11},
keywords = {Requirements elicitation, Requirements engineering, Software engineering, Stakeholder identification, Systematic review}
}

@inproceedings{10.1145/2733373.2806345,
author = {Zhu, Lei and Shen, Jialie and Xie, Liang},
title = {Topic Hypergraph Hashing for Mobile Image Retrieval},
year = {2015},
isbn = {9781450334594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2733373.2806345},
doi = {10.1145/2733373.2806345},
abstract = {Hashing is one of the promising solutions to support efficient Mobile Image Retrieval (MIR). However, most of existing hashing strategies simply rely on low-level features, which inevitably makes the generated hashing codes less semantic. Moreover, many of them fail to exploit complex and high-order semantic correlations of images. Motivated by these observations, we propose a novel unsupervised hashing scheme, emph{Topic Hypergraph Hashing} (THH), to address the limitations. A unified topic hypergraph, where images and topics are represented with independent vertices and hyperedges respectively, is first constructed to model latent semantics of images and their correlations. With topic hypergraph model, hashing codes and functions are then learned by simultaneously preserving similarity consistence and semantic correlation. Experiments on standard datasets demonstrate that THH can achieve superior performance compared with several state-of-the-art techniques, and it is more suitable for MIR.},
booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
pages = {843–846},
numpages = {4},
keywords = {high-order semantic correlations, mobile image retrieval, topic hypergraph hashing},
location = {Brisbane, Australia},
series = {MM '15}
}

@inproceedings{10.5555/2675983.2676315,
author = {Gutenschwager, Kai and Rabe, Markus and Sari, Mehmet Umut and Fechteler, Till},
title = {A data model for carbon footprint simulation in consumer goods supply chains},
year = {2013},
isbn = {9781479920778},
publisher = {IEEE Press},
abstract = {CO2 efficiency is currently a popular topic in supply chain management. Most approaches are based on the Life Cycle Assessment (LCA) which usually exploits data from a static database. This approach is effective when estimating the carbon footprint of products or groups of products in general. Simulation has been a proper method for metering the effectiveness of logistics systems, and could thus be expected to also support the analysis of CO2 efficiency in supply chains (SC) when combined with an LCA database. However, research shows that this combination does not deliver reliable results when the target of the study is improvement of the logistics in the SC. The paper demonstrates the shortcomings of the LCA-analogous approach and proposes a data model that enables discrete event simulation of SC logistics including its impact on the carbon footprint that is under development in the e-SAVE joint project funded by the European Commission.},
booktitle = {Proceedings of the 2013 Winter Simulation Conference: Simulation: Making Decisions in a Complex World},
pages = {2677–2688},
numpages = {12},
location = {Washington, D.C.},
series = {WSC '13}
}

@article{10.1145/1095430.1095431,
title = {Frontmatter (TOC, Letters, Philosophy of computer science, Interviewers needed, Taking software requirements creation from folklore to analysis, SW components and product lines: from business to systems and technology, Software engineering survey)},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1095430.1095431},
doi = {10.1145/1095430.1095431},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {0},
numpages = {45}
}

@inproceedings{10.1145/3460319.3464823,
author = {Mordahl, Austin and Wei, Shiyi},
title = {The impact of tool configuration spaces on the evaluation of configurable taint analysis for Android},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464823},
doi = {10.1145/3460319.3464823},
abstract = {The most popular static taint analysis tools for Android allow users to change the underlying analysis algorithms through configuration options. However, the large configuration spaces make it difficult for developers and users alike to understand the full capabilities of these tools, and studies to-date have only focused on individual configurations. In this work, we present the first study that evaluates the configurations in Android taint analysis tools, focusing on the two most popular tools, FlowDroid and DroidSafe. First, we perform a manual code investigation to better understand how configurations are implemented in both tools. We formalize the expected effects of configuration option settings in terms of precision and soundness partial orders which we use to systematically test the configuration space. Second, we create a new dataset of 756 manually classified flows across 18 open-source real-world apps and conduct large-scale experiments on this dataset and micro-benchmarks. We observe that configurations make significant tradeoffs on the performance, precision, and soundness of both tools. The studies to-date would reach different conclusions on the tools' capabilities were they to consider configurations or use real-world datasets. In addition, we study the individual options through a statistical analysis and make actionable recommendations for users to tune the tools to their own ends. Finally, we use the partial orders to test the tool configuration spaces and detect 21 instances where options behaved in unexpected and incorrect ways, demonstrating the need for rigorous testing of configuration spaces.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {466–477},
numpages = {12},
keywords = {Android taint analysis, configurable static analysis, empirical study},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3308560.3317591,
author = {D. Fern\'{a}ndez, Javier and J. Ekaputra, Fajar and Ruswono Aryan, Peb and Azzam, Amr and Kiesling, Elmar},
title = {Privacy-aware Linked Widgets},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317591},
doi = {10.1145/3308560.3317591},
abstract = {The European General Data Protection Regulation (GDPR) brings new challenges for companies, who must demonstrate that their systems and business processes comply with usage constraints specified by data subjects. However, due to the lack of standards, tools, and best practices, many organizations struggle to adapt their infrastructure and processes to ensure and demonstrate that all data processing is in compliance with users’ given consent. The SPECIAL EU H2020 project has developed vocabularies that can formally describe data subjects’ given consent as well as methods that use this description to automatically determine whether processing of the data according to a given policy is compliant with the given consent. Whereas this makes it possible to determine whether processing was compliant or not, integration of the approach into existing line of business applications and ex-ante compliance checking remains an open challenge. In this short paper, we demonstrate how the SPECIAL consent and compliance framework can be integrated into Linked Widgets, a mashup platform, in order to support privacy-aware ad-hoc integration of personal data. The resulting environment makes it possible to create data integration and processing workflows out of components that inherently respect usage policies of the data that is being processed and are able to demonstrate compliance. We provide an overview of the necessary meta data and orchestration towards a privacy-aware linked data mashup platform that automatically respects subjects’ given consents. The evaluation results show the potential of our approach for ex-ante usage policy compliance checking within the Linked Widgets Platforms and beyond.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {508–514},
numpages = {7},
keywords = {Compliance, GDPR, Linked Data, Privacy},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/1509239.1509260,
author = {Chitchyan, Ruzanna and Greenwood, Phil and Sampaio, Americo and Rashid, Awais and Garcia, Alessandro and Fernandes da Silva, Lyrene},
title = {Semantic vs. syntactic compositions in aspect-oriented requirements engineering: an empirical study},
year = {2009},
isbn = {9781605584423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1509239.1509260},
doi = {10.1145/1509239.1509260},
abstract = {Most current aspect composition mechanisms rely on syntactic references to the base modules or wildcard mechanisms quantifying over such syntactic references in pointcut expressions. This leads to the well-known problem of pointcut fragility. Semantics-based composition mechanisms aim to alleviate such fragility by focusing on the meaning and intention of the composition hence avoiding strong syntactic dependencies on the base modules. However, to date, there are no empirical studies validating whether semantics based composition mechanisms are indeed more expressive and less fragile compared to their syntax-based counterparts. In this paper we present a first study comparing semantics- and syntax-based composition mechanisms in aspect-oriented requirements engineering (AORE). In our empirical study the semantics-based compositions examined were found to be indeed more expressive and less fragile. The semantics-based compositions in the study also required one to reason about composition interdependencies early on hence potentially reducing the overhead of revisions arising from later trade-off analysis and stakeholder negotiations. However, this added to the overhead of specifying the compositions themselves. Furthermore, since the semantics-based compositions considered in the study were based on natural language analysis, they required initial effort investment into lexicon building as well as strongly depended on advanced tool support to expose the natural language semantics.},
booktitle = {Proceedings of the 8th ACM International Conference on Aspect-Oriented Software Development},
pages = {149–160},
numpages = {12},
keywords = {aspect-oriented composition specification, aspect-oriented requirements engineering, evaluation, requirements metrics},
location = {Charlottesville, Virginia, USA},
series = {AOSD '09}
}

@inproceedings{10.1145/2577080.2579816,
author = {Kamina, Tetsuo and Aotani, Tomoyuki and Masuhara, Hidehiko and Tamai, Tetsuo},
title = {Context-oriented software engineering: a modularity vision},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2579816},
doi = {10.1145/2577080.2579816},
abstract = {There are a number of constructs to implement context-dependent behavior, such as conditional branches using if statements, method dispatching in object-oriented programming (such as the state design pattern), dynamic deployment of aspects in aspect-oriented programming, and layers in context-oriented programming (COP). Uses of those constructs significantly affect the modularity of the obtained implementation. While there are a number of cases where COP improves modularity, it is not clear when we should use COP in general.This paper presents a preliminary study on our software development methodology, the context-oriented software engineering (COSE), which is a use-case-driven software development methodology that guides us to a specification of context-dependent requirements and design. We provide a way to map the requirements and design formed by COSE to the implementation in our COP language ServalCJ. We applied COSE to two applications in order to assess its feasibility. We also identify key linguistic constructs that make COSE effective by examining existing COP languages. These feasibility studies and examination raise a number of interesting open issues. We finally show our future research roadmap to address those issues.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {85–98},
numpages = {14},
keywords = {context-oriented programming, methodology, use cases},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@inbook{10.1145/3191315.3191324,
author = {Ramakrishnan, C. R.},
title = {State-space search with tabled logic programs},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191324},
abstract = {A number of problems involving state space search can be naturally formulated as query evaluation over tabled logic programs. This chapter uses model checking and planning problems to illustrate the effectiveness of logic programming to model and solve complex search problems over large state spaces. Logic programming techniques have been successfully employed to tackle several aspects of the model checking problem, ranging from the construction and abstraction of potentially infinite state spaces, to efficient search over the constructed state space to check a variety of properties, from simple reachability properties using transitive closure, to complex properties requiring computation of nested fixed points. In constrast to model checking, the formulation of planning problems typically involve queries with aggregates (e.g., finding minimum-cost plan instead of plan existence). We show that tabled evaluation with aggregation gives an effective means for computing minimum cost plans.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {427–472},
numpages = {46}
}

@article{10.5555/2795612.2795613,
author = {Moissa, Barbara and Gasparini, Isabela and Kemczinski, Avanilde},
title = {A Systematic Mapping on the Learning Analytics Field and Its Analysis in the Massive Open Online Courses Context},
year = {2015},
issue_date = {July 2015},
publisher = {IGI Global},
address = {USA},
volume = {13},
number = {3},
issn = {1539-3100},
abstract = {Learning Analytics LA is a field that aims to optimize learning through the study of dynamical processes occurring in the students' context. It covers the measurement, collection, analysis and reporting of data about students and their contexts. This study aims at surveying existing research on LA to identify approaches, topics, and needs for future research. A systematic mapping study is launched to find as much literature as possible. The 127 papers found resulting in 116 works are classified with respect to goals, data types, techniques, stakeholders and interventions. Despite the increasing interest in field, there are no studies relating it to the Massive Open Online Courses MOOCs context. The goal of this paper is twofold, first we present the systematic mapping on LA and after we analyze its findings in the MOOCs context. As results we provide an overview of LA and identify perspectives and challenges in the MOOCs context.},
journal = {Int. J. Distance Educ. Technol.},
month = jul,
pages = {1–24},
numpages = {24}
}

@inproceedings{10.1007/11526988_9,
author = {A\ss{}mann, Uwe},
title = {Reuse in semantic applications},
year = {2005},
isbn = {3540278281},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11526988_9},
doi = {10.1007/11526988_9},
abstract = {Applications using semantic technology are not fundamentally different from other software products. As standard applications, they need a well-defined development process, an appropriate modelling technology, and, to decrease construction cost, a good reuse technology for models and components. This paper shows that employing ontologies can help to enlarge the reuse factor. Ontologies improve the refinement process in object-oriented software development, simplify design of product lines, improve interoperability in component-based systems, and help in service-based applications, such as web services. Hence, ontologies will play an important role in the future engineering of software products.},
booktitle = {Proceedings of the First International Conference on Reasoning Web},
pages = {290–304},
numpages = {15},
location = {Msida, Malta}
}

@article{10.4018/IJIRR.2014070104,
author = {Belalem, Ghalem and Abbache, Ahmed and Barigou, Fatiha and Belkredim, Fatma Zohra},
title = {The Use of Arabic WordNet in Arabic Information Retrieval},
year = {2014},
issue_date = {July 2014},
publisher = {IGI Global},
address = {USA},
volume = {4},
number = {3},
issn = {2155-6377},
url = {https://doi.org/10.4018/IJIRR.2014070104},
doi = {10.4018/IJIRR.2014070104},
abstract = {Research and experimentation using Arabic WordNet in the field of information retrieval are relatively new. It is limited compared to the research that has been done using Princeton WordNet. This work attempts to study the impact of Arabic WordNet on the performance of Arabic information retrieval. We extend Lucene with Arabic WordNet to expand user's queries. The major contribution of this study is to propose an interactive query expansion IQE methodology using the word's part-of-speech, according to the part it plays in a query. First, the user selects the appropriate part of speech for each term in the original query, and then he reselects the appropriate synonyms. Experimental results show that our IQE strategy produces a good Mean Average Precision MAP, it is able to improve MAP by 12.6%, but no variant of automatic query expansion AQE strategies did. Nevertheless, the experiments allow us to conclude that with an appropriate use of Arabic WordNet as a source of linguistic information for AQE can improve effectiveness for Arabic information retrieval.},
journal = {Int. J. Inf. Retr. Res.},
month = jul,
pages = {54–65},
numpages = {12}
}

@inproceedings{10.1145/3360774.3360801,
author = {Lee, Yongwoo and Li, Jingjie and Kim, Younghyun},
title = {MicPrint: acoustic sensor fingerprinting for spoof-resistant mobile device authentication},
year = {2020},
isbn = {9781450372831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360774.3360801},
doi = {10.1145/3360774.3360801},
abstract = {Smartphones are the most commonly used computing platform for accessing sensitive and important information placed on the Internet. Authenticating the smartphone's identity in addition to the user's identity is a widely adopted security augmentation method since conventional user authentication methods, such as password entry, often fail to provide strong protection by itself.In this paper, we propose a sensor-based device fingerprinting technique for identifying and authenticating individual mobile devices. Our technique, called MicPrint, exploits the unique characteristics of embedded microphones in mobile devices due to manufacturing variations in order to uniquely identify each device. Unlike conventional sensor-based device fingerprinting that are prone to spoofing attack via malware, MicPrint is fundamentally spoof-resistant since it uses acoustic features that are prominent only when the user blocks the microphone hole. This simple user intervention acts as implicit permission to fingerprint the sensor and can effectively prevent unauthorized fingerprinting using malware. We implement MicPrint on Google Pixel 1 and Samsung Nexus to evaluate the accuracy of device identification. We also evaluate its security against simple raw data attacks and sophisticated impersonation attacks. The results show that after several incremental training cycles under various environmental noises, MicPrint can achieve high accuracy and reliability for both smartphone models.},
booktitle = {Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {248–257},
numpages = {10},
keywords = {device fingerprinting, microphone, multi-factor authentication, sensor},
location = {Houston, Texas, USA},
series = {MobiQuitous '19}
}

@article{10.1007/s10664-006-6405-5,
author = {Natt Och Dag, Johan and Thelin, Thomas and Regnell, Bj\"{o}rn},
title = {An experiment on linguistic tool support for consolidation of requirements from multiple sources in market-driven product development},
year = {2006},
issue_date = {June      2006},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {11},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-006-6405-5},
doi = {10.1007/s10664-006-6405-5},
abstract = {This paper presents an experiment with a linguistic support tool for consolidation of requirements sets. The experiment is designed based on the requirements management process at a large market-driven software development company that develops generic solutions to satisfy many different customers. New requirements and requests for information are continuously issued, which must be analyzed and responded to. The new requirements should first be consolidated with the old to avoid reanalysis of previously elicited requirements and to complement existing requirements with new information. In the presented experiment, a new open-source tool is evaluated in a laboratory setting. The tool uses linguistic engineering techniques to calculate similarities between requirements and presents a ranked list of suggested similar requirements, between which links may be assigned. It is hypothesized that the proposed technique for finding and linking similar requirements makes the consolidation more efficient. The results show that subjects that are given the support provided by the tool are significantly more efficient and more correct in consolidating two requirements sets, than are subjects that do not get the support. The results suggest that the proposed techniques may give valuable support and save time in an industrial requirements consolidation process.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {303–329},
numpages = {27},
keywords = {Linguistic engineering, Natural language requirements, Requirements management, Software product development}
}

@article{10.5555/1412312.1412330,
author = {Dascalu, Sergiu and Buntha, Sermsak and Saru, Daniela and Debnath, Narayan},
title = {Software tool for naval surface warfare simulation and training},
year = {2006},
issue_date = {April 2006},
publisher = {IOS Press},
address = {NLD},
volume = {6},
number = {5,6 Supplement 2},
issn = {1472-7978},
abstract = {Determining optimal operations involves performing complex tasks by navy commanders. In this paper we present a software tool with four main functions that facilitate commanders' tasks in naval surface warfare. First, a human-computer interaction function presents battlefield information using information-rich graphical symbols. Second, a command and control function allows commanders to achieve centralized control of naval warfare operation in battlefield situations. Third, an interception function is available to determine the optimum course for intercepting a particular enemy target ship. Fourth, an escape route planner can be invoked to automatically calculate routes that minimize detection by enemy platforms. The paper presents background information on naval surface warfare, excerpts of the proposed software tool's model, and details of the tool's user interface. A brief survey of related projects as well as several pointers to future work are also included.},
journal = {J. Comp. Methods in Sci. and Eng.},
month = apr,
pages = {427–444},
numpages = {18},
keywords = {Naval surface warfare, UML, command and control, simulation, training}
}

@article{10.1023/A:1013303202532,
author = {Port, Dan and Dincel, Ebru},
title = {Experiences Using Domain Specific Techniques within Multimedia Software Engineering},
year = {2001},
issue_date = {December 2001},
publisher = {J. C. Baltzer AG, Science Publishers},
address = {USA},
volume = {12},
number = {1},
issn = {1022-7091},
url = {https://doi.org/10.1023/A:1013303202532},
doi = {10.1023/A:1013303202532},
abstract = {Domain specific techniques take advantage of the commonalities among applications developed within a certain domain. They are known to improve quality and productivity by incorporating domain knowledge and previous project experiences and promote reuse. This paper describes six domain specific software engineering techniques for developing multimedia applications within the digital library domain. We provide examples of each technique from several projects in which they were used, how the techniques are used within general software engineering practice (in particular, MBASE), how the techniques address some of the particular challenges multimedia software engineering, and the positive impacts we have measured resulting from their use within a graduate level software engineering course.},
journal = {Ann. Softw. Eng.},
month = dec,
pages = {11–45},
numpages = {35}
}

@inproceedings{10.1007/978-3-319-99819-0_9,
author = {Fatemi Moghaddam, Faraz and \c{C}emberci, S\"{u}leyman Berk and Wieder, Philipp and Yahyapour, Ramin},
title = {A Multi-level Policy Engine to Manage Identities and Control Accesses in Cloud Computing Environment},
year = {2018},
isbn = {978-3-319-99818-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-99819-0_9},
doi = {10.1007/978-3-319-99819-0_9},
abstract = {Security challenges are the most important obstacles for the advancement of IT-based on-demand services and cloud computing as an emerging technology. Lack of coincidence in identity management models based on defined policies and various security levels in different cloud servers is one of the most challenging issues in clouds. In this paper, a policy-based user authentication model has been presented to provide a reliable and scalable identity management and to map cloud users’ access requests with defined polices of cloud servers. In the proposed schema several components are provided to define access policies by cloud servers, to apply policies based on a structural and reliable ontology, to manage user identities and to semantically map access requests by cloud users with defined polices.},
booktitle = {Service-Oriented and Cloud Computing: 7th IFIP WG 2.14 European Conference, ESOCC 2018, Como, Italy, September 12-14, 2018, Proceedings},
pages = {120–129},
numpages = {10},
keywords = {Cloud computing, Security, Policy management, Identity management, Access control},
location = {Como, Italy}
}

@article{10.1007/s00291-012-0290-7,
author = {Lang, Jan Christian and Widjaja, Thomas},
title = {OREX-J: towards a universal software framework for the experimental analysis of optimization algorithms},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {3},
issn = {0171-6468},
url = {https://doi.org/10.1007/s00291-012-0290-7},
doi = {10.1007/s00291-012-0290-7},
abstract = {The Operations Research EXperiment Framework for Java (OREX-J) is an object-oriented software framework that helps users to design, implement and conduct computational experiments for the analysis of optimization algorithms. As it was designed in a generic way using object-oriented programming and design patterns, it is not limited to a specific class of optimization problems and algorithms. The purpose of the framework is to reduce the amount of manual labor required for conducting and evaluating computational experiments: OREX-J provides a generic, extensible data model for storing detailed data on an experimental design and its results. Those data can include algorithm parameters, test instance generator settings, the instances themselves, run-times, algorithm logs, solution properties, etc. All data are automatically saved in a relational database (MySQL, http://www.mysql.com/ ) by means of the object-relational mapping library Hibernate ( http://www.hibernate.org/ ). This simplifies the task of analyzing computational results, as even complex analyses can be performed using comparatively simple Structured Query Language (SQL) queries. Also, OREX-J simplifies the comparison of algorithms developed by different researchers: Instead of integrating other researchers' algorithms into proprietary test beds, researchers could use OREX-J as a common experiment framework. This paper describes the architecture and features of OREX-J and exemplifies its usage in a case study. OREX-J has already been used for experiments in three different areas: Algorithms and reformulations for mixed-integer programming models for dynamic lot-sizing with substitutions, a simulation-based optimization approach for a stochastic multi-location inventory control model, and an optimization model for software supplier selection and product portfolio planning.},
journal = {OR Spectr.},
month = jul,
pages = {735–769},
numpages = {35},
keywords = {Design patterns, Experimental analysis of algorithms, Object-oriented design, Operations research methodology, Software framework, Test bed}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@article{10.1145/638750.638788,
author = {You-Sheng, Zhang and Yu-Yun, He},
title = {Architecture-based SW process model},
year = {2003},
issue_date = {March 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/638750.638788},
doi = {10.1145/638750.638788},
abstract = {Explicit software architecture is principal in the development of a software project. In this paper Architecture-Based Software Process model (ABSP) is presented. ABSP model divides software process based on architecture into six sub processes: requirements, design, documentation, review, implementation, and evolution. Compared with traditional software process model, ABSP model has many advantages such as explicit structure, easy understandability, better portability and large reusable granularity.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {16},
numpages = {5},
keywords = {evolution, reuse, software architecture, software process}
}

@article{10.1016/j.jss.2016.05.024,
author = {S\'{a}nchez Guinea, Alejandro and Nain, Gr\'{e}gory and Le Traon, Yves},
title = {A systematic review on the engineering of software for ubiquitous systems},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {118},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.05.024},
doi = {10.1016/j.jss.2016.05.024},
abstract = {A systematic literature review on engineering software for ubiquitous systems.We identified 132 approaches addressing issues on different phases of the software engineering cycle for ubiquitous systems.Implementation, evolution/maintenance, and feedback phases have been the most studied.The testing phase needs to receive more attention, especially in what respect to simulations. Context: Software engineering for ubiquitous systems has experienced an important and rapid growth, however the vast research corpus makes it difficult to obtain valuable information from it.Objective: To identify, evaluate, and synthesize research about the most relevant approaches addressing the different phases of the software development life cycle for ubiquitous systems.Method: We conducted a systematic literature review of papers presenting and evaluating approaches for the different phases of the software development life cycle for ubiquitous systems. Approaches were classified according to the phase of the development cycle they addressed, identifying their main concerns and limitations.Results: We identified 128 papers reporting 132 approaches addressing issues related to different phases of the software development cycle for ubiquitous systems. Most approaches have been aimed at addressing the implementation, evolution/maintenance, and feedback phases, while others phases such as testing need more attention from researchers.Conclusion: We recommend to follow existing guidelines when conducting case studies to make the studies more reproducible and closer to real life cases. While some phases of the development cycle have been extensively explored, there is still room for research in other phases, toward a more agile and integrated cycle, from requirements to testing and feedback.},
journal = {J. Syst. Softw.},
month = aug,
pages = {251–276},
numpages = {26},
keywords = {Development methods, Empirical-software engineering, Evidence-based software engineering, Pervasive systems, Research synthesis, Software development cycle, Systematic review, Ubiquitous systems}
}

@article{10.1016/j.specom.2015.04.001,
title = {Subjective and objective measurement of synthesized speech intelligibility in modern telephone conditions},
year = {2015},
issue_date = {July 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {71},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2015.04.001},
doi = {10.1016/j.specom.2015.04.001},
abstract = {All the investigated telecom degradations seriously impact the subjective intelligibility of synthesized speech.PESQ Intelligibility is not capable to provide valid intelligibility predictions.POLQA Intelligibility is capable of providing good intelligibility predictions. This paper investigates the impact of different telephone channels, represented by impairments as introduced by modern telecommunication networks (e.g. speech coding, bandwidth limitation, packet loss, etc.), on the intelligibility of synthesized speech. Both subjective and objective assessments are used. Two different speech intelligibility prediction models, namely PESQ Intelligibility and POLQA Intelligibility, are evaluated by comparing the predictions with subjectively obtained intelligibility scores. The results show that all the investigated degradations seriously impact the intelligibility of the synthesized speech measured subjectively. Furthermore it is shown that PESQ Intelligibility provides too low correlations between predicted objective measurements and subjective scores for accurate prediction of speech intelligibility while POLQA Intelligibility is capable of providing good intelligibility predictions in the case that a closed response experimental set up is used.},
journal = {Speech Commun.},
month = jul,
pages = {1–9},
numpages = {9}
}

@inproceedings{10.1145/2818346.2820753,
author = {Wu, Yuhao and Jia, Jia and Leung, WaiKim and Liu, Yejun and Cai, Lianhong},
title = {MPHA: A Personal Hearing Doctor Based on Mobile Devices},
year = {2015},
isbn = {9781450339124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818346.2820753},
doi = {10.1145/2818346.2820753},
abstract = {As more and more people inquire to know their hearing level condition, audiometry is becoming increasingly important. However, traditional audiometric method requires the involvement of audiometers, which are very expensive and time consuming. In this paper, we present mobile personal hearing assessment (MPHA), a novel interactive mode for testing hearing level based on mobile devices. MPHA, 1) provides a general method to calibrate sound intensity for mobile devices to guarantee the reliability and validity of the audiometry system; 2) designs an audiometric correction algorithm for the real noisy audiometric environment. The experimental results show that MPHA is reliable and valid compared with conventional audiometric assessment.},
booktitle = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
pages = {155–162},
numpages = {8},
keywords = {assessment, audiometry, health care, hearing level, mobile device, multimodal interaction},
location = {Seattle, Washington, USA},
series = {ICMI '15}
}

@inproceedings{10.1007/978-3-642-34059-8_1,
author = {Broy, Manfred and Cengarle, Mar\'{\i}a Victoria and Geisberger, Eva},
title = {Cyber-physical systems: imminent challenges},
year = {2012},
isbn = {9783642340581},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34059-8_1},
doi = {10.1007/978-3-642-34059-8_1},
abstract = {A German project is presented which was initiated in order to analyse the potential and risks associated with Cyber-Physical Systems. These have been recognised as the next wave of innovation in information and communication technology. Cyber-Physical Systems are herein understood in a very broad sense as the integration of embedded systems with global networks such as the Internet. The survey aims at deepening understanding the impact of those systems at technological and economical level as well as at political and sociological level. The goal of the study is to collect arguments for decision makers both in business and politics to take actions in research, legislation and business development.},
booktitle = {Proceedings of the 17th Monterey Conference on Large-Scale Complex IT Systems: Development, Operation and Management},
pages = {1–28},
numpages = {28},
location = {Oxford, UK}
}

@article{10.1007/s00500-021-05766-6,
author = {Agudelo, Oscar Esneider Acosta and Mar\'{\i}n, Carlos Enrique Montenegro and Crespo, Rub\'{e}n Gonz\'{a}lez},
title = {Sound measurement and automatic vehicle classification and counting applied to road traffic noise characterization},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {18},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05766-6},
doi = {10.1007/s00500-021-05766-6},
abstract = {Increase in population density in large cities has increased the environmental noise present in these environments, causing negative effects on human health. There are different sources of environmental noise; however, noise from road traffic is the most prevalent in cities. Therefore, it is necessary to have tools that allow noise characterization to establish strategies that permit obtaining levels that do not affect the quality of life of people. This research discusses the implementation of a system that allows the acquisition of data to characterize the noise generated by road traffic. First, the methodology for obtaining acoustic indicators with an electret measurement microphone is described, so that it adjusts to the data collection needs for road traffic noise analyses. Then, an approach for the classification and counting of automatic vehicular traffic through deep learning is presented. Results showed that there were differences of 0.2 dBA in terms of RMSE between a type 1 sound level meter and the measurement microphone used. With reference to vehicle classification and counting for four categories, the approximate error is between 3.3% and -15.5%.},
journal = {Soft Comput.},
month = sep,
pages = {12075–12087},
numpages = {13},
keywords = {Environmental noise, Road traffic, Vehicle, Classification, Deep learning}
}

@inproceedings{10.1109/ESEM.2009.5315982,
author = {Liu, Dapeng and Wang, Qing and Xiao, Junchao},
title = {The role of software process simulation modeling in software risk management: A systematic review},
year = {2009},
isbn = {9781424448425},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ESEM.2009.5315982},
doi = {10.1109/ESEM.2009.5315982},
abstract = {Nowadays software projects are still suffering from many problems due to various kinds of software risks. Software risk management is a crucial part of successful project management, but it is often not well implemented in real-world software projects. One reason is that project managers lack effective and practical tools to manage software risks. Software Process Simulation Modeling (SPSM) has been emerging as a promising approach to address a variety of issues in software engineering area, including risk management. However, the current state of how SPSM supports software risk management is not yet clear. This paper presents a systematic literature review which purpose is to obtain the state of the art of the applications of SPSM in software risk management. We drew the following conclusions from the review results: (1) The number of SPSM studies on software risk management is relatively small, but increasing gradually in recent years. (2) SPSM is mainly applied in risk analysis and risk management planning activities. (3) Software risks related to requirements, development process and management process are the ones most studied by SPSM. (4) Discrete-Event Simulation and System Dynamics are two most popular simulation paradigms, while Hybrid simulation methods are more and more widely used. (5) Extend, iThink and Vensim are the most popular simulation tools in SPSM. (6) Most of SPSM approaches and models have not been well applied into real-world risk management practices.},
booktitle = {Proceedings of the 2009 3rd International Symposium on Empirical Software Engineering and Measurement},
pages = {302–311},
numpages = {10},
series = {ESEM '09}
}

@inproceedings{10.1109/ICSE-SEIP.2019.00045,
author = {Tsuda, Naohiko and Washizaki, Hironori and Honda, Kiyoshi and Nakai, Hidenori and Fukazawa, Yoshiaki and Azuma, Motoei and Komiyama, Toshihiro and Nakano, Tadashi and Suzuki, Hirotsugu and Morita, Sumie and Kojima, Katsue and Hando, Akiyoshi},
title = {WSQF: comprehensive software quality evaluation framework and benchmark based on SQuaRE},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2019.00045},
doi = {10.1109/ICSE-SEIP.2019.00045},
abstract = {Conventional quality evaluations of software concentrate on specific quality characteristics. Moreover, the measurement data are limited to specific products and organizations. Consequently, the present state of product quality and quality-in-use characteristics are not fully understood, preventing effective decision-making for software stakeholders. To alleviate this problem, ISO/IEC defined international standards called the SQuaRE (Systems and software Quality Requirements an Evaluation) series for comprehensive quality measurement and evaluation. However, these standards remain rather general and abstract, making them difficult to apply. In this paper, we establish a SQuaRE-based comprehensive software quality evaluation framework, Waseda Software Quality Framework (WSQF), which concretizes many product quality and quality-in-use measurement methods originally defined in the SQuaRE series. By applying the WSQF to 21 commercial ready-to-use software products, we revealed the status of software product quality. A resulted comprehensive benchmark includes trends of the quality measurement values, relationships among quality characteristics, relationship between quality-in-use and product quality, and relationship between the quality characteristics and product contexts within the limits of an application.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice},
pages = {312–321},
numpages = {10},
keywords = {quality assurance, software quality management, square series},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIP '19}
}

@article{10.1016/j.csi.2015.09.005,
title = {Knowledge acquisition in information technology and software engineering towards excellence of information systems based on the standardisation platform},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {44},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2015.09.005},
doi = {10.1016/j.csi.2015.09.005},
abstract = {This paper presents a study of the collective knowledge in information technology (IT) and the comparative analyses of innovative trends in the standardisation of the roads of knowledge in the subfields of software engineering (SE).The focus is on the amount of required innovation that will be necessary in the examples database of standardised units in IT and SE for the improvement of the information systems (IS). The goal is to determine how to obtain appropriate knowledge in IT and SE to model the excellence of IS.The contribution to the modelling of IS excellence in PDCA (Plan-Do-Check-Act) is presented. Originality of trend lines resources planning (Pi) for IT/ICS-1=35 field: y35/ISO/2014, and y35/SRPS/2014 trend lines for development SE (ICS-2=35.060 and ICS-2=35.080) segments.Paths of knowledge acquisition in SE fields are based on ISO.Originality of the topic for Check(i) innovation trend KB in time dimension and PiDiCiAi methodology.Platform of standardisation for knowledge improvement, model and products IS excellence (Ai) in PiDiCiAi.},
journal = {Comput. Stand. Interfaces},
month = feb,
pages = {1–17},
numpages = {17}
}

@article{10.1016/j.csl.2012.01.008,
author = {Li, Ming and Han, Kyu J. and Narayanan, Shrikanth},
title = {Automatic speaker age and gender recognition using acoustic and prosodic level information fusion},
year = {2013},
issue_date = {January, 2013},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {27},
number = {1},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2012.01.008},
doi = {10.1016/j.csl.2012.01.008},
abstract = {The paper presents a novel automatic speaker age and gender identification approach which combines seven different methods at both acoustic and prosodic levels to improve the baseline performance. The three baseline subsystems are (1) Gaussian mixture model (GMM) based on mel-frequency cepstral coefficient (MFCC) features, (2) Support vector machine (SVM) based on GMM mean supervectors and (3) SVM based on 450-dimensional utterance level features including acoustic, prosodic and voice quality information. In addition, we propose four subsystems: (1) SVM based on UBM weight posterior probability supervectors using the Bhattacharyya probability product kernel, (2) Sparse representation based on UBM weight posterior probability supervectors, (3) SVM based on GMM maximum likelihood linear regression (MLLR) matrix supervectors and (4) SVM based on the polynomial expansion coefficients of the syllable level prosodic feature contours in voiced speech segments. Contours of pitch, time domain energy, frequency domain harmonic structure energy and formant for each syllable (segmented using energy information in the voiced speech segment) are considered for analysis in subsystem (4). The proposed four subsystems have been demonstrated to be effective and able to achieve competitive results in classifying different age and gender groups. To further improve the overall classification performance, weighted summation based fusion of these seven subsystems at the score level is demonstrated. Experiment results are reported on the development and test set of the 2010 Interspeech Paralinguistic Challenge aGender database. Compared to the SVM baseline system (3), which is the baseline system suggested by the challenge committee, the proposed fusion system achieves 5.6% absolute improvement in unweighted accuracy for the age task and 4.2% for the gender task on the development set. On the final test set, we obtain 3.1% and 3.8% absolute improvement, respectively.},
journal = {Comput. Speech Lang.},
month = jan,
pages = {151–167},
numpages = {17},
keywords = {Age recognition, Formant, GMM, Gender recognition, Harmonic structure, Maximum likelihood linear regression, Pitch, Polynomial expansion, Prosodic features, SVM, Score level fusion, Sparse representation, UBM weight posterior probability supervectors}
}

@article{10.1007/s00766-015-0233-3,
author = {Bjarnason, Elizabeth and Sharp, Helen},
title = {The role of distances in requirements communication: a case study},
year = {2017},
issue_date = {March     2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-015-0233-3},
doi = {10.1007/s00766-015-0233-3},
abstract = {Requirements communication plays a vital role in development projects in coordinating the customers, the business roles and the software engineers. Communication gaps represent a significant source of project failures and overruns. For example, misunderstood or uncommunicated requirements can lead to software that does not meet the customers' requirements, and subsequent low number of sales or additional cost required to redo the implementation. We propose that requirements engineering (RE) distance measures are useful for locating gaps in requirements communication and for improving on development practice. In this paper, we present a case study of one software development project to evaluate this proposition. Thirteen RE distances were measured including geographical and cognitive distances between project members, and semantic distances between requirements and testing artefacts. The findings confirm that RE distances impact requirements communication and project coordination. Furthermore, the concept of distances was found to enable constructive group reflection on communication gaps and improvements to development practices. The insights reported in this paper can provide practitioners with an increased awareness of distances and their impact. Furthermore, the results provide a stepping stone for further research into RE distances and methods for improving on software development processes and practices.},
journal = {Requir. Eng.},
month = mar,
pages = {1–26},
numpages = {26},
keywords = {Agile, Case study, Communication, Distances, Measurements, Requirements, Testing}
}

@book{10.5555/2815511,
author = {Wu, Caesar and Buyya, Rajkumar},
title = {Cloud Data Centers and Cost Modeling: A Complete Guide To Planning, Designing and Building a Cloud Data Center},
year = {2015},
isbn = {012801413X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Cloud Data Centers and Cost Modeling establishes a framework for strategic decision-makers to facilitate the development of cloud data centers. Just as building a house requires a clear understanding of the blueprints, architecture, and costs of the project; building a cloud-based data center requires similar knowledge. The authors take a theoretical and practical approach, starting with the key questions to help uncover needs and clarify project scope. They then demonstrate probability tools to test and support decisions, and provide processes that resolve key issues. After laying a foundation of cloud concepts and definitions, the book addresses data center creation, infrastructure development, cost modeling, and simulations in decision-making, each part building on the previous. In this way the authors bridge technology, management, and infrastructure as a service, in one complete guide to data centers that facilitates educated decision making. Explains how to balance cloud computing functionality with data center efficiency Covers key requirements for power management, cooling, server planning, virtualization, and storage management Describes advanced methods for modeling cloud computing cost including Real Option Theory and Monte Carlo Simulations Blends theoretical and practical discussions with insights for developers, consultants, and analysts considering data center development}
}

@inproceedings{10.1145/2494603.2480297,
author = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
title = {RBUIS: simplifying enterprise application user interfaces through engineering role-based adaptive behavior},
year = {2013},
isbn = {9781450321389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494603.2480297},
doi = {10.1145/2494603.2480297},
abstract = {Enterprise applications such as customer relationship management (CRM) and enterprise resource planning (ERP) are very large scale, encompassing millions of lines-of-code and thousands of user interfaces (UI). These applications have to be sold as feature-bloated off-the-shelf products to be used by people with diverse needs in required feature-set and layout preferences based on aspects such as skills, culture, etc. Although several approaches have been proposed for adapting UIs to various contexts-of-use, little work has focused on simplifying enterprise application UIs through engineering adaptive behavior. We define UI simplification as a mechanism for increasing usability through adaptive behavior by providing users with a minimal feature-set and an optimal layout based on the context-of-use. In this paper we present Role-Based UI Simplification (RBUIS), a tool supported approach based on our CEDAR architecture for simplifying enterprise application UIs through engineering role-based adaptive behavior. RBUIS is integrated in our general-purpose platform for developing adaptive model-driven enterprise UIs. Our approach is validated from the technical and end-user perspectives by applying it to developing a prototype enterprise application and user-testing the outcome.},
booktitle = {Proceedings of the 5th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {3–12},
numpages = {10},
keywords = {adaptive user interfaces, enterprise applications, model-driven engineering, role-based, simplification},
location = {London, United Kingdom},
series = {EICS '13}
}

@inproceedings{10.1145/2162049.2162068,
author = {Ali, Shaukat and Yue, Tao and Malik, Zafar Iqbal},
title = {Comprehensively evaluating conformance error rates of applying aspect state machines},
year = {2012},
isbn = {9781450310925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2162049.2162068},
doi = {10.1145/2162049.2162068},
abstract = {Aspect Oriented Modeling (AOM) aims to provide enhanced separation of concerns during the design phase and proclaims many benefits (e.g., easier model evolution, reduced modeling effort, and reduced modeling errors) over traditional modeling paradigms such as object-oriented modeling. However, empirical evaluations of these benefits is severely lacking in the AOM community. In this paper, we empirically evaluate one of the AOM profiles: AspectSM, via a controlled experiment to assess if it can help in reducing modeling errors (referred as conformance errors in this paper), which is one of the benefits offered by AOM. AspectSM is a UML profile, which is developed to support automated state-based robustness testing. With AspectSM, crosscutting behaviors are modeled as aspect state machines using the stereotypes defined in AspectSM. We evaluate the conformance error rates of applying AspectSM from various perspectives by conducting four activities: 1) identifying modeling defects, 2) comprehending state machines, 3) modeling state machines, and 4) weaving aspect state machines into base state machines. For most of these activities, experimental results show that the error rates while performing these four activities using AspectSM are significantly lower than standard UML state machine modeling approaches.},
booktitle = {Proceedings of the 11th Annual International Conference on Aspect-Oriented Software Development},
pages = {155–166},
numpages = {12},
keywords = {UML state machines, aspect-oriented modeling, controlled experiment, modeling errors},
location = {Potsdam, Germany},
series = {AOSD '12}
}

@inproceedings{10.1145/2568225.2568230,
author = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
title = {Integrating adaptive user interface capabilities in enterprise applications},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568230},
doi = {10.1145/2568225.2568230},
abstract = {Many existing enterprise applications are at a mature stage in their development and are unable to easily benefit from the usability gains offered by adaptive user interfaces (UIs). Therefore, a method is needed for integrating adaptive UI capabilities into these systems without incurring a high cost or significantly disrupting the way they function. This paper presents a method for integrating adaptive UI behavior in enterprise applications based on CEDAR, a model-driven, service-oriented, and tool-supported architecture for devising adaptive enterprise application UIs. The proposed integration method is evaluated with a case study, which includes establishing and applying technical metrics to measure several of the method’s properties using the open-source enterprise application OFBiz as a test-case. The generality and flexibility of the integration method are also evaluated based on an interview and discussions with practitioners about their real-life projects.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {712–723},
numpages = {12},
keywords = {Adaptive user interfaces, enterprise systems, integration, model-driven engineering, software architectures, software metrics},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3318464.3389777,
author = {Shah, Vraj and Li, Side and Kumar, Arun and Saul, Lawrence},
title = {SpeakQL: Towards Speech-driven Multimodal Querying of Structured Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389777},
doi = {10.1145/3318464.3389777},
abstract = {Speech-driven querying is becoming popular in new device environments such as smartphones, tablets, and even conversational assistants. However, such querying is largely restricted to natural language. Typed SQL remains the gold standard for sophisticated structured querying although it is painful in many environments, which restricts when and how users consume their data. In this work, we propose to bridge this gap by designing a speech-driven querying system and interface for structured data we call SpeakQL. We support a practically useful subset of regular SQL and allow users to query in any domain with novel touch/speech based human-in-the-loop correction mechanisms. Automatic speech recognition (ASR) introduces myriad forms of errors in transcriptions, presenting us with a technical challenge. We exploit our observations of SQL's properties, its grammar, and the queried database to build a modular architecture. We present the first dataset of spoken SQL queries and a generic approach to generate them for any arbitrary schema. Our experiments show that SpeakQL can automatically correct a large fraction of errors in ASR transcriptions. User studies show that SpeakQL can help users specify SQL queries significantly faster with a speedup of average 2.7x and up to 6.7x compared to typing on a tablet device. SpeakQL also reduces the user effort in specifying queries by a factor of average 10x and up to 60x compared to raw typing effort.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2363–2374},
numpages = {12},
keywords = {SQL, multimodal, speech-driven},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/2048066.2048123,
author = {Lorenz, David H. and Rosenan, Boaz},
title = {Cedalion: a language for language oriented programming},
year = {2011},
isbn = {9781450309400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2048066.2048123},
doi = {10.1145/2048066.2048123},
abstract = {Language Oriented Programming (LOP) is a paradigm that puts domain specific programming languages (DSLs) at the center of the software development process. Currently, there are three main approaches to LOP: (1) the use of internal DSLs, implemented as libraries in a given host language; (2) the use of external DSLs, implemented as interpreters or compilers in an external language; and (3) the use of language workbenches, which are integrated development environments (IDEs) for defining and using external DSLs. In this paper, we contribute: (4) a novel language-oriented approach to LOP for defining and using internal DSLs. While language workbenches adapt internal DSL features to overcome some of the limitations of external DSLs, our approach adapts language workbench features to overcome some of the limitations of internal DSLs. We introduce Cedalion, an LOP host language for internal DSLs, featuring static validation and projectional editing. To validate our approach we present a case study in which Cedalion was used by biologists in designing a DNA microarray for molecular Biology research.},
booktitle = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications},
pages = {733–752},
numpages = {20},
keywords = {Cedalion, domain-specific languages (DSLs), language workbench, language-oriented programming (LOP), logic programming},
location = {Portland, Oregon, USA},
series = {OOPSLA '11}
}

@article{10.5555/3294043.3294045,
author = {Lee, Hye-jin and Lee, Katie Ka-hyun and Choi, Junho},
title = {A structural model for unity of experience: connecting user experience, customer experience, and brand experience},
year = {2018},
issue_date = {November 2018},
publisher = {Usability Professionals' Association},
address = {Bloomingdale, IL},
volume = {14},
number = {1},
abstract = {Understanding customer experience from a holistic perspective requires examination of user experience in the context of marketing and branding. This study attempts to underpin the effects of UX on brand equity by developing and verifying a conceptual framework that connects user experience (UX), customer experience (CX), and brand experience (BX). A structural equation modeling test using data from smartphone users verified the effects of UX on brand equity mediated by CX. In the UX dimension, usability had a strong effect on brand equity, and affect and user value had an effect on customer experience. As a mediator, customer experience had an impact on brand equity with a high path weight. By implementing UX strategies that cohere with management strategies, companies can establish a high level of consumer perception of customer experience and brand value. The results and analyses of this research can help businesses establish a strategy for examining which element of UX is related to CX and BX.},
journal = {J. Usability Studies},
month = nov,
pages = {8–34},
numpages = {27},
keywords = {affect, brand equity, customer experience, usability, user experience}
}

@inproceedings{10.1145/1961258.1961264,
author = {Ribaud, Vincent and Saliou, Philippe},
title = {Process assessment issues of the ISO/IEC 29110 emerging standard},
year = {2010},
isbn = {9781450302814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1961258.1961264},
doi = {10.1145/1961258.1961264},
abstract = {The emerging ISO/IEC 29110 standard "Software Engineering - Lifecycle Profiles for Very Small Entities (VSE)" is an ISO initiative to provide Very Small Entities (VSE) with a suitable set of profiles for Process Assessment and Process Improvement. The approach is conforming to ISO 15504 2-D model of process capability: a process dimension based on a Process Reference Model (PRM), and a capability dimension with a set of process attributes grouped into capability levels. The ISO/IEC 29110 standard is developing 4 profiles for VSEs developing generic software: Entry, Basic, Intermediate and Advanced. This paper establishes a reduced set of Base Practices profiled from ISO 15504-5 "An exemplar Process Assessment Model (PAM)". It applies recommendations of ISO/IEC 29110 DTR 29110-3 about assessment and questions the use of a separated capability dimension and its usability for a VSE.},
booktitle = {Proceedings of the 11th International Conference on Product Focused Software},
pages = {24–27},
numpages = {4},
keywords = {ISO/IEC 29110, process assessment model, software engineering processes},
location = {Limerick, Ireland},
series = {PROFES '10}
}

@inproceedings{10.1145/1852786.1852800,
author = {Kasurinen, Jussi and Taipale, Ossi and Smolander, Kari},
title = {Test case selection and prioritization: risk-based or design-based?},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852800},
doi = {10.1145/1852786.1852800},
abstract = {The objective of this qualitative study was to observe and empirically study how software organizations decide on which test cases to select for their software projects. As the software test processes are limited in resources such as time or money, a selection process usually exists for tested features. In this study we conducted a survey on 31 software-producing organizations, and interviewed 36 software professionals from 12 focus organizations to gain a better insight into testing practices. Our findings indicated that the basic approaches to test case selection are usually oriented towards two possible objectives. One is the risk-based selection, where the aim is to focus testing on those parts that are too expensive to fix after launch. The other is design-based selection, where the focus is on ensuring that the software is capable of completing the core operations it was designed to do. These results can then be used to develop testing organizations and to identify better practices for test case selection.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {10},
numpages = {10},
keywords = {empirical study, grounded theory, software testing, test case selection},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@book{10.5555/2505465,
author = {Schmidt, Richard},
title = {Software Engineering: Architecture-driven Software Development},
year = {2013},
isbn = {0124077684},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Engineering: Architecture-driven Software Development is the first comprehensive guide to the underlying skills embodied in the IEEE's Software Engineering Body of Knowledge (SWEBOK) standard. Standards expert Richard Schmidt explains the traditional software engineering practices recognized for developing projects for government or corporate systems. Software engineering education often lacks standardization, with many institutions focusing on implementation rather than design as it impacts product architecture. Many graduates join the workforce with incomplete skills, leading to software projects that either fail outright or run woefully over budget and behind schedule. Additionally, software engineers need to understand system engineering and architecture-the hardware and peripherals their programs will run on. This issue will only grow in importance as more programs leverage parallel computing, requiring an understanding of the parallel capabilities of processors and hardware. This book gives both software developers and system engineers key insights into how their skillsets support and complement each other. With a focus on these key knowledge areas, Software Engineering offers a set of best practices that can be applied to any industry or domain involved in developing software products. A thorough, integrated compilation on the engineering of software products, addressing the majority of the standard knowledge areas and topics Offers best practices focused on those key skills common to many industries and domains that develop software Learn how software engineering relates to systems engineering for better communication with other engineering professionals within a project environment}
}

@inproceedings{10.1145/2181101.2181102,
author = {von Wangenheim, Christiane Gresse and Hauck, Jean Carlo R. and Buglione, Luigi and McCaffery, Fergal and Lacerda, Thaisa Cardoso and da Cruz, Ronny F. Vieira},
title = {Building a maturity &amp; capability model repository},
year = {2011},
isbn = {9781450307833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2181101.2181102},
doi = {10.1145/2181101.2181102},
abstract = {A complicated and time-consuming phase in the development of Maturity/Capability Models (MCMs) is the identification of existing relevant source models as, currently, information on existing MCMs is provided in very different forms and levels of detail on diverse web sites, publications etc. In this paper, we present our ongoing research on developing a web-based repository to store and provide overview information on MCMs as a continuous knowledge management effort maintained within the Software Process Improvement (SPI) community. Such a centralized repository containing metadata on MCMs is expected to facilitate the identification of relevant models (as well as parts) and provide a systematic basis for the development/evolution or customization of MCMs.},
booktitle = {Proceedings of the 12th International Conference on Product Focused Software Development and Process Improvement},
pages = {2–5},
numpages = {4},
keywords = {content management, knowledge management, maturity/capability models, software process improvement},
location = {Torre Canne, Brindisi, Italy},
series = {Profes '11}
}

@article{10.1016/j.jss.2009.06.052,
author = {Chen, Chung-Yang and Chen, Pei-Chi},
title = {A holistic approach to managing software change impact},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.052},
doi = {10.1016/j.jss.2009.06.052},
abstract = {Change is inevitable in the software product lifecycle. When a software change occurs, all of the stakeholders and related artifacts should be considered in determining the success of the change action in a collaborative development environment such as JAD (joint application development). In this regard, current implementation-based or homogeneous impact analyses are insufficient; therefore, this paper presents a holistic approach to change impact analysis in handling not only software contents but also other items such as requirements, documents and data. This approach characterizes product contents and relates heterogeneous items by using attributes and linkages. It also uses an object-oriented propagation mechanism to handle dynamic looping in determining the impact of changes. A prototype, EPIC, was built to realize this approach and these concepts. A walkthrough example is provided in order to verify the work of the proposed approach. An empirical study is presented to discuss the benefits of the proposed approach and the application of EPIC in a software company. Lessons learned from the case study and improvement issues of the proposed approach and the tool are also discussed.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2051–2067},
numpages = {17},
keywords = {Collaborative development, Holistic approach, Object technology, Software change management}
}

@article{10.1287/isre.1090.0242,
author = {Austin, Robert D. and Devin, Lee},
title = {Research Commentary---Weighing the Benefits and Costs of Flexibility in Making Software: Toward a Contingency Theory of the Determinants of Development Process Design},
year = {2009},
issue_date = {September 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {20},
number = {3},
issn = {1526-5536},
url = {https://doi.org/10.1287/isre.1090.0242},
doi = {10.1287/isre.1090.0242},
abstract = {In recent years, flexibility has emerged as a divisive issue in discussions about the appropriate design of processes for making software. Partisans in both research and practice argue for and against plan-based (allegedly inflexible) and agile (allegedly too flexible) approaches. The stakes in this debate are high; questions raised about plan-based approaches undermine longstanding claims that those approaches, when realized, represent maturity of practice. In this commentary, we call for research programs that will move beyond partisan disagreement to a more nuanced discussion, one that takes into account both benefits and costs of flexibility. Key to such programs will be the development of a robust contingency framework for deciding when (in what conditions) plan-based and agile methods should be used. We develop a basic contingency framework in this paper, one that models the benefit/cost economics described in narratives about the transition from craft to industrial production of physical products. We use this framework to demonstrate the power of even a simple model to help us accomplish three objectives: (1) to refocus discussions about the appropriate design of software development processes, concentrating on when to use particular approaches and how they might be usefully combined; (2) to suggest and guide a trajectory of research that can support and enrich this discussion; and (3) to suggest a technology-based explanation for the emergence of agile development at this point in history. Although we are not the first to argue in favor of a contingency perspective, we show that there remain many opportunities for information systems (IS) research to have a major impact on practice in this area.},
journal = {Info. Sys. Research},
month = sep,
pages = {462–477},
numpages = {16},
keywords = {process design, software development, software methodologies, work design}
}

@article{10.1016/j.future.2017.08.015,
author = {Shang, Fengjun and Mao, Lin and Gong, Wenjuan},
title = {Service-aware adaptive link load balancing mechanism for Software-Defined Networking},
year = {2018},
issue_date = {April 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {81},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2017.08.015},
doi = {10.1016/j.future.2017.08.015},
abstract = {A Service-Oriented Load Balancing Mechanism for Software-Defined Networking is proposed to resolve the problems of network load imbalance and the scalability in control plane. The main research works includes: Firstly, a load balancing model is proposed for distributed controllers. In the model, the flow-requests information among the switches is used as the basic unit of the controller, all controllers publish respective the total of the flow-requests periodically and flow-requests deviation mean is introduced to aware load status for controllers. On this basis, a load balancing algorithm is proposed based on load-aware. This algorithm uses flow-requests information allocation strategy, taking into account the current load and propagation delay of idle controllers and allocating partial the flow-request information of the overload controller to the idle controller with the minimum of flow-requests deviation and propagation delay. In order to avoid inconsistency of the network status caused by performing load balancing algorithm at the same, each controller maintains a flow-request deviation the mean table and the overload controller performs the load balancing algorithm based on the magnitude of the value in the table. Secondly, service-aware adaptive link load balancing mechanism is designed which can aware the service types using the northbound interface and monitor the state of the network periodically. Adaptive link load balancing algorithm is proposed and the link weight is introduced based on QoS-aware, which measures the comprehensive quality of link through acquiring real-time QoS parameters. So the mechanism can select the path of the best link quality in the current to forward data, reduce the imbalance of the load distribution in the network. Meanwhile for different service types, dynamic QoS routing optimization strategy is proposed further. It adopts the Lagrange relaxation technique to compute a path which meets the QoS constraint. Experimental results show that the proposed mechanism can effectively balance the network traffic, the average utilization of link bandwidth up to 79%, and achieve QoS guarantee for different services. A Service-Oriented Load Balancing Mechanism for Software-Defined Networking is proposed.Adaptive link load balancing algorithm is proposed and the link weight is introduced based on QoS-aware.The proposed method adopts the Lagrange relaxation technique to compute a path which meets the QoS constraint.},
journal = {Future Gener. Comput. Syst.},
month = apr,
pages = {452–464},
numpages = {13},
keywords = {Distributed controllers, Load balancing, Load-aware, SDN, Service-aware}
}

@article{10.1016/j.jss.2005.10.017,
author = {Roeller, Ronny and Lago, Patricia and van Vliet, Hans},
title = {Recovering architectural assumptions},
year = {2006},
issue_date = {April 2006},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {79},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2005.10.017},
doi = {10.1016/j.jss.2005.10.017},
abstract = {During the creation of a software architecture, the architects and stakeholders take a lot of decisions. Many of these decisions can be directly related to functional or quality requirements. Some design decisions, though, are more or less arbitrarily made on the fly because of personal experience, domain knowledge, budget constraints, available expertise, and the like. These decisions, as well as the reasons for those decisions, are often not explicit upfront. They are implicit, and usually remain undocumented. We call them assumptions. There is no accepted way to document assumptions, and the relation between the software architecture and these assumptions easily gets lost, becomes hidden in the girders of the architecture. They are rediscovered at a later stage, when the software evolves and assumptions become invalid or new assumptions contradict earlier ones. In this paper, we develop a method to recover such assumptions from an existing software product. We illustrate the method by applying it to a commercial software product, and show how the results can help assess the evolutionary capabilities of its architecture.},
journal = {J. Syst. Softw.},
month = apr,
pages = {552–573},
numpages = {22},
keywords = {Architectural knowledge, Architecture recovery, Assumptions, Software architecture}
}

@inproceedings{10.1007/978-3-642-25953-1_12,
author = {Beltran, Victoria and Arabshian, Knarig and Schulzrinne, Henning},
title = {Ontology-Based user-defined rules and context-aware service composition system},
year = {2011},
isbn = {9783642259524},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25953-1_12},
doi = {10.1007/978-3-642-25953-1_12},
abstract = {The World Wide Web is becoming increasingly personalized as users provide more of their information on the Web. Thus, Web service functionality is becoming reliant on user profile information and context in order to provide user-specific data. In this paper, we discuss enhancements to SECE (Sense Everything, Control Everything), a platform for context-aware service composition based on user-defined rules. We have enhanced SECE to interpret ontology descriptions of services. With this enhancement, SECE can now create user-defined rules based on the ontology description of the service and interoperate within any service domain that has an ontology description. Additionally, it can use an ontology-based service discovery system like GloServ as its service discovery back-end in order to issue more complex queries for service discovery and composition. This paper discusses the design and implementation of these improvements.},
booktitle = {Proceedings of the 8th International Conference on The Semantic Web},
pages = {139–155},
numpages = {17},
keywords = {context-aware systems, ontologies, rule-based systems, semantic web, service composition, service discovery, web services},
location = {Heraklion, Crete, Greece},
series = {ESWC'11}
}

@inproceedings{10.1007/978-3-642-40270-8_4,
author = {Bari, A. T. and Reaz, Mst. Rokeya and Choi, Ho-Jin and Jeong, Byeong-Soo},
title = {DNA Encoding for Splice Site Prediction in Large DNA Sequence},
year = {2013},
isbn = {9783642402692},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-40270-8_4},
doi = {10.1007/978-3-642-40270-8_4},
abstract = {Splice site prediction in the pre-mRNA is a very important task for understanding gene structure and its function. To predict splice sites, SVM support vector machine based classification technique is frequently used because of its classification accuracy. High classification accuracy of SVM largely depends on DNA encoding method for feature extraction of DNA sequences. However, existing encoding approaches do not reveal the characteristics of DNA sequence very well enough to provide as much information as DNA sequences have. In this paper, we propose new effective DNA encoding method which can give more information of DNA sequence. Our encoding method can provide density information of each nucleotide along with positional information and chemical property. Extensive performance study shows that our method can provide better performance than existing encoding methods based on several performance criteria such as classification accuracy, sensitivity, specificity and area under receiver operating characteristics curve ROC.},
booktitle = {Proceedings of the 18th International Conference on Database Systems for Advanced Applications - Volume 7827},
pages = {46–58},
numpages = {13},
keywords = {DNA sequence, ROC, gene prediction, nucleotide density, orthogonal encoding, splice site, support vector machine}
}

@inproceedings{10.5555/2343576.2343600,
author = {Paraschos, Alexandros and Spanoudakis, Nikolaos I. and Lagoudakis, Michail G.},
title = {Model-driven behavior specification for robotic teams},
year = {2012},
isbn = {0981738117},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Modern model-driven engineering and Agent-Oriented Software Engineering (AOSE) methods are rarely utilized in developing robotic software. In this paper, we show how a Model-Driven AOSE methodology can be used for specifying the behavior of multi-robot teams. Specifically, the Agent Systems Engineering Methodology (ASEME) was used for developing the software that realizes the behavior of a physical robot team competing in the Standard Platform League of the RoboCup competition (the robot soccer world cup). The team consists of four humanoid robots, which play soccer autonomously in real time utilizing the on-board sensing, processing, and actuating capabilities, while communicating and coordinating with each other in order to achieve their common goal of winning the game. Our work focuses on the challenges of coordinating the base functionalities (object recognition, localization, motion skills) within each robot (intra-agent control) and coordinating the activities of the robots towards a desired team behavior (inter-agent control). We discuss the difficulties we faced and present the solutions we gave to a number of practical issues, which, in our view, are inherent in applying any AOSE methodology to robotics. We demonstrate the added value of using an AOSE methodology in the development of robotic systems, as ASEME allowed for a platform-independent team behavior specification, automated a large part of the code generation process, and reduced the total development time.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {171–178},
numpages = {8},
keywords = {agent-oriented software engineering, intra-agent control, model-driven engineering, robotic software development},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@book{10.5555/2669162,
author = {Felfernig, Alexander and Hotz, Lothar and Bagley, Claire and Tiihonen, Juha},
title = {Knowledge-based Configuration: From Research to Business Cases},
year = {2014},
isbn = {012415817X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1},
abstract = {Knowledge-based Configuration incorporates knowledge representation formalisms to capture complex product models and reasoning methods to provide intelligent interactive behavior with the user. This book represents the first time that corporate and academic worlds collaborate integrating research and commercial benefits of knowledge-based configuration. Foundational interdisciplinary material is provided for composing models from increasingly complex products and services. Case studies, the latest research, and graphical knowledge representations that increase understanding of knowledge-based configuration provide a toolkit to continue to push the boundaries of what configurators can do and how they enable companies and customers to thrive.Includes detailed discussion of state-of-the art configuration knowledge engineering approaches such as automated testing and debugging, redundancy detection, and conflict management Provides an overview of the application of knowledge-based configuration technologies in the form of real-world case studies from SAP, Siemens, Kapsch, and more Explores the commercial benefits of knowledge-based configuration technologies to business sectors from services to industrial equipment Uses concepts that are based on an example personal computer configuration knowledge base that is represented in an UML-based graphical language}
}

@article{10.1016/j.engappai.2009.08.004,
author = {Fuentes-Fern\'{a}ndez, Rub\'{e}n and Garc\'{\i}a-Magari\~{n}o, Iv\'{a}n and G\'{o}mez-Rodr\'{\i}guez, Alma Mar\'{\i}a and Gonz\'{a}lez-Moreno, Juan Carlos},
title = {A technique for defining agent-oriented engineering processes with tool support},
year = {2010},
issue_date = {April, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {23},
number = {3},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.08.004},
doi = {10.1016/j.engappai.2009.08.004},
abstract = {The formalization of engineering processes is necessary for guaranteeing the quality and cost of the products involved. Agent-oriented engineering has already proposed development processes that still need to be further formalized to be applicable by non-researchers. This paper proposes a technique to instantiate processes for specific agent-oriented methodologies. This technique is based on three orthogonal views that are respectively related with lifecycles, disciplines and guidances. In addition, processes are modeled with a tool, which is automatically generated from a process metamodel inspired by the software &amp; systems process engineering metamodel. Accordingly, engineers can choose the methodology-process pair best-suited for the characteristics of their project. The paper illustrates the approach based on the unified development process and the scrum process for the INGENIAS methodology and compares the results with other existing alternatives.},
journal = {Eng. Appl. Artif. Intell.},
month = apr,
pages = {432–444},
numpages = {13},
keywords = {Agent-oriented engineering, Model-driven development, Multi-agent system, Process engineering, Software &amp; systems process engineering metamodel}
}

@inproceedings{10.1007/978-3-642-33765-9_53,
author = {Klaudiny, Martin and Budd, Chris and Hilton, Adrian},
title = {Towards optimal non-rigid surface tracking},
year = {2012},
isbn = {9783642337642},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33765-9_53},
doi = {10.1007/978-3-642-33765-9_53},
abstract = {This paper addresses the problem of optimal alignment of non-rigid surfaces from multi-view video observations to obtain a temporally consistent representation. Conventional non-rigid surface tracking performs frame-to-frame alignment which is subject to the accumulation of errors resulting in drift over time. Recently, non-sequential tracking approaches have been introduced which re-order the input data based on a dissimilarity measure. One or more input sequences are represented in a tree with reducing alignment path length. This limits drift and increases robustness to large non-rigid deformations. However, jumps may occur in the aligned mesh sequence where tree branches meet due to independent error accumulation. Optimisation of the tree for non-sequential tracking is proposed to minimise the errors in temporal consistency due to both the drift and jumps. A novel cluster tree enforces sequential tracking in local segments of the sequence while allowing global non-sequential traversal among these segments. This provides a mechanism to create a tree structure which reduces the number of jumps between branches and limits the length of branches. Comprehensive evaluation is performed on a variety of challenging non-rigid surfaces including faces, cloth and people. This demonstrates that the proposed cluster tree achieves better temporal consistency than the previous sequential and non-sequential tracking approaches. Quantitative ground-truth comparison on a synthetic facial performance shows reduced error with the cluster tree.},
booktitle = {Proceedings of the 12th European Conference on Computer Vision - Volume Part IV},
pages = {743–756},
numpages = {14},
keywords = {cluster tree, dense motion capture, dissimilarity, minimum spanning tree, non-rigid surface alignment, non-sequential tracking},
location = {Florence, Italy},
series = {ECCV'12}
}

@inproceedings{10.1145/1753326.1753454,
author = {Sj\"{o}lie, Daniel and Bodin, Kenneth and Elgh, Eva and Eriksson, Johan and Janlert, Lars-Erik and Nyberg, Lars},
title = {Effects of interactivity and 3D-motion on mental rotation brain activity in an immersive virtual environment},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753454},
doi = {10.1145/1753326.1753454},
abstract = {The combination of virtual reality (VR) and brain measurements is a promising development of HCI, but the maturation of this paradigm requires more knowledge about how brain activity is influenced by parameters of VR applications. To this end we investigate the influence of two prominent VR parameters, 3d-motion and interactivity, while brain activity is measured for a mental rotation task, using functional MRI (fMRI). A mental rotation network of brain areas is identified, matching previous results. The addition of interactivity increases the activation in core areas of this network, with more profound effects in frontal and preparatory motor areas. The increases from 3d-motion are restricted to primarily visual areas. We relate these effects to emerging theories of cognition and potential applications for brain-computer interfaces (BCIs). Our results demonstrate one way to provoke increased activity in task-relevant areas, making it easier to detect and use for adaptation and development of HCI.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {869–878},
numpages = {10},
keywords = {bci, brain imaging, fmri, reality-based interaction, virtual reality, vrfmri},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@article{10.1145/2529981,
author = {Misra, Prasant and Kottege, Navinda and Kusy, Branislav and Ostry, Diethelm and Jha, Sanjay},
title = {Acoustical ranging techniques in embedded wireless sensor networked devices},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/2529981},
doi = {10.1145/2529981},
abstract = {Location sensing provides endless opportunities for a wide range of applications in GPS-obstructed environments, where, typically, there is a need for a higher degree of accuracy. In this article, we focus on robust range estimation, an important prerequisite for fine-grained localization. Motivated by the promise of acoustic in delivering high ranging accuracy, we present the design, implementation, and evaluation of acoustic (both ultrasound and audible) ranging systems. We distill the limitations of acoustic ranging and present efficient signal designs and detection algorithms to overcome the challenges of coverage, range, accuracy/resolution, tolerance to Doppler's effect, and audible intensity. We evaluate our proposed techniques experimentally on TWEET, a low-power platform purpose-built for acoustic ranging applications. Our experiments demonstrate an operational range of 20m (outdoor) and an average accuracy ≈2cm in the ultrasound domain. Finally, we present the design of an audible-range acoustic tracking service that encompasses the benefits of a near-inaudible acoustic broadband chirp and approximately two times increase in Doppler tolerance to achieve better performance.},
journal = {ACM Trans. Sen. Netw.},
month = dec,
articleno = {15},
numpages = {38},
keywords = {Doppler's effect, Ranging, audible-range acoustics, envelope detection, linear chirp, localization, near-inaudible acoustic signal design, tracking, ultrasound}
}

@article{10.1016/j.is.2005.05.003,
author = {Rosemann, M. and van der Aalst, W. M. P.},
title = {A configurable reference modelling language},
year = {2007},
issue_date = {March, 2007},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {32},
number = {1},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2005.05.003},
doi = {10.1016/j.is.2005.05.003},
abstract = {Enterprise Systems (ES) are comprehensive off-the-shelf packages that have to be configured to suit the requirements of an organization. Most ES solutions provide reference models that describe the functionality and structure of the system. However, these models do not capture the potential configuration alternatives. This paper discusses the shortcomings of current reference modelling languages using Event-Driven Process Chains (EPCs) as an example. We propose Configurable EPCs (C-EPCs) as an extended reference modelling language which allows capturing the core configuration patterns. A formalization of this language as well as examples for typical configurations are provided. A program of further research including the identification of a comprehensive list of configuration patterns, deriving possible notations for reference model configurations and testing the quality of these proposed extensions in experiments and focus groups is presented.},
journal = {Inf. Syst.},
month = mar,
pages = {1–23},
numpages = {23},
keywords = {Configuration, Enterprise systems, Event-Driven Process Chains, Reference model}
}

@article{10.3233/JIFS-191385,
author = {Qin, Yuchu and Qi, Qunfen and Shi, Peizhi and Scott, Paul J. and Jiang, Xiangqian},
title = {Linguistic interval-valued intuitionistic fuzzy Archimedean prioritised aggregation operators for multi-criteria decision making},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {38},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-191385},
doi = {10.3233/JIFS-191385},
abstract = {Two key steps in multi-criteria decision making (MCDM) are to quantify the considered criteria and to fuse the quantified criterion information to sort all alternatives. One of the most recent and important tools for the first step is linguistic interval-valued intuitionistic fuzzy number (LIVIFN) and one of the most common and effective ways for the second step is aggregation operator (AO). So far, a number of AOs of LIVIFNs have been presented. Each AO can work well under certain conditions. But there is not yet an AO of LIVIFNs that can deal with the situation where the weights of the considered criteria are unknown and the criteria are in different priority levels and concurrently provide satisfying generality and flexibility in the aggregation of criterion information. To this end, a linguistic interval-valued intuitionistic fuzzy Archimedean prioritised and (LIVIFAPA) operator and a linguistic interval-valued intuitionistic fuzzy Archimedean prioritised or (LIVIFAPO) operator, which have such capabilities, are presented in this paper. The formal definitions and generalised expressions of the two AOs are firstly provided. Then their properties are explored and proved and specific expressions are established. After that, a new method for solving the LIVIFNs based MCDM problems is proposed on the basis of the presented AOs. Finally, the proposed method is illustrated via an example about additive manufacturing machine selection and is evaluated via a comparison with existing methods. The major contribution of the paper is the development of the LIVIFAPA and LIVIFAPO operators for MCDM, which can make up for the above shortcoming of the existing AOs of LIVIFNs.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {4643–4666},
numpages = {24},
keywords = {Prioritised and operator, prioritised or operator, linguistic interval-valued intuitionistic fuzzy set, Archimedean t-norm and t-conorm, multi-criteria decision making}
}

@article{10.4018/IJUDH.2016010104,
author = {Demurjian, Steven A. and Agresta, Thomas and Ziminski, Timoteus B. and Sanzi, Eugene and Baihan, Mohammed},
title = {An Architectural Solution for Health Information Exchange},
year = {2016},
issue_date = {January 2016},
publisher = {IGI Global},
address = {USA},
volume = {6},
number = {1},
issn = {2156-1818},
url = {https://doi.org/10.4018/IJUDH.2016010104},
doi = {10.4018/IJUDH.2016010104},
abstract = {Health information technology HIT systems including electronic health records EHRs have a market saturation nearing 92% at individual institutions but are still unsuited for cross-institutional collaboration of stakeholders e.g., medical providers such as physicians, hospitals, clinics, labs, etc. in support of health information exchange HIE of different HIT systems in geographically separate locations. In the computer science field, software architectures such as service-oriented architecture, grid computing, publish/subscribe paradigm, and data warehousing are well-established approaches for interoperation. However, the application of these software architectures to support HIE has not been significantly explored. To address this issue, this paper proposes an architectural solution for HIE that leverages established software architectural styles in conjunction with the emergent HL7 standard Fast Healthcare Interoperability Resources FHIR. FHIR models healthcare data with XML or JSON schemas using a set of 93 resources to track a patient's clinical findings, problems, allergies, adverse events, history, suggested physician orders, care planning, etc. For each resource, a FHIR CRUD RESTful Application Program Interface API is defined to share data in a common format for each of the HITs that can then be easily accessible by mobile applications. This paper details an architectural solution for HIE using software architectural styles in conjunction with FHIR to allow HIT systems of stakeholders to be integrated to facilitate collaboration among medical providers. To demonstrate the feasibility and utility of HHIEA, a realistic regional healthcare scenario is introduced that illustrates the interactions of stakeholders across an integrated collection of HIT systems.},
journal = {Int. J. User-Driven Healthc.},
month = jan,
pages = {65–103},
numpages = {39},
keywords = {Cloud Computing, Data Integration, Data Warehouse, Fast Healthcare Interoperability Resources, Grid Computing, Health Information Exchange, Health Information Systems, Service-Oriented Architecture, Software Architecture, System Integration}
}

@article{10.1007/s10922-009-9119-3,
author = {Strassner, John and Souza, Jos\'{e} Neuman and Meer, Sven and Davy, Steven and Barrett, Keara and Raymer, David and Samudrala, Srini},
title = {The Design of a New Policy Model to Support Ontology-Driven Reasoning for Autonomic Networking},
year = {2009},
issue_date = {June      2009},
publisher = {Plenum Press},
address = {USA},
volume = {17},
number = {1–2},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-009-9119-3},
doi = {10.1007/s10922-009-9119-3},
abstract = {The purpose of autonomic networking is to manage the business and technical complexity of networked components and systems. However, the lack of a common lingua franca makes it impossible to use vendor-specific network management data to ascertain the state of the network at any given time. Furthermore, the tools used to analyze management data, which include information and data models, ontologies, machine learning algorithms, and policy languages, are all different, and hence require different data in different formats. This paper describes a new version of the Directory Enabled Networks next generation (DEN-ng) policy model, which is part of the FOCALE autonomic network architecture. This new policy model has been built using three guiding principles: (1) the policy model is rooted in information models, so that it can govern managed entities, (2) the model is expressly constructed to facilitate the generation of ontologies, so that reasoning about policies constructed from the model may be done, and (3) the model is expressly constructed so that a policy language can be developed from it.},
journal = {J. Netw. Syst. Manage.},
month = jun,
pages = {5–32},
numpages = {28},
keywords = {Autonomic networking, FOCALE autonomic architecture, Next generation services, Ontology-based management, Policy management, Semantic reasoning}
}

@inproceedings{10.1007/978-3-319-05476-6_15,
author = {Indyka-Piasecka, Agnieszka and Jacewicz, Piotr},
title = {Using Lexical Semantic Relation and Multi-attribute Structures for User Profile Adaptation},
year = {2014},
isbn = {9783319054759},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-05476-6_15},
doi = {10.1007/978-3-319-05476-6_15},
abstract = {This contribution presents a new approach to the representation of user interests and preferences at information retrieval process. The adaptive user profile includes both interests given explicitly by the user, as a query, and also preferences expressed by the valuation of relevance of retrieved documents, so to express field independent translation between terminology used by user and terminology accepted in some field of knowledge. Building, modifying, expanding by semantically related terms and using procedures for the profile are presented. Experiments concerning the profile, as a personalization mechanism of Web retrieval system, are presented and discussed.},
booktitle = {Proceedings, Part I, of the 6th Asian Conference on Intelligent Information and Database Systems - Volume 8397},
pages = {143–152},
numpages = {10},
location = {Bangkok, Thailand},
series = {ACIIDS 2014}
}

@inproceedings{10.1145/3098279.3098564,
author = {Ghosh, Surjya and Ganguly, Niloy and Mitra, Bivas and De, Pradipta},
title = {TapSense: combining self-report patterns and typing characteristics for smartphone based emotion detection},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098564},
doi = {10.1145/3098279.3098564},
abstract = {Typing based communication applications on smartphones, like WhatsApp, can induce emotional exchanges. The effects of an emotion in one session of communication can persist across sessions. In this work, we attempt automatic emotion detection by jointly modeling the typing characteristics, and the persistence of emotion. Typing characteristics, like speed, number of mistakes, special characters used, are inferred from typing sessions. Self reports recording emotion states after typing sessions capture persistence of emotion. We use this data to train a personalized machine learning model for multi-state emotion classification. We implemented an Android based smartphone application, called TapSense, that records typing related metadata, and uses a carefully designed Experience Sampling Method (ESM) to collect emotion self reports. We are able to classify four emotion states - happy, sad, stressed, and relaxed, with an average accuracy (AUCROC) of 84% for a group of 22 participants who installed and used TapSense for 3 weeks.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {2},
numpages = {12},
keywords = {Markov chain, SMOTE, Smartphone typing, emotion detection, emotion persistence, experience sampling method},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inbook{10.1145/3191315.3191327,
title = {Index},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191327},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {548–587},
numpages = {40}
}

@article{10.1023/A:1013116910400,
author = {Gao, Jerry Z. and Itaru, Fukao and Toyoshima, Y.},
title = {Managing Problems for Global Software Production – Experience and Lessons},
year = {2002},
issue_date = {January 2002},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {3},
number = {1–2},
issn = {1385-951X},
url = {https://doi.org/10.1023/A:1013116910400},
doi = {10.1023/A:1013116910400},
abstract = {With the increase in size and complexity of current software projects, many large companies have established global software production lines over the world to develop and deliver software products with collaborative software development processes involving multiple teams located at different sites. Supporting global software production needs an effective software-engineering environment to meet the special requirements of the collaborative software development process, diverse management methods and engineering practice. WWW technology provides powerful means to set up an enterprise-oriented software engineering environment for global software production due to its advantages in networking, global access, internationalization, and communication. Although there are many articles addressing the methods and experience in building web-based applications systems and tools, very few papers discuss the real-world problems and solutions in the development and deployment of web-based software tools to support a collaborative software development process for global software production. This paper discusses the real world issues, and reports our experience and lessons in building and deploying a web-based problem information management system (PIMS) to support global software development processes at Fujitsu. It focuses on the real issues and needs of current collaborative development process involving multiple teams, and highlights the benefits and impact of the PIMS on global software production. Moreover, it discusses our technical solutions and trade-offs in the development of PIMS, and shares our experience and lessons. Furthermore, it introduces a new data-centered conceptual process model to support diverse collaborative processes for project and problem management in global software production. Finally, the paper shares our key successes and weaknesses, and reports our experience and lessons in the deployment of the system.},
journal = {Inf. Technol. and Management},
month = jan,
pages = {85–112},
numpages = {28},
keywords = {problem management, software engineering, software maintenance, software management tool, web application systems}
}

@article{10.1145/3241744,
author = {Troya, Javier and Segura, Sergio and Parejo, Jose Antonio and Ruiz-Cort\'{e}s, Antonio},
title = {Spectrum-Based Fault Localization in Model Transformations},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3241744},
doi = {10.1145/3241744},
abstract = {Model transformations play a cornerstone role in Model-Driven Engineering (MDE), as they provide the essential mechanisms for manipulating and transforming models. The correctness of software built using MDE techniques greatly relies on the correctness of model transformations. However, it is challenging and error prone to debug them, and the situation gets more critical as the size and complexity of model transformations grow, where manual debugging is no longer possible.Spectrum-Based Fault Localization (SBFL) uses the results of test cases and their corresponding code coverage information to estimate the likelihood of each program component (e.g., statements) of being faulty. In this article we present an approach to apply SBFL for locating the faulty rules in model transformations. We evaluate the feasibility and accuracy of the approach by comparing the effectiveness of 18 different state-of-the-art SBFL techniques at locating faults in model transformations. Evaluation results revealed that the best techniques, namely Kulcynski2, Mountford, Ochiai, and Zoltar, lead the debugger to inspect a maximum of three rules to locate the bug in around 74% of the cases. Furthermore, we compare our approach with a static approach for fault localization in model transformations, observing a clear superiority of the proposed SBFL-based method.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {13},
numpages = {50},
keywords = {Model transformation, debugging, fault localization, spectrum-based, testing}
}

@inproceedings{10.1145/2493432.2493515,
author = {Aumi, Md Tanvir Islam and Gupta, Sidhant and Goel, Mayank and Larson, Eric and Patel, Shwetak},
title = {DopLink: using the doppler effect for multi-device interaction},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493515},
doi = {10.1145/2493432.2493515},
abstract = {Mobile and embedded electronics are pervasive in today's environment. As such, it is necessary to have a natural and intuitive way for users to indicate the intent to connect to these devices from a distance. We present DopLink, an ultrasonic-based device selection approach. It utilizes the already embedded audio hardware in smart devices to determine if a particular device is being pointed at by another device (i.e., the user waves their mobile phone at a target in a pointing motion). We evaluate the accuracy of DopLink in a controlled user study, showing that, within 3 meters, it has an average accuracy of 95% for device selection and 97% for finding relative device position. Finally, we show three applications of DopLink: rapid device pairing, home automation, and multi-display synchronization.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {583–586},
numpages = {4},
keywords = {doppler effect, multi-device interaction, pairing, pointing},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@article{10.1109/TASLP.2016.2584705,
author = {Kortlang, Steffen and Grimm, Giso and Hohmann, Volker and Kollmeier, Birger and Ewert, Stephan D. and Kortlang, Steffen and Grimm, Giso and Hohmann, Volker and Kollmeier, Birger and Ewert, Stephan D. and Grimm, Giso and Ewert, Stephan D. and Hohmann, Volker and Kollmeier, Birger and Kortlang, Steffen},
title = {Auditory Model-Based Dynamic Compression Controlled by Subband Instantaneous Frequency and Speech Presence Probability Estimates},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2584705},
doi = {10.1109/TASLP.2016.2584705},
abstract = {Sensorineural hearing loss typically results in elevated thresholds and steepened loudness growth significantly conditioned by a damage of outer hair cells OHC. In hearing aids, amplification and dynamic compression aim at widening the limited available dynamic range. However, speech perception particularly in complex acoustic scenes often remains difficult. Here, a physiologically motivated, fast acting, model-based dynamic compression algorithm MDC is introduced which aims at restoring the behaviorally estimated basilar membrane input-output BM I/O function in normal-hearing listeners. A system-specific gain prescription rule is suggested, based on the same model BM I/O function and a behavioral estimate of the individual OHC loss. Cochlear off-frequency component suppression is mimicked using an instantaneous frequency IF estimate. Increased loudness as a consequence of widened filters in the impaired system is considered in a further compensation stage. In an extended version, a subband estimate of the speech presence probability MDC+SPP additionally provides speech-selective amplification in stationary noise. Instrumental evaluation revealed that the IF control enhances the spectral contrast of vowels and benefits in quality predictions at higher signal-to-noise ratios SNRs were observed. Compared with a conventional multiband dynamic compressor, MDC achieved objective quality and intelligibility benefits for a competing talker at lower SNRs. MDC+SPP outperformed the conventional compressor in the quality predictions and reached comparable instrumental speech intelligibility as achieved with linear amplification. The proposed algorithm provides a first promising basis for auditory model-based compression with signal-type- and bandwidth-dependent gains.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1759–1772},
numpages = {14}
}

@article{10.1016/j.csl.2015.08.005,
author = {Chen, Yi-Ping Phoebe and Johnson, Caddi and Lalbakhsh, Pooia and Caelli, Terry and Deng, Guang and Tay, David and Erickson, Shane and Broadbridge, Philip and El Refaie, Amr and Doube, Wendy and Morris, Meg E.},
title = {Systematic review of virtual speech therapists for speech disorders},
year = {2016},
issue_date = {May 2016},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {37},
number = {C},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2015.08.005},
doi = {10.1016/j.csl.2015.08.005},
abstract = {A systematic review on virtual speech therapists (VSTs) was presented.The comparison of VSTs and traditional speech therapy was discussed.We analyzed intervention methods used in VSTs such as articulation therapy.Hearing impairments was observed as the most frequent disorder targeted by VSTs.We explored 3D virtual heads and games as efficient therapy delivery approaches. In this paper, a systematic review of relevant published studies on computer-based speech therapy systems or virtual speech therapists (VSTs) for people with speech disorders is presented. We structured this work based on the PRISMA framework. The advancements in speech technology and the increased number of successful real-world projects in this area point to a thriving market for VSTs in the near future; however, there is no standard roadmap to pinpoint how these systems should be designed, implemented, customized, and evaluated with respect to the various speech disorders. The focus of this systematic review is on articulation and phonological impairments. This systematic review addresses three research questions: what types of articulation and phonological disorders do VSTs address, how effective are virtual speech therapists, and what technological elements have been utilized in VST projects. The reviewed papers were sourced from comprehensive digital libraries, and were published in English between 2004 and 2014. All the selected studies involve computer-based intervention in the form of a VST regarding articulation or phonological impairments, followed by qualitative and/or quantitative assessments. To generate this review, we encountered several challenges. Studies were heterogeneous in terms of disorders, type and frequency of therapy, sample size, level of functionality, etc. Thus, overall conclusions were difficult to draw. Commonly, publications with rigorous study designs did not describe the technical elements used in their VST, and publications that did describe technical elements had poor study designs. Despite this heterogeneity, the selected studies reported the effectiveness of computers as a more engaging type of intervention with more tools to enrich the intervention programs, particularly when it comes to children; however, it was emphasized that virtual therapists should not drive the intervention but must be used as a medium to deliver the intervention planned by speech-language pathologists. Based on the reviewed papers, VSTs are significantly effective in training people with a variety of speech disorders; however, it cannot be claimed that a consensus exists in the superiority of VSTs over speech-language pathologists regarding rehabilitation outcomes. Our review shows that hearing-impaired cases were the most frequently addressed disorder in the reviewed studies. Automatic speech recognition, speech corpus, and speech synthesizers were the most popular technologies used in the VSTs.},
journal = {Comput. Speech Lang.},
month = may,
pages = {98–128},
numpages = {31},
keywords = {Computer-based intervention, Computer-based speech therapy, Speech and language disorders, Virtual speech therapist}
}

@inproceedings{10.1007/11893011_138,
author = {Corallo, Angelo and Lorenzo, Gianluca and Solazzo, Gianluca},
title = {A semantic recommender engine enabling an etourism scenario},
year = {2006},
isbn = {3540465421},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11893011_138},
doi = {10.1007/11893011_138},
abstract = {This work approaches the problem of delivering services in a personalized way in an eTourism scenario. Our research, on one side, exploits semantic annotation of either services and user profiles to add a layer of business description that allows the system to supply the most suitable service to the user who requested it. On the other side, this work aims to extent Service Oriented Architecture with the use of semantics and ontologies to enable e-business relations in the tourism applicative domain. This approach is adopted in the MAIS project, in which a Service Oriented Architecture has been developed. Our model relies on semantic description of services, rule-based user profile and the use of semantic matching algorithms.},
booktitle = {Proceedings of the 10th International Conference on Knowledge-Based Intelligent Information and Engineering Systems - Volume Part III},
pages = {1092–1101},
numpages = {10},
location = {Bournemouth, UK},
series = {KES'06}
}

@inproceedings{10.5555/3351736.3351757,
author = {Cuadrado, Jes\'{u}s S\'{a}nchez and Guerra, Esther and de Lara, Juan},
title = {Quick fixing ATL model transformations},
year = {2015},
isbn = {9781467369084},
publisher = {IEEE Press},
abstract = {The correctness of model transformations is key to obtain reliable MDE solutions. However, current transformation tools provide limited support to statically detect and correct errors. This way, the identification of errors and their correction are mostly manual activities. Our aim is to improve this situation.Based on a static analyser for ATL model transformations which we have previously built, we present a method and a system to propose quick fixes for transformation errors. The analyser is based on a combination of program analysis and constraint solving, and our quick fix generation technique makes use of the analyser features to provide a range of fixes, notably some non-trivial, transformation-specific ones. Our approach integrates seamlessly with the ATL editor. We provide an evaluation based on an existing faulty transformation, and automatically generated transformation mutants, showing overall good results.},
booktitle = {Proceedings of the 18th International Conference on Model Driven Engineering Languages and Systems},
pages = {146–155},
numpages = {10},
keywords = {ATL, model transformation, quick fixes, transformation static analysis, verification and testing},
location = {Ottawa, Ontario, Canada},
series = {MODELS '15}
}

@inproceedings{10.1145/2666620.2666623,
author = {Diao, Wenrui and Liu, Xiangyu and Zhou, Zhe and Zhang, Kehuan},
title = {Your Voice Assistant is Mine: How to Abuse Speakers to Steal Information and Control Your Phone},
year = {2014},
isbn = {9781450331555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666620.2666623},
doi = {10.1145/2666620.2666623},
abstract = {Previous research about sensor based attacks on Android platform focused mainly on accessing or controlling over sensitive components, such as camera, microphone and GPS. These approaches obtain data from sensors directly and need corresponding sensor invoking permissions.This paper presents a novel approach (GVS-Attack) to launch permission bypassing attacks from a zero-permission Android application (VoicEmployer) through the phone speaker. The idea of GVS-Attack is to utilize an Android system built-in voice assistant module -- Google Voice Search. With Android Intent mechanism, VoicEmployer can bring Google Voice Search to foreground, and then plays prepared audio files (like "call number 1234 5678") in the background. Google Voice Search can recognize this voice command and perform corresponding operations. With ingenious design, our GVS-Attack can forge SMS/Email, access privacy information, transmit sensitive data and achieve remote control without any permission. Moreover, we found a vulnerability of status checking in Google Search app, which can be utilized by GVS-Attack to dial arbitrary numbers even when the phone is securely locked with password.A prototype of VoicEmployer has been implemented to demonstrate the feasibility of GVS-Attack. In theory, nearly all Android (4.1+) devices equipped with Google Services Framework can be affected by GVS-Attack. This study may inspire application developers and researchers to rethink that zero permission doesn't mean safety and the speaker can be treated as a new attack surface.},
booktitle = {Proceedings of the 4th ACM Workshop on Security and Privacy in Smartphones &amp; Mobile Devices},
pages = {63–74},
numpages = {12},
keywords = {android security, permission bypassing, speaker, voice assistant, zero permission attack},
location = {Scottsdale, Arizona, USA},
series = {SPSM '14}
}

@inbook{10.1145/3191315.3191317,
author = {Maier, David and Tekle, K. Tuncay and Kifer, Michael and Warren, David S.},
title = {Datalog: concepts, history, and outlook},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191317},
abstract = {This chapter is a survey of the history and the main concepts of Datalog.We begin with an introduction to the language and its use for database definition and querying. We then look back at the threads from logic languages, databases, artificial intelligence, and expert systems that led to the emergence of Datalog and reminiscence about the origin of the name. We consider the interaction of recursion with other common data language features, such as negation and aggregation, and look at other extensions, such as constraints, updates, and object-oriented features.We provide an overview of the main approaches to Datalog evaluation and their variants, then recount some early implementations of Datalog and of similar deductive database systems.We speculate on the reasons for the decline in the interest in the language in the 1990s and the causes for its later resurgence in a number of application areas.We conclude with several examples of current systems based on or supporting Datalog and briefly examine the performance of some of them.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {3–100},
numpages = {98}
}

@inproceedings{10.1007/978-3-319-11200-8_16,
author = {Klemke, Roland and Ternier, Stefaan and Kalz, Marco and Schmitz, Birgit and Specht, Marcus},
title = {Immersive Multi-user Decision Training Games with ARLearn},
year = {2014},
isbn = {9783319111995},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-11200-8_16},
doi = {10.1007/978-3-319-11200-8_16},
abstract = {Serious gaming approaches so far focus mainly on skill development, motivational aspects or providing immersive learning situations. Little work has been reported to foster awareness and decision competencies in complex decision situations involving incomplete information and multiple stakeholders. We address this issue exploring the technical requirements and possibilities to design games for such situations in three case studies: a hostage taking situation, a multi-stakeholder logistics case, and a health-care related emergency case. To implement the games, we use a multi-user enabled mobile game development platform (ARLearn). We describe the underlying real world situations and educational challenges and analyse how these are reflected in the ARLearn games realized. Based on these cases we propose a way to increase the immersiveness of mobile learning games.},
booktitle = {Proceedings of the 9th European Conference on Open Learning and Teaching in Educational Communities - Volume 8719},
pages = {207–220},
numpages = {14},
keywords = {decision processes, game-based learning, immersiveness, mobile learning, multi-role game-design, multi-user games},
location = {Graz, Austria},
series = {EC-TEL 2014}
}

@article{10.1016/j.eswa.2012.03.061,
author = {Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S. and Garc\'{\i}a-Torres, M.},
title = {Fast feature selection aimed at high-dimensional data via hybrid-sequential-ranked searches},
year = {2012},
issue_date = {September, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {12},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.03.061},
doi = {10.1016/j.eswa.2012.03.061},
abstract = {We address the feature subset selection problem for classification tasks. We examine the performance of two hybrid strategies that directly search on a ranked list of features and compare them with two widely used algorithms, the fast correlation based filter (FCBF) and sequential forward selection (SFS). The proposed hybrid approaches provide the possibility of efficiently applying any subset evaluator, with a wrapper model included, to large and high-dimensional domains. The experiments performed show that our two strategies are competitive and can select a small subset of features without degrading the classification error or the advantages of the strategies under study.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {11094–11102},
numpages = {9},
keywords = {Classification, Data mining, Feature ranking, Feature selection}
}

@article{10.1016/j.cosrev.2015.05.001,
author = {Hosseini, Mahmood and Shahri, Alimohammad and Phalp, Keith and Taylor, Jacqui and Ali, Raian},
title = {Crowdsourcing},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {17},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2015.05.001},
doi = {10.1016/j.cosrev.2015.05.001},
abstract = {Context:Crowdsourcing, or tapping into the power of the crowd for problem solving, has gained ever-increasing attraction since it was first introduced. Crowdsourcing has been used in different disciplines, and it is becoming well-accepted in the marketplace as a new business model which utilizes Human Intelligence Tasks (HITs).Objective:While both academia and industry have extensively delved into different aspects of crowdsourcing, there seems to be no common understanding of what crowdsourcing really means and what core and optional features it has. Also, we still lack information on the kinds and disciplines of studies conducted on crowdsourcing and how they defined it in the context of their application area. This paper will clarify this ambiguity by analysing the distribution and demographics of research in crowdsourcing and extracting taxonomy of the variability and commonality in the constructs defining the concept in the literature.Method:We conduct a systematic mapping study and analyse 113 papers, selected via a formal process, and report and discuss the results. The study is combined by a content analysis process to extract a taxonomy of features describing crowdsourcing.Results:We extract and describe the taxonomy of features which characterize crowdsourcing in its four constituents; the crowd, the crowdsourcer, the crowdsourced task and the crowdsourcing platform. In addition, we report on different mappings between these features and the characteristics of the studied papers. We also analyse the distribution of the research using multiple criteria and draw conclusions. For example, our results show a constantly increasing interest in the area, especially in North America and a significant interest from industry. Also, we illustrate that although crowdsourcing is shown to be useful in a variety of disciplines, the research in the field of computer science still seems to be dominant in investigating it.Conclusions:This study allows forming a clear picture of the research in crowdsourcing and understanding the different features of crowdsourcing and their popularity, what type of research was conducted, where and how and by whom. The study enables researchers and practitioners to estimate the current status of the research in this new field. Our taxonomy of extracted features provides a reference model which could be used to configure crowdsourcing and also define it precisely and make design decisions on which of its variation to adopt.},
journal = {Comput. Sci. Rev.},
month = aug,
pages = {43–69},
numpages = {27},
keywords = {Crowdsourcing, Crowdsourcing features, Systematic mapping, Taxonomy}
}

@article{10.1016/j.comnet.2011.09.014,
author = {Han, Weili and Lei, Chang},
title = {Survey Paper: A survey on policy languages in network and security management},
year = {2012},
issue_date = {January, 2012},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {56},
number = {1},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2011.09.014},
doi = {10.1016/j.comnet.2011.09.014},
abstract = {Policy-driven management is gaining popularity today, mainly due to the ever-growing scale and complexity of large networked systems. In these systems, policies are usually used to simplify the tasks of the system management, thus making way for further system enlargement. Accordingly, a variety of policy languages are proposed to express intentions of administrators in the policy-driven systems, especially for the network and security management. This paper, therefore, investigates current works, discusses the key issues, and then outlines the future work of policy languages.},
journal = {Comput. Netw.},
month = jan,
pages = {477–489},
numpages = {13},
keywords = {Network management, Policy language, Policy-driven management, Security management}
}

@article{10.1016/j.jss.2014.12.050,
author = {Kapitsaki, Georgia M. and Tselikas, Nikolaos D. and Foukarakis, Ioannis E.},
title = {An insight into license tools for open source software systems},
year = {2015},
issue_date = {April 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {102},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.12.050},
doi = {10.1016/j.jss.2014.12.050},
abstract = {We provide a licensing comparative overview of existing assistive approaches/tools.License identification tools present a more mature current state.License compliance requires the collaboration of various techniques.License graphs are useful in detecting license conflicts. Free/Libre/Open Source Software (FLOSS) has gained a lot of attention lately allowing organizations to incorporate third party source code into their implementations. When open source software libraries are used, software resources may be linked directly or indirectly with multiple open source licenses giving rise to potential license incompatibilities. Adequate support in license use is vital in order to avoid such violations and address how diverse licenses should be handled. In the current work we investigate software licensing giving a critical and comparative overview of existing assistive approaches and tools. These approaches are centered on three main categories: license information identification from source code and binaries, software metadata stored in code repositories, and license modeling and associated reasoning actions. We also give a formalization of the license compatibility problem and demonstrate the role of existing approaches in license use decisions.},
journal = {J. Syst. Softw.},
month = apr,
pages = {72–87},
numpages = {16},
keywords = {Free/Libre/Open Source Software, License compatibility, License identification}
}

@article{10.1007/s10772-017-9429-x,
author = {Phu, Vo Ngoc and Tran, Vo Thi and Chau, Vo Thi and Dat, Nguyen Duy and Duy, Khanh Ly},
title = {A decision tree using ID3 algorithm for English semantic analysis},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-017-9429-x},
doi = {10.1007/s10772-017-9429-x},
abstract = {Natural language processing has been studied for many years, and it has been applied to many researches and commercial applications. A new model is proposed in this paper, and is used in the English document-level emotional classification. In this survey, we proposed a new model by using an ID3 algorithm of a decision tree to classify semantics (positive, negative, and neutral) for the English documents. The semantic classification of our model is based on many rules which are generated by applying the ID3 algorithm to 115,000 English sentences of our English training data set. We test our new model on the English testing data set including 25,000 English documents, and achieve 63.6% accuracy of sentiment classification results.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {593–613},
numpages = {21},
keywords = {Decision tree, English document opinion mining, English sentiment classification, ID3 algorithm, Sentiment classification, id3}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@inproceedings{10.5555/2041845.2041882,
author = {Indyka-Piasecka, Agnieszka},
title = {Using multi-attribute structures and significance term evaluation for user profile adaptation},
year = {2011},
isbn = {9783642239342},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This contribution presents a new approach to the representation of user's interests and preferences. The adaptive user profile includes both interests given explicitly by the user, as a query, and also preferences expressed by the valuation of relevance of retrieved documents, so to express field independent translation between terminology used by user and terminology accepted in some field of knowledge. Procedures for building, modifying and using the profile, heuristic-based significant terms selection from relevant documents are presented. Experiments concerning the profile, as a personalization mechanism of Web search system, are presented and discussed.},
booktitle = {Proceedings of the Third International Conference on Computational Collective Intelligence: Technologies and Applications - Volume Part I},
pages = {336–345},
numpages = {10},
location = {Gdynia, Poland},
series = {ICCCI'11}
}

@article{10.1016/j.specom.2011.04.002,
author = {Ghio, A. and Pouchoulin, G. and Teston, B. and Pinto, S. and Fredouille, C. and De Looze, C. and Robert, D. and Viallet, F. and Giovanni, A.},
title = {How to manage sound, physiological and clinical data of 2500 dysphonic and dysarthric speakers?},
year = {2012},
issue_date = {June, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {5},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2011.04.002},
doi = {10.1016/j.specom.2011.04.002},
abstract = {The aim of this contribution is to propose a database model designed for the storage and accessibility of various speech disorder data including signals, clinical evaluations and patients' information. This model is the result of 15 years of experience in the management and the analysis of this type of data. We present two important French corpora of voice and speech disorders that we have been recording in hospitals in Marseilles (MTO corpus) and Aix-en-Provence (AHN corpus). The population consists of 2500 dysphonic, dysarthric and control subjects, a number of speakers which, as far as we know, constitutes currently one of the largest corpora of ''pathological'' speech. The originality of this data lies in the presence of physiological data (such as oral airflow or estimated sub-glottal pressure) associated with acoustic recordings. This activity led us to raise the question of how we can manage the sound, physiological and clinical data of such a large quantity of data. Consequently, we developed a database model that we present here. Recommendations and technical solutions based on MySQL, a relational database management system, are discussed.},
journal = {Speech Commun.},
month = jun,
pages = {664–679},
numpages = {16},
keywords = {Clinical phonetics, Database, Dysarthria, Dysphonia, Voice/speech disorders}
}

@inproceedings{10.1145/3299869.3314040,
author = {Begoli, Edmon and Akidau, Tyler and Hueske, Fabian and Hyde, Julian and Knight, Kathryn and Knowles, Kenneth},
title = {One SQL to Rule Them All - an Efficient and Syntactically Idiomatic Approach to Management of Streams and Tables},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3314040},
doi = {10.1145/3299869.3314040},
abstract = {Real-time data analysis and management are increasingly critical for today's businesses. SQL is the de facto lingua franca for these endeavors, yet support for robust streaming analysis and management with SQL remains limited. Many approaches restrict semantics to a reduced subset of features and/or require a suite of non-standard constructs. Additionally, use of event timestamps to provide native support for analyzing events according to when they actually occurred is not pervasive, and often comes with important limitations. We present a three-part proposal for integrating robust streaming into SQL, namely: (1) time-varying relations as a foundation for classical tables as well as streaming data, (2) event time semantics, (3) a limited set of optional keyword extensions to control the materialization of time-varying query results. We show how with these minimal additions it is possible to utilize the complete suite of standard SQL semantics to perform robust stream processing. We motivate and illustrate these concepts using examples and describe lessons learned from implementations in Apache Calcite, Apache Flink, and Apache Beam. We conclude with syntax and semantics of a concrete proposal for extensions of the SQL standard and note further areas of exploration.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1757–1772},
numpages = {16},
keywords = {data management, query processing, stream processing},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@article{10.5555/1377776.1377779,
author = {De Meo, Pasquale and Quattrone, Giovanni and Terracina, Giorgio and Ursino, Domenico},
title = {Utilization of intelligent agents for supporting citizens in their access to e-government services},
year = {2007},
issue_date = {August 2007},
publisher = {IOS Press},
address = {NLD},
volume = {5},
number = {3},
issn = {1570-1263},
abstract = {This paper aims at studying the utilization of Intelligent Agents for supporting citizens to access e-government services. For this purpose, it proposes a multi-agent system capable of suggesting to the citizens the most interesting services for them; these suggestions are determined by considering both their needs/preferences and the capabilities of the devices used by them. The paper first describes the proposed system and, then, reports various experimental results. Finally, it presents a comparison between the proposed system and other related ones already presented in the literature.},
journal = {Web Intelli. and Agent Sys.},
month = aug,
pages = {273–310},
numpages = {38},
keywords = {Multi-agent systems, device adaptivity, e-government, recommender systems, user profiling}
}

@article{10.1145/3301299,
author = {Constantin, Mihai Gabriel and Redi, Miriam and Zen, Gloria and Ionescu, Bogdan},
title = {Computational Understanding of Visual Interestingness Beyond Semantics: Literature Survey and Analysis of Covariates},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3301299},
doi = {10.1145/3301299},
abstract = {Understanding visual interestingness is a challenging task addressed by researchers in various disciplines ranging from humanities and psychology to, more recently, computer vision and multimedia. The rise of infographics and the visual information overload that we are facing today have given this task a crucial importance. Automatic systems are increasingly needed to help users navigate through the growing amount of visual information available, either on the web or our personal devices, for instance by selecting relevant and interesting content. Previous studies indicate that visual interest is highly related to concepts like arousal, unusualness, or complexity, where these connections are found based on psychological theories, user studies, or computational approaches. However, the link between visual interestingness and other related concepts has been only partially explored so far, for example, by considering only a limited subset of covariates at a time. In this article, we present a comprehensive survey on visual interestingness and related concepts, aiming to bring together works based on different approaches, highlighting controversies, and identifying links that have not been fully investigated yet. Finally, we present some open questions that may be addressed in future works. Our work aims to support researchers interested in visual interestingness and related subjective or abstract concepts, providing an in-depth overlook at state-of-the-art theories in humanities and methods in computational approaches, as well as providing an extended list of datasets.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {25},
numpages = {37},
keywords = {Interestingness, aesthetic value, affective value and emotions, complexity, coping potential, creativity, humour, memorability, novelty, saliency, social interestingness, urban perception, visual composition and stylistic attributes}
}

@inbook{10.1145/3191315.3191322,
author = {Borraz-S\'{a}nchez, Conrado and Klabjan, Diego and Pasalic, Emir and Aref, Molham},
title = {SolverBlox: algebraic modeling in datalog},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191322},
abstract = {Datalog is a deductive query language for relational databases. We introduce LogiQL, a language based on Datalog and show how it can be used to specify mixedinteger linear optimization models and solve them. Unlike pure algebraic modeling languages, LogiQL allows the user to both specify models, and manipulate and transform the inputs and outputs of the models. This is an advantage over conventional optimization modeling languages that rely on reading data via plug-in tools or importing data from external sources via files. In this chapter, we give a brief overview of LogiQL and describe two mixed integer programming case studies: a production-transportation model and a formulation of the traveling salesman problem.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {331–354},
numpages = {24}
}

@inbook{10.1145/3191315.3191326,
author = {Liu, Yanhong A.},
title = {Logic programming applications: what are the abstractions and implementations?},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191326},
abstract = {This chapter presents an overview of applications of logic programming, classifying them based on the abstractions and implementations of logic languages that support the applications. The three key abstractions are join, recursion, and constraint. Their essential implementations are for-loops, fixed points, and backtracking, respectively. The corresponding kinds of applications are database queries, inductive analysis, and combinatorial search, respectively. We also discuss language extensions and programming paradigms, summarize example application problems by application areas, and touch on example systems that support variants of the abstractions with different implementations.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {519–548},
numpages = {30}
}

@article{10.1016/j.jnca.2017.12.001,
author = {Dias de Assuno, Marcos and da Silva Veith, Alexandre and Buyya, Rajkumar},
title = {Distributed data stream processing and edge computing},
year = {2018},
issue_date = {February 2018},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {103},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2017.12.001},
doi = {10.1016/j.jnca.2017.12.001},
abstract = {Under several emerging application scenarios, such as in smart cities, operational monitoring of large infrastructure, wearable assistance, and Internet of Things, continuous data streams must be processed under very short delays. Several solutions, including multiple software engines, have been developed for processing unbounded data streams in a scalable and efficient manner. More recently, architecture has been proposed to use edge computing for data stream processing. This paper surveys state of the art on stream processing engines and mechanisms for exploiting resource elasticity features of cloud computing in stream processing. Resource elasticity allows for an application or service to scale out/in according to fluctuating demands. Although such features have been extensively investigated for enterprise applications, stream processing poses challenges on achieving elastic systems that can make efficient resource management decisions based on current load. Elasticity becomes even more challenging in highly distributed environments comprising edge and cloud computing resources. This work examines some of these challenges and discusses solutions proposed in the literature to address them. HighlightsThe paper surveys state of the art on stream processing engines and mechanisms.The work describes how existing solutions exploit resource elasticity features of cloud computing in stream processing.It presents a gap analysis and future directions on stream processing on heterogeneous environments.},
journal = {J. Netw. Comput. Appl.},
month = feb,
pages = {1–17},
numpages = {17},
keywords = {Big Data, Cloud computing, Resource elasticity, Stream processing}
}

@article{10.1007/s00165-018-0461-7,
author = {Schneider, David and Leuschel, Michael and Witt, Tobias},
title = {Model-based problem solving for university timetable validation and improvement},
year = {2018},
issue_date = {Sep 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {5},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-018-0461-7},
doi = {10.1007/s00165-018-0461-7},
abstract = {Constraint satisfaction problems can be expressed very elegantly in state-based formal methods such as B. But can such specifications be directly used for solving real-life problems? In other words, can a formal model be more than a design artefact but also be used at runtime for inference and problem solving? We will try and answer this important question in the present paper with regard to the university timetabling problem. We report on an ongoing project to build a curriculum timetable validation tool where we use a formal model as the basis to validate timetables from a student’s perspective and to support incremental modification of timetables. In this article we describe the problem domain, the formalization in B and our approach to execute the formal model in a production system using ProB.},
journal = {Form. Asp. Comput.},
month = sep,
pages = {545–569},
numpages = {25},
keywords = {B-method, Constraint programming, Timetabling, Scheduling}
}

@inproceedings{10.1007/978-3-642-41924-9_5,
author = {Siena, Alberto and Ingolfo, Silvia and Perini, Anna and Susi, Angelo and Mylopoulos, John},
title = {Automated Reasoning for Regulatory Compliance},
year = {2013},
isbn = {9783642419232},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41924-9_5},
doi = {10.1007/978-3-642-41924-9_5},
abstract = {Regulatory compliance is gaining attention from information systems engineers who must design systems that at the same time satisfy stakeholder requirements and comply with applicable laws. In our previous work, we have introduced a conceptual modelling language called N\`{o}mos 2 that aids requirements engineers analyze law to identify alternative ways for compliance. This paper presents an implemented reasoning tool that supports analysis of law models. The technical contributions of the paper include the formalization of reasoning mechanisms, their implementation in the NRTool, as well as an elaborated evaluation framework intended to determine whether the tool is scalable with respect to problem size, complexity as well as search space. The results of our experiments with the tool suggest that this conceptual modelling approach scales to real life regulatory compliance problems.},
booktitle = {Proceedings of the 32nd International Conference on Conceptual Modeling - Volume 8217},
pages = {47–60},
numpages = {14},
keywords = {Automated Reasoning, Conceptual Modeling, Experimental Evaluation, Regulatory Compliance},
location = {Hong-Kong, China},
series = {ER 2013}
}

@article{10.1017/S0269888904000190,
author = {Lacave, Carmen and Diez, Francisco J.},
title = {A review of explanation methods for heuristic expert systems},
year = {2004},
issue_date = {June 2004},
publisher = {Cambridge University Press},
address = {USA},
volume = {19},
number = {2},
issn = {0269-8889},
url = {https://doi.org/10.1017/S0269888904000190},
doi = {10.1017/S0269888904000190},
abstract = {Explanation of reasoning is one of the most important abilities an expert system should provide in order to be widely accepted. In fact, since MYCIN, many expert systems have tried to include some explanation capability. This paper reviews the methods developed to date for explanation in heuristic expert systems.},
journal = {Knowl. Eng. Rev.},
month = jun,
pages = {133–146},
numpages = {14}
}

@inproceedings{10.1007/978-3-642-41248-6_5,
author = {Hu, Ruimin and Dong, Shi and Wang, Heng and Zhang, Maosheng and Wang, Song and Li, Dengshi},
title = {Perceptual Characteristic and Compression Research in 3D Audio Technology},
year = {2012},
isbn = {9783642412479},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41248-6_5},
doi = {10.1007/978-3-642-41248-6_5},
abstract = {The 3D audio coding forms a competitive research area due to the standardization of both international standards i.e. MPEG and localized standards i.e. Audio and Video Coding Standard workgroup of China, AVS. Perception of 3D audio is a key issue for standardization and remains a challenging problem. Besides current solutions adopted from traditional audio engineering, we are working for an original 3D audio solution for compression. This paper represents our initial results about 3D audio perception include directional measurement of Just Noticeable Difference JND and Perceptual Entropy PE. We also represent the possible applications of these results in our future researches.},
booktitle = {Revised Selected Papers of the 9th International Symposium on From Sounds to Music and Emotions - Volume 7900},
pages = {82–98},
numpages = {17},
keywords = {3D audio, audio compression, perceptual audio processing},
location = {London, UK},
series = {CMMR 2012}
}

@inbook{10.1145/3191315.3191325,
author = {Christiansen, Henning and Dahl, Ver\'{o}nica},
title = {Natural language processing with (tabled and constraint) logic programming},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191325},
abstract = {We survey the evolution of natural language processing as it relates to Logic Programming, with particular focus on David Scott Warren's crucial contributions such as tabling, and the relationship with hypothetical reasoning and constraint based programming. These topics lead naturally to a view of parsing as constraint solving, which extends to grammar inference. Our exposition of the subject is intuitive and example-driven, with references to more formal presentations when needed.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {477–511},
numpages = {35}
}

@inproceedings{10.1145/1141277.1141350,
author = {De Meo, Pasquale and Quattrone, Giovanni and Fadil, Hind and Ursino, Domenico},
title = {A multi-agent system for efficiently managing query answering in an e-government scenario},
year = {2006},
isbn = {1595931082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1141277.1141350},
doi = {10.1145/1141277.1141350},
abstract = {This paper aims at studying the exploitation of the Intelligent Agent technology for supporting citizens in their access to e-government services. It appears particularly suited in the present e-government scenario characterized by a huge amount of heterogeneous data and services, delivered by government agencies, that makes difficult to quickly answer citizen queries. In this paper we show that the exploitation of the Intelligent Agent technology facilitates the search of information on government data sources in such a way to completely and precisely satisfy citizen queries. Our system creates and maintains suitable profiles of involved users, representing their preferences and exigencies; in addition, it adopts suitable algorithms that exploit information stored in citizen profiles for producing recommendations.},
booktitle = {Proceedings of the 2006 ACM Symposium on Applied Computing},
pages = {308–312},
numpages = {5},
keywords = {e-government, multi-agent systems, user profiling},
location = {Dijon, France},
series = {SAC '06}
}

@book{10.1145/3191315,
editor = {Kifer, Michael and Liu, Yanhong Annie},
title = {Declarative Logic Programming: Theory, Systems, and Applications},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {20},
abstract = {Logic Programming (LP) is at the nexus of knowledge representation, AI, mathematical logic, databases, and programming languages. It allows programming to be more declarative, by specifying “what” to do instead of “how” to do it. This field is fascinating and intellectually stimulating due to the fundamental interplay among theory, systems, and applications brought about by logic. Several books cover the basics of LP but they focus mostly on the Prolog language. There is generally a lack of accessible collections of articles covering the key aspects of LP, such as the well-founded vs. stable semantics for negation, constraints, object-oriented LP, updates, probabilistic LP, and implementation methods, including top-down vs. bottom-up evaluation and tabling.For systems, the situation is even less satisfactory, lacking expositions of LP inference machinery that supports tabling and other state-of-the-art implementation techniques. There is also a dearth of articles about systems that support truly declarative languages, especially those that tie into first-order logic, mathematical programming, and constraint programming. Also rare are surveys of challenging application areas of LP, such as bioinformatics, natural language processing, verification, and planning, as well as analysis of LP applications based on language abstractions and implementations methods.The goal of this book is to help fill in the void in the literature with state-of-the-art surveys on key aspects of LP. Much attention was paid to making these surveys accessible to researchers, practitioners, and graduate students alike.}
}

@inproceedings{10.1145/264107.264129,
author = {Stunkel, Craig B. and Sivaram, Rajeev and Panda, Dhabaleswar K.},
title = {Implementing multidestination worms in switch-based parallel systems: architectural alternatives and their impact},
year = {1997},
isbn = {0897919017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/264107.264129},
doi = {10.1145/264107.264129},
abstract = {Multidestination message passing has been proposed as an attractive mechanism for efficiently implementing multicast and other collective operations on direct networks. However, applying this mechanism to switch-based parallel systems is non-trivial. In this paper we propose alternative switch architectures with differing buffer organizations to implement multidestination worms on switch-based parallel systems. First, we discuss issues related to such implementation (deadlock-freedom, replication mechanisms, header encoding, and routing). Next, we demonstrate how an existing central-buffer-based switch architecture supporting unicast message passing can be enhanced to accommodate multidestination message passing. Similarly, implementing multidestination worms on an input-buffer-based switch architecture is discussed. Both of these implementations are evaluated against each other as well as against a software-based scheme using the central buffer organization. Simulation experiments under a range of traffic (multiple multicast, bimodal, varying degree of multicast, and message length) and system size are used for evaluation. The study demonstrates the superiority of the central-buffer-based switch architecture. It also indicates that under bimodal traffic the central-buffer-based hardware multicast implementation affects background unicast traffic less adversely compared to a software-based multicast implementation. Thus, multidestination message passing can easily be applied to switch-based parallel systems to deliver good collective communication performance.},
booktitle = {Proceedings of the 24th Annual International Symposium on Computer Architecture},
pages = {50–61},
numpages = {12},
location = {Denver, Colorado, USA},
series = {ISCA '97}
}

@article{10.1016/j.neucom.2016.09.070,
author = {Tareef, Afaf and Song, Yang and Cai, Weidong and Huang, Heng and Chang, Hang and Wang, Yue and Fulham, Michael and Feng, Dagan and Chen, Mei},
title = {Automatic segmentation of overlapping cervical smear cells based on local distinctive features and guided shape deformation},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {221},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.09.070},
doi = {10.1016/j.neucom.2016.09.070},
abstract = {Automated segmentation of cells from cervical smears poses great challenge to biomedical image analysis because of the noisy and complex background, poor cytoplasmic contrast and the presence of fuzzy and overlapping cells. In this paper, we propose an automated segmentation method for the nucleus and cytoplasm in a cluster of cervical cells based on distinctive local features and guided sparse shape deformation. Our proposed approach is performed in two stages: segmentation of nuclei and cellular clusters, and segmentation of overlapping cytoplasm. In the first stage, a set of local discriminative shape and appearance cues of image superpixels is incorporated and classified by the Support Vector Machine (SVM) to segment the image into nuclei, cellular clusters, and background. In the second stage, a robust shape deformation framework is proposed, based on Sparse Coding (SC) theory and guided by representative shape features, to construct the cytoplasmic shape of each overlapping cell. Then, the obtained shape is refined by the Distance Regularized Level Set Evolution (DRLSE) model. We evaluated our approach using the ISBI 2014 challenge dataset, which has 135 synthetic cell images for a total of 810 cells. Our results show that our approach outperformed existing approaches in segmenting overlapping cells and obtaining accurate nuclear boundaries. HighlightsA fully automated segmentation method is proposed for overlapping cervical cells.Our approach is based on superpixel-based features and guided shape deformation.Our shape initialization procedure is able to work with the different cell types.The practicality of our approach in segmenting highly overlapping cells is proved.Our approach outperformed existing approaches in nuclei and cytoplasm segmentation.},
journal = {Neurocomput.},
month = jan,
pages = {94–107},
numpages = {14},
keywords = {Distance regularized level set, Feature extraction, Overlapping cervical smear cells, Shape deformation, Sparse coding}
}

@inproceedings{10.1145/1866307.1866319,
author = {Halevi, Tzipora and Saxena, Nitesh},
title = {On pairing constrained wireless devices based on secrecy of auxiliary channels: the case of acoustic eavesdropping},
year = {2010},
isbn = {9781450302456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866307.1866319},
doi = {10.1145/1866307.1866319},
abstract = {Secure "pairing" of wireless devices based on auxiliary or out-of-band (OOB) - audio, visual or tactile - communication is a well-established research direction. Lack of good quality interfaces on or physical access to certain constrained devices (e.g., headsets, access points, medical implants) makes pairing a challenging problem in practice. Prior work shows that pairing of constrained devices based on authenticated OOB (A-OOB) channels can be prone to human errors that eventually translate into man-in-the-middle attacks. An alternative and more usable solution is to use OOB channel(s) that are authenticated as well as secret (AS-OOB). AS-OOB pairing can be achieved by simply transmitting the key or a short password over the AS-OOB channel, avoiding potential serious human errors.A higher level goal of this paper is to analyze the security of AS-OOB pairing. More specifically, we take a closer look at three notable prior AS-OOB pairing proposals and challenge the direct or indirect assumption upon which the security of these proposals relies, i.e., the secrecy of underlying or associated audio channels. The first proposal (IMD Pairing [9]) uses a low frequency audio channel to pair an implanted RFID tag with an external reader. The second proposal (PIN-Vibra [20]) uses an automated vibrational channel to pair a mobile phone with a personal RFID tag. The third proposal (BEDA [22]) uses vibration (or blinking) on one device and manually synchronized button pressing on the other device.In particular, we demonstrate the feasibility of eavesdropping over acoustic emanations associated with these methods. Based on our results, we conclude that these methods provide a weaker level of security compared to what was originally assumed or is desired for the pairing operation.},
booktitle = {Proceedings of the 17th ACM Conference on Computer and Communications Security},
pages = {97–108},
numpages = {12},
keywords = {audio emanations, authentication, device pairing, signal processing},
location = {Chicago, Illinois, USA},
series = {CCS '10}
}

@article{10.1016/j.is.2017.04.001,
title = {Extending the framework for mobile health information systems Research},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {69},
number = {C},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2017.04.001},
doi = {10.1016/j.is.2017.04.001},
abstract = {This paper describes findings of a comprehensive content analysis on the current state of M-health research for IS researchers and professionals.The paper identifies eight application categories (themes), ten design issues as well as the stakeholders and development techniques involved.It is anticipated that the findings that reinforce use of design science research, theoretically motivate the central role of M-health IS design. Whilst researchers and professionals recognise that mobile health (M-health) systems offer unprecedented opportunities, most existing work has comprised individual project-based developments in specialised areas. Existing review articles generally utilise medical literature and categories: none investigates M-health from an information systems (IS) design point of view. Identifying application areas, design issues and IS research techniques will demonstrate models, issues, approaches and gaps to inform future research. A comprehensive analysis of the literature from this viewpoint is thus valuable, both for theoretical progression and for guiding real-world innovative system developments.Drawing from key IS and healthcare multidisciplinary journals we analyse recent (20102016) articles concerning M-health application developments and their associated design or development issues, with particular focus on the use of contemporary research methods. Our analysis suggests that M-health is an emerging field to which, although underused, contemporary approaches such as design science research are particularly appropriate. We identify eight application categories, eleven design issues (security, privacy, literacy, accessibility, acceptability, reliability, usability, confidentiality, integrity, knowledge sharing and flexibility) as well as the stakeholders and development techniques involved. This goes beyond previous frameworks, and theoretically integrates the central role of IS design within the sub-field.},
journal = {Inf. Syst.},
month = sep,
pages = {1–24},
numpages = {24}
}

@inproceedings{10.1007/978-3-540-31797-5_15,
author = {Zhu, Qiang and Nakata, Tsuneo and Mine, Masataka and Kuroki, Kenichiro and Endo, Yoichi and Hasegawa, Takashi},
title = {System-on-chip verification process using UML},
year = {2004},
isbn = {3540250816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-31797-5_15},
doi = {10.1007/978-3-540-31797-5_15},
abstract = {In this paper, we propose a verification methodology for System-On-Chip (SoC) design using Unified Modeling Language (UML). We introduce UML as a formal model to analyze and formalize the specification. The specification and implementation validation can be performed systematically by introducing UML. We applied our method to a Mobile Media Processors SoC. We improved the quality of ζ the specification written in informal natural language through UML modeling techniques. The test scenarios and coverage metrics for implementation are derived from the UML model systematically. The result shows that our proposal is effective for eliminating errors from both specification and implementation.},
booktitle = {Proceedings of the 2004 International Conference on UML Modeling Languages and Applications},
pages = {138–149},
numpages = {12},
location = {Lisbon, Portugal},
series = {UML'04}
}

@inbook{10.5555/2206963.2206980,
author = {Zhu, Qiang and Nakata, Tsuneo and Mine, Masataka and Kuroki, Kenichiro and Endo, Yoichi and Hasegawa, Takashi},
title = {System-on-chip verification process using UML},
year = {2004},
isbn = {3540250816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we propose a verification methodology for System-On-Chip (SoC) design using Unified Modeling Language (UML). We introduce UML as a formal model to analyze and formalize the specification. The specification and implementation validation can be performed systematically by introducing UML. We applied our method to a Mobile Media Processors SoC. We improved the quality of ζ the specification written in informal natural language through UML modeling techniques. The test scenarios and coverage metrics for implementation are derived from the UML model systematically. The result shows that our proposal is effective for eliminating errors from both specification and implementation.},
booktitle = {UML Modeling Languages and Applications},
pages = {138–149},
numpages = {12}
}

@inproceedings{10.1145/2493432.2493480,
author = {Lee, Sang-Su and Chae, Jeonghun and Kim, Hyunjeong and Lim, Youn-kyung and Lee, Kun-pyo},
title = {Towards more natural digital content manipulation via user freehand gestural interaction in a living room},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493480},
doi = {10.1145/2493432.2493480},
abstract = {Advances in dynamic gesture recognition technologies now make it possible to investigate freehand input techniques. This study tried to understand how users manipulate digital content on a distant screen by hand gesture interaction in a living room environment. While there have been many existing studies that investigate freehand input techniques, we developed and applied a novel study methodology based on a combination of both an existing user elicitation study and conventional Wizard-of-Oz study that involved another non-technical user for providing feedback. Through the study, many useful issues and implications for making freehand gesture interaction design more natural in a living room environment were generated which have not been covered in previous works. Furthermore, we could observe how the initial user-defined gestures are changed over time.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {617–626},
numpages = {10},
keywords = {design method., gesture interaction, interaction design},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inbook{10.1145/3191315.3191320,
author = {Warren, David S.},
title = {WAM for everyone: a virtual machine for logic programming},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191320},
abstract = {This chapter is a tutorial presentation of the Warren Abstract Machine (akaWAM), which is a virtual machine for Prolog designed by David H. D.Warren [Warren 1983] (note not David S. Warren, who is the author of this chapter.) We present a variant of the WAM by developing it incrementally through a series of examples. Its similarity to a traditional implementation of a procedural language is emphasized. The development starts with a description of the run-time environment that supports Prolog program execution. Then it is shown how deterministic Datalog programs compile to WAM instructions, followed by deterministic Prolog programs that include complex terms, and finally nondeterministic Prolog programs that require backtracking.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {237–277},
numpages = {41}
}

@article{10.1007/s10270-011-0212-1,
author = {Gr\O{}nmo, Roy and Runde, Ragnhild Kobro and M\O{}ller-Pedersen, Birger},
title = {Confluence of aspects for sequence diagrams},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-011-0212-1},
doi = {10.1007/s10270-011-0212-1},
abstract = {The last decade has seen several aspect language proposals for UML 2 sequence diagrams. Aspects allow the modeler to define crosscutting concerns of sequence diagrams and to have these woven with the sequence diagrams of a so-called base model, in order to create a woven model. In a real-world scenario, there may be multiple aspects applicable to the same base model. This raises the need to analyse the set of aspects to identify possible aspect interactions (dependencies and conflicts) between applications of aspects. We call a set of aspects terminating if they may not be applied infinitely many times for any given base model. Furthermore, we call a set of terminating aspects confluent, if they, for any given base model, always yield the same final result regardless of the order in which they are applied. Since confluence must hold for any base model, this is a much stronger result than many of the current approaches that have addressed detection of aspect interactions limited to a specific base model. Our aspects are specified using standard sequence diagrams with some extensions. In this paper, we present a confluence theory specialized for our highly expressive aspect language. For the most expressive aspects, we prove that confluence is undecidable. For another class of aspects with considerable expressiveness, we prescribe an algorithm to check confluence. This algorithm is based on what we call an extended critical pair analysis. These results are useful both for modelers and researchers working with sequence diagram aspects and for researchers wanting to establish a confluence theory for other aspect-oriented modelling or model transformation approaches.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {789–824},
numpages = {36},
keywords = {Aspect, Aspect interaction, Aspect interference, Confluence, Critical pair, Graph transformation, Sequence diagram, UML, Weave}
}

@book{10.1145/3477355,
editor = {Jones, Cliff B. and Misra, Jayadev},
title = {Theories of Programming: The Life and Works of Tony Hoare},
year = {2021},
isbn = {9781450387286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {39},
abstract = {Sir Tony Hoare has had an enormous influence on computer science, from the Quicksort algorithm to the science of software development, concurrency and program verification. His contributions have been widely recognised: He was awarded the ACM’s Turing Award in 1980, the Kyoto Prize from the Inamori Foundation in 2000, and was knighted for “services to education and computer science” by Queen Elizabeth II of England in 2000.This book presents the essence of his various works—the quest for effective abstractions—both in his own words as well as chapters written by leading experts in the field, including many of his research collaborators. In addition, this volume contains biographical material, his Turing award lecture, the transcript of an interview and some of his seminal papers.Hoare’s foundational paper “An Axiomatic Basis for Computer Programming”, presented his approach, commonly known as Hoare Logic, for proving the correctness of programs by using logical assertions. Hoare Logic and subsequent developments have formed the basis of a wide variety of software verification efforts. Hoare was instrumental in proposing the Verified Software Initiative, a cooperative international project directed at the scientific challenges of large-scale software verification, encompassing theories, tools and experiments.Tony Hoare’s contributions to the theory and practice of concurrent software systems are equally impressive. The process algebra called Communicating Sequential Processes (CSP) has been one of the fundamental paradigms, both as a mathematical theory to reason about concurrent computation as well as the basis for the programming language occam. CSP served as a framework for exploring several ideas in denotational semantics such as powerdomains, as well as notions of abstraction and refinement. It is the basis for a series of industrial-strength tools which have been employed in a wide range of applications.This book also presents Hoare’s work in the last few decades. These works include a rigorous approach to specifications in software engineering practice, including procedural and data abstractions, data refinement, and a modular theory of designs. More recently, he has worked with collaborators to develop Unifying Theories of Programming (UTP). Their goal is to identify the common algebraic theories that lie at the core of sequential, concurrent, reactive and cyber-physical computations. Theories of Programming: The Life and Works of Tony Hoare’ is available as a printed book (DOI: ) and an on-line version. In addition to the book itself, a number of on-line resources might be of interest to readers:
A bibliography of Tony Hoare’s papers with clickable DOIs/URLs where available (ACM: INSERT URL)Appendix E of the book provides links to talks and interviews featuring Tony Hoare ()The Oxford archive of Hoare’s manuscripts:  
Supplementary Material: Tony Hoare’ is a PDF of additional material (not included in the book) containing the following:
Stories from a Life in Interesting Times (A transcription by Jayadev Misra of Tony Hoare’s acceptance speech for the 2000 Kyoto prize)Tony Hoare’s Heidelberg comments: (A transcription by Margaret Gray of Tony Hoare’s part in the 2020 Heidelberg event)Milestones in Tony’s Life and Work: A ‘cv’ of Tony Hoare prepared by Margaret GrayExtended version - ’Bernard Sufrin: Teaching at Belfast and Oxford’}
}

@inproceedings{10.1145/1007568.1007653,
author = {Chakrabarti, Kaushik and Chaudhuri, Surajit and Hwang, Seung-won},
title = {Automatic categorization of query results},
year = {2004},
isbn = {1581138598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1007568.1007653},
doi = {10.1145/1007568.1007653},
abstract = {Exploratory ad-hoc queries could return too many answers - a phenomenon commonly referred to as "information overload". In this paper, we propose to automatically categorize the results of SQL queries to address this problem. We dynamically generate a labeled, hierarchical category structure - users can determine whether a category is relevant or not by examining simply its label; she can then explore just the relevant categories and ignore the remaining ones, thereby reducing information overload. We first develop analytical models to estimate information overload faced by a user for a given exploration. Based on those models, we formulate the categorization problem as a cost optimization problem and develop heuristic algorithms to compute the min-cost categorization.},
booktitle = {Proceedings of the 2004 ACM SIGMOD International Conference on Management of Data},
pages = {755–766},
numpages = {12},
location = {Paris, France},
series = {SIGMOD '04}
}

@inbook{10.5555/1985653.1985660,
author = {Clemente, Patrice and Rouzaud-Cornabas, Jonathan and Toinard, Christian},
title = {From a generic framework for expressing integrity properties to a dynamic MAC enforcement for operating systems},
year = {2010},
isbn = {3642176968},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Protection deals with the enforcement of integrity and confidentiality. Integrity violations often lead to confidentiality vulnerabilities. This paper proposes a novel approach of Mandatory Access Control enforcement for guaranteeing a large range of integrity properties. In the literature, many integrity models are proposed such as the Biba model, data integrity, subject integrity, domain integrity and Trusted Path Execution. There can be numerous integrity models. In practice, an administrator needs to combine various integrity models. The major limitations of existing solutions deal first with the support of indirect activities aiming at violating integrity and second with the impossibility to extend existing models or even define new ones.This paper proposes a novel framework for expressing integrity requirements associated with direct or indirect activities, mostly in terms of information flows. It presents a formalization for the major integrity properties of the literature. The formalization of the required security is efficient and a straightforward enforcement is proposed. In contrast with our previous work, an information flow graph provides a dynamic analysis of the requested properties.The paper also provides a MAC implementation that enforces every integrity property supported by our formalization. Thus, a system call fails if it could violate the required security properties.A large scale experiment on high interaction honeypots shows the relevance, robustness and efficiency of our approach. This experimentation sets up two kinds of hosts. Hosts with our solution in IDS mode detect the violation of the requested properties. That IDS allows us to verify the completeness of our MAC protection. Hosts with our MAC protection guarantee all the required properties.},
booktitle = {Transactions on Computational Science XI: Special Issue on Security in Computing, Part II},
pages = {131–161},
numpages = {31}
}

@inbook{10.1145/3191315.3191321,
author = {De Cat, Broes and Bogaerts, Bart and Bruynooghe, Maurice and Janssens, Gerda and Denecker, Marc},
title = {Predicate logic as a modeling language: the IDP system},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191321},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {279–323},
numpages = {45}
}

@book{10.5555/2901596,
author = {Talia, Domenico and Trunfio, Paolo and Marozzo, Fabrizio},
title = {Data Analysis in the Cloud: Models, Techniques and Applications},
year = {2015},
isbn = {0128028815},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
edition = {1st},
abstract = {Data Analysis in the Cloud introduces and discusses models, methods, techniques, and systems to analyze the large number of digital data sources available on the Internet using the computing and storage facilities of the cloud. Coverage includes scalable data mining and knowledge discovery techniques together with cloud computing concepts, models, and systems. Specific sections focus on map-reduce and NoSQL models. The book also includes techniques for conducting high-performance distributed analysis of large data on clouds. Finally, the book examines research trends such as Big Data pervasive computing, data-intensive exascale computing, and massive social network analysis.Introduces data analysis techniques and cloud computing conceptsDescribes cloud-based models and systems for Big Data analyticsProvides examples of the state-of-the-art in cloud data analysisExplains how to develop large-scale data mining applications on cloudsOutlines the main research trends in the area of scalable Big Data analysis}
}

@article{10.1145/381790.381800,
author = {Svoboda, Frank and Maymir-Ducharme, Fred and Poulin, Jeff},
title = {SRI workshop summary: ”domain analysis in the DoD“},
year = {1996},
issue_date = {Jan 1 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/381790.381800},
doi = {10.1145/381790.381800},
abstract = {The Workshop on "Domain Analysis in the DoD," sponsored by the Software Reuse Initiative (SRI) and Defense Information Systems Agency (DISA), was held at MITRE Corporation, in McLean, Virginia on 26 - 27 September 1995. The primary purpose of the workshop was to discuss issues related to identifying and scoping domains with emphasis on product lines and to assess the usefulness of the strawman SRI Domain Scoping Framework as a proposed basis for this scoping activity. To this end, two specific objectives were identified:1. to identify the barriers facing Program Executive Offices (PEOs) and Program Managers (PMs) in incorporating domain analysis technology in their organizations and programs and2. to recommend a range of solutions and/or approaches to address and overcome these barriers.Many DoD and Industry programs can benefit from the application of domain analysis technology (concepts, processes, methods, and tools). However, PEOs/PMs rarely have enough information regarding why, when, or how to use domain analysis on their programs. A more basic question is whether there are sufficient reuse benefits within an organization/domain to warrant the associated investment in domain engineering - does a product line exist that justifies reuse costs? The solution to this problem involves bringing together leading domain analysis experts and PEO/PM representatives and managers to discuss a framework, currently under development by the DoD SRI, to guide DoD managers in applying domain analysis in their organizations and programs.The workshop brought together over fifty representatives from DoD, commercial, and academic organizations, with varied interests and perspectives on Domain Analysis, including those of methodologist, practitioner, management, and customer. The workshop approach included a Program Management panel, Domain Analysis Experts panel, a follow-up plenary discussion session and two working group sessions. In the working groups, the attendees were divided into 5 color-coded teams: Blue, Green, Gold, Orange, and Red, and were instructed to 1) identify issues in performing domain scoping, using the SRI Domain Scoping Framework as a "strawman"; and 2) make recommendations for improving the framework and applying its principles in real-world situations.The teams were chosen to provide a diversified mix of view-points. Each team included a facilitator/rapporteur (responsible for keeping the discussion activities relevant to the task at hand) and a Framework expert (who served as the technical authority on issues relating to framework content). The general team approach involved focusing on issues and actions within the individual teams and then looking across teams for common threads in the plenary sessions. The ultimate goal was to build consensus and move ahead with implementation recommendations.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {55–67},
numpages = {13}
}

@inbook{10.1145/3191315.3191323,
author = {Dal Pal\`{u}, Alessandro and Dovier, Agostino and Formisano, Andrea and Pontelli, Enrico},
title = {Exploring life: answer set programming in bioinformatics},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191323},
abstract = {This chapter provides a broad overview of howlogic programming, and more specifically Answer Set Programming (ASP), can be used to model and solve some popular and challenging classes of problems in the general domain of bioinformatics. In particular, the chapter explores the use of ASP in Genomics studies, such as Haplotype inference and Phylogenetic inference, in Structural studies, such as RNA secondary structure prediction and Protein structure prediction, and in Systems Biology. The chapter offers a brief introduction to biology and bioinformatics and working ASP code fragments for the various problems investigated. The chapter serves a dual role: (1) it offers a declarative characterization of a number of core problems in bioinformatics, making them easily understandable; and (2) it provides an "entry point" to the extensive literature on the use of logic-based methods to address such bioinformatics problems, by offering the basic modeling of the problem and pointers to the relevant literature.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {359–412},
numpages = {54}
}

@inbook{10.1145/3191315.3191319,
author = {Riguzzi, Fabrizio and Swift, Theresa},
title = {A survey of probabilistic logic programming},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191319},
abstract = {The combination of logic programming and probability has proven useful for modeling domains with complex and uncertain relationships among elements. Many probabilistic logic programming (PLP) semantics have been proposed; among these, the distribution semantics has recently gained increased attention and has been adopted by many languages such as the Independent Choice Logic, PRISM, Logic Programs with Annotated Disjunctions, ProbLog, and P-log.This chapter reviews the distribution semantics, beginning with the simplest case with stratified Datalog programs, and showing how the definition is extended to programs that include function symbols and non-stratified negation. The languages that adopt the distribution semantics are also discussed and compared both to one another and to Bayesian networks.We then survey existing approaches for inference in PLP languages that follow the distribution semantics. We concentrate on the PRISM, ProbLog, and PITA systems. The PRISM system was one of the first and can be applied when certain restrictions on the program hold. ProbLog introduced the use of Binary Decision Diagrams that provide a computational basis for removing these restrictions and so performing inference over more general classes of logic programs. PITA speeds up inference by using tabling and answer subsumption. It supports general probabilistic programs, but can easily be optimized for simpler settings and even possibilistic uncertain reasoning. The chapter also discusses the computational complexity of the various approaches together with techniques for limiting it by resorting to approximation.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {185–228},
numpages = {44}
}

@article{10.1162/comj.2007.31.2.87,
title = {Products of Interest},
year = {2007},
issue_date = {Summer 2007},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {31},
number = {2},
issn = {0148-9267},
url = {https://doi.org/10.1162/comj.2007.31.2.87},
doi = {10.1162/comj.2007.31.2.87},
journal = {Comput. Music J.},
month = jun,
pages = {87–98},
numpages = {12}
}

@inproceedings{10.1145/1852761.1852780,
author = {Bettini, Lorenzo and Damiani, Ferruccio and Schaefer, Ina and Strocco, Fabio},
title = {A prototypical Java-like language with records and traits},
year = {2010},
isbn = {9781450302692},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852761.1852780},
doi = {10.1145/1852761.1852780},
abstract = {Traits have been designed as units of fine-grained behavior reuse in the object-oriented paradigm. In this paper, we present the language Sugared Welterweight Record-Trait Java (SWRTJ), a Java dialect with records and traits. Records have been devised to complement traits for fine-grained state reuse. Records and traits can be composed by explicit linguistic operations, allowing code manipulations to achieve fine-grained code reuse. Classes are assembled from (composite) records and traits and instantiated to generate objects. We present the prototypical implementation of SWRTJ using Xtext, an Eclipse framework for the development of programming languages as well as other domain-specific languages. Our implementation comprises an Eclipse-based editor for SWRTJ with typical IDE functionalities, and a stand-alone compiler, which translates SWRTJ programs into standard Java programs.},
booktitle = {Proceedings of the 8th International Conference on the Principles and Practice of Programming in Java},
pages = {129–138},
numpages = {10},
keywords = {Eclipse, Java, implementation, trait, type system},
location = {Vienna, Austria},
series = {PPPJ '10}
}

@article{10.1016/j.dsp.2010.01.003,
author = {Lee, Suk-Hwan and Kwon, Ki-Ryong},
title = {CAD drawing watermarking scheme},
year = {2010},
issue_date = {September, 2010},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {20},
number = {5},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2010.01.003},
doi = {10.1016/j.dsp.2010.01.003},
abstract = {A CAD (computer-aided design) drawing based on vector data is very important art work in industrial fields. It is considered to be content for which copyright protection is urgently required. This paper presents a watermarking scheme based on k-means++ for CAD drawings. A CAD drawing consists of several layers, and each layer consists of various geometric objects such as LINE, POLYLINE, CIRCLE, ARC, 3DFACE, and POLYGON. POLYLINE along with LINE, 3DFACE, and ARC objects are the most commonly used fundamental objects in a CAD drawing. The proposed scheme embeds the watermark into the geometric distribution of POLYLINE, 3DFACE, and ARC objects in the main layers. Hence, in the proposed scheme, we select the target object with a high distribution among POLYLINE, 3DFACE, and ARC objects in a CAD drawing and then select layers that include the maximum number of instances of the target object. Then, we cluster the target objects in the selected layers by using k-means++ and embed the watermark into the geometric distribution of each group. The geometric distribution is the normalized length distribution in a POLYLINE object, the normalized area distribution in a 3DFACE object, and the angle distribution in an ARC object. Experimental results have verified that the proposed scheme is robust against file format conversion, layer attacks, and various types of geometric editing carried out using the CAD editing tools.},
journal = {Digit. Signal Process.},
month = sep,
pages = {1379–1399},
numpages = {21},
keywords = {3DFACE, ARC, CAD drawing, POLYLINE, Watermarking, k-means++}
}

@article{10.1016/j.scico.2011.06.007,
author = {Bettini, Lorenzo and Damiani, Ferruccio and Schaefer, Ina and Strocco, Fabio},
title = {TraitRecordJ: A programming language with traits and records},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {5},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2011.06.007},
doi = {10.1016/j.scico.2011.06.007},
abstract = {Traits have been designed as units for fine-grained reuse of behavior in the object-oriented paradigm. Records have been devised to complement traits for fine-grained reuse of state. In this paper, we present the language TraitRecordJ, a Java dialect with records and traits. Records and traits can be composed by explicit linguistic operations, allowing code manipulations to achieve fine-grained code reuse. Classes are assembled from (composite) records and traits and instantiated to generate objects. We introduce the language through examples and illustrate the prototypical implementation of TraitRecordJ using Xtext, an Eclipse framework for the development of programming languages as well as other domain-specific languages. Our implementation comprises an Eclipse-based editor for TraitRecordJ with typical IDE functionalities, and a stand-alone compiler, which translates TraitRecordJ programs into standard Java programs. As a case study, we present the TraitRecordJ implementation of a part of the software used in a web-based information system previously implemented in Java.},
journal = {Sci. Comput. Program.},
month = may,
pages = {521–541},
numpages = {21},
keywords = {Eclipse, Implementation, Java, Trait, Type system}
}

@article{10.1162/comj.2006.30.2.110,
author = {Staff},
title = {Products of Interest},
year = {2006},
issue_date = {June 2006},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {30},
number = {2},
issn = {0148-9267},
url = {https://doi.org/10.1162/comj.2006.30.2.110},
doi = {10.1162/comj.2006.30.2.110},
journal = {Comput. Music J.},
month = jun,
pages = {110–124},
numpages = {15}
}

@article{10.1109/TASLP.2015.2438543,
author = {Lee, Lin-shan and Glass, James and Lee, Hung-yi and Chan, Chun-an},
title = {Spoken content retrieval: beyond cascading speech recognition with text retrieval},
year = {2015},
issue_date = {September 2015},
publisher = {IEEE Press},
volume = {23},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2438543},
doi = {10.1109/TASLP.2015.2438543},
abstract = {Spoken content retrieval refers to directly indexing and retrieving spoken content based on the audio rather than text descriptions. This potentially eliminates the requirement of producing text descriptions for multimedia content for indexing and retrieval purposes, and is able to precisely locate the exact time the desired information appears in the multimedia. Spoken content retrieval has been very successfully achieved with the basic approach of cascading automatic speech recognition (ASR) with text information retrieval: after the spoken content is transcribed into text or lattice format, a text retrieval engine searches over the ASR output to find desired information. This framework works well when the ASR accuracy is relatively high, but becomes less adequate when more challenging real-world scenarios are considered, since retrieval performance depends heavily on ASR accuracy. This challenge leads to the emergence of another approach to spoken content retrieval: to go beyond the basic framework of cascading ASR with text retrieval in order to have retrieval performances that are less dependent on ASR accuracy. This overview article is intended to provide a thorough overview of the concepts, principles, approaches, and achievements of major technical contributions along this line of investigation. This includes five major directions: 1) Modified ASR for Retrieval Purposes: cascading ASR with text retrieval, but the ASR is modified or optimized for spoken content retrieval purposes; 2) Exploiting the Information not present in ASR outputs: to try to utilize the information in speech signals inevitably lost when transcribed into phonemes and words; 3) Directly Matching at the Acoustic Level without ASR: for spoken queries, the signals can be directly matched at the acoustic level, rather than at the phoneme or word levels, bypassing all ASR issues; 4) Semantic Retrieval of Spoken Content: trying to retrieve spoken content that is semantically related to the query, but not necessarily including the query terms themselves; 5) Interactive Retrieval and Efficient Presentation of the Retrieved Objects: with efficient presentation of the retrieved objects, an interactive retrieval process incorporating user actions may produce better retrieval results and user experiences.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1389–1420},
numpages = {32},
keywords = {graph-based random walk, interactive retrieval, joint optimization, key term extraction, pseudo-relevance feedback, query by example, query expansion, semantic retrieval, spoken content retrieval, spoken term detection, summarization, unsupervised acoustic pattern discovery}
}

@article{10.1162/comj.2007.31.1.109,
title = {Products of Interest},
year = {2007},
issue_date = {Spring 2007},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {31},
number = {1},
issn = {0148-9267},
url = {https://doi.org/10.1162/comj.2007.31.1.109},
doi = {10.1162/comj.2007.31.1.109},
journal = {Comput. Music J.},
month = mar,
pages = {109–122},
numpages = {14}
}

@article{10.1162/014892606776021290,
author = {Staff},
title = {Products of Interest},
year = {2006},
issue_date = {March 2006},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {30},
number = {1},
issn = {0148-9267},
url = {https://doi.org/10.1162/014892606776021290},
doi = {10.1162/014892606776021290},
journal = {Comput. Music J.},
month = mar,
pages = {103–117},
numpages = {15}
}

@article{10.1007/s00450-012-0232-2,
author = {Souza, V\'{\i}tor E. and Lapouchnian, Alexei and Angelopoulos, Konstantinos and Mylopoulos, John},
title = {Requirements-driven software evolution},
year = {2013},
issue_date = {November  2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {4},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-012-0232-2},
doi = {10.1007/s00450-012-0232-2},
abstract = {It is often the case that stakeholders want to strengthen/weaken or otherwise change their requirements for a system-to-be when certain conditions apply at runtime. For example, stakeholders may decide that if requirement  R  is violated more than  N  times in a week, it should be relaxed to a less demanding one  R  . Such  evolution requirements  play an important role in the lifetime of a software system in that they define possible changes to requirements, along with the conditions under which these changes apply. In this paper we focus on this family of requirements, how to model them and how to operationalize them at runtime. In addition, we evaluate our proposal with a case study adopted from the literature.},
journal = {Comput. Sci.},
month = nov,
pages = {311–329},
numpages = {19},
keywords = {Adaptive systems, Evolution, Modeling, Requirements, Requirements engineering}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@inproceedings{10.1109/KBSE.1992.252912,
author = {Smith, Tobiah E. and Setliff, Dorothy E.},
title = {Knowledge-based constraint-driven software synthesis},
year = {1992},
isbn = {0818628804},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/KBSE.1992.252912},
doi = {10.1109/KBSE.1992.252912},
abstract = {This paper describes a constraint-driven, real-time software synthesis architecture called RT-Syn. RT-Syn formulates design space constraints for each task in the real-time software system from timing requirements and a behavior description. RT-Syn then uses these constraints within a simulated annealing-like approach, selects an abstract implementation for every data structure and algorithm required io implement the desired behavior, and transforms these selections into executable code. We present experimental results covering ihe synthesis of two real-time software tasks that meet the desired constraints. These results illustrate the effectiveness of the simulated annealing-like approach in searching the software design space and the high reusability and maintainability provided by the use of synthesis technology.},
booktitle = {Proceedings of the 7th International Conference on Knowledge-Based Software Engineering},
pages = {18–27},
numpages = {10},
location = {McLean, Virginia, USA},
series = {KBSE'92}
}

@inproceedings{10.5555/2976248.2976434,
author = {Weiner, Inna and Hertz, Tomer and Nelken, Israel and Weinshall, Daphna},
title = {Analyzing auditory neurons by learning distance functions},
year = {2005},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a novel approach to the characterization of complex sensory neurons. One of the main goals of characterizing sensory neurons is to characterize dimensions in stimulus space to which the neurons are highly sensitive (causing large gradients in the neural responses) or alternatively dimensions in stimulus space to which the neuronal response are invariant (defining iso-response manifolds). We formulate this problem as that of learning a geometry on stimulus space that is compatible with the neural responses: the distance between stimuli should be large when the responses they evoke are very different, and small when the responses they evoke are similar. Here we show how to successfully train such distance functions using rather limited amount of information. The data consisted of the responses of neurons in primary auditory cortex (A1) of anesthetized cats to 32 stimuli derived from natural sounds. For each neuron, a subset of all pairs of stimuli was selected such that the responses of the two stimuli in a pair were either very similar or very dissimilar. The distance function was trained to fit these constraints. The resulting distance functions generalized to predict the distances between the responses of a test stimulus and the trained stimuli.},
booktitle = {Proceedings of the 19th International Conference on Neural Information Processing Systems},
pages = {1481–1488},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'05}
}

@inbook{10.1145/3191315.3191318,
author = {Truszczynski, Miroslaw},
title = {An introduction to the stable and well-founded semantics of logic programs},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191318},
abstract = {This chapter provides a brief introduction to two main semantics of logic programs with negation, the stable-model semantics of Gelfond and Lifschitz, and the well-founded semantics of Van Gelder, Ross, and Schlipf. We present definitions, introduce basic results, and relate the two semantics to each other. We restrict attention to the syntax of normal logic programs and focus on classical results. However, throughout the chapter and in concluding remarks we briefly discuss generalizations of the syntax and extensions of the semantics, and mention several recent developments.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {121–177},
numpages = {57}
}

@article{10.1016/S0167-4048(98)80100-4,
author = {Schwartau, Winn},
title = {Special feature: Time-based security explained: Provable security models and formulas for the practitioner and vendor},
year = {1998},
issue_date = {January, 1998},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {17},
number = {8},
issn = {0167-4048},
url = {https://doi.org/10.1016/S0167-4048(98)80100-4},
doi = {10.1016/S0167-4048(98)80100-4},
journal = {Comput. Secur.},
month = jan,
pages = {693–714},
numpages = {22}
}

@book{10.1145/2534860,
author = {Joint Task Force on Computing Curricula, Association for Computing Machinery (ACM) and IEEE Computer Society},
title = {Computer Science Curricula 2013: Curriculum Guidelines for Undergraduate Degree Programs in Computer Science},
year = {2013},
isbn = {9781450323093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@inproceedings{10.1145/234828.234846,
author = {Weiss, Ron and V\'{e}lez, Bienvenido and Sheldon, Mark A.},
title = {HyPursuit: a hierarchical network search engine that exploits content-link hypertext clustering},
year = {1996},
isbn = {0897917782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/234828.234846},
doi = {10.1145/234828.234846},
booktitle = {Proceedings of the the Seventh ACM Conference on Hypertext},
pages = {180–193},
numpages = {14},
location = {Bethesda, Maryland, USA},
series = {HYPERTEXT '96}
}

@inproceedings{10.1145/267437.267452,
author = {Balasubramanian, V. and Bashian, Alf and Porcher, Daniel},
title = {A large-scale hypermedia application using document management and Web technologies},
year = {1997},
isbn = {0897918665},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/267437.267452},
doi = {10.1145/267437.267452},
booktitle = {Proceedings of the Eighth ACM Conference on Hypertext},
pages = {134–145},
numpages = {12},
keywords = {WWW, distributed authoring, document management, information retrival, publishing, systematic hypermedia design, templates, views, workflow},
location = {Southampton, United Kingdom},
series = {HYPERTEXT '97}
}

@article{10.1016/j.is.2007.03.003,
author = {De Meo, Pasquale and Palopoli, Luigi and Quattrone, Giovanni and Ursino, Domenico},
title = {Combining Description Logics with synopses for inferring complex knowledge patterns from XML sources},
year = {2007},
issue_date = {December, 2007},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {32},
number = {8},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2007.03.003},
doi = {10.1016/j.is.2007.03.003},
abstract = {This paper illustrates how a Description Logics fragment, combined with a specific data compression technique, can be used for inferring complex intensional knowledge patterns from a set of semantically heterogeneous XML sources. The paper first provides a detailed description of the various steps of our approach; then, it describes some experiments performed to test its accuracy; after this, it presents a wide range of applications possibly benefiting from inferred patterns; finally, it compares our approach with the related ones previously proposed in the literature.},
journal = {Inf. Syst.},
month = dec,
pages = {1184–1224},
numpages = {41},
keywords = {Description Logics, Interschema property extraction, Synopses, XML}
}

@article{10.1023/A:1009736205722,
author = {Khoshgoftaar, Taghi M. and Allen, Edward B.},
title = {Classification of Fault-Prone Software Modules: Prior Probabilities,Costs, and Model Evaluation},
year = {1998},
issue_date = {September 1998},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {3},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1023/A:1009736205722},
doi = {10.1023/A:1009736205722},
abstract = {Software
quality models can give timely predictions of reliability indicators,
for targeting software improvement efforts. In some cases, classification
techniques are sufficient for useful software quality models.The software engineering
community has not applied informed prior probabilities widely
to software quality classification modeling studies. Moreover,
even though costs are of paramount concern to software managers,
costs of misclassification have received little attention in
the software engineering literature. This paper applies informed
prior probabilities and costs of misclassification to software
quality classification. We also discuss the advantages and limitations
of several statistical methods for evaluating the accuracy of
software quality classification models.We conducted two full-scale industrial case studies which integrated
these concepts with nonparametric discriminant analysis to illustrate
how they can be used by a classification technique. The case
studies supported our hypothesis that classification models of
software quality can benefit by considering informed prior probabilities
and by minimizing the expected cost of misclassifications. The
case studies also illustrated the advantages and limitations
of resubstitution, cross-validation, and data splitting for model
evaluation.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {275–298},
numpages = {24},
keywords = {cost, cross-validation, data splitting, fault-prone, misclassification, nonparametric discriminant analysis, resubstitution, software quality modeling}
}

@book{10.5555/2886235,
author = {Bird, Christian and Menzies, Tim and Zimmermann, Thomas},
title = {The Art and Science of Analyzing Software Data},
year = {2015},
isbn = {0124115195},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {The Art and Science of Analyzing Software Data provides valuable information on analysis techniques often used to derive insight from software data. This book shares best practices in the field generated by leading data scientists, collected from their experience training software engineering students and practitioners to master data science. The book covers topics such as the analysis of security data, code reviews, app stores, log files, and user telemetry, among others. It covers a wide variety of techniques such as co-change analysis, text analysis, topic analysis, and concept analysis, as well as advanced topics such as release planning and generation of source code comments. It includes stories from the trenches from expert data scientists illustrating how to apply data analysis in industry and open source, present results to stakeholders, and drive decisions.Presents best practices, hints, and tips to analyze data and apply tools in data science projectsPresents research methods and case studies that have emerged over the past few years to further understanding of software dataShares stories from the trenches of successful data science initiatives in industry}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@article{10.1007/BF03037371,
author = {Basta, Stefano and Flesca, Sergio and Greco, Sergio},
title = {Functional queries in Datalog},
year = {2002},
issue_date = {Dec 2002},
publisher = {Ohmsha},
address = {JPN},
volume = {20},
number = {4},
issn = {0288-3635},
url = {https://doi.org/10.1007/BF03037371},
doi = {10.1007/BF03037371},
abstract = {A ‘functional’ query is a query whose answer is always defined and unique i.e. it is either true or false in all models. It has been shown that the expressive powers of the various types of stable models, when restricted to the class of DATALOG¬ functional queries, do not in practice go beyond those of well-founded semantics, except for the least undefined stable models which, instead, capture the whole boolean hierarchyBH.In this paper we present a ‘functional’ language which, by means of a disciplined use of negation, achieves the desired level of expressiveness up toBH. Although the semantics of the new language is partial, all atoms in the source program are defined and possibly undefined atoms are introduced in a rewriting phase to increase the expressive power. We show that the language satisfies ‘desirable’ properties better than classical languages with (unstratified) negation and stable model semantics. We present an algorithm for the evaluation of functional queries and we show that exponential time resolution is required for hard problems only. Finally we present the architecture of a prototype of the language which has been developed.},
journal = {New Gen. Comput.},
month = dec,
pages = {339–371},
numpages = {33},
keywords = {Logic Programming, Datalog, Stable Models, Functional Queries, Expressive Power, Data Complexity}
}

@techreport{10.5555/886915,
author = {K. K., Ahuja and R. J. Gaeta, Jr},
title = {Active Control of Liner Impedance by Varying Perforate Orifice Geometry},
year = {2000},
publisher = {NASA Langley Technical Report Server},
abstract = {The present work explored the feasibility of controlling the acoustic impedance of a resonant type acoustic liner. This was accomplished by translating one perforate over another of the same porosity, creating a totally new perforate that had an intermediate porosity. This type of adjustable perforate created a variable orifice perforate with non-circular orifices. The key objective of the present study was to quantify the degree of attenuation control that can be achieved by applying such a concept to the buried septum in a two-degree-of-freedom (2DOF) acoustic liner. Different orifice shapes with equivalent area were also examined to determine if highly non-circular orifices had a significant impact on the impedance. This was primarily an experimental study conducted in the Aeroacoustics facilities at the Georgia Tech Research Institute Aerospace and Transportation Laboratory. Results indicated that perforate translational movements on the order of the orifice diameter (1.6 mm) resulted in shifting of the primary resonance frequency approximately 14 percent (200 Hz) and the secondary frequency approximately 16 percent (800 Hz). Furthermore, the variable orifice perforate was found to produce a lower mass reactance as the porosity was reduced than would be predicted by using an equivalent porosity perforate consisting of elliptical holes and linear theory.}
}

@article{10.1145/36077.36080,
author = {Huijsman, R. D. and van Katwijk, J. and Pronk, C. and Toetenel, W. J.},
title = {Translating algol 60 programs into Ada},
year = {1987},
issue_date = {Sept./Oct. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {VII},
number = {5},
issn = {1094-3641},
url = {https://doi.org/10.1145/36077.36080},
doi = {10.1145/36077.36080},
journal = {Ada Lett.},
month = sep,
pages = {42–50},
numpages = {9}
}

@article{10.1006/jvlc.2000.0184,
author = {POLESE, G. and PANNELLA, A. and TORTORA, G.},
title = {An Extended Relational Data Model for Multimedia DBMSs},
year = {2000},
issue_date = {December 2000},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {11},
number = {6},
issn = {1045-926X},
url = {https://doi.org/10.1006/jvlc.2000.0184},
doi = {10.1006/jvlc.2000.0184},
abstract = {We have extended the canonical relational data model to enable the management of multimedia objects. In an attempt to provide a smooth paradigm shift to multimedia information system development, we have enhanced the relational data model framework with techniques for modeling, storing and manipulating multimedia data. In particular, we have provided a graphical conceptual model for structuring a multimedia document and mapping rules for translating it into an extended relational data schema. Extensions have regarded the management of foreign keys, active components, mechanisms for the management of spatial and temporal relations, and finally functions for handling multimedia presentations. As a consequence, we have also provided extensions to the SQL language to handle these new mechanisms.},
journal = {J. Vis. Lang. Comput.},
month = dec,
pages = {663–685},
numpages = {23},
keywords = {RDBMS, SQL3, content-based retrieval., multimedia document, multimedia presentation, spatial relation, temporal relation}
}

@inproceedings{10.3115/981658.981692,
author = {Knight, Kevin and Hatzivassiloglou, Vasileios},
title = {Two-level, many-paths generation},
year = {1995},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/981658.981692},
doi = {10.3115/981658.981692},
abstract = {Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual. A robust generator must be able to operate well even when pieces of knowledge are missing. It must also be robust against incomplete or inaccurate inputs. To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods. We describe algorithms and show experimental results. We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable.},
booktitle = {Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics},
pages = {252–260},
numpages = {9},
location = {Cambridge, Massachusetts},
series = {ACL '95}
}

@inproceedings{10.1145/223904.223952,
author = {Yankelovich, Nicole and Levow, Gina-Anne and Marx, Matt},
title = {Designing SpeechActs: issues in speech user interfaces},
year = {1995},
isbn = {0201847051},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
address = {USA},
url = {https://doi.org/10.1145/223904.223952},
doi = {10.1145/223904.223952},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {369–376},
numpages = {8},
location = {Denver, Colorado, USA},
series = {CHI '95}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@article{10.1016/S0096-0551(97)00012-X,
author = {Chakravarty, Manuel M. T. and Lock, Hendrik C. R.},
title = {Towards the uniform implementation of declarative languages},
year = {1997},
issue_date = {July, 1997},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {23},
number = {2–4},
issn = {0096-0551},
url = {https://doi.org/10.1016/S0096-0551(97)00012-X},
doi = {10.1016/S0096-0551(97)00012-X},
abstract = {Current implementation techniques for functional languages differ considerably from those for logic languages. This complicates the development of flexible and efficient abstract machines that can be used for the compilation of declarative languages combining concepts of functional and logic programming. We propose an abstract machine, called the JUMP-machine, which systematically integrates the operational concepts needed to implement the functional and logic programming paradigm. The use of a tagless representation for heap objects, which originates from the Spineless Tagless G-machine, supports the integration of different concepts. In this paper, we provide a functional logic kernel language and show how to translate it into the abstract machine language of the JUMP-machine. Furthermore, we define the operational semantics of the machine language formally and discuss the mapping of the abstract machine to concrete machine architectures. We tested the approach by writing a compiler for the functional logic language GTML. The obtained performance results indicate that the proposed method allows to implement functional logic languages efficiently.},
journal = {Comput. Lang.},
month = jul,
pages = {121–160},
numpages = {40},
keywords = {abstract machine, functional programming, logic programming}
}

@inproceedings{10.1145/240678.240685,
author = {Meyer, Charles A. and Reznick, Michael G.},
title = {Design and test strategies for a safety-critical embedded executive},
year = {1996},
isbn = {0897918088},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/240678.240685},
doi = {10.1145/240678.240685},
booktitle = {Proceedings of the Conference on TRI-Ada '96: Disciplined Software Development with Ada},
pages = {29–37},
numpages = {9},
location = {Philadelphia, Pennsylvania, USA},
series = {TRI-Ada '96}
}

@article{10.1145/227604.227624,
author = {Mumick, Inderpal Singh and Finkelstein, Sheldon J. and Pirahesh, Hamid and Ramakrishnan, Raghu},
title = {Magic conditions},
year = {1996},
issue_date = {March 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {0362-5915},
url = {https://doi.org/10.1145/227604.227624},
doi = {10.1145/227604.227624},
abstract = {Much recent work has focused on the bottom-up evaluation of Datalog programs [Bancilhon and Ramakrishnan 1988]. One approach, called magic-sets, is based on rewriting a logic program so that bottom-up fixpoint evaluation of the program avoids generation of irrelevant facts [Bancilhon et al. 1986; Beeri and Ramakrishnan 1987; Ramakrishnan 1991]. It was widely believed for some time that the principal application of the magic-sets technique is to restrict computation in recursive queries using equijoin predicates. We extend the magic-sets transformation to use predicates other than equality (X&gt;10, for example) in restricting computation. The resulting ground magic-sets transformation is an important step in developing an extended magic-sets transformation that has practical utility in “real” relational databases, not only for recursive queries, but for nonrecursive queries as well [Mumick et al. 1990b; Mumick 1991].},
journal = {ACM Trans. Database Syst.},
month = mar,
pages = {107–155},
numpages = {49},
keywords = {Starburst, bottom-up evaluation, constraint logic programming, constraints, deductive databases, magic sets, query optimization, relational databases}
}

@book{10.5555/1564551,
author = {Cox, Ingemar and Miller, Matthew and Bloom, Jeffrey and Fridrich, Jessica and Kalker, Ton},
title = {Digital Watermarking and Steganography},
year = {2007},
isbn = {9780080555805},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Digital audio, video, images, and documents are flying through cyberspace to their respective owners. Unfortunately, along the way, individuals may choose to intervene and take this content for themselves. Digital watermarking and steganography technology greatly reduces the instances of this by limiting or eliminating the ability of third parties to decipher the content that he has taken. The many techiniques of digital watermarking (embedding a code) and steganography (hiding information) continue to evolve as applications that necessitate them do the same. The authors of this second edition provide an update on the framework for applying these techniques that they provided researchers and professionals in the first well-received edition. Steganography and steganalysis (the art of detecting hidden information) have been added to a robust treatment of digital watermarking, as many in each field research and deal with the other. New material includes watermarking with side information, QIM, and dirty-paper codes. The revision and inclusion of new material by these influential authors has created a must-own book for anyone in this profession. *This new edition now contains essential information on steganalysis and steganography *New concepts and new applications including QIM introduced *Digital watermark embedding is given a complete update with new processes and applications}
}

@inproceedings{10.1145/800008.808038,
author = {Slamecka, Vladimir},
title = {Conference abstracts},
year = {1977},
isbn = {9781450373739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800008.808038},
doi = {10.1145/800008.808038},
abstract = {One problem in computer program testing arises when errors are found and corrected after a portion of the tests have run properly. How can it be shown that a fix to one area of the code does not adversely affect the execution of another area? What is needed is a quantitative method for assuring that new program modifications do not introduce new errors into the code. This model considers the retest philosophy that every program instruction that could possibly be reached and tested from the modified code be retested at least once. The problem is how to determine the minimum number of test cases to be rerun. The process first involves generating the test case dependency matrix and the reachability matrix. Using the test case dependency matrix and the appropriate rows of the reachability matrix, a 0-1 integer program can be specified. The solution of the integer program yields the minimum number of test cases to be rerun, and the coefficients of the objective function identify which specific test cases to rerun.},
booktitle = {Proceedings of the 5th Annual ACM Computer Science Conference},
pages = {1–36},
numpages = {36},
series = {CSC '77}
}

@inbook{10.1145/234286.1057816,
author = {Whitaker, William A.},
title = {ADA---the project: the DoD high order language working group},
year = {1996},
isbn = {0201895021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/234286.1057816},
abstract = {The Department of Defense (DoD) High Order Language Commonality program began in 1975, with the goal of establishing a single high order computer programming language appropriate for DoD real-time embedded computer systems. A High Order Language Working Group (HOLWG) was chartered to formulate the DoD requirements for High Order Languages, to evaluate existing languages against those requirements, and to implement the minimal set of languages required for DoD use. Other parts of the effort included administrative initiatives toward the eventual goal: specifically, DoD Directive 5000.29, which provided that new defense systems should be programmed in a DoD "approved" and centrally controlled high order language, and DoD Instruction 5000.31, which gave the interim defining list of approved languages. The HOLWG language requirements were widely distributed for comment throughout the military and civil communities worldwide. Each successive version of the requirements, from STRAWMAN through STEELMAN, produced a more refined definition of the proposed language. During the requirement development process, it was determined that the set of requirements generated was both necessary and sufficient for all major DoD applications (and the analogous large commercial applications). Formal evaluations were performed on dozens of existing languages. It was concluded that no existing language could be adopted as a single common high order language for the DoD, but that a single language, meeting essentially all the requirements, was both feasible and desirable. Four contractors were funded to produce competitive prototypes. A first-phase evaluation reduced the designs to two, which were carried to completion. In turn, a single language design was subsequently chosen. Follow-on steps included the test and evaluation of the language, control of the language, and validation of compilers. The production of compilers and a program development and tool environment were to be accomplished separately by the individual Service Components. The general requirements and expectations for the environment and the control of the language were addressed in another iterative series of documents. A language validation capability (the test code suite), and associated facilities, were established to assure compliance to the language definition of compilers using the name "Ada." The name Ada was initially protected by a DoD-owned trademark.},
booktitle = {History of Programming Languages---II},
pages = {173–232},
numpages = {60}
}

@book{10.5555/2742301,
author = {Preim, Bernhard and Botha, Charl P.},
title = {Visual Computing for Medicine: Theory, Algorithms, and Applications},
year = {2013},
isbn = {9780124159792},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {Visual Computing for Medicine, Second Edition, offers cutting-edge visualization techniques and their applications in medical diagnosis, education, and treatment. The book includes algorithms, applications, and ideas on achieving reliability of results and clinical evaluation of the techniques covered. Preim and Botha illustrate visualization techniques from research, but also cover the information required to solve practical clinical problems. They base the book on several years of combined teaching and research experience. This new edition includes six new chapters on treatment planning, guidance and training; an updated appendix on software support for visual computing for medicine; and a new global structure that better classifies and explains the major lines of work in the field.}
}

@book{10.5555/2829067,
author = {Celko, Joe},
title = {Joe Celko's SQL for Smarties: Advanced SQL Programming},
year = {2014},
isbn = {0128007613},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {5th},
abstract = {SQL for Smarties was hailed as the first book devoted explicitly to the advanced techniques needed to transform an experienced SQL programmer into an expert. Now, 20 years later and in its fifth edition, this classic reference still reigns supreme as the only book written by a SQL master that teaches programmers and practitioners to become SQL masters themselves! These are not just tips and techniques; also offered are the best solutions to old and new challenges. Joe Celko conveys the way you need to think in order to get the most out of SQL programming efforts for both correctness and performance. New to the fifth edition, Joe features new examples to reflect the ANSI/ISO Standards so anyone can use it. He also updates data element names to meet new ISO-11179 rules with the same experience-based teaching style that made the previous editions the classics they are today. You will learn new ways to write common queries, such as finding coverings, partitions, runs in data, auctions and inventory, relational divisions and so forth. SQL for Smarties explains some of the principles of SQL programming as well as the code. A new chapter discusses design flaws in DDL, such as attribute splitting, non-normal forum redundancies and tibbling. There is a look at the traditional acid versus base transaction models, now popular in NoSQL products. Youll learn about computed columns and the DEFERRABLE options in constraints. An overview of the bi-temporal model is new to this edition and there is a longer discussion about descriptive statistic aggregate functions. The book finishes with an overview of SQL/PSM that is applicable to proprietary 4GL vendor extensions.New to the 5th Edition: Downloadable data sets, code samples, and vendor-specific implementations!Overview of the bitemporal modelExtended coverage of descriptive statistic aggregate functionsNew chapter covers flaws in DDLExamination of traditional acid versus base transaction modelsReorganized to help you navigate related topics with easeExpert advice from a noted SQL authority and award-winning columnist Joe Celko, who served on the ANSI SQL standards committee for over a decadeTeaches scores of advanced techniques that can be used with any product, in any SQL environment, whether it is SQL 92 or SQL 2011 Offers tips for working around deficiencies and gives insight into real-world challenges}
}

@techreport{10.1145/2594148,
title = {Computing Curricula 1991: Report of the ACM/IEEE-CS Joint Curriculum Task Force},
year = {1991},
isbn = {089793817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This report contains curricular recommendations for baccalaureate programs in the discipline of computing, which includes programs with the titles "computer science," "computer engineering," "computer science and engineering," and other similar titles. These recommendations provide a uniform basis for curriculum design across all segments of the educational community---schools and colleges of engineering, arts and sciences, and liberal arts. This report is the first comprehensive undergraduate curriculum report to be endorsed by the two major professional societies in the computing discipline---the Association for Computing Machinery and the Computer Society of the IEEE.}
}

@book{10.5555/1564784,
author = {Wang},
title = {System-on-Chip Test Architectures: Nanometer  Design for Testability},
year = {2007},
isbn = {9780080556802},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Modern electronics testing has a legacy of more than 40 years. The introduction of new technologies, especially nanometer technologies with 90nm or smaller geometry, has allowed the semiconductor industry to keep pace with the increased performance-capacity demands from consumers. As a result, semiconductor test costs have been growing steadily and typically amount to 40% of today's overall product cost. This book is a comprehensive guide to new VLSI Testing and Design-for-Testability techniques that will allow students, researchers, DFT practitioners, and VLSI designers to master quickly System-on-Chip Test architectures, for test debug and diagnosis of digital, memory, and analog/mixed-signal designs. KEY FEATURES * Emphasizes VLSI Test principles and Design for Testability architectures, with numerous illustrations/examples. * Most up-to-date coverage available, including Fault Tolerance, Low-Power Testing, Defect and Error Tolerance, Network-on-Chip (NOC) Testing, Software-Based Self-Testing, FPGA Testing, MEMS Testing, and System-In-Package (SIP) Testing, which are not yet available in any testing book. * Covers the entire spectrum of VLSI testing and DFT architectures, from digital and analog, to memory circuits, and fault diagnosis and self-repair from digital to memory circuits. * Discusses future nanotechnology test trends and challenges facing the nanometer design era; promising nanotechnology test techniques, including Quantum-Dots, Cellular Automata, Carbon-Nanotubes, and Hybrid Semiconductor/Nanowire/Molecular Computing. * Practical problems at the end of each chapter for students.}
}

@inproceedings{10.1145/1667239.1667254,
author = {Manocha, Dinesh and Calamia, Paul and Lin, Ming C. and Manocha, Dinesh and Savioja, Lauri and Tsingos, Nicolas},
title = {Interactive sound rendering},
year = {2009},
isbn = {9781450379380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1667239.1667254},
doi = {10.1145/1667239.1667254},
abstract = {An overview of algorithmic and software technologies related to interactive sound rendering. The course lectures cover three main topics: physically based techniques to synthesize sounds generated from colliding objects or liquid sounds, efficient computation of sound propagation paths based on reflection or diffraction paths and converting those paths into audible sound, exploiting the computational capabilities of current multi-core commodity processors for real-time sound propagation and sound rendering for gaming and interactive applications. The presentations include audio demonstrations that show the meaning of various processing components in practice.},
booktitle = {ACM SIGGRAPH 2009 Courses},
articleno = {15},
numpages = {338},
location = {New Orleans, Louisiana},
series = {SIGGRAPH '09}
}

@book{10.5555/2505467,
author = {Vacca, John R. and Vacca, John R.},
title = {Computer and Information Security Handbook, Second Edition},
year = {2013},
isbn = {0123943973},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2nd},
abstract = {Thesecond editionof this comprehensive handbook of computer and information securityprovides the most complete view of computer security and privacy available. It offers in-depth coverage of security theory, technology, and practice as they relate to established technologies as well as recent advances. It explores practical solutions to many security issues. Individual chapters are authored by leading experts in the field and address the immediate and long-term challenges in the authors' respective areas of expertise. The book is organized into10 parts comprised of70 contributed chapters by leading experts in the areas of networking and systems security, information management, cyber warfare and security, encryption technology, privacy, data storage, physical security, and a host of advanced security topics. New to this edition are chapters on intrusion detection, securing the cloud, securing web apps, ethical hacking, cyber forensics, physical security, disaster recovery, cyber attack deterrence, and more. Chapters by leaders in the field on theory and practice of computer and information security technology, allowing the reader to develop a new level of technical expertise Comprehensive and up-to-date coverage of security issues allows the reader to remain current and fully informed from multiple viewpoints Presents methods of analysis and problem-solving techniques, enhancing the reader's grasp of the material and ability to implement practical solutions}
}

@inbook{10.1145/800025.1198417,
author = {Griswold, Ralph E.},
title = {A history of the SNOBOL programming languages},
year = {1978},
isbn = {0127450408},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800025.1198417},
booktitle = {History of Programming Languages},
pages = {601–645},
numpages = {45}
}

@article{10.1145/960118.808393,
author = {Griswold, Ralph E.},
title = {A history of the SNOBOL programming languages},
year = {1978},
issue_date = {August 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/960118.808393},
doi = {10.1145/960118.808393},
abstract = {Development of the SNOBOL language began in 1962. It was followed by SNOBOL2, SNOBOL3, and SNOBOL4. Except for SNOBOL2 and SNOBOL3 (which were closely related), the others differ substantially and hence are more properly considered separate languages than versions of one language. In this paper historical emphasis is placed on the original language, SNOBOL, although important aspects of the subsequent languages are covered.},
journal = {SIGPLAN Not.},
month = aug,
pages = {275–308},
numpages = {34}
}

@book{10.5555/1537084,
author = {Shinder, Thomas W. and Shinder, Debra Littlejohn and Dimcev, Adrian F. and Eaton-Lee, James and Jones, Jason and Moffat, Steve},
title = {Dr. Tom Shinder's ISA Server 2006 Migration Guide},
year = {2007},
isbn = {9780080555515},
publisher = {Syngress Publishing},
abstract = {Dr. Tom Shinder's ISA Server 2006 Migration Guide provides a clear, concise, and thorough path to migrate from previous versions of ISA Server to ISA Server 2006. ISA Server 2006 is an incremental upgrade from ISA Server 2004, this book provides all of the tips and tricks to perform a successful migration, rather than rehash all of the features which were rolled out in ISA Server 2004. Also, learn to publish Exchange Server 2007 with ISA 2006 and to build a DMZ. * Highlights key issues for migrating from previous versions of ISA Server to ISA Server 2006. * Learn to Publish Exchange Server 2007 Using ISA Server 2006. * Create a DMZ using ISA Server 2006. * Dr. Tom Shinder's previous two books on configuring ISA Server have sold more than 50,000 units worldwide. * Dr. Tom Shinder is a Microsoft Most Valuable Professional (MVP) for ISA Server and a member of the ISA Server beta testing team. * This book will be the "Featured Product" on the Internet's most popular ISA Server site www.isaserver.org.}
}

